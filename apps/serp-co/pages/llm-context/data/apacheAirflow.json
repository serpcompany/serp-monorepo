[
  {
    "owner": "apache",
    "repo": "airflow",
    "content": "TITLE: Complex TaskFlow ETL Pipeline\nDESCRIPTION: Demonstrates a complete ETL pipeline using TaskFlow API with Asset handling, data transformation, and file operations.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/taskflow.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport pendulum\nimport requests\n\nfrom airflow import Asset\nfrom airflow.sdk import dag, task\n\nSRC = Asset(\n    \"https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/global/time-series/globe/land_ocean/ytd/12/1880-2022.json\"\n)\nnow = pendulum.now()\n\n\n@dag(start_date=now, schedule=\"@daily\", catchup=False)\ndef etl():\n    @task()\n    def retrieve(src: Asset) -> dict:\n        resp = requests.get(url=src.uri)\n        data = resp.json()\n        return data[\"data\"]\n\n    @task()\n    def to_fahrenheit(temps: dict[int, dict[str, float]]) -> dict[int, float]:\n        ret: dict[int, float] = {}\n        for year, info in temps.items():\n            ret[year] = float(info[\"anomaly\"]) * 1.8 + 32\n\n        return ret\n\n    @task()\n    def load(fahrenheit: dict[int, float]) -> Asset:\n        filename = \"/tmp/fahrenheit.json\"\n        s = json.dumps(fahrenheit)\n        f = open(filename, \"w\")\n        f.write(s)\n        f.close()\n\n        return Asset(f\"file:///{filename}\")\n\n    data = retrieve(SRC)\n    fahrenheit = to_fahrenheit(data)\n    load(fahrenheit)\n\n\netl()\n```\n\n----------------------------------------\n\nTITLE: Executing Python Callable with PythonOperator in Airflow\nDESCRIPTION: Shows how to use the classic PythonOperator to execute a Python callable in an Airflow DAG. This is an alternative to the @task decorator method.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef print_context(ds=None, **kwargs):\n    \"\"\"Print the Airflow context and ds variable from the context.\"\"\"\n    print(kwargs)\n    print(ds)\n    return 'Whatever you return gets printed in the logs'\n\nprint_the_context = PythonOperator(\n    task_id='print_the_context',\n    python_callable=print_context,\n)\n```\n\n----------------------------------------\n\nTITLE: Declaring an Airflow DAG using Context Manager in Python\nDESCRIPTION: This snippet demonstrates how to declare a DAG using Python's context manager (with statement). This approach implicitly adds operators to the DAG when they are defined within the context.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nfrom airflow.sdk import DAG\nfrom airflow.providers.standard.operators.empty import EmptyOperator\n\nwith DAG(\n    dag_id=\"my_dag_name\",\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@daily\",\n):\n    EmptyOperator(task_id=\"task\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic ETL Pipeline with TaskFlow API\nDESCRIPTION: Demonstrates a complete ETL pipeline implementation using TaskFlow API with extract, transform and load tasks. Uses decorators to define tasks and automatically handles data passing between them.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/taskflow.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=['example'],\n)\ndef tutorial_taskflow_api():\n    @task(multiple_outputs=True)\n    def extract():\n        data_string = '{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}'\n        order_data_dict = json.loads(data_string)\n        return order_data_dict\n\n    @task()\n    def transform(order_data_dict: dict):\n        total_order_value = sum(order_data_dict.values())\n        return {\"total_order_value\": total_order_value}\n\n    @task()\n    def load(total_order_value: float):\n        print(f\"Total order value is: {total_order_value:.2f}\")\n\n    order_data = extract()\n    order_summary = transform(order_data)\n    load(order_summary[\"total_order_value\"])\n\ntutorial_taskflow_api()\n```\n\n----------------------------------------\n\nTITLE: Creating a Hyperparameter Tuning Job in Deferrable Mode using Vertex AI Operator - Python\nDESCRIPTION: Demonstrates launching a hyperparameter tuning job with Airflow's CreateHyperparameterTuningJobOperator in deferrable mode, allowing the task to yield execution until completion. Dependencies and parameters mirror standard creation but with 'deferrable=True'. The job ID is available via XCom for subsequent tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ncreate_hpt_job_deferrable_task = CreateHyperparameterTuningJobOperator(\n    task_id=\"create_hptuning_job_vertex_ai_deferrable\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    display_name=\"your_hptuning_job_name\",\n    custom_job_spec=YOUR_CUSTOM_JOB_SPEC,\n    max_trial_count=MAX_TRIALS,\n    parallel_trial_count=PARALLEL_TRIALS,\n    study_spec=YOUR_STUDY_SPEC,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Queries in BigQuery with Airflow BigQueryInsertJobOperator (Sync and Async) - Python\nDESCRIPTION: Multiple snippets demonstrate how to use the BigQueryInsertJobOperator to submit SQL queries to BigQuery from Airflow, supporting both synchronous and deferrable (async) modes. Required dependencies include airflow.providers.google.cloud.operators.bigquery and Airflow's templating framework. Parameters include job_id, configuration (with query or DML SQL), and options for templated SQL or included config files. The operator can accept external .sql files using Jinja's include, supports idempotency using job_id, and can reattach to existing jobs for retries. Async mode requires a running Triggerer in Airflow. Outputs are query execution results, and limitations include error handling for template expansion and BigQuery job conflicts.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Submit a simple BigQuery query as an Airflow task\ndef bigquery_query():\n    query = \"SELECT 1 AS col\"\n    bq_query_task = BigQueryInsertJobOperator(\n        task_id=\"bq_query_task\",\n        configuration={\n            \"query\": {\n                \"query\": query,\n                \"useLegacySql\": False,\n            }\n        },\n    )\n    return bq_query_task\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Use BigQueryInsertJobOperator to execute a query from a template included via Jinja\nbq_select_job = BigQueryInsertJobOperator(\n    task_id=\"bq_select_job\",\n    configuration=\"{{ include 'queries/my_query_config.json' }}\",\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Use job_id parameter to ensure idempotent job submission\nbq_insert_job_with_id = BigQueryInsertJobOperator(\n    task_id=\"bq_insert_job_with_id\",\n    job_id=\"my_custom_job_id\",\n    configuration={\n        \"query\": {\n            \"query\": \"SELECT * FROM my_table\",\n            \"useLegacySql\": False,\n        }\n    },\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Submit a BigQuery job in deferrable (async) mode\ndef bigquery_insert_job_async():\n    return BigQueryInsertJobOperator(\n        task_id=\"bigquery_insert_job_async\",\n        deferrable=True,\n        configuration={\n            \"query\": {\n                \"query\": \"SELECT COUNT(*) FROM my_table\",\n                \"useLegacySql\": False,\n            }\n        },\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Detect Text in Image - Google Cloud Vision Airflow Operator - Python\nDESCRIPTION: This Airflow operator task example in Python illustrates how to perform text detection on an image using the CloudVisionDetectTextOperator. Required dependencies include the Airflow Google provider and the 'Retry' object for error handling. The operator requires parameters such as the GCP project ID, location, and image reference, and can return detected text attributes. Useful for automating OCR workflows in pipelines, and tailored for task-level configuration of GCP and input data.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndetect_text = CloudVisionDetectTextOperator(\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n    image=GCP_VISION_IMAGE,\n    retry=Retry(),\n    task_id=\"detect_text\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Importing CSV Data with DatabricksCopyIntoOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the DatabricksCopyIntoOperator to import CSV data into a Databricks table. It specifies the table name, file location, file format, and SQL endpoint name as required parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/copy_into.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDatabricksCopyIntoOperator(\n    task_id=\"copy_into_table\",\n    table_name=\"default.my_table\",\n    file_location=\"s3://my-bucket/path/to/files\",\n    file_format=\"CSV\",\n    sql_endpoint_name=\"My SQL Endpoint\",\n    force_copy=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Basic TaskFlow Example with Email Notification\nDESCRIPTION: Demonstrates how to use TaskFlow API to create tasks that get an IP address and compose an email, showing automatic XCom handling and task dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/taskflow.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import task\nfrom airflow.providers.smtp.operators.smtp import EmailOperator\n\n@task\ndef get_ip():\n    return my_ip_service.get_main_ip()\n\n@task(multiple_outputs=True)\ndef compose_email(external_ip):\n    return {\n        'subject':f'Server connected from {external_ip}',\n        'body': f'Your server executing Airflow is connected from the external IP {external_ip}<br>'\n    }\n\nemail_info = compose_email(get_ip())\n\nEmailOperator(\n    task_id='send_email_notification',\n    to='example@example.com',\n    subject=email_info['subject'],\n    html_content=email_info['body']\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Python Callable with @task Decorator in Airflow\nDESCRIPTION: Demonstrates how to use the @task decorator to execute a Python callable in an Airflow DAG. This method is recommended over the classic PythonOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@task(task_id=\"print_the_context\")\ndef print_context(ds=None, **kwargs):\n    \"\"\"Print the Airflow context and ds variable from the context.\"\"\"\n    print(kwargs)\n    print(ds)\n    return 'Whatever you return gets printed in the logs'\n```\n\n----------------------------------------\n\nTITLE: Listing Transfer Operations with Airflow and Google Cloud Storage Transfer Service in Python\nDESCRIPTION: This Python code shows how to use Airflow's CloudDataTransferServiceListOperationsOperator to list transfer operations from Google Cloud Storage Transfer Service. The operator returns results to XCOM, and can be templated with dynamic fields. Dependencies include Apache Airflow with the Google provider installed, and users must supply proper GCP credentials. Key parameters define the transfer job and operation query filters, and the output is a list of matching operations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlist_operations = CloudDataTransferServiceListOperationsOperator(\n    task_id=\"list_operations\",\n    job_name=TRANSFER_JOB_NAME,\n    filter={\"project_id\": PROJECT_ID},\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"job_name\",\n    \"filter\",\n    \"gcp_conn_id\",\n)\n```\n\n----------------------------------------\n\nTITLE: Nested Field Templating with PythonOperator in Python\nDESCRIPTION: Demonstrates how to use templating with nested fields in custom classes used as arguments for PythonOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MyDataReader:\n    template_fields: Sequence[str] = (\"path\",)\n\n    def __init__(self, my_path):\n        self.path = my_path\n\n    # [additional code here...]\n\n\nt = PythonOperator(\n    task_id=\"transform_data\",\n    python_callable=transform_data,\n    op_args=[MyDataReader(\"/tmp/{{ ds }}/my_file\")],\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Using Jinja Templating with BashOperator in Python\nDESCRIPTION: Shows how to use Jinja templating to pass the start of the data interval as an environment variable to a Bash script using the BashOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# The start of the data interval as YYYY-MM-DD\ndate = \"{{ ds }}\"\nt = BashOperator(\n    task_id=\"test_env\",\n    bash_command=\"/tmp/test.sh \",\n    dag=dag,\n    env={\"DATA_INTERVAL_START\": date},\n)\n```\n\n----------------------------------------\n\nTITLE: Starting a Google Cloud Dataproc Cluster\nDESCRIPTION: This code uses the DataprocStartClusterOperator to start a previously stopped Dataproc cluster. This allows for cost savings by stopping clusters when not in use.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nstart_cluster = DataprocStartClusterOperator(\n    task_id=\"start_cluster\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster_name=CLUSTER_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from S3 to Azure Blob Storage with S3ToWasbOperator (Airflow, Python)\nDESCRIPTION: This code demonstrates how to use the S3ToWasbOperator within an Apache Airflow DAG to copy a file from an Amazon S3 bucket into an Azure Blob Storage container. It requires the `airflow.providers.microsoft.azure` and `airflow.providers.amazon.aws` packages, configured Azure and AWS connections, and valid access credentials. The operator expects parameters for source S3 bucket/key, target Azure container/blob, and connection IDs. The task outputs a file transfer into Azure Blob Storage and is suitable for workflows needing cross-cloud data ETL.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/transfer/s3_to_wasb.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_s3_to_wasb\",\n    schedule_interval=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    transfer_task = S3ToWasbOperator(\n        task_id=\"copy_s3_to_wasb\",\n        file_source=\"S3://my-bucket/data.csv\",\n        container_name=\"my-azure-container\",\n        blob_name=\"data.csv\",\n        wasb_conn_id=\"azure_blob_storage_default\",\n        aws_conn_id=\"aws_default\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Invoking Claude V3 Sonnet Model using Messages API (Python)\nDESCRIPTION: Demonstrates invoking the Claude V3 Sonnet model via the Messages API using BedrockInvokeModelOperator. The code constructs the message body for a chat-based interaction and sends it to Bedrock, obtaining the AI's structured response. Requires proper permissions for Claude v3 family.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclaude_v3_sonnet_task = BedrockInvokeModelOperator(\n    task_id=\"claude_v3_sonnet_messages\",\n    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    body={\n        \"messages\": [{\"role\": \"user\", \"content\": \"Summarize the benefits of serverless architectures.\"}]\n    },\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Item Deferrable Operator in Python\nDESCRIPTION: Example of an operator that processes multiple items with deferral, showing how to handle item iteration and result processing across deferrals.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/deferring.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom airflow.sdk import BaseOperator\nfrom airflow.triggers.base import BaseTrigger, TriggerEvent\n\n\nclass MyItemTrigger(BaseTrigger):\n    def __init__(self, item):\n        super().__init__()\n        self.item = item\n\n    def serialize(self):\n        return (self.__class__.__module__ + \".\" + self.__class__.__name__, {\"item\": self.item})\n\n    async def run(self):\n        result = None\n        try:\n            # Somehow process the item to calculate the result\n            ...\n            yield TriggerEvent({\"result\": result})\n        except Exception as e:\n            yield TriggerEvent({\"error\": str(e)})\n\n\nclass MyItemsOperator(BaseOperator):\n    def __init__(self, items, **kwargs):\n        super().__init__(**kwargs)\n        self.items = items\n\n    def execute(self, context, current_item_index=0, event=None):\n        last_result = None\n        if event is not None:\n            # execute method was deferred\n            if \"error\" in event:\n                raise Exception(event[\"error\"])\n            last_result = event[\"result\"]\n            current_item_index += 1\n\n        try:\n            current_item = self.items[current_item_index]\n        except IndexError:\n            return last_result\n\n        self.defer(\n            trigger=MyItemTrigger(item),\n            method_name=\"execute\",  # The trigger will call this same method again\n            kwargs={\"current_item_index\": current_item_index},\n        )\n```\n\n----------------------------------------\n\nTITLE: Submitting a Google Cloud Batch Job using CloudBatchSubmitJobOperator\nDESCRIPTION: Demonstrates how to use the CloudBatchSubmitJobOperator to submit a job to Google Cloud Batch. This operator waits for the job to complete and pushes the job's dictionary representation to XCom.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_batch.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsubmit_job = CloudBatchSubmitJobOperator(\n    task_id=\"submit_job\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    job=job,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring KubernetesExecutor Task Settings in Python\nDESCRIPTION: Example of setting executor-specific configuration for a task, demonstrating how to specify a custom Docker image for the KubernetesExecutor.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/tasks.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nMyOperator(...,\n    executor_config={\n        \"KubernetesExecutor\":\n            {\"image\": \"myCustomDockerImage\"}\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Declaring an Airflow DAG using Decorator in Python\nDESCRIPTION: This snippet demonstrates using the @dag decorator to transform a function into a DAG generator. This is a more modern, functional approach to DAG creation that can simplify complex DAG definitions.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nfrom airflow.sdk import dag\nfrom airflow.providers.standard.operators.empty import EmptyOperator\n\n\n@dag(start_date=datetime.datetime(2021, 1, 1), schedule=\"@daily\")\ndef generate_dag():\n    EmptyOperator(task_id=\"task\")\n\n\ngenerate_dag()\n```\n\n----------------------------------------\n\nTITLE: Importing Airflow Modules\nDESCRIPTION: Essential module imports required for creating an Airflow DAG, including datetime and DAG objects.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/fundamentals.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.utils.dates import days_ago\n```\n\n----------------------------------------\n\nTITLE: Defining Airflow Plugin Class in Python\nDESCRIPTION: This snippet demonstrates how to create an AirflowPlugin class with various components such as macros, Flask blueprints, FastAPI apps, AppBuilder views, and menu items.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/plugins.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass AirflowTestPlugin(AirflowPlugin):\n    name = \"test_plugin\"\n    macros = [plugin_macro]\n    flask_blueprints = [bp]\n    fastapi_apps = [app_with_metadata]\n    fastapi_root_middlewares = [middleware_with_metadata]\n    appbuilder_views = [v_appbuilder_package, v_appbuilder_nomenu_package]\n    appbuilder_menu_items = [appbuilder_mitem, appbuilder_mitem_toplevel]\n```\n\n----------------------------------------\n\nTITLE: Analyzing Data with DuckDB\nDESCRIPTION: Task that uses DuckDB to perform SQL analysis on Parquet data stored in cloud storage. Shows integration between ObjectStoragePath and DuckDB.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/objectstorage.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef analyze(path: ObjectStoragePath) -> pd.DataFrame:\n    \"\"\"Analyze air quality data using DuckDB.\"\"\"\n    import duckdb\n\n    # Register the filesystem with DuckDB\n    duckdb.register_filesystem(path.fs)\n\n    # Create connection and query data\n    con = duckdb.connect()\n    df = con.execute(\n        f\"\"\"\n        SELECT\n            date_trunc('hour', timestamp) as hour,\n            avg(no2_concentration) as avg_no2,\n            avg(pm25_concentration) as avg_pm25\n        FROM read_parquet('{path.as_uri()}')\n        GROUP BY 1\n        ORDER BY 1\n        \"\"\"\n    ).df()\n    return df\n```\n\n----------------------------------------\n\nTITLE: Boolean Parameter Configuration in Python\nDESCRIPTION: Example of configuring a boolean parameter that generates a toggle button in the UI.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/params.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nParam(True, type=\"boolean\")\n```\n\n----------------------------------------\n\nTITLE: Creating Dataproc Metastore Service using Airflow Operator in Python\nDESCRIPTION: Demonstrates the usage of the `DataprocMetastoreCreateServiceOperator` within an Airflow DAG to provision a new Google Cloud Dataproc Metastore service. Key parameters include `region`, `project_id`, `service` (the configuration dictionary), and `service_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncreate_service = DataprocMetastoreCreateServiceOperator(\n    task_id=\"create_service\",\n    region=REGION,\n    project_id=PROJECT_ID,\n    service=SERVICE,\n    service_id=SERVICE_ID,\n    trigger_rule=TriggerRule.ALL_DONE,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Configuration via Environment Variables in Bash\nDESCRIPTION: These bash commands demonstrate how to set Airflow configuration options using environment variables. This method is recommended for configurations that change across deployments.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/production-deployment.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW__DATABASE__SQL_ALCHEMY_CONN=my_conn_id\nAIRFLOW__WEBSERVER__BASE_URL=http://host:port\n```\n\n----------------------------------------\n\nTITLE: Setting Default DAG Arguments\nDESCRIPTION: Defines default arguments for the DAG including owner, start date, email settings, and retry configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/fundamentals.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'email': ['airflow@example.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n```\n\n----------------------------------------\n\nTITLE: Multiple Parameter Assignment with expand_kwargs\nDESCRIPTION: Shows how to assign multiple parameters to non-TaskFlow operators using expand_kwargs function with BashOperator and PythonOperator examples.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nBashOperator.partial(task_id=\"bash\").expand_kwargs(\n    [\n        {\"bash_command\": \"echo $ENV1\", \"env\": {\"ENV1\": \"1\"}},\n        {\"bash_command\": \"printf $ENV2\", \"env\": {\"ENV2\": \"2\"}},\n    ],\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef print_args(x, y):\n    print(x)\n    print(y)\n    return x + y\n\n\nPythonOperator.partial(task_id=\"task-1\", python_callable=print_args).expand_kwargs(\n    [\n        {\"op_kwargs\": {\"x\": 1, \"y\": 2}, \"show_return_value_in_logs\": True},\n        {\"op_kwargs\": {\"x\": 3, \"y\": 4}, \"show_return_value_in_logs\": False},\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Running a Pod on GKE with XCom Support using GKEStartPodOperator\nDESCRIPTION: Example of using the GKEStartPodOperator to run a pod on a Google Kubernetes Engine cluster with XCom enabled, allowing data to be passed between tasks via a sidecar container.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npod_task = GKEStartPodOperator(\n    task_id=\"pod_task\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    cluster_name=CLUSTER_NAME,\n    namespace=\"default\",\n    image=\"perl\",\n    name=\"test-pod\",\n    cmds=[\"perl\"],\n    arguments=[\"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"],\n    is_delete_operator_pod=True,\n    startup_check_interval_seconds=120,\n    get_logs=True,\n    xcom_push=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Loading Data into Postgres using Python in Airflow\nDESCRIPTION: Python task that downloads a CSV file from a URL, saves it locally, and then loads it into a Postgres table using PostgresHook. This demonstrates combining HTTP requests, file operations, and database interactions in an Airflow task.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/pipeline.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport requests\nfrom airflow.sdk import task\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\n\n\n@task\ndef get_data():\n    # NOTE: configure this as appropriate for your Airflow environment\n    data_path = \"/opt/airflow/dags/files/employees.csv\"\n    os.makedirs(os.path.dirname(data_path), exist_ok=True)\n\n    url = \"https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/tutorial/pipeline_example.csv\"\n\n    response = requests.request(\"GET\", url)\n\n    with open(data_path, \"w\") as file:\n        file.write(response.text)\n\n    postgres_hook = PostgresHook(postgres_conn_id=\"tutorial_pg_conn\")\n    conn = postgres_hook.get_conn()\n    cur = conn.cursor()\n    with open(data_path, \"r\") as file:\n        cur.copy_expert(\n            \"COPY employees_temp FROM STDIN WITH CSV HEADER DELIMITER AS ',' QUOTE '\\\"'\",\n            file,\n        )\n    conn.commit()\n```\n\n----------------------------------------\n\nTITLE: Instantiating SQLExecuteQueryOperator - Apache Airflow - Python\nDESCRIPTION: Demonstrates how to create a SQLExecuteQueryOperator task in Apache Airflow to execute SQL statements against a target database. Requires Airflow with provider 'common.sql.operators.sql', and appropriate database connection setup. Key parameters include 'sql' (string, list, or template), 'autocommit' (whether to commit transactions), 'parameters' (for templating), 'handler' (to process cursor), 'split_statements' (to split SQL scripts), and 'return_last' (to control result fetching). Inputs are SQL code/config, and output is the query execution result as handled. Dependencies: Airflow DAG context and valid DB connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSQLExecuteQueryOperator(\n    task_id=\"execute_sql_query\",\n    sql=\"SELECT COUNT(*) FROM users;\",\n    autocommit=True,\n    parameters=None,\n    handler=None,\n    split_statements=False,\n    return_last=True,\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Watcher Pattern in Airflow DAG\nDESCRIPTION: Example implementation of a watcher pattern DAG that monitors task states and fails the DAG run if any task fails. Includes a failing task, passing task, teardown task with ALL_DONE trigger rule, and a watcher task with ONE_FAILED trigger rule.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom airflow.sdk import DAG\nfrom airflow.sdk import task\nfrom airflow.exceptions import AirflowException\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n@task(trigger_rule=TriggerRule.ONE_FAILED, retries=0)\ndef watcher():\n    raise AirflowException(\"Failing task because one or more upstream tasks failed.\")\n\n\nwith DAG(\n    dag_id=\"watcher_example\",\n    schedule=\"@once\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    failing_task = BashOperator(task_id=\"failing_task\", bash_command=\"exit 1\", retries=0)\n    passing_task = BashOperator(task_id=\"passing_task\", bash_command=\"echo passing_task\")\n    teardown = BashOperator(\n        task_id=\"teardown\",\n        bash_command=\"echo teardown\",\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n\n    failing_task >> passing_task >> teardown\n    list(dag.tasks) >> watcher()\n```\n\n----------------------------------------\n\nTITLE: Overriding template_ext in Python\nDESCRIPTION: Shows how to override the template_ext property of an operator to prevent Airflow from treating a value as a file reference.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfixed_print_script = BashOperator(\n    task_id=\"fixed_print_script\",\n    bash_command=\"cat script.sh\",\n)\nfixed_print_script.template_ext = ()\n```\n\n----------------------------------------\n\nTITLE: Creating AutoML Tabular Training Job in Vertex AI\nDESCRIPTION: Example demonstrating how to create an AutoML tabular training job using the CreateAutoMLTabularTrainingJobOperator. Requires a pre-created Tabular dataset.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nCreateAutoMLTabularTrainingJobOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    display_name=TRAINING_JOB_NAME,\n    dataset_id=TABULAR_DATASET,\n    target_column=TARGET_COLUMN,\n    prediction_type=\"classification\",\n    sync=True,\n    task_id=\"training_job\",)\n```\n\n----------------------------------------\n\nTITLE: Instantiating SparkKubernetesOperator in Python\nDESCRIPTION: Demonstrates how to instantiate the `SparkKubernetesOperator` within an Airflow DAG task. Key parameters include `task_id`, the Spark application `image`, the `code_path` (local path to the Spark code), and the `application_file` which points to the YAML or JSON configuration template.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. code-block:: python\n\n    SparkKubernetesOperator(\n        task_id=\"spark_task\",\n        image=\"gcr.io/spark-operator/spark-py:v3.1.1\",  # OR custom image using that\n        code_path=\"local://path/to/spark/code.py\",\n        application_file=\"spark_job_template.yaml\",  # OR spark_job_template.json\n        dag=dag,\n    )\n```\n\n----------------------------------------\n\nTITLE: Using SparkSubmitOperator in Apache Airflow DAG\nDESCRIPTION: Example of using SparkSubmitOperator to launch Spark applications. This operator uses the spark-submit script to set up the classpath and supports different cluster managers and deploy modes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/operators.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspark_submit = SparkSubmitOperator(\n    task_id=\"spark_submit_task\",\n    application=\"test_application.py\",\n    conn_id=\"spark_default\",\n    verbose=True,\n    execution_timeout=timedelta(minutes=10),\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing Operator UI Appearance\nDESCRIPTION: Example showing how to customize operator appearance in the Airflow UI by setting color and display name properties.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass HelloOperator(BaseOperator):\n    ui_color = \"#ff0000\"\n    ui_fgcolor = \"#000000\"\n    custom_operator_name = \"Howdy\"\n    # ...\n```\n\n----------------------------------------\n\nTITLE: Building Bash Command Dynamically using Python Function with @task.bash\nDESCRIPTION: Shows how to dynamically construct a Bash command within a `@task.bash` task by calling another Python function (`_build_cmd`). This allows for complex command generation logic using Python before the Bash command is executed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef _build_cmd(prefix: str) -> str:\n    return f\"{prefix} world\"\n\n\n@task.bash\ndef build_command_function_output() -> str:\n    return _build_cmd(\"hello\")\n\n\nbuild_command_function_output()\n```\n\n----------------------------------------\n\nTITLE: Managing Expensive Imports in Airflow DAGs\nDESCRIPTION: Example demonstrating how to handle expensive imports in Airflow DAGs by moving them into task scope rather than at the top level. This approach reduces DAG parsing time by only importing heavy libraries when tasks actually run.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# It's ok to import modules that are not expensive to load at top-level of a DAG file\nimport random\nimport pendulum\n\n# Expensive imports should be avoided as top level imports, because DAG files are parsed frequently, resulting in top-level code being executed.\n#\n# import pandas\n# import torch\n# import tensorflow\n#\n\n...\n\n\n@task()\ndef do_stuff_with_pandas_and_torch():\n    import pandas\n    import torch\n\n    # do some operations using pandas and torch\n\n\n@task()\ndef do_stuff_with_tensorflow():\n    import tensorflow\n\n    # do some operations using tensorflow\n```\n\n----------------------------------------\n\nTITLE: Triggering Airbyte Sync Job Asynchronously with Python\nDESCRIPTION: This Python code snippet shows how to use the AirbyteTriggerSyncOperator to trigger an Airbyte synchronization job asynchronously by setting `asynchronous=True`. The operator starts the job specified by `connection_id` using the Airbyte connection (`airbyte_conn_id`) but does not wait for completion. Instead, it returns the Airbyte `job_id`, which can be used with sensors like AirbyteSensor to monitor the job status. Requires the `airflow.providers.airbyte.operators.AirbyteTriggerSyncOperator` class and a configured Airbyte connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/operators/airbyte.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrigger_sync_async = AirbyteTriggerSyncOperator(\n    task_id=\"airbyte_sync_source_destination_op_async\",\n    airbyte_conn_id=\"airbyte_conn_example\",\n    connection_id=\"<YOUR_AIRBYTE_CONNECTION_ID>\",\n    asynchronous=True,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Generating Content with Vertex AI using Airflow Operator in Python\nDESCRIPTION: Shows the usage of `GenerativeModelGenerateContentOperator` from `airflow.providers.google.cloud.operators.vertex_ai.generative_model` to generate content with a Vertex AI generative model. The model's response is pushed to XCom under the 'model_response' key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_48\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/vertex_ai/example_vertex_ai_generative_model.py\n    :language: python\n    :dedent: 4\n    :start-after: [START how_to_cloud_vertex_ai_generative_model_generate_content_operator]\n    :end-before: [END how_to_cloud_vertex_ai_generative_model_generate_content_operator]\n```\n\n----------------------------------------\n\nTITLE: Instantiating the DAG\nDESCRIPTION: Instantiates the ProcessEmployees DAG class and assigns it to a variable that Airflow will recognize.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/pipeline.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndag = ProcessEmployees()\n```\n\n----------------------------------------\n\nTITLE: Detect Document Text in Image - Google Cloud Vision Airflow Operator - Python\nDESCRIPTION: This Python snippet provides an Airflow operator task setup to detect document text from an image, using CloudVisionTextDetectOperator. It requires Airflow with the Google provider, and utilizes GCP-specific identifiers and input image data. Inputs are the project, location, and image parameters, outputting detailed document text information suitable for advanced OCR pipelines.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndocument_detect_text = CloudVisionTextDetectOperator(\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n    image=GCP_VISION_IMAGE,\n    retry=Retry(),\n    task_id=\"document_detect_text\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing External Python Task with TaskFlow API\nDESCRIPTION: Example showing how to execute Python code in a pre-defined environment using the @task.external_python decorator. The task uses a specific Python binary path and requires numpy package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@task.external_python(python=PATH_TO_PYTHON_BINARY, requirements=['numpy'])\ndef external_python_task():\n    import numpy\n    print(f\"Numpy version: {numpy.__version__}\")\n    return \"done\"\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-common-compat via pip - Bash\nDESCRIPTION: This bash code snippet demonstrates how to install the apache-airflow-providers-common-compat provider package for Apache Airflow using pip, including optional extras for cross-provider dependencies. It is intended for use in terminal environments with an existing Airflow 2 installation (minimum version 2.9.0). The key parameter is the package name, with extras such as [openlineage] to pull in additional, related provider distributions as needed. The expected output is the pip installation process, which may raise errors if prerequisites such as the correct Airflow version are not met.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/compat/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-common-compat[openlineage]\n```\n\n----------------------------------------\n\nTITLE: Executing Python Callable in Virtual Environment with @task.virtualenv Decorator\nDESCRIPTION: Demonstrates how to use the @task.virtualenv decorator to execute a Python callable inside a new Python virtual environment in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@task.virtualenv(\n    task_id=\"virtualenv_python\",\n    requirements=[\"colorama==0.4.0\"],\n    system_site_packages=False,\n)\ndef callable_virtualenv():\n    from time import sleep\n    from colorama import Back, Fore, Style\n\n    print(Fore.RED + 'some red text')\n    print(Back.GREEN + 'and with a green background')\n    print(Style.DIM + 'and in dim text')\n    print(Style.RESET_ALL)\n    for _ in range(4):\n        print(Style.DIM + 'Please wait...', flush=True)\n        sleep(1)\n    print('Finished')\n    return 'Finished'\n\nvirtualenv_task = callable_virtualenv()\n```\n\n----------------------------------------\n\nTITLE: Avoiding 'Jinja template not found' Error with BashOperator in Python\nDESCRIPTION: Illustrates the workaround for the 'Jinja template not found' error when using `BashOperator` to execute a script that doesn't require templating. Adding a space after the script path in the `bash_command` parameter prevents Airflow from attempting Jinja rendering.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nBashOperator(\n    task_id=\"bash_example\",\n    # This fails with 'Jinja template not found' error\n    # bash_command=\"/home/batcher/test.sh\",\n    # This works (has a space after)\n    bash_command=\"/home/batcher/test.sh \",\n)\n```\n\n----------------------------------------\n\nTITLE: Testing and Running Airflow Tasks\nDESCRIPTION: Commands to test individual tasks and run backfill operations in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/start.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# run your first task instance\nairflow tasks test example_bash_operator runme_0 2015-01-01\n# run a backfill over 2 days\nairflow backfill create --dag-id example_bash_operator \\\n    --start-date 2015-01-01 \\\n    --end-date 2015-01-02\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Secrets Manager Backend in Airflow\nDESCRIPTION: Sample configuration for enabling AWS Secrets Manager as the secrets backend in Airflow's airflow.cfg file. It includes settings for connections, variables, and config prefixes, as well as profile name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-secrets-manager.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend\nbackend_kwargs = {\n  \"connections_prefix\": \"airflow/connections\",\n  \"connections_lookup_pattern\": null,\n  \"variables_prefix\": \"airflow/variables\",\n  \"variables_lookup_pattern\": null,\n  \"config_prefix\": \"airflow/config\",\n  \"config_lookup_pattern\": null,\n  \"profile_name\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Default Executor for a DAG in Airflow Python\nDESCRIPTION: Demonstrates how to set a default executor for an entire DAG using default arguments, which applies to all tasks unless overridden.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/index.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef hello_world():\n    print(\"hello world!\")\n\n\ndef hello_world_again():\n    print(\"hello world again!\")\n\n\nwith DAG(\n    dag_id=\"hello_worlds\",\n    default_args={\"executor\": \"LocalExecutor\"},  # Applies to all tasks in the DAG\n) as dag:\n    # All tasks will use the executor from default args automatically\n    hw = hello_world()\n    hw_again = hello_world_again()\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow with Celery Using Constraints\nDESCRIPTION: Command to install Apache Airflow with Celery extra in a reproducible way using version-specific constraints file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-pypi.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"apache-airflow[celery]==|version|\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-|version|/constraints-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Testing DAG Loading in Bash\nDESCRIPTION: Simple command to test if a DAG file can be loaded without errors. Tests for syntax errors, missing dependencies and other loading issues.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npython your-dag-file.py\n```\n\n----------------------------------------\n\nTITLE: Testing Airflow Task Instances\nDESCRIPTION: These commands demonstrate how to test specific task instances ('print_date', 'sleep', and 'templated') in the 'tutorial' DAG for a given logical date (2015-06-01). This allows simulation of the scheduler running tasks for a particular date and time.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/fundamentals.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# testing print_date\nairflow tasks test tutorial print_date 2015-06-01\n\n# testing sleep\nairflow tasks test tutorial sleep 2015-06-01\n\n# testing templated\nairflow tasks test tutorial templated 2015-06-01\n```\n\n----------------------------------------\n\nTITLE: Invalid Template Field Inheritance\nDESCRIPTION: Example showing incorrect template field implementation in inherited operator where parameter is passed implicitly through kwargs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass HelloOperator(BaseOperator):\n    template_fields = \"foo\"\n\n    def __init__(self, foo) -> None:\n        self.foo = foo\n\n\nclass MyHelloOperator(HelloOperator):\n    template_fields = (\"foo\", \"bar\")\n\n    def __init__(self, bar, **kwargs) -> None:  # should be def __init__(self, foo, bar, **kwargs)\n        super().__init__(**kwargs)  # should be super().__init__(foo=foo, **kwargs)\n        self.bar = bar\n```\n\n----------------------------------------\n\nTITLE: Executing a Google Cloud Run Job with Overrides using CloudRunExecuteJobOperator in Airflow\nDESCRIPTION: Demonstrates how to use the CloudRunExecuteJobOperator with overrides to execute a Cloud Run job with custom parameters in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_run.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nexecute_job_with_overrides = CloudRunExecuteJobOperator(\n    task_id=\"execute_job_with_overrides\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    job_id=JOB_NAME,\n    overrides={\n        \"container_overrides\": [\n            {\n                \"name\": None,\n                \"command\": [\"echo\"],\n                \"args\": [\"Job executed with overrides!\"]\n            }\n        ]\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Using OpenAITriggerBatchOperator in Python\nDESCRIPTION: Example showing implementation of OpenAITriggerBatchOperator for triggering and monitoring batch jobs. The operator requires a file_id, endpoint, and OpenAI connection ID for execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openai/docs/operators/openai.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_openai_trigger_operator]\n[END howto_operator_openai_trigger_operator]\n```\n\n----------------------------------------\n\nTITLE: Sending Message to Azure Service Bus Queue in Python\nDESCRIPTION: This code shows how to use the AzureServiceBusSendMessageOperator to send a message or list of messages to an Azure Service Bus queue.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/asb.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_send_message_to_service_bus_queue]\n# [END howto_operator_send_message_to_service_bus_queue]\n```\n\n----------------------------------------\n\nTITLE: Fetching and Storing Air Quality Data\nDESCRIPTION: Task that fetches air quality data from an API, converts it to a pandas DataFrame, and saves it as a Parquet file in cloud storage using ObjectStoragePath.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/objectstorage.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef get_air_quality_data(data_path: ObjectStoragePath, logical_date: DateTime) -> ObjectStoragePath:\n    \"\"\"Get air quality data from Finnish Meteorological Institute and save to object storage.\"\"\"\n    import pandas as pd\n    import requests\n\n    # Get air quality data from API\n    response = requests.get(\n        \"https://www.ilmatieteenlaitos.fi/api/air-quality/station/201/latest\"\n    )\n    df = pd.DataFrame(response.json())\n\n    # Generate output path with date\n    output_path = data_path / f\"air_quality_{logical_date.strftime('%Y-%m-%d')}.parquet\"\n\n    # Write directly to cloud storage\n    df.to_parquet(output_path.as_uri())\n    return output_path\n```\n\n----------------------------------------\n\nTITLE: Adding Steps to an EMR Job Flow using EmrAddStepsOperator in Python\nDESCRIPTION: Illustrates how to use the `EmrAddStepsOperator` to add new steps to an already running EMR cluster. It requires the `job_flow_id` of the target cluster, a list of `steps` to add (defined elsewhere, e.g., `SPARK_STEPS`), and an `aws_conn_id`. The `task_id` identifies the step addition task. This operator also supports deferrable mode (`deferrable=True`).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nadd_steps_task = EmrAddStepsOperator(\n    task_id=\"add_steps_task\",\n    job_flow_id=cluster_id,\n    aws_conn_id=\"aws_default\",\n    steps=SPARK_STEPS,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Handling Pagination with HttpOperator in Python\nDESCRIPTION: This example demonstrates using `HttpOperator` with a `pagination_function` to handle paginated API responses. The function extracts the next page number from the 'args'.'page' field in the JSON response and returns it as parameters for the next request. The operator continues making GET requests to `/get` until the pagination function returns `None` (or empty dict).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/operators.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef pagination_function(response):\n    \"\"\"Paginate until page 5.\"\"\"\n    data = response.json()\n    page = int(data[\"args\"][\"page\"])\n    if page < 5:\n        return {\"page\": page + 1}\n    else:\n        return None\n\n\ntask_paginated_op = SimpleHttpOperator(\n    task_id=\"paginated_op\",\n    http_conn_id=\"http_default\",\n    method=\"GET\",\n    endpoint=\"get\",\n    data={\"page\": 1},\n    pagination_function=pagination_function,\n)\n```\n\n----------------------------------------\n\nTITLE: Triggering a Workflow Template in Google Cloud Dataproc\nDESCRIPTION: This code shows how to trigger an existing workflow template using DataprocInstantiateWorkflowTemplateOperator. This executes the defined workflow on the specified Dataproc cluster.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ntrigger_workflow_template = DataprocInstantiateWorkflowTemplateOperator(\n    task_id=\"trigger_workflow_template\",\n    template_id=WORKFLOW_NAME,\n    project_id=PROJECT_ID,\n    region=REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Default Version on Model using Vertex AI Model Service Operator - Python\nDESCRIPTION: Describes setting a specific model version as default via SetDefaultVersionOnModelOperator in Airflow. Inputs are the model's identification details and version. This operation will update the default version, impacting model serving unless additional version-specific configuration is used.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nset_default_version_task = SetDefaultVersionOnModelOperator(\n    task_id=\"set_model_version_as_default_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    model_id=MODEL_ID,\n    version_id=VERSION_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Conditional Execution with TaskFlow API\nDESCRIPTION: This snippet demonstrates how to use @task.run_if() or @task.skip_if() decorators to control whether a task runs based on dynamic conditions at runtime, without altering the DAG structure.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/taskflow.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@task.run_if(lambda ctx: ctx[\"task_instance\"].task_id == \"run\")\n@task.bash()\ndef echo():\n    return \"echo 'run'\"\n```\n\n----------------------------------------\n\nTITLE: Posting Block Kit Layouts with SlackAPIPostOperator in Python\nDESCRIPTION: This snippet shows how to use the SlackAPIPostOperator in Python to send advanced messages using Slack's Block Kit layouts. It requires the Airflow Slack provider, a configured Slack connection, and knowledge of Block Kit structure. The user specifies the 'blocks' parameter with JSON layouts as input, which allows richer message formatting. The output is a custom-styled message in the target Slack channel. Limitations include the need for correct Block Kit formatting and Slack's API constraints.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/operators/slack_api.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.providers.slack.operators.slack import SlackAPIPostOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2021, 1, 1),\n}\n\ndag = DAG(\n    'example_slack_post_blocks',\n    default_args=default_args,\n    schedule_interval=None,\n    catchup=False,\n)\n\npost_blocks = SlackAPIPostOperator(\n    task_id='post_blocks',\n    channel='#random',\n    blocks=[\n        {\n            'type': 'section',\n            'text': {\n                'type': 'mrkdwn',\n                'text': 'This is a message with a block kit layout!'\n            },\n        },\n    ],\n    dag=dag,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing PythonSensor using @task.sensor decorator in Python\nDESCRIPTION: This snippet demonstrates how to use the @task.sensor decorator to create a sensor that waits for a specific condition. The wait_function checks if the current minute is even and returns True if so, causing the sensor to succeed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/python.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@task.sensor(poke_interval=30, timeout=60 * 60)\ndef wait_for_condition():\n    minute = datetime.now().minute\n    if minute % 2 == 0:\n        return True\n    else:\n        return False\n```\n\n----------------------------------------\n\nTITLE: Using CloudVideoIntelligenceDetectVideoShotsOperator to Detect Video Shots (Python)\nDESCRIPTION: Demonstrates application of CloudVideoIntelligenceDetectVideoShotsOperator in an Airflow DAG to identify shot boundaries within a video stored in GCS. The operator requires predefined arguments, such as input_uri, and stores results using Airflow's result_path or XCom. Dependencies include Airflow's Google provider and access to Cloud Video Intelligence API. Expected output is the detected video shots.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/video_intelligence.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndetect_video_shots = CloudVideoIntelligenceDetectVideoShotsOperator(\n    task_id=\"detect_video_shots\",\n    location=GCP_LOCATION,\n    **OTHER_ARGS,\n    result_path=RESULT_PATH,\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Video Shot Detection Results from XCom in Airflow (Python)\nDESCRIPTION: This snippet retrieves video shot detection results from Airflow's XCom, following the execution of CloudVideoIntelligenceDetectVideoShotsOperator. It uses XCom.get_value with specified keys to access and print the annotation output. Inputs are task ID, DAG ID, and execution context. Prerequisites include a running Airflow instance and available XCom records for the specified task.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/video_intelligence.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndetect_video_shots_result = XCom.get_value(\n    key=\"return_value\",\n    task_id=\"detect_video_shots\",\n    dag_id=dag.dag_id,\n    execution_date=kwargs['execution_date'],\n)\nprint(f\"Video shots detection result: {detect_video_shots_result}\")\n```\n\n----------------------------------------\n\nTITLE: Applying Default Arguments to DAG Tasks in Python\nDESCRIPTION: Demonstrates how to set default arguments for all tasks in a DAG using the default_args parameter. This example sets a default retry count of 2 for all tasks in the DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pendulum\n\nwith DAG(\n    dag_id=\"my_dag\",\n    start_date=pendulum.datetime(2016, 1, 1),\n    schedule=\"@daily\",\n    default_args={\"retries\": 2},\n):\n    op = BashOperator(task_id=\"hello_world\", bash_command=\"Hello World!\")\n    print(op.retries)  # 2\n```\n\n----------------------------------------\n\nTITLE: Pulling Default XCom Value in Python\nDESCRIPTION: Demonstrates pulling the default 'return_value' XCom without specifying a key. This is useful when working with operators that auto-push their results.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/xcoms.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nvalue = task_instance.xcom_pull(task_ids='pushing_task')\n```\n\n----------------------------------------\n\nTITLE: Invoking Claude V2 Model using Completions API (Python)\nDESCRIPTION: Illustrates the use of BedrockInvokeModelOperator to call Anthropic's Claude V2 model with the Completions API. This snippet includes required operator parameters such as model_id and prompts, delivering AI-generated completions as output.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclaude_v2_task = BedrockInvokeModelOperator(\n    task_id=\"claude_v2_completion\",\n    model_id=\"anthropic.claude-v2\",\n    body={\"prompt\": \"Explain the theory of relativity in simple terms.\"},\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Poor Practice with Top-Level Code in Airflow DAG\nDESCRIPTION: Example showing an anti-pattern where expensive operations are performed at the top level of a DAG file, causing performance issues during DAG parsing. The expensive_api_call function is executed during DAG loading rather than during task execution.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pendulum\n\nfrom airflow.sdk import DAG\nfrom airflow.sdk import task\n\n\ndef expensive_api_call():\n    print(\"Hello from Airflow!\")\n    sleep(1000)\n\n\nmy_expensive_response = expensive_api_call()\n\nwith DAG(\n    dag_id=\"example_python_operator\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task()\n    def print_expensive_api_call():\n        print(my_expensive_response)\n```\n\n----------------------------------------\n\nTITLE: Launching Deferrable Custom Training Job via Script in VertexAI (Python)\nDESCRIPTION: Illustrates how to execute a script-based VertexAI training job in deferrable mode using Airflow. Inputs are the same as in standard script job, with deferrable mode enabled for resource optimization during long VertexAI operations. Outputs the trained model's information on completion.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n    create_training_job_deferrable = CreateCustomTrainingJobOperator(\n        task_id=\"train_model_deferrable\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        display_name=DISPLAY_NAME,\n        script_path=SCRIPT_PATH,\n        dataset_id=DATASET_ID,\n        gcp_conn_id=GCP_CONN_ID,\n        deferrable=True,\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Task Dependencies in Airflow DAGs using Python\nDESCRIPTION: This snippet demonstrates two methods of defining task dependencies in Airflow DAGs. The first uses the '>>' and '<<' operators, while the second uses the set_upstream and set_downstream methods. These dependencies determine the order in which tasks are executed in the DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/overview.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfirst_task >> [second_task, third_task]\nfourth_task << third_task\n```\n\nLANGUAGE: python\nCODE:\n```\nfirst_task.set_downstream([second_task, third_task])\nfourth_task.set_upstream(third_task)\n```\n\n----------------------------------------\n\nTITLE: Implementing Branch Task with Classic Operator\nDESCRIPTION: Example showing branching implementation using PythonBranchOperator. Returns different task IDs based on random selection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef random_choice():\n    import random\n\n    items = [\"branch_a\", \"branch_b\", \"branch_c\"]\n    selected = random.choice(items)\n    if selected == \"branch_a\":\n        return [\"branch_a_task_1\", \"branch_a_task_2\"]\n    elif selected == \"branch_b\":\n        return [\"branch_b_task_1\", \"branch_b_task_2\"]\n    elif selected == \"branch_c\":\n        return [\"branch_c_task_1\", \"branch_c_task_2\"]\n\nbranch_op = PythonBranchOperator(\n    task_id=\"make_choice\",\n    python_callable=random_choice,\n)\n```\n\n----------------------------------------\n\nTITLE: De-identifying Content with Airflow DLP Operator - Python\nDESCRIPTION: This example uses the CloudDLPDeidentifyContentOperator within an Airflow DAG to remove identifying information from data using a specified DeidentifyConfig. It requires airflow.providers.google.cloud.operators.cloud.dlp.CloudDLPDeidentifyContentOperator, an Airflow environment, and the Google Cloud DLP API. Key parameters include the content to process, the GCP project ID, and the de-identification configuration. The operator processes input text and outputs de-identified content, facilitating data privacy compliance. Limitations include the need for Airflow and appropriate API/account permissions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/data_loss_prevention.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    'example_gcp_dlp_deidentify_content',\n    default_args=default_args,\n    schedule_interval=None,  # Set your schedule interval\n    catchup=False,\n) as dag:\n    deidentify_content = CloudDLPDeidentifyContentOperator(\n        task_id='deidentify_content',\n        project_id=PROJECT_ID,\n        item={'value': 'My email is test@example.com'},\n        deidentify_config=deidentify_config,\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Sending Text Messages with Airflow SlackWebhookOperator - Python\nDESCRIPTION: This snippet demonstrates how to instantiate and use the SlackWebhookOperator in Airflow to send a plain text message to a predefined Slack channel via Incoming Webhook. Required dependencies include an Airflow environment with the 'airflow.providers.slack' provider installed, and a valid Slack webhook URL configured in your Airflow connections or variables. The primary input is the message text, and the expected output is a Slack channel message. Key parameters include the webhook connection ID, message content, and optional formatting arguments.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/operators/slack_webhook.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nslack_webhook = SlackWebhookOperator(\n    task_id=\"send_slack_message\",\n    http_conn_id=\"slack_webhook\",\n    message=\"Hello from Airflow! \",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Complete DAG for Employee Data Processing in Airflow\nDESCRIPTION: Python code that defines a complete Airflow DAG for processing employee data. It includes tasks for creating tables, retrieving data, and merging data. This demonstrates how to structure a DAG with multiple tasks and dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/pipeline.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\nimport pendulum\nimport os\n\nimport requests\nfrom airflow.sdk import dag, task\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\n\n@dag(\n    dag_id=\"process_employees\",\n    schedule=\"0 0 * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    dagrun_timeout=datetime.timedelta(minutes=60),\n)\ndef ProcessEmployees():\n    create_employees_table = SQLExecuteQueryOperator(\n        task_id=\"create_employees_table\",\n        conn_id=\"tutorial_pg_conn\",\n        sql=\"\"\"\n            CREATE TABLE IF NOT EXISTS employees (\n                \"Serial Number\" NUMERIC PRIMARY KEY,\n                \"Company Name\" TEXT,\n                \"Employee Markme\" TEXT,\n                \"Description\" TEXT,\n                \"Leave\" INTEGER\n            );\"\"\",\n    )\n\n    create_employees_temp_table = SQLExecuteQueryOperator(\n        task_id=\"create_employees_temp_table\",\n        conn_id=\"tutorial_pg_conn\",\n        sql=\"\"\"\n            DROP TABLE IF EXISTS employees_temp;\n            CREATE TABLE employees_temp (\n                \"Serial Number\" NUMERIC PRIMARY KEY,\n                \"Company Name\" TEXT,\n                \"Employee Markme\" TEXT,\n                \"Description\" TEXT,\n                \"Leave\" INTEGER\n            );\"\"\",\n    )\n\n    @task\n    def get_data():\n        # NOTE: configure this as appropriate for your Airflow environment\n        data_path = \"/opt/airflow/dags/files/employees.csv\"\n        os.makedirs(os.path.dirname(data_path), exist_ok=True)\n\n        url = \"https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/tutorial/pipeline_example.csv\"\n\n        response = requests.request(\"GET\", url)\n```\n\n----------------------------------------\n\nTITLE: Starting Deferrable Dataflow YAML Job with Airflow\nDESCRIPTION: Shows how to run the `DataflowStartYamlJobOperator` in deferrable mode by setting `deferrable=True`. This enhances efficiency by releasing the worker slot during the job startup phase. Requires `gcloud` SDK.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_yaml.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_dataflow_start_yaml_job_def]\n    :end-before: [END howto_operator_dataflow_start_yaml_job_def]\n```\n\n----------------------------------------\n\nTITLE: Modifying an EMR Cluster using EmrModifyClusterOperator in Python\nDESCRIPTION: Demonstrates using the `EmrModifyClusterOperator` to modify attributes of an existing EMR cluster, such as the step concurrency level. It requires the `cluster_id` and the desired `step_concurrency_level`. An `aws_conn_id` is needed for authentication.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodify_cluster_task = EmrModifyClusterOperator(\n    task_id=\"modify_cluster_task\",\n    cluster_id=cluster_id,\n    step_concurrency_level=1,\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Dynamic Named Mapping with Custom Context Variables in Airflow\nDESCRIPTION: Demonstrates injecting custom variables into the rendering context for map_index_template. This allows complex naming logic to be implemented in Python code rather than in Jinja templates.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.standard.operators.python import get_current_context\n\n\n@task(map_index_template=\"{{ my_variable }}\")\ndef my_task(my_value: str):\n    context = get_current_context()\n    context[\"my_variable\"] = my_value * 3\n    ...  # Normal execution...\n\n\n# The task instances will be named \"aaa\" and \"bbb\".\nmy_task.expand(my_value=[\"a\", \"b\"])\n```\n\n----------------------------------------\n\nTITLE: Pushing XCom Value in Python Task\nDESCRIPTION: Demonstrates how to push a value into XCom within a task. This allows other tasks to access the data later in the workflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/xcoms.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntask_instance.xcom_push(key=\"identifier as a string\", value=any_serializable_value)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom DateTime Trigger in Python for Airflow\nDESCRIPTION: Implementation of a basic DateTime trigger that demonstrates the essential components of a custom trigger including initialization, serialization, and asynchronous execution. The trigger yields an event when a specified datetime moment is reached.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/deferring.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom airflow.triggers.base import BaseTrigger, TriggerEvent\nfrom airflow.utils import timezone\n\n\nclass DateTimeTrigger(BaseTrigger):\n    def __init__(self, moment):\n        super().__init__()\n        self.moment = moment\n\n    def serialize(self):\n        return (\"airflow.providers.standard.triggers.temporal.DateTimeTrigger\", {\"moment\": self.moment})\n\n    async def run(self):\n        while self.moment > timezone.utcnow():\n            await asyncio.sleep(1)\n        yield TriggerEvent(self.moment)\n```\n\n----------------------------------------\n\nTITLE: Testing DAG Pipeline Commands\nDESCRIPTION: Command line instructions for testing and validating the DAG configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/fundamentals.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython ~/airflow/dags/tutorial.py\nairflow db migrate\nairflow dags list\nairflow tasks list tutorial\n```\n\n----------------------------------------\n\nTITLE: Running an Azure Data Factory Pipeline Synchronously with Airflow Operator (Python)\nDESCRIPTION: This example demonstrates the basic usage of `AzureDataFactoryRunPipelineOperator` to execute an Azure Data Factory pipeline. By default (`wait_for_termination=True`), the operator triggers the pipeline and waits for its completion, succeeding only if the pipeline run succeeds. Requires an Azure Data Factory connection configured in Airflow (`azure_data_factory_conn_id`).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/adf_run_pipeline.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../tests/system/microsoft/azure/example_adf_run_pipeline.py\n#     :language: python\n#     :dedent: 0\n#     :start-after: [START howto_operator_adf_run_pipeline]\n#     :end-before: [END howto_operator_adf_run_pipeline]\n\n# Example (simulated based on description):\nfrom airflow.providers.microsoft.azure.operators.data_factory import AzureDataFactoryRunPipelineOperator\n\nrun_pipeline1 = AzureDataFactoryRunPipelineOperator(\n    task_id=\"run_pipeline1\",\n    pipeline_name=\"pipeline1\",\n    azure_data_factory_conn_id=\"azure_data_factory_default\",\n    # wait_for_termination defaults to True\n)\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataplex Zone with Airflow Operator\nDESCRIPTION: Uses the DataplexCreateZoneOperator to create a Dataplex zone with the specified configuration. This operator creates a zone within a lake in Google Cloud Dataplex.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ncreate_zone = DataplexCreateZoneOperator(\n    task_id=\"create_zone\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    lake_id=LAKE_ID,\n    zone_id=ZONE_ID,\n    body=ZONE,\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieval-Augmented Generation (RaG) on Bedrock Knowledge Base (Python)\nDESCRIPTION: Executes a retrieve-and-generate pipeline using BedrockRaGOperator to query a knowledge base and produce an AI-generated response, combining retrieval results and large language model output in one step. This operator requires both a knowledge base and compatible model.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nrag_task = BedrockRaGOperator(\n    task_id=\"rag_example\",\n    knowledge_base_id=\"kb-abcdef123456\",\n    input_text=\"Summarize key differences between Anthropic Claude and Amazon Titan models.\",\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring SFTP Sensor with Timeouts in Airflow\nDESCRIPTION: Illustrates how to set up an SFTP Sensor task with execution timeout, overall timeout, and retry settings. This example demonstrates the use of both execution_timeout and timeout parameters for sensors in reschedule mode.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/tasks.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsensor = SFTPSensor(\n    task_id=\"sensor\",\n    path=\"/root/test\",\n    execution_timeout=timedelta(seconds=60),\n    timeout=3600,\n    retries=2,\n    mode=\"reschedule\",\n)\n```\n\n----------------------------------------\n\nTITLE: Mapping Classic Operators with Output Reference\nDESCRIPTION: Demonstrates how to map over the result of a classic operator by referencing its output for transformation and loading operations.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Create a list of data inputs.\nextract = ExtractOperator(task_id=\"extract\")\n\n# Expand the operator to transform each input.\ntransform = TransformOperator.partial(task_id=\"transform\").expand(input=extract.output)\n\n# Collect the transformed inputs, expand the operator to load each one of them to the target.\nload = LoadOperator.partial(task_id=\"load\").expand(input=transform.output)\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple DAG in Python for Apache Airflow\nDESCRIPTION: This code snippet demonstrates how to define a simple DAG (Directed Acyclic Graph) in Apache Airflow. It includes setting up the DAG, creating tasks using BashOperator and the @task decorator, and defining task dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom airflow.sdk import DAG, task\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n# A DAG represents a workflow, a collection of tasks\nwith DAG(dag_id=\"demo\", start_date=datetime(2022, 1, 1), schedule=\"0 0 * * *\") as dag:\n    # Tasks are represented as operators\n    hello = BashOperator(task_id=\"hello\", bash_command=\"echo hello\")\n\n    @task()\n    def airflow():\n        print(\"airflow\")\n\n    # Set dependencies between tasks\n    hello >> airflow()\n```\n\n----------------------------------------\n\nTITLE: Deleting BigQuery Tables, Views, and Materialized Views with Airflow BigQueryDeleteTableOperator - Python\nDESCRIPTION: This set of snippets shows how to delete BigQuery tables, views, or materialized views using the BigQueryDeleteTableOperator in Airflow. The operator requires the table identifiers and can remove the specified resource from BigQuery. Dependencies include airflow.providers.google.cloud.operators.bigquery, and parameters include project_id, dataset_id, and table_id. The operator can be used in multiple scenarios: removal of standard tables, views, or materialized views, by specifying the appropriate table_id. Successful execution results in the deletion of the specified resource.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndelete_table = BigQueryDeleteTableOperator(\n    task_id=\"delete_table\",\n    deletion_dataset_table=f\"{TEST_PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME}\",\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\ndelete_view = BigQueryDeleteTableOperator(\n    task_id=\"delete_view\",\n    deletion_dataset_table=f\"{TEST_PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_VIEW_NAME}\",\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\ndelete_materialized_view = BigQueryDeleteTableOperator(\n    task_id=\"delete_materialized_view\",\n    deletion_dataset_table=f\"{TEST_PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_MATERIALIZED_VIEW_NAME}\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Defining DAG Task Dependencies\nDESCRIPTION: Sets up the task dependencies for the Airflow DAG, specifying that table creation tasks must run before data retrieval, which must run before data merging.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/pipeline.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n[create_employees_table, create_employees_temp_table] >> get_data() >> merge_data()\n```\n\n----------------------------------------\n\nTITLE: Using Jinja Templates\nDESCRIPTION: Demonstrates using Jinja templating in Airflow for dynamic command generation with date stamps and loops.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/fundamentals.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntemplated_command = \"\"\"\n{% for i in range(5) %}\n    echo \\\"{{ ds }}\\\"\n    echo \\\"{{ macros.ds_add(ds, 7)}}\\\"\n    echo \\\"{{ params.my_param }}\\\"\n{% endfor %}\n\"\"\"\n\nt3 = BashOperator(\n    task_id='templated',\n    depends_on_past=False,\n    bash_command=templated_command,\n    params={'my_param': 'Parameter I passed in'},\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Using the @dag Decorator for DAG Definition in Python\nDESCRIPTION: Shows how to use the @dag decorator to define a DAG as a Python function. This approach allows for cleaner DAG definitions and automatic parameter handling.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dag(catchup=False, schedule=None, default_args={\"owner\": \"airflow\"})\ndef example_dag_decorator(email: str = \"example@example.com\"):\n    \"\"\"Example DAG using the @dag decorator\"\"\"\n\n    @task\n    def extract():\n        return {\"1001\": 301340, \"1002\": 543070, \"1003\": 192839}\n\n    @task\n    def load(orders_data: dict) -> None:\n        pass\n\n    order_data = extract()\n    load(order_data)\n\n\nexample_dag = example_dag_decorator()\n```\n\n----------------------------------------\n\nTITLE: Using LatestOnlyOperator for Conditional Execution in Python\nDESCRIPTION: Demonstrates the use of LatestOnlyOperator to conditionally execute tasks only for the most recent DAG run. This example shows how different tasks are affected by the LatestOnlyOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"latest_only_with_trigger\",\n    schedule=\"@once\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example3\"],\n) as dag:\n    latest_only = LatestOnlyOperator(task_id=\"latest_only\")\n    task1 = EmptyOperator(task_id=\"task1\")\n    task2 = EmptyOperator(task_id=\"task2\")\n    task3 = EmptyOperator(task_id=\"task3\")\n    task4 = EmptyOperator(task_id=\"task4\", trigger_rule=\"all_done\")\n\n    latest_only >> task1 >> [task3, task4]\n    task2 >> [task3, task4]\n```\n\n----------------------------------------\n\nTITLE: Invoking Meta Llama Model via Amazon Bedrock Operator (Python)\nDESCRIPTION: Demonstrates invoking a Meta Llama foundation model within an Airflow DAG using the BedrockInvokeModelOperator. Requires airflow.providers.amazon and proper AWS credentials. The operator takes model identifier, input payload, and other configuration parameters to request inference from the specified model, returning generation results.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nllama_task = BedrockInvokeModelOperator(\n    task_id=\"llama_invocation\",\n    model_id=\"meta.llama2-13b-chat-v1\",\n    body={\"prompt\": \"Hello, world!\"},\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Comparing Time-based Metrics in BigQuery with Airflow BigQueryIntervalCheckOperator (Sync and Async) - Python\nDESCRIPTION: These snippets use BigQueryIntervalCheckOperator and BigQueryIntervalCheckAsyncOperator in Airflow to compare metric values over time, ensuring the results from a recent SQL query do not deviate beyond specified tolerance from their historical values (as defined by days_back). Dependencies are airflow.providers.google.cloud.operators.bigquery with input parameters such as table name, metrics expressions, tolerances, and days_back for time comparison. Async operator usage requires Airflow's triggerer service. Output is successful completion if metrics fall within tolerances; failure otherwise. Limitations include support for numeric metrics and well-defined intervals.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Compare table metrics against prior day's metrics\nbq_interval_check = BigQueryIntervalCheckOperator(\n    task_id=\"bq_interval_check\",\n    table=\"my_table\",\n    metrics_thresholds={\"count\": 1.1},\n    date_filter_column=\"date\",\n    days_back=1,\n    metrics_sqls={\"count\": \"SELECT COUNT(*) FROM my_table WHERE date = CURRENT_DATE()\"},\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Compare metrics in async (deferrable) mode\nbq_interval_check_async = BigQueryIntervalCheckAsyncOperator(\n    task_id=\"bq_interval_check_async\",\n    table=\"my_table\",\n    metrics_thresholds={\"avg_revenue\": 1.05},\n    date_filter_column=\"date\",\n    days_back=7,\n    metrics_sqls={\"avg_revenue\": \"SELECT AVG(revenue) FROM my_table WHERE date = CURRENT_DATE()\"},\n    deferrable=True,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Using KubernetesPodOperator with TaskFlow API\nDESCRIPTION: This example shows how to use the KubernetesPodOperator with the TaskFlow API to run a task inside a Kubernetes pod. It's ideal for large tasks or those requiring custom runtimes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/taskflow.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@task.kubernetes()\ndef start_kubernetes_pod():\n    return \"Kubernetes Pod task finished\"\n```\n\n----------------------------------------\n\nTITLE: Running Parallel Provider Tests\nDESCRIPTION: Command to run specific provider tests in parallel with database testing enabled\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-tests --run-in-parallel --run-db-tests-only --parallel-test-types \"Providers[google] Providers[amazon]\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating DAG Object\nDESCRIPTION: Creates a DAG instance with specified ID, default arguments, and schedule interval.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/fundamentals.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndag = DAG(\n    'tutorial',\n    default_args=default_args,\n    description='A simple tutorial DAG',\n    schedule_interval=timedelta(days=1),\n    start_date=days_ago(2),\n    tags=['example']\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Connection via CLI with JSON\nDESCRIPTION: Command line example for adding an Airflow connection using JSON format. Shows full connection configuration including extra parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nairflow connections add 'my_prod_db' \\\n    --conn-json '{\n        \"conn_type\": \"my-conn-type\",\n        \"login\": \"my-login\",\n        \"password\": \"my-password\",\n        \"host\": \"my-host\",\n        \"port\": 1234,\n        \"schema\": \"my-schema\",\n        \"extra\": {\n            \"param1\": \"val1\",\n            \"param2\": \"val2\"\n        }\n    }'\n```\n\n----------------------------------------\n\nTITLE: Setting Connection via JSON Environment Variable\nDESCRIPTION: Example showing how to set an Airflow connection using JSON format in an environment variable. Demonstrates the structure for specifying connection details including type, credentials, host, port, and extra parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_MY_PROD_DATABASE='{\\n    \"conn_type\": \"my-conn-type\",\\n    \"login\": \"my-login\",\\n    \"password\": \"my-password\",\\n    \"host\": \"my-host\",\\n    \"port\": 1234,\\n    \"schema\": \"my-schema\",\\n    \"extra\": {\\n        \"param1\": \"val1\",\\n        \"param2\": \"val2\"\\n    }\\n}'\n```\n\n----------------------------------------\n\nTITLE: Templating and Accessing Context Variables in TaskFlow API\nDESCRIPTION: This example shows how to access context variables and use templating in TaskFlow API functions. It demonstrates accessing specific context variables and using the entire context.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/taskflow.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef my_python_callable(*, ti, next_ds):\n    pass\n\n@task\ndef my_python_callable(**kwargs):\n    ti = kwargs[\"ti\"]\n    next_ds = kwargs[\"next_ds\"]\n\nfrom airflow.providers.standard.operators.python import get_current_context\n\ndef some_function_in_your_library():\n    context = get_current_context()\n    ti = context[\"ti\"]\n\n@task(templates_exts=[\".sql\"])\ndef read_sql(sql): ...\n```\n\n----------------------------------------\n\nTITLE: Declaring an Airflow DAG using Constructor in Python\nDESCRIPTION: This snippet shows how to create a DAG using the standard constructor approach, where the DAG object is explicitly passed to operators. This method provides more flexibility when working with DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nfrom airflow.sdk import DAG\nfrom airflow.providers.standard.operators.empty import EmptyOperator\n\nmy_dag = DAG(\n    dag_id=\"my_dag_name\",\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@daily\",\n)\nEmptyOperator(task_id=\"task\", dag=my_dag)\n```\n\n----------------------------------------\n\nTITLE: Implementing Deferrable ExternalTaskSensor in Python for Apache Airflow\nDESCRIPTION: This example shows how to use ExternalTaskSensor in deferrable mode. It sets up an asynchronous sensor that waits for a task in another DAG, specifying the external DAG ID and task ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/external_task_sensor.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwait_for_task_2 = ExternalTaskSensor(\n    task_id=\"wait_for_task_2\",\n    external_dag_id=\"example_external_task_parent\",\n    external_task_id=\"task_2\",\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Best Practices with Task-Level Execution in Airflow DAG\nDESCRIPTION: Example showing the recommended pattern where expensive operations are performed within task execution context rather than at DAG parse time. The expensive_api_call function is only called when the task runs, improving DAG parsing performance.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pendulum\n\nfrom airflow.sdk import DAG\nfrom airflow.sdk import task\n\n\ndef expensive_api_call():\n    sleep(1000)\n    return \"Hello from Airflow!\"\n\n\nwith DAG(\n    dag_id=\"example_python_operator\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task()\n    def print_expensive_api_call():\n        my_expensive_response = expensive_api_call()\n        print(my_expensive_response)\n```\n\n----------------------------------------\n\nTITLE: Configuring DAG Serialization Settings in Airflow Configuration\nDESCRIPTION: Configuration settings in airflow.cfg file for DAG Serialization including update intervals, fetch intervals, rendered task instance field limits, and compression options.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/dag-serialization.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[core]\n\n# You can also update the following default configurations based on your needs\nmin_serialized_dag_update_interval = 30\nmin_serialized_dag_fetch_interval = 10\nmax_num_rendered_ti_fields_per_task = 30\ncompress_serialized_dags = False\n```\n\n----------------------------------------\n\nTITLE: Listing Cloud Memorystore Instances\nDESCRIPTION: Example showing how to list all Cloud Memorystore instances in a project\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlist_instances = CloudMemorystoreListInstancesOperator(task_id=\"list-instances\", location=\"europe-north1\", project_id=PROJECT_ID)\n```\n\n----------------------------------------\n\nTITLE: Implementing Hello Operator in Python\nDESCRIPTION: Basic implementation of a custom operator that extends BaseOperator to print a hello message. Demonstrates the required __init__ and execute methods.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import BaseOperator\n\nclass HelloOperator(BaseOperator):\n    def __init__(self, name: str, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.name = name\n\n    def execute(self, context):\n        message = f\"Hello {self.name}\"\n        print(message)\n        return message\n```\n\n----------------------------------------\n\nTITLE: Unit Test for DAG Loading in Airflow\nDESCRIPTION: A pytest-based unit test that verifies a DAG can be loaded properly using the DagBag object. It checks for import errors, confirms the DAG object exists, and validates the expected number of tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\nfrom airflow.models import DagBag\n\n\n@pytest.fixture()\ndef dagbag():\n    return DagBag()\n\n\ndef test_dag_loaded(dagbag):\n    dag = dagbag.get_dag(dag_id=\"hello_world\")\n    assert dagbag.import_errors == {}\n    assert dag is not None\n    assert len(dag.tasks) == 1\n```\n\n----------------------------------------\n\nTITLE: Creating Amazon EMR EKS Virtual Cluster with Airflow Operator (Python)\nDESCRIPTION: This snippet demonstrates how to use the EmrEksCreateClusterOperator in Python to initialize a virtual cluster for Amazon EMR on EKS via an Airflow DAG. It requires AWS credentials, a target EKS cluster, and a namespace. The operator parameters include the cluster and namespace configuration, and the expected output is the creation of a virtual EMR on EKS cluster. Users should ensure the necessary IAM permissions and AWS environment configurations are provided.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr_eks.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# This is a mock representation of the included example; replace with your own code.\nfrom airflow import DAG\nfrom airflow.providers.amazon.aws.operators.emr.eks import EmrEksCreateClusterOperator\n\nwith DAG('example_emr_eks_cluster_creation', default_args=default_args, schedule_interval=None) as dag:\n    create_emr_eks_cluster = EmrEksCreateClusterOperator(\n        task_id='create_emr_eks_cluster',\n        virtual_cluster_name='example-virtual-cluster',\n        eks_cluster_name='my-eks-cluster',\n        eks_namespace='default',\n        aws_conn_id='aws_default',\n    )\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Connection Using Environment Variable in Bash\nDESCRIPTION: This snippet demonstrates how to set up a Docker connection in Apache Airflow using an environment variable. It includes the connection URI syntax with username, password, host, port, and additional parameters like email and reauth.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/docker/docs/connections/docker.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_DOCKER_DEFAULT='docker://username:password@https%3A%2F%2Findex.docker.io%2Fv1:80?email=myemail%40my.com&reauth=False'\n```\n\n----------------------------------------\n\nTITLE: Implementing Branching in Airflow DAG\nDESCRIPTION: Example of implementing branching logic in an Airflow DAG using the EmptyOperator and branch task decorator. Shows how to handle task dependencies and branching operations.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# dags/branch_without_trigger.py\nimport pendulum\n\nfrom airflow.sdk import task\nfrom airflow.sdk import DAG\nfrom airflow.providers.standard.operators.empty import EmptyOperator\n\ndag = DAG(\n    dag_id=\"branch_without_trigger\",\n    schedule=\"@once\",\n    start_date=pendulum.datetime(2019, 2, 28, tz=\"UTC\"),\n)\n\nrun_this_first = EmptyOperator(task_id=\"run_this_first\", dag=dag)\n\n@task.branch(task_id=\"branching\")\ndef do_branching():\n    return \"branch_a\"\n\nbranching = do_branching()\n\nbranch_a = EmptyOperator(task_id=\"branch_a\", dag=dag)\nfollow_branch_a = EmptyOperator(task_id=\"follow_branch_a\", dag=dag)\n\nbranch_false = EmptyOperator(task_id=\"branch_false\", dag=dag)\n\njoin = EmptyOperator(task_id=\"join\", dag=dag)\n\nrun_this_first >> branching\nbranching >> branch_a >> follow_branch_a >> join\nbranching >> branch_false >> join\n```\n\n----------------------------------------\n\nTITLE: Installing CeleryKubernetesExecutor Dependencies in Python\nDESCRIPTION: This snippet shows how to install the necessary dependencies for using the CeleryKubernetesExecutor in Airflow 2.7.0 and above. It requires installing both the celery and cncf.kubernetes provider packages.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/celery_kubernetes_executor.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[celery,cncf.kubernetes]'\n```\n\n----------------------------------------\n\nTITLE: Running Python Pipeline with DirectRunner using GCS File in Apache Beam\nDESCRIPTION: This example shows how to use BeamRunPythonPipelineOperator to execute a Python pipeline using DirectRunner with a file stored in Google Cloud Storage. The operator downloads the file from GCS and specifies output location.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_direct_runner_gcs_file = BeamRunPythonPipelineOperator(\n    task_id=\"beam_task_direct_runner_gcs_file\",\n    py_file=\"{{ var.json.beam_variables.gcs_file_path }}\",\n    py_options=[],\n    pipeline_options={\n        \"output\": \"/tmp/apache_beam/direct_runner_output_gcs\",\n    },\n    py_requirements=[\"apache-beam[gcp]==2.44.0\"],\n    py_interpreter=\"python3\",\n    py_system_site_packages=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Starting Dataflow Flex Template Job with Airflow\nDESCRIPTION: Shows how to use the `DataflowStartFlexTemplateOperator` in Airflow to launch a Dataflow job using a Flex Template. Flex Templates package the pipeline code within a Docker image, offering more flexibility than Classic Templates.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_template.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_start_flex_template_job]\n    :end-before: [END howto_operator_start_flex_template_job]\n```\n\n----------------------------------------\n\nTITLE: Using Python Conditionals within @task.bash in Python\nDESCRIPTION: This example demonstrates embedding Python conditional logic within a `@task.bash` decorated function. The Python code evaluates a condition (`should_run`) to determine which Bash command (`echo 'run'`) or message (`echo 'skip'`) is returned and subsequently executed by the Bash task.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@task.bash\ndef run_conditionally(should_run: bool) -> str:\n    if should_run:\n        return \"echo 'run'\"\n    return \"echo 'skip'\"\n\n\nrun_conditionally(True)\n```\n\n----------------------------------------\n\nTITLE: Using Templated Hello Operator\nDESCRIPTION: Example showing how to use the templated HelloOperator with Jinja template substitution.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith dag:\n    hello_task = HelloOperator(\n        task_id=\"task_id_1\",\n        name=\"{{ task_instance.task_id }}\",\n        world=\"Earth\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Traditional Operator-based ETL Implementation\nDESCRIPTION: Shows the same ETL pipeline implemented using traditional PythonOperator approach with manual XCom handling and task dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/taskflow.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport pendulum\nfrom airflow.sdk import DAG, PythonOperator\n\n\ndef extract():\n    data_string = '{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}'\n    return json.loads(data_string)\n\n\ndef transform(ti):\n    order_data_dict = ti.xcom_pull(task_ids=\"extract\")\n    total_order_value = sum(order_data_dict.values())\n    return {\"total_order_value\": total_order_value}\n\n\ndef load(ti):\n    total = ti.xcom_pull(task_ids=\"transform\")[\"total_order_value\"]\n    print(f\"Total order value is: {total:.2f}\")\n\n\nwith DAG(\n    dag_id=\"legacy_etl_pipeline\",\n    schedule_interval=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    extract_task = PythonOperator(task_id=\"extract\", python_callable=extract)\n    transform_task = PythonOperator(task_id=\"transform\", python_callable=transform)\n    load_task = PythonOperator(task_id=\"load\", python_callable=load)\n\n    extract_task >> transform_task >> load_task\n```\n\n----------------------------------------\n\nTITLE: Transferring Files with Airflow FileTransferOperator (Python)\nDESCRIPTION: This snippet demonstrates how to instantiate and configure the FileTransferOperator in an Apache Airflow DAG to transfer files from a local filesystem to an S3 bucket. It requires Airflow with the common IO provider installed and appropriate connections configured. Parameters such as 'src', 'dst', 'src_conn_id', 'dst_conn_id', and 'overwrite' are specified to control source/destination, authentication, and overwrite behavior. Input consists of filesystem paths or object storage URIs; output is the successful transfer or copy of the file, with object storage copy optimization where supported. The snippet assumes access to valid Airflow connections and may error if resources or permissions are misconfigured.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/io/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfile_transfer_task = FileTransferOperator(\n    task_id='transfer_local_to_s3',\n    src='/tmp/my-test-file.txt',\n    dst='s3://my-test-bucket/test-destination.txt',\n    dst_conn_id='aws_default',\n    overwrite=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Branching DAG Based on Logical Date - Airflow Python\nDESCRIPTION: Illustrates how to configure BranchDateTimeOperator to use the logical_date (DAG run timestamp) for determining the branching path. This technique is recommended for idempotent data pipelines and backfill scenarios, since logical_date does not change between re-runs. Dependencies include Airflow and its scheduling context. Key parameters include use_task_logical_date set to True, and the operator makes the selection based on the DAG's scheduled time rather than execution time.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/datetime.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith DAG('example_branch_datetime_operator_logical_date', schedule_interval='@daily', start_date=datetime(2021, 1, 1)) as dag:\n    branch = BranchDateTimeOperator(\n        task_id='branch_logical_date',\n        follow_task_ids_if_true=['run_if_in_window'],\n        follow_task_ids_if_false=['skip_if_not_in_window'],\n        target_lower=datetime(2021, 1, 1, 9, 0),\n        target_upper=datetime(2021, 1, 1, 17, 0),\n        use_task_logical_date=True,\n    )\n    run_if_in_window = DummyOperator(task_id='run_if_in_window')\n    skip_if_not_in_window = DummyOperator(task_id='skip_if_not_in_window')\n    branch >> [run_if_in_window, skip_if_not_in_window]\n\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Tasks\nDESCRIPTION: Creates two BashOperator tasks with different commands and retry configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/fundamentals.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nt1 = BashOperator(\n    task_id='print_date',\n    bash_command='date',\n    dag=dag,\n)\n\nt2 = BashOperator(\n    task_id='sleep',\n    depends_on_past=False,\n    bash_command='sleep 5',\n    retries=3,\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with extras using pip\nDESCRIPTION: Command to install Apache Airflow with additional provider packages (postgres and google in this example) using pip with constraint files to ensure compatible dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/generated/PYPI_README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[postgres,google]==2.10.5' \\\n --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.5/constraints-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Creating Amazon Bedrock Data Source via Operator (Python)\nDESCRIPTION: Creates a new Bedrock Data Source for knowledge ingestion with BedrockCreateDataSourceOperator in Airflow. Specifies S3 paths and configuration needed for the data source object, which will subsequently be available for data ingestion and queries.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncreate_data_source_task = BedrockCreateDataSourceOperator(\n    task_id=\"create_ds\",\n    knowledge_base_id=\"kb-abcdef123456\",\n    name=\"my-data-source\",\n    s3_configuration={\n        \"bucket\": \"my-data-bucket\",\n        \"prefix\": \"data-source-files/\"\n    },\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Cloud Composer Environment in Python\nDESCRIPTION: This snippet shows how to use the CloudComposerCreateEnvironmentOperator to create a Cloud Composer environment. It specifies the project ID, region, environment name, and configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncreate_environment_task = CloudComposerCreateEnvironmentOperator(\n    task_id=\"create-environment\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    environment_name=ENVIRONMENT_NAME,\n    environment=environment_config,\n)\n```\n\n----------------------------------------\n\nTITLE: Receiving Message from Azure Service Bus Queue in Python\nDESCRIPTION: This example illustrates how to use the AzureServiceBusReceiveMessageOperator to receive a message or list of messages from an Azure Service Bus queue.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/asb.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_receive_message_service_bus_queue]\n# [END howto_operator_receive_message_service_bus_queue]\n```\n\n----------------------------------------\n\nTITLE: Defining Tasks with Operators in Python\nDESCRIPTION: Demonstrates how to define tasks using operators declaratively inside a DAG. Shows examples of using HttpOperator and EmailOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\"my-dag\") as dag:\n    ping = HttpOperator(endpoint=\"http://example.com/update/\")\n    email = EmailOperator(to=\"admin@example.com\", subject=\"Update complete\")\n\n    ping >> email\n```\n\n----------------------------------------\n\nTITLE: Exporting Trino Query Results as CSV to Google Cloud Storage using Airflow Operator in Python\nDESCRIPTION: Shows how to use Airflow's TrinoToGCSOperator to export query results in CSV format by setting the export_format parameter. This requires Airflow with the google and trino provider packages. Parameters include the SQL statement, bucket, and filename with a placeholder for automatic chunk numbering. The operator will write CSV files to the destination in GCS. Use this approach to enable downstream workflows or meet data ingestion requirements expecting CSV.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/trino_to_gcs.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrino_to_gcs_csv = TrinoToGCSOperator(\n    task_id=\"trino_to_gcs_csv\",\n    sql=\"SELECT * FROM my_table;\",\n    bucket=\"my-gcs-bucket\",\n    filename=\"data_csv_{}.csv\",\n    export_format=\"csv\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using SFTP Sensor with TaskFlow API\nDESCRIPTION: Demonstrates how to implement an SFTP sensor using Airflow's TaskFlow API with decorators. Includes support for op_args and op_kwargs for additional flexibility.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/sensors/sftp_sensor.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.decorators import task\n\n@task.sensor(sftp_conn_id=\"sftp_default\")\ndef check_file(path=\"/path/to/file/file.txt\"):\n    return path\n```\n\n----------------------------------------\n\nTITLE: Running a Google Cloud Build Trigger via Airflow Operator - Python\nDESCRIPTION: Uses CloudBuildRunBuildTriggerOperator in Airflow to execute a Google Cloud Build trigger at a specific git revision, supporting automated CI/CD flows. Requires trigger ID, project ID, optional source parameters, and proper authentication. Outputs build information to XCom, enabling downstream Airflow task integration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_build.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nrun_build_trigger = CloudBuildRunBuildTriggerOperator(\n    task_id=\"run_build_trigger\",\n    project_id=PROJECT_ID,\n    trigger_id=TRIGGER_ID,\n    source={\n        \"branchName\": \"main\"\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Creating AutoML Translation Dataset with TranslateCreateDatasetOperator in Python\nDESCRIPTION: Illustrates using the `TranslateCreateDatasetOperator` within an Airflow DAG to create a new AutoML translation dataset using the Cloud Translate API V3. It requires specifying the `dataset_name`, `project_id`, `location`, `source_language_code`, and `target_language_code`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncreate_dataset = TranslateCreateDatasetOperator(\n    task_id=\"create_dataset\",\n    dataset_name=DATASET_ID,\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n    source_language_code=LANG_FROM,\n    target_language_code=LANG_TO,\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving an Entry from Dataplex Catalog in Python\nDESCRIPTION: This snippet demonstrates how to use the DataplexCatalogGetEntryOperator to retrieve an Entry from a specific location in the Dataplex Catalog. It requires the appropriate Airflow provider and Google Cloud credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_56\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_dataplex_catalog_get_entry]\nget_entry = DataplexCatalogGetEntryOperator(\n    task_id=\"get_entry\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    entry_id=ENTRY_ID,\n    location=LOCATION,\n)\n# [END howto_operator_dataplex_catalog_get_entry]\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenLineage Tracking for an Airflow DAG using Python Helper\nDESCRIPTION: Demonstrates how to programmatically enable OpenLineage tracking for all tasks within a specific DAG using the `enable_lineage` helper function from `airflow.providers.openlineage.utils.selective_enable`. This functionality requires the `selective_enable` policy to be active in the Airflow OpenLineage configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.openlineage.utils.selective_enable import disable_lineage, enable_lineage\n\nwith enable_lineage(DAG(...)):\n    # Tasks within this DAG will have lineage tracking enabled\n    MyOperator(...)\n\n    AnotherOperator(...)\n```\n\n----------------------------------------\n\nTITLE: Creating BigQuery Table - Python\nDESCRIPTION: Example of using BigQueryCreateTableOperator to create a new table with specified schema.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncreate_table_task = BigQueryCreateTableOperator(\n    task_id=\"create_table\",\n    dataset_id=DATASET_NAME,\n    table_id=TABLE_NAME,\n    schema_fields=[\n        {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n        {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Detection Result from XCom - Airflow Python Operator\nDESCRIPTION: This snippet shows how to extract the result of a text detection task from XCom in an Airflow DAG using Python. The code retrieves the output from a prior task ('detect_text') for downstream usage, demonstrating dynamic dependency management within a DAG. Dependencies include the Airflow XCom API; the primary input is the task ID, and the output is the detected text data retrieved for further processing.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nresult = detect_text.output\n\n```\n\n----------------------------------------\n\nTITLE: Disabling Notification Channels with StackdriverDisableNotificationChannelsOperator in Python\nDESCRIPTION: This example shows how to use the StackdriverDisableNotificationChannelsOperator to disable Notification Channels identified by a given filter. The operator can be used with or without a project ID, which will be retrieved from the Google Cloud connection if not provided.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/stackdriver.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nStackdriverDisableNotificationChannelsOperator(\n    task_id='disable_notification_channels',\n    filter_='',\n    project_id=None,\n    gcp_conn_id='google_cloud_default'\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Deferrable Python Package Training Job in VertexAI with Deferrable Mode (Python)\nDESCRIPTION: Shows how to execute a Python package training job on VertexAI in deferrable (async) mode using Airflow's operator. Inputs and workflow are identical to standard mode, but with 'deferrable=True' for long-running jobs. Upon completion, the operator registers the resulting model in VertexAI and XCom.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n    create_python_package_training_job_deferrable = CreateCustomPythonPackageTrainingJobOperator(\n        task_id=\"train_model_with_python_package_deferrable\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        display_name=DISPLAY_NAME,\n        python_package_gcs_uri=PYTHON_PACKAGE_GCS_URI,\n        python_module_name=PYTHON_MODULE_NAME,\n        dataset_id=DATASET_ID,\n        gcp_conn_id=GCP_CONN_ID,\n        deferrable=True,\n    )\n```\n\n----------------------------------------\n\nTITLE: Using Template Parameters in Python\nDESCRIPTION: Demonstrates how to reference params in templated strings and set native object rendering. Shows parameter type handling in templates.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/params.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nPythonOperator(\n    task_id=\"from_template\",\n    op_args=[\n        \"{{ params.my_int_param + 10 }}\",\n    ],\n    python_callable=(\n        lambda my_int_param: print(my_int_param)\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a GKE Cluster Asynchronously with GKEDeleteClusterOperator\nDESCRIPTION: Example of using the GKEDeleteClusterOperator in deferrable mode to asynchronously delete a Google Kubernetes Engine cluster, freeing up worker resources during cluster deletion.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndelete_cluster_async = GKEDeleteClusterOperator(\n    task_id=\"delete_cluster_async\",\n    name=CLUSTER_NAME,\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Using DataflowJobMetricsSensor in Airflow\nDESCRIPTION: Demonstrates using the `DataflowJobMetricsSensor` to check if metrics reported by a Dataflow job meet certain criteria. This allows reacting to specific performance or data processing indicators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_native_python_async.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_sensor_wait_for_job_metric]\n    :end-before: [END howto_sensor_wait_for_job_metric]\n```\n\n----------------------------------------\n\nTITLE: Importing Data into Google Cloud VertexAI Dataset Operator in Airflow (Python)\nDESCRIPTION: Shows use of Airflow's ImportDataOperator to import data from Google Cloud Storage into a VertexAI dataset. Requires Airflow Google provider and a pre-existing dataset with data in a GCS bucket. User supplies dataset_id, project, location, and gcs_source_uris. Inputs: dataset_id (from creation), GCS file locations. The import operation loads data to the referenced VertexAI dataset.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    import_data = ImportDataOperator(\n        task_id=\"import_data\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        dataset_id=create_dataset.output[\"dataset_id\"],\n        import_configs=[import_config],\n        gcp_conn_id=GCP_CONN_ID,\n    )\n```\n\n----------------------------------------\n\nTITLE: Waiting for EMR on EKS Job Completion with Airflow Sensor (Python)\nDESCRIPTION: This snippet utilizes Airflow's EmrContainerSensor in Python to monitor the completion status of a submitted EMR on EKS job. By specifying the job_id and AWS connection, the sensor polls the job's state until it reaches a terminal status (success, failed, or cancelled). It depends on the job submission task's output and requires preconfigured AWS credentials and virtual cluster setup.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr_eks.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nfrom airflow.providers.amazon.aws.sensors.emr import EmrContainerSensor\n\nwait_for_emr_job = EmrContainerSensor(\n    task_id='wait_for_job',\n    virtual_cluster_id=virtual_cluster_id,\n    job_id=task_submit_job.output,\n    aws_conn_id='aws_default',\n)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Emitting an Asset from a Task using Python in Airflow\nDESCRIPTION: This code shows how to emit an asset from a task using the @task decorator with an outlet. It also demonstrates how to consume that asset in another task using the @asset decorator.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nexample_asset = Asset(\"example_asset\")\n\n\n@task(outlets=[example_asset])\ndef emit_example_asset():\n    \"\"\"Write to example_asset...\"\"\"\n\n\n@asset(schedule=None)\ndef process_example_asset(example_asset):\n    \"\"\"Process inlet example_asset...\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using Snowpark Task Decorator in Airflow DAG\nDESCRIPTION: Example implementation of the @task.snowpark decorator in an Airflow DAG. It demonstrates how to use the decorator to run Snowpark Python code in a Snowflake database, showing two methods to access the Snowpark session object.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/decorators/snowpark.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@task.snowpark\ndef get_sum(a, b, session=None):\n    # Method 1: Use the session object that's injected by the decorator\n    return session.sql(f\"SELECT {a} + {b}\").collect()[0][0]\n\n\n@task.snowpark\ndef get_sum_alt(a, b):\n    # Method 2: Get the active session that's already established by the decorator\n    from snowflake.snowpark import functions as F\n    from snowflake.snowpark.context import get_active_session\n\n    session = get_active_session()\n    return session.create_dataframe([[a, b]], schema=[\"a\", \"b\"]).select(F.col(\"a\") + F.col(\"b\")).collect()[0][0]\n```\n\n----------------------------------------\n\nTITLE: Accessing Airflow Context Variables in TaskFlow Python Tasks\nDESCRIPTION: Demonstrates how to access Airflow context variables directly from within a TaskFlow decorated Python function. This approach allows tasks to use runtime information without relying on Jinja templating.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/templates-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef my_task(ti=None, run_id=None, logical_date=None, **context):\n    print(f\"Task Instance: {ti}\")\n    print(f\"Run ID: {run_id}\")\n    print(f\"Logical Date: {logical_date}\")\n    # Access other context variables as needed\n    data_interval_start = context['data_interval_start']\n    print(f\"Data Interval Start: {data_interval_start}\")\n```\n\n----------------------------------------\n\nTITLE: Correct Implementation of Custom Timetable\nDESCRIPTION: This snippet demonstrates the correct way to implement a custom timetable in Airflow, where Airflow Variables are accessed during execution rather than at the top level.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import Variable\nfrom airflow.timetables.interval import CronDataIntervalTimetable\n\n\nclass CustomTimetable(CronDataIntervalTimetable):\n    def __init__(self, *args, something=\"something\", **kwargs):\n        self._something = Variable.get(something)\n        super().__init__(*args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from SFTP to Amazon S3 using SFTPToS3Operator in Python\nDESCRIPTION: Example showing how to configure and use the SFTPToS3Operator to load data from a SFTP server to an Amazon S3 file. This snippet demonstrates the operator setup with path specifications for both source and destination.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/sftp_to_s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_transfer_sftp_to_s3]\nsftp_to_s3 = SFTPToS3Operator(\n    task_id=\"sftp_to_s3\",\n    sftp_path=\"{{ var.json.example_sftp_to_s3_config.sftp_path }}\",\n    s3_bucket=\"{{ var.json.example_sftp_to_s3_config.s3_bucket }}\",\n    s3_key=\"{{ var.json.example_sftp_to_s3_config.s3_key }}\",\n)\n# [END howto_transfer_sftp_to_s3]\n```\n\n----------------------------------------\n\nTITLE: Implementing Self-Checks in Airflow DAGs\nDESCRIPTION: An example of adding validation tasks after data processing operations in a DAG. Uses S3KeySensor to verify that a file was successfully created in S3 as a result of the previous task's execution.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ntask = PushToS3(...)\ncheck = S3KeySensor(\n    task_id=\"check_parquet_exists\",\n    bucket_key=\"s3://bucket/key/foo.parquet\",\n    poke_interval=0,\n    timeout=0,\n)\ntask >> check\n```\n\n----------------------------------------\n\nTITLE: Accessing Airflow Connections in Templates (Python/Jinja)\nDESCRIPTION: Illustrates how to access Airflow connection data in templates, including login, password, and extras. Shows examples of fetching connections by string and providing defaults.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/templates-ref.rst#2025-04-22_snippet_3\n\nLANGUAGE: jinja\nCODE:\n```\n{{ conn.my_conn_id.login }}\n```\n\nLANGUAGE: jinja\nCODE:\n```\n{{ conn.my_conn_id.password }}\n```\n\nLANGUAGE: jinja\nCODE:\n```\n{{ conn.get('my_conn_id_'+index).host }}\n```\n\nLANGUAGE: jinja\nCODE:\n```\n{{ conn.get('my_conn_id', {\"host\": \"host1\", \"login\": \"user1\"}).host }}\n```\n\nLANGUAGE: jinja\nCODE:\n```\n{{ conn.my_aws_conn_id.extra_dejson.region_name }}\n```\n\nLANGUAGE: jinja\nCODE:\n```\n{{ conn.my_aws_conn_id.extra_dejson.get('region_name', 'Europe (Frankfurt)') }}\n```\n\n----------------------------------------\n\nTITLE: Inserting a Compute Engine Instance with ComputeEngineInsertInstanceOperator in Python\nDESCRIPTION: Creates a new Google Compute Engine instance using the ComputeEngineInsertInstanceOperator. This snippet demonstrates how to use the operator with and without specifying a project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineInsertInstanceOperator(\n    project_id=GCP_PROJECT_ID,\n    zone=GCE_ZONE,\n    resource_id=INSTANCE_NAME,\n    body=GCE_INSTANCE_BODY,\n    task_id=\"gcp_compute_create_instance\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineInsertInstanceOperator(\n    zone=GCE_ZONE,\n    resource_id=INSTANCE_NAME,\n    body=GCE_INSTANCE_BODY,\n    task_id=\"gcp_compute_create_instance\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Tag with CloudDataCatalogCreateTagOperator in Python\nDESCRIPTION: Example of creating a tag on an entry in Google Cloud DataCatalog using the CloudDataCatalogCreateTagOperator. The result is saved to XCom for use in other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_create_tag]\ncreate_tag = CloudDataCatalogCreateTagOperator(\n    task_id=\"create_tag\",\n    location=LOCATION,\n    entry_group=ENTRY_GROUP_ID,\n    entry=ENTRY_ID,\n    template_id=TEMPLATE_ID,\n    tag={\n        \"fields\": {\n            \"key\": {\n                \"string_value\": \"value\",\n            }\n        }\n    },\n)\n# [END howto_operator_gcp_datacatalog_create_tag]\n```\n\n----------------------------------------\n\nTITLE: Creating Amazon Comprehend Document Classifier via Airflow Operator - Python\nDESCRIPTION: Shows how to use the ComprehendCreateDocumentClassifierOperator in Airflow to launch document classifier training in Amazon Comprehend. Depends on Airflow's Amazon provider and correct AWS resource access. Key parameters include classifier name, input data location, output data location, data access role, and language code. The operator submits an asynchronous request and returns the classifier ARN.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/comprehend.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncreate_document_classifier = ComprehendCreateDocumentClassifierOperator(\n    task_id=\"create_document_classifier\",\n    input_data_config={\n        \"S3Uri\": INPUT_BUCKET,\n        \"LabelDelimiter\": \",\",\n    },\n    output_data_config={\"S3Uri\": OUTPUT_BUCKET},\n    data_access_role_arn=DATA_ACCESS_ROLE_ARN,\n    document_classifier_name=CLASSIFIER_NAME,\n    language_code=\"en\",\n)\n```\n\n----------------------------------------\n\nTITLE: Fetching SQL Data as Pandas DataFrames using DbApiHook in Airflow (Python)\nDESCRIPTION: This snippet demonstrates using the `get_df` and `get_df_by_chunks` methods of Airflow's `DbApiHook` to execute SQL queries and return the results as Pandas DataFrames. `get_df` retrieves the entire result set, while `get_df_by_chunks` allows processing large results iteratively. Requires the `pandas` extra to be installed. The `df_type` parameter must be set to `\"pandas\"`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/dataframes.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Get complete DataFrame in a single operation\ndf = hook.get_df(\n    sql=\"SELECT * FROM my_table WHERE date_column >= %s\", parameters=[\"2023-01-01\"], df_type=\"pandas\"\n)\n\n# Get DataFrame in chunks for memory-efficient processing of large results\nfor chunk_df in hook.get_df_by_chunks(sql=\"SELECT * FROM large_table\", chunksize=10000, df_type=\"pandas\"):\n    process_chunk(chunk_df)\n```\n\n----------------------------------------\n\nTITLE: Configuring SQS Failure Notifications in Airflow Python\nDESCRIPTION: This Python code demonstrates how to configure notifications to be sent to an Amazon SQS queue upon DAG or task failure within an Airflow workflow. It utilizes the `send_sqs_notification` function to create callback functions, specifying the AWS connection ID (`aws_conn_id`), SQS queue URL (`queue_url`), and message body (`message_body`), which can include Jinja templating like `{{ dag.dag_id }}`. These functions are then assigned to the `on_failure_callback` parameter at the DAG and Task levels respectively. Requires an AWS connection configured in Airflow (e.g., 'aws_default') and appropriate AWS permissions to send messages to the specified SQS queue.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/notifications/sqs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, timezone\nfrom airflow import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.amazon.aws.notifications.sqs import send_sqs_notification\n\ndag_failure_sqs_notification = send_sqs_notification(\n    aws_conn_id=\"aws_default\",\n    queue_url=\"https://sqs.eu-west-1.amazonaws.com/123456789098/MyQueue\",\n    message_body=\"The DAG {{ dag.dag_id }} failed\",\n)\ntask_failure_sqs_notification = send_sqs_notification(\n    aws_conn_id=\"aws_default\",\n    region_name=\"eu-west-1\",\n    queue_url=\"https://sqs.eu-west-1.amazonaws.com/123456789098/MyQueue\",\n    message_body=\"The task {{ ti.task_id }} failed\",\n)\n\nwith DAG(\n    dag_id=\"mydag\",\n    schedule=\"@once\",\n    start_date=datetime(2023, 1, 1, tzinfo=timezone.utc),\n    on_failure_callback=[dag_failure_sqs_notification],\n    catchup=False,\n):\n    BashOperator(task_id=\"mytask\", on_failure_callback=[task_failure_sqs_notification], bash_command=\"fail\")\n\n```\n\n----------------------------------------\n\nTITLE: Custom Branching Operator Implementation in Python\nDESCRIPTION: Shows how to create a custom branching operator by subclassing BaseBranchOperator. This example implements a branching logic based on the day of the month.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass MyBranchOperator(BaseBranchOperator):\n    def choose_branch(self, context):\n        \"\"\"\n        Run an extra branch on the first day of the month\n        \"\"\"\n        if context['data_interval_start'].day == 1:\n            return ['daily_task_id', 'monthly_task_id']\n        elif context['data_interval_start'].day == 2:\n            return 'daily_task_id'\n        else:\n            return None\n```\n\n----------------------------------------\n\nTITLE: Starting Java Dataflow Job with GCS JAR\nDESCRIPTION: Example showing how to create and run a Java Dataflow pipeline using a JAR file stored in Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n_ = BeamRunJavaPipelineOperator(\n    task_id=\"start-java-job\",\n    jar=f\"gs://{BUCKET_NAME}/{JAR_FILE}\",\n    pipeline_options=DATAFLOW_JAVA_OPTIONS,\n    job_class=\"org.apache.beam.examples.WordCount\",\n    location=location,\n)\n```\n\n----------------------------------------\n\nTITLE: Branching DAG Based on Day of Week - Airflow Python\nDESCRIPTION: This code demonstrates how to use the BranchDayOfWeekOperator to conditionally branch workflow execution based on the day of the week. The operator is provided with a list of weekdays and two branch task IDs for true/false outcomes. Required dependencies are Airflow and the standard providers package. Inputs involve the value of logical_date or current execution context, and outputs are which downstream task(s) are executed according to the weekday condition.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/datetime.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith DAG('example_branch_day_of_week_operator', start_date=datetime(2021, 1, 1)) as dag:\n    branch = BranchDayOfWeekOperator(\n        task_id='branch_day_of_week',\n        follow_task_ids_if_true=['run_on_weekday'],\n        follow_task_ids_if_false=['run_on_weekend'],\n        week_day=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n        use_task_logical_date=True,\n    )\n    run_on_weekday = DummyOperator(task_id='run_on_weekday')\n    run_on_weekend = DummyOperator(task_id='run_on_weekend')\n    branch >> [run_on_weekday, run_on_weekend]\n\n```\n\n----------------------------------------\n\nTITLE: Pulling Messages with Deferrable PubSubPullSensor\nDESCRIPTION: Example showing how to use the PubSubPullSensor in deferrable mode for asynchronous message pulling.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/pubsub.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npull_messages_async = PubSubPullSensor(\n    task_id=\"pull_messages_async\",\n    project_id=PROJECT_ID,\n    subscription=SUBSCRIPTION_ID,\n    max_messages=5,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing TaskGroups in Airflow\nDESCRIPTION: Shows how to organize tasks using TaskGroups for better visualization and management. Includes examples of task dependencies and group-level configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import task_group\n\n@task_group()\ndef group1():\n    task1 = EmptyOperator(task_id=\"task1\")\n    task2 = EmptyOperator(task_id=\"task2\")\n\ntask3 = EmptyOperator(task_id=\"task3\")\n\ngroup1() >> task3\n```\n\n----------------------------------------\n\nTITLE: Adding Operator Links to Existing Operators in Python\nDESCRIPTION: This code snippet shows how to add an extra link to an existing operator (GCSToS3Operator) using an Airflow plugin. It defines an S3LogLink class and registers it through an AirflowExtraLinkPlugin.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/define-extra-link.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import BaseOperator, BaseOperatorLink\nfrom airflow.models.taskinstancekey import TaskInstanceKey\nfrom airflow.plugins_manager import AirflowPlugin\nfrom airflow.providers.amazon.aws.transfers.gcs_to_s3 import GCSToS3Operator\n\n\nclass S3LogLink(BaseOperatorLink):\n    name = \"S3\"\n\n    # Add list of all the operators to which you want to add this OperatorLinks\n    # Example: operators = [GCSToS3Operator, GCSToBigQueryOperator]\n    operators = [GCSToS3Operator]\n\n    def get_link(self, operator: BaseOperator, *, ti_key: TaskInstanceKey):\n        # Invalid bucket name because upper case letters and underscores are used\n        # This will not be a valid bucket in any region\n        bucket_name = \"Invalid_Bucket_Name\"\n        return \"https://s3.amazonaws.com/airflow-logs/{bucket_name}/{dag_id}/{task_id}/{run_id}\".format(\n            bucket_name=bucket_name,\n            dag_id=operator.dag_id,\n            task_id=operator.task_id,\n            run_id=ti_key.run_id,\n        )\n\n\n# Defining the plugin class\nclass AirflowExtraLinkPlugin(AirflowPlugin):\n    name = \"extra_link_plugin\"\n    operator_extra_links = [\n        S3LogLink(),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Migrating Deprecated Airflow Functions in Python\nDESCRIPTION: Code migration examples for deprecated Airflow functions and constants that have been removed in version 3.0. Shows the replacement of TemporaryDirectory, mkdirs, test_cycle functions and removal of SHUTDOWN and terminating_states constants.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41395.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old code\nfrom airflow.utils.file import TemporaryDirectory\nfrom airflow.utils.file import mkdirs\nfrom airflow.utils.dag_cycle_tester import test_cycle\nfrom airflow.utils.state import SHUTDOWN, terminating_states\n\n# New code\nfrom tempfile import TemporaryDirectory\nfrom pathlib import Path\nfrom airflow.utils.dag_cycle_tester import check_cycle\n\n# Usage example\nPath(path).mkdir(parents=True, exist_ok=True)  # Instead of mkdirs\ncheck_cycle(dag)  # Instead of test_cycle\n```\n\n----------------------------------------\n\nTITLE: Ingesting Weaviate Data with Custom Vectors from Callable using Python\nDESCRIPTION: This snippet shows how to use the `WeaviateIngestOperator` to ingest data into a Weaviate collection (`ClassName`) with custom vectors provided by a Python callable (`get_data_with_vectors_callable`). It uses the specified Weaviate connection (`weaviate_default`).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/weaviate/docs/operators/weaviate.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ningest_data_with_vectors_from_callable = WeaviateIngestOperator(\n    task_id=\"ingest_data_with_vectors_from_callable\",\n    conn_id=\"weaviate_default\",\n    class_name=\"ClassName\",\n    input_data=get_data_with_vectors_callable(),\n)\n```\n\n----------------------------------------\n\nTITLE: Creating/Updating EventBridge Rules using PutRuleOperator\nDESCRIPTION: Demonstrates using EventBridgePutRuleOperator to create or update rules in Amazon EventBridge. The operator configures event patterns and schedules for rule execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eventbridge.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nput_rule = EventBridgePutRuleOperator(\n    task_id=\"put_rule\",\n    name=\"test-rule\",\n    event_pattern=json.dumps({\n        \"source\": [\"test.source\"],\n        \"detail-type\": [\"TestMessage\"],\n    }),\n    description=\"Test rule\",\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Bash Scripts from Files with BashOperator in Airflow\nDESCRIPTION: Shows how to execute Bash scripts stored in files using the BashOperator in Airflow. The scripts must have a .sh or .bash extension.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nt2 = BashOperator(\n    task_id=\"bash_example\",\n    # \"scripts\" folder is under \"/usr/local/airflow/dags\"\n    bash_command=\"scripts/test.sh\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Branching Logic with @task.branch in Python\nDESCRIPTION: Demonstrates how to use the @task.branch decorator to implement conditional task execution based on XCom values. This example chooses between different tasks based on a value pulled from XCom.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@task.branch(task_id=\"branch_task\")\ndef branch_func(ti=None):\n    xcom_value = int(ti.xcom_pull(task_ids=\"start_task\"))\n    if xcom_value >= 5:\n        return \"continue_task\"\n    elif xcom_value >= 3:\n        return \"stop_task\"\n    else:\n        return None\n\n\nstart_op = BashOperator(\n    task_id=\"start_task\",\n    bash_command=\"echo 5\",\n    do_xcom_push=True,\n    dag=dag,\n)\n\nbranch_op = branch_func()\n\ncontinue_op = EmptyOperator(task_id=\"continue_task\", dag=dag)\nstop_op = EmptyOperator(task_id=\"stop_task\", dag=dag)\n\nstart_op >> branch_op >> [continue_op, stop_op]\n```\n\n----------------------------------------\n\nTITLE: Creating Amazon Comprehend PII Entities Detection Job via Airflow Operator - Python\nDESCRIPTION: Demonstrates how to use the ComprehendStartPiiEntitiesDetectionJobOperator in Airflow to initialize a PII entity detection job on Amazon Comprehend. Requires the Airflow Amazon provider, appropriate AWS credentials, and prerequisite S3 input/output buckets. Parameters typically specify the job name, input/output configurations, data access role, and language code. The operator starts the job and returns its response metadata.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/comprehend.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncomprehend_start_pii_entities_detection_job = ComprehendStartPiiEntitiesDetectionJobOperator(\n    task_id=\"comprehend_start_pii_entities_detection_job\",\n    input_data_config={\n        \"S3Uri\": INPUT_BUCKET,\n        \"InputFormat\": \"ONE_DOC_PER_FILE\",\n    },\n    output_data_config={\"S3Uri\": OUTPUT_BUCKET},\n    mode=\"ONLY_REDACTION\",\n    data_access_role_arn=DATA_ACCESS_ROLE_ARN,\n    language_code=\"en\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using Extended Template Fields Operator\nDESCRIPTION: Example showing how to use the extended operator with template fields in a DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nwith dag:\n    hello_task = MyHelloOperator(\n        task_id=\"task_id_1\",\n        name=\"{{ task_instance.task_id }}\",\n        world=\"{{ var.value.my_world }}\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Uploading Google Sheets Data to GCS using GoogleSheetsToGCSOperator\nDESCRIPTION: Example showing how to configure and use the GoogleSheetsToGCSOperator to transfer data from a Google Spreadsheet to Google Cloud Storage. The operator allows specifying the spreadsheet ID, range, target bucket and object name for the transfer.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/sheets_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nupload_sheet_to_gcs = GoogleSheetsToGCSOperator(\n    task_id=\"upload_sheet_to_gcs\",\n    spreadsheet_id=SPREADSHEET_ID,\n    range_=RANGE,\n    destination_bucket=BUCKET,\n    destination_object=OBJECT_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Merging and Cleaning Data in Postgres using Python in Airflow\nDESCRIPTION: Python task that executes a SQL query to merge and deduplicate data from a staging table into a final table. This task demonstrates using PostgresHook to execute complex SQL operations within an Airflow task.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/pipeline.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import task\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\n\n\n@task\ndef merge_data():\n    query = \"\"\"\n        INSERT INTO employees\n        SELECT *\n        FROM (\n            SELECT DISTINCT *\n            FROM employees_temp\n        ) t\n        ON CONFLICT (\"Serial Number\") DO UPDATE\n        SET\n              \"Employee Markme\" = excluded.\"Employee Markme\",\n              \"Description\" = excluded.\"Description\",\n              \"Leave\" = excluded.\"Leave\";\n    \"\"\"\n    try:\n        postgres_hook = PostgresHook(postgres_conn_id=\"tutorial_pg_conn\")\n        conn = postgres_hook.get_conn()\n        cur = conn.cursor()\n        cur.execute(query)\n        conn.commit()\n        return 0\n    except Exception as e:\n        return 1\n```\n\n----------------------------------------\n\nTITLE: Waiting for an EMR Serverless Application State with Airflow Sensor - Python\nDESCRIPTION: Shows how to monitor the state of an EMR Serverless Application using EmrServerlessApplicationSensor in Airflow. Dependencies are airflow.providers.amazon.aws and optionally aiobotocore for async polling. Parameters include application_id; the sensor waits until the application reaches a desired terminal state. Proper error handling is important for production workloads.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr_serverless.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.amazon.aws.sensors.emr import EmrServerlessApplicationSensor\n\nwait_for_emr_serverless_application = EmrServerlessApplicationSensor(\n    task_id=\"wait_for_emr_serverless_application\",\n    application_id=\"{{ task_instance.xcom_pull(task_ids='create_emr_serverless_application') }}\",\n    deferrable=True,  # Optional\n    dag=dag,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Creating BigQuery Materialized View - Python\nDESCRIPTION: Example of creating a materialized view that caches query results in BigQuery.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncreate_materialized_view_task = BigQueryCreateTableOperator(\n    task_id=\"create_materialized_view\",\n    dataset_id=DATASET_NAME,\n    table_id=MATERIALIZED_VIEW_NAME,\n    materialized_view={\"query\": f\"SELECT SUM(salary) FROM {DATASET_NAME}.{TABLE_NAME}\", \"enableRefresh\": True},\n)\n```\n\n----------------------------------------\n\nTITLE: Processing Output with BashOperator in Airflow\nDESCRIPTION: Shows how to use the output_processor parameter to process the output of a bash script before storing it as an XCom when using the BashOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nbash_task = BashOperator(\n    task_id=\"filter_today_changes\",\n    bash_command=\"\"\"\n        jq -c '.[] | select(.lastModified > \"{{ data_interval_start | ts_zulu }}\" or .created > \"{{ data_interval_start | ts_zulu }}\")' \\\n        example.json\n    \"\"\",\n    output_processor=lambda output: json.loads(output),\n)\n```\n\n----------------------------------------\n\nTITLE: Restricting DAG Access for Specific User\nDESCRIPTION: Policy that explicitly forbids access to 'secret-dag-1' and 'secret-dag-2' DAGs for a specific user, overriding any permissive policies that might apply through group membership.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/manage/index.rst#2025-04-22_snippet_8\n\nLANGUAGE: cedar\nCODE:\n```\nforbid(\n  principal == Airflow::User::\"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\",\n  action,\n  resource in [Airflow::Dag::\"secret-dag-1\", Airflow::Dag::\"secret-dag-2\"]\n);\n```\n\n----------------------------------------\n\nTITLE: Defining Environment Variables for Apache Airflow in Docker Compose\nDESCRIPTION: This YAML snippet defines the environment variables supported by Docker Compose for Apache Airflow. It includes variables for specifying the Airflow image, user ID, and additional configuration options for testing and development purposes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\n+--------------------------------+-----------------------------------------------------+--------------------------+\n|   Variable                     | Description                                         | Default                  |\n+================================+=====================================================+==========================+\n| ``AIRFLOW_IMAGE_NAME``         | Airflow Image to use.                               | apache/airflow:|version| |\n+--------------------------------+-----------------------------------------------------+--------------------------+\n| ``AIRFLOW_UID``                | UID of the user to run Airflow containers as.       | ``50000``                |\n|                                | Override if you want to use non-default Airflow     |                          |\n|                                | UID (for example when you map folders from host,    |                          |\n|                                | it should be set to result of ``id -u`` call.       |                          |\n|                                | When it is changed, a user with the UID is          |                          |\n|                                | created with ``default`` name inside the container  |                          |\n|                                | and home of the use is set to ``/airflow/home/``    |                          |\n|                                | in order to share Python libraries installed there. |                          |\n|                                | This is in order to achieve the  OpenShift          |                          |\n|                                | compatibility. See more in the                      |                          |\n|                                | :ref:`Arbitrary Docker User <arbitrary-docker-user>`|                          |\n+--------------------------------+-----------------------------------------------------+--------------------------+\n```\n\n----------------------------------------\n\nTITLE: Passing Paths Between Tasks Using XCOM\nDESCRIPTION: Shows how to create and write to files in object storage by passing ObjectStoragePath instances between tasks using Airflow's XCOM mechanism.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/objectstorage.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef create(path: ObjectStoragePath) -> ObjectStoragePath:\n    return path / \"new_file.txt\"\n\n\n@task\ndef write_file(path: ObjectStoragePath, content: str):\n    with path.open(\"wb\") as f:\n        f.write(content)\n\n\nnew_file = create(base)\nwrite = write_file(new_file, b\"data\")\n\nread >> write\n```\n\n----------------------------------------\n\nTITLE: Listing Airflow Tasks in Tree Format\nDESCRIPTION: This command lists the tasks in the 'tutorial' DAG in a tree format, providing a hierarchical view of the task structure.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/fundamentals.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nairflow tasks list tutorial --tree\n```\n\n----------------------------------------\n\nTITLE: Visualizing DAG Execution Metrics Timeline with Mermaid\nDESCRIPTION: A Gantt chart showing the timeline of DAG execution events and corresponding metrics in Apache Airflow. The diagram illustrates key milestones like DAG scheduling, task scheduling, execution, and completion, along with associated timing metrics such as schedule delay and task duration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/METRICS.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\n---\ndisplayMode: compact\n---\ngantt\n  title Airflow metrics for a DAG run\n  dateFormat HH:mm\n  axisFormat %H:%M\n  tickInterval 1hour\n\n  section Events\n    %% Milestones are \"point in time\" events, but they still take a duration.\n    %% Setting it to 2 minutes here, I don't think it has any importance.\n\n    DAG Scheduled         : milestone, dag_sched,   12:00, 2min\n    DAG Starts            : milestone, dag_start,   13:00, 2min\n    First task Scheduled  : milestone, task1_sched, 14:00, 2min\n    Task N Scheduled      : milestone, taskN_sched, 15:00, 2min\n    Task N starts running : milestone, taskN_start, 16:00, 2min\n    Task N done           : milestone, taskN_done,  17:00, 2min\n    Last task ends, DAG execution ends : milestone, end, 18:00, 2min\n\n  section Metrics\n    %% The start of the metrics can be conveniently marked with `after`,\n    %% but with this kind of diagram, we have no way to \"bind\" the end to a milestone,\n    %% so the durations have to be computed manually, sorry.\n    %%\n    %% If you modify the events above, make sure with your eyes that those metrics still match what they're supposed to.\n\n    dagrun.schedule_delay           : after dag_sched, 1h\n    first_task_scheduling_delay     : after dag_sched, 2h\n    duration.success/failure.dag_id : after dag_start, 5h\n    task_id.duration                : after taskN_start, 1h\n    task N landing time (only in airflow UI) : after dag_sched, 5h\n```\n\n----------------------------------------\n\nTITLE: Creating a Cloud SQL Database in Python\nDESCRIPTION: Example of using CloudSQLCreateInstanceDatabaseOperator to create a new database inside a Cloud SQL instance. Shows usage with and without project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsql_create_database = CloudSQLCreateInstanceDatabaseOperator(\n    instance=INSTANCE_NAME,\n    body=db_create_body,\n    database=DB_NAME,\n    task_id=\"sql_create_database\",\n)\n\nsql_create_database_with_project = CloudSQLCreateInstanceDatabaseOperator(\n    project_id=GCP_PROJECT_ID,\n    instance=INSTANCE_NAME,\n    body=db_create_body,\n    database=DB_NAME,\n    task_id=\"sql_create_database_with_project\",\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Pipeline with AzureSynapseRunPipelineOperator in Python\nDESCRIPTION: This snippet shows how to use the AzureSynapseRunPipelineOperator to execute a pipeline application within Azure Synapse Analytics. The operator executes a Synapse Pipeline.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/azure_synapse.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../tests/system/microsoft/azure/example_synapse_run_pipeline.py\n      :language: python\n      :dedent: 4\n      :start-after: [START howto_operator_azure_synapse_run_pipeline]\n      :end-before: [END howto_operator_azure_synapse_run_pipeline]\n```\n\n----------------------------------------\n\nTITLE: Multiple Asset Scheduling in Python\nDESCRIPTION: Shows how to schedule a DAG based on updates to multiple assets. The DAG runs only after all specified assets have been updated at least once since the last DAG run.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/asset-scheduling.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"multiple_assets_example\",\n    schedule=[\n        example_asset_1,\n        example_asset_2,\n        example_asset_3,\n    ],\n    ...,\n):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Querying Google Search Ads Reports with GoogleSearchAdsSearchOperator\nDESCRIPTION: This snippet demonstrates how to use the GoogleSearchAdsSearchOperator to query Search Ads reports. The operator supports Jinja templating for dynamically determining parameter values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/search_ads.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[START howto_search_ads_search_query_reports]\n[END howto_search_ads_search_query_reports]\n```\n\n----------------------------------------\n\nTITLE: Pulling Multiple XComs in Python Task\nDESCRIPTION: Demonstrates how to pull multiple XCom values that were pushed by a task with multiple outputs. It shows pulling individual keys and the entire XCom data.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/xcoms.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef xcom_pull_with_multiple_outputs(**context):\n    key1 = context[\"ti\"].xcom_pull(task_ids=\"push_multiple\", key=\"key1\")  # to pull key1\n    key2 = context[\"ti\"].xcom_pull(task_ids=\"push_multiple\", key=\"key2\")  # to pull key2\n\n    # Pulling entire xcom data from push_multiple task\n    data = context[\"ti\"].xcom_pull(task_ids=\"push_multiple\", key=\"return_value\")\n```\n\n----------------------------------------\n\nTITLE: Initializing DAG-level Params in Python\nDESCRIPTION: Demonstrates how to configure DAG-level parameters using the params kwarg when initializing a DAG. Shows parameter access within a task and usage of the Param class for validation.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/params.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import DAG\nfrom airflow.sdk import task\nfrom airflow.sdk import Param\n\nwith DAG(\n    \"the_dag\",\n    params={\n        \"x\": Param(5, type=\"integer\", minimum=3),\n        \"my_int_param\": 6\n    },\n) as dag:\n\n    @task.python\n    def example_task(params: dict):\n        # This will print the default value, 6:\n        dag.log.info(dag.params['my_int_param'])\n\n        # This will print the manually-provided value, 42:\n        dag.log.info(params['my_int_param'])\n\n        # This will print the default value, 5, since it wasn't provided manually:\n        dag.log.info(params['x'])\n\n    example_task()\n\nif __name__ == \"__main__\":\n    dag.test(\n        run_conf={\"my_int_param\": 42}\n    )\n```\n\n----------------------------------------\n\nTITLE: Using Asset Decorator Pattern\nDESCRIPTION: Demonstrates the simplified decorator pattern for creating asset-based tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import asset\n\n\n@asset(uri=\"s3://asset-bucket/example.csv\", schedule=\"@daily\")\ndef example_asset():\n    \"\"\"Write data to example_asset...\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Loading S3 Data into Existing DynamoDB Table Using S3ToDynamoDBOperator in Python\nDESCRIPTION: This code snippet shows how to use the S3ToDynamoDBOperator to load data from an Amazon S3 bucket into an existing Amazon DynamoDB table. It includes additional parameters for specifying the existing table name and primary key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/s3_to_dynamodb.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nS3ToDynamoDBOperator(\n    task_id=\"s3_to_dynamodb_existing_table\",\n    s3_bucket=\"{{ BUCKET_NAME }}\",\n    s3_key=\"{{ S3_KEY }}\",\n    dynamodb_table_name=\"{{ DYNAMODB_TABLE_NAME }}\",\n    aws_conn_id=\"aws_default\",\n    region=\"{{ AWS_DEFAULT_REGION }}\",\n    target_table_name=\"{{ DYNAMODB_TABLE_NAME }}\",\n    primary_key=\"id\",\n)\n```\n\n----------------------------------------\n\nTITLE: Uploading Data to Azure DataLake Storage with ADLSCreateObjectOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the ADLSCreateObjectOperator to upload data to Azure DataLake Storage. It requires the airflow.providers.microsoft.azure.operators.adls module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/adls.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_adls_create]\n# Code snippet for ADLSCreateObjectOperator\n# [END howto_operator_adls_create]\n```\n\n----------------------------------------\n\nTITLE: Implementing ExternalTaskMarker in Python for Apache Airflow\nDESCRIPTION: This example demonstrates the use of ExternalTaskMarker to create a relationship between tasks in different DAGs. It allows for clearing dependent tasks across DAGs when the 'Recursive' option is selected during task clearing.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/external_task_sensor.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nparent_task = ExternalTaskMarker(\n    task_id='parent_task',\n    external_dag_id='example_external_task_marker_child',\n    external_task_id='child_task1',\n)\n```\n\n----------------------------------------\n\nTITLE: Combining Upstream Data (Zipping) in Python for Apache Airflow\nDESCRIPTION: Illustrates how to combine multiple input sources into one task mapping iterable using the zip() function. This example shows downloading files from S3 while renaming them.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nlist_filenames_a = S3ListOperator(\n    task_id=\"list_files_in_a\",\n    bucket=\"bucket\",\n    prefix=\"incoming/provider_a/{{ data_interval_start|ds }}\",\n)\nlist_filenames_b = [\"rename_1\", \"rename_2\", \"rename_3\", ...]\n\nfilenames_a_b = list_filenames_a.output.zip(list_filenames_b)\n\n\n@task\ndef download_filea_from_a_rename(filenames_a_b):\n    fn_a, fn_b = filenames_a_b\n    S3Hook().download_file(fn_a, local_path=fn_b)\n\n\ndownload_filea_from_a_rename.expand(filenames_a_b=filenames_a_b)\n```\n\n----------------------------------------\n\nTITLE: Deleting Version Aliases from Model using Vertex AI Model Service Operator - Python\nDESCRIPTION: Gives an example of removing aliases from a model version using DeleteVersionAliasesOnModelOperator. The task decreases the set of human-friendly version pointers for a model version, requiring model/version identification.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndelete_version_aliases_task = DeleteVersionAliasesOnModelOperator(\n    task_id=\"delete_version_aliases_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    model_id=MODEL_ID,\n    version_id=VERSION_ID,\n    version_aliases=[\"alias_to_remove\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Simple TaskFlow Task with Direct Parameter\nDESCRIPTION: Shows how to create a simple TaskFlow task that accepts a direct parameter value.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/taskflow.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef hello_name(name: str):\n    print(f'Hello {name}!')\n\nhello_name('Airflow users')\n```\n\n----------------------------------------\n\nTITLE: Creating EKS Cluster and Fargate Profile Simultaneously using EksCreateClusterOperator in Python\nDESCRIPTION: This Python snippet demonstrates creating an EKS cluster along with an AWS Fargate profile using `EksCreateClusterOperator`. This allows running pods on Fargate without managing EC2 instances. The operator can run in deferrable mode (`deferrable=True`). Requires IAM role with specific trust relationships and policies (`AmazonEC2ContainerRegistryReadOnly`, `AmazonEKSClusterPolicy`, `AmazonEKSWorkerNodePolicy`).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eks.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Example using EksCreateClusterOperator with Fargate profile creation\n# Assumes necessary imports and DAG context\n\ncreate_cluster_and_fargate_profile = EksCreateClusterOperator(\n    task_id=\"create_cluster_and_fargate_profile\",\n    cluster_name=\"my-fargate-cluster\",\n    cluster_role_arn=\"arn:aws:iam::123456789012:role/EksClusterRole\",\n    resources_vpc_config={\n        \"subnetIds\": [\"subnet-xxxxxxxxxxxxxxxxx\", \"subnet-yyyyyyyyyyyyyyyyy\"],\n        \"securityGroupIds\": [\"sg-xxxxxxxxxxxxxxxxx\"],\n    },\n    # Fargate profile configuration\n    fargate_profile_name=\"my-fargate-profile\",\n    fargate_pod_execution_role_arn=\"arn:aws:iam::123456789012:role/EksFargatePodExecutionRole\",\n    fargate_selectors=[{\"namespace\": \"default\"}],\n    deferrable=False,  # Set to True for deferrable mode\n    # other cluster and Fargate parameters...\n)\n\n# [END howto_operator_eks_create_cluster_with_fargate_profile]\n```\n\n----------------------------------------\n\nTITLE: Templating Bash Commands with BashOperator in Airflow\nDESCRIPTION: Shows how to use Jinja templates to parameterize Bash commands when using the BashOperator in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nt1 = BashOperator(\n    task_id=\"bash_example\",\n    bash_command=(\n        \"echo \\\"execution_date={{ execution_date }}\\\" && \"\n        \"echo \\\"execution_date.add(days=4)={{ execution_date.add(days=4) }}\\\" && \"\n        \"echo \\\"execution_date.add(days=4).isoformat()={{ execution_date.add(days=4).isoformat() }}\\\"\"\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Task-Generated Mapping in Airflow with Python\nDESCRIPTION: Example showing how to create mapped tasks based on the output of an upstream task. The task 'make_list' dynamically generates values that are then consumed by multiple instances of the 'consumer' task.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef make_list():\n    # This can also be from an API call, checking a database, -- almost anything you like, as long as the\n    # resulting list/dictionary can be stored in the current XCom backend.\n    return [1, 2, {\"a\": \"b\"}, \"str\"]\n\n\n@task\ndef consumer(arg):\n    print(arg)\n\n\nwith DAG(dag_id=\"dynamic-map\", start_date=datetime(2022, 4, 2)) as dag:\n    consumer.expand(arg=make_list())\n```\n\n----------------------------------------\n\nTITLE: Deleting an Amazon EKS Cluster using EksDeleteClusterOperator in Python\nDESCRIPTION: This Python snippet illustrates using the `EksDeleteClusterOperator` to delete an existing Amazon EKS cluster. The operator requires the `cluster_name` parameter. It can run in deferrable mode by setting `deferrable=True`. Note that deletion fails if attached resources like nodegroups exist, unless the `force` parameter is used.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eks.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Example using EksDeleteClusterOperator\n# Assumes necessary imports and DAG context\n\ndelete_cluster = EksDeleteClusterOperator(\n    task_id=\"delete_eks_cluster\",\n    cluster_name=\"my-eks-cluster\", # Specify the cluster to delete\n    deferrable=False, # Set to True for deferrable mode\n)\n\n# [END howto_operator_eks_delete_cluster]\n```\n\n----------------------------------------\n\nTITLE: Running a Pod on EKS using EksPodOperator in Python\nDESCRIPTION: This Python snippet shows how to run a Kubernetes pod on an existing Amazon EKS cluster using the `EksPodOperator`. It requires an EKS cluster with compute infrastructure (nodegroups or Fargate profiles) already set up. Key parameters include `cluster_name`, pod configuration details (`namespace`, `image`, `name`, `cmds`, etc.).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eks.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Example using EksPodOperator\n# Assumes necessary imports and DAG context, and an existing EKS cluster with compute\n\nrun_this = EksPodOperator(\n    task_id=\"run_pod_on_eks\",\n    cluster_name=\"my-existing-eks-cluster\",\n    pod_name=\"my-test-pod\",\n    namespace=\"default\",\n    image=\"ubuntu:latest\",\n    cmds=[\"bash\", \"-cx\"],\n    arguments=[\"echo 10; sleep 30; echo \"$(hostname)\"\"],\n    # other pod configuration parameters...\n)\n\n# [END howto_operator_eks_pod_operator]\n```\n\n----------------------------------------\n\nTITLE: Submitting EMR on EKS Job Using Airflow EmrContainerOperator (Python)\nDESCRIPTION: This Python snippet shows how to instantiate and use Airflow's EmrContainerOperator to submit a Spark job to an existing EMR on EKS virtual cluster. It takes parameters such as virtual_cluster_id, execution_role_arn, job configuration, and AWS connection id. The operator triggers the job and returns a job ID. Ensuring proper AWS credentials and Airflow connection configuration is essential for successful operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr_eks.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nfrom airflow.providers.amazon.aws.operators.emr import EmrContainerOperator\n\ntask_submit_job = EmrContainerOperator(\n    task_id='submit_pi_job',\n    virtual_cluster_id=virtual_cluster_id,\n    execution_role_arn=execution_role_arn,\n    release_label='emr-6.2.0-latest',\n    job_driver=job_driver,\n    configuration_overrides=configuration_overrides,\n    aws_conn_id='aws_default',\n)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Deferrable Sensor in Python\nDESCRIPTION: Example of a simple sensor that defers execution for one hour using TimeDeltaTrigger. Shows basic implementation of execute and execute_complete methods.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/deferring.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\n\nfrom airflow.sdk import BaseSensorOperator\nfrom airflow.providers.standard.triggers.temporal import TimeDeltaTrigger\n\nif TYPE_CHECKING:\n    from airflow.utils.context import Context\n\n\nclass WaitOneHourSensor(BaseSensorOperator):\n    def execute(self, context: Context) -> None:\n        self.defer(trigger=TimeDeltaTrigger(timedelta(hours=1)), method_name=\"execute_complete\")\n\n    def execute_complete(self, context: Context, event: dict[str, Any] | None = None) -> None:\n        # We have no more work to do here. Mark as complete.\n        return\n```\n\n----------------------------------------\n\nTITLE: Named Mapping with SQL Operators in Airflow\nDESCRIPTION: Shows how to use map_index_template to provide custom names for mapped tasks in the Airflow UI based on the task's input parameters. This improves visibility and debugging by making task instances more identifiable.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\n\n# The two expanded task instances will be named \"2024-01-01\" and \"2024-01-02\".\nSQLExecuteQueryOperator.partial(\n    ...,\n    sql=\"SELECT * FROM data WHERE date = %(date)s\",\n    map_index_template=\"\"\"{{ task.parameters['date'] }}\"\"\",\n).expand(\n    parameters=[{\"date\": \"2024-01-01\"}, {\"date\": \"2024-01-02\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Running Streaming Dataflow Python Job in Airflow\nDESCRIPTION: Shows how to execute a streaming Dataflow job using Airflow. This requires setting the streaming option in the Python pipeline or using an unbounded data source. The `drain_pipeline=True` parameter allows stopping the job gracefully by draining instead of canceling.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_streaming_python.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_start_streaming_python_job]\n    :end-before: [END howto_operator_start_streaming_python_job]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Google Dataprep Jobs for a Job Group in Python\nDESCRIPTION: Example usage of the DataprepGetJobsForJobGroupOperator to get information about batch jobs within a Cloud Dataprep job group.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataprep.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [START how_to_dataprep_get_jobs_for_job_group_operator]\n# Example usage code would be here\n# [END how_to_dataprep_get_jobs_for_job_group_operator]\n```\n\n----------------------------------------\n\nTITLE: Canceling a Google Cloud Workflow Execution using Airflow Operator in Python\nDESCRIPTION: Illustrates how to cancel an ongoing workflow execution using the `WorkflowsCancelExecutionOperator`. Requires `workflow_id`, `location_id`, and the `execution_id` of the execution to be cancelled.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/workflows.rst#2025-04-22_snippet_10\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/workflows/example_workflows.py\n      :language: python\n      :dedent: 4\n      :start-after: [START how_to_cancel_execution]\n      :end-before: [END how_to_cancel_execution]\n```\n\n----------------------------------------\n\nTITLE: Defining Task Dependencies in Airflow using Set Methods\nDESCRIPTION: Shows an alternative way to define task dependencies in Airflow using the set_upstream and set_downstream methods. This is equivalent to using bitshift operators but less commonly used.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/tasks.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfirst_task.set_downstream(second_task)\nthird_task.set_upstream(second_task)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Task Decorator Function in Python\nDESCRIPTION: This code snippet demonstrates how to create a custom task decorator function for a 'foo' task. It uses the task_decorator_factory to convert a FooDecoratedOperator into a TaskFlow function decorator.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/create-custom-decorator.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import TYPE_CHECKING\nfrom airflow.sdk.bases.decorator import task_decorator_factory\n\nif TYPE_CHECKING:\n    from airflow.sdk.bases.decorator import TaskDecorator\n\n\ndef foo_task(\n    python_callable: Callable | None = None,\n    multiple_outputs: bool | None = None,\n    **kwargs,\n) -> \"TaskDecorator\":\n    return task_decorator_factory(\n        python_callable=python_callable,\n        multiple_outputs=multiple_outputs,\n        decorated_operator_class=FooDecoratedOperator,\n        **kwargs,\n    )\n```\n\n----------------------------------------\n\nTITLE: S3 File Processing with Mixed TaskFlow and Classic Operators\nDESCRIPTION: Example showing how to process multiple S3 files using a combination of TaskFlow and classic operators, including file listing and line counting functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom airflow.sdk import DAG\nfrom airflow.sdk import task\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.operators.s3 import S3ListOperator\n\n\nwith DAG(dag_id=\"mapped_s3\", start_date=datetime(2020, 4, 7)) as dag:\n    list_filenames = S3ListOperator(\n        task_id=\"get_input\",\n        bucket=\"example-bucket\",\n        prefix='incoming/provider_a/{{ data_interval_start.strftime(\"%Y-%m-%d\") }}',\n    )\n\n    @task\n    def count_lines(aws_conn_id, bucket, filename):\n        hook = S3Hook(aws_conn_id=aws_conn_id)\n\n        return len(hook.read_key(filename, bucket).splitlines())\n\n    @task\n    def total(lines):\n        return sum(lines)\n\n    counts = count_lines.partial(aws_conn_id=\"aws_default\", bucket=list_filenames.bucket).expand(\n        filename=list_filenames.output\n    )\n\n    total(lines=counts)\n```\n\n----------------------------------------\n\nTITLE: Configuring DAG Schedules in Python using Cron and Timedelta\nDESCRIPTION: Examples of setting up DAG schedules using different methods: cron expression ('0 0 * * *' for daily at midnight), cron preset ('@daily'), and timedelta object (1 day interval). These demonstrate the various ways to define recurring execution schedules for Airflow DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/cron.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import DAG\n\nimport datetime\n\ndag = DAG(\"regular_interval_cron_example\", schedule=\"0 0 * * *\", ...)\n\ndag = DAG(\"regular_interval_cron_preset_example\", schedule=\"@daily\", ...)\n\ndag = DAG(\"regular_interval_timedelta_example\", schedule=datetime.timedelta(days=1), ...)\n```\n\n----------------------------------------\n\nTITLE: Listing Models using Vertex AI Model Service Operator - Python\nDESCRIPTION: Shows how to list all models in a Vertex AI project using the ListModelsOperator. The operator takes project_id and region as inputs and delivers a list of model resources, suitable for iteration or selection in larger model management workflows.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nlist_models_task = ListModelsOperator(\n    task_id=\"list_models_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Running an Athena Query using Airflow AthenaOperator in Python\nDESCRIPTION: This Python snippet demonstrates how to use the `airflow.providers.amazon.aws.operators.athena.AthenaOperator` to execute an SQL query against an Amazon Athena database. The results of the query are stored in the specified S3 output location. It requires an AWS connection configured in Airflow (`aws_conn_id`), the target database name, the query to execute, and the S3 path for output.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/athena/athena_boto.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nrun_query = AthenaOperator(\n    task_id=\"run_query\",\n    query=\"SELECT * FROM my_table\",\n    database=\"my_database\",\n    output_location=\"s3://my_bucket/my_location\",\n    aws_conn_id=\"aws_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Building Customized Airflow Image with Complex Dependencies\nDESCRIPTION: Builds a highly customized Airflow image with additional extras, PyPI dependencies, custom apt sources, and both dev and runtime apt dependencies. This example demonstrates the flexibility of the build process.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . \\\\\\n    --build-arg ADDITIONAL_AIRFLOW_EXTRAS=\"slack,odbc\" \\\\\\n    --build-arg ADDITIONAL_PYTHON_DEPS=\"azure-storage-blob oauth2client beautifulsoup4 dateparser rocketchat_API typeform\" \\\\\\n    --build-arg ADDITIONAL_DEV_APT_ENV=\"ACCEPT_EULA=Y\" \\\\\\n    --build-arg ADDITIONAL_RUNTIME_APT_ENV=\"ACCEPT_EULA=Y\" \\\\\\n    --build-arg ADDITIONAL_DEV_APT_COMMAND=\"curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - && curl https://packages.microsoft.com/config/debian/11/prod.list > /etc/apt/sources.list.d/mssql-release.list\" \\\\\\n    --build-arg ADDITIONAL_RUNTIME_APT_COMMAND=\"curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - && curl https://packages.microsoft.com/config/debian/11/prod.list > /etc/apt/sources.list.d/mssql-release.list\" \\\\\\n    --build-arg ADDITIONAL_DEV_APT_DEPS=\"msodbcsql17 unixodbc-dev g++\" \\\\\\n    --build-arg ADDITIONAL_RUNTIME_APT_DEPS=\"msodbcsql17 unixodbc git procps vim\" \\\\\\n    --tag my-image:my-tag\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Resource on GKE using GKECreateCustomResourceOperator in Python\nDESCRIPTION: This snippet illustrates how to use the `GKECreateCustomResourceOperator` to create a custom resource within a specified GKE cluster. This operator facilitates the management of Kubernetes Custom Resource Definitions (CRDs) via Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/kubernetes_engine/example_kubernetes_engine_resource.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_gke_create_resource]\n    :end-before: [END howto_operator_gke_create_resource]\n```\n\n----------------------------------------\n\nTITLE: Running Airflow Standalone Command\nDESCRIPTION: Command to initialize the database, create a user, and start all Airflow components in standalone mode.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/start.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairflow standalone\n```\n\n----------------------------------------\n\nTITLE: Using Default AWS Instance Profile for Airflow Connection (Bash)\nDESCRIPTION: This Bash code snippet sets an environment variable in the required format to let Airflow use the instance's default AWS credential lookup chain. By exporting 'AIRFLOW_CONN_AWS_DEFAULT' with a value of 'aws://', Airflow will rely on boto's default behavior, typically using instance profile credentials or the default user in '~/.boto'. No explicit credentials are provided here. This approach requires Airflow to be running in an environment where AWS credentials are discoverable by Boto3.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AWS_DEFAULT=aws://\n\n```\n\n----------------------------------------\n\nTITLE: Adding IDE Auto-completion Support for Custom Task Decorator\nDESCRIPTION: This code snippet demonstrates how to add IDE auto-completion support for a custom task decorator by defining its signature in a type stub file. It includes an example of declaring the signature and adding an overload for bare decorator usage.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/create-custom-decorator.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@overload\ndef foo(\n    *,\n    multiple_outputs: bool | None = None,\n    # Copy other arguments from FooOperator's constructor.\n    # Exclude those filled by FooDecoratedOperator.\n) -> Callable[[Callable], TaskDecoratorCollection[FT]]:\n    ...\n\n@overload\ndef foo(f: Callable[P, T]) -> TaskDecoratorCollection[FT]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Upserting BigQuery Table - Python\nDESCRIPTION: Example of using BigQueryUpsertTableOperator to update or create a table in BigQuery.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nupsert_table_task = BigQueryUpsertTableOperator(\n    task_id=\"upsert_table\",\n    dataset_id=DATASET_NAME,\n    table_resource={\n        \"tableReference\": {\"tableId\": TABLE_NAME},\n        \"schema\": {\n            \"fields\": [{\"name\": \"emp_name\", \"type\": \"STRING\"}]\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Short-Circuit with Trigger Rules Using ShortCircuitOperator in Airflow\nDESCRIPTION: Example of using the ShortCircuitOperator to conditionally skip downstream tasks while respecting trigger rules. The example creates a workflow where downstream tasks are skipped when the operator returns False, but task_7 still runs due to its ALL_DONE trigger rule configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id='example_short_circuit_operator',\n    start_date=pendulum.datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START howto_operator_short_circuit_trigger_rules]\n    cond = ShortCircuitOperator(\n        task_id='short_circuit',\n        python_callable=lambda: False,\n        ignore_downstream_trigger_rules=False,\n    )\n\n    task_1 = DummyOperator(task_id='task_1')\n    task_2 = DummyOperator(task_id='task_2')\n    task_3 = DummyOperator(task_id='task_3')\n    task_4 = DummyOperator(task_id='task_4')\n    task_5 = DummyOperator(task_id='task_5')\n    task_6 = DummyOperator(task_id='task_6')\n    task_7 = DummyOperator(task_id='task_7', trigger_rule=TriggerRule.ALL_DONE)\n\n    # This task will be skipped because the short-circuit will return False\n    cond >> task_1 >> [task_2, task_3]\n\n    # But this task and others downstream of it will be triggered, default behavior would skip them\n    task_3 >> task_4 >> task_5 >> task_6 >> task_7\n```\n\n----------------------------------------\n\nTITLE: Creating Apache Kafka Cluster with ManagedKafkaCreateClusterOperator in Python\nDESCRIPTION: This snippet demonstrates how to create an Apache Kafka cluster using the ManagedKafkaCreateClusterOperator. It specifies the cluster configuration including project ID, region, and cluster details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster = ManagedKafkaCreateClusterOperator(\n    task_id=\"create_cluster\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n    retry=Retry(\n        maximum=10.0,\n        total=300.0,\n        initial=3.0,\n        multiplier=2.0,\n    ),\n    timeout=300,\n)\n```\n\n----------------------------------------\n\nTITLE: Running a Databricks Notebook on a New Cluster in Python\nDESCRIPTION: This snippet demonstrates how to use the DatabricksNotebookOperator to run a Databricks notebook on a newly created cluster. It includes configuration for the new cluster and notebook parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/notebook.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../databricks/tests/system/databricks/example_databricks.py\n    :language: python\n    :start-after: [START howto_operator_databricks_notebook_new_cluster]\n    :end-before: [END howto_operator_databricks_notebook_new_cluster]\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete Airflow Plugin Example\nDESCRIPTION: Comprehensive example showing implementation of a custom Airflow plugin with Flask blueprints, FastAPI integration, custom views, and macro definitions.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/plugins.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.plugins_manager import AirflowPlugin\nfrom airflow.security import permissions\nfrom airflow.providers.fab.www.auth import has_access\n\nfrom fastapi import FastAPI\nfrom fastapi.middleware.trustedhost import TrustedHostMiddleware\nfrom flask import Blueprint\nfrom flask_appbuilder import expose, BaseView as AppBuilderBaseView\n\nfrom airflow.hooks.base import BaseHook\nfrom airflow.providers.amazon.aws.transfers.gcs_to_s3 import GCSToS3Operator\n\ndef plugin_macro():\n    pass\n\nbp = Blueprint(\n    \"test_plugin\",\n    __name__,\n    template_folder=\"templates\",\n    static_folder=\"static\",\n    static_url_path=\"/static/test_plugin\",\n)\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World from FastAPI plugin\"}\n\napp_with_metadata = {\"app\": app, \"url_prefix\": \"/some_prefix\", \"name\": \"Name of the App\"}\n\nmiddleware_with_metadata = {\n    \"middleware\": TrustedHostMiddleware,\n    \"args\": [],\n    \"kwargs\": {\"allowed_hosts\": [\"example.com\", \"*.example.com\"]},\n    \"name\": \"Name of the Middleware\",\n}\n\nclass TestAppBuilderBaseView(AppBuilderBaseView):\n    default_view = \"test\"\n\n    @expose(\"/\")\n    @has_access([\n        (permissions.ACTION_CAN_READ, permissions.RESOURCE_WEBSITE),\n    ])\n    def test(self):\n        return self.render_template(\"test_plugin/test.html\", content=\"Hello galaxy!\")\n\nclass TestAppBuilderBaseNoMenuView(AppBuilderBaseView):\n    default_view = \"test\"\n\n    @expose(\"/\")\n    @has_access([\n        (permissions.ACTION_CAN_READ, permissions.RESOURCE_WEBSITE),\n    ])\n    def test(self):\n        return self.render_template(\"test_plugin/test.html\", content=\"Hello galaxy!\")\n\nv_appbuilder_view = TestAppBuilderBaseView()\nv_appbuilder_package = {\n    \"name\": \"Test View\",\n    \"category\": \"Test Plugin\",\n    \"view\": v_appbuilder_view,\n}\n\nv_appbuilder_nomenu_view = TestAppBuilderBaseNoMenuView()\nv_appbuilder_nomenu_package = {\"view\": v_appbuilder_nomenu_view}\n\nappbuilder_mitem = {}\n```\n\n----------------------------------------\n\nTITLE: Creating an EKS Managed Nodegroup using EksCreateNodegroupOperator in Python\nDESCRIPTION: This Python snippet shows how to use the `EksCreateNodegroupOperator` to add a managed node group to an existing EKS cluster. It requires the `cluster_name`, `nodegroup_name`, `nodegroup_role_arn`, and subnet information. Requires an IAM role with `ec2.amazonaws.com` trust and specific policies (`AmazonEC2ContainerRegistryReadOnly`, `AmazonEKSWorkerNodePolicy`). Can run in deferrable mode (`deferrable=True`).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eks.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Example using EksCreateNodegroupOperator\n# Assumes necessary imports and DAG context, and an existing EKS cluster\n\ncreate_nodegroup = EksCreateNodegroupOperator(\n    task_id=\"create_eks_nodegroup\",\n    cluster_name=\"my-existing-eks-cluster\",\n    nodegroup_name=\"my-new-managed-nodegroup\",\n    nodegroup_role_arn=\"arn:aws:iam::123456789012:role/EksNodegroupRole\",\n    subnets=[\"subnet-xxxxxxxxxxxxxxxxx\", \"subnet-yyyyyyyyyyyyyyyyy\"],\n    deferrable=False, # Set to True for deferrable mode\n    # other nodegroup parameters like instance types, scaling config...\n)\n\n# [END howto_operator_eks_create_nodegroup]\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple DAG Bundles in Airflow Configuration\nDESCRIPTION: This snippet demonstrates how to configure multiple DAG Bundles in the Airflow configuration file (airflow.cfg). It shows the setup for both a Git-based DAG Bundle and a Local DAG Bundle.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/dag-bundles.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[dag_processor]\ndag_bundle_config_list = [\n    {\n      \"name\": \"my_git_repo\",\n      \"classpath\": \"airflow.dag_processing.bundles.git.GitDagBundle\",\n      \"kwargs\": {\"tracking_ref\": \"main\", \"git_conn_id\": \"my_git_conn\"}\n    },\n    {\n      \"name\": \"dags-folder\",\n      \"classpath\": \"airflow.dag_processing.bundles.local.LocalDagBundle\",\n      \"kwargs\": {}\n    }\n  ]\n```\n\n----------------------------------------\n\nTITLE: Generating DAGs with Environment Variables in Python\nDESCRIPTION: This snippet demonstrates how to use environment variables to configure DAG structure dynamically. It checks the 'DEPLOYMENT' variable to determine whether to create a production or development task.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/dynamic-dag-generation.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndeployment = os.environ.get(\"DEPLOYMENT\", \"PROD\")\nif deployment == \"PROD\":\n    task = Operator(param=\"prod-param\")\nelif deployment == \"DEV\":\n    task = Operator(param=\"dev-param\")\n```\n\n----------------------------------------\n\nTITLE: Using GCSObjectExistenceSensor in Python\nDESCRIPTION: Demonstrates how to use the GCSObjectExistenceSensor to wait for the existence of a file in Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsensor_task = GCSObjectExistenceSensor(\n    task_id=\"sensor_task\",\n    bucket=BUCKET_NAME,\n    object=f\"{OBJECT_1}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting an EKS Managed Nodegroup using EksDeleteNodegroupOperator in Python\nDESCRIPTION: This Python snippet demonstrates deleting an existing EKS managed node group using the `EksDeleteNodegroupOperator`. It requires the `cluster_name` and `nodegroup_name` parameters. Can run in deferrable mode (`deferrable=True`).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eks.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Example using EksDeleteNodegroupOperator\n# Assumes necessary imports and DAG context\n\ndelete_nodegroup = EksDeleteNodegroupOperator(\n    task_id=\"delete_eks_nodegroup\",\n    cluster_name=\"my-existing-eks-cluster\",\n    nodegroup_name=\"my-managed-nodegroup-to-delete\",\n    deferrable=False, # Set to True for deferrable mode\n)\n\n# [END howto_operator_eks_delete_nodegroup]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Explicit Content Detection Results from XCom in Airflow (Python)\nDESCRIPTION: This snippet extracts the results of explicit content detection with Airflow's XCom after using CloudVideoIntelligenceDetectVideoExplicitContentOperator. It fetches and prints the detection output via XCom.get_value. Inputs include the relevant task_id and context. This code is dependent on a successful prior execution of the explicit content detection operator and an accessible Airflow XCom environment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/video_intelligence.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndetect_explicit_content_result = XCom.get_value(\n    key=\"return_value\",\n    task_id=\"detect_explicit_content\",\n    dag_id=dag.dag_id,\n    execution_date=kwargs['execution_date'],\n)\nprint(f\"Explicit content detection result: {detect_explicit_content_result}\")\n```\n\n----------------------------------------\n\nTITLE: Using @task.bash Decorator for Bash Commands in Airflow\nDESCRIPTION: Demonstrates how to use the @task.bash decorator to execute Bash commands in Airflow. This method is recommended over the classic BashOperator for executing Bash commands.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@task.bash\ndef generate_random_number():\n    return \"echo $RANDOM\"\n\n\n@task\ndef print_number(random_number):\n    print(f\"The randomly generated number is: {random_number}\")\n\n\ngenerate_random_number() >> print_number(generate_random_number())\n```\n\n----------------------------------------\n\nTITLE: Configuring Scheduler Options in Apache Airflow\nDESCRIPTION: This snippet shows various configuration options for the Apache Airflow Scheduler. These settings control aspects such as DAG run creation, task scheduling, locking behavior, and performance metrics reporting.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/scheduler.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[scheduler]\nmax_dagruns_to_create_per_loop = <value>\nmax_dagruns_per_loop_to_schedule = <value>\nuse_row_level_locking = <boolean>\npool_metrics_interval = <seconds>\nrunning_metrics_interval = <seconds>\norphaned_tasks_check_interval = <seconds>\nmax_tis_per_query = <value>\nscheduler_idle_sleep_time = <seconds>\n```\n\n----------------------------------------\n\nTITLE: Example DAG for Embedding in Airflow Docker Image\nDESCRIPTION: This Python code snippet is an example DAG that can be embedded into the Airflow Docker image. It defines a simple DAG with two tasks using BashOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'email': ['airflow@example.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'example_dag',\n    default_args=default_args,\n    description='A simple tutorial DAG',\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example']\n)\n\nt1 = BashOperator(\n    task_id='print_date',\n    bash_command='date',\n    dag=dag,\n)\n\nt2 = BashOperator(\n    task_id='sleep',\n    depends_on_past=False,\n    bash_command='sleep 5',\n    retries=3,\n    dag=dag,\n)\n\nt1 >> t2\n```\n\n----------------------------------------\n\nTITLE: Implementing DatabricksWorkflowTaskGroup in Airflow DAG\nDESCRIPTION: Example DAG demonstrating how to use DatabricksWorkflowTaskGroup to run multiple Databricks notebooks in sequence with workflow parameters. This implementation creates or updates a Databricks job and executes notebooks with specific parameters while leveraging the cost savings of Databricks Workflow compute pricing.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/workflow.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    \"example_databricks_workflow\",\n    default_args={\n        \"databricks_conn_id\": \"databricks_default\",\n    },\n    start_date=datetime(2021, 1, 1),\n    schedule_interval=\"@daily\",\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    with DatabricksWorkflowTaskGroup(\n        group_id=\"test_workflow\",\n        databricks_conn_id=\"databricks_default\",\n        notebook_params={\"date\": \"{{ ds }}\"},\n    ) as workflow:\n        notebook_1 = DatabricksNotebookOperator(\n            task_id=\"notebook_1\",\n            databricks_conn_id=\"databricks_default\",\n            notebook_path=\"/Users/foo@bar.com/notebook_1\",\n            source=\"WORKSPACE\",\n        )\n\n        notebook_2 = DatabricksNotebookOperator(\n            task_id=\"notebook_2\",\n            databricks_conn_id=\"databricks_default\",\n            notebook_path=\"/Users/foo@bar.com/notebook_2\",\n            source=\"WORKSPACE\",\n        )\n\n        notebook_1 >> notebook_2\n```\n\n----------------------------------------\n\nTITLE: Retrieving PowerBI Workspace Info with MSGraphAsyncOperator (Python)\nDESCRIPTION: This snippet details how to use the MSGraphAsyncOperator to retrieve extended information about a specific PowerBI workspace. Make sure that the workspace identifier is correctly specified in the endpoint, and that the operator is configured with valid access credentials. The response yields detailed workspace metadata including properties and membership.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/msgraph.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npowerbi_workspace_info = MSGraphAsyncOperator(\n    task_id=\"get_powerbi_workspace_info\",\n    endpoint=\"groups/{workspace_id}\",\n    msgraph_conn_id=\"msgraph_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using OpenAIEmbeddingOperator in Python\nDESCRIPTION: Example demonstrating how to use the OpenAIEmbeddingOperator to create embeddings from input text. The operator requires an input_text parameter and connection ID for OpenAI API authentication.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openai/docs/operators/openai.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_openai_embedding]\n[END howto_operator_openai_embedding]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenLineage Provider with Optional Dependencies - Bash\nDESCRIPTION: This snippet demonstrates how to use pip to install the apache-airflow-providers-openlineage package including the optional 'common.compat' extra needed for certain cross-provider features. Requires an existing Airflow 2 installation and supports Python versions 3.9 through 3.12. The command should be run in a bash shell with necessary permissions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-openlineage[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Implementing Branch Task with TaskFlow API\nDESCRIPTION: Example demonstrating branching logic using the @task.branch decorator. The task returns different task IDs based on a random choice.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@task.branch\ndef random_choice():\n    import random\n\n    items = [\"branch_a\", \"branch_b\", \"branch_c\"]\n    selected = random.choice(items)\n    if selected == \"branch_a\":\n        return [\"branch_a_task_1\", \"branch_a_task_2\"]\n    elif selected == \"branch_b\":\n        return [\"branch_b_task_1\", \"branch_b_task_2\"]\n    elif selected == \"branch_c\":\n        return [\"branch_c_task_1\", \"branch_c_task_2\"]\n\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with Extras and Providers\nDESCRIPTION: This script installs Apache Airflow with specified extras (async, postgres, google) and applies constraints from a dynamically generated URL based on the Airflow version and Python version.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-pypi.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_VERSION=|version|\nPYTHON_VERSION=\"$(python -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")')\"\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\npip install \"apache-airflow[async,postgres,google]==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Serverless Pinecone Index using CreateServerlessIndexOperator in Python\nDESCRIPTION: This snippet illustrates how to use the `CreateServerlessIndexOperator` in an Airflow DAG to create a new serverless index in Pinecone. It requires index details such as `index_name` and `dimension`, along with serverless configuration parameters like `cloud` provider and `region`. Similar to other Pinecone operators, the `api_key` and `environment` can be passed as arguments or configured via an Airflow connection. The example code is included from a separate file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pinecone/docs/operators/pinecone.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../pinecone/tests/system/pinecone/example_create_serverless_index.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_create_serverless_index]\n    :end-before: [END howto_operator_create_serverless_index]\n```\n\n----------------------------------------\n\nTITLE: Implementing ExternalTaskSensor in Python for Apache Airflow\nDESCRIPTION: This snippet demonstrates how to use ExternalTaskSensor to make tasks in one DAG wait for a task in another DAG. It shows setting up the sensor with specific parameters like external DAG ID, task ID, and allowed states.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/external_task_sensor.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsensor = ExternalTaskSensor(\n    task_id='wait_for_task_2',\n    external_dag_id='example_external_task_marker_parent',\n    external_task_id='task_2',\n    allowed_states=['success'],\n    failed_states=['failed', 'skipped'],\n    mode='reschedule',\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing CronTriggerTimetable with custom data interval in Airflow DAG\nDESCRIPTION: Example of using CronTriggerTimetable with a cron expression and a specific data interval. This runs every Friday at 18:00 with a data interval spanning 4 days and 9 hours ending at the trigger time.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timetable.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\n\nfrom airflow.timetables.trigger import CronTriggerTimetable\n\n\n@dag(\n    # Runs every Friday at 18:00 to cover the work week (9:00 Monday to 18:00 Friday).\n    schedule=CronTriggerTimetable(\n        \"0 18 * * 5\",\n        timezone=\"UTC\",\n        interval=timedelta(days=4, hours=9),\n    ),\n    ...,\n)\ndef example_dag():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Implementing CronDataIntervalTimetable in Airflow DAG\nDESCRIPTION: Example of using a cron expression directly as the schedule parameter, which is automatically converted to a CronDataIntervalTimetable. This schedules a DAG to run at 01:00 every Wednesday.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timetable.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dag(schedule=\"0 1 * * 3\")  # At 01:00 on Wednesday.\ndef example_dag():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Implementing Templated Hello Operator\nDESCRIPTION: Implementation of an operator that uses Jinja templating for parameter substitution.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass HelloOperator(BaseOperator):\n    template_fields: Sequence[str] = (\"name\",)\n\n    def __init__(self, name: str, world: str, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.name = name\n        self.world = world\n\n    def execute(self, context):\n        message = f\"Hello {self.world} it's {self.name}!\"\n        print(message)\n        return message\n```\n\n----------------------------------------\n\nTITLE: Monitoring Dataproc Batch with Airflow Sensor in Python\nDESCRIPTION: This snippet illustrates using the `DataprocBatchSensor` to wait for a specific Dataproc Batch job to reach a terminal state (e.g., SUCCEEDED, FAILED). It polls the status of the batch identified by `batch_id`, `region`, and `project_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n# Code extracted from: /../../google/tests/system/google/cloud/dataproc/example_dataproc_batch.py\n# Between markers: [START how_to_cloud_dataproc_batch_async_sensor] and [END how_to_cloud_dataproc_batch_async_sensor]\n# \n# Example using DataprocBatchSensor(...)\n# ... (actual Python code would be here)\n\n```\n\n----------------------------------------\n\nTITLE: Starting Deferrable Java Dataflow Job with GCS JAR\nDESCRIPTION: Example demonstrating how to run a Java Dataflow pipeline in deferrable mode using a JAR from Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n_ = BeamRunJavaPipelineOperator(\n    task_id=\"start-java-job-deferrable\",\n    jar=f\"gs://{BUCKET_NAME}/{JAR_FILE}\",\n    pipeline_options=DATAFLOW_JAVA_OPTIONS,\n    job_class=\"org.apache.beam.examples.WordCount\",\n    location=location,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring SalesforceToGcsOperator in Python\nDESCRIPTION: Example of setting up a SalesforceToGcsOperator to execute a Salesforce query and transfer the results to Google Cloud Storage. The operator handles authentication and data transfer between Salesforce and GCS.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/salesforce_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsalesforce_to_gcs_example_task = SalesforceToGcsOperator(\n    task_id='salesforce_to_gcs_example',\n    salesforce_query=\"select Id, Name, Company, State from Lead\",\n    bucket_name=BUCKET,\n    object_name=f\"{PATH_TO_SAVE_OBJECTS}/LEAD.csv\",\n    salesforce_conn_id=\"salesforce_default\",\n    export_format=\"csv\",\n    coerce_to_timestamp=True,\n    record_time_added=True\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing PySpark Task with @task.pyspark Decorator in Apache Airflow\nDESCRIPTION: This snippet demonstrates how to use the @task.pyspark decorator to create a PySpark task in Apache Airflow. The function is injected with SparkSession and SparkContext objects, allowing direct interaction with Spark.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/decorators/pyspark.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@task.pyspark()\ndef test_pyspark_task(spark: SparkSession, sc: SparkContext):\n    instances = spark.createDataFrame(\n        [\n            (1, \"foo\"),\n            (2, \"bar\"),\n        ],\n        [\"id\", \"name\"],\n    )\n\n    assert instances.count() == 2\n    assert sc.version\n```\n\n----------------------------------------\n\nTITLE: Generating JWT Token for Airflow API Authentication\nDESCRIPTION: This bash snippet demonstrates how to make a POST request to the /auth/token endpoint to obtain a JWT token for Airflow API authentication. It requires the username and password to be provided in the request body.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/api.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nENDPOINT_URL=\"http://localhost:8080/\"\ncurl -X POST ${ENDPOINT_URL}/auth/token \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"username\": \"your-username\",\n    \"password\": \"your-password\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Configuring SNS Failure Notifications in Airflow DAGs - Python\nDESCRIPTION: This Python code demonstrates how to set up Amazon SNS notifications for Airflow using the send_sns_notification function. It creates notification callbacks for both DAG and task-level failures by passing AWS connection details, region, recipient SNS topic ARN, and message templates. Dependencies include Airflow (with the aws and standard providers), plus appropriate AWS credentials and IAM permissions. Key parameters such as aws_conn_id, region_name, message, and target_arn must be specified; on failure, Airflow will invoke these notifications with templated context values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/notifications/sns.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\\nfrom airflow import DAG\\nfrom airflow.providers.standard.operators.bash import BashOperator\\nfrom airflow.providers.amazon.aws.notifications.sns import send_sns_notification\\n\\ndag_failure_sns_notification = send_sns_notification(\\n    aws_conn_id=\"aws_default\",\\n    region_name=\"eu-west-2\",\\n    message=\"The DAG {{ dag.dag_id }} failed\",\\n    target_arn=\"arn:aws:sns:us-west-2:123456789098:TopicName\",\\n)\\ntask_failure_sns_notification = send_sns_notification(\\n    aws_conn_id=\"aws_default\",\\n    region_name=\"eu-west-2\",\\n    message=\"The task {{ ti.task_id }} failed\",\\n    target_arn=\"arn:aws:sns:us-west-2:123456789098:AnotherTopicName\",\\n)\\n\\nwith DAG(\\n    dag_id=\"mydag\",\\n    schedule=\"@once\",\\n    start_date=datetime(2023, 1, 1),\\n    on_failure_callback=[dag_failure_sns_notification],\\n    catchup=False,\\n):\\n    BashOperator(\\n        task_id=\"mytask\",\\n        on_failure_callback=[task_failure_sns_notification],\\n        bash_command=\"fail\",\\n    )\n```\n\n----------------------------------------\n\nTITLE: Passing Arguments to @task Decorated Function in Airflow\nDESCRIPTION: Illustrates how to pass extra arguments to a Python function decorated with @task in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@task(task_id=\"multiply_by_2\")\ndef multiply_by_2(number: int):\n    return number * 2\n\nmultiply_by_2(12)\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloud SQL Connection via Environment Variables in Python\nDESCRIPTION: This example shows how to configure a Google Cloud SQL connection using environment variables in Apache Airflow. It demonstrates the standard Airflow notation for defining connections through environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_cloudsql_query_connections_env]\n# [END howto_operator_cloudsql_query_connections_env]\n```\n\n----------------------------------------\n\nTITLE: Creating Amazon Bedrock Knowledge Base via Operator (Python)\nDESCRIPTION: Creates a Bedrock Knowledge Base using BedrockCreateKnowledgeBaseOperator. The snippet defines the S3 data source and vector store configuration, triggering knowledge base creation for storing and querying documents. Certain models are required to support embeddings.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncreate_knowledge_base_task = BedrockCreateKnowledgeBaseOperator(\n    task_id=\"create_kb\",\n    name=\"my-knowledge-base\",\n    description=\"Knowledge base for Bedrock experiments\",\n    data_source={\n        \"s3\": {\n            \"bucket\": \"my-data-bucket\",\n            \"prefix\": \"kb-data/\"\n        }\n    },\n    vector_store_configuration={\n        \"type\": \"FAISS\",\n        \"parameters\": {\"dimension\": 768}\n    },\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Reusing Sessions with RedshiftDataOperator in Python\nDESCRIPTION: Demonstrates how to maintain a session across multiple RedshiftDataOperator tasks by using session_keep_alive_seconds parameter and XCom for session ID sharing. Useful for operations involving temporary tables.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/redshift/redshift_data.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# First task keeps the session alive\nfirst_task = RedshiftDataOperator(\n    task_id='first_task',\n    sql='create temp table my_temp_table as select * from public.test',\n    database='dev',\n    db_user='awsuser',\n    cluster_identifier='redshift_cluster_1',\n    session_keep_alive_seconds=3600,\n    wait_for_completion=True,\n    aws_conn_id='aws_conn_id',\n)\n\n# Second task reuses the session\nsecond_task = RedshiftDataOperator(\n    task_id='second_task',\n    sql='select * from my_temp_table',\n    database='dev',\n    db_user='awsuser',\n    cluster_identifier='redshift_cluster_1',\n    session_id=\"{{ task_instance.xcom_pull('first_task')['session_id'] }}\",\n    aws_conn_id='aws_conn_id',\n)\n```\n\n----------------------------------------\n\nTITLE: Using BigQueryTablePartitionExistenceSensorAsync for Asynchronous Operation\nDESCRIPTION: Illustrates the use of the async version of the BigQueryTablePartitionExistenceSensor for asynchronous operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nbigquery_table_partition_sensor_async = BigQueryTablePartitionExistenceSensorAsync(\n    task_id=\"bq_table_partition_sensor_async\",\n    project_id=GCP_PROJECT_ID,\n    dataset_id=DATASET_NAME,\n    table_id=TABLE_NAME,\n    partition_id=PARTITION_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting an AWS Fargate Profile using EksDeleteFargateProfileOperator in Python\nDESCRIPTION: This Python snippet demonstrates how to delete an existing AWS Fargate profile from an EKS cluster using the `EksDeleteFargateProfileOperator`. It requires the `cluster_name` and `fargate_profile_name` of the profile to be deleted.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eks.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Example using EksDeleteFargateProfileOperator\n# Assumes necessary imports and DAG context\n\ndelete_fargate_profile = EksDeleteFargateProfileOperator(\n    task_id=\"delete_fargate_profile\",\n    cluster_name=\"my-existing-eks-cluster\",\n    fargate_profile_name=\"my-fargate-profile-to-delete\",\n)\n\n# [END howto_operator_eks_delete_fargate_profile]\n```\n\n----------------------------------------\n\nTITLE: Creating Dynamic DAG Tasks with Loops\nDESCRIPTION: Demonstrates how to dynamically generate DAG tasks using a Python for loop. Creates multiple tasks with sequential dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\"loop_example\", ...):\n    first = EmptyOperator(task_id=\"first\")\n    last = EmptyOperator(task_id=\"last\")\n\n    options = [\"branch_a\", \"branch_b\", \"branch_c\", \"branch_d\"]\n    for option in options:\n        t = EmptyOperator(task_id=option)\n        first >> t >> last\n```\n\n----------------------------------------\n\nTITLE: Simple Dynamic Task Mapping in Python for Airflow DAGs\nDESCRIPTION: A basic example demonstrating Dynamic Task Mapping where tasks are dynamically generated at runtime. This snippet creates mapped tasks that add one to each value in an array, then sums the results.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../src/airflow/example_dags/example_dynamic_task_mapping.py\n    :language: python\n```\n\n----------------------------------------\n\nTITLE: Stopping a Google Cloud Dataproc Cluster\nDESCRIPTION: This code uses the DataprocStopClusterOperator to stop a running Dataproc cluster. Stopping a cluster preserves its configuration but stops incurring compute charges.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nstop_cluster = DataprocStopClusterOperator(\n    task_id=\"stop_cluster\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster_name=CLUSTER_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring EMR Job Flow with Steps in Python\nDESCRIPTION: Defines a Python dictionary `JOB_FLOW_OVERRIDES` specifying the configuration for an EMR cluster creation. It includes cluster name, release label, instance types, application settings (like Spark), a list of steps to execute (e.g., a Spark job to calculate Pi), and configuration to automatically terminate the cluster after steps complete (`KeepJobFlowAliveWhenNoSteps`). This dictionary is intended to be used as a parameter for `EmrCreateJobFlowOperator`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nJOB_FLOW_OVERRIDES = {\n    \"Name\": \"PiCalc\",\n    \"ReleaseLabel\": \"emr-6.9.0\",\n    \"Applications\": [{\"Name\": \"Spark\"}],\n    \"Instances\": {\n        \"InstanceGroups\": [\n            {\n                \"Name\": \"Master nodes\",\n                \"Market\": \"ON_DEMAND\",\n                \"InstanceRole\": \"MASTER\",\n                \"InstanceType\": \"m5.xlarge\",\n                \"InstanceCount\": 1,\n            }\n        ],\n        \"KeepJobFlowAliveWhenNoSteps\": False,\n        \"TerminationProtected\": False,\n    },\n    \"Steps\": [\n        {\n            \"Name\": \"calculate_pi\",\n            \"ActionOnFailure\": \"TERMINATE_CLUSTER\",\n            \"HadoopJarStep\": {\n                \"Jar\": \"command-runner.jar\",\n                \"Args\": [\"/usr/lib/spark/bin/run-example\", \"SparkPi\", \"10\"],\n            },\n        }\n    ],\n    \"JobFlowRole\": EMR_JOB_FLOW_ROLE_NAME,\n    \"ServiceRole\": EMR_SERVICE_ROLE_NAME,\n}\n\n```\n\n----------------------------------------\n\nTITLE: Monitoring Amazon Bedrock Provision Model Throughput Job with Python\nDESCRIPTION: This snippet demonstrates the use of BedrockProvisionModelThroughputCompletedSensor to wait for an Amazon Bedrock provision model throughput job to reach a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_sensor_provision_throughput]\n# Example code not provided in the original text\n# [END howto_sensor_provision_throughput]\n```\n\n----------------------------------------\n\nTITLE: Creating Parameterized DAG in Python for Airflow\nDESCRIPTION: This code snippet shows how to create a parameterized DAG in Airflow using Python. It defines a DAG with a BashOperator that uses a parameter from the DAG run configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dag-run.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pendulum\n\nfrom airflow.sdk import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\ndag = DAG(\n    \"example_parameterized_dag\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n)\n\nparameterized_task = BashOperator(\n    task_id=\"parameterized_task\",\n    bash_command=\"echo value: {{ dag_run.conf['conf1'] }}\",\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Arguments for CloudTranslateSpeechOperator in Python\nDESCRIPTION: Defines the arguments required by the `CloudTranslateSpeechOperator`. `config` and `audio` arguments should be dictionaries or corresponding objects from `google.cloud.speech_v1.types`. Translation arguments like `target_language` are also specified.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate_speech.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud.speech_v1.types import RecognitionAudio, RecognitionConfig\n\n\nCONFIG = RecognitionConfig(encoding=RecognitionConfig.AudioEncoding.LINEAR16, language_code=\"en_US\")\nAUDIO = RecognitionAudio(uri=\"gs://cloud-samples-tests-bucket/Taskflow_API/translate_speech.wav\")\nTARGET_LANGUAGE = \"es\"\nFORMAT = \"text\"\nSOURCE_LANGUAGE = None\nMODEL = None\n```\n\n----------------------------------------\n\nTITLE: Uploading Local Files to GCS using LocalFilesystemToGCSOperator\nDESCRIPTION: Example demonstrating how to use LocalFilesystemToGCSOperator to upload files from local filesystem to Google Cloud Storage. The operator supports optional data compression and chunked uploads.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/local_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example DAG demonstrating the usage of the LocalFilesystemToGCSOperator.\"\"\"\n\nPATH_TO_LOCAL_FILE = <path-to-local-file>\nGCS_BUCKET = \"<google-cloud-storage-bucket>\"\nGCS_OBJECT_NAME = \"<object-name-in-gcs>\"\n\nLocalFilesystemToGCSOperator(\n    task_id=\"upload_file_to_gcs\",\n    src=PATH_TO_LOCAL_FILE,\n    dst=GCS_OBJECT_NAME,\n    bucket=GCS_BUCKET,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Slack Webhook Notifications in Airflow DAG\nDESCRIPTION: Example showing how to set up Slack Webhook notifications for both DAG-level and task-level failures. Uses the send_slack_webhook_notification function with a specified Slack webhook connection ID and customizable notification messages. The notifications are configured through on_failure_callback parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/notifications/slackwebhook_notifier_howto_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, timezone\nfrom airflow import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.slack.notifications.slack_webhook import send_slack_webhook_notification\n\ndag_failure_slack_webhook_notification = send_slack_webhook_notification(\n    slack_webhook_conn_id=\"slackwebhook\", text=\"The dag {{ dag.dag_id }} failed\"\n)\ntask_failure_slack_webhook_notification = send_slack_webhook_notification(\n    slack_webhook_conn_id=\"slackwebhook\",\n    text=\"The task {{ ti.task_id }} failed\",\n)\n\nwith DAG(\n    dag_id=\"mydag\",\n    schedule=\"@once\",\n    start_date=datetime(2023, 1, 1, tzinfo=timezone.utc),\n    on_failure_callback=[dag_failure_slack_webhook_notification],\n    catchup=False,\n):\n    BashOperator(\n        task_id=\"mytask\", on_failure_callback=[task_failure_slack_webhook_notification], bash_command=\"fail\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Listing Google Cloud Workflows using Airflow Operator in Python\nDESCRIPTION: Shows how to list workflows within a specific Google Cloud project and location using the `WorkflowsListWorkflowsOperator`. Requires `location_id` and optionally supports filtering.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/workflows.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/workflows/example_workflows.py\n      :language: python\n      :dedent: 4\n      :start-after: [START how_to_list_workflows]\n      :end-before: [END how_to_list_workflows]\n```\n\n----------------------------------------\n\nTITLE: Accessing Task Instance in Airflow Template (Python/Jinja)\nDESCRIPTION: Demonstrates how to access task instance attributes and methods using dot notation in Airflow templates. Examples include accessing owner, task_id, and hostname.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/templates-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{{ task.owner }}\n```\n\nLANGUAGE: jinja\nCODE:\n```\n{{ task.task_id }}\n```\n\nLANGUAGE: jinja\nCODE:\n```\n{{ ti.hostname }}\n```\n\n----------------------------------------\n\nTITLE: Creating Bigtable Instance with BigtableCreateInstanceOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the BigtableCreateInstanceOperator to create a Google Cloud Bigtable instance. It shows two variants: one with a specified project ID and another without, where the project ID is retrieved from the Google Cloud connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigtable.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_instance_task = BigtableCreateInstanceOperator(\n    project_id=GCP_PROJECT_ID,\n    instance_id=INSTANCE_ID,\n    main_cluster_id=CLUSTER_ID,\n    main_cluster_zone=CLUSTER_ZONE,\n    instance_display_name=INSTANCE_ID,\n    instance_type=Instance.TYPE_PRODUCTION,\n    instance_labels={\"env\": \"test\"},\n    task_id=\"create_instance\",\n)\n\n# The same operator can be created without project_id:\ncreate_instance_task_no_project_id = BigtableCreateInstanceOperator(\n    instance_id=INSTANCE_ID,\n    main_cluster_id=CLUSTER_ID,\n    main_cluster_zone=CLUSTER_ZONE,\n    instance_display_name=INSTANCE_ID,\n    instance_type=Instance.TYPE_PRODUCTION,\n    instance_labels={\"env\": \"test\"},\n    task_id=\"create_instance\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring User Self-Registration in Airflow (INI)\nDESCRIPTION: Details the settings required in `$AIRFLOW_HOME/webserver_config.py` to enable user self-registration via the login page. It involves enabling the feature (`AUTH_USER_REGISTRATION`), setting a default role for registered users, configuring Google reCAPTCHA keys for spam prevention, and providing SMTP mail server credentials (`MAIL_*` settings) for sending registration confirmation emails. Requires the `Flask-Mail` package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/webserver-authentication.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\nAUTH_USER_REGISTRATION = True\nAUTH_USER_REGISTRATION_ROLE = \"Desired Role For The Self Registered User\"\nRECAPTCHA_PRIVATE_KEY = 'private_key'\nRECAPTCHA_PUBLIC_KEY = 'public_key'\n\nMAIL_SERVER = 'smtp.gmail.com'\nMAIL_USE_TLS = True\nMAIL_USERNAME = 'yourappemail@gmail.com'\nMAIL_PASSWORD = 'passwordformail'\nMAIL_DEFAULT_SENDER = 'sender@gmail.com'\n```\n\n----------------------------------------\n\nTITLE: F-string Templating Examples in BashOperator\nDESCRIPTION: Demonstration of correct f-string usage with Jinja templating in BashOperator, showing proper escape sequence handling.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nt1 = BashOperator(\n    task_id=\"fstring_templating_correct\",\n    bash_command=f\"echo Data interval start: {{{{ ds }}}}\",\n    dag=dag,\n)\n\npython_var = \"echo Data interval start:\"\n\nt2 = BashOperator(\n    task_id=\"fstring_templating_simple\",\n    bash_command=f\"{python_var} {{{{ ds }}}}\",\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: CloudSQL Import Operator Initialization Example (Python)\nDESCRIPTION: Shows how to instantiate and configure CloudSQLImportInstanceOperator in an Airflow DAG for SQL import, both with and without an explicit project ID. This snippet requires the Airflow Google provider package and relies on a properly configured Google Cloud connection. Inputs are operator parameters including instance name, body, and optional project ID. The result is an Airflow operator ready to be included in a DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"cloud_sql_import = CloudSQLImportInstanceOperator(\\n    task_id=\\\"cloud_sql_import_task\\\",\\n    instance=\\\"my-instance\\\",\\n    body=import_body,\\n    project_id=\\\"my-gcp-project\\\",  # pass None to use project from connection\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Deleting an AWS CloudFormation Stack with Airflow Operator - Python\nDESCRIPTION: This Python code snippet illustrates how to use the CloudFormationDeleteStackOperator in Airflow to delete an AWS CloudFormation stack by specifying its name. Dependencies include the Airflow AWS provider and valid AWS credentials. The primary input is the stack name, and upon execution, the operator triggers the deletion of the specified stack and returns status or errors based on the process outcome.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/cloudformation.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndelete_stack = CloudFormationDeleteStackOperator(\n    task_id=\"delete_aws_stack\",\n    stack_name=STACK_NAME,\n    aws_conn_id=\"aws_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Resuming Amazon Redshift Cluster with RedshiftResumeClusterOperator\nDESCRIPTION: Example showing how to resume a paused Redshift cluster using RedshiftResumeClusterOperator. Can be run in deferrable mode.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/redshift/redshift_cluster.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresume_cluster = RedshiftResumeClusterOperator(\n    task_id='resume_cluster',\n    cluster_identifier=redshift_cluster_identifier,\n    aws_conn_id='aws_default',\n)\n```\n\n----------------------------------------\n\nTITLE: Inserting GCE Instance Group Manager in Python\nDESCRIPTION: Creates a ComputeEngineInsertInstanceGroupManagerOperator to insert a new instance group manager with a specified instance template.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_gce_insert_igm]\nfrom airflow.providers.google.cloud.operators.compute import ComputeEngineInsertInstanceGroupManagerOperator\n\ninsert_igm = ComputeEngineInsertInstanceGroupManagerOperator(\n    task_id=\"insert_igm\",\n    project_id=GCP_PROJECT_ID,\n    zone=GCE_ZONE,\n    body={\n        \"name\": IGM_NAME,\n        \"baseInstanceName\": \"instances\",\n        \"instanceTemplate\": f\"global/instanceTemplates/{TEMPLATE_NAME}\",\n        \"targetSize\": 1,\n    },\n)\n[END howto_operator_gce_insert_igm]\n```\n\n----------------------------------------\n\nTITLE: Starting Deferrable Dataflow Classic Template Job with Airflow\nDESCRIPTION: Demonstrates using the `DataflowTemplatedJobStartOperator` in deferrable mode (`deferrable=True`). This allows the operator to release the worker slot while waiting for the Dataflow job to start, improving Airflow scalability.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_template.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_start_template_job_deferrable]\n    :end-before: [END howto_operator_start_template_job_deferrable]\n```\n\n----------------------------------------\n\nTITLE: Creating Dataproc Cluster on Compute Engine with Python\nDESCRIPTION: This snippet demonstrates how to create a Dataproc cluster on Compute Engine using the DataprocCreateClusterOperator. It includes configuration for the cluster, specifying node types and counts.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nCLUSTER_CONFIG = {\n    \"master_config\": {\n        \"num_instances\": 1,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"secondary_worker_config\": {\n        \"num_instances\": 2,\n        \"machine_type_uri\": \"n1-standard-4\",\n        \"disk_config\": {\"boot_disk_type\": \"pd-standard\", \"boot_disk_size_gb\": 1024},\n    },\n    \"software_config\": {\n        \"image_version\": \"2.0-debian10\",\n    },\n}\n```\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster = DataprocCreateClusterOperator(\n    task_id=\"create_cluster\",\n    project_id=GCP_PROJECT_ID,\n    cluster_config=CLUSTER_CONFIG,\n    region=GCP_REGION,\n    cluster_name=CLUSTER_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Extending Airflow's API with FastAPI in Python\nDESCRIPTION: This snippet demonstrates how to create an Airflow plugin that extends the core RestAPI using fastapi_apps and fastapi_root_middlewares. It allows adding custom endpoints and middleware to the Airflow web application.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-view-plugin.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.plugins_manager import AirflowPlugin\n\nclass CustomPlugin(AirflowPlugin):\n    name = \"custom_plugin\"\n    fastapi_apps = [\n        {\n            \"name\": \"custom_app\",\n            \"app\": custom_fastapi_app,\n            \"url_prefix\": \"/custom\"\n        }\n    ]\n    fastapi_root_middlewares = [\n        {\n            \"name\": \"custom_middleware\",\n            \"middleware_factory\": custom_middleware_factory,\n            \"kwargs\": {\"some_param\": \"value\"}\n        }\n    ]\n```\n\n----------------------------------------\n\nTITLE: Creating a Google Cloud Workflow Execution using Airflow Operator in Python\nDESCRIPTION: Demonstrates triggering a new execution of a workflow using the `WorkflowsCreateExecutionOperator`. Requires `workflow_id`, `location_id`, and optionally `execution` arguments. Note that this operator is not idempotent.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/workflows.rst#2025-04-22_snippet_6\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/workflows/example_workflows.py\n      :language: python\n      :dedent: 4\n      :start-after: [START how_to_create_execution]\n      :end-before: [END how_to_create_execution]\n```\n\n----------------------------------------\n\nTITLE: Creating Postgres Tables using SQLExecuteQueryOperator in Python\nDESCRIPTION: Python code using SQLExecuteQueryOperator to create two tables in Postgres: 'employees' for final data and 'employees_temp' for staging data. This demonstrates how to execute SQL queries within Airflow tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/pipeline.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\ncreate_employees_table = SQLExecuteQueryOperator(\n    task_id=\"create_employees_table\",\n    conn_id=\"tutorial_pg_conn\",\n    sql=\"\"\"\n        CREATE TABLE IF NOT EXISTS employees (\n            \"Serial Number\" NUMERIC PRIMARY KEY,\n            \"Company Name\" TEXT,\n            \"Employee Markme\" TEXT,\n            \"Description\" TEXT,\n            \"Leave\" INTEGER\n        );\"\"\",\n)\n\ncreate_employees_temp_table = SQLExecuteQueryOperator(\n    task_id=\"create_employees_temp_table\",\n    conn_id=\"tutorial_pg_conn\",\n    sql=\"\"\"\n        DROP TABLE IF EXISTS employees_temp;\n        CREATE TABLE employees_temp (\n            \"Serial Number\" NUMERIC PRIMARY KEY,\n            \"Company Name\" TEXT,\n            \"Employee Markme\" TEXT,\n            \"Description\" TEXT,\n            \"Leave\" INTEGER\n        );\"\"\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Timezone Aware Datetime Objects in Airflow\nDESCRIPTION: Examples of creating timezone aware datetime objects using Airflow's timezone utility functions. Shows how to create current time and specific dates with proper timezone awareness.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timezone.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.utils import timezone\n\nnow = timezone.utcnow()\na_date = timezone.datetime(2017, 1, 1)\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Function with Airflow Operator\nDESCRIPTION: Example showing how to use LambdaCreateFunctionOperator to create an AWS Lambda function. The operator can be run in deferrable mode by passing deferrable=True parameter and requires aiobotocore module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/lambda.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nlambda_create_task = LambdaCreateFunctionOperator(\n    task_id=\"create_lambda_function\",\n    function_name=\"test_function\",\n    runtime=\"python3.9\",\n    role=\"arn:aws:iam::XXXXXXXXXXXX:role/test-lambda\",\n    handler=\"lambda_function.lambda_handler\",\n    code={\n        \"ZipFile\": b\"import json\\ndef lambda_handler(event, context):\\n    return {'statusCode': 200,'body': 'Hello'}\"\n    },\n    aws_conn_id=\"aws_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Copying Files Using Match Glob Pattern in GCSToGCSOperator\nDESCRIPTION: Example showing how to copy files matching a glob pattern from a source GCS bucket to a destination bucket. This method is preferred over using wildcards or delimiters which are deprecated.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gcs.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncopy_files_with_match_glob = GCSToGCSOperator(\n    task_id=\"gcs_to_gcs_match_glob\",\n    source_bucket=BUCKET_1_SRC,\n    source_object=\"data/\",\n    destination_bucket=BUCKET_1_DST,\n    destination_object=\"backup/\",\n    match_glob=\"**/*.csv\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Slack Notifications in Airflow DAG\nDESCRIPTION: Example demonstrating how to set up Slack notifications for both DAG-level success callbacks and task-level failure callbacks using send_slack_notification. The code shows configuration for sending messages to the #general channel with custom text templates that include DAG and task information.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/notifications/slack_notifier_howto_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.slack.notifications.slack import send_slack_notification\n\nwith DAG(\n    start_date=datetime(2023, 1, 1),\n    on_success_callback=[\n        send_slack_notification(\n            text=\"The DAG {{ dag.dag_id }} succeeded\",\n            channel=\"#general\",\n            username=\"Airflow\",\n        )\n    ],\n):\n    BashOperator(\n        task_id=\"mytask\",\n        on_failure_callback=[\n            send_slack_notification(\n                text=\"The task {{ ti.task_id }} failed\",\n                channel=\"#general\",\n                username=\"Airflow\",\n            )\n        ],\n        bash_command=\"fail\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Pulling Messages with PubSubPullSensor\nDESCRIPTION: Example demonstrating how to pull messages from a PubSub subscription using the PubSubPullSensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/pubsub.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npull_messages = PubSubPullSensor(\n    task_id=\"pull_messages\",\n    project_id=PROJECT_ID,\n    subscription=SUBSCRIPTION_ID,\n    max_messages=5,\n    return_immediately=False,\n    ack_messages=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Connection Form Fields in Python\nDESCRIPTION: Example showing how to add custom form fields to the Airflow connection UI by implementing the get_connection_form_widgets static method. The fields are stored in the Connection.extra field as JSON.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\ndef get_connection_form_widgets() -> dict[str, Any]:\n    \"\"\"Returns connection widgets to add to connection form\"\"\"\n    from flask_appbuilder.fieldwidgets import BS3TextFieldWidget\n    from flask_babel import lazy_gettext\n    from wtforms import StringField\n\n    return {\n        \"workspace\": StringField(lazy_gettext(\"Workspace\"), widget=BS3TextFieldWidget()),\n        \"project\": StringField(lazy_gettext(\"Project\"), widget=BS3TextFieldWidget()),\n    }\n```\n\n----------------------------------------\n\nTITLE: Parallel DynamoDB to S3 Transfer in Python using Airflow\nDESCRIPTION: This example shows how to parallelize the replication process using multiple DynamoDBToS3Operator tasks. It creates two tasks with different segment and total segment values for parallel execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/dynamodb_to_s3.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndynamodb_to_s3_task_0 = DynamoDBToS3Operator(\n    task_id=\"dynamodb_to_s3_0\",\n    dynamodb_table_name=\"{{ redshift_cluster_identifier }}_table\",\n    s3_bucket_name=\"{{ s3_bucket }}\",\n    file_size=1000,\n    dynamodb_scan_kwargs={\"TotalSegments\": 2, \"Segment\": 0},\n    aws_conn_id=\"aws_default\",\n)\n\ndynamodb_to_s3_task_1 = DynamoDBToS3Operator(\n    task_id=\"dynamodb_to_s3_1\",\n    dynamodb_table_name=\"{{ redshift_cluster_identifier }}_table\",\n    s3_bucket_name=\"{{ s3_bucket }}\",\n    file_size=1000,\n    dynamodb_scan_kwargs={\"TotalSegments\": 2, \"Segment\": 1},\n    aws_conn_id=\"aws_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing DeltaTriggerTimetable with custom data interval in Airflow DAG\nDESCRIPTION: Example of using DeltaTriggerTimetable with a specific trigger time and data interval. This configuration runs every Friday at 18:00 with a data interval of 4 days and 9 hours ending at the trigger time.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timetable.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import UTC, datetime, timedelta\n\nfrom dateutil.relativedelta import relativedelta, FR\n\nfrom airflow.timetables.trigger import DeltaTriggerTimetable\n\n\n@dag(\n    # Runs every Friday at 18:00 to cover the work week.\n    schedule=DeltaTriggerTimetable(\n        relativedelta(weekday=FR(), hour=18),\n        interval=timedelta(days=4, hours=9),\n    ),\n    start_date=datetime(2025, 1, 3, 18, tzinfo=UTC),\n    ...,\n)\ndef example_dag():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Updating Azure Service Bus Subscription in Python\nDESCRIPTION: This code demonstrates how to use the AzureServiceBusUpdateSubscriptionOperator to update an existing Azure Service Bus subscription with specific parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/asb.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_update_service_bus_subscription]\n# [END howto_operator_update_service_bus_subscription]\n```\n\n----------------------------------------\n\nTITLE: Monitoring Flink Application Stop Status - Python\nDESCRIPTION: Example code showing how to monitor the stop status of a Flink application using the KinesisAnalyticsV2StopApplicationCompletedSensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/kinesis_analytics.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwait_for_application_stop = KinesisAnalyticsV2StopApplicationCompletedSensor(\n    task_id=\"wait_for_application_stop\",\n    application_name=APPLICATION_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Callback for KubernetesPodOperator in Python\nDESCRIPTION: Example demonstrating how to define a custom callback class inheriting from `KubernetesPodOperatorCallback`. This specific callback, triggered `on_pod_creation`, creates a corresponding Kubernetes Service for the pod using the Kubernetes client API. The snippet also shows instantiating `KubernetesPodOperator` with the custom callback.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. code-block:: python\n\n    import kubernetes.client as k8s\n    import kubernetes_asyncio.client as async_k8s\n\n    from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n    from airflow.providers.cncf.kubernetes.callbacks import KubernetesPodOperatorCallback\n\n\n    class MyCallback(KubernetesPodOperatorCallback):\n        @staticmethod\n        def on_pod_creation(*, pod: k8s.V1Pod, client: k8s.CoreV1Api, mode: str, **kwargs) -> None:\n            client.create_namespaced_service(\n                namespace=pod.metadata.namespace,\n                body=k8s.V1Service(\n                    metadata=k8s.V1ObjectMeta(\n                        name=pod.metadata.name,\n                        labels=pod.metadata.labels,\n                        owner_references=[\n                            k8s.V1OwnerReference(\n                                api_version=pod.api_version,\n                                kind=pod.kind,\n                                name=pod.metadata.name,\n                                uid=pod.metadata.uid,\n                                controller=True,\n                                block_owner_deletion=True,\n                            )\n                        ],\n                    ),\n                    spec=k8s.V1ServiceSpec(\n                        selector=pod.metadata.labels,\n                        ports=[\n                            k8s.V1ServicePort(\n                                name=\"http\",\n                                port=80,\n                                target_port=80,\n                            )\n                        ],\n                    ),\n                ),\n            )\n\n\n    k = KubernetesPodOperator(\n        task_id=\"test_callback\",\n        image=\"alpine\",\n        cmds=[\"/bin/sh\"],\n        arguments=[\"-c\", \"echo hello world; echo Custom error > /dev/termination-log; exit 1;\"],\n        name=\"test-callback\",\n        callbacks=MyCallback,\n    )\n```\n\n----------------------------------------\n\nTITLE: Polling PowerBI Workspace Status with MSGraphSensor in Python\nDESCRIPTION: This code snippet demonstrates how to use the MSGraphSensor to poll the status of a PowerBI workspace. It sets up the sensor with specific parameters including the endpoint, method, and failure criteria.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/sensors/msgraph.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npowerbi_sensor = MSGraphSensor(\n    task_id=\"powerbi_sensor\",\n    endpoint=\"/v1.0/myorg/admin/workspaces/scanStatus\",\n    method=\"GET\",\n    token_path=\"token\",\n    failure_criteria=lambda response: response[\"status\"] == \"Failed\",\n    poke_interval=5,\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Environment Variables in Airflow Local Settings\nDESCRIPTION: Example implementation of get_airflow_context_vars function in airflow_local_settings.py that returns custom environment variables to be injected into the Airflow context. The function takes a context parameter and must return a dictionary with string key-value pairs. Note that certain keys like dag_id and task_id are reserved.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/export-more-env-vars.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_airflow_context_vars(context) -> dict[str, str]:\n    \"\"\"\n    :param context: The context for the task_instance of interest.\n    \"\"\"\n    # more env vars\n    return {\"airflow_cluster\": \"main\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring a Dataplex Zone in Python\nDESCRIPTION: Defines the configuration for a Google Cloud Dataplex zone. This configuration specifies the properties needed before creating a zone with the DataplexCreateZoneOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Configuration to create a simple Dataplex zone.\nZONE_ID = \"test-zone\"\nLAKE_ID = \"test-lake\"\n\nZONE = {\n    \"display_name\": \"Test Zone\",\n    \"type_\": \"RAW\",\n    \"resource_spec\": {\"location_type\": \"SINGLE_REGION\"},\n    \"discovery_spec\": {\"enabled\": True},\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Airflow Connection URIs Programmatically\nDESCRIPTION: Example showing how to programmatically generate a connection URI string using the Connection class and get_uri method.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> import json\n>>> from airflow.sdk import Connection\n>>> c = Connection(\n...     conn_id=\"some_conn\",\n...     conn_type=\"mysql\", \n...     description=\"connection description\",\n...     host=\"myhost.com\",\n...     login=\"myname\",\n...     password=\"mypassword\",\n...     extra=json.dumps(dict(this_param=\"some val\", that_param=\"other val*\")),\n... )\n>>> print(f\"AIRFLOW_CONN_{c.conn_id.upper()}='{c.get_uri()}'\")\nAIRFLOW_CONN_SOME_CONN='mysql://myname:mypassword@myhost.com?this_param=some+val&that_param=other+val%2A'\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS SSM Parameter Store with IAM Role\nDESCRIPTION: This configuration snippet demonstrates how to set up the AWS SSM Parameter Store backend with an IAM role for authentication. It includes prefixes for connections, variables, and config, along with the role ARN.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-ssm-parameter-store.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.amazon.aws.secrets.systems_manager.SystemsManagerParameterStoreBackend\nbackend_kwargs = {\n  \"connections_prefix\": \"airflow/connections\",\n  \"variables_prefix\": \"airflow/variables\",\n  \"config_prefix\": \"airflow/config\",\n  \"role_arn\": \"arn:aws:iam::123456789098:role/role-name\"\n}\n```\n\n----------------------------------------\n\nTITLE: Stopping an Amazon EMR Serverless Application with Airflow - Python\nDESCRIPTION: Illustrates the use of EmrServerlessStopApplicationOperator to gracefully stop an EMR Serverless Application via Airflow. Requires airflow.providers.amazon.aws, optionally aiobotocore for async mode. The primary parameter is application_id, referencing the running application; output is confirmation of application stop. Permission to call emr-serverless:StopApplication must be granted.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr_serverless.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.amazon.aws.operators.emr import EmrServerlessStopApplicationOperator\n\nstop_emr_serverless_app = EmrServerlessStopApplicationOperator(\n    task_id=\"stop_emr_serverless_application\",\n    application_id=\"{{ task_instance.xcom_pull(task_ids='create_emr_serverless_application') }}\",\n    deferrable=True,  # Optional\n    dag=dag,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Parameterizing DAGs with Environment Variables\nDESCRIPTION: Demonstrates how to make DAGs environment-aware by using environment variables for configuration. This approach allows the same DAG code to run in different environments (staging, production) by changing variables externally.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\ndest = os.environ.get(\"MY_DAG_DEST_PATH\", \"s3://default-target/path/\")\n```\n\n----------------------------------------\n\nTITLE: Repeated Mapping in Airflow with Python\nDESCRIPTION: Demonstrates how to chain mapped tasks together, where the output of one mapped task becomes the input to another mapped task. This creates a pipeline of transformations on a dataset.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(dag_id=\"repeated_mapping\", start_date=datetime(2022, 3, 4)) as dag:\n\n    @task\n    def add_one(x: int):\n        return x + 1\n\n    first = add_one.expand(x=[1, 2, 3])\n    second = add_one.expand(x=first)\n```\n\n----------------------------------------\n\nTITLE: Implementing Short-Circuit with Trigger Rules Using @task.short_circuit Decorator in Airflow\nDESCRIPTION: Example of using the @task.short_circuit decorator to conditionally skip downstream tasks while respecting trigger rules. The example shows how tasks are arranged in a workflow where some tasks are skipped when the short-circuit returns False, but task_7 still runs due to its ALL_DONE trigger rule.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(dag_id=\"example_short_circuit_decorator\", start_date=pendulum.datetime(2021, 1, 1), catchup=False) as dag:\n\n    # [START howto_operator_short_circuit_trigger_rules]\n    @task\n    def make_arg():\n        return 1\n\n    @task.short_circuit(ignore_downstream_trigger_rules=False)\n    def short_circuit(arg):\n        \"\"\"Return a value that will determine whether or not downstream tasks run\"\"\"\n        return False\n\n    @task\n    def task_1():\n        pass\n\n    @task\n    def task_2():\n        pass\n\n    @task\n    def task_3():\n        pass\n\n    @task\n    def task_4():\n        pass\n\n    @task\n    def task_5():\n        pass\n\n    @task\n    def task_6():\n        pass\n\n    @task(trigger_rule=TriggerRule.ALL_DONE)\n    def task_7():\n        pass\n\n    arg1 = make_arg()\n    short_circuit_return = short_circuit(arg1)\n\n    task_5_obj = task_5()\n    task_7_obj = task_7()\n\n    # This task will be skipped because the short-circuit will return False\n    short_circuit_return >> task_1() >> [task_2(), task_3()]\n\n    # But this task and others downstream of it will be triggered, default behavior would skip them\n    task_3() >> task_4() >> task_5_obj >> task_6() >> task_7_obj\n```\n\n----------------------------------------\n\nTITLE: Instantiating SQLTableCheckOperator for Table-Level Data Quality - Apache Airflow - Python\nDESCRIPTION: Presents a configuration for Airflow's SQLTableCheckOperator, enabling table-level validation by specifying a 'checks' dictionary mapping custom-named checks to boolean-resolving SQL statements. Needs Airflow, the operator provider, a configured database connection, and an appropriately structured checks dictionary. Example shows row count and column sum checks, with optional partitioning. Results in pass/fail based on user-defined conditions. Inputs: table name, checks dict; outputs: boolean pass/fail report for each check.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/operators.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nchecks = (\n    {\n        \"row_count_check\": {\n            \"check_statement\": \"COUNT(*) = 1000\",\n        },\n        \"column_sum_check\": {\n            \"check_statement\": \"col_a + col_b < col_c\",\n            \"partition_clause\": \"col_a IS NOT NULL\",\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Starting EC2 Instances with Airflow AWS Operator - Python\nDESCRIPTION: This code snippet illustrates how to use Airflow's EC2StartInstanceOperator to start one or more Amazon EC2 instances programmatically from an Airflow DAG. It requires Airflow version supporting the aws provider, proper configuration of AWS credentials, and the Boto3 library. Parameters include the instance IDs to start and the AWS connection, and upon execution, the specified EC2 instances are startedthe operator emits the instance IDs as output. Ensure the instances are in a stopped state and the user has the appropriate IAM permissions before invoking this operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/ec2.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# There is no code in the provided input itself, only includes and code references to other files.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating BigQuery View - Python\nDESCRIPTION: Example of creating a view on top of an existing BigQuery table.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncreate_view_task = BigQueryCreateTableOperator(\n    task_id=\"create_view\",\n    dataset_id=DATASET_NAME,\n    table_id=VIEW_NAME,\n    view={\"query\": f\"SELECT * FROM {DATASET_NAME}.{TABLE_NAME}\", \"useLegacySql\": False},\n)\n```\n\n----------------------------------------\n\nTITLE: Complete Example DAG Implementation\nDESCRIPTION: Full implementation of a DAG using the custom workday timetable, including the EmptyOperator task.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/timetable.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pendulum\n\nfrom airflow.sdk import DAG\nfrom airflow.example_dags.plugins.workday import AfterWorkdayTimetable\nfrom airflow.providers.standard.operators.empty import EmptyOperator\n\n\nwith DAG(\n    dag_id=\"example_workday_timetable\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=AfterWorkdayTimetable(),\n    tags=[\"example\", \"timetable\"],\n):\n    EmptyOperator(task_id=\"run_this\")\n```\n\n----------------------------------------\n\nTITLE: Implementing MultipleCronTriggerTimetable with data interval in Airflow DAG\nDESCRIPTION: Example of using MultipleCronTriggerTimetable with multiple cron expressions and a specific data interval. This schedules at 1:10 and 2:40 each day with a data interval of 1 hour.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timetable.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\n\nfrom airflow.timetables.trigger import MultipleCronTriggerTimetable\n\n\n@dag(\n    schedule=MultipleCronTriggerTimetable(\n        \"10 1 * * *\",\n        \"40 2 * * *\",\n        timezone=\"UTC\",\n        interval=timedelta(hours=1),\n    ),\n    ...,\n)\ndef example_dag():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Sending Block Kit Layouts with Airflow SlackWebhookOperator - Python\nDESCRIPTION: This snippet showcases using the SlackWebhookOperator in Airflow to send messages with rich layouts by leveraging Slack Block Kit formatting. Dependencies include Airflow with the Slack provider and a properly configured Slack webhook. Inputs consist of a JSON-formatted list defining Block Kit blocks, allowing for richly formatted Slack messages. The output is a Slack channel message rendered with the specified blocks. Key parameters are the webhook connection ID and the blocks JSON, with potential constraints around Slack block payload sizes and composition.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/operators/slack_webhook.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nslack_webhook_block = SlackWebhookOperator(\n    task_id=\"send_slack_block_message\",\n    http_conn_id=\"slack_webhook\",\n    message=None,\n    blocks=[\n        {\n            \"type\": \"section\",\n            \"text\": {\n                \"type\": \"mrkdwn\",\n                \"text\": \"*Hello*, this is a Block Kit _message_ from Airflow!\"\n            }\n        },\n        {\n            \"type\": \"divider\"\n        },\n        {\n            \"type\": \"section\",\n            \"text\": {\n                \"type\": \"plain_text\",\n                \"text\": \"This section is plain text.\"\n            }\n        }\n    ],\n)\n\n```\n\n----------------------------------------\n\nTITLE: Using Custom Hello Operator in DAG\nDESCRIPTION: Example showing how to import and use the custom HelloOperator in an Airflow DAG file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom custom_operator.hello_operator import HelloOperator\n\nwith dag:\n    hello_task = HelloOperator(task_id=\"sample-task\", name=\"foo_bar\")\n```\n\n----------------------------------------\n\nTITLE: Inserting GCE Instance Group Manager Template Without Project ID in Python\nDESCRIPTION: Creates a ComputeEngineInsertInstanceTemplateOperator without specifying a project ID, which will be automatically retrieved from the Google Cloud connection used.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ninsert_template_no_project_id = ComputeEngineInsertInstanceTemplateOperator(\n    task_id=\"insert_template\",\n    body={\n        \"name\": TEMPLATE_NAME,\n        \"properties\": {\n            \"machineType\": MACHINE_TYPE,\n            \"disks\": [{\n                \"boot\": True,\n                \"type\": \"PERSISTENT\",\n                \"autoDelete\": True,\n                \"licenses\": [\"projects/debian-cloud/global/licenses/debian-11-bullseye\"],\n                \"initializeParams\": {\n                    \"sourceImage\": DEBIAN_IMAGE,\n                    \"diskSizeGb\": 10,\n                },\n            }],\n            \"networkInterfaces\": [{\"network\": \"global/networks/default\"}],\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Entire GCS Buckets\nDESCRIPTION: Demonstrates how to synchronize files between two GCS buckets with overwrite and delete behavior. This operation ensures all files in BUCKET_1_SRC are copied to BUCKET_1_DST, overwriting existing files and deleting files in the destination that don't exist in the source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gcs.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsync_full_gcs = GCSToGCSOperator(\n    task_id=\"sync_full_gcs\",\n    source_bucket=BUCKET_1_SRC,\n    destination_bucket=BUCKET_1_DST,\n    source_object=\"*\",\n    allow_overwrite=True,\n    delete_extra_files=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing DAG with Custom Timetable\nDESCRIPTION: Example of how to use a custom timetable in a DAG definition. Shows DAG configuration with the AfterWorkdayTimetable.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/timetable.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pendulum\n\nfrom airflow.sdk import DAG\nfrom airflow.example_dags.plugins.workday import AfterWorkdayTimetable\n\n\nwith DAG(\n    dag_id=\"example_after_workday_timetable_dag\",\n    start_date=pendulum.datetime(2021, 3, 10, tz=\"UTC\"),\n    schedule=AfterWorkdayTimetable(),\n    tags=[\"example\", \"timetable\"],\n):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Retrieving Sharepoint Site Pages with MSGraphAsyncOperator (Python)\nDESCRIPTION: This example shows how to use the MSGraphAsyncOperator to get pages belonging to a Sharepoint site by targeting the Microsoft Graph API's site pages endpoint. It presumes existing configuration of MSGraph Operator in Airflow, proper authentication setup, and the existence of a Sharepoint site. The endpoint parameter should be set to include '/pages' for the target site, and the operator returns a collection of page objects as output.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/msgraph.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsite_pages = MSGraphAsyncOperator(\n    task_id=\"get_sharepoint_site_pages\",\n    endpoint=\"sites/root/pages\",\n    msgraph_conn_id=\"msgraph_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using DataflowJobMessagesSensor in Deferrable Mode\nDESCRIPTION: Example of using the DataflowJobMessagesSensor in deferrable mode. This allows the sensor to release its worker slot while waiting for the specified job messages.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Code snippet not provided in the input text\n```\n\n----------------------------------------\n\nTITLE: Using @task.docker Decorator in Apache Airflow\nDESCRIPTION: This example demonstrates how to use the @task.docker decorator to execute a Python function within a Docker container as part of an Airflow task. The function takes a value, transforms it, and returns the result.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/docker/docs/decorators/docker.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@task.docker(\n    image=\"python:3.8-slim-buster\",\n    network_mode=\"bridge\",\n    mount_tmp_dir=False,\n)\ndef transform_docker(x: int):\n    import random\n\n    y = x * random.random()\n    print(f\"Transformed {x} to {y}\")\n    return y\n```\n\n----------------------------------------\n\nTITLE: Triggering DAG Run in MWAA Environment\nDESCRIPTION: Example showing how to trigger a DAG run in an Amazon MWAA environment using MwaaTriggerDagRunOperator. The task triggers the 'hello_world' DAG in 'MyAirflowEnvironment' and waits for completion.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/mwaa.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrigger_dag_run = MwaaTriggerDagRunOperator(\n    task_id=\"trigger_dag_run\",\n    environment_name=\"MyAirflowEnvironment\",\n    dag_name=\"hello_world\",\n    wait_for_completion=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL Parameters for PostgreSQL Connection in Airflow (JSON)\nDESCRIPTION: This JSON snippet demonstrates how to configure SSL settings within the 'Extra' field of an Airflow PostgreSQL connection. It specifies the SSL mode, client certificate, server CA certificate, and client key paths. These parameters correspond to libpq connection string options.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/connections/postgres.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"sslmode\": \"verify-ca\",\n   \"sslcert\": \"/tmp/client-cert.pem\",\n   \"sslca\": \"/tmp/server-ca.pem\",\n   \"sslkey\": \"/tmp/client-key.pem\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Logger Name for a DAG Task in Airflow\nDESCRIPTION: This example shows how to set a custom logger name for a specific task in a DAG file and how to configure that custom logger in the logging configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/advanced-logging-configuration.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# In your Dag file\nSQLExecuteQueryOperator(..., logger_name=\"sql.big_query\")\n\n# In your custom `log_config.py`\nLOGGING_CONFIG = deep_update(\n    deepcopy(DEFAULT_LOGGING_CONFIG),\n    {\n        \"loggers\": {\n            \"airflow.task.operators.sql.big_query\": {\n                \"handlers\": [\"task\"],\n                \"level\": \"WARNING\",\n                \"propagate\": True,\n            },\n        }\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Airflow DB and Creating Admin User (Bash)\nDESCRIPTION: This command initializes the Airflow metadata database if it hasn't been initialized yet and creates a new admin user. It should be executed on the host(s) running the Airflow Scheduler and Webserver before starting those services. Replace the placeholder values for first name, last name, and email with actual user details. This step is crucial for the initial setup and login to the Airflow UI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/executors/general.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairflow users create --username admin --password admin --firstname <your first name> --lastname <your last name> --email <your email> --role Admin\n```\n\n----------------------------------------\n\nTITLE: Running a Pipeline Job using Vertex AI Pipeline Job Operator - Python\nDESCRIPTION: Provides a snippet to submit a Vertex AI pipeline job using RunPipelineJobOperator in Airflow. Requires pipeline job configuration, DAG context, and relevant GCP credentials. Output includes pipeline job ID available via XCom for further management operations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nrun_pipeline_job_task = RunPipelineJobOperator(\n    task_id=\"run_pipeline_job_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    display_name=\"vertex-ai-pipeline-job\",\n    template_path=PIPELINE_TEMPLATE_PATH,\n    parameter_values=PIPELINE_PARAMETERS,\n    pipeline_root=PIPELINE_ROOT,\n)\n```\n\n----------------------------------------\n\nTITLE: Using HttpSensor in Deferrable Mode in Python\nDESCRIPTION: This snippet shows how to use `HttpSensor` in deferrable mode for efficient polling. It targets the `/get` endpoint of `httpbin` using the `http_default` connection and waits for the response text to contain 'httpbin'. Deferrable mode avoids occupying a worker slot during the wait.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntask_http_sensor_check_deferrable = HttpSensor(\n    task_id=\"http_sensor_check_deferrable\",\n    http_conn_id=\"http_default\",\n    endpoint=\"get\",\n    request_params={},\n    response_check=lambda response: \"httpbin\" in response.text,\n    poke_interval=5,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow Database Backend in INI\nDESCRIPTION: This snippet shows how to configure the database backend for Airflow using an INI configuration file. It sets the SQL Alchemy connection string to use an external database instead of the default SQLite.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/production-deployment.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[database]\nsql_alchemy_conn = my_conn_string\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from MSSQL to GCS using MSSQLToGCSOperator\nDESCRIPTION: Example code demonstrating how to use the MSSQLToGCSOperator to upload data from a Microsoft SQL Server database to Google Cloud Storage. The operator handles the data extraction and upload process.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/mssql_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/gcs/example_mssql_to_gcs.py\n    :language: python\n    :start-after: [START howto_operator_mssql_to_gcs]\n    :end-before: [END howto_operator_mssql_to_gcs]\n```\n\n----------------------------------------\n\nTITLE: Getting an Entry Group with CloudDataCatalogGetEntryGroupOperator in Python\nDESCRIPTION: Uses CloudDataCatalogGetEntryGroupOperator to get an entry group from Google Cloud Data Catalog. The result is saved to XCom for use by other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nget_entry_group = CloudDataCatalogGetEntryGroupOperator(\n    task_id=\"get_entry_group\",\n    location=LOCATION,\n    entry_group=ENTRY_GROUP_ID,\n    project_id=PROJECT_ID,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nget_entry_group_result = get_entry_group.output\n```\n\n----------------------------------------\n\nTITLE: Deleting GCE Instance Group Manager Without Project ID in Python\nDESCRIPTION: Creates a ComputeEngineDeleteInstanceGroupManagerOperator without specifying a project ID, which will be automatically retrieved from the Google Cloud connection used.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndelete_igm = ComputeEngineDeleteInstanceGroupManagerOperator(\n    task_id=\"delete_igm\",\n    resource_id=IGM_NAME,\n    zone=GCE_ZONE,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting Amazon S3 Bucket with S3DeleteBucketOperator\nDESCRIPTION: Shows how to delete an existing S3 bucket using the S3DeleteBucketOperator. The operator requires bucket name and AWS connection ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/s3.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndelete_bucket = S3DeleteBucketOperator(\n    task_id=\"delete_bucket\",\n    bucket_name=BUCKET_NAME,\n    force_delete=True,\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Uploading a Model using Vertex AI Model Service Operator - Python\nDESCRIPTION: Demonstrates uploading a model to Google Vertex AI using the UploadModelOperator. Dependencies are Airflow's GCP provider; inputs include the model GCS path and metadata. On success, the model ID is returned in XCom for chaining into deployment or registration tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nupload_model_task = UploadModelOperator(\n    task_id=\"upload_model_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    display_name=\"your_model_display_name\",\n    model=VERTEX_MODEL_CONFIG,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating AutoML Forecasting Training Job in Vertex AI\nDESCRIPTION: Example demonstrating how to create an AutoML forecasting training job using the CreateAutoMLForecastingTrainingJobOperator. Requires a pre-created TimeSeries dataset.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nCreateAutoMLForecastingTrainingJobOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    display_name=TRAINING_JOB_NAME,\n    dataset_id=FORECASTING_DATASET,\n    target_column=TARGET_COLUMN,\n    time_column=TIME_COLUMN,\n    time_series_identifier_column=TIME_SERIES_IDENTIFIER_COLUMN,\n    available_at_forecast_columns=[TIME_COLUMN],\n    unavailable_at_forecast_columns=[TARGET_COLUMN],\n    time_series_attribute_columns=[TIME_SERIES_ATTRIBUTE_COLUMN],\n    forecast_horizon=30,\n    context_window=30,\n    data_granularity_unit=\"day\",\n    data_granularity_count=1,\n    optimization_objective=\"minimize-rmse\",\n    sync=True,\n    task_id=\"training_job\",)\n```\n\n----------------------------------------\n\nTITLE: Creating and Starting EC2 Instances with Airflow AWS Operator - Python\nDESCRIPTION: This code snippet shows how to deploy new Amazon EC2 instances and start them using the EC2CreateInstanceOperator from Airflow. It expects details such as AMI ID, instance type, count, key name, and subnet ID as parameters. Dependencies include Airflow's AWS provider and Boto3. The operator outputs information about the created instance(s), and users must have EC2 create-instance permissions and valid AMI, key, and VPC/subnet details configured.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/ec2.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# There is no code in the provided input itself, only includes and code references to other files.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Posting Text Messages Using SlackAPIPostOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the SlackAPIPostOperator in Python to post a simple text message to a Slack channel from an Apache Airflow DAG. It requires the Airflow Slack provider and assumes a valid Slack connection is configured. The primary parameter is the message text and the target channel; the operator manages authentication and posting. Inputs include the message string and channel identifier; output is a posted message on Slack. The operator is constrained by Slack API limitations and channel permissions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/operators/slack_api.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.providers.slack.operators.slack import SlackAPIPostOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2021, 1, 1),\n}\n\ndag = DAG(\n    'example_slack_post_text',\n    default_args=default_args,\n    schedule_interval=None,\n    catchup=False,\n)\n\npost_text = SlackAPIPostOperator(\n    task_id='post_text',\n    channel='#general',\n    text='Hello from Airflow!',\n    dag=dag,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Creating Dataproc Metastore Backup using Airflow Operator in Python\nDESCRIPTION: Demonstrates using the `DataprocMetastoreCreateBackupOperator` in an Airflow DAG to create a backup of a specific Dataproc Metastore service. Requires the `backup` configuration dictionary, `backup_id`, `service_id`, `location`, and `project_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncreate_backup_service = DataprocMetastoreCreateBackupOperator(\n    task_id=\"create_backup_service\",\n    backup=BACKUP,\n    backup_id=BACKUP_ID,\n    service_id=SERVICE_ID,\n    location=REGION,\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting Firestore Database to GCS with CloudFirestoreExportDatabaseOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the CloudFirestoreExportDatabaseOperator to export all or a subset of documents from Google Cloud Firestore to Google Cloud Storage. The operator supports Jinja templating for dynamic parameter configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/firebase/firestore.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nexport_database_to_gcs = CloudFirestoreExportDatabaseOperator(\n    task_id=\"export_database_to_gcs\",\n    database_id=FIRESTORE_DATABASE_ID,\n    body={\"outputUriPrefix\": f\"gs://{BUCKET_NAME}/\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Restoring Dataproc Metastore Service using Airflow Operator in Python\nDESCRIPTION: Demonstrates how to use the `DataprocMetastoreRestoreServiceOperator` in an Airflow DAG. This operator restores a Dataproc Metastore service to the state captured in a specified backup. Requires `service_id`, `location`, `project_id`, and `backup_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrestore_service = DataprocMetastoreRestoreServiceOperator(\n    task_id=\"restore_service\",\n    service_id=SERVICE_ID,\n    location=REGION,\n    project_id=PROJECT_ID,\n    backup_id=BACKUP_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Copying Data Between BigQuery Tables Using BigQueryToBigQueryOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the BigQueryToBigQueryOperator to copy data from one or more BigQuery tables to another. It configures source tables, destination table, and defines write and create disposition parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/bigquery_to_bigquery.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncopy_selected_data = BigQueryToBigQueryOperator(\n    task_id=\"copy_selected_data\",\n    source_project_dataset_tables=[f\"{DATASET_NAME}.{LOCATION_SOURCE_TABLE}\"],\n    destination_project_dataset_table=f\"{DATASET_NAME}.{COPY_DATASET_TABLE}\",\n    write_disposition=\"WRITE_TRUNCATE\",\n    create_disposition=\"CREATE_IF_NEEDED\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Security Manager for Airflow Keycloak Integration in Python\nDESCRIPTION: This snippet defines a CustomSecurityManager class that extends AirflowSecurityManager. It implements the get_oauth_user_info method to extract user information from the Keycloak JWT token, including roles and user details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/webserver-authentication.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass CustomSecurityManager(AirflowSecurityManager):\n    def get_oauth_user_info(self, provider, response):\n        if provider == \"keycloak\":\n            token = response[\"access_token\"]\n            me = jwt.decode(token, public_key, algorithms=[\"HS256\", \"RS256\"])\n\n            # Extract roles from resource access\n            realm_access = me.get(\"realm_access\", {})\n            groups = realm_access.get(\"roles\", [])\n\n            log.info(\"groups: {0}\".format(groups))\n\n            if not groups:\n                groups = [\"Viewer\"]\n\n            userinfo = {\n                \"username\": me.get(\"preferred_username\"),\n                \"email\": me.get(\"email\"),\n                \"first_name\": me.get(\"given_name\"),\n                \"last_name\": me.get(\"family_name\"),\n                \"role_keys\": groups,\n            }\n\n            log.info(\"user info: {0}\".format(userinfo))\n\n            return userinfo\n        else:\n            return {}\n\n\n# Make sure to replace this with your own implementation of AirflowSecurityManager class\nSECURITY_MANAGER_CLASS = CustomSecurityManager\n```\n\n----------------------------------------\n\nTITLE: Running a Dataplex Data Quality Scan in Deferrable Mode - Python\nDESCRIPTION: Executes a Data Quality scan using DataplexRunDataQualityScanOperator in deferrable mode, allowing asynchronous and resource-efficient task execution in Airflow. Requires the operator to be declared with deferrable=True; outputs job initiation response.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n\"run_dq_scan_def = DataplexRunDataQualityScanOperator(\\n    task_id=\\\"run_dq_scan_def\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    data_quality_scan_id=DATA_QUALITY_SCAN_ID,\\n    deferrable=True,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Creating SageMaker Training Job\nDESCRIPTION: Example showing how to use SageMakerTrainingOperator to create a model training job in Amazon SageMaker.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntraining_task = SageMakerTrainingOperator(\n    task_id='training_task',\n    config={\n        \"AlgorithmSpecification\": {},\n        \"HyperParameters\": {},\n        \"InputDataConfig\": [],\n        \"OutputDataConfig\": {},\n        \"ResourceConfig\": {},\n        \"RoleArn\": \"test_role\",\n        \"StoppingCondition\": {},\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing PythonSensor using classic operator in Python\nDESCRIPTION: This snippet shows how to use the classic PythonSensor operator to create a sensor that waits for a specific condition. It defines a custom function to check if files exist and creates a PythonSensor task that uses this function.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/python.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef check_file():\n    return os.path.exists(\"/tmp/stop.txt\") and os.path.exists(\"/tmp/go.txt\")\n\nsensor = PythonSensor(\n    task_id=\"python_sensor\",\n    python_callable=check_file,\n    mode=\"reschedule\",\n    poke_interval=5,\n    timeout=60,\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Connection via URI Environment Variable\nDESCRIPTION: Example of setting an Airflow connection using URI format in an environment variable. Shows the structure for connection string with parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_MY_PROD_DATABASE='my-conn-type://login:password@host:port/schema?param1=val1&param2=val2'\n```\n\n----------------------------------------\n\nTITLE: Configuring Task-level Params in Python\nDESCRIPTION: Shows how to set parameters at the task level using PythonOperator. Task-level params override DAG-level params, and user-supplied params override task-level params.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/params.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef print_my_int_param(params):\n  print(params.my_int_param)\n\nPythonOperator(\n    task_id=\"print_my_int_param\",\n    params={\"my_int_param\": 10},\n    python_callable=print_my_int_param,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Airflow Database Migration in Bash\nDESCRIPTION: This bash command runs the Airflow database migration to create all necessary tables in the configured database. It's safe to run multiple times as it keeps track of applied migrations.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/production-deployment.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairflow db migrate\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from MongoDB to Amazon S3 using MongoToS3Operator in Python\nDESCRIPTION: This code snippet demonstrates how to use the MongoToS3Operator to transfer data from a MongoDB collection to an Amazon S3 file. It includes setting up the task with necessary parameters such as S3 bucket, MongoDB connection, and query.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/mongo_to_s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nMongoToS3Operator(\n    task_id=\"mongo_to_s3_task\",\n    mongo_conn_id=\"mongo_default\",\n    aws_conn_id=\"aws_default\",\n    mongo_collection=\"test_collection\",\n    mongo_query={\n        \"$and\": [\n            {\"state\": \"{{ states.kwargs.get('state') }}\"},\n            {\"date\": {\"$gte\": \"{{ data_interval_start | ds }}\"}},\n            {\"date\": {\"$lt\": \"{{ data_interval_end | ds }}\"}},\n        ]\n    },\n    s3_bucket=\"test-bucket\",\n    s3_key=\"mongo_export_{{ ds_nodash }}.json\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Operator Extra Links in Python\nDESCRIPTION: This snippet demonstrates how to create a custom operator with an extra link using Airflow's plugin system. It defines a GoogleLink class, a custom operator MyFirstOperator, and an AirflowExtraLinkPlugin to register the extra link.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/define-extra-link.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import BaseOperator\nfrom airflow.sdk import BaseOperatorLink\nfrom airflow.models.taskinstancekey import TaskInstanceKey\nfrom airflow.plugins_manager import AirflowPlugin\n\n\nclass GoogleLink(BaseOperatorLink):\n    name = \"Google\"\n\n    def get_link(self, operator: BaseOperator, *, ti_key: TaskInstanceKey):\n        return \"https://www.google.com\"\n\n\nclass MyFirstOperator(BaseOperator):\n    operator_extra_links = (GoogleLink(),)\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def execute(self, context):\n        self.log.info(\"Hello World!\")\n\n\n# Defining the plugin class\nclass AirflowExtraLinkPlugin(AirflowPlugin):\n    name = \"extra_link_plugin\"\n    operator_extra_links = [\n        GoogleLink(),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Creating AlloyDB Cluster with Airflow Operator\nDESCRIPTION: Uses AlloyDBCreateClusterOperator to create an AlloyDB cluster (primary and secondary) in Google Cloud.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/alloy_db.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster = AlloyDBCreateClusterOperator(\n    task_id=\"create_cluster\",\n    project_id=GCP_PROJECT_ID,\n    cluster_id=CLUSTER_ID,\n    instance=ALLOYDB_CLUSTER,\n    network=NETWORK,\n    region=GCP_ALLOYDB_REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from BigQuery to PostgreSQL using BigQueryToPostgresOperator\nDESCRIPTION: This snippet demonstrates how to use the BigQueryToPostgresOperator to copy data from a BigQuery table to a PostgreSQL table. It includes options for selecting specific fields and appending or replacing data in the destination table.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/bigquery_to_postgres.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nBigQueryToPostgresOperator(\n    task_id=\"bigquery_to_postgres\",\n    dataset_table=\"{{ DATASET_NAME }}.{{ TABLE_NAME }}\",\n    postgres_conn_id=\"postgres_default\",\n    postgres_table=\"postgres_table\",\n    replace=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Instantiating SQLColumnCheckOperator for Column-Level Data Quality - Apache Airflow - Python\nDESCRIPTION: Shows how to configure an Airflow SQLColumnCheckOperator to perform structured column-level data quality checks on a database table. Requires Airflow, the 'common.sql.operators.sql' provider, valid DB connection, and a column_mapping dict that defines the checks (null, min, max, etc.), using standardized condition fields. Example covers parameter flexibility and illustrates use of partition clauses. Input is a mapping specifying columns and checks; output is pass/fail status for the checks. Optional tolerance and database override parameters supported.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncolumn_mapping = {\n    \"col_name\": {\n        \"null_check\": {\"equal_to\": 0, \"partition_clause\": \"other_col LIKE 'this'\"},\n        \"min\": {\n            \"greater_than\": 5,\n            \"leq_to\": 10,\n            \"tolerance\": 0.2,\n        },\n        \"max\": {\"less_than\": 1000, \"geq_to\": 10, \"tolerance\": 0.01},\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Amazon S3 to SFTP Using S3ToSFTPOperator in Python\nDESCRIPTION: This example demonstrates how to use the S3ToSFTPOperator to transfer a file from an Amazon S3 bucket to a remote SFTP server. The operator requires specifying the S3 bucket, key, and SFTP connection details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/s3_to_sftp.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_transfer_s3_to_sftp]\ns3_to_sftp_task = S3ToSFTPOperator(\n    task_id=\"s3_to_sftp_task\",\n    sftp_conn_id=\"sftp_default\",\n    s3_conn_id=\"aws_default\",\n    s3_bucket=BUCKET_NAME,\n    s3_key=S3_KEY,\n    sftp_path=\"/tmp/tmp-{{ ds }}.json\",\n)\n# [END howto_transfer_s3_to_sftp]\n```\n\n----------------------------------------\n\nTITLE: Configuring Impersonation Chain in Airflow\nDESCRIPTION: Python code that configures the impersonation chain in Airflow by specifying the sequence of service accounts to be used for authentication. Uses environment variables for project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nPROJECT_ID = os.environ.get(\"TF_VAR_project_id\", \"your_project_id\")\nIMPERSONATION_CHAIN = [\n    f\"impersonation-chain-2@{PROJECT_ID}.iam.gserviceaccount.com\",\n    f\"impersonation-chain-3@{PROJECT_ID}.iam.gserviceaccount.com\",\n    f\"impersonation-chain-4@{PROJECT_ID}.iam.gserviceaccount.com\",\n]\n```\n\n----------------------------------------\n\nTITLE: Generating Python Code with Embedded Metadata\nDESCRIPTION: This example shows how to generate a Python file containing metadata as importable constants, which can be used to construct DAG objects and build dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/dynamic-dag-generation.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# This file is generated automatically !\nALL_TASKS = [\"task1\", \"task2\", \"task3\"]\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom my_company_utils.common import ALL_TASKS\n\nwith DAG(\n    dag_id=\"my_dag\",\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n):\n    for task in ALL_TASKS:\n        # create your operators and relations here\n        ...\n```\n\n----------------------------------------\n\nTITLE: Making GET Requests with HttpOperator in Python\nDESCRIPTION: This snippet demonstrates using `HttpOperator` to make a GET request to the `/get` endpoint of `httpbin` using the `http_default` connection. It passes query parameters using the `data` parameter. The task succeeds regardless of the response content as no `response_check` is defined.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/operators.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntask_get_op = SimpleHttpOperator(\n    task_id=\"get_op\",\n    http_conn_id=\"http_default\",\n    method=\"GET\",\n    endpoint=\"get\",\n    data={\"param1\": \"value1\", \"param2\": \"value2\"},\n    headers={},\n)\n```\n\n----------------------------------------\n\nTITLE: Running Python Pipeline with DataflowRunner in Async Mode using GCS File\nDESCRIPTION: This example demonstrates running a Python pipeline on Google Cloud Dataflow with a GCS file in async mode. It specifies both pipeline options and Dataflow job-specific parameters like project ID and region.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_dataflow_runner_async_gcs_file = BeamRunPythonPipelineOperator(\n    task_id=\"beam_task_dataflow_runner_async_gcs_file\",\n    py_file=\"{{ var.json.beam_variables.gcs_file_path }}\",\n    runner=\"DataflowRunner\",\n    pipeline_options={\n        \"output\": \"{{ var.json.beam_variables.output_path }}\",\n    },\n    py_interpreter=\"python3\",\n    py_system_site_packages=False,\n    py_requirements=[\n        \"apache-beam[gcp]==2.44.0\",\n    ],\n    dataflow_config={\n        \"job_name\": \"{{task.task_id}}\",\n        \"project_id\": \"{{ var.json.beam_variables.gcp_project }}\",\n        \"location\": \"{{ var.json.beam_variables.gcp_region }}\",\n        \"staging_location\": \"{{ var.json.beam_variables.gcp_dataflow_staging }}\",\n        \"temp_location\": \"{{ var.json.beam_variables.gcp_dataflow_temp }}\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Creating S3 Object with S3CreateObjectOperator\nDESCRIPTION: Shows how to create or replace an object in S3 using the S3CreateObjectOperator. The operator supports specifying data directly or reading from a file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/s3.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncreate_object = S3CreateObjectOperator(\n    task_id=\"create_object\",\n    bucket_name=BUCKET_NAME,\n    key=KEY,\n    data=b\"Test data\",\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Embedding DAGs in Airflow Docker Image\nDESCRIPTION: This Dockerfile example shows how to embed a DAG file (test_dag.py) into the Airflow image by copying it to the /opt/airflow/dags folder.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_3\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apache/airflow:2.7.1\nCOPY test_dag.py /opt/airflow/dags\n```\n\n----------------------------------------\n\nTITLE: Native Python Object Rendering in DAG\nDESCRIPTION: Alternative solution using NativeEnvironment for Jinja templating by setting render_template_as_native_obj=True in DAG configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_template_as_python_object\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    render_template_as_native_obj=True,\n):\n    transform = PythonOperator(\n        task_id=\"transform\",\n        op_kwargs={\"order_data\": \"{{ ti.xcom_pull('extract') }}\"},\n        python_callable=transform,\n    )\n```\n\n----------------------------------------\n\nTITLE: Passing Parameters to Inline SQL Query (Python)\nDESCRIPTION: Illustrates using the `parameters` argument of `SQLExecuteQueryOperator` to pass a dictionary of values (begin_date, end_date) into an inline SQL query. The SQL uses psycopg2 named variable binding style (e.g., `%(begin_date)s`) which are substituted securely at runtime to filter pets by birth date.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/operators.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nget_birth_date = SQLExecuteQueryOperator(\n    task_id=\"get_birth_date\",\n    conn_id=\"postgres_default\",\n    sql=\"SELECT * FROM pet WHERE birth_date BETWEEN SYMMETRIC %(begin_date)s AND %(end_date)s\",\n    parameters={\"begin_date\": \"2020-01-01\", \"end_date\": \"2020-12-31\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Using CloudDataTransferServiceUpdateJobOperator in Python\nDESCRIPTION: Example of using the CloudDataTransferServiceUpdateJobOperator to update a transfer job in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nupdate_transfer = CloudDataTransferServiceUpdateJobOperator(\n    task_id=\"update_transfer\",\n    job_name=\"{{ task_instance.xcom_pull('create_transfer', key='name') }}\",\n    body=body,\n    gcp_conn_id=GCP_CONN_ID\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataflow Pipeline Using JSON Configuration\nDESCRIPTION: Example showing how to create a Dataflow pipeline by passing its structure in JSON format using DataflowCreatePipelineOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n_ = DataflowCreatePipelineOperator(\n    task_id=\"create_dataflow_pipeline\",\n    project_id=project_id,\n    location=location,\n    pipeline_id=PIPELINE_ID,\n    pipeline=PIPELINE,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Created Tag ID from XCom in Python\nDESCRIPTION: Example of retrieving the newly created tag ID from XCom after creating a tag with CloudDataCatalogCreateTagOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_create_tag_result]\ntask_instance = kwargs[\"ti\"]\ncreate_tag_result = task_instance.xcom_pull(task_ids=\"create_tag\", key=\"tag_id\")\nprint(create_tag_result)\n# [END howto_operator_gcp_datacatalog_create_tag_result]\n```\n\n----------------------------------------\n\nTITLE: Configuring a PySpark Job for Google Cloud Dataproc\nDESCRIPTION: This code defines a configuration for a PySpark job to be submitted to a Dataproc cluster. It specifies the main Python file, arguments, and jar file locations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nPYSPARK_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"pyspark_job\": {\n        \"main_python_file_uri\": f\"gs://{BUCKET_NAME}/{PYSPARK_MAIN}\",\n        \"args\": [f\"gs://{BUCKET_NAME}/{PYSPARK_FILE}\"],\n        \"jar_file_uris\": [f\"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\"],\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an AWS Connection and Generating URI in Apache Airflow (Python)\nDESCRIPTION: This Python snippet demonstrates how to programmatically create an AWS Connection object in Airflow using specific credentials and extra configuration (such as region), then generate its URI and set it as an environment variable for Airflow processing. It also demonstrates testing the connection credentials with the Connection object's test_connection method. Dependencies: Airflow installed; requires 'airflow.models.connection.Connection'. Key parameters include AWS Access Key ID (login), AWS Secret Access Key (password), and region_name in the 'extra' dictionary. Inputs are hardcoded credentials; the output is a formatted connection URI and a connection test result. Credentials should be managed securely and never hardcoded in production environments.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom airflow.models.connection import Connection\n\n\nconn = Connection(\n    conn_id=\"sample_aws_connection\",\n    conn_type=\"aws\",\n    login=\"AKIAIOSFODNN7EXAMPLE\",  # Reference to AWS Access Key ID\n    password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",  # Reference to AWS Secret Access Key\n    extra={\n        # Specify extra parameters here\n        \"region_name\": \"eu-central-1\",\n    },\n)\n\n# Generate Environment Variable Name and Connection URI\nenv_key = f\"AIRFLOW_CONN_{conn.conn_id.upper()}\"\nconn_uri = conn.get_uri()\nprint(f\"{env_key}={conn_uri}\")\n# AIRFLOW_CONN_SAMPLE_AWS_CONNECTION=aws://AKIAIOSFODNN7EXAMPLE:wJalrXUtnFEMI%2FK7MDENG%2FbPxRfiCYEXAMPLEKEY@/?region_name=eu-central-1\n\nos.environ[env_key] = conn_uri\nprint(conn.test_connection())  # Validate connection credentials.\n\n```\n\n----------------------------------------\n\nTITLE: Creating Vertex AI Batch Prediction Job using Python\nDESCRIPTION: This snippet shows how to create a Vertex AI batch prediction job using the CreateBatchPredictionJobOperator in Airflow. It specifies various parameters such as job display name, model name, input config, and output config.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/automl.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nCreateBatchPredictionJobOperator(\n    job_display_name=f\"test-batch-prediction-{int(time.time())}\",\n    model_name=test_model_name,\n    predictions_format=\"jsonl\",\n    input_config={\n        \"instances_format\": \"jsonl\",\n        \"gcs_source\": {\"uris\": [INPUT_DATA]},\n    },\n    output_config={\"gcs_destination\": {\"output_uri_prefix\": OUTPUT_DATA}},\n    region=REGION,\n    task_id=\"create_batch_prediction_job\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Cached Content for Vertex AI using Airflow Operator in Python\nDESCRIPTION: Shows the usage of `CreateCachedContentOperator` from `airflow.providers.google.cloud.operators.vertex_ai.generative_model` to create cached content in Vertex AI. The resource name of the created cached content is pushed to XCom under the 'return_value' key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_52\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/vertex_ai/example_vertex_ai_generative_model.py\n    :language: python\n    :dedent: 4\n    :start-after: [START how_to_cloud_vertex_ai_create_cached_content_operator]\n    :end-before: [END how_to_cloud_vertex_ai_create_cached_content_operator]\n```\n\n----------------------------------------\n\nTITLE: SQL Query for Merging Employee Data\nDESCRIPTION: SQL query that inserts distinct records from a temporary table into the main employees table, handling duplicate entries with an upsert pattern based on Serial Number.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/pipeline.rst#2025-04-22_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO employees\nSELECT *\nFROM (\n    SELECT DISTINCT *\n    FROM employees_temp\n) t\nON CONFLICT (\"Serial Number\") DO UPDATE\nSET\n  \"Employee Markme\" = excluded.\"Employee Markme\",\n  \"Description\" = excluded.\"Description\",\n  \"Leave\" = excluded.\"Leave\";\n```\n\n----------------------------------------\n\nTITLE: Executing Snowflake SQL via SQL API with SnowflakeSqlApiOperator in Python\nDESCRIPTION: This Python snippet illustrates using the SnowflakeSqlApiOperator to execute multiple SQL statements ('SELECT 1; SELECT 2;') against Snowflake using its SQL API. It uses the 'snowflake_conn_id' for connection, explicitly defines database, schema, role, and warehouse, and binds parameters. This operator can also run in deferrable mode for improved resource utilization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/operators/snowflake.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsnowflake_sql_api_query = SnowflakeSqlApiOperator(\n    task_id=\"snowflake_sql_api_query\",\n    snowflake_conn_id=SNOWFLAKE_CONN_ID,\n    sql=\"SELECT 1; SELECT 2;\",\n    database=SNOWFLAKE_DATABASE,\n    schema=SNOWFLAKE_SCHEMA,\n    role=SNOWFLAKE_ROLE,\n    warehouse=SNOWFLAKE_WAREHOUSE,\n    params={\"id\": 123},\n)\n\n```\n\n----------------------------------------\n\nTITLE: Exporting a Model using Vertex AI Model Service Operator - Python\nDESCRIPTION: Outlines the process of exporting a Google Vertex AI model using Airflow's ExportModelOperator. Prerequisites include the model's resource name and desired export configuration. Resulting export location or status is handled for use in subsequent workflow steps.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nexport_model_task = ExportModelOperator(\n    task_id=\"export_model_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    model_id=MODEL_ID,\n    export_config=MODEL_EXPORT_CONFIG,\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Google Cloud Connection Using Environment Variable\nDESCRIPTION: Example showing how to set up a basic Google Cloud connection using an environment variable with an empty URI for Application Default Credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT='google-cloud-platform://'\n```\n\n----------------------------------------\n\nTITLE: Creating Amazon S3 Bucket with S3CreateBucketOperator\nDESCRIPTION: Demonstrates how to create a new S3 bucket using the S3CreateBucketOperator. This operator allows specifying the bucket name, region, and ACL configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_bucket = S3CreateBucketOperator(\n    task_id=\"create_bucket\",\n    bucket_name=BUCKET_NAME,\n    aws_conn_id=AWS_CONN_ID,\n    region_name=\"us-east-1\",\n)\n```\n\n----------------------------------------\n\nTITLE: Moving Multiple Files in GCS Using GCSToGCSOperator\nDESCRIPTION: Example showing how to move multiple files by providing a list of source objects with the move parameter set to True. This copies the specified files to the destination and then deletes them from the source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gcs.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmove_files_with_list = GCSToGCSOperator(\n    task_id=\"gcs_to_gcs_list_move\",\n    source_bucket=BUCKET_1_SRC,\n    source_objects=[OBJECT_1, OBJECT_2],\n    destination_bucket=BUCKET_1_DST,\n    move=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Using PinotAdminHook in Apache Airflow\nDESCRIPTION: This code snippet demonstrates how to use the PinotAdminHook in an Apache Airflow DAG. It shows the usage of various methods such as add_schema, create_segment, and upload_segment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/docs/hooks.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npinot_admin_hook = PinotAdminHook(conn_id=\"pinot_admin_hook\", cmd_path=\"pinot-admin.sh\")\n\npinot_admin_hook.add_schema(schema_file=\"path/to/schema/file\", table_name=\"my_table\")\npinot_admin_hook.add_table(file_path=\"path/to/table/config\", table_name=\"my_table\")\npinot_admin_hook.create_segment(\n    segment_file_path=\"path/to/data/file\",\n    schema_file_path=\"path/to/schema/file\",\n    table_name=\"my_table\",\n    segment_name=\"my_segment\",\n)\npinot_admin_hook.upload_segment(\n    segment_file_path=\"path/to/segment/file\", table_name=\"my_table\"\n)\n```\n\n----------------------------------------\n\nTITLE: Triggering a Workflow Template Asynchronously in Google Cloud Dataproc\nDESCRIPTION: This code demonstrates how to trigger a workflow template in deferrable mode for asynchronous execution, which improves Airflow resource utilization during long-running workflows.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ntrigger_workflow_template_async = DataprocInstantiateWorkflowTemplateOperator(\n    task_id=\"trigger_workflow_template_async\",\n    template_id=WORKFLOW_NAME,\n    project_id=PROJECT_ID,\n    region=REGION,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Empty BigQuery Dataset - Python\nDESCRIPTION: Example of using BigQueryCreateEmptyDatasetOperator to create an empty dataset in BigQuery.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_dataset_task = BigQueryCreateEmptyDatasetOperator(\n    task_id=\"create_dataset\",\n    dataset_id=DATASET_NAME,\n    location=LOCATION,\n)\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Postgres to Google Cloud Storage using PostgresToGCSOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the PostgresToGCSOperator in Apache Airflow to transfer data from a Postgres database to Google Cloud Storage. It includes setting up the database connection, specifying the SQL query, and configuring the GCS destination.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/postgres_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_postgres_to_gcs]\nPostgresToGCSOperator(\n    task_id=\"postgres_to_gcs\",\n    postgres_conn_id=\"postgres_default\",\n    sql=\"SELECT * FROM users\",\n    bucket=\"my-bucket\",\n    filename=\"data/{{ ds_nodash }}/my_file.json\",\n    gcp_conn_id=\"google_cloud_default\",\n)\n# [END howto_operator_postgres_to_gcs]\n```\n\n----------------------------------------\n\nTITLE: Using File References in BashOperator\nDESCRIPTION: Shows how to reference a script file in BashOperator instead of including the script directly in the DAG code.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrun_script = BashOperator(\n    task_id=\"run_script\",\n    bash_command=\"script.sh\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Product to ProductSet with Explicit IDs in Google Cloud Vision Operator\nDESCRIPTION: Shows how to use CloudVisionAddProductToProductSetOperator with explicitly specified product and product set IDs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nadd_product_to_product_set = CloudVisionAddProductToProductSetOperator(\n    product_set_id=PRODUCT_SET_ID,\n    product_id=PRODUCT_ID,\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    task_id=\"add_product_to_product_set\",\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a GKE Cluster with GKEDeleteClusterOperator\nDESCRIPTION: Example of using the GKEDeleteClusterOperator to delete a Google Kubernetes Engine cluster, which also deletes all nodes allocated to the cluster.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndelete_cluster = GKEDeleteClusterOperator(\n    task_id=\"delete_cluster\",\n    name=CLUSTER_NAME,\n    project_id=PROJECT_ID,\n    location=LOCATION,\n)\n```\n\n----------------------------------------\n\nTITLE: Making DELETE Requests with HttpOperator in Python\nDESCRIPTION: This example uses `HttpOperator` to send a DELETE request to the `/delete` endpoint of `httpbin` using the `http_default` connection. It sends data as form parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/operators.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntask_del_op = SimpleHttpOperator(\n    task_id=\"del_op\",\n    http_conn_id=\"http_default\",\n    method=\"DELETE\",\n    endpoint=\"delete\",\n    data=\"some=data\",\n    headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating RDS Database Instance\nDESCRIPTION: Creates an AWS RDS database instance using RdsCreateDbInstanceOperator. Can be run in deferrable mode.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncreate_db = RdsCreateDbInstanceOperator(\n    task_id=\"create_db_instance\",\n    db_instance_identifier=DB_INSTANCE_NAME,\n    db_instance_class=\"db.t3.micro\",\n    engine=\"postgres\",\n    rds_kwargs={\n        \"MasterUsername\": \"master_user\",\n        \"MasterUserPassword\": \"secret\",\n        \"AllocatedStorage\": 20,\n        \"DBName\": DB_NAME,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Creating WinRM Hook in Apache Airflow (Python)\nDESCRIPTION: This snippet demonstrates how to initialize a WinRM hook to establish a secure connection between Apache Airflow and a remote Windows host. This requires the Airflow WinRM provider package and configuration of proper connection credentials (host, username, password, etc.). The hook serves as an interface for executing commands remotely via WinRM and must be instantiated before using related operators. Inputs are typically connection parameters, and the output is a hook object ready to be used by downstream operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/winrm/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nhook = WinRMHook(\n    remote_host=\"my.remote.host\",\n    username=\"airflow\",\n    password=\"password123\",\n    transport=\"ntlm\"\n)\n```\n\n----------------------------------------\n\nTITLE: Object Parameter Configuration in Python\nDESCRIPTION: Example of configuring an object parameter that accepts JSON input with optional null value support.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/params.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nParam({\"key\": \"value\"}, type=[\"object\", \"null\"])\n```\n\n----------------------------------------\n\nTITLE: Creating AutoML Image Training Job in Vertex AI\nDESCRIPTION: Example showing how to create an AutoML image training job using the CreateAutoMLImageTrainingJobOperator. Requires a pre-created Image dataset.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nCreateAutoMLImageTrainingJobOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    display_name=f\"temp_image_training_job_test_{SUFFIX}\",\n    dataset_id=DATASET_ID,\n    prediction_type=\"classification\",\n    multi_label=False,\n    model_type=\"CLOUD\",\n    training_budget_milli_node_hours=8000,\n    task_id=\"image_training_job\",)\n```\n\n----------------------------------------\n\nTITLE: Wait for AWS Glue Job State Example\nDESCRIPTION: Example showing how to wait for an AWS Glue Job to reach terminal state using GlueJobSensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/glue.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n[START howto_sensor_glue]\n[END howto_sensor_glue]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Airflow Variables in Python\nDESCRIPTION: Demonstrates how to retrieve variables in Python DAGs using the Variable class from airflow.sdk. Shows both simple string retrieval and JSON deserialization.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/variable.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import Variable\n\nfoo = Variable.get(\"foo\")\nfoo_json = Variable.get(\"foo_baz\", deserialize_json=True)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataproc Batch Asynchronously (Deferrable) in Python\nDESCRIPTION: This snippet shows how to use the `DataprocCreateBatchOperator` in deferrable mode (`deferrable=True`). This allows the operator to release the Airflow worker slot while waiting for the batch creation to complete, improving resource utilization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n# Code extracted from: /../../google/tests/system/google/cloud/dataproc/example_dataproc_batch_deferrable.py\n# Between markers: [START how_to_cloud_dataproc_create_batch_operator_async] and [END how_to_cloud_dataproc_create_batch_operator_async]\n# \n# Example using DataprocCreateBatchOperator(..., deferrable=True)\n# ... (actual Python code would be here)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Task Logging for Custom Executor in Python\nDESCRIPTION: Example of how to implement the get_task_log method to provide additional logging capabilities for a custom Airflow executor. This method fetches logs from the execution environment and includes them in the Airflow task logs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/index.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef get_task_log(self, ti: TaskInstance, try_number: int) -> tuple[list[str], list[str]]:\n    messages = []\n    log = []\n    try:\n        res = helper_function_to_fetch_logs_from_execution_env(ti, try_number)\n        for line in res:\n            log.append(remove_escape_codes(line.decode()))\n        if log:\n            messages.append(\"Found logs from execution environment!\")\n    except Exception as e:  # No exception should cause task logs to fail\n        messages.append(f\"Failed to find logs from execution environment: {e}\")\n    return messages, [\"\\n\".join(log)]\n```\n\n----------------------------------------\n\nTITLE: Implementing ExternalTaskSensor with Task Group Dependency in Python for Apache Airflow\nDESCRIPTION: This snippet illustrates how to use ExternalTaskSensor to make tasks wait for a task group in a different DAG. It specifies the external DAG ID and the task group ID instead of a specific task ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/external_task_sensor.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsensor_task_group = ExternalTaskSensor(\n    task_id='child_task1_sensor',\n    external_dag_id='example_external_task_marker_parent',\n    external_task_group_id='group1',\n    mode='reschedule',\n)\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-http using pip\nDESCRIPTION: This command installs the apache-airflow-providers-http package using pip, the Python package installer. It should be run in a command-line environment on top of an existing Airflow 2 installation (version 2.9.0 or higher) where pip is available. This command adds HTTP connection, operator, and sensor capabilities to Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-http\n```\n\n----------------------------------------\n\nTITLE: Getting a Dataplex Catalog Entry Group with Airflow Operator\nDESCRIPTION: Uses the DataplexCatalogGetEntryGroupOperator to retrieve a specific Entry Group. This operator fetches detailed information about an entry group in Google Cloud Dataplex Catalog.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nget_entry_group = DataplexCatalogGetEntryGroupOperator(\n    task_id=\"get_entry_group\",\n    project_id=PROJECT_ID,\n    location=REGION,\n    entry_group_id=ENTRY_GROUP_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Using PinotDbApiHook in Apache Airflow\nDESCRIPTION: This code snippet demonstrates how to use the PinotDbApiHook in an Apache Airflow DAG. It shows how to execute a SQL query and fetch the results using the hook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/docs/hooks.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npinot_hook = PinotDbApiHook(pinot_conn_id=\"pinot_conn_id\")\n\nsql = \"\"\"SELECT COUNT(*) AS cnt FROM my_table WHERE columnA = 'value'\"\"\"\nrecords = pinot_hook.get_records(sql)\n\nprint(f\"Number of records: {records[0][0]}\")\n```\n\n----------------------------------------\n\nTITLE: Updating BigQuery Dataset - Python\nDESCRIPTION: Example of using BigQueryUpdateDatasetOperator to update dataset properties.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nupdate_dataset_task = BigQueryUpdateDatasetOperator(\n    task_id=\"update_dataset\",\n    dataset_id=DATASET_NAME,\n    dataset_resource={\"description\": \"Updated dataset\"}\n)\n```\n\n----------------------------------------\n\nTITLE: Grouped Parallel Setup and Teardown\nDESCRIPTION: Demonstrates organizing parallel setup and teardown tasks using task groups for better visualization.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nwith TaskGroup(\"setup\") as tg_s:\n    create_cluster = create_cluster()\n    create_bucket = create_bucket()\nrun_query = run_query()\nwith TaskGroup(\"teardown\") as tg_t:\n    delete_cluster = delete_cluster().as_teardown(setups=create_cluster)\n    delete_bucket = delete_bucket().as_teardown(setups=create_bucket)\ntg_s >> run_query >> tg_t\n```\n\n----------------------------------------\n\nTITLE: Using Callable for Complex Templating in Python\nDESCRIPTION: Demonstrates how to use a callable function instead of a Jinja template for more complex templating needs with BashOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef build_complex_command(context, jinja_env):\n    with open(\"file.csv\") as f:\n        return do_complex_things(f)\n\n\nt = BashOperator(\n    task_id=\"complex_templated_echo\",\n    bash_command=build_complex_command,\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating an Amazon EKS Cluster using EksCreateClusterOperator in Python\nDESCRIPTION: This Python snippet demonstrates how to use the `EksCreateClusterOperator` in an Airflow DAG to create a new Amazon EKS cluster. It requires an AWS IAM role with `eks.amazonaws.com` in Trusted Relationships and the `AmazonEKSClusterPolicy` attached.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eks.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example using EksCreateClusterOperator\n# Assumes necessary imports and DAG context\n\ncreate_cluster = EksCreateClusterOperator(\n    task_id=\"create_eks_cluster\",\n    # cluster_name, cluster_role_arn, resources_vpc_config parameters are required\n    cluster_name=\"my-eks-cluster\",\n    cluster_role_arn=\"arn:aws:iam::123456789012:role/EksClusterRole\",\n    resources_vpc_config={\n        \"subnetIds\": [\"subnet-xxxxxxxxxxxxxxxxx\", \"subnet-yyyyyyyyyyyyyyyyy\"],\n        \"securityGroupIds\": [\"sg-xxxxxxxxxxxxxxxxx\"],\n    },\n    # other optional parameters...\n)\n\n# [END howto_operator_eks_create_cluster]\n```\n\n----------------------------------------\n\nTITLE: Launching Custom Training Job via Script with CreateCustomTrainingJobOperator in VertexAI (Python)\nDESCRIPTION: Provides example of running a custom training job using a local script via VertexAI with Airflow. Key inputs include script_path (path to training script), project, location, display_name, and dataset_id. On completion, model details are output via XCom. Airflow DAG setup, VertexAI access, and authentication are required.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwith models.DAG(\n    \"vertex_ai_custom_job\",\n    default_args=default_args,\n    schedule_interval=None,\n    start_date=days_ago(1),\n    catchup=False,\n) as dag:\n    create_training_job = CreateCustomTrainingJobOperator(\n        task_id=\"train_model\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        display_name=DISPLAY_NAME,\n        script_path=SCRIPT_PATH,\n        dataset_id=DATASET_ID,\n        gcp_conn_id=GCP_CONN_ID,\n    )\n```\n\n----------------------------------------\n\nTITLE: Extracting Document Text Detection Result from XCom - Airflow Python Operator\nDESCRIPTION: This SNIPPET retrieves the output from a CloudVisionTextDetectOperator task via XCom in an Airflow DAG, allowing subsequent tasks to access detailed OCR results. Inputs are the reference to the 'document_detect_text' task, and output is the detected document text data. It leverages Airflow's cross-task communication for modular workflow design.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nresult = document_detect_text.output\n\n```\n\n----------------------------------------\n\nTITLE: Transferring Email Attachment from IMAP to S3 using ImapAttachmentToS3Operator in Python\nDESCRIPTION: This code snippet demonstrates how to use the ImapAttachmentToS3Operator to transfer an email attachment from an IMAP server to an Amazon S3 bucket. It specifies the IMAP connection details, email search criteria, and S3 destination information.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/imap_attachment_to_s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nImapAttachmentToS3Operator(\n    task_id=\"transfer_imap_attachment_to_s3\",\n    imap_attachment_name=\"test.txt\",\n    s3_key=\"test_file_{{ ds_nodash }}.txt\",\n    imap_check_regex=False,\n    imap_mail_folder=\"INBOX\",\n    imap_mail_filter=\"ALL\",\n    s3_bucket=S3_BUCKET,\n    imap_mail_ssl=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Connection for Unicode Support in Airflow\nDESCRIPTION: This snippet demonstrates how to set up a MySQL connection string in Airflow to support extended ASCII or Unicode characters. It explicitly defines the charset as UTF-8 to prevent encoding issues.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/faq.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nsql_alchemy_conn = mysql://airflow@localhost:3306/airflow?charset=utf8\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Logging in Airflow via airflow.cfg - INI\nDESCRIPTION: This snippet configures remote logging in Apache Airflow to send logs to Amazon Cloudwatch using the airflow.cfg file. It requires an existing Airflow AWS connection (referenced by remote_log_conn_id) with permissions to access the specified AWS log group ARN (remote_base_log_folder). Users should modify the configuration with the appropriate AWS region, account ID, and log group name for their environment. All fields are required and must match deployed AWS resources and Airflow connections.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/logging/cloud-watch-task-handlers.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\n# Airflow can store logs remotely in AWS Cloudwatch. Users must supply a log group\n# ARN (starting with 'cloudwatch://...') and an Airflow connection\n# id that provides write and read access to the log location.\nremote_logging = True\nremote_base_log_folder = cloudwatch://arn:aws:logs:<region name>:<account id>:log-group:<group name>\nremote_log_conn_id = MyCloudwatchConn\n```\n\n----------------------------------------\n\nTITLE: Using Jinja Filters for Date Formatting in Airflow (Jinja)\nDESCRIPTION: Demonstrates the use of Jinja filters in Airflow for formatting date and time values. Shows an example of formatting the logical_date using the 'ds' filter.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/templates-ref.rst#2025-04-22_snippet_4\n\nLANGUAGE: jinja\nCODE:\n```\n{{ logical_date | ds }}\n```\n\n----------------------------------------\n\nTITLE: Setup and Teardown with Task Groups\nDESCRIPTION: Example of using setup and teardown tasks within task groups.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwith TaskGroup(\"my_group\") as tg:\n    s1 = s1()\n    w1 = w1()\n    t1 = t1()\n    s1 >> w1 >> t1.as_teardown(setups=s1)\nw2 = w2()\ntg >> w2\n```\n\n----------------------------------------\n\nTITLE: Building and Running a Custom Airflow Image\nDESCRIPTION: Commands to build a custom Airflow Docker image and run it with a custom script, demonstrating the usage of extended images.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . --pull --tag my-image:0.0.1\ndocker run -it my-image:0.0.1 bash -c \"/my_after_entrypoint_script.sh\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Dataproc Batch with Airflow Operator in Python\nDESCRIPTION: This snippet demonstrates using the `DataprocDeleteBatchOperator` to delete a specific Dataproc Batch job. It requires `batch_id`, `region`, and `project_id` to identify the batch to be deleted.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n# Code extracted from: /../../google/tests/system/google/cloud/dataproc/example_dataproc_batch.py\n# Between markers: [START how_to_cloud_dataproc_delete_batch_operator] and [END how_to_cloud_dataproc_delete_batch_operator]\n# \n# Example using DataprocDeleteBatchOperator(...)\n# ... (actual Python code would be here)\n\n```\n\n----------------------------------------\n\nTITLE: Deleting an Entry Group with CloudDataCatalogDeleteEntryGroupOperator in Python\nDESCRIPTION: Uses CloudDataCatalogDeleteEntryGroupOperator to delete an entry group from Google Cloud Data Catalog.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndelete_entry_group = CloudDataCatalogDeleteEntryGroupOperator(\n    task_id=\"delete_entry_group\",\n    location=LOCATION,\n    entry_group=ENTRY_GROUP_ID,\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Getting a Model by ID using Vertex AI Model Service Operator - Python\nDESCRIPTION: Provides a code example for retrieving model details by ID from Vertex AI using the GetModelOperator. Key inputs include project ID, region, and the unique model ID. Outputs full model metadata and configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nget_model_task = GetModelOperator(\n    task_id=\"get_model_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    model_id=MODEL_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating an Amazon EMR Serverless Application with Airflow - Python\nDESCRIPTION: Demonstrates how to use the EmrServerlessCreateApplicationOperator in Airflow to create an EMR Serverless Application in AWS. The example requires the airflow.providers.amazon and optionally aiobotocore modules if using deferrable mode. Key parameters include application_name, release_label, and optionally deferrable; the operator outputs an application ID. Ensure proper permissions for IAM and EMR Serverless to succeed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr_serverless.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.providers.amazon.aws.operators.emr import EmrServerlessCreateApplicationOperator\nfrom datetime import datetime\n\ndefault_args = {\n    \"owner\": \"airflow\",\n}\n\ndag = DAG(\n    dag_id=\"example_emr_serverless_create_application\",\n    start_date=datetime(2022, 1, 1),\n    default_args=default_args,\n    schedule_interval=None,\n)\n\ncreate_emr_serverless_app = EmrServerlessCreateApplicationOperator(\n    task_id=\"create_emr_serverless_application\",\n    application_name=\"my-emr-serverless-app\",\n    release_label=\"emr-6.4.0\",\n    deferrable=True,  # Optional: use deferrable mode\n    dag=dag,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Deleting AutoML Translation Model with TranslateDeleteModelOperator in Python\nDESCRIPTION: Shows how to use the `TranslateDeleteModelOperator` in an Airflow DAG to delete a specific AutoML translation model using the V3 API. Requires `model_id`, `project_id`, and `location` to identify the model for deletion.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndelete_model = TranslateDeleteModelOperator(\n    task_id=\"delete_model\",\n    model_id=MODEL_ID,\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n)\n```\n\n----------------------------------------\n\nTITLE: TaskFlow Logging Implementation\nDESCRIPTION: Demonstrates how to implement logging in TaskFlow tasks using Python's logging system.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/taskflow.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlogger = logging.getLogger(\"airflow.task\")\n```\n\n----------------------------------------\n\nTITLE: Using TranslateTextOperator for Advanced Text Translation in Python\nDESCRIPTION: Demonstrates using the `TranslateTextOperator` to translate an array of text strings ('Hello', 'World') to German ('de') using the Google Cloud Translate API V3. It specifies the source language ('en'), target language, project ID, location, and glossary configuration. This operator is suitable for moderate amounts of text.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntranslate_text_advanced = TranslateTextOperator(\n    task_id=\"translate_text_advanced\",\n    values=[\"Hello\", \"World\"],\n    target_language_code=\"de\",\n    source_language_code=\"en\",\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n    model=f\"projects/{GCP_PROJECT_ID}/locations/us-central1/models/general/nmt\",\n    glossary_config=GLOSSARY_CONFIG,\n    mime_type=\"text/plain\",  # Mime types: \"text/plain\", \"text/html\"\n)\n```\n\n----------------------------------------\n\nTITLE: Searching Data Catalog Resources with Airflow\nDESCRIPTION: Example showing how to use CloudDataCatalogSearchCatalogOperator to search Data Catalog resources and access the results via XCom.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nsearch_catalog_task = CloudDataCatalogSearchCatalogOperator(\n    task_id=\"search_catalog\",\n    scope={\n        \"include_org_ids\": [\"12345\"],\n        \"include_project_ids\": [\"example-project\"],\n    },\n    query=\"type=dataset\",\n    order_by=\"relevance\",\n    page_size=1,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nsearch_catalog_result = search_catalog_task.output\n```\n\n----------------------------------------\n\nTITLE: Setting up dag.test() for Debugging Airflow DAGs in Python\nDESCRIPTION: This snippet shows how to add the necessary code to enable dag.test() functionality in an Airflow DAG file. It allows for debugging the DAG in a single serialized Python process.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/debug.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    dag.test()\n```\n\n----------------------------------------\n\nTITLE: Using Asynchronous GCSUploadSessionCompleteSensor in Python\nDESCRIPTION: Shows how to use the GCSUploadSessionCompleteSensor in asynchronous mode to free up worker slots while the sensor is running.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ngcs_upload_session_async = GCSUploadSessionCompleteSensor(\n    task_id=\"gcs_upload_session_async\",\n    bucket=BUCKET_NAME,\n    prefix=PREFIX,\n    inactivity_period=15,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Context Variables in @task.bash Decorator in Airflow\nDESCRIPTION: Demonstrates how to access execution context variables directly in tasks decorated with @task.bash in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@task.bash\ndef bash_context_vars(ds_nodash, dag_run):\n    message = dag_run.conf[\"message\"] if dag_run.conf else \"no message\"\n    return f\"echo \\\"execution_date_nodash={ds_nodash} | message={message}\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Listing Vertex AI Batch Prediction Jobs using Python\nDESCRIPTION: This code snippet demonstrates how to list Vertex AI batch prediction jobs using the ListBatchPredictionJobsOperator in Airflow. It specifies the project ID and region for the list operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/automl.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nListBatchPredictionJobsOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    task_id=\"list_batch_prediction_jobs\",\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Data Quality Checks on Athena with SQLTableCheckOperator - Python\nDESCRIPTION: Illustrates instantiation of Airflow's SQLTableCheckOperator to conduct simple data quality checks on Amazon Athena tables. This operator expects a valid Athena connection ID and a table with columns to check. Requires Airflow with an Athena connection and the appropriate SQLAlchemy/Athena driver package (such as PyAthena). Inputs include table name and defined column checks, outputs are pass/fail task states depending on check results.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/athena/athena_sql.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate SQLTableCheckOperator for Athena\nathena_table_check = SQLTableCheckOperator(\n    task_id=\"athena_table_check\",\n    table=\"my_athena_table\",\n    conn_id=\"my_athena_conn_id\",  # Athena connection\n    column_checks={\n        \"total_rows\": {\n            \"check_statement\": \"COUNT(*) > 0\"\n        }\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Using Private Images with KubernetesPodOperator in Python\nDESCRIPTION: Demonstrates how to use images from a private registry with KubernetesPodOperator by creating and using a Kubernetes Secret.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Example code not provided in the original text\n```\n\n----------------------------------------\n\nTITLE: Instantiating TeradataToTeradataOperator in Python\nDESCRIPTION: This snippet demonstrates the basic usage of the `TeradataToTeradataOperator` in an Airflow DAG. It shows how to instantiate the operator to transfer data from a source Teradata table to a destination Teradata table, specifying connection IDs and SQL commands. Dependencies include the Apache Airflow `apache-airflow-providers-teradata` package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/operators/teradata_to_teradata.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../teradata/tests/system/teradata/example_teradata_to_teradata_transfer.py\n#    :language: python\n#    :start-after: [START teradata_to_teradata_transfer_operator_howto_guide_transfer_data]\n#    :end-before: [END teradata_to_teradata_transfer_operator_howto_guide_transfer_data]\n\n# This placeholder represents the Python code included from the specified file.\n# The actual code would show the instantiation of TeradataToTeradataOperator, e.g.:\n# transfer_task = TeradataToTeradataOperator(\n#     task_id='transfer_teradata_data',\n#     source_teradata_conn_id='teradata_source_conn',\n#     dest_teradata_conn_id='teradata_dest_conn',\n#     sql='SELECT * FROM source_table;',\n#     destination_table='destination_table',\n#     # ... other parameters ...\n# )\n\n```\n\n----------------------------------------\n\nTITLE: Retrieving Entity Sentiment Analysis Results in Apache Airflow\nDESCRIPTION: This snippet shows how to retrieve and process the results of entity sentiment analysis using XCom in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/natural_language.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nentities = analyze_entity_sentiment.output[\"entities\"]\nfor entity in entities:\n    print(f\"{entity.name}: {entity.sentiment.score}\")\n```\n\n----------------------------------------\n\nTITLE: CloudSQL Import GCS Permissions ACL Operator Example (Python)\nDESCRIPTION: Illustrates setting GCS bucket object permissions so that the Cloud SQL instance's service account can read SQL import files. It uses Airflow's GCSBucketCreateAclEntryOperator to grant the 'READER' role. Required dependencies include the Airflow GCS provider. The snippet expects the bucket, object name, entity, and role as parameters. Side effect is a modified ACL for the GCS object.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"set_acl = GCSBucketCreateAclEntryOperator(\\n    task_id=\\\"set_gcs_acl\\\",\\n    bucket=\\\"my-bucket\\\",\\n    object_name=\\\"file.sql\\\",\\n    entity=\\\"serviceAccount:service-1234567890@cloudsql.iam.gserviceaccount.com\\\",\\n    role=\\\"READER\\\",\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Listing AutoML Translation Models with TranslateModelsListOperator in Python\nDESCRIPTION: Example of using `TranslateModelsListOperator` in an Airflow DAG to list AutoML translation models using the V3 API. It requires the `project_id` and `location` to filter the models within a specific Google Cloud project and region.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlist_models = TranslateModelsListOperator(\n    task_id=\"list_models\",\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Multiple S3 Keys with Deferrable S3KeySensor in Python\nDESCRIPTION: This snippet demonstrates how to use the S3KeySensor to check for the existence of multiple keys in an S3 bucket. It uses the deferrable mode for efficient worker utilization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/s3.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nS3KeySensor(\n    task_id=\"wait_for_keys\",\n    bucket_key=[\"{{ ds_nodash }}/my_file.csv\", \"{{ ds_nodash }}/my_file.json\"],\n    bucket_name=\"my-bucket\",\n    aws_conn_id=\"aws_default\",\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Airflow Client with Basic Authentication - Python\nDESCRIPTION: This Python snippet demonstrates initializing the Airflow Python client with HTTP basic authentication, by providing the host URL, username, and password to airflow_client.client.Configuration. It is a required step before making requests to the Airflow REST API. The dependencies are airflow_client and a running Airflow instance at the specified host/port. The configuration object is then used to authenticate client operations in testing or integration scripts.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport airflow_client\n\n# Configure HTTP basic authorization: Basic\nconfiguration = airflow_client.client.Configuration(\n    host=\"http://localhost:8080/api/v1\", username=\"admin\", password=\"admin\"\n)\n\n```\n\n----------------------------------------\n\nTITLE: Making PUT Requests with HttpOperator in Python\nDESCRIPTION: This snippet illustrates using `HttpOperator` to send a PUT request with JSON data to the `/put` endpoint of `httpbin` via the `http_default` connection. It specifies the method as 'PUT' and includes JSON data.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/operators.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntask_put_op = SimpleHttpOperator(\n    task_id=\"put_op\",\n    http_conn_id=\"http_default\",\n    method=\"PUT\",\n    endpoint=\"put\",\n    data=json.dumps({\"priority\": 5}),\n    headers={\"Content-Type\": \"application/json\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting BigQuery Data Transfer Configuration in Python\nDESCRIPTION: This snippet shows how to delete a BigQuery Data Transfer configuration using the BigQueryDeleteDataTransferConfigOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery_dts.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndelete_transfer = BigQueryDeleteDataTransferConfigOperator(\n    task_id=\"delete_transfer\",\n    transfer_config_id=\"{{ task_instance.xcom_pull('create_transfer')['transfer_config_id'] }}\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring a Flink Job for Google Cloud Dataproc\nDESCRIPTION: This code defines a configuration for a Flink job to be submitted to a Dataproc cluster. It specifies the main jar file and arguments for the Flink execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nFLINK_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"flink_job\": {\n        \"jar_file_uris\": [\"file:///usr/lib/flink/examples/streaming/WordCount.jar\"],\n        \"args\": [\"--input\", \"hello\", \"world\", \"hello\", \"again\", \"--output\", \"out\"],\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Vertex AI Endpoint\nDESCRIPTION: Example showing how to create an endpoint for model deployment using the CreateEndpointOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nCreateEndpointOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    endpoint={\n        \"display_name\": f\"temp_endpoint_test_{SUFFIX}\",\n    },\n    task_id=\"endpoint\",)\n```\n\n----------------------------------------\n\nTITLE: Triggering a dbt Cloud Job Without Job ID in Python\nDESCRIPTION: This snippet demonstrates how to trigger a dbt Cloud job using project_name, environment_name, and job_name instead of job_id. This approach only works if these parameters uniquely identify a job in the account.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/operators.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndbt_cloud_run_job_without_job_id = DbtCloudRunJobOperator(\n    task_id=\"dbt_cloud_run_job_without_job_id\",\n    project_name=PROJECT_NAME,\n    environment_name=ENVIRONMENT_NAME,\n    job_name=JOB_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Secrets Backend in Airflow Config\nDESCRIPTION: INI configuration for setting up secrets backend in Airflow's configuration file. Defines the backend class and optional kwargs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/secrets-backend/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend =\nbackend_kwargs =\n```\n\n----------------------------------------\n\nTITLE: Submit AWS Glue Job Example\nDESCRIPTION: Example demonstrating how to submit a new AWS Glue job using the GlueJobOperator. Requires IAM role with access to output location for result data.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/glue.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_glue]\n[END howto_operator_glue]\n```\n\n----------------------------------------\n\nTITLE: Detect Image Safe Search - Google Cloud Vision Airflow Operator - Python\nDESCRIPTION: This Python Airflow operator example runs safe search detection using Google Cloud Vision. It configures the required project, location, and image parameters, uses a retry policy, and outputs safe search attributes for the provided image. This is essential for automated moderation or compliance workflows in image data pipelines.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ndetect_safe_search = CloudVisionDetectImageSafeSearchOperator(\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n    image=GCP_VISION_IMAGE,\n    retry=Retry(),\n    task_id=\"detect_safe_search\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Starting a SageMaker Notebook Instance in Python\nDESCRIPTION: This code snippet demonstrates how to use the SageMakerStartNotebookOperator to launch a SageMaker Notebook Instance and re-attach an ML storage volume. It creates a new ML compute instance with the latest libraries and attaches the ML storage volume.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_sagemaker_notebook_start]\n# Code snippet not provided in the original text\n# [END howto_operator_sagemaker_notebook_start]\n```\n\n----------------------------------------\n\nTITLE: Generating Default Airflow Configuration in Bash\nDESCRIPTION: This command generates the default Airflow configuration, which can be used as a starting point for customization. The output can be redirected to the airflow.cfg file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-config.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairflow config list --defaults\n```\n\nLANGUAGE: bash\nCODE:\n```\nairflow config list --defaults > \"${AIRFLOW_HOME}/airflow.cfg\"\n```\n\n----------------------------------------\n\nTITLE: Updating a Google Cloud Dataproc Cluster Asynchronously\nDESCRIPTION: This code demonstrates how to update a Dataproc cluster using deferrable mode, which runs the operator asynchronously to improve resource utilization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nupdate_cluster_async = DataprocUpdateClusterOperator(\n    task_id=\"update_cluster_async\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster_name=CLUSTER_NAME,\n    cluster=CLUSTER_CONFIG,\n    update_mask=UPDATE_MASK,\n    graceful_decommission_timeout={\n        \"seconds\": GRACEFUL_DECOMISSION_TIMEOUT,\n    },\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Instance Creation Body for CloudSQL (Python)\nDESCRIPTION: Presents an example body for creating a Cloud SQL instance with a failover replica using CloudSQLCreateInstanceOperator. Requires the Airflow Google provider and the correct Google Cloud SQL API schema. The input is a dictionary configuring instance settings, tier, region, and replicas, prepared for use as an operator argument.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"create_body = {\\n    \\\"name\\\": \\\"my-instance\\\",\\n    \\\"settings\\\": {\\n        \\\"tier\\\": \\\"db-n1-standard-1\\\",\\n        \\\"availabilityType\\\": \\\"REGIONAL\\\",\\n    },\\n    \\\"region\\\": \\\"us-central1\\\",\\n    \\\"failoverReplica\\\": {\\n        \\\"name\\\": \\\"my-failover-replica\\\"\\n    }\\n}\"\n```\n\n----------------------------------------\n\nTITLE: Defining Airflow Plugin Interface Class\nDESCRIPTION: Core interface definition for creating Airflow plugins, showing the AirflowPlugin base class structure with all available integration points including macros, views, and hooks.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/plugins.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass AirflowPlugin:\n    # The name of your plugin (str)\n    name = None\n    # A list of references to inject into the macros namespace\n    macros = []\n    # A list of Blueprint object created from flask.Blueprint. For use with the flask_appbuilder based GUI\n    flask_blueprints = []\n    # A list of dictionaries containing FastAPI app objects and some metadata. See the example below.\n    fastapi_apps = []\n    # A list of dictionaries containing FastAPI middleware factory objects and some metadata. See the example below.\n    fastapi_root_middlewares = []\n    # A list of dictionaries containing FlaskAppBuilder BaseView object and some metadata. See example below\n    appbuilder_views = []\n    # A list of dictionaries containing kwargs for FlaskAppBuilder add_link. See example below\n    appbuilder_menu_items = []\n\n    def on_load(*args, **kwargs):\n        # ... perform Plugin boot actions\n        pass\n\n    global_operator_extra_links = []\n    operator_extra_links = []\n    timetables = []\n    listeners = []\n```\n\n----------------------------------------\n\nTITLE: Validating Query Results in BigQuery with Airflow Check and ValueCheck Operators (Sync and Async) - Python\nDESCRIPTION: These examples show how to validate query results using BigQueryCheckOperator and BigQueryValueCheckOperator in Airflow. The CheckOperator asserts that all values returned by a query evaluate as True in Python, while ValueCheckOperator compares query results to a specified pass_value with optional tolerance for numeric values. Both operators support synchronous and deferrable (async) modes. Required dependencies are airflow.providers.google.cloud.operators.bigquery and BigQuery service access. Parameters include the SQL query, pass_value (for value check), and optional tolerance. Outputs are boolean results and task pass/fail. Async usage requires Airflow's triggerer component.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Check that BigQuery query returns non-empty/non-falsy results\nbq_check = BigQueryCheckOperator(\n    task_id=\"bq_check\",\n    sql=\"SELECT COUNT(*) FROM my_table WHERE date = CURRENT_DATE()\"\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Compare BigQuery query result to a pass value\nbq_value_check = BigQueryValueCheckOperator(\n    task_id=\"bq_value_check\",\n    sql=\"SELECT COUNT(*) FROM my_table\",\n    pass_value=100,\n    tolerance=10,\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# BigQuery check operator in async (deferrable) mode\nbq_check_async = BigQueryCheckOperator(\n    task_id=\"bq_check_async\",\n    sql=\"SELECT COUNT(*) > 0 FROM my_table\",\n    deferrable=True,\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# BigQuery value check operator in async (deferrable) mode\nbq_value_check_async = BigQueryValueCheckOperator(\n    task_id=\"bq_value_check_async\",\n    sql=\"SELECT COUNT(*) FROM my_table\",\n    pass_value=100,\n    tolerance=10,\n    deferrable=True,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Google Cloud Run Job with Python Dictionary in Airflow\nDESCRIPTION: Shows how to create a Google Cloud Run Job configuration using a Python dictionary in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_run.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\njob = {\n    \"launch_stage\": \"BETA\",\n    \"template\": {\n        \"template\": {\n            \"containers\": [\n                {\n                    \"image\": \"python:3.9-slim\",\n                    \"command\": [\"python\"],\n                    \"args\": [\"-c\", \"print('Hello World!')\"]\n                }\n            ]\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Tables in BigQuery Dataset - Python\nDESCRIPTION: Example of using BigQueryGetDatasetTablesOperator to list all tables in a dataset.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nget_dataset_tables_task = BigQueryGetDatasetTablesOperator(\n    task_id=\"get_dataset_tables\", dataset_id=DATASET_NAME\n)\n```\n\n----------------------------------------\n\nTITLE: Loading External Configuration for Dynamic DAGs in Python\nDESCRIPTION: This snippet demonstrates how to load external configuration data from a YAML file located in the same directory as the DAG file, using the __file__ attribute to determine the file path.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/dynamic-dag-generation.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmy_dir = os.path.dirname(os.path.abspath(__file__))\nconfiguration_file_path = os.path.join(my_dir, \"config.yaml\")\nwith open(configuration_file_path) as yaml_file:\n    configuration = yaml.safe_load(yaml_file)\n# Configuration dict is available here\n```\n\n----------------------------------------\n\nTITLE: Setting template_searchpath with DAG Constructor in Python\nDESCRIPTION: This snippet shows how to specify a custom directory for finding template files (like Bash scripts) when defining a DAG using the traditional `DAG` class. The `template_searchpath` argument is passed to the `DAG` constructor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\"example_bash_dag\", ..., template_searchpath=\"/opt/scripts\"):\n    t2 = BashOperator(\n        task_id=\"bash_example\",\n        bash_command=\"test.sh \",\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Sidecar Containers in Airflow Kubernetes\nDESCRIPTION: Example configuration for adding sidecar containers to Airflow scheduler and worker pods. Shows how to configure containers that sync DAGs from object storage, with customizable image and pull policy settings.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/using-additional-containers.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nscheduler:\n  extraContainers:\n    - name: s3-sync\n      image: my-company/s3-sync:latest\n      imagePullPolicy: Always\nworkers:\n  extraContainers:\n    - name: s3-sync\n      image: my-company/s3-sync:latest\n      imagePullPolicy: Always\n```\n\n----------------------------------------\n\nTITLE: Updating AlloyDB Instance with Airflow Operator\nDESCRIPTION: Uses AlloyDBUpdateInstanceOperator to update an existing AlloyDB instance in Google Cloud.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/alloy_db.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nupdate_instance = AlloyDBUpdateInstanceOperator(\n    task_id=\"update_instance\",\n    project_id=GCP_PROJECT_ID,\n    cluster_id=CLUSTER_ID,\n    instance_id=INSTANCE_ID,\n    instance=ALLOYDB_INSTANCE,\n    region=GCP_ALLOYDB_REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Airflow Plugin as Python Package\nDESCRIPTION: This example demonstrates how to create an Airflow plugin as a Python package, including the plugin class definition and Flask blueprint setup.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/plugins.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# my_package/my_plugin.py\nfrom airflow.plugins_manager import AirflowPlugin\nfrom flask import Blueprint\n\n# Creating a flask blueprint to integrate the templates and static folder\nbp = Blueprint(\n    \"test_plugin\",\n    __name__,\n    template_folder=\"templates\",  # registers airflow/plugins/templates as a Jinja template folder\n    static_folder=\"static\",\n    static_url_path=\"/static/test_plugin\",\n)\n\n\nclass MyAirflowPlugin(AirflowPlugin):\n    name = \"my_namespace\"\n    flask_blueprints = [bp]\n```\n\n----------------------------------------\n\nTITLE: Passing Arguments to PythonOperator in Airflow\nDESCRIPTION: Demonstrates how to pass extra arguments to a Python function using the PythonOperator in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef multiply_by_2(number):\n    return number * 2\n\nmultiply_by_2_task = PythonOperator(\n    task_id='multiply_by_2',\n    python_callable=multiply_by_2,\n    op_kwargs={'number': 12},\n)\n```\n\n----------------------------------------\n\nTITLE: Copying Multiple Files Using a List in GCSToGCSOperator\nDESCRIPTION: Example showing how to copy multiple specific files by providing a list of source objects to the GCSToGCSOperator. This allows precise control over which files are transferred.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gcs.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncopy_files_with_list = GCSToGCSOperator(\n    task_id=\"gcs_to_gcs_list\",\n    source_bucket=BUCKET_1_SRC,\n    source_objects=[OBJECT_1, OBJECT_2],\n    destination_bucket=BUCKET_1_DST,\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Sentry Configuration in Airflow\nDESCRIPTION: Basic configuration example for enabling Sentry in Airflow's configuration file. This configuration enables Sentry integration and sets the Sentry DSN (Data Source Name) which is required for sending errors to Sentry.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/errors.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[sentry]\nsentry_on = True\nsentry_dsn = http://foo@sentry.io/123\n```\n\n----------------------------------------\n\nTITLE: Implementing Cluster Policy for DAG Filtering\nDESCRIPTION: Example of a cluster policy that skips DAGs based on tags using AirflowClusterPolicySkipDag exception. Used to control which DAGs are loaded in specific Airflow deployments.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef dag_policy(dag: DAG):\n    \"\"\"Skipping the DAG with `only_for_beta` tag.\"\"\"\n\n    if \"only_for_beta\" in dag.tags:\n        raise AirflowClusterPolicySkipDag(\n            f\"DAG {dag.dag_id} is not loaded on the production cluster, due to `only_for_beta` tag.\"\n        )\n```\n\n----------------------------------------\n\nTITLE: Getting Dataform Compilation Result in Python\nDESCRIPTION: This snippet shows how to get a Compilation Result using the DataformGetCompilationResultOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_get_compilation_result]\n# [END howto_operator_get_compilation_result]\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud SQL Connection Extra Parameters in JSON\nDESCRIPTION: Example JSON configuration for the 'extras' field in a Google Cloud SQL connection, including database type, project details, instance information, and proxy settings.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp_sql.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"database_type\": \"mysql\",\n   \"project_id\": \"example-project\",\n   \"location\": \"europe-west1\",\n   \"instance\": \"testinstance\",\n   \"use_proxy\": true,\n   \"sql_proxy_use_tcp\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Windows Batch Script for Edge Worker Configuration\nDESCRIPTION: This batch script sets environment variables for configuring the Edge Worker, including paths for DAGs and logs, API URL, executor type, and other critical settings. It also includes commented lines for proxy configuration if needed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/edge3/docs/install_on_windows.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@echo off\nset AIRFLOW__CORE__DAGS_FOLDER=dags\nset AIRFLOW__LOGGING__BASE_LOG_FOLDER=edge_logs\nset AIRFLOW__EDGE__API_URL=https://your-hostname-and-port/edge_worker/v1/rpcapi\nset AIRFLOW__CORE__EXECUTOR=airflow.providers.edge3.executors.edge_executor.EdgeExecutor\nset AIRFLOW__CORE__INTERNAL_API_SECRET_KEY=<steal this from your deployment...>\nset AIRFLOW__CORE__LOAD_EXAMPLES=False\nset AIRFLOW_ENABLE_AIP_44=true\n@REM Add if needed: set http_proxy=http://my-company-proxy.com:3128\n@REM Add if needed: set https_proxy=http://my-company-proxy.com:3128\nairflow edge worker --concurrency 4 --queues windows\n```\n\n----------------------------------------\n\nTITLE: Templating with PythonOperator in Airflow\nDESCRIPTION: Illustrates how to use Jinja templating with PythonOperator in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef render_sql(sql_template):\n    # Jinja template variables are rendered automatically\n    return sql_template\n\nrender_sql_task = PythonOperator(\n    task_id='render_sql',\n    python_callable=render_sql,\n    op_kwargs={\n        'sql_template': \"SELECT * FROM {{ params.table }} WHERE DATE = {{ data_interval_start.strftime('%Y-%m-%d') }}\"\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Waiting for Transfer Operation Status using Airflow Sensor and Google Cloud Storage Transfer Service in Python\nDESCRIPTION: This code uses Airflow's CloudDataTransferServiceJobStatusSensor to wait until at least one job operation matches the expected status within Google Cloud Storage Transfer Service. Required parameters include the job name, expected status, project ID, and connection information. The sensor can be templated for job and project identifiers, and outputs a sensor result on successful status match. Dependencies are the Airflow Google provider and valid credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nwait_for_operation = CloudDataTransferServiceJobStatusSensor(\n    task_id=\"wait_for_operation\",\n    job_name=TRANSFER_JOB_NAME,\n    expected_status=Operation.Status.SUCCESS,\n    project_id=PROJECT_ID,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"job_name\",\n    \"expected_status\",\n    \"project_id\",\n    \"gcp_conn_id\",\n)\n```\n\n----------------------------------------\n\nTITLE: Starting an EMR Notebook Execution using EmrStartNotebookExecutionOperator in Python\nDESCRIPTION: Illustrates initiating an EMR notebook execution using `EmrStartNotebookExecutionOperator`. Key parameters include `editor_id`, `relative_path` to the notebook file, the target `cluster_id`, the `service_role` for execution permissions, and optionally `notebook_params`. Requires an `aws_conn_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstart_notebook_execution = EmrStartNotebookExecutionOperator(\n    task_id=\"start_notebook_execution\",\n    editor_id=editor_id,\n    cluster_id=cluster_id,\n    relative_path=\"/path/to/notebook.ipynb\",\n    service_role=EMR_SERVICE_ROLE_NAME,\n    notebook_params='{\"my_variable\": \"my_value\"}',\n    notebook_execution_name=\"my-notebook-execution-{{ dag_run.id }}\",\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Registering Custom OpenLineage Extractors via Environment Variable\nDESCRIPTION: Sets the `AIRFLOW__OPENLINEAGE__EXTRACTORS` environment variable with a semicolon-separated list of custom extractor class import paths. This allows registering custom extractors for the OpenLineage integration as an alternative to INI configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_20\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__EXTRACTORS='full.path.to.ExtractorClass;full.path.to.AnotherExtractorClass'\n```\n\n----------------------------------------\n\nTITLE: Creating Deferrable GCS Object Update/Exists Sensor in Airflow\nDESCRIPTION: This code demonstrates how to create a GCS sensor that checks for object updates or existence asynchronously using Airflow's deferrable execution. The sensor monitors a specific object in a GCS bucket and only succeeds when the object exists or is updated.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nobject_update_exists_task_async = GCSObjectUpdateSensor(\n    task_id=\"object_update_exists_task_async\",\n    bucket=BUCKET_NAME,\n    object=FILE_NAME,\n    deferrable=True,\n    poke_interval=timedelta(seconds=5).total_seconds(),\n)\n```\n\n----------------------------------------\n\nTITLE: Running Go Pipeline with DataflowRunner using GCS File in Apache Beam\nDESCRIPTION: This example demonstrates executing a Go pipeline on Google Cloud Dataflow using a file from GCS. It includes Dataflow-specific parameters like project, region, staging and temp locations for the job.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_go_dataflow_runner_gcs_file = BeamRunGoPipelineOperator(\n    task_id=\"beam_task_go_dataflow_runner_gcs_file\",\n    go_file=\"{{ var.json.beam_variables.gcs_go_file_path }}\",\n    runner=\"DataflowRunner\",\n    pipeline_options={\n        \"output\": \"{{ var.json.beam_variables.output_path }}\",\n        \"temp_location\": \"{{ var.json.beam_variables.gcp_dataflow_temp }}\",\n        \"staging_location\": \"{{ var.json.beam_variables.gcp_dataflow_staging }}\",\n        \"project\": \"{{ var.json.beam_variables.gcp_project }}\",\n        \"region\": \"{{ var.json.beam_variables.gcp_region }}\",\n        \"job_name\": \"{{task.task_id}}\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Google Cloud Batch Job using CloudBatchDeleteJobOperator\nDESCRIPTION: Demonstrates the use of CloudBatchDeleteJobOperator to delete a job in Google Cloud Batch. This operator waits for the job to be deleted and pushes the deleted job's dictionary representation to XCom.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_batch.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndelete_job = CloudBatchDeleteJobOperator(\n    task_id=\"delete_job\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    job_name=JOB_NAME,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Amazon Redshift to Amazon S3 using RedshiftToS3Operator in Python\nDESCRIPTION: This code snippet demonstrates how to use the RedshiftToS3Operator to transfer data from an Amazon Redshift table to an Amazon S3 bucket. It includes the operator configuration with necessary parameters such as schema, table, s3_bucket, and s3_key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/redshift_to_s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nRedshiftToS3Operator(\n    task_id='transfer_redshift_to_s3',\n    schema='schema',\n    table='table',\n    s3_bucket='bucket',\n    s3_key='key',\n    redshift_conn_id='redshift_default',\n    aws_conn_id='aws_default',\n    unload_options=[\n        \"HEADER\",\n        \"FORMAT AS CSV\",\n        \"DELIMITER AS ','\",\n        \"ALLOWOVERWRITE\",\n        \"PARALLEL OFF\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Ingesting Data with PgVectorIngestOperator in Airflow (Python)\nDESCRIPTION: Demonstrates the use of PgVectorIngestOperator in Apache Airflow to insert data with vector embeddings into a PostgreSQL table with the pgvector extension enabled. Dependencies include Airflow, the pgvector extension on a PostgreSQL database, and proper configuration of database connections within the Airflow environment. Parameters involve specifying a SQL insert query that accommodates a vector column, with expected output being correctly inserted rows containing embeddings. The main constraint is the prerequisite for successful installation and activation of the pgvector extension.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/operators/pgvector.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.providers.pgvector.operators.pgvector import PgVectorIngestOperator\nfrom airflow.utils.dates import days_ago\n\ndef sample_vector():\n    # Just a dummy vector for demonstrate purposes\n    return [0.1, 0.2, 0.3]\n\ndef sample_data():\n    # Example list of rows, each with id and vector\n    return [\n        {\"id\": 1, \"embeddings\": [0.1, 0.2, 0.3]},\n        {\"id\": 2, \"embeddings\": [0.4, 0.5, 0.6]}\n    ]\n\ndef insert_query():\n    return \"\"\"\n    INSERT INTO my_vector_table (id, embeddings)\n    VALUES (%(id)s, %(embeddings)s)\n    \"\"\"\n\ndef get_pgvector_conn_id():\n    return \"my_postgres_conn\"\n\nwith DAG(\n    dag_id=\"example_pgvector_ingest\",\n    schedule_interval=None,\n    start_date=days_ago(1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    ingest_task = PgVectorIngestOperator(\n        task_id=\"ingest_vectors\",\n        sql=insert_query(),\n        data=sample_data(),\n        pgvector_conn_id=get_pgvector_conn_id(),\n    )\n```\n\n----------------------------------------\n\nTITLE: Getting Dataplex Data Profile Scan Results with Airflow Operator\nDESCRIPTION: Uses the DataplexGetDataProfileScanResultOperator to retrieve the results of a Dataplex Data Profile scan job. This operator fetches detailed information about a completed data profile scan job in Google Cloud Dataplex.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nget_data_scan_job = DataplexGetDataProfileScanResultOperator(\n    task_id=\"get_data_scan_job\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    data_scan_id=DATA_PROFILE_SCAN_ID,\n    job_id=\"{{ task_instance.xcom_pull('run_data_profile')['job']['name'].split('/')[-1] }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Skipping Tasks with BashOperator in Airflow\nDESCRIPTION: Illustrates how to use exit code 99 or a custom exit code to skip a task when using the BashOperator in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nt1 = BashOperator(\n    task_id=\"bash_skip_with_99\",\n    bash_command=\"echo 'hello world' && exit 99\",\n)\nt2 = BashOperator(\n    task_id=\"bash_skip_with_custom_code\",\n    bash_command=\"echo 'hello world' && exit 0\",\n    skip_on_exit_code=0,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating an AWS Fargate Profile using EksCreateFargateProfileOperator in Python\nDESCRIPTION: This Python snippet illustrates using `EksCreateFargateProfileOperator` to add an AWS Fargate profile to an existing EKS cluster. This allows scheduling pods onto Fargate. Requires `cluster_name`, `fargate_profile_name`, `fargate_pod_execution_role_arn`, and `selectors`. Needs an IAM role with `ec2.amazonaws.com` trust and specific policies (`AmazonEC2ContainerRegistryReadOnly`, `AmazonEKSWorkerNodePolicy`).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eks.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Example using EksCreateFargateProfileOperator\n# Assumes necessary imports and DAG context, and an existing EKS cluster\n\ncreate_fargate_profile = EksCreateFargateProfileOperator(\n    task_id=\"create_fargate_profile\",\n    cluster_name=\"my-existing-eks-cluster\",\n    fargate_profile_name=\"my-new-fargate-profile\",\n    fargate_pod_execution_role_arn=\"arn:aws:iam::123456789012:role/EksFargatePodExecutionRole\",\n    selectors=[{\"namespace\": \"fargate-apps\"}],\n    # other optional parameters like subnets...\n)\n\n# [END howto_operator_eks_create_fargate_profile]\n```\n\n----------------------------------------\n\nTITLE: Integer Parameter Configuration in Python\nDESCRIPTION: Example of configuring an integer parameter with minimum value and multipleOf validation rules.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/params.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nParam(42, type=\"integer\", minimum=14, multipleOf=7)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sensitive Field Names in Airflow INI\nDESCRIPTION: Configuration snippet showing how to extend the list of sensitive field names that Airflow will automatically mask in the core configuration section.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/mask-sensitive-values.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nsensitive_var_conn_names = comma,separated,sensitive,names\n```\n\n----------------------------------------\n\nTITLE: Custom Run ID Generation for Timetable\nDESCRIPTION: Implementation of custom run_id generation method for a timetable, showing how to create human-friendly date-based run IDs for scheduled DAG runs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/timetable.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef generate_run_id(\n    self,\n    *,\n    run_type: DagRunType,\n    logical_date: DateTime,\n    data_interval: DataInterval | None,\n    **extra,\n) -> str:\n    if run_type == DagRunType.SCHEDULED and data_interval:\n        return data_interval.end.format(\"YYYY-MM-DD dddd\")\n    return super().generate_run_id(\n        run_type=run_type, logical_date=logical_date, data_interval=data_interval, **extra\n    )\n```\n\n----------------------------------------\n\nTITLE: Starting RDS Database\nDESCRIPTION: Starts an Amazon RDS database instance or cluster using RdsStartDbOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nstart_db = RdsStartDbOperator(\n    task_id=\"start_db\",\n    db_identifier=DB_INSTANCE_NAME,\n    db_type=\"instance\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using FileSensor in Apache Airflow\nDESCRIPTION: This snippet demonstrates how to use the FileSensor operator to detect files appearing in the local filesystem. It requires a defined connection (default is 'fs_default') and specifies the file path to monitor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/file.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nFileSensor(\n    task_id=\"wait_for_file\",\n    filepath=\"/tmp/file.txt\",\n)\n```\n\n----------------------------------------\n\nTITLE: Copying Data from GCS to GCS Using Airflow and Google Cloud Storage Transfer Service in Python\nDESCRIPTION: This example shows how to use Airflow's CloudDataTransferServiceGCSToGCSOperator to copy data between two Google Cloud Storage buckets. It requires source and destination bucket details, credentials, and project specification as parameters, which can be templated for reusability. The code outputs a transfer operation status or job details and expects the Airflow Google provider and necessary IAM permissions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntransfer_gcs_to_gcs = CloudDataTransferServiceGCSToGCSOperator(\n    task_id=\"transfer_gcs_to_gcs\",\n    source_bucket=SOURCE_BUCKET,\n    destination_bucket=DESTINATION_BUCKET,\n    project_id=PROJECT_ID,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"source_bucket\",\n    \"destination_bucket\",\n    \"project_id\",\n    \"gcp_conn_id\",\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Entity Sentiment with Google Cloud Natural Language in Apache Airflow\nDESCRIPTION: This snippet demonstrates how to use the CloudNaturalLanguageAnalyzeEntitySentimentOperator to perform entity sentiment analysis on a given text.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/natural_language.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nanalyze_entity_sentiment = CloudNaturalLanguageAnalyzeEntitySentimentOperator(\n    task_id=\"analyze_entity_sentiment\",\n    document=document,\n)\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Local Filesystem to Amazon S3 using LocalFilesystemToS3Operator in Python\nDESCRIPTION: This code snippet demonstrates how to use the LocalFilesystemToS3Operator in Apache Airflow to transfer data from the local filesystem to an Amazon S3 bucket. It sets up the necessary parameters such as AWS connection ID, S3 bucket, local file path, and S3 key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/local_to_s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_transfer_local_to_s3]\nLocalFilesystemToS3Operator(\n    task_id=\"local_file_to_s3_task\",\n    filename=\"path/to/local/file\",\n    dest_key=\"path/in/s3/file.ext\",\n    dest_bucket=\"example-bucket\",\n    aws_conn_id=\"aws_default\",\n    replace=False,\n)\n# [END howto_transfer_local_to_s3]\n```\n\n----------------------------------------\n\nTITLE: Implementing Version-Compatible Task Instance Failure Listener in Python\nDESCRIPTION: Example showing how to implement a version-compatible listener that handles the error parameter added in Airflow 2.10.0.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/listeners.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom importlib.metadata import version\nfrom packaging.version import Version\nfrom airflow.listeners import hookimpl\n\nairflow_version = Version(version(\"apache-airflow\"))\nif airflow_version >= Version(\"2.10.0\"):\n\n    class ClassBasedListener:\n        @hookimpl\n        def on_task_instance_failed(self, previous_state, task_instance, error: None | str | BaseException):\n            # Handle error case here\n            pass\n\nelse:\n\n    class ClassBasedListener:  # type: ignore[no-redef]\n        @hookimpl\n        def on_task_instance_failed(self, previous_state, task_instance):\n            # Handle no error case here\n            pass\n```\n\n----------------------------------------\n\nTITLE: Accessing Asset Event Context\nDESCRIPTION: Demonstrates how to access and modify asset event information directly through the context.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@asset(schedule=None)\ndef write_to_s3(self, context):\n    context[\"outlet_events\"][self].extra = {\"row_count\": len(df)}\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote S3 Logging in airflow.cfg (INI)\nDESCRIPTION: Configures the `[logging]` section in `airflow.cfg` to enable remote logging to Amazon S3. It requires setting `remote_logging` to `True`, specifying the S3 bucket and path in `remote_base_log_folder`, and providing the `remote_log_conn_id` which corresponds to an existing Airflow AWS connection. The `encrypt_s3_logs` flag controls server-side encryption.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/logging/s3-task-handler.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\n# Airflow can store logs remotely in AWS S3. Users must supply a remote\n# location URL (starting with either 's3://...') and an Airflow connection\n# id that provides access to the storage location.\nremote_logging = True\nremote_base_log_folder = s3://my-bucket/path/to/logs\nremote_log_conn_id = my_s3_conn\n# Use server-side encryption for logs stored in S3\nencrypt_s3_logs = False\n```\n\n----------------------------------------\n\nTITLE: Templating Bash Commands with @task.bash in Airflow\nDESCRIPTION: Illustrates how to use Jinja templates to parameterize Bash commands when using the @task.bash decorator in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@task.bash\ndef bash_with_templates(execution_date):\n    return (\n        \"echo \\\"execution_date={{ execution_date }}\\\" && \"\n        \"echo \\\"execution_date.add(days=4)={{ execution_date.add(days=4) }}\\\" && \"\n        \"echo \\\"execution_date.add(days=4).isoformat()={{ execution_date.add(days=4).isoformat() }}\\\"\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Submitting a Job to a Google Cloud Dataproc Cluster\nDESCRIPTION: This code uses the DataprocSubmitJobOperator to submit a preconfigured job to a Dataproc cluster. The job_config parameter contains the specific job details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npyspark_task = DataprocSubmitJobOperator(\n    task_id=\"pyspark_task\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    job=PYSPARK_JOB,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing TimeSensor in Apache Airflow\nDESCRIPTION: This snippet demonstrates the use of TimeSensor to end sensing at a specific time. It creates two sensors: one that waits until 6:00 AM and another that waits until 10 PM.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/datetime.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nTimeSensor(task_id=\"wait_until_0600\", target_time=time(6, 0))\nTimeSensor(task_id=\"wait_until_2200\", target_time=time(22, 0))\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Amazon S3 to Amazon Redshift using S3ToRedshiftOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the S3ToRedshiftOperator to transfer data from an Amazon S3 file to an Amazon Redshift table. It specifies the S3 bucket, key, and Redshift table details for the transfer operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/s3_to_redshift.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nS3ToRedshiftOperator(\n    task_id=\"transfer_s3_to_redshift\",\n    schema=\"public\",\n    table=\"s3_to_redshift_table\",\n    s3_bucket=\"{{ BUCKET_NAME }}\",\n    s3_key=\"{{ S3_KEY }}\",\n    redshift_conn_id=\"aws_default\",\n    aws_conn_id=\"aws_default\",\n    copy_options=[\"csv\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Getting a Dataplex Data Quality Scan - Python\nDESCRIPTION: Uses DataplexGetDataQualityScanOperator to fetch configuration and metadata for a specific Data Quality scan. Requires project_id, region, and data quality scan ID for lookup. Result is a scan resource object representing the scan's details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"get_data_quality_scan = DataplexGetDataQualityScanOperator(\\n    task_id=\\\"get_data_quality_scan\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    data_quality_scan_id=DATA_QUALITY_SCAN_ID,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Creating IAM Role for Service Account (IRSA) using eksctl\nDESCRIPTION: This bash command creates an IAM role and associates it with a Kubernetes service account using eksctl. It attaches a specified IAM policy to the role, enabling fine-grained permissions for pods running on EKS.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\neksctl create iamserviceaccount --cluster=\"<EKS_CLUSTER_ID>\" --name=\"<SERVICE_ACCOUNT_NAME>\" --namespace=\"<NAMESPACE>\" --attach-policy-arn=\"<IAM_POLICY_ARN>\" --approve\n```\n\n----------------------------------------\n\nTITLE: Defining Transaction Options for Google Cloud Datastore in Python\nDESCRIPTION: This snippet demonstrates the definition of transaction options required by the CloudDatastoreBeginTransactionOperator. It specifies the read-write transactional configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nTRANSACTION_OPTIONS = {\"readWrite\": {}}\n```\n\n----------------------------------------\n\nTITLE: Deleting an Entry with CloudDataCatalogDeleteEntryOperator in Python\nDESCRIPTION: Uses CloudDataCatalogDeleteEntryOperator to delete an entry from Google Cloud Data Catalog.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndelete_entry = CloudDataCatalogDeleteEntryOperator(\n    task_id=\"delete_entry\",\n    location=LOCATION,\n    entry_group=ENTRY_GROUP_ID,\n    entry=ENTRY_ID,\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataplex Aspect Type using Airflow Python\nDESCRIPTION: This snippet demonstrates how to create a new Aspect Type in a specific location within Google Cloud Dataplex Catalog using the `DataplexCatalogCreateAspectTypeOperator` in an Airflow DAG. It takes a configuration dictionary (like the one shown previously) and references an external example file for the complete operator usage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_47\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_create_aspect_type]\n#     :end-before: [END howto_operator_dataplex_catalog_create_aspect_type]\n\n# This example uses DataplexCatalogCreateAspectTypeOperator with a configuration dictionary.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Deleting a Google Cloud Run Service using CloudRunDeleteServiceOperator in Airflow\nDESCRIPTION: Shows how to use the CloudRunDeleteServiceOperator to delete a Cloud Run service in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_run.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndelete_service = CloudRunDeleteServiceOperator(\n    task_id=\"delete_service\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    service_id=SERVICE_NAME\n)\n```\n\n----------------------------------------\n\nTITLE: Example Asana DAG Implementation in Python\nDESCRIPTION: Example DAG showing usage of various Asana operators including creating, finding, updating and deleting tasks. The DAG demonstrates proper configuration of Asana operators with connection IDs and task parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/docs/operators/asana.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.asana.operators.asana import (\n    AsanaCreateTaskOperator,\n    AsanaDeleteTaskOperator,\n    AsanaFindTaskOperator,\n    AsanaUpdateTaskOperator,\n)\n\nwith DAG(\n    dag_id='example_asana',\n    start_date=datetime(2022, 1, 1),\n    schedule_interval=None,\n    catchup=False,\n    tags=['example'],\n) as dag:\n    # [START create_task]\n    create_task = AsanaCreateTaskOperator(\n        task_id='create_task',\n        name='example task created by airflow',\n        conn_id='asana_conn_id',\n        task_parameters={'notes': 'test notes'},\n    )\n    # [END create_task]\n\n    # [START find_task]\n    find_task = AsanaFindTaskOperator(\n        task_id='find_task',\n        conn_id='asana_conn_id',\n        search_parameters={'name': 'example'},\n    )\n    # [END find_task]\n\n    # [START update_task]\n    update_task = AsanaUpdateTaskOperator(\n        task_id='update_task',\n        asana_task_gid=create_task.output,\n        conn_id='asana_conn_id',\n        task_parameters={'notes': 'updated notes'},\n    )\n    # [END update_task]\n\n    # [START delete_task]\n    delete_task = AsanaDeleteTaskOperator(\n        task_id='delete_task',\n        asana_task_gid=create_task.output,\n        conn_id='asana_conn_id',\n    )\n    # [END delete_task]\n\n    create_task >> find_task >> update_task >> delete_task\n```\n\n----------------------------------------\n\nTITLE: Starting AWS Step Functions Execution in Airflow\nDESCRIPTION: This snippet demonstrates how to use the StepFunctionStartExecutionOperator to start a new AWS Step Functions state machine execution. It can be run in deferrable mode by setting the 'deferrable' parameter to True.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/step_functions.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_step_function_start_execution]\n# [END howto_operator_step_function_start_execution]\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Pool Slots for Tasks with Different Weights\nDESCRIPTION: This example shows how to configure tasks to use different numbers of pool slots based on their computational requirements. It defines three BashOperator tasks: one heavy task using 2 slots and two light tasks using 1 slot each, all in the same 'maintenance' pool.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/pools.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nBashOperator(\n    task_id=\"heavy_task\",\n    bash_command=\"bash backup_data.sh\",\n    pool_slots=2,\n    pool=\"maintenance\",\n)\n\nBashOperator(\n    task_id=\"light_task1\",\n    bash_command=\"bash check_files.sh\",\n    pool_slots=1,\n    pool=\"maintenance\",\n)\n\nBashOperator(\n    task_id=\"light_task2\",\n    bash_command=\"bash remove_files.sh\",\n    pool_slots=1,\n    pool=\"maintenance\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring SMTP Notification Callbacks in Airflow DAG and Task - Python\nDESCRIPTION: This Python snippet demonstrates how to use the send_smtp_notification utility from airflow.providers.smtp.notifications.smtp to send emails when a DAG or task fails. Dependencies include Apache Airflow, specifically the BashOperator and the SMTP provider, as well as the datetime standard library. The DAG and task are configured with on_failure_callback callbacks that trigger email notifications to specified recipients with dynamic subjects and content. The main parameters include from_email, to, subject, and html_content. This setup requires a properly configured Airflow SMTP backend. Limitations include proper SMTP server credentials/configuration and template variables supported by Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/smtp/docs/notifications/smtp_notifier_howto_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.smtp.notifications.smtp import send_smtp_notification\n\nwith DAG(\n    dag_id=\"smtp_notifier\",\n    schedule=None,\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    on_failure_callback=[\n        send_smtp_notification(\n            from_email=\"someone@mail.com\",\n            to=\"someone@mail.com\",\n            subject=\"[Error] The dag {{ dag.dag_id }} failed\",\n            html_content=\"debug logs\",\n        )\n    ],\n):\n    BashOperator(\n        task_id=\"mytask\",\n        on_failure_callback=[\n            send_smtp_notification(\n                from_email=\"someone@mail.com\",\n                to=\"someone@mail.com\",\n                subject=\"[Error] The Task {{ ti.task_id }} failed\",\n                html_content=\"debug logs\",\n            )\n        ],\n        bash_command=\"fail\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Listing Google Cloud Tasks Tasks in Python\nDESCRIPTION: This snippet shows how to list all tasks in a specific Google Cloud Tasks queue using the CloudTasksTasksListOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# [START list_tasks]\nCloudTasksTasksListOperator(\n    task_id=\"tasks_list\",\n    location=LOCATION,\n    queue_name=QUEUE_ID,\n).execute(context=context)\n# [END list_tasks]\n```\n\n----------------------------------------\n\nTITLE: Refreshing PowerBI Dataset with MSGraphAsyncOperator (Python)\nDESCRIPTION: This example demonstrates use of the MSGraphAsyncOperator to initiate a refresh on a PowerBI dataset through the Microsoft Graph API. Requires specification of the group and dataset identifiers in the endpoint. Proper permissions and access tokens are needed for this operation. The operator triggers a refresh and returns status information regarding the refresh process.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/msgraph.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrefresh_dataset = MSGraphAsyncOperator(\n    task_id=\"refresh_powerbi_dataset\",\n    endpoint=\"groups/{group_id}/datasets/{dataset_id}/refreshes\",\n    method=\"POST\",\n    msgraph_conn_id=\"msgraph_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFusion Pipeline with CloudDataFusionCreatePipelineOperator in Python\nDESCRIPTION: This snippet demonstrates the use of CloudDataFusionCreatePipelineOperator to create a pipeline in Google Cloud DataFusion. It includes parameters for instance name, pipeline name, pipeline definition, and runtime arguments.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datafusion.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nCloudDataFusionCreatePipelineOperator(\n    task_id=\"create_pipeline\",\n    pipeline_name=PIPELINE_NAME,\n    pipeline={\n        \"name\": \"test\",\n        \"description\": \"test pipeline\",\n        \"artifact\": {\"name\": \"artifact\", \"version\": \"1.2.3\"},\n    },\n    instance_name=INSTANCE_NAME,\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    namespace=\"default\",\n    runtime_args={\"arg1\": \"a\", \"arg2\": \"b\"},\n    retry=RETRY,\n    timeout=TIMEOUT,\n    metadata=METADATA,\n)\n```\n\n----------------------------------------\n\nTITLE: Checking S3 Keys with Regex Pattern using Deferrable S3KeySensor in Python\nDESCRIPTION: This example shows how to use the S3KeySensor with a regular expression pattern to check for files in an S3 bucket. It operates in deferrable mode for improved efficiency.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/s3.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nS3KeySensor(\n    task_id=\"wait_for_keys_regex\",\n    bucket_key=r\"{{ ds_nodash }}/my_file_\\d+.csv\",\n    bucket_name=\"my-bucket\",\n    aws_conn_id=\"aws_default\",\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing WasbBlobSensor in Apache Airflow\nDESCRIPTION: This snippet demonstrates how to use the WasbBlobSensor to wait for a specific blob to arrive in Azure Blob Storage. It specifies the container name, blob name, and checks for the blob's existence every 60 seconds.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/sensors/wasb_sensors.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwasb_blob_sensor = WasbBlobSensor(\n    container_name='wasb_sensor_container',\n    blob_name='wasb_sensor_blob',\n    poke_interval=60,\n    timeout=60 * 60,\n    task_id='wasb_sensor_task',\n)\n```\n\n----------------------------------------\n\nTITLE: Transforming Expanding Data in Python for Apache Airflow\nDESCRIPTION: Shows how to use the map() function to transform output data format for task mapping, especially useful for non-TaskFlow operators. It also demonstrates handling skipped tasks using AirflowSkipException.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.exceptions import AirflowSkipException\n\nlist_filenames = S3ListOperator(...)  # Unchanged.\n\n\ndef create_copy_kwargs(filename):\n    if filename.rsplit(\".\", 1)[-1] not in (\"json\", \"yml\"):\n        raise AirflowSkipException(f\"skipping {filename!r}; unexpected suffix\")\n    return {\n        \"source_bucket_key\": filename,\n        \"dest_bucket_key\": filename,\n        \"dest_bucket_name\": \"my_other_bucket\",\n    }\n\n\ncopy_kwargs = list_filenames.output.map(create_copy_kwargs)\n\n# Unchanged.\ncopy_filenames = S3CopyObjectOperator.partial(...).expand_kwargs(copy_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Reading Messages from Amazon SQS using SqsSensor\nDESCRIPTION: Example showing how to read messages from an Amazon SQS queue using the SqsSensor. The sensor can be run in deferrable mode by setting the deferrable parameter to True.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sqs.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nread_from_queue = SqsSensor(\n    task_id=\"read_from_queue\",\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Starting RDS Export Task to S3\nDESCRIPTION: Exports an Amazon RDS snapshot to S3 using RDSStartExportTaskOperator. Requires an IAM role with S3 bucket access.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstart_export = RDSStartExportTaskOperator(\n    task_id=\"start_export\",\n    export_task_identifier=EXPORT_TASK_NAME,\n    source_arn=DB_SNAPSHOT_ARN,\n    s3_bucket_name=S3_BUCKET_NAME,\n    iam_role_arn=IAM_ROLE_ARN,\n    kms_key_id=KMS_KEY_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from MySQL to GCS using MySQLToGCSOperator\nDESCRIPTION: Example demonstrating how to configure and use MySQLToGCSOperator to upload data from a MySQL database to Google Cloud Storage. The operator supports optional data compression and flexible configuration options for the transfer process.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/mysql_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_mysql_to_gcs]\n[END howto_operator_mysql_to_gcs]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Settings in Airflow\nDESCRIPTION: Basic configuration settings for enabling OpenTelemetry in Airflow's configuration file (airflow.cfg). Includes settings for host, port, application name, SSL, and task logging.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/traces.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[traces]\notel_on = True\notel_host = localhost\notel_port = 8889\notel_application = airflow\notel_ssl_active = False\notel_task_log_event = True\n```\n\n----------------------------------------\n\nTITLE: Listing Google Cloud Batch Jobs using CloudBatchListJobsOperator\nDESCRIPTION: Shows how to use CloudBatchListJobsOperator to list jobs in Google Cloud Batch. Optional parameters include 'limit' to restrict the number of jobs returned and 'filter' to list only jobs matching specific criteria.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_batch.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlist_jobs = CloudBatchListJobsOperator(\n    task_id=\"list_jobs\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Getting Apache Kafka Consumer Group Info with ManagedKafkaGetConsumerGroupOperator in Python\nDESCRIPTION: This code shows how to retrieve information about an Apache Kafka consumer group using the ManagedKafkaGetConsumerGroupOperator. It requires the project ID, region, cluster, and consumer group.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nget_consumer_group = ManagedKafkaGetConsumerGroupOperator(\n    task_id=\"get_consumer_group\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n    consumer_group=CONSUMER_GROUP,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL-enabled Cloud SQL Connection in Python\nDESCRIPTION: This snippet demonstrates how to configure an SSL-enabled connection for Google Cloud SQL in Apache Airflow. It includes setting up the connection parameters and SSL certificate paths.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_cloudsql_query_connections]\n# [END howto_operator_cloudsql_query_connections]\n```\n\n----------------------------------------\n\nTITLE: Using TelegramOperator in Apache Airflow\nDESCRIPTION: This code snippet demonstrates how to use the TelegramOperator to send a message to a Telegram group or chat. It includes setting up the connection and specifying the message content.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_telegram]\nfrom airflow.models import DAG\nfrom airflow.providers.telegram.operators.telegram import TelegramOperator\nfrom airflow.utils.dates import days_ago\n\nwith DAG(\n    'example_telegram',\n    default_args={'owner': 'airflow'},\n    schedule_interval=None,\n    start_date=days_ago(1),\n    tags=['example'],\n) as dag:\n    # [START howto_operator_telegram_task]\n    send_message_telegram_task = TelegramOperator(\n        task_id='send_message_telegram',\n        telegram_conn_id='telegram_default',\n        chat_id='{{ var.value.telegram_chat_id }}',\n        text='Hello from Airflow!',\n        dag=dag,\n    )\n    # [END howto_operator_telegram_task]\n# [END howto_operator_telegram]\n```\n\n----------------------------------------\n\nTITLE: Defining Op Permissions - Flask AppBuilder (FAB) - Python\nDESCRIPTION: This Python code snippet describes the additional permissions attributed to the Op (operator) role, which build upon all previous role permissions. The Op role typically receives broader edit/create/delete permissions, suitable for advanced operational users without full administrative rights. Dependencies: FAB SecurityManager and correct Airflow role mappings are required. Adds privileges such as configuration and editing audit/user/role information.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/access-control.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nop_perms = user_perms + [\n    (\"Configurations\", \"can_read\"),\n    (\"Configurations\", \"can_edit\"),\n    (\"Import Errors\", \"can_delete\"),\n    (\"Audit Logs\", \"can_edit\"),\n    (\"Users\", \"can_edit\"),\n    (\"Roles\", \"can_edit\"),\n    (\"Pools\", \"can_import\"),\n]\n\n```\n\n----------------------------------------\n\nTITLE: Executing Single Statement with RedshiftDataOperator in Python\nDESCRIPTION: Example showing how to use RedshiftDataOperator to execute a single SQL statement against an Amazon Redshift cluster. The operator uses AWS API directly without requiring a Postgres connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/redshift/redshift_data.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Begin execution\nRedshiftDataOperator(\n    task_id='select_task',\n    sql='select * from public.test',\n    database='dev',\n    db_user='awsuser',\n    cluster_identifier='redshift_cluster_1',\n    aws_conn_id='aws_conn_id',\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting Azure Service Bus Queue in Python\nDESCRIPTION: This snippet shows how to use the AzureServiceBusDeleteQueueOperator to delete an Azure Service Bus queue.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/asb.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_delete_service_bus_queue]\n# [END howto_operator_delete_service_bus_queue]\n```\n\n----------------------------------------\n\nTITLE: Using Custom Waiter for Async Redshift Cluster Pause Check in Python\nDESCRIPTION: Shows how to use a custom waiter to asynchronously check for a paused Redshift cluster state using the get_waiter function from the hook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/triggers/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync with await self.redshift_hook.get_async_conn() as client:\n    waiter = self.redshift_hook.get_waiter(\"cluster_paused\", deferrable=True, client=client)\n    await waiter.wait(\n        ClusterIdentifier=self.cluster_identifier,\n        WaiterConfig={\n            \"Delay\": int(self.poll_interval),\n            \"MaxAttempts\": int(self.max_attempt),\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Executing Snowflake SQL with SQLExecuteQueryOperator in Python\nDESCRIPTION: This Python snippet demonstrates how to use the SQLExecuteQueryOperator to run a simple SQL query ('SELECT 1') against a Snowflake database. It requires an Airflow connection configured with the ID 'snowflake_conn_id' containing credentials and connection details (login, password, schema, warehouse, account, etc.). The 'sql' parameter specifies the SQL command to be executed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/operators/snowflake.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsnowflake_query = SQLExecuteQueryOperator(\n    task_id=\"snowflake_query\",\n    conn_id=SNOWFLAKE_CONN_ID,\n    sql=\"SELECT 1\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Using CloudTranslateSpeechOperator in Airflow DAG (Python)\nDESCRIPTION: Demonstrates the instantiation and usage of the `CloudTranslateSpeechOperator` within an Airflow DAG. It passes the previously defined configuration, audio source, target language, and other parameters to the operator task.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate_speech.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntranslate_speech = CloudTranslateSpeechOperator(\n    task_id=\"translate_speech\",\n    config=CONFIG,\n    audio=AUDIO,\n    target_language=TARGET_LANGUAGE,\n    format_=FORMAT,\n    source_language=SOURCE_LANGUAGE,\n    model=MODEL,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating AutoML Translation Model with TranslateCreateModelOperator in Python\nDESCRIPTION: Illustrates using the `TranslateCreateModelOperator` in an Airflow DAG to create an AutoML translation model using the V3 API. It requires a `model` dictionary defining the model name and linked dataset ID, along with the `project_id` and `location`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncreate_model = TranslateCreateModelOperator(\n    task_id=\"create_model\",\n    model=MODEL,\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Dataform Workflow Invocation Actions in Python\nDESCRIPTION: This snippet demonstrates how to query Workflow Invocation Actions using the DataformQueryWorkflowInvocationActionsOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_query_workflow_invocation_actions]\n# [END howto_operator_query_workflow_invocation_actions]\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos in Airflow.cfg\nDESCRIPTION: INI configuration settings to enable Kerberos authentication in Airflow's configuration file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/kerberos.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nsecurity = kerberos\n\n[kerberos]\nkeytab = /etc/airflow/airflow.keytab\nreinit_frequency = 3600\nprincipal = airflow\n```\n\n----------------------------------------\n\nTITLE: Mocking Airflow Variables in Tests\nDESCRIPTION: Shows how to mock Airflow variables in unit tests using environment variables, which avoids the need to interact with the database. Uses unittest.mock.patch.dict to simulate the presence of variables during test execution.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nwith mock.patch.dict(\"os.environ\", AIRFLOW_VAR_KEY=\"env-value\"):\n    assert \"env-value\" == Variable.get(\"key\")\n```\n\n----------------------------------------\n\nTITLE: Starting a Job on GKE with Kueue using GKEStartKueueJobOperator in Python\nDESCRIPTION: This snippet illustrates the use of `GKEStartKueueJobOperator` to run a Job on a GKE cluster where Kueue (a Kubernetes-native job queueing system) is enabled. This operator is specifically designed for environments utilizing Kueue for job management.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/kubernetes_engine/example_kubernetes_engine_kueue.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_kueue_start_job]\n    :end-before: [END howto_operator_kueue_start_job]\n```\n\n----------------------------------------\n\nTITLE: Ingesting Vectors with QdrantIngestOperator in Airflow Python DAG\nDESCRIPTION: This example demonstrates how to use the QdrantIngestOperator within an Airflow DAG to ingest a batch of vectors into a specified Qdrant collection. The operator requires the 'conn_id' parameter for specifying the Airflow connection configuration to the Qdrant instance, 'collection_name' to target the desired collection, and 'vectors' as a list of embeddings or documents (with optional metadata). Inputs include the connection ID, collection name, and a list of vectors; output is the successful ingestion of these vectors into Qdrant. Dependencies include Airflow (with the Qdrant provider installed), and a configured Qdrant connection in Airflow. There are restrictions on the format of the vectors as required by Qdrant and provider version compatibility.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/qdrant/docs/operators/qdrant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nQdrantIngestOperator(\n    task_id=\"qdrant_ingest\",\n    conn_id=\"qdrant_default\",\n    collection_name=\"my_collection\",\n    vectors=[\n        {\n            \"id\": \"vector-1\",\n            \"vector\": [0.71, 1.3, -2.9],\n            \"payload\": {\"source\": \"example text 1\"},\n        },\n        {\n            \"id\": \"vector-2\",\n            \"vector\": [-0.2, 0.0, 2.1],\n            \"payload\": {\"source\": \"example text 2\"},\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Running Python Pipeline with DataflowRunner using GCS File in Apache Beam\nDESCRIPTION: This example demonstrates executing a Python pipeline on Google Cloud Dataflow using a file from GCS. It includes Dataflow-specific parameters like project, staging location, temp location, and region for the job.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_dataflow_runner_gcs_file = BeamRunPythonPipelineOperator(\n    task_id=\"beam_task_dataflow_runner_gcs_file\",\n    py_file=\"{{ var.json.beam_variables.gcs_file_path }}\",\n    runner=\"DataflowRunner\",\n    pipeline_options={\n        \"output\": \"{{ var.json.beam_variables.output_path }}\",\n        \"temp_location\": \"{{ var.json.beam_variables.gcp_dataflow_temp }}\",\n        \"staging_location\": \"{{ var.json.beam_variables.gcp_dataflow_staging }}\",\n        \"project\": \"{{ var.json.beam_variables.gcp_project }}\",\n        \"region\": \"{{ var.json.beam_variables.gcp_region }}\",\n        \"job_name\": \"{{task.task_id}}\",\n    },\n    py_interpreter=\"python3\",\n    py_system_site_packages=False,\n    py_requirements=[\n        \"apache-beam[gcp]==2.44.0\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Searching YouTube Videos and Saving Results to Amazon S3 using GoogleApiToS3Operator in Python\nDESCRIPTION: This snippet shows how to use GoogleApiToS3Operator to search for YouTube videos within a specific time range and channel, then save the results to Amazon S3. It demonstrates the use of xcom to pass data between tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/google_api_to_s3.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsearch_youtube_videos = GoogleApiToS3Operator(\n    task_id=\"search_youtube_videos\",\n    google_api_service_name=\"youtube\",\n    google_api_service_version=\"v3\",\n    google_api_endpoint_path=\"search\",\n    google_api_endpoint_params={\n        \"part\": \"id,snippet\",\n        \"channelId\": YOUTUBE_CHANNEL_ID,\n        \"maxResults\": 50,\n        \"publishedAfter\": YOUTUBE_VIDEO_PUBLISHED_AFTER,\n        \"publishedBefore\": YOUTUBE_VIDEO_PUBLISHED_BEFORE,\n        \"type\": \"video\",\n    },\n    s3_destination_key=f\"s3://{S3_BUCKET_NAME}/youtube_videos_search_{{ ds_nodash }}.json\",\n    google_api_response_via_xcom=\"youtube_search\",\n    delegate_to=GOOGLE_DELEGATE_TO,\n    gcp_conn_id=GOOGLE_GCP_CONN_ID,\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating BigQuery Data Transfer Configuration in Python\nDESCRIPTION: This snippet demonstrates how to create a BigQuery Data Transfer configuration using the BigQueryCreateDataTransferOperator in Apache Airflow. It sets up the transfer parameters and creates the configuration, with the option to disable automatic scheduling.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery_dts.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntransfer_config = {\n    \"destination_dataset_id\": DATASET_ID,\n    \"display_name\": \"test_transfer_config\",\n    \"data_source_id\": \"google_cloud_storage\",\n    \"params\": {\n        \"data_path_template\": \"gs://cloud-samples-data/bigquery/us-states/us-states.csv\",\n        \"destination_table_name_template\": \"us_states\",\n        \"file_format\": \"CSV\",\n        \"max_bad_records\": \"0\",\n        \"ignore_unknown_values\": \"true\",\n        \"field_delimiter\": \",\",\n        \"skip_leading_rows\": \"1\",\n        \"allow_quoted_newlines\": \"true\",\n        \"allow_jagged_rows\": \"false\",\n        \"delete_source_files\": \"false\",\n    },\n    \"schedule_options\": {\"disable_auto_scheduling\": True},\n}\n```\n\nLANGUAGE: python\nCODE:\n```\ncreate_transfer = BigQueryCreateDataTransferOperator(\n    task_id=\"create_transfer\",\n    transfer_config=transfer_config,\n    project_id=PROJECT_ID,\n    location=LOCATION,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating EKS Cluster and Nodegroup Simultaneously using EksCreateClusterOperator in Python\nDESCRIPTION: This Python snippet shows how to use `EksCreateClusterOperator` to create both an EKS cluster and an associated managed node group in a single step. It requires an AWS IAM role with `ec2.amazonaws.com` and `eks.amazonaws.com` in Trusted Relationships, and `AmazonEC2ContainerRegistryReadOnly`, `AmazonEKSClusterPolicy`, and `AmazonEKSWorkerNodePolicy` attached.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eks.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Example using EksCreateClusterOperator with nodegroup creation\n# Assumes necessary imports and DAG context\n\ncreate_cluster_and_nodegroup = EksCreateClusterOperator(\n    task_id=\"create_cluster_and_nodegroup\",\n    cluster_name=\"my-eks-cluster-with-nodegroup\",\n    cluster_role_arn=\"arn:aws:iam::123456789012:role/EksClusterRole\",\n    resources_vpc_config={\n        \"subnetIds\": [\"subnet-xxxxxxxxxxxxxxxxx\", \"subnet-yyyyyyyyyyyyyyyyy\"],\n        \"securityGroupIds\": [\"sg-xxxxxxxxxxxxxxxxx\"],\n    },\n    # Nodegroup configuration\n    nodegroup_name=\"my-managed-nodegroup\",\n    nodegroup_role_arn=\"arn:aws:iam::123456789012:role/EksNodegroupRole\",\n    # other cluster and nodegroup parameters...\n)\n\n# [END howto_operator_eks_create_cluster_with_nodegroup]\n```\n\n----------------------------------------\n\nTITLE: Copying Single File from SFTP to GCS using Python\nDESCRIPTION: Example showing how to copy a single file from SFTP to Google Cloud Storage using the SFTPToGCSOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/sftp_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncopy_single_file = SFTPToGCSOperator(\n    task_id=\"copy-single-file\",\n    source_path=\"path/to/{{ds}}/data.csv\",\n    destination_bucket=BUCKET_NAME,\n    destination_path=\"data.csv\",\n    sftp_conn_id=\"sftp_default\",\n    gcp_conn_id=\"google_cloud_default\",\n    dag=dag\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting Dataplex Entry using Airflow Python\nDESCRIPTION: This snippet demonstrates how to delete an existing Entry from a specific Entry Group in Google Cloud Dataplex Catalog using the `DataplexCatalogDeleteEntryOperator` within an Airflow DAG. It references an external example file for the specific implementation details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_54\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_delete_entry]\n#     :end-before: [END howto_operator_dataplex_catalog_delete_entry]\n\n# This example uses DataplexCatalogDeleteEntryOperator.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Customizing Amazon Bedrock Model via Fine-Tuning Operator (Python)\nDESCRIPTION: Creates a fine-tuning job for an existing Bedrock model using BedrockCustomizeModelOperator in Airflow. This snippet triggers asynchronous customization with specified job details and monitoring options. Dependencies include access to Airflow AWS Provider and relevant data storage for training.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncustomize_model_task = BedrockCustomizeModelOperator(\n    task_id=\"fine_tune_model\",\n    model_id=\"amazon.titan-text-lite-v1\",\n    training_data_s3_uri=\"s3://my-bucket/training-data.jsonl\",\n    validation_data_s3_uri=\"s3://my-bucket/validation-data.jsonl\",\n    job_name=\"titan-finetune-job-1\",\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Basic Asset-Based Scheduling in Python\nDESCRIPTION: Demonstrates how to define a task that updates an asset and schedule another DAG based on that asset update.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/asset-scheduling.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import DAG, Asset\n\nwith DAG(...):\n    MyOperator(\n        # this task updates example.csv\n        outlets=[Asset(\"s3://asset-bucket/example.csv\")],\n        ...,\n    )\n\n\nwith DAG(\n    # this DAG should be run when example.csv is updated (by dag1)\n    schedule=[Asset(\"s3://asset-bucket/example.csv\")],\n    ...,\n):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Task Instance Running Listener in Python\nDESCRIPTION: Code example showing how to implement a listener for task instance running events.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/listeners.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@hookimpl\ndef on_task_instance_running(previous_state, task_instance):\n    \"\"\"Listen for task instance running events.\"\"\"\n    print(f\"Task instance {task_instance} switched from {previous_state} to running\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a DAG Cluster Policy to Enforce Tags\nDESCRIPTION: A policy function that ensures each DAG has at least one tag defined, raising an AirflowClusterPolicyViolation if the requirement is not met.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/cluster-policies.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef dag_policy(dag):\n    \"\"\"\n    Ensure that every dag has at least one tag defined.\n    \"\"\"\n    if not dag.tags:\n        raise AirflowClusterPolicyViolation(\n            f\"DAG {dag.dag_id} has no tags. At least one tag is required\"\n        )\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Data from GCS to Presto Table with Airflow Operator (Python)\nDESCRIPTION: Demonstrates how to use the Airflow GCSToPrestoOperator to transfer a CSV file from Google Cloud Storage to a Presto table. The operator requires the CSV to be headerless and formatted according to the target table's columns. Optionally, the schema can be provided as a tuple or list of strings, or as a JSON file path within the same GCS bucket. Key parameters include the source bucket and object, Presto connection ID, target table, and schema. The operator depends on Airflow, the Presto provider package, and proper authentication for Google Cloud and Presto. The expected input is a CSV file; the output is the population of the specified Presto table. Note: CSV must match the table schema, and errors may occur if schema mismatches exist.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/presto/docs/gcs_to_presto.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_gcs_to_presto_data_pipeline\",\n    schedule_interval=None,\n    start_date=days_ago(1),\n    tags=[\"example\"],\n) as dag:\n    load_csv = GCSToPrestoOperator(\n        task_id=\"gcs_csv_to_presto_table\",\n        bucket=\"example-bucket\",\n        source_objects=[\"path/to/example.csv\"],\n        presto_conn_id=\"presto_default\",\n        schema=[\"col1\", \"col2\", \"col3\"],  # OR schema=\"schema.json\" (path in GCS)\n        table=\"example_table\",\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Airflow DAG with PythonOperator\nDESCRIPTION: This snippet shows how to define a basic Airflow DAG using the PythonOperator. It includes two Python functions and demonstrates how to set up the DAG with various parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import DAG\nfrom airflow.providers.standard.operators.python import PythonOperator\nimport pendulum\n\n\ndef get_task_id():\n    print(\"Executing 1\")\n    return \"print_array_task\"  # <- is that code going to be executed? YES\n\n\ndef get_array():\n    print(\"Executing 2\")\n    return [1, 2, 3]  # <- is that code going to be executed? NO\n\n\nwith DAG(\n    dag_id=\"example_python_operator\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    operator = PythonOperator(\n        task_id=get_task_id(),\n        python_callable=get_array,\n        dag=dag,\n    )\n```\n\n----------------------------------------\n\nTITLE: Getting an Entry with CloudDataCatalogGetEntryOperator in Python\nDESCRIPTION: Uses CloudDataCatalogGetEntryOperator to get an entry from Google Cloud Data Catalog using Project ID, Entry Group ID, and Entry ID. The result is saved to XCom for use by other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nget_entry = CloudDataCatalogGetEntryOperator(\n    task_id=\"get_entry\",\n    location=LOCATION,\n    entry_group=ENTRY_GROUP_ID,\n    entry=ENTRY_ID,\n    project_id=PROJECT_ID,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nget_entry_result = get_entry.output\n```\n\n----------------------------------------\n\nTITLE: Writing File to Dataform Workspace in Python\nDESCRIPTION: This snippet shows how to write a file with given content to a specified workspace using the DataformWriteFileOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_write_file]\n# [END howto_operator_write_file]\n```\n\n----------------------------------------\n\nTITLE: Waiting for AWS DMS Task Completion using DmsTaskCompletedSensor in Python\nDESCRIPTION: This snippet shows how to use the DmsTaskCompletedSensor to pause an Airflow DAG until a specific AWS DMS replication task reaches a completed status (e.g., 'stopped', 'failed', 'deleted'). It requires the ARN of the task to monitor. Depends on the airflow.providers.amazon.aws.sensors.dms module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dms.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Example using DmsTaskCompletedSensor\n# from airflow.providers.amazon.aws.sensors.dms import DmsTaskCompletedSensor\n\n# wait_for_task_completion = DmsTaskCompletedSensor(\n#     task_id='wait_for_dms_task_completed',\n#     replication_task_arn='arn:aws:dms:us-east-1:123456789012:task:ABCDEF123GHIJKL',\n#     # target_statuses=['stopped', 'failed', 'deleted'] # Optional: Specify target states\n#     # ... other sensor parameters like poke_interval, timeout\n# )\n```\n\n----------------------------------------\n\nTITLE: Templated Fields for CloudTranslateSpeechOperator in Python\nDESCRIPTION: Lists the fields of the `CloudTranslateSpeechOperator` that support Jinja templating. This allows for dynamic values to be passed to these parameters at runtime using Airflow's templating engine.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate_speech.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields: Sequence[str] = (\n    \"audio\",\n    \"config\",\n    \"target_language\",\n    \"format_\",\n    \"source_language\",\n    \"model\",\n    \"gcp_conn_id\",\n    \"impersonation_chain\",\n)\n```\n\n----------------------------------------\n\nTITLE: Monitoring Data Fusion Pipeline State with Airflow Sensor\nDESCRIPTION: This snippet shows how to use a sensor to monitor the state of a Data Fusion pipeline. The CloudDataFusionPipelineStateSensor checks if a pipeline has reached the target state, with parameters for project ID, location, instance, pipeline, and expected state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datafusion.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npipeline_sensor = CloudDataFusionPipelineStateSensor(\n    task_id=\"pipeline_sensor\",\n    pipeline_name=PIPELINE_NAME,\n    pipeline_id=PIPELINE_ID,\n    instance_name=INSTANCE_NAME,\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    target_statuses={\"COMPLETED\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Overriding Airflow Values in YAML\nDESCRIPTION: Example of overriding default Airflow chart values in values.yaml\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/extending-the-chart.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nairflow:\n  executor: KubernetesExecutor\n```\n\n----------------------------------------\n\nTITLE: Adding Airflow Provider and Apt Dependency (Dockerfile)\nDESCRIPTION: Dockerfile example showing how to add an Airflow provider (apache-spark) that requires both a system package (java installed via apt) and a Python package (installed via pip). It demonstrates switching between root and airflow users for installations.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_9\n\nLANGUAGE: Dockerfile\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/add-providers/Dockerfile\n    :language: Dockerfile\n    :start-after: [START Dockerfile]\n    :end-before: [END Dockerfile]\n```\n\n----------------------------------------\n\nTITLE: Creating AWS DMS Serverless Replication Config using DmsCreateReplicationConfigOperator\nDESCRIPTION: This snippet demonstrates using the DmsCreateReplicationConfigOperator to set up a configuration for an AWS DMS serverless replication. It requires parameters defining the replication settings, source/target endpoints, compute configuration, and a unique identifier. Depends on the airflow.providers.amazon.aws.operators.dms module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dms.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Example using DmsCreateReplicationConfigOperator\n# from airflow.providers.amazon.aws.operators.dms import DmsCreateReplicationConfigOperator\n\n# create_repl_config = DmsCreateReplicationConfigOperator(\n#     task_id=\"create_dms_serverless_replication_config\",\n#     replication_config_identifier=\"my-serverless-config\",\n#     source_endpoint_arn=\"arn:aws:dms:us-east-1:123456789012:endpoint:ABCDEF123456\",\n#     target_endpoint_arn=\"arn:aws:dms:us-east-1:123456789012:endpoint:GHIJKL789012\",\n#     compute_config={\"MaxCapacityUnits\": 2}, # Example compute config\n#     replication_type=\"full-load\", # Or 'cdc', 'full-load-and-cdc'\n#     table_mappings={}, # Specify table mappings JSON\n#     # ... other parameters\n# )\n```\n\n----------------------------------------\n\nTITLE: Manual Template Rendering in Python Callable\nDESCRIPTION: Shows how to manually render templates in a callable function used for complex templating with BashOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef build_complex_command(context, jinja_env):\n    with open(\"file.csv\") as f:\n        data = do_complex_things(f)\n    return context[\"task\"].render_template(data, context, jinja_env)\n```\n\n----------------------------------------\n\nTITLE: Creating Federated AWS Session in Airflow\nDESCRIPTION: Method to create a federated AWS session using refreshable credentials. It initializes a boto3 session with credentials that can be refreshed automatically when they expire.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef _create_federated_session(self, session_kwargs: dict[str, Any]) -> boto3.session.Session:\n    username = self.extra_config[\"federation\"][\"username\"]\n    region_name = self._get_region_name()\n    self.log.debug(\n        f\"Creating federated session with username={username} region_name={region_name} for \"\n        f\"connection {self.conn.conn_id}\"\n    )\n    credentials = RefreshableCredentials.create_from_metadata(\n        metadata=self._refresh_federated_credentials(),\n        refresh_using=self._refresh_federated_credentials,\n        method=\"custom-federation\",\n    )\n    session = botocore.session.get_session()\n    session._credentials = credentials\n    session.set_config_variable(\"region\", region_name)\n    return boto3.session.Session(botocore_session=session, **session_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Creating AWS DMS Replication Task using DmsCreateTaskOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the DmsCreateTaskOperator in an Airflow DAG to create a new AWS DMS replication task. It requires parameters like task ID, source/target endpoints, replication instance ARN, migration type, and table mappings. Depends on the airflow.providers.amazon.aws.operators.dms module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dms.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example using DmsCreateTaskOperator\n# from airflow.providers.amazon.aws.operators.dms import DmsCreateTaskOperator\n\n# create_task = DmsCreateTaskOperator(\n#     task_id='create_dms_task',\n#     replication_task_id='my-replication-task',\n#     source_endpoint_arn='arn:aws:dms:us-east-1:123456789012:endpoint:ABCDEF123456',\n#     target_endpoint_arn='arn:aws:dms:us-east-1:123456789012:endpoint:GHIJKL789012',\n#     replication_instance_arn='arn:aws:dms:us-east-1:123456789012:rep:MNOPQR345678',\n#     migration_type='full-load', # Or 'cdc', 'full-load-and-cdc'\n#     table_mappings={}, # Specify table mappings JSON\n#     # ... other parameters\n# )\n```\n\n----------------------------------------\n\nTITLE: Executing multiple SQL statements with DatabricksSqlOperator\nDESCRIPTION: Example of using DatabricksSqlOperator to execute multiple SQL statements in a single task. This demonstrates passing a list of SQL statements to be executed sequentially.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/sql.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndatabricks_multiple = DatabricksSqlOperator(\n    task_id=\"databricks_sql_multiple\",\n    sql=[\"SELECT 1 as One\", \"SELECT 2 as Two\"],\n    sql_warehouse_name=\"my_warehouse\",\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Alert Policies with StackdriverListAlertPoliciesOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the StackdriverListAlertPoliciesOperator to fetch all Alert Policies identified by a given filter. The operator can be used with or without a project ID, which will be retrieved from the Google Cloud connection if not provided.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/stackdriver.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nStackdriverListAlertPoliciesOperator(\n    task_id='list_alert_policies',\n    filter_='',\n    project_id=None,\n    gcp_conn_id='google_cloud_default'\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Scrapbook Glue in Python\nDESCRIPTION: This snippet demonstrates how to save data using the scrapbook glue function. The saved data can be later retrieved in Airflow DAGs using the specified 'scrap' name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/tests/system/papermill/input_notebook.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsb.glue(\"message\", msgs)\n```\n\n----------------------------------------\n\nTITLE: Generating Response from Cached Content in Vertex AI using Airflow Operator in Python\nDESCRIPTION: Demonstrates using `GenerateFromCachedContentOperator` from `airflow.providers.google.cloud.operators.vertex_ai.generative_model` to generate a response from previously cached content in Vertex AI. The response is returned via XCom under the 'return_value' key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_53\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/vertex_ai/example_vertex_ai_generative_model.py\n    :language: python\n    :dedent: 4\n    :start-after: [START how_to_cloud_vertex_ai_generate_from_cached_content_operator]\n    :end-before: [END how_to_cloud_vertex_ai_generate_from_cached_content_operator]\n```\n\n----------------------------------------\n\nTITLE: Publishing Messages to Amazon SQS using SqsPublishOperator\nDESCRIPTION: Example demonstrating how to publish a message to an Amazon SQS queue using the SqsPublishOperator. The message contains the task instance and execution date information.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sqs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npublish_to_queue = SqsPublishOperator(\n    task_id=\"publish_to_queue\",\n    message_content=f\"{{ task_instance }}-{{ execution_date }}\",\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing WasbPrefixSensor in Apache Airflow\nDESCRIPTION: This snippet shows how to use the WasbPrefixSensor to wait for blobs matching a specific prefix to arrive in Azure Blob Storage. It specifies the container name, prefix to match, and checks for matching blobs every 60 seconds.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/sensors/wasb_sensors.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwasb_prefix_sensor = WasbPrefixSensor(\n    container_name='wasb_sensor_container',\n    prefix='wasb_sensor_prefix',\n    poke_interval=60,\n    timeout=60 * 60,\n    task_id='wasb_prefix_sensor_task',\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkSubmitOperator with OpenLineage Lineage Components\nDESCRIPTION: Example of how to set up a SparkSubmitOperator to pass lineage information to Spark jobs using individual lineage components (namespace, job name, and run ID) through Spark configuration properties.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/macros.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSparkSubmitOperator(\n    task_id=\"my_task\",\n    application=\"/script.py\",\n    conf={\n        # separated components\n        \"spark.openlineage.parentJobNamespace\": \"{{ macros.OpenLineageProviderPlugin.lineage_job_namespace() }}\",\n        \"spark.openlineage.parentJobName\": \"{{ macros.OpenLineageProviderPlugin.lineage_job_name(task_instance) }}\",\n        \"spark.openlineage.parentRunId\": \"{{ macros.OpenLineageProviderPlugin.lineage_run_id(task_instance) }}\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Running Python Pipeline with DirectRunner using Local File in Deferrable Mode\nDESCRIPTION: This example demonstrates running a Python pipeline using DirectRunner with a local file in deferrable (async) mode. This approach frees up the worker while the pipeline is running by using a trigger to resume the operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_direct_runner_local_file_async = BeamRunPythonPipelineOperator(\n    task_id=\"beam_task_direct_runner_local_file_async\",\n    py_file=\"/tmp/apache_beam/wordcount.py\",\n    py_options=[],\n    pipeline_options={\n        \"output\": \"/tmp/apache_beam/direct_runner_output_local_async\",\n    },\n    py_interpreter=\"python3\",\n    py_system_site_packages=False,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Jupyter Notebook with PapermillOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the PapermillOperator to execute a Jupyter notebook within an Airflow DAG. It includes parameter passing and output handling.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nexecute_notebook = PapermillOperator(\n    task_id=\"run_example_notebook\",\n    input_nb=\"/tmp/hello_world.ipynb\",\n    output_nb=\"/tmp/out-{{ execution_date }}.ipynb\",\n    parameters={\"msgs\": \"Ran from Airflow at {{ execution_date }}!\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Info-Type in Google Cloud DLP\nDESCRIPTION: Example of creating a custom info-type using CloudDLPCreateStoredInfoTypeOperator in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/data_loss_prevention.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_dlp_create_info_type]\n[END howto_operator_dlp_create_info_type]\n```\n\n----------------------------------------\n\nTITLE: Creating Apache Kafka Topic with ManagedKafkaCreateTopicOperator in Python\nDESCRIPTION: This code shows how to create an Apache Kafka topic using the ManagedKafkaCreateTopicOperator. It requires the project ID, region, cluster, and topic configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncreate_topic = ManagedKafkaCreateTopicOperator(\n    task_id=\"create_topic\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n    topic=TOPIC,\n)\n```\n\n----------------------------------------\n\nTITLE: Full Point-in-Time DynamoDB to S3 Export in Python using Airflow\nDESCRIPTION: This snippet illustrates how to perform a full point-in-time export from DynamoDB to S3. It uses the point_in_time_export parameter to recover data from a specific point in time.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/dynamodb_to_s3.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndynamodb_to_s3_full_export_task = DynamoDBToS3Operator(\n    task_id=\"dynamodb_to_s3_full_export\",\n    dynamodb_table_name=\"{{ redshift_cluster_identifier }}_table\",\n    s3_bucket_name=\"{{ s3_bucket }}\",\n    file_size=1000,\n    aws_conn_id=\"aws_default\",\n    point_in_time_export=True,\n    export_time=datetime(2023, 1, 1, 0, 0, 0)\n)\n```\n\n----------------------------------------\n\nTITLE: Running Supervised Fine-Tuning Job on Vertex AI using Airflow Operator in Python\nDESCRIPTION: Example of using `SupervisedFineTuningTrainOperator` from `airflow.providers.google.cloud.operators.vertex_ai.generative_model` to run a supervised fine-tuning job on a Vertex AI generative model. The tuned model's endpoint name is returned via XCom under the 'tuned_model_endpoint_name' key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_49\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/vertex_ai/example_vertex_ai_generative_model_tuning.py\n    :language: python\n    :dedent: 4\n    :start-after: [START how_to_cloud_vertex_ai_supervised_fine_tuning_train_operator]\n    :end-before: [END how_to_cloud_vertex_ai_supervised_fine_tuning_train_operator]\n```\n\n----------------------------------------\n\nTITLE: Conditional Task Skipping in Airflow DAG Testing with Python\nDESCRIPTION: This example demonstrates how to conditionally skip tasks during DAG testing by using the mark_success_pattern argument. It shows a DAG structure with sensors and cleanup tasks that are skipped during testing.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/debug.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\"example_dag\", default_args=default_args) as dag:\n    sensor = ExternalTaskSensor(task_id=\"wait_for_ingestion_dag\", external_dag_id=\"ingest_raw_data\")\n    sensor2 = ExternalTaskSensor(task_id=\"wait_for_dim_dag\", external_dag_id=\"ingest_dim\")\n    collect_stats = PythonOperator(task_id=\"extract_stats_csv\", python_callable=extract_stats_csv)\n    # ... run other tasks\n    cleanup = PythonOperator(task_id=\"cleanup\", python_callable=Path.unlink, op_args=[collect_stats.output])\n\n    [sensor, sensor2] >> collect_stats >> cleanup\n\nif __name__ == \"__main__\":\n    ingest_testing_data()\n    run = dag.test(mark_success_pattern=\"wait_for_.*|cleanup\")\n    print(f\"Intermediate csv: {run.get_task_instance('collect_stats').xcom_pull(task_id='collect_stats')}\")\n```\n\n----------------------------------------\n\nTITLE: Deleting a Model using Vertex AI Model Service Operator - Python\nDESCRIPTION: Describes deleting a model from Vertex AI with Airflow's DeleteModelOperator. This process requires model identification and GCP configuration. Once run, the specified model is irreversibly removed from the Vertex AI service.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndelete_model_task = DeleteModelOperator(\n    task_id=\"delete_model_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    model_id=MODEL_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Building Optimized Airflow Image with Dev and Runtime Dependencies\nDESCRIPTION: Builds a production Airflow image with Python 3.9, additional Airflow extras, and both dev and runtime apt dependencies. Dev dependencies are only used during build time to reduce the final image size.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . --pull --build-arg PYTHON_BASE_IMAGE=\"python:3.9-slim-bullseye\" --build-arg ADDITIONAL_AIRFLOW_EXTRAS=\"amazon\" --build-arg ADDITIONAL_PYTHON_DEPS=\"apache-airflow-providers-amazon pandas\" --build-arg ADDITIONAL_DEV_APT_DEPS=\"gcc g++\" --build-arg ADDITIONAL_RUNTIME_APT_DEPS=\"default-jre-headless\" --tag \"my-company/airflow:2.3.0-python3.9-amazon\"\n```\n\n----------------------------------------\n\nTITLE: Stopping EC2 Instances with Airflow AWS Operator - Python\nDESCRIPTION: This snippet demonstrates the use of the EC2StopInstanceOperator in an Airflow DAG to stop specified EC2 instances. It requires Airflow's AWS provider and the Boto3 library, as well as correctly configured AWS credentials. The key parameter is a list of EC2 instance IDs to stop, and the operator returns the list of stopped instance IDs. This operator is useful for automating EC2 cost-saving measures within data workflows and assumes the user has stop-instance permissions and the target instances are running.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/ec2.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# There is no code in the provided input itself, only includes and code references to other files.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Connection with IAM Keys via Environment Variable (JSON Format) in Bash\nDESCRIPTION: This snippet provides an alternative method to configure an Airflow AWS connection using an environment variable, but this time with a JSON string format. It explicitly defines the connection type, login (Access Key ID), and password (Secret Access Key).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AWS_DEFAULT='{\n  \"conn_type\": \"aws\",\n  \"login\": \"AKIAIOSFODNN7EXAMPLE\",\n  \"password\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Triggering an Asynchronous dbt Cloud Job in Python\nDESCRIPTION: This example shows how to use the DbtCloudRunJobOperator to trigger a dbt Cloud job asynchronously, with custom runtime configuration for threads_override.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndbt_cloud_run_job_async = DbtCloudRunJobOperator(\n    task_id=\"dbt_cloud_run_job_async\",\n    job_id=JOB_ID,\n    wait_for_termination=False,\n    additional_run_config={\"threads_override\": 8},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating PubSub Subscription with PubSubCreateSubscriptionOperator\nDESCRIPTION: Example demonstrating how to create a subscription to a PubSub topic using the PubSubCreateSubscriptionOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/pubsub.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsubscribe_task = PubSubCreateSubscriptionOperator(\n    task_id=\"subscribe_task\",\n    project_id=PROJECT_ID,\n    topic=TOPIC_ID,\n    subscription=SUBSCRIPTION_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Apache Kafka Consumer Groups with ManagedKafkaListConsumerGroupsOperator in Python\nDESCRIPTION: This snippet demonstrates how to list Apache Kafka consumer groups using the ManagedKafkaListConsumerGroupsOperator. It specifies the project ID, region, and cluster.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlist_consumer_groups = ManagedKafkaListConsumerGroupsOperator(\n    task_id=\"list_consumer_groups\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n)\n```\n\n----------------------------------------\n\nTITLE: Fetching Data from BigQuery Table Asynchronously - Python\nDESCRIPTION: Example of using BigQueryGetDataOperator in async mode to fetch data from a BigQuery table.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nget_data_async = BigQueryGetDataOperator(\n    task_id=\"get_data_from_bq_async\",\n    dataset_id=DATASET_NAME,\n    table_id=TABLE_NAME,\n    max_results=10,\n    selected_fields=\"value\",\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Running a Query in Google Cloud Datastore using Python\nDESCRIPTION: This example demonstrates how to use CloudDatastoreRunQueryOperator to run a query for entities in Google Cloud Datastore. It specifies the project ID and query parameters for the operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrun_query = CloudDatastoreRunQueryOperator(\n    task_id=\"run_query\",\n    project_id=GCP_PROJECT_ID,\n    query=QUERY,\n)\n```\n\n----------------------------------------\n\nTITLE: Wait for AWS Glue Crawler State Example\nDESCRIPTION: Example demonstrating how to wait for an AWS Glue crawler to reach terminal state using GlueCrawlerSensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/glue.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n[START howto_sensor_glue_crawler]\n[END howto_sensor_glue_crawler]\n```\n\n----------------------------------------\n\nTITLE: Looking up an Entry with CloudDataCatalogLookupEntryOperator in Python\nDESCRIPTION: Uses CloudDataCatalogLookupEntryOperator to look up an entry from Google Cloud Data Catalog using the resource name. The result is saved to XCom for use by other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlookup_entry = CloudDataCatalogLookupEntryOperator(\n    task_id=\"lookup_entry\",\n    linked_resource=TEST_LINKED_RESOURCE,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nlookup_entry_result = lookup_entry.output\n```\n\n----------------------------------------\n\nTITLE: Implementing TimeSensorAsync in Apache Airflow\nDESCRIPTION: This snippet shows the usage of TimeSensorAsync, an asynchronous version of TimeSensor. It creates two sensors: one that waits until 6:00 AM and another that waits until 10 PM, both requiring a Triggerer to run.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/datetime.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nTimeSensorAsync(task_id=\"wait_until_0600_async\", target_time=time(6, 0))\nTimeSensorAsync(task_id=\"wait_until_2200_async\", target_time=time(22, 0))\n```\n\n----------------------------------------\n\nTITLE: Downloading Data from Google Drive to Local Filesystem using GoogleDriveToLocalOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the GoogleDriveToLocalOperator in Apache Airflow to download a file from Google Drive to a local filesystem. It specifies the Google Drive file ID, local file path, and impersonation chain for authentication.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gdrive_to_local.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nGoogleDriveToLocalOperator(\n    task_id=\"download_from_gdrive_to_local\",\n    file_id=\"{{ task_instance.xcom_pull('upload_to_gdrive', key='return_value') }}\",\n    local_output_directory_path=LOCAL_PATH,\n    impersonation_chain=IMPERSONATION_CHAIN,\n)\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Azure Blob Storage to Amazon S3 using AzureBlobStorageToS3Operator in Python\nDESCRIPTION: This code snippet demonstrates how to use the AzureBlobStorageToS3Operator to copy data from an Azure Blob Storage container to an Amazon S3 bucket. It includes the necessary parameters for configuring the transfer operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/azure_blob_to_s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_transfer_azure_blob_to_s3]\nAzureBlobStorageToS3Operator(\n    task_id=\"transfer_azure_blob_to_s3\",\n    azure_container_name=\"{{ AZURE_CONTAINER_NAME }}\",\n    azure_blob_name=\"{{ AZURE_BLOB_NAME }}\",\n    aws_bucket_name=\"{{ S3_BUCKET_NAME }}\",\n    aws_key=\"{{ S3_KEY }}\",\n)\n# [END howto_transfer_azure_blob_to_s3]\n```\n\n----------------------------------------\n\nTITLE: Executing Python Callable in Virtual Environment with PythonVirtualenvOperator\nDESCRIPTION: Shows how to use PythonVirtualenvOperator to execute a Python callable inside a new Python virtual environment in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef callable_virtualenv():\n    from time import sleep\n    from colorama import Back, Fore, Style\n\n    print(Fore.RED + 'some red text')\n    print(Back.GREEN + 'and with a green background')\n    print(Style.DIM + 'and in dim text')\n    print(Style.RESET_ALL)\n    for _ in range(4):\n        print(Style.DIM + 'Please wait...', flush=True)\n        sleep(1)\n    print('Finished')\n    return 'Finished'\n\nvirtualenv_task = PythonVirtualenvOperator(\n    task_id=\"virtualenv_python\",\n    python_callable=callable_virtualenv,\n    requirements=[\"colorama==0.4.0\"],\n    system_site_packages=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Producing Messages to Kafka Topics using ProduceToTopicOperator\nDESCRIPTION: Example demonstrating the usage of ProduceToTopicOperator to produce messages to Kafka topics. The operator uses a user-supplied producer_function to create key/value pairs for messages.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/operators/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nhowto_operator_produce_to_topic\n```\n\n----------------------------------------\n\nTITLE: Selectively Disabling OpenLineage for a Task within an Enabled Airflow DAG\nDESCRIPTION: Shows how to enable OpenLineage for an entire DAG using `enable_lineage` and then explicitly disable it for a specific task (`t1`) using the `disable_lineage` helper function. This selective control depends on the `selective_enable` policy being activated in the configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.openlineage.utils.selective_enable import disable_lineage, enable_lineage\n\nwith DAG(...) as dag:\n    t1 = MyOperator(...)\n    t2 = AnotherOperator(...)\n\n# Enable lineage for the entire DAG\nenable_lineage(dag)\n\n# Disable lineage for task t1\ndisable_lineage(t1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Task and DAG Callbacks in Python for Apache Airflow\nDESCRIPTION: This code snippet demonstrates how to set up callbacks for task failures and DAG success in Apache Airflow. It defines two callback functions and creates a DAG with three tasks, applying the callbacks appropriately.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/callbacks.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\nimport pendulum\n\nfrom airflow.sdk import DAG\nfrom airflow.providers.standard.operators.empty import EmptyOperator\n\n\ndef task_failure_alert(context):\n    print(f\"Task has failed, task_instance_key_str: {context['task_instance_key_str']}\")\n\n\ndef dag_success_alert(context):\n    print(f\"DAG has succeeded, run_id: {context['run_id']}\")\n\n\nwith DAG(\n    dag_id=\"example_callback\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    dagrun_timeout=datetime.timedelta(minutes=60),\n    catchup=False,\n    on_success_callback=None,\n    on_failure_callback=task_failure_alert,\n    tags=[\"example\"],\n):\n    task1 = EmptyOperator(task_id=\"task1\")\n    task2 = EmptyOperator(task_id=\"task2\")\n    task3 = EmptyOperator(task_id=\"task3\", on_success_callback=[dag_success_alert])\n    task1 >> task2 >> task3\n```\n\n----------------------------------------\n\nTITLE: Executing Athena Queries via Airflow SQLExecuteQueryOperator - Python\nDESCRIPTION: Demonstrates how to use the Airflow SQLExecuteQueryOperator to run SQL queries directly against Amazon Athena using an Athena connection. Requires an Airflow environment with the Athena connection configured and PyAthena installed. The main parameters include the SQL statement to be executed and the connection ID for Athena. Inputs are the operator task definition within an Airflow DAG, and output is query execution on Athena without retrieving results to Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/athena/athena_sql.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_athena_sql_execute_query\",\n    schedule_interval=None,\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"athena\"],\n) as dag:\n\n    # Execute an Athena SQL statement\n    execute_athena_query = SQLExecuteQueryOperator(\n        task_id=\"execute_athena_query\",\n        sql=\"SELECT * FROM my_table LIMIT 10;\",\n        conn_id=\"my_athena_conn_id\"  # Athena connection configured in Airflow\n    )\n```\n\n----------------------------------------\n\nTITLE: Copying S3 Object with S3CopyObjectOperator\nDESCRIPTION: Demonstrates how to copy an object from one S3 location to another using the S3CopyObjectOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/s3.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncopy_object = S3CopyObjectOperator(\n    task_id=\"copy_object\",\n    source_bucket_name=BUCKET_NAME,\n    dest_bucket_name=BUCKET_NAME,\n    source_bucket_key=KEY,\n    dest_bucket_key=NEW_KEY,\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Using DatabricksSQLStatementsOperator in Airflow DAG\nDESCRIPTION: Example of how to use the DatabricksSQLStatementsOperator to execute a SQL statement on Databricks. The example shows configuration of required parameters like statement and warehouse_id along with optional parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/sql_statements.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsql_statements_op = DatabricksSQLStatementsOperator(\n    task_id=\"run_sql_statements\",\n    statement=\"SELECT 1\",\n    warehouse_id=\"warehouse_id\",\n    catalog=\"sample_catalog\",\n    schema=\"sample_schema\",\n    api_version=\"2.0\",\n    deferrable=False,\n    parameters=[{\"name\": \"variable_name\", \"value\": \"variable_value\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Transferring GCS Files to BigQuery Using Standard Operator\nDESCRIPTION: Example showing how to configure GCSToBigQueryOperator to load data from GCS into a BigQuery table. Demonstrates setting source objects, destination table, and schema configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_bigquery.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ngcs_to_bigquery = GCSToBigQueryOperator(\n    task_id=\"gcs_to_bigquery\",\n    bucket=\"cloud-samples-data\",\n    source_objects=[\"bigquery/us-states/us-states.csv\"],\n    destination_project_dataset_table=\"airflow_test.gcs_to_bq_table\",\n    schema_fields=[\n        {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n        {\"name\": \"post_abbr\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n    ],\n    write_disposition=\"WRITE_TRUNCATE\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running Dataflow Pipeline\nDESCRIPTION: Example demonstrating how to run a previously created Dataflow pipeline using DataflowRunPipelineOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n_ = DataflowRunPipelineOperator(\n    task_id=\"run_dataflow_pipeline\",\n    project_id=project_id,\n    location=location,\n    pipeline_id=PIPELINE_ID,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Backfill for Airflow DAG using CLI\nDESCRIPTION: This command demonstrates how to create a backfill for an Airflow DAG named 'tutorial' using the CLI. It specifies a date range, reprocessing behavior, concurrency limit, run order, and custom DAG run configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/backfill.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairflow backfill create --dag-id tutorial \\\n    --start-date 2015-06-01 \\\n    --end-date 2015-06-07 \\\n    --reprocessing-behavior failed \\\n    --max-active-runs 3 \\\n    --run-backwards \\\n    --dag-run-conf '{\"my\": \"param\"}'\n```\n\n----------------------------------------\n\nTITLE: Creating Dataproc Cluster for Persistent History Server in Python\nDESCRIPTION: This snippet shows the prerequisite step of creating a Dataproc cluster configured specifically for use with a Persistent History Server (PHS). This cluster configuration is necessary before creating a Dataproc Batch that utilizes PHS. Refer to Google Cloud documentation for specific cluster setup parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n# Code extracted from: /../../google/tests/system/google/cloud/dataproc/example_dataproc_batch_persistent.py\n# Between markers: [START how_to_cloud_dataproc_create_cluster_for_persistent_history_server] and [END how_to_cloud_dataproc_create_cluster_for_persistent_history_server]\n# \n# Example showing cluster creation/configuration for PHS\n# ... (actual Python code would be here)\n\n```\n\n----------------------------------------\n\nTITLE: Deleting a Google Cloud Tasks Queue in Python\nDESCRIPTION: This snippet shows how to delete a Google Cloud Tasks queue using the CloudTasksQueueDeleteOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# [START delete_queue]\nCloudTasksQueueDeleteOperator(\n    task_id=\"delete_queue\",\n    location=LOCATION,\n    queue_name=QUEUE_ID,\n).execute(context=context)\n# [END delete_queue]\n```\n\n----------------------------------------\n\nTITLE: Creating Dataform Workflow Invocation in Python\nDESCRIPTION: This snippet demonstrates how to create a Workflow Invocation using the DataformCreateWorkflowInvocationOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_create_workflow_invocation]\n# [END howto_operator_create_workflow_invocation]\n```\n\n----------------------------------------\n\nTITLE: Creating Google Analytics Property with GoogleAnalyticsAdminCreatePropertyOperator in Python\nDESCRIPTION: This code snippet illustrates the use of GoogleAnalyticsAdminCreatePropertyOperator to create a new property in Google Analytics. It demonstrates the operator usage and notes that Jinja templating can be applied to the operator's template fields.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/analytics_admin.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nGoogleAnalyticsAdminCreatePropertyOperator(\n    task_id=\"create_property\",\n    property={\n        \"display_name\": \"test property\",\n        \"time_zone\": \"America/Los_Angeles\",\n        \"industry_category\": \"TECHNOLOGY\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Running an ADF Pipeline Asynchronously with Airflow Operator and Sensor (Python)\nDESCRIPTION: This example illustrates triggering an Azure Data Factory pipeline asynchronously using `AzureDataFactoryRunPipelineOperator` with `wait_for_termination=False`. A separate `AzureDataFactoryPipelineRunStatusSensor` task is then used to monitor the pipeline's status based on the `run_id` passed via XComs. Requires an Azure Data Factory connection (`azure_data_factory_conn_id`).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/adf_run_pipeline.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../tests/system/microsoft/azure/example_adf_run_pipeline.py\n#     :language: python\n#     :dedent: 0\n#     :start-after: [START howto_operator_adf_run_pipeline_async]\n#     :end-before: [END howto_operator_adf_run_pipeline_async]\n\n# Example (simulated based on description):\nfrom airflow.providers.microsoft.azure.operators.data_factory import AzureDataFactoryRunPipelineOperator\nfrom airflow.providers.microsoft.azure.sensors.data_factory import AzureDataFactoryPipelineRunStatusSensor\n\nrun_pipeline_async = AzureDataFactoryRunPipelineOperator(\n    task_id=\"run_pipeline_async\",\n    pipeline_name=\"pipeline1\",\n    azure_data_factory_conn_id=\"azure_data_factory_default\",\n    wait_for_termination=False,\n)\n\npipeline_run_sensor_sync = AzureDataFactoryPipelineRunStatusSensor(\n    task_id=\"pipeline_run_sensor_sync\",\n    run_id=run_pipeline_async.output[\"run_id\"],\n    azure_data_factory_conn_id=\"azure_data_factory_default\",\n)\n\nrun_pipeline_async >> pipeline_run_sensor_sync\n\n```\n\n----------------------------------------\n\nTITLE: Defining Custom GitHub Team-Based Authorization Logic (Python)\nDESCRIPTION: Implements a custom security manager class `GithubTeamAuthorizer` by inheriting from `FabAirflowSecurityManagerOverride`. This class overrides the `get_oauth_user_info` method to fetch user details and team memberships from the GitHub API after successful OAuth authentication. It includes helper functions (`team_parser`, `map_roles`) to process the GitHub API response, extract relevant team IDs, and map these IDs to predefined Airflow roles (Admin, Viewer, Public). The method returns a dictionary containing the username (prefixed with 'github_') and the list of assigned Airflow roles (`role_keys`) for FAB to use in authorization. Requires `logging`, `os`, and interaction with the Flask-AppBuilder OAuth remote app object.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/webserver-authentication.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.fab.auth_manager.security_manager.override import FabAirflowSecurityManagerOverride\nimport logging\nfrom typing import Any, List, Union\nimport os\n\nlog = logging.getLogger(__name__)\nlog.setLevel(os.getenv(\"AIRFLOW__LOGGING__FAB_LOGGING_LEVEL\", \"INFO\"))\n\nFAB_ADMIN_ROLE = \"Admin\"\nFAB_VIEWER_ROLE = \"Viewer\"\nFAB_PUBLIC_ROLE = \"Public\"  # The \"Public\" role is given no permissions\nTEAM_ID_A_FROM_GITHUB = 123  # Replace these with real team IDs for your org\nTEAM_ID_B_FROM_GITHUB = 456  # Replace these with real team IDs for your org\n\n\ndef team_parser(team_payload: dict[str, Any]) -> list[int]:\n    # Parse the team payload from GitHub however you want here.\n    return [team[\"id\"] for team in team_payload]\n\n\ndef map_roles(team_list: list[int]) -> list[str]:\n    # Associate the team IDs with Roles here.\n    # The expected output is a list of roles that FAB will use to Authorize the user.\n\n    team_role_map = {\n        TEAM_ID_A_FROM_GITHUB: FAB_ADMIN_ROLE,\n        TEAM_ID_B_FROM_GITHUB: FAB_VIEWER_ROLE,\n    }\n    return list(set(team_role_map.get(team, FAB_PUBLIC_ROLE) for team in team_list))\n\n\nclass GithubTeamAuthorizer(FabAirflowSecurityManagerOverride):\n    # In this example, the oauth provider == 'github'.\n    # If you ever want to support other providers, see how it is done here:\n    # https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/security/manager.py#L550\n    def get_oauth_user_info(self, provider: str, resp: Any) -> dict[str, Union[str, list[str]]]:\n        # Creates the user info payload from Github.\n        # The user previously allowed your app to act on their behalf,\n        #   so now we can query the user and teams endpoints for their data.\n        # Username and team membership are added to the payload and returned to FAB.\n\n        remote_app = self.appbuilder.sm.oauth_remotes[provider]\n        me = remote_app.get(\"user\")\n        user_data = me.json()\n        team_data = remote_app.get(\"user/teams\")\n        teams = team_parser(team_data.json())\n        roles = map_roles(teams)\n        log.debug(f\"User info from Github: {user_data}\\nTeam info from Github: {teams}\")\n        return {\"username\": \"github_\" + user_data.get(\"login\"), \"role_keys\": roles}\n```\n\n----------------------------------------\n\nTITLE: Creating a Google Cloud Tasks Task in Python\nDESCRIPTION: This snippet demonstrates how to create a new task in a Google Cloud Tasks queue using the CloudTasksTaskCreateOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# [START create_task]\nCloudTasksTaskCreateOperator(\n    task_id=\"create_task\",\n    location=LOCATION,\n    queue_name=QUEUE_ID,\n    task={\n        \"app_engine_http_request\": {\n            \"http_method\": \"POST\",\n            \"relative_uri\": \"/example_task_handler\",\n        }\n    },\n).execute(context=context)\n# [END create_task]\n```\n\n----------------------------------------\n\nTITLE: Defining Text-to-Speech API Arguments in Python\nDESCRIPTION: Shows how to define the 'input_data', 'voice_config', and 'audio_config' parameters for the CloudTextToSpeechSynthesizeOperator using Python dictionaries. These arguments correspond to types from the 'google.cloud.texttospeech_v1.types' module and configure the text input, voice selection, and audio output format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/text_to_speech.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# The input text to synthesize\ninput_data = {\"text\": \"Sample text for synthesize.\"}\n\n# The configuration of the voice synthesize request\nvoice_config = {\"language_code\": \"en-US\", \"name\": \"en-US-Standard-F\"}\n\n# The configuration of the type of audio file\naudio_config = {\"audio_encoding\": \"LINEAR16\"}\n```\n\n----------------------------------------\n\nTITLE: Listing Endpoints using Vertex AI Endpoint Service Operator - Python\nDESCRIPTION: Illustrates retrieving a list of available Vertex AI endpoints using Airflow's ListEndpointsOperator. This snippet requires a configured project_id and location. Upon execution, endpoints information is returned, typically pushed to XCom for downstream tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nlist_endpoints_task = ListEndpointsOperator(\n    task_id=\"list_vertex_ai_endpoints\",\n    project_id=GCP_PROJECT_ID,\n    location=REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Monitoring Amazon Bedrock Knowledge Base with Python\nDESCRIPTION: This example illustrates how to use the BedrockKnowledgeBaseActiveSensor to wait for an Amazon Bedrock Knowledge Base to reach a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_sensor_bedrock_knowledge_base_active]\n# Example code not provided in the original text\n# [END howto_sensor_bedrock_knowledge_base_active]\n```\n\n----------------------------------------\n\nTITLE: Generating Fernet Key in Python\nDESCRIPTION: This code snippet shows how to generate a new Fernet key using Python's cryptography library. The generated key should be kept secure.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/fernet.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom cryptography.fernet import Fernet\n\nfernet_key = Fernet.generate_key()\nprint(fernet_key.decode())  # your fernet_key, keep it in secured place!\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from BigQuery to MsSQL using BigQueryToMsSqlOperator in Python\nDESCRIPTION: This code demonstrates how to use the BigQueryToMsSqlOperator to copy data from a BigQuery table to a Microsoft SQL Server table. It shows how to configure the operator with source and destination parameters, along with connection information for both services.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/bigquery_to_mssql.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncopy_bq_to_mssql = BigQueryToMsSqlOperator(\n    task_id=\"copy_bq_to_mssql\",\n    source_project_dataset_table=\"{\".join([project_id, f\"{DATASET}_{ENV_ID}\", f\"{TABLE}_{ENV_ID}\"])\",\n    mssql_table=f\"test_airflow_{ENV_ID}\",\n    selected_fields=[\"value\"],\n    replace=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Vertex AI Dataset using Python\nDESCRIPTION: This snippet demonstrates how to update a Vertex AI dataset using the UpdateDatasetOperator in Airflow. It specifies the project ID, region, dataset ID, and display name for the update operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/automl.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nUpdateDatasetOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    dataset_id=dataset_id,\n    display_name=\"updated_test_dataset\",\n    task_id=\"update_dataset\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a GKE Cluster Asynchronously with GKECreateClusterOperator\nDESCRIPTION: Example of using the GKECreateClusterOperator in deferrable mode to asynchronously create a Google Kubernetes Engine cluster, freeing up worker resources during cluster creation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster_async = GKECreateClusterOperator(\n    task_id=\"create_cluster_async\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    body=CLUSTER,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud SQL with IAM Authentication in Python\nDESCRIPTION: Example Python code demonstrating how to configure Google Cloud SQL connections with IAM authentication in Airflow DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp_sql.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_cloudsql_iam_connections]\n# Create connection for use in CloudSQLExecuteQueryOperator\nsql_iam_connection = Connection(\n    conn_id=\"gcpcloudsql_iam_connection\",\n    conn_type=\"gcpcloudsql\",\n    host=\"127.0.0.1\",  # Host where the proxy is running\n    login=\"SERVICE_ACCOUNT_NAME@PROJECT_ID.iam.gserviceaccount.com\",  # IAM account name\n    port=3306,\n    schema=\"test_db\",  # Database name\n    extra=json.dumps(\n        dict(\n            database_type=\"mysql\",  # or \"postgres\"\n            project_id=\"project_id\",\n            location=\"europe-west1\",\n            instance=\"instance_name\",\n            use_proxy=True,\n            use_iam=True,\n            sql_proxy_use_tcp=True,\n        )\n    ),\n)\n# [END howto_operator_cloudsql_iam_connections]\n```\n\n----------------------------------------\n\nTITLE: Merging Data with SQL Upsert Operation\nDESCRIPTION: A task function that merges data from a temporary table into the main employees table using a SQL query with ON CONFLICT handling for maintaining data integrity.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/pipeline.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef merge_data():\n    query = \"\"\"\n        INSERT INTO employees\n        SELECT *\n        FROM (\n            SELECT DISTINCT *\n            FROM employees_temp\n        ) t\n        ON CONFLICT (\"Serial Number\") DO UPDATE\n        SET\n          \"Employee Markme\" = excluded.\"Employee Markme\",\n          \"Description\" = excluded.\"Description\",\n          \"Leave\" = excluded.\"Leave\";\n    \"\"\"\n    try:\n        postgres_hook = PostgresHook(postgres_conn_id=\"tutorial_pg_conn\")\n        conn = postgres_hook.get_conn()\n        cur = conn.cursor()\n        cur.execute(query)\n        conn.commit()\n        return 0\n    except Exception as e:\n        return 1\n```\n\n----------------------------------------\n\nTITLE: Implementing a Task Cluster Policy for Maximum Timeout\nDESCRIPTION: A policy function that enforces a maximum timeout on every task, setting it to a default of 12 hours if not specified or if exceeding the limit.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/cluster-policies.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef task_policy(task):\n    \"\"\"\n    This task policy enforces a maximum timeout for all operators.\n    \"\"\"\n    if task.execution_timeout is None:\n        task.execution_timeout = timedelta(hours=12)\n    elif task.execution_timeout > timedelta(hours=12):\n        task.execution_timeout = timedelta(hours=12)\n```\n\n----------------------------------------\n\nTITLE: Embedding DAGs into Airflow Image (Dockerfile)\nDESCRIPTION: Dockerfile example demonstrating how to copy local DAG files into the standard DAGs folder (`/opt/airflow/dags`) within the Airflow image using the `COPY` instruction.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_18\n\nLANGUAGE: Dockerfile\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/embedding-dags/Dockerfile\n    :language: Dockerfile\n    :start-after: [START Dockerfile]\n    :end-before: [END Dockerfile]\n```\n\n----------------------------------------\n\nTITLE: Accessing Airflow Variables in Templates (Python/Jinja)\nDESCRIPTION: Shows how to access Airflow variables as plain-text or JSON in templates. Includes examples of accessing nested structures and fetching variables by string with fallback values.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/templates-ref.rst#2025-04-22_snippet_2\n\nLANGUAGE: jinja\nCODE:\n```\n{{ var.json.my_dict_var.key1 }}\n```\n\nLANGUAGE: jinja\nCODE:\n```\n{{ var.value.get('my.var', 'fallback') }}\n```\n\nLANGUAGE: jinja\nCODE:\n```\n{{ var.json.get('my.dict.var', {'key1': 'val1'}) }}\n```\n\n----------------------------------------\n\nTITLE: Using AwaitMessageTriggerFunctionSensor in Apache Airflow with Kafka\nDESCRIPTION: This snippet shows how to use the AwaitMessageTriggerFunctionSensor in an Apache Airflow DAG. It sets up a sensor that triggers a function when a specific message is received in a Kafka topic.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/sensors.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nAwaitMessageTriggerFunctionSensor(\n    task_id=\"awaiting_message_trigger_function\",\n    kafka_config_id=\"kafka_default\",\n    topic=\"sensor-topic\",\n    apply_function=lambda x: x if x == b\"test message\" else None,\n    event_triggered_function=lambda: print(\"Event triggered!\"),\n    poke_interval=0,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting SSH Connection as Environment Variable\nDESCRIPTION: Example of setting an SSH connection string as an environment variable using AIRFLOW_CONN_{CONN_ID}. This includes specifying connection details and extra parameters in the URI format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/docs/connections/ssh.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_MAIN_SERVER='ssh://user:pass@localhost:22?conn_timeout=10&compress=false&no_host_key_check=false&allow_host_key_change=true&key_file=%2Fhome%2Fairflow%2F.ssh%2Fid_rsa'\n```\n\n----------------------------------------\n\nTITLE: Setting MSSQL Connection via URI Environment Variable in Bash\nDESCRIPTION: Demonstrates configuring an Apache Airflow MSSQL connection using the standard URI format within an environment variable (`AIRFLOW_CONN_{CONN_ID}`). Requires URL encoding for components. The example sets the `mssql_default` connection with username, password, host, port, and database name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/connections/mssql.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_MSSQL_DEFAULT='mssql://username:password@server.com:1433/database_name'\n```\n\n----------------------------------------\n\nTITLE: Creating Bigtable Table with BigtableCreateTableOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the BigtableCreateTableOperator to create a table in a Cloud Bigtable instance. It shows how to create the operator with and without specifying the project ID, and includes options for setting initial split keys and column families.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigtable.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncreate_table_task = BigtableCreateTableOperator(\n    project_id=GCP_PROJECT_ID,\n    instance_id=INSTANCE_ID,\n    table_id=TABLE_ID,\n    initial_split_keys=[\n        b\"split1\",\n        b\"split2\",\n        b\"split3\",\n    ],\n    column_families={\n        \"cf1\": {},\n        \"cf2\": {},\n        \"cf3\": {},\n        \"cf4\": {},\n    },\n    task_id=\"create_table\",\n)\n\n# The same operator can be created without project_id:\ncreate_table_task_no_project_id = BigtableCreateTableOperator(\n    instance_id=INSTANCE_ID,\n    table_id=TABLE_ID,\n    initial_split_keys=[\n        b\"split1\",\n        b\"split2\",\n        b\"split3\",\n    ],\n    column_families={\n        \"cf1\": {},\n        \"cf2\": {},\n        \"cf3\": {},\n        \"cf4\": {},\n    },\n    task_id=\"create_table\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Memcached Instance using Airflow Operator\nDESCRIPTION: Example of using CloudMemorystoreMemcachedCreateInstanceOperator to create a new Memcached instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore_memcached.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncreate_instance = CloudMemorystoreMemcachedCreateInstanceOperator(\n    task_id=\"create-instance\",\n    location=LOCATION,\n    instance_id=MEMCACHED_INSTANCE_ID,\n    instance=MEMCACHED_INSTANCE,\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Waiting for AWS Step Functions Execution State in Airflow\nDESCRIPTION: This snippet illustrates how to use the StepFunctionExecutionSensor to wait on the state of an AWS Step Function state machine execution until it reaches a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/step_functions.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_sensor_step_function_execution]\n# [END howto_sensor_step_function_execution]\n```\n\n----------------------------------------\n\nTITLE: Dynamic DAG Generation with Selective Parsing\nDESCRIPTION: Demonstrates a pattern for dynamically generating multiple Airflow DAGs from a list of items, with logic to selectively generate only the currently parsed DAG. Uses the Airflow SDK's parsing context to determine which DAG is being loaded.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/dynamic-dag-generation.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import DAG\nfrom airflow.sdk import get_parsing_context\n\ncurrent_dag_id = get_parsing_context().dag_id\n\nfor thing in list_of_things:\n    dag_id = f\"generated_dag_{thing}\"\n    if current_dag_id is not None and current_dag_id != dag_id:\n        continue  # skip generation of non-selected DAG\n\n    with DAG(dag_id=dag_id, ...):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Yandex Lockbox Backend Parameters in Airflow (INI)\nDESCRIPTION: Demonstrates configuring backend-specific parameters using `backend_kwargs` in the `airflow.cfg`. This example sets custom prefixes for connections and variables stored in Yandex Lockbox, provided as a JSON dictionary string.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.yandex.secrets.lockbox.LockboxSecretBackend\nbackend_kwargs = {\"connections_prefix\": \"example-connections-prefix\", \"variables_prefix\": \"example-variables-prefix\"}\n```\n\n----------------------------------------\n\nTITLE: Updating a Cloud Composer Environment in Deferrable Mode\nDESCRIPTION: This snippet shows how to update a Cloud Composer environment using the CloudComposerUpdateEnvironmentOperator in deferrable mode. It includes additional parameters for deferrable execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nupdate_environment_task_deferrable = CloudComposerUpdateEnvironmentOperator(\n    task_id=\"update-environment-deferrable-mode\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    environment_name=ENVIRONMENT_NAME,\n    update_mask=UPDATE_MASK,\n    environment=UPDATE_CONFIG,\n    deferrable=True,\n    poll_interval=10,\n)\n```\n\n----------------------------------------\n\nTITLE: Associating Tasks with Pools in Airflow\nDESCRIPTION: This snippet demonstrates how to associate a BashOperator task with a specific Airflow pool using the 'pool' parameter. The example creates a task for aggregating database messages and assigns it to a custom pool named 'ep_data_pipeline_db_msg_agg'.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/pools.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naggregate_db_message_job = BashOperator(\n    task_id=\"aggregate_db_message_job\",\n    execution_timeout=timedelta(hours=3),\n    pool=\"ep_data_pipeline_db_msg_agg\",\n    bash_command=aggregate_db_message_job_cmd,\n    dag=dag,\n)\naggregate_db_message_job.set_upstream(wait_for_empty_queue)\n```\n\n----------------------------------------\n\nTITLE: Invoking Amazon Titan Model via Amazon Bedrock Operator (Python)\nDESCRIPTION: Shows how to invoke the Amazon Titan model using Airflow's BedrockInvokeModelOperator. This example requires configuration of AWS connection and model-specific input body. It handles submitting an inference task to the Titan model, retrieving the generated output as response.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntitan_task = BedrockInvokeModelOperator(\n    task_id=\"titan_invocation\",\n    model_id=\"amazon.titan-text-lite-v1\",\n    body={\"inputText\": \"What is the weather today?\"},\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Launching Python Package Training Job in VertexAI with CreateCustomPythonPackageTrainingJobOperator (Python)\nDESCRIPTION: Demonstrates how to launch a custom training job using a Python package on VertexAI through Airflow. Requires you to upload your training package (as a GCS URI), specify the training module, and provide relevant job parameters. Inputs include python_package_gcs_uri and python_module_name. Trains and registers a new model upon completion; GCP and VertexAI setup required.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwith models.DAG(\n    \"vertex_ai_custom_job_python_package\",\n    default_args=default_args,\n    schedule_interval=None,\n    start_date=days_ago(1),\n    catchup=False,\n) as dag:\n    create_python_package_training_job = CreateCustomPythonPackageTrainingJobOperator(\n        task_id=\"train_model_with_python_package\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        display_name=DISPLAY_NAME,\n        python_package_gcs_uri=PYTHON_PACKAGE_GCS_URI,\n        python_module_name=PYTHON_MODULE_NAME,\n        dataset_id=DATASET_ID,\n        gcp_conn_id=GCP_CONN_ID,\n    )\n```\n\n----------------------------------------\n\nTITLE: Waiting for Any DynamoDB Attribute Value from a List using DynamoDBValueSensor in Python\nDESCRIPTION: This Python snippet shows how to configure the `DynamoDBValueSensor` to wait until a specific attribute in a DynamoDB item matches *any* of the values provided in a list. The sensor checks the specified attribute against each value in the `attribute_value` list and proceeds once a match is found. Similar to the single value check, it requires table name, partition key details, attribute name, and an AWS connection ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dynamodb.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwait_for_any_value = DynamoDBValueSensor(\n    task_id=\"wait_for_any_value_sensor\",\n    table_name=TABLE_NAME,\n    partition_key_name=PARTITION_KEY_NAME,\n    partition_key_value=ROW_KEY,\n    attribute_name=ATTRIBUTE_NAME,\n    attribute_value=[ROW_ITEM_VALUE, \"another_value\", \"yet_another_value\"],\n    aws_conn_id=AWS_CONN_ID,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Deleting Spanner Database in Python\nDESCRIPTION: Example of using SpannerDeleteDatabaseInstanceOperator to delete a database from a specified Cloud Spanner instance. The operator can be created with or without a project ID, which will be retrieved from the Google Cloud connection if not provided.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/spanner.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_spanner_database_delete]\n# Without project_id\ndelete_database = SpannerDeleteDatabaseInstanceOperator(\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    database_id=\"data_database\",\n    task_id=\"delete_database\",\n)\n\n# With project_id\ndelete_database = SpannerDeleteDatabaseInstanceOperator(\n    project_id=\"{{ var.value.spanner_project }}\",\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    database_id=\"data_database\",\n    task_id=\"delete_database\",\n)\n# [END howto_operator_spanner_database_delete]\n```\n\n----------------------------------------\n\nTITLE: Triggering Airbyte Sync Job Synchronously with Python\nDESCRIPTION: This Python code snippet demonstrates how to use the AirbyteTriggerSyncOperator to trigger an Airbyte synchronization job synchronously. The operator initiates the job specified by `connection_id` using the configured Airbyte connection (`airbyte_conn_id`) and waits for the job to complete before proceeding. This is the default behavior when `asynchronous` is not set or is `False`. Requires the `airflow.providers.airbyte.operators.AirbyteTriggerSyncOperator` class and a configured Airbyte connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/operators/airbyte.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrigger_sync = AirbyteTriggerSyncOperator(\n    task_id=\"airbyte_sync_source_destination_op_sync\",\n    airbyte_conn_id=\"airbyte_conn_example\",\n    connection_id=\"<YOUR_AIRBYTE_CONNECTION_ID>\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Retrieving Created Instance Details\nDESCRIPTION: Example showing how to retrieve details of the created instance using XCom\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncreated_instance = \"{{ task_instance.xcom_pull('create-instance') }}\"\n```\n\n----------------------------------------\n\nTITLE: Creating and Running an Inline Workflow Template Asynchronously in Google Cloud Dataproc\nDESCRIPTION: This code demonstrates how to use inline workflow templates in deferrable mode for asynchronous execution, which is useful for long-running one-time workflow executions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ninstantiate_inline_workflow_template_async = DataprocInstantiateInlineWorkflowTemplateOperator(\n    task_id=\"instantiate_inline_workflow_template_async\",\n    template=WORKFLOW_TEMPLATE,\n    project_id=PROJECT_ID,\n    region=REGION,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Oracle Connection Using Host and Service Name in Python\nDESCRIPTION: Example of configuring an Oracle connection using host and service name in a descriptive format that includes connection details such as protocol, host, port, and service name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/connections/oracle.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nHost = \"(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=dbhost.example.com)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=orclpdb1)))\"\n```\n\n----------------------------------------\n\nTITLE: Synchronous S3 to GCS Transfer Operation in Python\nDESCRIPTION: Example showing how to transfer data from Amazon S3 to Google Cloud Storage using the S3ToGCSOperator in synchronous mode. The operator handles the direct transfer of data between the two storage services.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/s3_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntransfer_from_s3_to_gcs = S3ToGCSOperator(\n    task_id=\"transfer_from_s3_to_gcs\",\n    bucket=\"s3_bucket_name\",\n    prefix=\"s3_prefix\",\n    dest_gcs=\"gs://gcs_bucket_name\",\n    replace=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Producing and Consuming Asset Alias Events in Python with Airflow\nDESCRIPTION: This code demonstrates how to produce asset events using AssetAlias in one DAG and consume them in another DAG. It shows the use of outlet_events for producing and inlet_events for consuming the events.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(dag_id=\"asset-alias-producer\"):\n\n    @task(outlets=[AssetAlias(\"example-alias\")])\n    def produce_asset_events(*, outlet_events):\n        outlet_events[AssetAlias(\"example-alias\")].add(Asset(\"s3://bucket/my-task\"), extra={\"row_count\": 1})\n\n\nwith DAG(dag_id=\"asset-alias-consumer\", schedule=None):\n\n    @task(inlets=[AssetAlias(\"example-alias\")])\n    def consume_asset_alias_events(*, inlet_events):\n        events = inlet_events[AssetAlias(\"example-alias\")]\n        last_row_count = events[-1].extra[\"row_count\"]\n```\n\n----------------------------------------\n\nTITLE: Running a Google Dataprep Job Group in Python\nDESCRIPTION: Example usage of the DataprepRunJobGroupOperator to run a specified job group in Google Dataprep.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataprep.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# [START how_to_dataprep_run_job_group_operator]\n# Example usage code would be here\n# [END how_to_dataprep_run_job_group_operator]\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic SFTP Sensor\nDESCRIPTION: Example showing how to use the basic SFTP sensor to monitor files on an SFTP server. The sensor looks for either specific files or files matching a pattern.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/sensors/sftp_sensor.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.sftp.sensors.sftp import SFTPSensor\n\nsensor = SFTPSensor(\n    task_id=\"sftp_sensor\",\n    path=\"/path/to/file/file.txt\",\n    sftp_conn_id=\"sftp_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Polling ADF Pipeline Status Asynchronously with Deferrable Sensor (Python)\nDESCRIPTION: This example shows how to use the `AzureDataFactoryPipelineRunStatusAsyncSensor` (which is the deferrable version of `AzureDataFactoryPipelineRunStatusSensor`) to periodically check the status of an Azure Data Factory pipeline run. Polling occurs on the Airflow triggerer, making efficient use of worker resources. Requires the pipeline `run_id` and an Azure connection ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/adf_run_pipeline.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../tests/system/microsoft/azure/example_adf_run_pipeline.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_adf_run_pipeline_async]\n#     :end-before: [END howto_operator_adf_run_pipeline_async]\n\n# Example (simulated based on description with dedent: 4):\n    from airflow.providers.microsoft.azure.operators.data_factory import AzureDataFactoryRunPipelineOperator\n    # Note: AzureDataFactoryPipelineRunStatusAsyncSensor is just AzureDataFactoryPipelineRunStatusSensor(deferrable=True)\n    from airflow.providers.microsoft.azure.sensors.data_factory import AzureDataFactoryPipelineRunStatusSensor \n\n    run_pipeline_async = AzureDataFactoryRunPipelineOperator(\n        task_id=\"run_pipeline_async\",\n        pipeline_name=\"pipeline1\",\n        azure_data_factory_conn_id=\"azure_data_factory_default\",\n        wait_for_termination=False,\n    )\n\n    pipeline_run_sensor_async = AzureDataFactoryPipelineRunStatusSensor(\n        task_id=\"pipeline_run_sensor_async\", # Often named reflecting async/deferrable nature\n        run_id=run_pipeline_async.output[\"run_id\"],\n        azure_data_factory_conn_id=\"azure_data_factory_default\",\n        deferrable=True, # This makes it the \"AsyncSensor\"\n    )\n\n    # run_pipeline_async >> pipeline_run_sensor_async\n\n```\n\n----------------------------------------\n\nTITLE: WaitFiveHourTrigger Implementation\nDESCRIPTION: Implementation of a trigger that can either return control to the worker or end the task directly based on configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/deferring.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass WaitFiveHourTrigger(BaseTrigger):\n    def __init__(self, duration: timedelta, *, end_from_trigger: bool = False):\n        super().__init__()\n        self.duration = duration\n        self.end_from_trigger = end_from_trigger\n\n    def serialize(self) -> tuple[str, dict[str, Any]]:\n        return (\n            \"your_module.WaitFiveHourTrigger\",\n            {\"duration\": self.duration, \"end_from_trigger\": self.end_from_trigger},\n        )\n\n    async def run(self) -> AsyncIterator[TriggerEvent]:\n        await asyncio.sleep(self.duration.total_seconds())\n        if self.end_from_trigger:\n            yield TaskSuccessEvent()\n        else:\n            yield TriggerEvent({\"duration\": self.duration})\n```\n\n----------------------------------------\n\nTITLE: Retrieving Dataset Details using Google Cloud VertexAI GetDatasetOperator in Airflow (Python)\nDESCRIPTION: Shows how to use GetDatasetOperator to fetch details for a specific dataset from VertexAI in an Airflow workflow. Dependencies include Airflow and a valid GCP connection. Key inputs: dataset_id, project ID, and region. Returns dataset metadata (schema, name, etc.) as an XCom output.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    get_dataset = GetDatasetOperator(\n        task_id=\"get_dataset\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        dataset_id=DATASET_ID,\n        gcp_conn_id=GCP_CONN_ID,\n    )\n```\n\n----------------------------------------\n\nTITLE: Starting Java Streaming Dataflow Pipeline\nDESCRIPTION: Example demonstrating how to create and run a streaming Java Dataflow pipeline using a JAR from Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n_ = BeamRunJavaPipelineOperator(\n    task_id=\"start-java-streaming-job\",\n    jar=f\"gs://{BUCKET_NAME}/{JAR_FILE}\",\n    job_class=\"org.apache.beam.examples.WindowedWordCount\",\n    pipeline_options=DATAFLOW_STREAMING_OPTIONS,\n    location=location,\n    wait_until_finished=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Google Cloud Batch Job Configuration in Python\nDESCRIPTION: Defines a simple job configuration for Google Cloud Batch, including job name, task groups, and container settings. This configuration is used as input for submitting a job.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_batch.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\njob = {\n    \"name\": JOB_NAME,\n    \"task_groups\": [\n        {\n            \"task_count\": 1,\n            \"task_spec\": {\n                \"runnables\": [\n                    {\n                        \"container\": {\n                            \"image_uri\": \"gcr.io/google-containers/busybox\",\n                            \"entrypoint\": \"/bin/sh\",\n                            \"commands\": [\"-c\", \"echo Hello world! This is task ${BATCH_TASK_INDEX}. This job has a total of ${BATCH_TASK_COUNT} tasks.\"],\n                        }\n                    }\n                ],\n            },\n        }\n    ],\n}\n```\n\n----------------------------------------\n\nTITLE: Defining OperatorLineage Structure for Extraction Results in Python\nDESCRIPTION: This Python data class defines the structure returned by lineage extractors in OpenLineage for Airflow. The OperatorLineage class holds lists of input and output datasets, along with dictionaries for run-level and job-level facets. Dependencies include Airflow's OpenLineage base extractor module and facet models. Parameters include 'inputs', 'outputs', 'run_facets', and 'job_facets', all of which are initialized with factory defaults.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/developer.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@define\\nclass OperatorLineage:\\n    \\\"\\\"\\\"Structure returned from lineage extraction.\\\"\\\"\\\"\\n\\n    inputs: list[Dataset] = Factory(list)\\n    outputs: list[Dataset] = Factory(list)\\n    run_facets: dict[str, RunFacet] = Factory(dict)\\n    job_facets: dict[str, BaseFacet] = Factory(dict)\\n\n```\n\n----------------------------------------\n\nTITLE: Creating SageMaker Processing Job\nDESCRIPTION: Example showing how to use SageMakerProcessingOperator to create a data processing job in Amazon SageMaker for dataset sanitization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprocessing_task = SageMakerProcessingOperator(\n    task_id='processing_task',\n    config={\n        \"ProcessingJobName\": \"demo-processing-job\",\n        \"ProcessingInputs\": [],\n        \"ProcessingOutputs\": [],\n        \"AppSpecification\": {},\n        \"ProcessingResources\": {},\n        \"RoleArn\": \"test_role\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Defining function body for CloudFunctionDeployFunctionOperator in Python\nDESCRIPTION: This snippet demonstrates how to define the function body for CloudFunctionDeployFunctionOperator, including various source code options like sourceArchiveUrl, sourceRepository, and sourceUploadUrl.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/functions.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nbody = {\n    \"name\": FUNCTION_NAME,\n    \"entryPoint\": FUNCTION_NAME,\n    \"runtime\": \"python37\",\n    \"httpsTrigger\": {},\n}\n\n# Option 1 - zip file\nbody[\"sourceArchiveUrl\"] = \"gs://bucket/object.zip\"\n\n# Option 2 - source repo\nbody[\"sourceRepository\"] = {\n    \"url\": \"https://source.developers.google.com/projects/\"  # continue URL\n}\n\n# Option 3 - local filesystem\nzip_path = \"/path/to/function.zip\"\nbody[\"sourceUploadUrl\"] = \"\"\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Data into PostgreSQL with copy_expert\nDESCRIPTION: Uses PostgresHook to establish a database connection and loads CSV data from a file into a temporary employees table using PostgreSQL's COPY command.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/pipeline.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npostgres_hook = PostgresHook(postgres_conn_id=\"tutorial_pg_conn\")\nconn = postgres_hook.get_conn()\ncur = conn.cursor()\nwith open(data_path, \"r\") as file:\n    cur.copy_expert(\n        \"COPY employees_temp FROM STDIN WITH CSV HEADER DELIMITER AS ',' QUOTE '\\\"'\",\n        file,\n    )\nconn.commit()\n```\n\n----------------------------------------\n\nTITLE: Listing Cloud Composer Environments in Python\nDESCRIPTION: This snippet demonstrates how to use the CloudComposerListEnvironmentsOperator to list all Cloud Composer environments in a specific project and region.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlist_environments_task = CloudComposerListEnvironmentsOperator(\n    task_id=\"list-environments\",\n    project_id=PROJECT_ID,\n    region=REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Branching DAG Based on Current Time - Airflow Python\nDESCRIPTION: Demonstrates how to use the BranchDateTimeOperator in Airflow with the current time as the basis for a branching decision in a DAG. This example shows how to configure the operator with target_lower and target_upper parameters, which can be datetime.datetime, datetime.time, or None, influencing whether the branching occurs within a certain time range. Dependencies include Airflow core and the standard providers package. The main input is the current system time when the DAG executes, and the output is which branch is selected for downstream tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/datetime.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith DAG('example_branch_datetime_operator', schedule_interval='@hourly', start_date=datetime(2021, 1, 1)) as dag:\n    branch = BranchDateTimeOperator(\n        task_id='branch_datetime',\n        follow_task_ids_if_true=['process_data'],\n        follow_task_ids_if_false=['skip_data'],\n        target_lower=time(8, 0),\n        target_upper=time(16, 0),\n        use_task_logical_date=False,\n    )\n    process_data = DummyOperator(task_id='process_data')\n    skip_data = DummyOperator(task_id='skip_data')\n    branch >> [process_data, skip_data]\n\n```\n\n----------------------------------------\n\nTITLE: Monitoring Redshift Cluster State with RedshiftClusterSensor\nDESCRIPTION: Example demonstrating how to monitor the state of a Redshift cluster until it reaches the target state using RedshiftClusterSensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/redshift/redshift_cluster.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncluster_sensor = RedshiftClusterSensor(\n    task_id='wait_for_cluster',\n    cluster_identifier=redshift_cluster_identifier,\n    target_status='available',\n    aws_conn_id='aws_default',\n)\n```\n\n----------------------------------------\n\nTITLE: SparkSubmitOperator Fields Example\nDESCRIPTION: Code example showing deprecated and renamed fields in SparkSubmitOperator, including queue being renamed to yarn_queue.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"queue\" # Deprecated field name\n\"yarn_queue\" # New field name for SparkSubmitOperator\n```\n\n----------------------------------------\n\nTITLE: Creating AutoML Video Tracking Job in Vertex AI\nDESCRIPTION: Example showing how to create an AutoML video tracking job for object tracking in videos.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nCreateAutoMLVideoTrainingJobOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    display_name=TRAINING_JOB_NAME,\n    dataset_id=VIDEO_DATASET,\n    prediction_type=\"action_recognition\",\n    model_type=\"CLOUD\",\n    training_budget_milli_node_hours=8000,\n    sync=True,\n    task_id=\"training_job\",)\n```\n\n----------------------------------------\n\nTITLE: Implementing get_openlineage_facets_on_complete for GcsToGcsOperator (Python)\nDESCRIPTION: Example implementation of the `get_openlineage_facets_on_complete` method within the `GcsToGcsOperator`. This method is called upon successful task completion. It constructs and returns an `OperatorLineage` object containing input and output datasets based on the operator's resolved source and destination GCS objects/buckets, which are typically determined during the `execute` phase. Note the local import of OpenLineage classes, a best practice to ensure the operator functions even if the OpenLineage provider is not installed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/developer.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_openlineage_facets_on_complete(self, task_instance):\n    \"\"\"\n    Implementing _on_complete because execute method does preprocessing on internals.\n    This means we won't have to normalize self.source_object and self.source_objects,\n    destination bucket and so on.\n    \"\"\"\n    from airflow.providers.common.compat.openlineage.facet import Dataset\n    from airflow.providers.openlineage.extractors import OperatorLineage\n\n    return OperatorLineage(\n        inputs=[\n            Dataset(namespace=f\"gs://{self.source_bucket}\", name=source)\n            for source in sorted(self.resolved_source_objects)\n        ],\n        outputs=[\n            Dataset(namespace=f\"gs://{self.destination_bucket}\", name=target)\n            for target in sorted(self.resolved_target_objects)\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Debugging KubernetesPodOperator in Python\nDESCRIPTION: Demonstrates how to use the dry_run method to print out the Kubernetes manifest for the pod that would be created at runtime.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n\nk = KubernetesPodOperator(\n    name=\"hello-dry-run\",\n    image=\"debian\",\n    cmds=[\"bash\", \"-cx\"],\n    arguments=[\"echo\", \"10\"],\n    labels={\"foo\": \"bar\"},\n    task_id=\"dry_run_demo\",\n    do_xcom_push=True,\n)\n\nk.dry_run()\n```\n\n----------------------------------------\n\nTITLE: Manual Airflow Component Setup\nDESCRIPTION: Commands to manually initialize and run individual Airflow components instead of using standalone mode.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/start.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nairflow db migrate\n\nairflow users create \\\n    --username admin \\\n    --firstname Peter \\\n    --lastname Parker \\\n    --role Admin \\\n    --email spiderman@superhero.org\n\nairflow api-server --port 8080\n\nairflow scheduler\n\nairflow dag-processor\n\nairflow triggerer\n```\n\n----------------------------------------\n\nTITLE: Waiting on EMR Notebook Execution State using EmrNotebookExecutionSensor in Python\nDESCRIPTION: Demonstrates using `EmrNotebookExecutionSensor` to pause DAG execution until a specific EMR notebook execution reaches a target state (e.g., 'FINISHED', 'FAILED'). Requires the `notebook_execution_id` to monitor and an `aws_conn_id`. Optional parameters include `target_states` and `failed_states`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwait_for_notebook_execution = EmrNotebookExecutionSensor(\n    task_id=\"wait_for_notebook_execution\",\n    notebook_execution_id=notebook_execution_id,\n    target_states=[\"FINISHED\"],\n    failed_states=[\"FAILED\"],\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Google Cloud Workflow using Airflow Operator in Python\nDESCRIPTION: Demonstrates fetching details of a specific workflow using the `WorkflowsGetWorkflowOperator`. Requires identifying the workflow via `workflow_id` and `location_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/workflows.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/workflows/example_workflows.py\n      :language: python\n      :dedent: 4\n      :start-after: [START how_to_get_workflow]\n      :end-before: [END how_to_get_workflow]\n```\n\n----------------------------------------\n\nTITLE: Running Python Pipeline with DataflowRunner using GCS File in Deferrable Mode\nDESCRIPTION: This example demonstrates executing a Python pipeline on Google Cloud Dataflow in deferrable mode. It allows Airflow to asynchronously monitor the Dataflow job completion, freeing up worker resources during execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_dataflow_runner_gcs_file_async = BeamRunPythonPipelineOperator(\n    task_id=\"beam_task_dataflow_runner_gcs_file_async\",\n    py_file=\"{{ var.json.beam_variables.gcs_file_path }}\",\n    runner=\"DataflowRunner\",\n    pipeline_options={\n        \"output\": \"{{ var.json.beam_variables.output_path }}\",\n        \"temp_location\": \"{{ var.json.beam_variables.gcp_dataflow_temp }}\",\n        \"staging_location\": \"{{ var.json.beam_variables.gcp_dataflow_staging }}\",\n        \"project\": \"{{ var.json.beam_variables.gcp_project }}\",\n        \"region\": \"{{ var.json.beam_variables.gcp_region }}\",\n        \"job_name\": \"{{task.task_id}}\",\n    },\n    py_interpreter=\"python3\",\n    py_system_site_packages=False,\n    py_requirements=[\n        \"apache-beam[gcp]==2.44.0\",\n    ],\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Making POST Requests with HttpOperator in Python\nDESCRIPTION: This example uses `HttpOperator` to send a POST request with JSON data to the `/post` endpoint of `httpbin` via the `http_default` connection. It includes a `response_check` lambda function to verify that the returned JSON data matches the sent data.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/operators.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntask_post_op = SimpleHttpOperator(\n    task_id=\"post_op\",\n    http_conn_id=\"http_default\",\n    endpoint=\"post\",\n    data=json.dumps({\"priority\": 5}),\n    headers={\"Content-Type\": \"application/json\"},\n    response_check=lambda response: response.json()[\"json\"][\"priority\"] == 5,\n)\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Amazon S3 Glacier to Google Cloud Storage using Python\nDESCRIPTION: This code snippet demonstrates how to use the GlacierToGCSOperator to transfer data from an Amazon Glacier vault to Google Cloud Storage. It includes setting up the task with necessary parameters such as AWS and GCS credentials, vault name, and file paths.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/glacier_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntransfer_glacier_to_gcs = GlacierToGCSOperator(\n    task_id=\"transfer_glacier_to_gcs\",\n    aws_conn_id=\"aws_default\",\n    gcp_conn_id=\"google_cloud_default\",\n    vault_name=\"{{ var.json.glacier_to_gcs_config['vault_name'] }}\",\n    bucket_name=\"{{ var.json.glacier_to_gcs_config['bucket_name'] }}\",\n    object_name=\"{{ var.json.glacier_to_gcs_config['object_name'] }}\",\n    filename=\"{{ var.json.glacier_to_gcs_config['filename'] }}\",\n    gzip=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Executing a Google Cloud Run Job using CloudRunExecuteJobOperator in Airflow\nDESCRIPTION: Demonstrates how to use the CloudRunExecuteJobOperator to execute a Cloud Run job in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_run.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nexecute_job = CloudRunExecuteJobOperator(\n    task_id=\"execute_job\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    job_id=JOB_NAME\n)\n```\n\n----------------------------------------\n\nTITLE: Triggering Azure Batch Task with AzureBatchOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the AzureBatchOperator to trigger a task on Azure Batch. It includes the necessary imports and configuration for setting up the operator within an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/batch.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../tests/system/microsoft/azure/example_azure_batch_operator.py\n    :language: python\n    :dedent: 0\n    :start-after: [START howto_azure_batch_operator]\n    :end-before: [END howto_azure_batch_operator]\n```\n\n----------------------------------------\n\nTITLE: Running Databricks Job with DatabricksRunNowOperator\nDESCRIPTION: Example showing how to pair DatabricksCreateJobsOperator with DatabricksRunNowOperator to execute the created job\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/jobs_create.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrun_job = DatabricksRunNowOperator(\n    task_id=\"run_now\",\n    databricks_conn_id=DATABRICKS_CONN_ID,\n    job_id=\"{{ task_instance.xcom_pull('create_json')['job_id'] }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting default impersonation in Airflow configuration\nDESCRIPTION: This code snippet demonstrates how to set a default impersonation user in the Airflow configuration file. This setting is used when 'run_as_user' is not specified for a task.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/workload.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[core]\ndefault_impersonation = airflow\n```\n\n----------------------------------------\n\nTITLE: Copying a Single File from GCS to SFTP in Python\nDESCRIPTION: This example demonstrates how to copy a single file from Google Cloud Storage to an SFTP server using the GCSToSFTPOperator. It specifies the source bucket, object path in GCS, and the destination path on the SFTP server.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_sftp.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncopy_file_from_gcs_to_sftp = GCSToSFTPOperator(\n    task_id=\"copy-file-from-gcs-to-sftp\",\n    source_bucket=BUCKET_NAME,\n    source_object=GCS_SRC_FILE,\n    destination_path=SFTP_DST_FILE,\n    move_object=False,\n    sftp_conn_id=\"sftp_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Azure Connection via Environment Variable in Bash\nDESCRIPTION: This Bash snippet demonstrates how to configure the default Azure connection (`azure_default`) using the `AIRFLOW_CONN_AZURE_DEFAULT` environment variable. It utilizes URI syntax where connection parameters are provided in the query string. In this example, it specifies the `key_path` for JSON file authentication, and the path value (`/keys/key.json`) is URL-encoded (`%2Fkeys%2Fkey.json`). This method allows setting Airflow connections externally.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/azure.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AZURE_DEFAULT='azure://?key_path=%2Fkeys%2Fkey.json'\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from HTTP to Amazon S3 using HttpToS3Operator in Python\nDESCRIPTION: This code snippet demonstrates how to use the HttpToS3Operator in Apache Airflow to transfer content from a HTTP endpoint to an Amazon S3 file. It shows the configuration of the operator with necessary parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/http_to_s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models import DAG\nfrom airflow.providers.amazon.aws.transfers.http_to_s3 import HttpToS3Operator\nfrom airflow.utils.dates import days_ago\n\nwith DAG(\n    dag_id='example_http_to_s3',\n    start_date=days_ago(1),\n    tags=['example'],\n    catchup=False,\n) as dag:\n    # [START howto_transfer_http_to_s3]\n    upload_to_s3 = HttpToS3Operator(\n        task_id='upload_to_s3',\n        endpoint='https://www.google.com',\n        bucket_name='example-bucket',\n        s3_key='example-key',\n        replace=True,\n    )\n    # [END howto_transfer_http_to_s3]\n```\n\n----------------------------------------\n\nTITLE: Synchronizing to GCS Bucket Subdirectory\nDESCRIPTION: Shows how to synchronize files from a source bucket to a specific subdirectory in the destination bucket. Files are copied without overwriting existing files or deleting extra files in the destination subdirectory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gcs.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsync_to_subdir = GCSToGCSOperator(\n    task_id=\"sync_to_subdir\",\n    source_bucket=BUCKET_1_SRC,\n    destination_bucket=BUCKET_1_DST,\n    source_object=\"*\",\n    destination_object=\"subdir\",\n    allow_overwrite=False,\n    delete_extra_files=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Triggering a dbt Cloud Job in Python\nDESCRIPTION: This snippet demonstrates how to use the DbtCloudRunJobOperator to trigger a dbt Cloud job with synchronous waiting for run termination. The account_id is referenced from the default_args of the DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndbt_cloud_run_job = DbtCloudRunJobOperator(\n    task_id=\"dbt_cloud_run_job\",\n    job_id=JOB_ID,\n    check_interval=CHECK_INTERVAL,\n    timeout=TIMEOUT,\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Bash Script without Jinja Templating using BashOperator in Python\nDESCRIPTION: Shows how to execute an external Bash script using the `BashOperator` while disabling Jinja templating. A space is appended to the `bash_command` parameter (containing the script path) to achieve this.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nrun_script = BashOperator(\n    task_id=\"run_command_from_script\",\n    bash_command=\"$AIRFLOW_HOME/scripts/example.sh \",\n)\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Query on Apache Druid using SQLExecuteQueryOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the SQLExecuteQueryOperator to execute a SQL query on an Apache Druid cluster. It includes setting up the connection, defining the SQL query, and creating the operator task.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/druid/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': days_ago(1),\n}\n\ndag = DAG(\n    'example_druid_operator',\n    default_args=default_args,\n    tags=['example'],\n)\n\n# [START howto_operator_druid]\nsql_query = \"\"\"\nSELECT COUNT(*)\nFROM wikiticker\nWHERE\n  \"__time\" >= TIMESTAMP '2015-09-12 00:00:00'\n  AND \"__time\" < TIMESTAMP '2015-09-13 00:00:00'\n\"\"\"\n\ndruid_operator_task = SQLExecuteQueryOperator(\n    task_id='druid_task',\n    conn_id='druid_demo_conn',\n    sql=sql_query,\n    dag=dag,\n)\n# [END howto_operator_druid]\n```\n\n----------------------------------------\n\nTITLE: Listing Repositories with GithubOperator in Airflow\nDESCRIPTION: Example of using GithubOperator to list all repositories owned by a user. This snippet demonstrates how to implement the PyGithub client.get_user().get_repos() method using Airflow's GithubOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/github/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nlist_user_repos = GithubOperator(\n    task_id=\"list_user_repos\",\n    github_method=\"get_user\",\n    github_method_args={},\n    result_processor=lambda res: [repo.full_name for repo in res.get_repos()],\n)\n```\n\n----------------------------------------\n\nTITLE: Emitting Multiple Assets from a Task using Python in Airflow\nDESCRIPTION: This snippet illustrates how to emit multiple assets from a single task using the @task decorator with multiple outlets. It processes an input asset and splits it into two output assets.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import DAG, Asset, task\n\ninput_asset = Asset(\"input_asset\")\nout_asset_1 = Asset(\"out_asset_1\")\nout_asset_2 = Asset(\"out_asset_2\")\n\nwith DAG(dag_id=\"process_input\", schedule=None):\n\n    @task(inlets=[input_asset], outlets=[out_asset_1, out_asset_2])\n    def process_input():\n        \"\"\"Split input into two.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Initializing DAG with Catchup Configuration in Python\nDESCRIPTION: Example code demonstrating how to create a DAG with specific configuration including catchup settings, start date, schedule interval, and retry parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dag-run.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\\nCode that goes along with the Airflow tutorial located at:\\nhttps://github.com/apache/airflow/blob/main/airflow/example_dags/tutorial.py\\n\"\"\"\\n\\nfrom airflow.sdk import DAG\\nfrom airflow.providers.standard.operators.bash import BashOperator\\n\\nimport datetime\\nimport pendulum\\n\\ndag = DAG(\\n    \"tutorial\",\\n    default_args={\\n        \"depends_on_past\": True,\\n        \"retries\": 1,\\n        \"retry_delay\": datetime.timedelta(minutes=3),\\n    },\\n    start_date=pendulum.datetime(2015, 12, 1, tz=\"UTC\"),\\n    description=\"A simple tutorial DAG\",\\n    schedule=\"@daily\",\\n)\n```\n\n----------------------------------------\n\nTITLE: Moving Single File from SFTP to GCS with Destination Path\nDESCRIPTION: Example demonstrating how to move a single file from SFTP to Google Cloud Storage with a specific destination path. The move_object parameter causes the original file to be deleted after transfer.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/sftp_to_gcs.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmove_single_file_destination = SFTPToGCSOperator(\n    task_id=\"move-single-file-destination\",\n    source_path=\"path/to/{{ds}}/data.csv\",\n    destination_bucket=BUCKET_NAME,\n    destination_path=\"path/in/bucket/data.csv\",\n    move_object=True,\n    sftp_conn_id=\"sftp_default\",\n    gcp_conn_id=\"google_cloud_default\",\n    dag=dag\n)\n```\n\n----------------------------------------\n\nTITLE: Using CloudDataTransferServiceRunJobOperator in Python\nDESCRIPTION: Example of using the CloudDataTransferServiceRunJobOperator to run a transfer job in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrun_transfer = CloudDataTransferServiceRunJobOperator(\n    task_id=\"run_transfer\",\n    job_name=\"{{ task_instance.xcom_pull('create_transfer', key='name') }}\",\n    project_id=PROJECT_ID,\n    gcp_conn_id=GCP_CONN_ID\n)\n```\n\n----------------------------------------\n\nTITLE: Registering Dynamic DAGs with @dag Decorator in Python\nDESCRIPTION: This example shows how to dynamically generate and register multiple DAGs using the @dag decorator. It creates a separate DAG for each configuration in the 'configs' dictionary.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/dynamic-dag-generation.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom airflow.sdk import dag, task\n\nconfigs = {\n    \"config1\": {\"message\": \"first DAG will receive this message\"},\n    \"config2\": {\"message\": \"second DAG will receive this message\"},\n}\n\nfor config_name, config in configs.items():\n    dag_id = f\"dynamic_generated_dag_{config_name}\"\n\n    @dag(dag_id=dag_id, start_date=datetime(2022, 2, 1))\n    def dynamic_generated_dag():\n        @task\n        def print_message(message):\n            print(message)\n\n        print_message(config[\"message\"])\n\n    dynamic_generated_dag()\n```\n\n----------------------------------------\n\nTITLE: Pulling Messages with PubSubPullOperator\nDESCRIPTION: Example showing how to pull messages using the PubSubPullOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/pubsub.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npull_task = PubSubPullOperator(\n    task_id=\"pull_task\",\n    project_id=PROJECT_ID,\n    subscription=SUBSCRIPTION_ID,\n    max_messages=5,\n    return_immediately=False,\n    ack_messages=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Priority Weight Strategy in Python\nDESCRIPTION: Example showing how to create a custom priority weight strategy by extending the PriorityWeightStrategy class. This implementation demonstrates a decreasing priority weight calculation based on task depth.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/priority-weight.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass DecreasingPriorityWeightStrategy(PriorityWeightStrategy):\n    \"\"\"A strategy where task priority decreases as depth increases.\"\"\"\n\n    def __init__(self, base_weight: int = 10) -> None:\n        \"\"\"Initialize the strategy with a base weight.\"\"\"\n        self.base_weight = base_weight\n\n    def apply(self, task: BaseOperator) -> int:\n        \"\"\"Apply the priority weight strategy to the task.\"\"\"\n        return self.base_weight - task.depth\n```\n\n----------------------------------------\n\nTITLE: Invoking AWS Lambda Function with Airflow Operator\nDESCRIPTION: Example demonstrating the use of LambdaInvokeFunctionOperator to invoke an AWS Lambda function. Includes handling of synchronous invocation timeouts and configuration options.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/lambda.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlambda_invoke_task = LambdaInvokeFunctionOperator(\n    task_id=\"invoke_lambda_function\",\n    function_name=\"test_function\",\n    payload=json.dumps({\"SampleEvent\": {\"SampleData\": { \"Name\": \"XYZ\"}}}),\n    aws_conn_id=\"aws_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataproc Cluster with Async Mode in Python\nDESCRIPTION: This snippet demonstrates how to create a Dataproc cluster using the DataprocCreateClusterOperator in deferrable (async) mode. This allows the operator to run asynchronously.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster_operator_async = DataprocCreateClusterOperator(\n    task_id=\"create_dataproc_cluster_async\",\n    project_id=PROJECT_ID,\n    cluster_config=CLUSTER_CONFIG,\n    region=REGION,\n    cluster_name=CLUSTER_NAME,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Granting Read-Only Airflow Permissions to a User Group in Cedar (Viewer Role)\nDESCRIPTION: This Cedar policy grants read-only permissions to users within a specified AWS IAM Identity Center group (identified by ID `aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee`). It allows specific GET actions across various Airflow resources like Configuration, Connection, DAGs, Pools, Variables, Assets, and Views. This policy is equivalent to the 'Viewer' role in the default Flask AppBuilder authentication manager.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/manage/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: cedar\nCODE:\n```\npermit(\n  principal in Airflow::Group::\"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\",\n  action in [\n    Airflow::Action::\"Configuration.GET\",\n    Airflow::Action::\"Connection.GET\",\n    Airflow::Action::\"Custom.GET\",\n    Airflow::Action::\"Dag.GET\",\n    Airflow::Action::\"Menu.MENU\",\n    Airflow::Action::\"Pool.GET\",\n    Airflow::Action::\"Variable.GET\",\n    Airflow::Action::\"Asset.GET\",\n    Airflow::Action::\"AssetAlias.GET\",\n    Airflow::Action::\"Backfill.GET\",\n    Airflow::Action::\"View.GET\"\n  ],\n  resource\n);\n```\n\n----------------------------------------\n\nTITLE: Extracting Image Label Detection Result from XCom - Airflow Python Operator\nDESCRIPTION: This Python snippet illustrates retrieving label detection results from XCom after running a CloudVisionDetectImageLabelsOperator. It enables further processing by downstream tasks by accessing the labels output. The snippet assumes proper configuration of the task and Airflow context.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nresult = detect_labels.output\n\n```\n\n----------------------------------------\n\nTITLE: Submitting an AWS Batch Job using BatchOperator in Python\nDESCRIPTION: Demonstrates how to use the Airflow BatchOperator (:class:`~airflow.providers.amazon.aws.operators.batch.BatchOperator`) to submit a new AWS Batch job and monitor it until completion. The operator handles the interaction with the AWS Batch service for job submission and status polling.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/batch.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../amazon/tests/system/amazon/aws/example_batch.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_batch]\n    :end-before: [END howto_operator_batch]\n```\n\n----------------------------------------\n\nTITLE: Updating VertexAI Dataset with Google Cloud VertexAI UpdateDatasetOperator in Airflow (Python)\nDESCRIPTION: Demonstrates updating metadata or configuration of an existing VertexAI dataset using UpdateDatasetOperator. Requires dataset_id, update_mask (fields to update), and updated values. User specifies what attributes to modify; updates dataset accordingly with output in XCom as confirmation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n    update_dataset = UpdateDatasetOperator(\n        task_id=\"update_dataset\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        dataset_id=DATASET_ID,\n        update_mask=UPDATE_MASK,\n        metadata=UPDATED_DATASET,\n        gcp_conn_id=GCP_CONN_ID,\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining a GKE Cluster in Python\nDESCRIPTION: Example of defining a Google Kubernetes Engine cluster configuration using a Python dictionary. This defines the cluster name, location, network settings, and node pools.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nCLUSTER = {\n    \"name\": CLUSTER_NAME,\n    \"node_pools\": [{\n        \"name\": \"default-pool\",\n        \"initial_node_count\": 1\n    }],\n    \"network\": f\"projects/{PROJECT_ID}/global/networks/{NETWORK}\",\n    \"subnetwork\": f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{SUBNETWORK}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Dataproc Cluster on GKE with Python\nDESCRIPTION: This snippet shows how to create a Dataproc cluster on Google Kubernetes Engine (GKE) using the DataprocCreateClusterOperator. It includes configuration specific to GKE clusters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCLUSTER_CONFIG_GKE = {\n    \"gke_cluster_config\": {\n        \"gke_cluster_target\": GKE_CLUSTER_LINK,\n        \"node_pool_target\": [\n            {\n                \"node_pool\": GKE_NODE_POOL_LINK,\n                \"roles\": [\"DEFAULT\"],\n                \"node_pool_config\": {\n                    \"config\": {\n                        \"machine_type\": \"n1-standard-4\",\n                    },\n                    \"node_count\": 2,\n                },\n            }\n        ],\n    },\n    \"software_config\": {\n        \"image_version\": \"2.0-debian10\",\n    },\n}\n```\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster_operator = DataprocCreateClusterOperator(\n    task_id=\"create_dataproc_cluster\",\n    project_id=PROJECT_ID,\n    cluster_config=CLUSTER_CONFIG_GKE,\n    region=REGION,\n    cluster_name=CLUSTER_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: CloudSQL Import Operator Templated Fields (Python)\nDESCRIPTION: Exposes which fields of CloudSQLImportInstanceOperator support Airflow's Jinja templating, allowing run-time injection of dynamic values. This can help parametrize your DAG and SQL imports for different environments. You need Airflow's Google provider and templates can be provided using Jinja syntax. Output specifies the operator's template_fields property.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"template_fields = (\\\"project_id\\\", \\\"body\\\", \\\"instance\\\")\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Sentiment Analysis Results in Apache Airflow\nDESCRIPTION: This snippet shows how to retrieve and process the results of sentiment analysis using XCom in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/natural_language.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsentiment = analyze_sentiment.output[\"sentiment\"]\nprint(f\"Score: {sentiment.score}\")\nprint(f\"Magnitude: {sentiment.magnitude}\")\n```\n\n----------------------------------------\n\nTITLE: Uploading In-Memory File Contents with SlackAPIFileOperator in Python\nDESCRIPTION: This snippet provides an example of sending file content directly (in-memory) to Slack using the SlackAPIFileOperator in Python within Airflow. It requires Airflow's Slack provider, a Slack connection, and appropriate permissions. The user supplies the target channels and the file content (contents parameter) rather than a file path. The operator uploads the data to Slack where it appears as a file posted to the specified channels. This method avoids the need for a physical file but may be constrained by Slacks file upload limitations (size, type, etc.).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/operators/slack_api.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.providers.slack.operators.slack import SlackAPIFileOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2021, 1, 1),\n}\n\ndag = DAG(\n    'example_slack_file_upload_content',\n    default_args=default_args,\n    schedule_interval=None,\n    catchup=False,\n)\n\nupload_content = SlackAPIFileOperator(\n    task_id='upload_content',\n    channels=['#random'],\n    filename='example.txt',\n    contents='This is a file sent from Airflow without saving to disk!',\n    initial_comment='See attached generated file.',\n    dag=dag,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Passing Parameters to External SQL File via `params` (Python)\nDESCRIPTION: Shows how to use the `params` argument (inherited from `BaseOperator`) in `SQLExecuteQueryOperator` to pass parameters to an external SQL file (`sql/birth_date.sql`) that utilizes Jinja templating (`{{ params.key }}`). This allows dynamic value injection into SQL stored in files.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/operators.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nget_birth_date = SQLExecuteQueryOperator(\n    task_id=\"get_birth_date\",\n    conn_id=\"postgres_default\",\n    sql=\"sql/birth_date.sql\",\n    params={\"begin_date\": \"2020-01-01\", \"end_date\": \"2020-12-31\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Running Asynchronous Dataflow Python Job in Airflow\nDESCRIPTION: Demonstrates how to start a native Dataflow Python job asynchronously using Airflow. Asynchronous execution relies on the pipeline code itself not blocking (e.g., not calling `wait_until_finish`). The example likely uses a Dataflow Python operator within an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_native_python_async.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_start_python_job_async]\n    :end-before: [END howto_operator_start_python_job_async]\n```\n\n----------------------------------------\n\nTITLE: Implementing Apprise Notifications in Apache Airflow DAG\nDESCRIPTION: This code snippet demonstrates how to set up a DAG with Apprise notifications for both DAG-level success and task-level failure callbacks. It uses the AppriseNotifier to send customized messages to multiple services.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/docs/notifications/apprise_notifier_howto_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.apprise.notifications.apprise import send_apprise_notification\nfrom apprise import NotifyType\n\nwith DAG(\n    dag_id=\"apprise_notifier_testing\",\n    schedule=None,\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    on_success_callback=[\n        send_apprise_notification(body=\"The dag {{ dag.dag_id }} succeeded\", notify_type=NotifyType.SUCCESS)\n    ],\n):\n    BashOperator(\n        task_id=\"mytask\",\n        on_failure_callback=[\n            send_apprise_notification(body=\"The task {{ ti.task_id }} failed\", notify_type=NotifyType.FAILURE)\n        ],\n        bash_command=\"fail\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Commit Information for Google Cloud Datastore in Python\nDESCRIPTION: This snippet demonstrates the definition of commit information required by the CloudDatastoreCommitOperator. It includes the transaction ID and mutations for the commit operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nCOMMIT = {\n    \"transaction\": \"{{ task_instance.xcom_pull('begin_transaction1')[0] }}\",\n    \"mutations\": [\n        {\n            \"insert\": {\n                \"key\": {\n                    \"partitionId\": {\"projectId\": GCP_PROJECT_ID},\n                    \"path\": [{\"kind\": \"airflow-system-test-entities\", \"name\": \"test-entity-1\"}],\n                },\n                \"properties\": {\"symbol\": {\"stringValue\": \"GOOG\"}},\n            }\n        }\n    ],\n    \"mode\": \"TRANSACTIONAL\",\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Tags List from XCom in Python\nDESCRIPTION: Example of retrieving the list of tags from XCom after listing them with CloudDataCatalogListTagsOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_list_tags_result]\nlist_tags_result = kwargs[\"ti\"].xcom_pull(task_ids=\"list_tags\")\nprint(list_tags_result)\n# [END howto_operator_gcp_datacatalog_list_tags_result]\n```\n\n----------------------------------------\n\nTITLE: Updating BigQuery Table - Python\nDESCRIPTION: Example of using BigQueryUpdateTableOperator to update an existing table in BigQuery.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nupdate_table_task = BigQueryUpdateTableOperator(\n    task_id=\"update_table\",\n    dataset_id=DATASET_NAME,\n    table_id=TABLE_NAME,\n    fields=[{\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Starting an Amazon EMR Serverless Job with Airflow - Python\nDESCRIPTION: Shows how to initiate a new job on an existing EMR Serverless Application using EmrServerlessStartJobOperator in Airflow. Dependencies include airflow.providers.amazon and AIobotocore for async operation (if deferrable=True). Key parameters are application_id, execution_role_arn, and job_driver; expected input includes valid job specifications, and output is a job run ID. Logging and UI link configurations are available if enabled.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr_serverless.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.amazon.aws.operators.emr import EmrServerlessStartJobOperator\n\nstart_emr_serverless_job = EmrServerlessStartJobOperator(\n    task_id=\"start_emr_serverless_job\",\n    application_id=\"{{ task_instance.xcom_pull(task_ids='create_emr_serverless_application') }}\",\n    execution_role_arn=\"arn:aws:iam::123456789012:role/EMRServerlessExecutionRole\",\n    job_driver={\n        \"sparkSubmit\": {\n            \"entryPoint\": \"s3://my-bucket/my-app.py\",\n        }\n    },\n    configuration_overrides={},\n    enable_application_ui_links=True,  # Optional\n    deferrable=True,  # Optional\n    dag=dag,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Asset in Python\nDESCRIPTION: Demonstrates how to create a basic Asset object using a URI string in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import Asset\n\nexample_asset = Asset(\"s3://asset-bucket/example.csv\")\n```\n\n----------------------------------------\n\nTITLE: Setting up MySQL Database and User for Airflow in SQL\nDESCRIPTION: This SQL snippet creates a new database, user, and grants privileges for Airflow to use with MySQL. It sets up a database named 'airflow_db' and a user 'airflow_user' with the password 'airflow_pass'.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-up-database.rst#2025-04-22_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nCREATE DATABASE airflow_db CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\nCREATE USER 'airflow_user' IDENTIFIED BY 'airflow_pass';\nGRANT ALL PRIVILEGES ON airflow_db.* TO 'airflow_user';\n```\n\n----------------------------------------\n\nTITLE: Pausing Amazon Redshift Cluster with RedshiftPauseClusterOperator\nDESCRIPTION: Example demonstrating how to pause an available Redshift cluster using RedshiftPauseClusterOperator. Supports deferrable execution mode.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/redshift/redshift_cluster.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npause_cluster = RedshiftPauseClusterOperator(\n    task_id='pause_cluster',\n    cluster_identifier=redshift_cluster_identifier,\n    aws_conn_id='aws_default',\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Google Cloud Tasks Queue in Python\nDESCRIPTION: This snippet demonstrates how to create a new Google Cloud Tasks queue using the CloudTasksQueueCreateOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START create_queue]\nresult = CloudTasksQueueCreateOperator(\n    task_id=\"create_queue\",\n    location=LOCATION,\n    queue_name=QUEUE_ID,\n    retry=Retry(maximum=10.0),\n    timeout=5,\n).execute(context=context)\n# [END create_queue]\n```\n\n----------------------------------------\n\nTITLE: Creating Amazon Managed Service for Apache Flink Application - Python\nDESCRIPTION: Example code showing how to create an Amazon Managed Service for Apache Flink application using the KinesisAnalyticsV2CreateApplicationOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/kinesis_analytics.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_application = KinesisAnalyticsV2CreateApplicationOperator(\n    task_id=\"create_application\",\n    application_name=APPLICATION_NAME,\n    runtime_environment=\"SQL-1_0\",\n    service_execution_role=SERVICE_ROLE_ARN,\n    application_configuration={\n        \"SqlApplicationConfiguration\": {\n            \"Inputs\": [\n                {\n                    \"NamePrefix\": \"SOURCE_SQL_STREAM\",\n                    \"KinesisStreamsInput\": {\"ResourceARN\": INPUT_STREAM_ARN},\n                    \"InputSchema\": \"{}\",\n                }\n            ]\n        }\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Database Session Handling with provide_session Decorator\nDESCRIPTION: Demonstrates using the provide_session decorator for functions intended to be called by DAG authors, with automatic session management.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sqlalchemy.orm import Session\n\nfrom airflow.utils.session import NEW_SESSION, provide_session\n\n\n@provide_session\ndef my_method(arg, *, session: Session = NEW_SESSION):\n    ...\n    # You SHOULD not commit the session here. The wrapper will take care of commit()/rollback() if exception\n```\n\n----------------------------------------\n\nTITLE: Retrieving from Amazon Bedrock Knowledge Base via Operator (Python)\nDESCRIPTION: Retrieves relevant citations from a Bedrock Knowledge Base by executing BedrockRetrieveOperator. The snippet submits a query string and returns only relevant results to the user, requiring preconfigured knowledge base and operator permissions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nretrieve_task = BedrockRetrieveOperator(\n    task_id=\"retrieve_from_kb\",\n    knowledge_base_id=\"kb-abcdef123456\",\n    input_text=\"What are the main features of Amazon Bedrock?\",\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Waiting for EKS Cluster State using EksClusterStateSensor in Python\nDESCRIPTION: This Python snippet demonstrates using the `EksClusterStateSensor` to pause a DAG run until a specified EKS cluster reaches a target state (e.g., 'ACTIVE') or a terminal state. Requires `cluster_name` and optionally `target_state`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eks.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Example using EksClusterStateSensor\n# Assumes necessary imports and DAG context\n\nwait_for_cluster_active = EksClusterStateSensor(\n    task_id=\"wait_for_cluster_active\",\n    cluster_name=\"my-eks-cluster\",\n    target_state=ClusterStates.ACTIVE, # Or other ClusterStates enum value\n)\n\n# [END howto_sensor_eks_cluster]\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Home Directory in Bash\nDESCRIPTION: Sets the AIRFLOW_HOME environment variable to specify the Airflow installation directory.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/start.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_HOME=~/airflow\n```\n\n----------------------------------------\n\nTITLE: Using DatabricksSqlSensor to poll for SQL query results\nDESCRIPTION: Example of using DatabricksSqlSensor to wait for a condition in a Databricks table. The sensor polls a specified SQL warehouse or cluster with the given SQL statement until it returns a non-empty result.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/sql.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsql_sensor = DatabricksSqlSensor(\n    task_id=\"databricks_sql_sensor\",\n    sql=\"SELECT 10 as Rows_Count\",\n    sql_warehouse_name=endpoint,\n    conn_id=DATABRICKS_SYSTEM_TEST_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Converting URI to JSON Connection Format\nDESCRIPTION: Example showing how to convert an Airflow connection from URI format to JSON format using the Connection class. Demonstrates handling of AWS credentials and configuration parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from airflow.models.connection import Connection\\n>>> c = Connection(\\n...     conn_id=\"awesome_conn\",\\n...     description=\"Example Connection\",\\n...     uri=\"aws://AKIAIOSFODNN7EXAMPLE:wJalrXUtnFEMI%2FK7MDENG%2FbPxRfiCYEXAMPLEKEY@/?__extra__=%7B%22region_name%22%3A+%22eu-central-1%22%2C+%22config_kwargs%22%3A+%7B%22retries%22%3A+%7B%22mode%22%3A+%22standard%22%2C+%22max_attempts%22%3A+10%7D%7D%7D\",\\n... )\\n>>> print(f\"AIRFLOW_CONN_{c.conn_id.upper()}='{c.as_json()}'\")\n```\n\n----------------------------------------\n\nTITLE: Changing Airflow Configuration via ENV (Dockerfile)\nDESCRIPTION: Dockerfile example showing how to modify Airflow configuration settings by setting environment variables prefixed with `AIRFLOW__<SECTION>__<KEY>` within the image.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_20\n\nLANGUAGE: Dockerfile\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/add-airflow-configuration/Dockerfile\n    :language: Dockerfile\n    :start-after: [START Dockerfile]\n    :end-before: [END Dockerfile]\n```\n\n----------------------------------------\n\nTITLE: Starting a Compute Engine Instance with ComputeEngineStartInstanceOperator in Python\nDESCRIPTION: Starts an existing Google Compute Engine instance using the ComputeEngineStartInstanceOperator. This snippet shows how to use the operator with and without specifying a project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineStartInstanceOperator(\n    zone=GCE_ZONE,\n    resource_id=INSTANCE_NAME,\n    project_id=GCP_PROJECT_ID,\n    task_id=\"gcp_compute_start_instance\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineStartInstanceOperator(\n    zone=GCE_ZONE,\n    resource_id=INSTANCE_NAME,\n    task_id=\"gcp_compute_start_instance\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Spark3-submit as Allowed Binary Value\nDESCRIPTION: A change in version 4.0.1 that adds 'spark3-submit' to the list of allowed values for the spark-binary parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"Add spark3-submit to list of allowed spark-binary values (#30068)\"\n```\n\n----------------------------------------\n\nTITLE: Listing Files in Azure DataLake Storage with ADLSListOperator in Python\nDESCRIPTION: This example demonstrates how to use the ADLSListOperator to list all files in Azure DataLake Storage. It requires the airflow.providers.microsoft.azure.operators.adls module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/adls.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_adls_list]\n# Code snippet for ADLSListOperator\n# [END howto_operator_adls_list]\n```\n\n----------------------------------------\n\nTITLE: Making Form-Encoded POST Requests with HttpOperator in Python\nDESCRIPTION: This snippet demonstrates sending a POST request with URL-encoded form data using `HttpOperator`. It targets the `/post` endpoint of `httpbin` via the `http_default` connection, sets the method to 'POST', provides form data as a string, and sets the `Content-Type` header to `application/x-www-form-urlencoded`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/operators.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntask_post_op_formenc = SimpleHttpOperator(\n    task_id=\"post_op_formenc\",\n    http_conn_id=\"http_default\",\n    method=\"POST\",\n    endpoint=\"post\",\n    data=\"foo=bar&baz=qux\",\n    headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Using the Speech-to-Text Recognition Operator\nDESCRIPTION: Example showing how to use the CloudSpeechToTextRecognizeSpeechOperator for speech recognition.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/speech_to_text.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_speech_to_text_recognize]\n[END howto_operator_speech_to_text_recognize]\n```\n\n----------------------------------------\n\nTITLE: Importing PowerBIHook and PowerBIDatasetOperator in Python\nDESCRIPTION: This snippet shows the import statements for PowerBIHook and PowerBIDatasetOperator from the Microsoft Azure providers package in Airflow. These classes are used for interacting with Power BI services.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/powerbi.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.microsoft.azure.hooks.powerbi import PowerBIHook\nfrom airflow.providers.microsoft.azure.operators.powerbi import PowerBIDatasetOperator\n```\n\n----------------------------------------\n\nTITLE: Simplified Setup and Teardown Configuration\nDESCRIPTION: Shows a more concise way to configure setup and teardown tasks using the setups parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster >> run_query >> delete_cluster.as_teardown(setups=create_cluster)\n```\n\n----------------------------------------\n\nTITLE: Complete Airflow DAG Definition\nDESCRIPTION: Full DAG implementation combining all components into a complete workflow using the TaskFlow API.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/objectstorage.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    'tutorial_objectstorage',\n    start_date=datetime(2023, 1, 1),\n    schedule='@daily',\n    catchup=False,\n) as dag:\n    # Create a reference to our storage location\n    data_path = ObjectStoragePath(\"s3://airflow-tutorial-data/\")\n\n    # Get data and save to cloud storage\n    saved_path = get_air_quality_data(data_path, logical_date=dag.logical_date)\n\n    # Analyze the data\n    results = analyze(saved_path)\n\n    # Log the results\n    @task\n    def log_results(df: pd.DataFrame):\n        print(df)\n\n    log_results(results)\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Secrets Manager for Specific Connections\nDESCRIPTION: This configuration demonstrates how to set up the AWS Secrets Manager backend to look up only specific connections using a regex pattern. It sets the connections_prefix and a connections_lookup_pattern for connections starting with 'm'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-ssm-parameter-store.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend\nbackend_kwargs = {\n  \"connections_prefix\": \"airflow/connections\",\n  \"connections_lookup_pattern\": \"^m\",\n  \"profile_name\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Copying GCE Instance Template in Python\nDESCRIPTION: Creates a ComputeEngineCopyInstanceTemplateOperator to copy an existing instance template with modifications to specific properties.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncopy_template = ComputeEngineCopyInstanceTemplateOperator(\n    task_id=\"copy_template\",\n    project_id=GCP_PROJECT_ID,\n    source_instance_template=TEMPLATE_NAME,\n    destination_instance_template=UPDATED_TEMPLATE_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Default MongoDB Connection via Environment Variable in Bash\nDESCRIPTION: This example demonstrates how to configure the default Airflow MongoDB connection (`mongo_default`) using an environment variable. It uses the `export` command in Bash to set `AIRFLOW_CONN_MONGO_DEFAULT` with a MongoDB connection URI, which includes credentials, host, port, and URL-encoded options like `authSource`. Airflow parses this URI to establish the connection. Note that components of the URI must be URL-encoded.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mongo/docs/connections/mongo.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_MONGO_DEFAULT='mongo://username:password@mongodb.example.com:27317/%3FauthSource%3Dadmin'\n```\n\n----------------------------------------\n\nTITLE: Setup Without Teardown\nDESCRIPTION: Shows how to use setup tasks without corresponding teardown tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster >> run_query >> other_task\n```\n\n----------------------------------------\n\nTITLE: Using DataflowJobMetricsSensor in Deferrable Mode\nDESCRIPTION: Example of using the DataflowJobMetricsSensor in deferrable mode to monitor Dataflow job metrics. This sensor waits for specific job metrics to meet certain conditions before proceeding.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Code snippet not provided in the input text\n```\n\n----------------------------------------\n\nTITLE: Template Fields for CloudVideoIntelligenceDetectVideoExplicitContentOperator (Python)\nDESCRIPTION: This snippet specifies the template fields ('input_uri', 'result_path') for CloudVideoIntelligenceDetectVideoExplicitContentOperator, supporting runtime parameter substitution in Airflow DAGs. This enables flexible and reusable pipelines, letting users define video URIs and output paths dynamically. Requires Airflow's templating system and Google Cloud modules.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/video_intelligence.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\"input_uri\", \"result_path\")\n```\n\n----------------------------------------\n\nTITLE: Enabling IAM OIDC Provider on EKS Cluster using eksctl\nDESCRIPTION: This bash command uses eksctl to enable the IAM OIDC Provider on an existing EKS cluster. It's a prerequisite for using IAM Roles for Service Accounts (IRSA) on Amazon EKS.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\neksctl utils associate-iam-oidc-provider --cluster=\"<EKS_CLUSTER_ID>\" --approve\n```\n\n----------------------------------------\n\nTITLE: Custom Kubernetes Pod Template Configuration\nDESCRIPTION: Example of a custom pod template configuration for KubernetesExecutor or CeleryKubernetesExecutor, demonstrating how to set priorityClassName for workers.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/customizing-workers.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npodTemplate: |\n  apiVersion: v1\n  kind: Pod\n  metadata:\n    name: placeholder-name\n    labels:\n      tier: airflow\n      component: worker\n      release: {{ .Release.Name }}\n  spec:\n    priorityClassName: high-priority\n    containers:\n      - name: base\n```\n\n----------------------------------------\n\nTITLE: Conditionally Skipping Tests Based on Airflow Version (Python)\nDESCRIPTION: This Python snippet demonstrates importing a feature-detection flag (e.g., AIRFLOW_V_2_10_PLUS) and using it with pytest's skipif marker to conditionally skip a test when running on Airflow versions prior to 2.10. This pattern ensures that tests depending on newer features are not run on unsupported versions, improving robustness and compatibility. The reason parameter documents the skip condition in test output.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom tests_common.test_utils.version_compat import AIRFLOW_V_2_10_PLUS\n\n\n@pytest.mark.skipif(not AIRFLOW_V_2_10_PLUS, reason=\"The tests should be skipped for Airflow < 2.10\")\ndef some_test_that_only_works_for_airflow_2_10_plus():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Putting File via FTPS using FTPSFileTransmitOperator in Python\nDESCRIPTION: This example illustrates using the Airflow `FTPSFileTransmitOperator` for securely uploading ('put') a local file to a remote FTPS server. It depends on an FTPS connection set up in Airflow (`ftp_conn_id`) and requires specifying `local_filepath` and `remote_filepath`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ftp/docs/operators/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../ftp/tests/system/ftp/example_ftp.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_ftps_put]\n    :end-before: [END howto_operator_ftps_put]\n```\n\n----------------------------------------\n\nTITLE: Deleting Bigtable Instance with BigtableDeleteInstanceOperator in Python\nDESCRIPTION: This snippet demonstrates the usage of BigtableDeleteInstanceOperator to delete a Google Cloud Bigtable instance. It shows how to create the operator with and without specifying the project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigtable.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndelete_instance_task = BigtableDeleteInstanceOperator(\n    project_id=GCP_PROJECT_ID,\n    instance_id=INSTANCE_ID,\n    task_id=\"delete_instance\",\n)\n\n# The same operator can be created without project_id:\ndelete_instance_task_no_project_id = BigtableDeleteInstanceOperator(\n    instance_id=INSTANCE_ID,\n    task_id=\"delete_instance\",\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Query with Neo4jOperator in Airflow\nDESCRIPTION: Example demonstrating how to use Neo4jOperator to execute Cypher queries in a Neo4j database. The operator connects using a configured neo4j_conn_id and executes the specified query.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/neo4j/docs/operators/neo4j.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntask_neo4j_query = Neo4jOperator(\n    task_id=\"run_neo4j_query\",\n    neo4j_conn_id=\"neo4j_conn\",\n    sql=\"MATCH (p:Person) RETURN p LIMIT 5\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataplex Lake - Python\nDESCRIPTION: Demonstrates the use of DataplexCreateLakeOperator in Airflow to create a Dataplex lake using a previously defined lake_config object. Requires Google credentials, project_id, region, and lake_id as input; outputs the created lake resource and tracks creation status.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"create_lake = DataplexCreateLakeOperator(\\n    task_id=\\\"create_lake\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    lake_id=LAKE_ID,\\n    body=lake_config,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Listing Dataplex Tasks - Python\nDESCRIPTION: Utilizes DataplexListTasksOperator to retrieve and list all Dataplex tasks within a specified lake. Inputs required are project_id, region, and lake_id. Output is a list of Dataplex task resources; this operator can be useful for auditing or managing multiple tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"list_tasks = DataplexListTasksOperator(\\n    task_id=\\\"list_tasks\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    lake_id=LAKE_ID,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow AWS Extras via Pip\nDESCRIPTION: This command uses pip, the Python package installer, to install the 'apache-airflow' package along with the optional '[amazon]' extras. These extras include necessary libraries (like boto3) for Airflow to interact with various AWS services. This step is required before using Airflow operators designed for AWS.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/_partials/prerequisite_tasks.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[amazon]'\n```\n\n----------------------------------------\n\nTITLE: Extending Airflow Image with Custom Providers (Dockerfile)\nDESCRIPTION: Dockerfile example demonstrating how to install specific versions of Airflow Providers into the base Airflow image. This is useful for upgrading individual providers independently of the core Airflow version.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_8\n\nLANGUAGE: Dockerfile\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/custom-providers/Dockerfile\n    :language: Dockerfile\n    :start-after: [START Dockerfile]\n    :end-before: [END Dockerfile]\n```\n\n----------------------------------------\n\nTITLE: Configuring a Dataplex Task - Python\nDESCRIPTION: Defines the configuration dictionary required to create a Dataplex task via the corresponding Airflow operator. No external dependencies aside from Python and Airflow's Dataplex provider are required. The configuration includes all required fields accepted by the Dataplex Task API and will be the body parameter of the operator method.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"task_config = {\\n    'name': 'sample-task',\\n    'displayName': 'Sample Dataplex Task',\\n    'type': 'SPARK',\\n    # Additional configuration fields...\\n}\"\n```\n\n----------------------------------------\n\nTITLE: Creating Job Trigger in Google Cloud DLP\nDESCRIPTION: Example of creating a job trigger using CloudDLPCreateJobTriggerOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/data_loss_prevention.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_dlp_create_job_trigger]\n[END howto_operator_dlp_create_job_trigger]\n```\n\n----------------------------------------\n\nTITLE: Copying GCE Instance Template Without Project ID in Python\nDESCRIPTION: Creates a ComputeEngineCopyInstanceTemplateOperator without specifying a project ID, which will be automatically retrieved from the Google Cloud connection used.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncopy_template_no_project_id = ComputeEngineCopyInstanceTemplateOperator(\n    task_id=\"copy_template_no_project_id\",\n    source_instance_template=TEMPLATE_NAME,\n    destination_instance_template=UPDATED_TEMPLATE_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Fetching Triggering Asset Information in Python\nDESCRIPTION: Demonstrates how to access information about the asset that triggered a DAG run using the triggering_asset_events template or parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/asset-scheduling.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nexample_snowflake_asset = Asset(\"snowflake://my_db/my_schema/my_table\")\n\nwith DAG(dag_id=\"load_snowflake_data\", schedule=\"@hourly\", ...):\n    SQLExecuteQueryOperator(\n        task_id=\"load\", conn_id=\"snowflake_default\", outlets=[example_snowflake_asset], ...\n    )\n\nwith DAG(dag_id=\"query_snowflake_data\", schedule=[example_snowflake_asset], ...):\n    SQLExecuteQueryOperator(\n        task_id=\"query\",\n        conn_id=\"snowflake_default\",\n        sql=\"\"\"\n          SELECT *\n          FROM my_db.my_schema.my_table\n          WHERE \"updated_at\" >= '{{ (triggering_asset_events.values() | first | first).source_dag_run.data_interval_start }}'\n          AND \"updated_at\" < '{{ (triggering_asset_events.values() | first | first).source_dag_run.data_interval_end }}';\n        \"\"\",\n    )\n\n    @task\n    def print_triggering_asset_events(triggering_asset_events=None):\n        for asset, asset_list in triggering_asset_events.items():\n            print(asset, asset_list)\n            print(asset_list[0].source_dag_run.dag_id)\n\n    print_triggering_asset_events()\n```\n\n----------------------------------------\n\nTITLE: Getting a Dataplex Data Quality Scan Job - Python\nDESCRIPTION: Illustrates use of DataplexGetDataQualityScanResultOperator to fetch the results or status of a completed Data Quality scan job. Input parameters include project_id, region, data quality scan ID, and job ID. Output provides the job result resource.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n\"get_dq_scan_job = DataplexGetDataQualityScanResultOperator(\\n    task_id=\\\"get_dq_scan_job\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    data_quality_scan_id=DATA_QUALITY_SCAN_ID,\\n    job_id=JOB_ID,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Stopping RDS Database\nDESCRIPTION: Stops an Amazon RDS database instance or cluster using RdsStopDbOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nstop_db = RdsStopDbOperator(\n    task_id=\"stop_db\",\n    db_identifier=DB_INSTANCE_NAME,\n    db_type=\"instance\",\n)\n```\n\n----------------------------------------\n\nTITLE: Skipping Tasks for Zero-Record AppFlow Runs Using AppflowRecordsShortCircuitOperator - Apache Airflow - Python\nDESCRIPTION: Shows how to configure AppflowRecordsShortCircuitOperator to short-circuit downstream Airflow tasks if an AppFlow run returns zero records. Useful for optimizing executions and avoiding unnecessary task runs. Relies on the Airflow Amazon provider and AppFlow setup. Receives the flow_name and task id as input; task will skip downstream branches if no records are present in the current AppFlow output.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/appflow.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_appflow_shortcircuit\",\n    schedule_interval=None,\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"appflow\"],\n) as dag:\n\n    short_circuit = AppflowRecordsShortCircuitOperator(\n        task_id=\"appflow_shortcircuit_task\",\n        flow_name=\"your_appflow_flow_name\",\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Configuring airflow.cfg for Azure Remote Logging (INI)\nDESCRIPTION: This INI configuration snippet demonstrates the settings required in `airflow.cfg` to enable remote logging to Azure Blob Storage. It involves setting `remote_logging` to `True`, specifying the base log folder path prefixed with `wasb` under `[logging]`, and defining the target Azure container name under `[azure_remote_logging]`. An existing Azure connection configured in Airflow is a prerequisite.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/logging/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\n# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.\n# Users must supply an Airflow connection id that provides access to the storage\n# location. If remote_logging is set to true, see UPDATING.md for additional\n# configuration requirements.\nremote_logging = True\nremote_base_log_folder = wasb-base-folder/path/to/logs\n\n[azure_remote_logging]\nremote_wasb_log_container = my-container\n```\n\n----------------------------------------\n\nTITLE: Executing SQL queries on Databricks warehouse with DatabricksSqlOperator\nDESCRIPTION: Example of using DatabricksSqlOperator to execute a SELECT query against a Databricks SQL warehouse. This snippet demonstrates the basic usage of the operator with a simple SQL statement.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/sql.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndatabricks_select = DatabricksSqlOperator(\n    task_id=\"databricks_sql_select\",\n    sql=\"SELECT 1 as One\",\n    sql_warehouse_name=\"my_warehouse\",\n)\n```\n\n----------------------------------------\n\nTITLE: CloudSQL Patch Operator Templated Fields (Python)\nDESCRIPTION: Lists the template fields for CloudSQLInstancePatchOperator in Airflow, enabling runtime dynamic configuration. Fields include project_id, instance name, and body. Helps Airflow users inject runtime values when patching Cloud SQL instances. Lists the template_fields tuple as defined in the operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n\"template_fields = (\\\"project_id\\\", \\\"instance\\\", \\\"body\\\")\"\n```\n\n----------------------------------------\n\nTITLE: Using SparkSqlOperator in Apache Airflow DAG\nDESCRIPTION: Example of using SparkSqlOperator to run SQL queries on Spark Hive metastore service. The SQL query can be provided directly or as a file path.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nspark_sql = SparkSqlOperator(\n    task_id=\"spark_sql_task\",\n    sql=\"SELECT * FROM bar\",\n    conn_id=\"spark_default\",\n    total_executor_cores=4,\n    executor_cores=2,\n    executor_memory=\"3g\",\n    keytab=\"privileged_user.keytab\",\n    principal=\"privileged_user/spark@FOO.COM\",\n    name=\"spark-sql\",\n    execution_timeout=timedelta(minutes=10),\n    conf={\"parquet.compression\": \"SNAPPY\"},\n    jars=\"test.jar\",\n    udf_jars=\"cloud-storage-udf.jar\",\n    pyfiles=\"sample_library.py\",\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Docker Compose for Apache Airflow\nDESCRIPTION: Bash commands to set up and run Apache Airflow using Docker Compose. This includes downloading the configuration file, creating necessary directories, and starting the Airflow services.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/pipeline.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Download the docker-compose.yaml file\ncurl -LfO 'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml'\n\n# Make expected directories and set an expected environment variable\nmkdir -p ./dags ./logs ./plugins\necho -e \"AIRFLOW_UID=$(id -u)\" > .env\n\n# Initialize the database\ndocker compose up airflow-init\n\n# Start up all services\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Uploading Single File to Google Drive using LocalFilesystemToGoogleDriveOperator\nDESCRIPTION: Example showing how to configure and use the LocalFilesystemToGoogleDriveOperator to upload a single file from local filesystem to Google Drive. The operator is part of the google.suite.transfers package in Airflow providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/local_to_drive.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nupload_local_to_drive = LocalFilesystemToGoogleDriveOperator(\n    task_id=\"upload_local_to_drive_single_file\",\n    local_paths=[LOCAL_FILE_PATH],\n    drive_folder=DRIVE_FOLDER,\n)\n```\n\n----------------------------------------\n\nTITLE: Waiting for EC2 Instance State with Airflow AWS Sensor - Python\nDESCRIPTION: This code snippet demonstrates use of EC2InstanceStateSensor in Airflow to wait for an EC2 instance to reach a specified state (such as 'running' or 'stopped'). It requires instance ID and target state parameters, and relies on Airflow's sensing mechanism along with the AWS provider and Boto3. The task polls EC2 API until the desired state is observed, emitting a success signal; suitable permissions to describe EC2 instances are necessary, and sensor polling interval should be configured wisely to avoid API throttling.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/ec2.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# There is no code in the provided input itself, only includes and code references to other files.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Running a Future-Records-Only AppFlow Flow Using AppflowRunAfterOperator - Apache Airflow - Python\nDESCRIPTION: Demonstrates how to invoke AppflowRunAfterOperator in an Airflow DAG to process only records after a specified point (e.g., future records relative to DAG execution). Useful for handling data partitioned by timestamps. Dependencies include the Airflow Amazon provider and established AppFlow connections. The input flow_name defines which AppFlow flow to trigger; output is record set matching the after date parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/appflow.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_appflow_run_after\",\n    schedule_interval=None,\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"appflow\"],\n) as dag:\n\n    run_flow_after = AppflowRunAfterOperator(\n        task_id=\"appflow_run_after_task\",\n        flow_name=\"your_appflow_flow_name\",\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Running a Pipeline with CloudBatchSubmitJobOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the CloudBatchSubmitJobOperator to submit a job to Google Cloud Batch. It includes the creation of a DAG and a task that submits a batch job with specified parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/life_sciences.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_batch_job_creation]\nwith models.DAG(\n    \"example_gcp_batch\",\n    schedule_interval='@once',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n) as dag:\n\n    # [START howto_operator_batch_job_creation]\n    submit_batch_job = CloudBatchSubmitJobOperator(\n        task_id=\"submit_batch_job\",\n        project_id=GCP_PROJECT_ID,\n        region=GCP_PROJECT_REGION,\n        job={\n            \"taskGroups\": [\n                {\n                    \"taskSpec\": {\n                        \"runnables\": [{\"container\": {\"imageUri\": \"gcr.io/google-containers/busybox\"}}],\n                        \"computeResource\": {\"cpuMilli\": 500, \"memoryMib\": 64},\n                    },\n                    \"taskCount\": 1,\n                }\n            ],\n            \"allocationPolicy\": {\n                \"instances\": [\n                    {\n                        \"policy\": {\n                            \"machineType\": \"e2-standard-4\",\n                            \"provisioningModel\": \"STANDARD\",\n                        }\n                    }\n                ]\n            },\n        },\n    )\n    # [END howto_operator_batch_job_creation]\n```\n\n----------------------------------------\n\nTITLE: Configuring EMR Virtual Cluster Job Parameters for Airflow Operator (Python)\nDESCRIPTION: This snippet illustrates how to construct structured job configuration dictionaries and parameters for Spark job submission on EMR-on-EKS using Airflow's EmrContainerOperator. It involves specifying virtual_cluster_id, release_label, execution_role_arn, Spark submit parameters, and optional application/monitoring configurations. Prerequisites include a correctly configured EMR EKS cluster and appropriate IAM roles. Output is used to define job run context in later operator usage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr_eks.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# Example config for EMR EKS job submission\nvirtual_cluster_id = 'xxxxxxxxxxxxxxxxxxx'\nexecution_role_arn = 'arn:aws:iam::123456789012:role/EMR_EKS_Job_Execution_Role'\njob_driver = {\n    'sparkSubmitJobDriver': {\n        'entryPoint': 'local:///usr/lib/spark/examples/src/main/python/pi.py',\n        'entryPointArguments': ['10'],\n        'sparkSubmitParameters': '--conf spark.executor.instances=2 --conf spark.executor.memory=2G'\n    }\n}\nconfiguration_overrides = {\n    'applicationConfiguration': [\n        {\n            'classification': 'spark-defaults',\n            'properties': {\n                'spark.hadoop.hive.metastore.client.factory.class': 'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory'\n            }\n        }\n    ],\n    'monitoringConfiguration': {\n        'persistentAppUI': 'ENABLED',\n        'cloudWatchMonitoringConfiguration': {\n            'logGroupName': '/aws/emr-eks-spark',\n            'logStreamNamePrefix': 'example'\n        }\n    }\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Exporting BigQuery Table to GCS using BigQueryToGCSOperator\nDESCRIPTION: Example showing how to configure and use BigQueryToGCSOperator to export data from a BigQuery table to Google Cloud Storage. The operator supports configuration options like compression, export format, and multiple destination URIs through Jinja templating.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/bigquery_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntable_extract = BigQueryToGCSOperator(\n    task_id=\"extract_table\",\n    source_project_dataset_table=\"{{ params.project_id }}.{{ params.dataset_id }}.{{ params.table_id }}\",\n    destination_cloud_storage_uris=[\"{{ params.gcs_path }}\"],\n    compression=\"NONE\",\n    export_format=\"CSV\",\n    field_delimiter=\",\",\n    print_header=True,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Consuming Messages from Kafka Topics using ConsumeFromTopicOperator\nDESCRIPTION: Example showing how to configure and use ConsumeFromTopicOperator to consume messages from Kafka topics. The operator creates a Kafka Consumer that reads messages in batches until reaching the end of log or maximum message limit.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nhowto_operator_consume_from_topic\n```\n\n----------------------------------------\n\nTITLE: Creating a BigQuery External Table from Google Cloud Storage using Airflow Operator in Python\nDESCRIPTION: Shows the usage of Airflow's BigQueryCreateExternalTableOperator to create an external table referencing files stored in Google Cloud Storage. After exporting data from Trino, this operator facilitates direct query access from BigQuery without loading data into BigQuery's native storage. Requires Airflow with the Google provider and proper GCP credentials. Key parameters specify the dataset, table ID, and the object path in GCS. Outputs a BigQuery external table linked to the provided data, enabling analytics across systems.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/trino_to_gcs.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncreate_bq_external_table = BigQueryCreateExternalTableOperator(\n    task_id=\"create_external_table\",\n    table_resource={\n        \"tableReference\": {\n            \"projectId\": \"my-gcp-project\",\n            \"datasetId\": \"my_dataset\",\n            \"tableId\": \"my_external_table\",\n        },\n        \"externalDataConfiguration\": {\n            \"sourceFormat\": \"NEWLINE_DELIMITED_JSON\",\n            \"sourceUris\": [\"gs://my-gcs-bucket/data_types_*.json\"],\n            \"schema\": {\"fields\": [...]},\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Sensing Empty Google Cloud Tasks Queue in Python\nDESCRIPTION: This code snippet demonstrates how to use the TaskQueueEmptySensor in Apache Airflow to detect when a Google Cloud Tasks queue is empty. It sets up a sensor task that checks the specified queue and project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/sensors/google-cloud-tasks.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nTaskQueueEmptySensor(\n    task_id=\"sensing_queue_empty\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    queue=QUEUE_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Nested Setup and Teardown with Task Groups\nDESCRIPTION: Complex example showing nested setup and teardown tasks with task groups.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwith TaskGroup(\"my_group\") as tg:\n    s1 = s1()\n    w1 = w1()\n    t1 = t1()\n    s1 >> w1 >> t1.as_teardown(setups=s1)\nw2 = w2()\ntg >> w2\ndag_s1 = dag_s1()\ndag_t1 = dag_t1()\ndag_s1 >> [tg, w2] >> dag_t1.as_teardown(setups=dag_s1)\n```\n\n----------------------------------------\n\nTITLE: Creating DV360 Query - Python Example\nDESCRIPTION: Example showing how to create a Display & Video 360 query using GoogleDisplayVideo360CreateQueryOperator. The result is saved to XCom for use by other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/display_video.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[START howto_google_display_video_create_query_operator]\n[END howto_google_display_video_create_query_operator]\n```\n\n----------------------------------------\n\nTITLE: Mapping Over Multiple Parameters in Airflow Tasks\nDESCRIPTION: Demonstrates mapping across multiple parameters to create a cross product of all parameter combinations. Each combination creates a separate task instance at runtime.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef add(x: int, y: int):\n    return x + y\n\n\nadded_values = add.expand(x=[2, 4, 8], y=[5, 10])\n# This results in the add function being called with\n# add(x=2, y=5)\n# add(x=2, y=10)\n# add(x=4, y=5)\n# add(x=4, y=10)\n# add(x=8, y=5)\n# add(x=8, y=10)\n```\n\n----------------------------------------\n\nTITLE: Creating Google Analytics Data Stream with GoogleAnalyticsAdminCreateDataStreamOperator in Python\nDESCRIPTION: This code demonstrates the use of GoogleAnalyticsAdminCreateDataStreamOperator to create a new data stream in Google Analytics. It shows the operator usage and notes that Jinja templating can be applied to the operator's template fields.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/analytics_admin.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nGoogleAnalyticsAdminCreateDataStreamOperator(\n    task_id=\"create_data_stream\",\n    property_id=\"{{ task_instance.xcom_pull('create_property')['name'].split('/')[-1] }}\",\n    data_stream={\n        \"display_name\": \"test data stream\",\n        \"web_stream_data\": {\"default_uri\": \"http://example.com\"}\n    },\n    type=\"WEB_DATA_STREAM\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Azure Service Bus Subscription in Python\nDESCRIPTION: This snippet illustrates how to use the AzureServiceBusSubscriptionCreateOperator to create an Azure Service Bus subscription with specific parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/asb.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_create_service_bus_subscription]\n# [END howto_operator_create_service_bus_subscription]\n```\n\n----------------------------------------\n\nTITLE: Deleting a Dataplex Zone with Airflow Operator\nDESCRIPTION: Uses the DataplexDeleteZoneOperator to delete a Dataplex zone. This operator removes a specified zone from a lake in Google Cloud Dataplex.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndelete_zone = DataplexDeleteZoneOperator(\n    task_id=\"delete_zone\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    lake_id=LAKE_ID,\n    zone_id=ZONE_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Verifying Azure Remote Logging Output (Log)\nDESCRIPTION: This example shows the expected log output in the Airflow UI when Azure Blob Storage logging is successfully configured. It indicates that remote logs were found and displays the URL to the log file stored in the specified Azure container and path, followed by standard task execution log entries. This confirms that Airflow is correctly writing to and potentially reading from Azure Blob Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/logging/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: none\nCODE:\n```\n*** Found remote logs:\n***   * https://my-container.blob.core.windows.net/wasb-base-folder/path/to/logs/dag_id=tutorial_dag/run_id=manual__2023-07-22T22:22:25.891267+00:00/task_id=load/attempt=1.log\n[2023-07-23, 03:52:47] {taskinstance.py:1144} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tutorial_dag.load manual__2023-07-22T22:22:25.891267+00:00 [queued]>\n[2023-07-23, 03:52:47] {taskinstance.py:1144} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tutorial_dag.load manual__2023-07-22T22:22:25.891267+00:00 [queued]>\n[2023-07-23, 03:52:47] {taskinstance.py:1346} INFO - Starting attempt 1 of 3\n```\n\n----------------------------------------\n\nTITLE: Basic Presto to GCS Transfer in Python\nDESCRIPTION: Basic example of using PrestoToGCSOperator to execute a Presto SQL query and save results to Google Cloud Storage in JSON Lines format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/presto_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntask_presto_to_gcs_basic = PrestoToGCSOperator(\n    task_id=\"presto_to_gcs_basic\",\n    sql=PRESTO_SQL_BASIC_TYPES,\n    bucket=BUCKET,\n    filename=f\"{DATASET_PREFIX}/basic_types/data*.json\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding DAG Documentation\nDESCRIPTION: Shows how to add documentation to DAGs and tasks using markdown and other formats.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/fundamentals.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndag.doc_md = __doc__\n\nt1.doc_md = \"\"\"\\\n#### Task Documentation\nYou can document your task using the attributes `doc_md` (markdown),\n`doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets\nrendered in the UI's Task Instance Details page.\n![img](http://montcs.bloomu.edu/~bobmon/Semesters/2012-01/491/import%20soul.png)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Running Java Pipeline with DirectRunner in Apache Beam\nDESCRIPTION: This example shows how to use BeamRunJavaPipelineOperator to execute a Java pipeline using DirectRunner. It specifies the JAR file, main class, and pipeline options including input and output locations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_java_direct_runner = BeamRunJavaPipelineOperator(\n    task_id=\"beam_task_direct_runner\",\n    jar=\"{{ var.json.beam_variables.jar }}\",\n    job_class=\"org.apache.beam.examples.WordCount\",\n    pipeline_options={\n        \"output\": \"/tmp/java/wordcount/output/\",\n        \"inputFile\": \"{{ var.json.beam_variables.input_file }}\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Filtering Responses with HttpOperator in Python\nDESCRIPTION: This example shows how to use the `response_filter` parameter in `HttpOperator` to process the response of a GET request to `/get`. It uses a lambda function to parse the JSON response and extract only the 'User-Agent' header value, which becomes the task's XCom output.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/operators.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Response filter allows to manipulate response and check for desired conditions\n# With response filter, response is not proxied to XCom (unless **kwargs[\"ti\"].xcom_push is called)\n# Here, response_filter returns the response payload only if the validation check passes\ntask_get_op_response_filter = SimpleHttpOperator(\n    task_id=\"get_op_response_filter\",\n    http_conn_id=\"http_default\",\n    method=\"GET\",\n    endpoint=\"/get\",\n    response_filter=lambda response: response.json()[\"headers\"][\"User-Agent\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Reading an Asset in a Task using Python in Airflow\nDESCRIPTION: This snippet demonstrates how to read an asset in a task using the @task decorator. It accesses the asset's events through the inlet_events parameter and retrieves the row count from the last event.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@task(inlets=[write_to_s3])\ndef post_process_s3_file(*, inlet_events):\n    events = inlet_events[example_s3_asset]\n    last_row_count = events[-1].extra[\"row_count\"]\n\n\npost_process_s3_file()\n```\n\n----------------------------------------\n\nTITLE: Updating BigQuery Table Schema with Airflow BigQueryUpdateTableSchemaOperator - Python\nDESCRIPTION: This snippet demonstrates how to use the BigQueryUpdateTableSchemaOperator in an Airflow DAG to update specified fields within a BigQuery table schema while leaving the rest unchanged. The operator requires the airflow.providers.google.cloud.operators.bigquery module and expects parameters such as project_id, dataset_id, table_id, and schema_fields. The input schema_fields defines the new or updated schema elements. The output is an updated table schema, and only the provided fields are overwritten, making it ideal for tasks like updating field descriptions without altering the full schema.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nbigquery_update_table_schema = BigQueryUpdateTableSchemaOperator(\n    task_id=\"bigquery_update_table_schema\",\n    project_id=GCP_PROJECT_ID,\n    dataset_id=DATASET_NAME,\n    table_id=TABLE_NAME,\n    schema_fields=[\n        {\"name\": \"field1\", \"type\": \"STRING\", \"mode\": \"NULLABLE\", \"description\": \"Updated description\"},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Processing Output with @task.bash in Airflow\nDESCRIPTION: Demonstrates how to use the output_processor parameter to process the output of a bash script before storing it as an XCom when using the @task.bash decorator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@task.bash(output_processor=lambda output: json.loads(output))\ndef bash_task() -> str:\n    return \"\"\"\n        jq -c '.[] | select(.lastModified > \"{{ data_interval_start | ts_zulu }}\" or .created > \"{{ data_interval_start | ts_zulu }}\")' \\\n        example.json\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Improper Usage of Airflow Variables in DAG Definition\nDESCRIPTION: This snippet illustrates incorrect ways of using Airflow Variables in DAG definitions, which can lead to performance issues during DAG parsing.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import Variable\n\nfoo_var = Variable.get(\"foo\")  # AVOID THAT\nbash_use_variable_bad_1 = BashOperator(\n    task_id=\"bash_use_variable_bad_1\", bash_command=\"echo variable foo=${foo_env}\", env={\"foo_env\": foo_var}\n)\n\nbash_use_variable_bad_2 = BashOperator(\n    task_id=\"bash_use_variable_bad_2\",\n    bash_command=f\"echo variable foo=${Variable.get('foo')}\",  # AVOID THAT\n)\n\nbash_use_variable_bad_3 = BashOperator(\n    task_id=\"bash_use_variable_bad_3\",\n    bash_command=\"echo variable foo=${foo_env}\",\n    env={\"foo_env\": Variable.get(\"foo\")},  # AVOID THAT\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting Data with Google Cloud VertexAI ExportDataOperator in Airflow (Python)\nDESCRIPTION: Illustrates the use of ExportDataOperator to export data from a VertexAI dataset to Google Cloud Storage. Requires a valid dataset, export config (destination), and proper GCP credentials. Parameters include dataset_id, export_configs, project, and location. The output is data files in the specified GCS location.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    export_data = ExportDataOperator(\n        task_id=\"export_data\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        dataset_id=DATASET_ID,\n        export_configs=[export_config],\n        gcp_conn_id=GCP_CONN_ID,\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Jira Notifications in Airflow DAG\nDESCRIPTION: Example showing how to configure Jira notifications for both DAG-level and task-level failure callbacks. The code demonstrates setting up a DAG with a BashOperator that creates Jira issues with customized fields like project ID, issue type, and labels when failures occur.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/atlassian/jira/docs/notifications/jira-notifier-howto-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.atlassian.jira.notifications.jira import send_jira_notification\n\nwith DAG(\n    \"test-dag\",\n    start_date=datetime(2023, 11, 3),\n    on_failure_callback=[\n        send_jira_notification(\n            jira_conn_id=\"my-jira-conn\",\n            description=\"Failure in the DAG {{ dag.dag_id }}\",\n            summary=\"Airflow DAG Issue\",\n            project_id=10000,\n            issue_type_id=10003,\n            labels=[\"airflow-dag-failure\"],\n        )\n    ],\n):\n    BashOperator(\n        task_id=\"mytask\",\n        on_failure_callback=[\n            send_jira_notification(\n                jira_conn_id=\"my-jira-conn\",\n                description=\"The task {{ ti.task_id }} failed\",\n                summary=\"Airflow Task Issue\",\n                project_id=10000,\n                issue_type_id=10003,\n                labels=[\"airflow-task-failure\"],\n            )\n        ],\n        bash_command=\"fail\",\n        retries=0,\n    )\n```\n\n----------------------------------------\n\nTITLE: Pausing a Transfer Operation with Airflow and Google Cloud Storage Transfer Service in Python\nDESCRIPTION: This code demonstrates how to use Airflow's CloudDataTransferServicePauseOperationOperator to pause an active transfer operation in Google Cloud Storage Transfer Service. It requires the operation name, Google Cloud project, and connection ID as key parameters. Dependencies include an Airflow environment configured with the necessary Google provider. The code supports template fields for dynamic DAG generation, and output is an acknowledgment of the paused operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npause_operation = CloudDataTransferServicePauseOperationOperator(\n    task_id=\"pause_operation\",\n    operation_name=OPERATION_NAME,\n    project_id=PROJECT_ID,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"operation_name\",\n    \"project_id\",\n    \"gcp_conn_id\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Python Training Job in Vertex AI\nDESCRIPTION: Example of creating a custom Python training job using the MLEngineCreateModelOperator. The code demonstrates how to set up a model container that can hold multiple versions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/mlengine.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_custom_training_job_op = CreateCustomPythonPackageTrainingJobOperator(\n    task_id=f\"custom_python_training_job_{project_id}\",\n    training_id=TEST_CUSTOM_TRAINING_JOB_ID,\n    project_id=project_id,\n    location=TEST_LOCATION,\n    display_name=TEST_CUSTOM_TRAINING_JOB_DISPLAY_NAME,\n    python_package_gcs_uri=TEST_PYTHON_PACKAGE_GCS_URI,\n    python_module_name=TEST_PYTHON_MODULE_NAME,\n    container_uri=TEST_CONTAINER_IMAGE_URI,\n    model_serving_container_image_uri=TEST_CONTAINER_IMAGE_URI,\n)\n```\n\n----------------------------------------\n\nTITLE: Stopping an EMR Notebook Execution using EmrStopNotebookExecutionOperator in Python\nDESCRIPTION: Shows how to stop a currently running EMR notebook execution using `EmrStopNotebookExecutionOperator`. It requires the `notebook_execution_id` of the execution to be stopped and an `aws_conn_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstop_notebook_execution = EmrStopNotebookExecutionOperator(\n    task_id=\"stop_notebook_execution\",\n    notebook_execution_id=notebook_execution_id,\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Creating Alerts with OpsgenieCreateAlertOperator in Airflow\nDESCRIPTION: Uses OpsgenieCreateAlertOperator to send an alert to Opsgenie with a specific message. This operator supports integration between Apache Airflow and Opsgenie's alerting system.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opsgenie/docs/operators/opsgenie_alert.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Creating PigOperator Task in Apache Airflow\nDESCRIPTION: This code snippet demonstrates how to create a PigOperator task in an Apache Airflow DAG. It shows the basic configuration of the operator, including specifying the Pig script file and any additional parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pig/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START create_pig]\n# [END create_pig]\n```\n\n----------------------------------------\n\nTITLE: Copying a Single File from GCS to Google Drive in Python\nDESCRIPTION: This snippet demonstrates how to copy a single file from Google Cloud Storage to Google Drive using the GCSToGoogleDriveOperator. It specifies the source bucket, object name, and destination file name in Google Drive.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gdrive.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntransfer_gcs_to_gdrive = GCSToGoogleDriveOperator(\n    task_id=\"gcs_to_drive_single_file\",\n    source_bucket=BUCKET_NAME,\n    source_object=GCS_FILE_PATH,\n    destination_object=DRIVE_FILE_PATH,\n)\n```\n\n----------------------------------------\n\nTITLE: Basic DynamoDB to S3 Transfer in Python using Airflow\nDESCRIPTION: This snippet demonstrates the basic usage of the DynamoDBToS3Operator to transfer data from a DynamoDB table to an S3 bucket. It specifies the source table, destination bucket, and file size limit.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/dynamodb_to_s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndynamodb_to_s3_task = DynamoDBToS3Operator(\n    task_id=\"dynamodb_to_s3\",\n    dynamodb_table_name=\"{{ redshift_cluster_identifier }}_table\",\n    s3_bucket_name=\"{{ s3_bucket }}\",\n    file_size=1000,\n    aws_conn_id=\"aws_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Executing MySQL Query using SQLExecuteQueryOperator\nDESCRIPTION: Example showing how to use SQLExecuteQueryOperator to execute MySQL queries. The operator connects to MySQL using a predefined connection ID and executes the specified SQL query.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\nmysql_task = SQLExecuteQueryOperator(\n    task_id='mysql_task',\n    conn_id='mysql_default',\n    sql='select * from mytable',\n    database='mydb'\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Bigtable Instance with BigtableUpdateInstanceOperator in Python\nDESCRIPTION: This snippet shows how to use the BigtableUpdateInstanceOperator to update an existing Google Cloud Bigtable instance. It demonstrates updating the instance display name, type, and labels, with variants for specifying the project ID or using the one from the Google Cloud connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigtable.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nupdate_instance_task = BigtableUpdateInstanceOperator(\n    project_id=GCP_PROJECT_ID,\n    instance_id=INSTANCE_ID,\n    instance_display_name=INSTANCE_ID,\n    instance_type=Instance.TYPE_PRODUCTION,\n    instance_labels={\"env\": \"prod\"},\n    task_id=\"update_instance\",\n)\n\n# The same operator can be created without project_id:\nupdate_instance_task_no_project_id = BigtableUpdateInstanceOperator(\n    instance_id=INSTANCE_ID,\n    instance_display_name=INSTANCE_ID,\n    instance_type=Instance.TYPE_PRODUCTION,\n    instance_labels={\"env\": \"prod\"},\n    task_id=\"update_instance\",\n)\n```\n\n----------------------------------------\n\nTITLE: Explicit Product Update\nDESCRIPTION: Updates a Product using explicitly specified product_id\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_vision_product_update_2]\\n[END howto_operator_vision_product_update_2]\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataproc Metastore Metadata using Airflow Operator in Python\nDESCRIPTION: Illustrates the use of the `DataprocMetastoreExportMetadataOperator` in an Airflow DAG. This operator exports metadata from a specified Dataproc Metastore service to a Google Cloud Storage folder. Requires `destination_gcs_folder`, `service_id`, `location`, and `project_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nexport_metadata = DataprocMetastoreExportMetadataOperator(\n    task_id=\"export_metadata\",\n    destination_gcs_folder=f\"gs://{BUCKET}/exported_location\",\n    service_id=SERVICE_ID,\n    location=REGION,\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Task Instance Success Listener in Python\nDESCRIPTION: Code example showing how to implement a listener for task instance success events.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/listeners.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@hookimpl\ndef on_task_instance_success(previous_state, task_instance):\n    \"\"\"Listen for task instance success events.\"\"\"\n    print(f\"Task instance {task_instance} switched from {previous_state} to success\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Setup and Teardown Tasks\nDESCRIPTION: Demonstrates how to mark tasks as setup and teardown using as_setup() and as_teardown() methods with proper dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster.as_setup() >> run_query >> delete_cluster.as_teardown()\ncreate_cluster >> delete_cluster\n```\n\n----------------------------------------\n\nTITLE: Save DV360 SDF to GCS - Python Example\nDESCRIPTION: Shows how to save SDF files to Google Cloud Storage using GoogleDisplayVideo360SDFtoGCSOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/display_video.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n[START howto_google_display_video_save_sdf_in_gcs_operator]\n[END howto_google_display_video_save_sdf_in_gcs_operator]\n```\n\n----------------------------------------\n\nTITLE: Templated Fields for CloudTranslateTextOperator in Python\nDESCRIPTION: Lists the fields within the `CloudTranslateTextOperator` that support Jinja templating in Airflow. This allows dynamic values for parameters like `values`, `target_language`, `model`, `gcp_conn_id`, `project_id`, etc., to be rendered at runtime.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields: Sequence[str] = (\n    \"values\",\n    \"target_language\",\n    \"format_\",\n    \"source_language\",\n    \"model\",\n    \"gcp_conn_id\",\n    \"project_id\",\n    \"impersonation_chain\",\n    \"delegate_to\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Build Arguments for Apache Airflow Docker Image\nDESCRIPTION: This snippet defines the basic build arguments that can be used to customize the Apache Airflow Docker image. It includes options for specifying the Python base image, Airflow version, extras, home directory, and user settings.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build-arg-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| Build argument                           | Default value                            | Description                                 |\n+==========================================+==========================================+=============================================+\n| ``PYTHON_BASE_IMAGE``                    | ``python:3.9-slim-bookworm``             | Base python image.                          |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``AIRFLOW_VERSION``                      | :subst-code:`|airflow-version|`          | version of Airflow.                         |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``AIRFLOW_EXTRAS``                       | (see below the table)                    | Default extras with which Airflow is        |\n|                                          |                                          | installed.                                  |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``ADDITIONAL_AIRFLOW_EXTRAS``            |                                          | Optional additional extras with which       |\n|                                          |                                          | Airflow is installed.                       |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``AIRFLOW_HOME``                         | ``/opt/airflow``                         | Airflow's HOME (that's where logs and       |\n|                                          |                                          | SQLite databases are stored).               |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``AIRFLOW_USER_HOME_DIR``                | ``/home/airflow``                        | Home directory of the Airflow user.         |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``AIRFLOW_PIP_VERSION``                  | ``<LATEST_AVAILABLE_IN_PYPI>``           |  PIP version used.                          |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``AIRFLOW_UV_VERSION``                   | ``<LATEST_AVAILABLE_IN_PYPI>``           |  UV version used.                           |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``AIRFLOW_USE_UV``                       | ``false``                                |  Whether to use UV to build the image.      |\n|                                          |                                          |  This is an experimental feature.           |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``UV_HTTP_TIMEOUT``                      | ``300``                                  |  Timeout in seconds for UV pull requests.   |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``ADDITIONAL_PIP_INSTALL_FLAGS``         |                                          | additional ``pip`` flags passed to the      |\n|                                          |                                          | installation commands (except when          |\n|                                          |                                          | reinstalling ``pip`` itself)                |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``PIP_PROGRESS_BAR``                     | ``on``                                   | Progress bar for PIP installation           |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``AIRFLOW_UID``                          | ``50000``                                | Airflow user UID.                           |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``AIRFLOW_CONSTRAINTS``                  | ``constraints``                          | Type of constraints to build the image.     |\n|                                          |                                          | This can be ``constraints`` for regular     |\n|                                          |                                          | images or ``constraints-no-providers`` for  |\n|                                          |                                          | slim images.                                |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n| ``AIRFLOW_CONSTRAINTS_REFERENCE``        |                                          | Reference (branch or tag) from GitHub       |\n|                                          |                                          | where constraints file is taken from        |\n|                                          |                                          | It can be ``constraints-main`` or           |\n|                                          |                                          | ``constraints-2-0`` for                     |\n|                                          |                                          | 2.0.* installation. In case of building     |\n|                                          |                                          | specific version you want to point it       |\n|                                          |                                          | to specific tag, for example                |\n|                                          |                                          | :subst-code:`constraints-|airflow-version|`.|\n|                                          |                                          | Auto-detected if empty.                     |\n+------------------------------------------+------------------------------------------+---------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Deleting a Custom Resource on GKE using GKEDeleteCustomResourceOperator in Python\nDESCRIPTION: This snippet shows how to use the `GKEDeleteCustomResourceOperator` to delete a specified custom resource from a GKE cluster. It requires details like the resource name, namespace, and group/version/plural to identify the resource for deletion.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/kubernetes_engine/example_kubernetes_engine_resource.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_gke_delete_resource]\n    :end-before: [END howto_operator_gke_delete_resource]\n```\n\n----------------------------------------\n\nTITLE: Triggering Power BI Dataset Refresh with PowerBIDatasetRefreshOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the PowerBIDatasetRefreshOperator to trigger a refresh for a specified dataset in a Power BI workspace. It includes setting up the operator with necessary parameters such as dataset ID, workspace ID, and Azure credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/powerbi.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nPowerBIDatasetRefreshOperator(\n    task_id=\"refresh_dataset\",\n    dataset_id=\"{{ DATASET_ID }}\",\n    workspace_id=\"{{ WORKSPACE_ID }}\",\n    azure_ad_token=\"{{ AZURE_AD_TOKEN }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Email Backend and Templates in Airflow (INI)\nDESCRIPTION: This snippet shows how to set the email backend, subject template, and HTML content template in the Airflow configuration file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[email]\nemail_backend = airflow.utils.email.send_email_smtp\nsubject_template = /path/to/my_subject_template_file\nhtml_content_template = /path/to/my_html_content_template_file\n```\n\n----------------------------------------\n\nTITLE: Importing Custom DAG in Python for Apache Airflow\nDESCRIPTION: Demonstrates how to import a custom DAG class using a full Python package path. This approach is recommended to avoid issues with relative imports in different Airflow contexts.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/modules_management.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom my_company.my_custom_dags.base_dag import BaseDag  # This is cool\n```\n\n----------------------------------------\n\nTITLE: Getting Apache Kafka Topic Info with ManagedKafkaGetTopicOperator in Python\nDESCRIPTION: This code shows how to retrieve information about an Apache Kafka topic using the ManagedKafkaGetTopicOperator. It requires the project ID, region, cluster, and topic name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nget_topic = ManagedKafkaGetTopicOperator(\n    task_id=\"get_topic\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n    topic=TOPIC,\n)\n```\n\n----------------------------------------\n\nTITLE: Verifying Papermill Notebook Output in Python\nDESCRIPTION: This snippet shows how to verify the output of a Papermill-executed notebook using a PythonOperator in an Airflow DAG. It checks for specific content in the notebook output.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef check_notebook_output(ti, **kwargs):\n    exec_date = kwargs.get('execution_date')\n    notebook_path = f\"/tmp/out-{exec_date}.ipynb\"\n    with open(notebook_path, 'r') as nb_file:\n        notebook_content = nb_file.read()\n    if 'Hello Airflow!' not in notebook_content:\n        raise ValueError(\"Unexpected output in notebook\")\n\nverify_notebook = PythonOperator(\n    task_id='verify_notebook',\n    python_callable=check_notebook_output,\n    provide_context=True,\n    dag=dag,\n)\n\nexecute_notebook >> verify_notebook\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow for Google Cloud Storage Logging\nDESCRIPTION: This snippet shows the necessary configuration in airflow.cfg to enable remote logging to Google Cloud Storage. It specifies the remote logging settings, including the GCS bucket path and connection ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/logging/gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\n# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.\n# Users must supply an Airflow connection id that provides access to the storage\n# location. If remote_logging is set to true, see UPDATING.md for additional\n# configuration requirements.\nremote_logging = True\nremote_base_log_folder = gs://my-bucket/path/to/logs\nremote_log_conn_id = my_gcs_conn\n```\n\n----------------------------------------\n\nTITLE: Setting Databricks Connection Using Environment Variable\nDESCRIPTION: Example showing how to configure a Databricks connection in Airflow using an environment variable with URI syntax. The connection string uses a URL-encoded format with the host URL and access token.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/connections/databricks.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_DATABRICKS_DEFAULT='databricks://@host-url?token=yourtoken'\n```\n\n----------------------------------------\n\nTITLE: BigQuery Dataset Creation with Service Account Impersonation\nDESCRIPTION: Example Python code demonstrating how to create a BigQuery dataset while impersonating a service account using the impersonation_chain parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom airflow.providers.google.cloud.operators.bigquery import (\n    BigQueryCreateEmptyDatasetOperator,\n)\n\nIMPERSONATION_CHAIN = \"impersonated_account@your_project_id.iam.gserviceaccount.com\"\n\ncreate_dataset = BigQueryCreateEmptyDatasetOperator(\n    task_id=\"create-dataset\",\n    gcp_conn_id=\"google_cloud_default\",\n    dataset_id=\"test_dataset\",\n    location=\"southamerica-east1\",\n    impersonation_chain=IMPERSONATION_CHAIN,\n)\n```\n\n----------------------------------------\n\nTITLE: Ingesting Weaviate Data without Vectors from XCom using Python\nDESCRIPTION: This example demonstrates using the `WeaviateIngestOperator` to ingest data into a Weaviate collection (`ClassName`) where the operator will generate the embedding vectors. The input data (without vectors) is retrieved from an Airflow XCom pushed by a previous task (`get_data_without_vectors.output`). The `vector_col` parameter specifies the column containing the text to be embedded. It uses the `weaviate_default` connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/weaviate/docs/operators/weaviate.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ningest_data_without_vectors_from_xcom = WeaviateIngestOperator(\n    task_id=\"ingest_data_without_vectors_from_xcom\",\n    conn_id=\"weaviate_default\",\n    class_name=\"ClassName\",\n    input_data=get_data_without_vectors.output,\n    vector_col=\"content\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Connection with Encoded Special Characters in Python\nDESCRIPTION: This snippet demonstrates the correct way to create a Connection object with a password containing a '/' character. It uses urllib.parse.quote_plus to encode the special character, allowing proper parsing of the URI.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_13\n\nLANGUAGE: pycon\nCODE:\n```\n>>> c = Connection(uri=\"my-conn-type://my-login:my-pa%2Fssword@my-host:5432/my-schema?param1=val1&param2=val2\")\n>>> print(c.password)\nmy-pa/ssword\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Mask in Operator\nDESCRIPTION: Implementation of custom masking within an Airflow operator's execute method, demonstrating how to protect sensitive values in operator execution logs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/mask-sensitive-values.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyOperator(BaseOperator):\n    def execute(self, context):\n        from airflow.sdk.execution_time.secrets_masker import mask_secret\n\n        mask_secret(\"custom_value\")\n\n        ...\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Make pyodbc.Row and databricks.Row JSON-serializable\nDESCRIPTION: This commit message (hash 064fc2b775, dated 2023-11-17) describes a change that makes `pyodbc.Row` and `databricks.Row` objects JSON-serializable by adding a new `make_serializable` method (issue #32319). This enhances interoperability, particularly for XComs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_25\n\nLANGUAGE: text\nCODE:\n```\n``Make pyodbc.Row and databricks.Row JSON-serializable via new 'make_serializable' method (#32319)``\n```\n\n----------------------------------------\n\nTITLE: Setting Default IMAP Connection via Environment Variable (SSL)\nDESCRIPTION: Sets the default Apache Airflow IMAP connection (ID `imap_default`) using a bash environment variable. The connection string follows URI syntax, specifying username, password, host (myimap.com), the standard SSL port (993), and explicitly enabling SSL via the `use_ssl=true` query parameter. Ensure username and password are URL-encoded if they contain special characters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/imap/docs/connections/imap.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_IMAP_DEFAULT='imap://username:password@myimap.com:993?use_ssl=true'\n```\n\n----------------------------------------\n\nTITLE: Monitoring SageMaker Tuning Job State in Python\nDESCRIPTION: This code snippet demonstrates the use of SageMakerTuningSensor to check the state of an Amazon SageMaker hyperparameter tuning job until it reaches a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_sensor_sagemaker_tuning]\n# Code snippet not provided in the original text\n# [END howto_sensor_sagemaker_tuning]\n```\n\n----------------------------------------\n\nTITLE: Undeploying a Model using Vertex AI Endpoint Service Operator - Python\nDESCRIPTION: Demonstrates how to undeploy a model from a Vertex AI endpoint using Airflow's UndeployModelOperator. Requires the airflow.providers.google cloud package and configuration for a valid GCP project and endpoint/model IDs. Key parameters include GCP connection information and model identification; upon execution, the operator removes the model deployment from the specified endpoint.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nwith models.DAG(\n    ...\n) as dag:\n    undeploy_model_task = UndeployModelOperator(\n        task_id=\"undeploy_model_vertex_ai\",\n        project_id=GCP_PROJECT_ID,\n        endpoint_id=ENDPOINT_ID,\n        deployed_model_id=DEPLOYED_MODEL_ID,\n        location=REGION,\n    )\n```\n\n----------------------------------------\n\nTITLE: Copying RDS Database Snapshot\nDESCRIPTION: Copies a snapshot of an Amazon RDS database using RDSCopyDBSnapshotOperator. The source snapshot must be in 'available' state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncopy_snapshot = RDSCopyDBSnapshotOperator(\n    task_id=\"copy_snapshot\",\n    db_type=\"instance\",\n    source_db_snapshot_identifier=DB_SNAPSHOT_NAME,\n    target_db_snapshot_identifier=DB_SNAPSHOT_NAME_COPY,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch External Log Link via INI\nDESCRIPTION: This snippet configures a link in the Airflow UI that points to an external log viewing system like Kibana. The `frontend` setting under the `[elasticsearch]` section specifies the URL template. The `{log_id}` placeholder is mandatory and will be replaced by Airflow using the `log_id_template` configured for writing logs, creating a direct link to the relevant logs in the external system.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/logging/index.rst#2025-04-22_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n[elasticsearch]\n# Qualified URL for an elasticsearch frontend (like Kibana) with a template argument for log_id\n# Code will construct log_id using the log_id template from the argument above.\n# NOTE: scheme will default to https if one is not provided\nfrontend = <host_port>/{log_id}\n```\n\n----------------------------------------\n\nTITLE: Extracting Image Annotation Results from XCOM in Airflow\nDESCRIPTION: Shows how to extract and process Google Cloud Vision image annotation results from XCOM in a subsequent task.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef check_response(response):\n    print(response)\n```\n\n----------------------------------------\n\nTITLE: Generating REST API Coverage Report for Airflow\nDESCRIPTION: Shows how to create a code coverage report for Airflow's REST API using a Python script within the Breeze environment.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_49\n\nLANGUAGE: python\nCODE:\n```\npython scripts/cov/restapi_coverage.py\n```\n\n----------------------------------------\n\nTITLE: Inserting GCE Instance Group Manager Template in Python\nDESCRIPTION: Creates a ComputeEngineInsertInstanceTemplateOperator to insert a new instance template with specified properties and machine configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninsert_template = ComputeEngineInsertInstanceTemplateOperator(\n    task_id=\"insert_template\",\n    project_id=GCP_PROJECT_ID,\n    body={\n        \"name\": TEMPLATE_NAME,\n        \"properties\": {\n            \"machineType\": MACHINE_TYPE,\n            \"disks\": [{\n                \"boot\": True,\n                \"type\": \"PERSISTENT\",\n                \"autoDelete\": True,\n                \"licenses\": [\"projects/debian-cloud/global/licenses/debian-11-bullseye\"],\n                \"initializeParams\": {\n                    \"sourceImage\": DEBIAN_IMAGE,\n                    \"diskSizeGb\": 10,\n                },\n            }],\n            \"networkInterfaces\": [{\"network\": \"global/networks/default\"}],\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Check status before DatabricksSubmitRunOperator execution\nDESCRIPTION: This commit message (hash 347373986c, dated 2024-01-23) describes an update to the DatabricksSubmitRunOperator, adding a status check before execution, including when running in deferrable mode (issue #36862).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n``check status before DatabricksSubmitRunOperator & DatabricksSubmitRunOperator executes in deferrable mode (#36862)``\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from S3 to DynamoDB Using S3ToDynamoDBOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the S3ToDynamoDBOperator to transfer data from an Amazon S3 bucket to a new Amazon DynamoDB table. It specifies the S3 bucket, object key, DynamoDB table name, and region.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/s3_to_dynamodb.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nS3ToDynamoDBOperator(\n    task_id=\"s3_to_dynamodb\",\n    s3_bucket=\"{{ BUCKET_NAME }}\",\n    s3_key=\"{{ S3_KEY }}\",\n    dynamodb_table_name=\"{{ DYNAMODB_TABLE_NAME }}\",\n    aws_conn_id=\"aws_default\",\n    region=\"{{ AWS_DEFAULT_REGION }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing KeyCloak OAuth Configuration in Airflow (Python)\nDESCRIPTION: Shows the initial setup for configuring KeyCloak OAuth authentication in `webserver_config.py`. This snippet includes necessary imports (`os`, `jwt`, `requests`, `logging`, `base64`, `cryptography`, Flask-AppBuilder, Airflow config, and security manager) and sets fundamental OAuth configurations: `AUTH_TYPE` to `AUTH_OAUTH`, enabling automatic user registration (`AUTH_USER_REGISTRATION = True`), and ensuring roles are synchronized upon each login (`AUTH_ROLES_SYNC_AT_LOGIN = True`). Further configuration specific to the KeyCloak provider (like client ID, secret, endpoints) would typically follow this initial setup.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/webserver-authentication.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport jwt\nimport requests\nimport logging\nfrom base64 import b64decode\nfrom cryptography.hazmat.primitives import serialization\nfrom flask_appbuilder.security.manager import AUTH_DB, AUTH_OAUTH\nfrom airflow import configuration as conf\nfrom airflow.www.security import AirflowSecurityManager\n\nlog = logging.getLogger(__name__)\n\nAUTH_TYPE = AUTH_OAUTH\nAUTH_USER_REGISTRATION = True\nAUTH_ROLES_SYNC_AT_LOGIN = True\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow with Extras and Constraints via Pip (Bash)\nDESCRIPTION: Illustrates the underlying pip command that Breeze might execute when building a PROD image from a specific PyPI version (e.g., 2.0.0 for Python 3.9). It installs 'apache-airflow' with specified extras (async, amazon, etc.) and applies version constraints from a specified file hosted on GitHub.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install \\\n  apache-airflow[async,amazon,celery,cncf.kubernetes,docker,elasticsearch,ftp,grpc,hashicorp,http,ldap,google,microsoft.azure,mysql,postgres,redis,sendgrid,sftp,slack,ssh,statsd,virtualenv]==2.0.0 \\\n  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.0.0/constraints-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Ingesting Data into Bedrock Data Source via Operator (Python)\nDESCRIPTION: Uses BedrockIngestDataOperator in Airflow to ingest files from an S3 bucket into an existing Bedrock Data Source. The operator submits ingestion configurations, facilitating vectorization of documents and future search capabilities. Appropriate role permissions and S3 access are required.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ningest_data_task = BedrockIngestDataOperator(\n    task_id=\"ingest_to_data_source\",\n    knowledge_base_id=\"kb-abcdef123456\",\n    data_source_id=\"ds-9876543210\",\n    s3_prefix=\"ingest-data/\",\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Enabling Selective OpenLineage Tracking Policy via Environment Variable\nDESCRIPTION: Sets the `AIRFLOW__OPENLINEAGE__SELECTIVE_ENABLE` environment variable to `true` to activate the selective lineage tracking policy. This allows enabling or disabling lineage for specific DAGs and tasks programmatically when the policy is active.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_26\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__SELECTIVE_ENABLE=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow Backend Connection with Bash Command in INI\nDESCRIPTION: This INI configuration snippet shows how to set the SQL Alchemy connection string using a bash command. This allows for dynamic configuration based on system commands.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/production-deployment.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\nsql_alchemy_conn_cmd = bash_command_to_run\n```\n\n----------------------------------------\n\nTITLE: Installing OpenLineage Provider Package for Apache Airflow\nDESCRIPTION: Command to install the OpenLineage provider package for Apache Airflow using pip, which is required before configuring the integration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\npip install apache-airflow-providers-openlineage\n```\n\n----------------------------------------\n\nTITLE: Using SQLExecuteQueryOperator to connect to Oracle in Apache Airflow\nDESCRIPTION: This code snippet demonstrates how to use the SQLExecuteQueryOperator to execute queries on an Oracle database. It establishes a connection using the oracle_conn_id and executes a specified SQL query that selects data from a table.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_oracle]\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\noracle_task = SQLExecuteQueryOperator(\n    task_id=\"oracle_task\",\n    conn_id=\"oracle_conn_id\",\n    sql=\"SELECT * FROM TEST_TABLE\",\n)\n# [END howto_operator_oracle]\n```\n\n----------------------------------------\n\nTITLE: Deleting Amazon Bedrock Data Source with TaskFlow API (Python)\nDESCRIPTION: Removes a specific Bedrock Data Source using Airflow TaskFlow, leveraging a boto3 call for deletion. This workflow pattern integrates simple lifecycle operations directly in Python logic within Airflow DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@task\n    def delete_data_source(knowledge_base_id: str, data_source_id: str):\n        import boto3\n        client = boto3.client(\"bedrock\")\n        client.delete_data_source(\n            knowledgeBaseId=knowledge_base_id,\n            dataSourceId=data_source_id\n        )\n\n```\n\n----------------------------------------\n\nTITLE: Provisioning Dedicated Throughput for Bedrock Model (Python)\nDESCRIPTION: Provisions dedicated model throughput using BedrockCreateProvisionedModelThroughputOperator. Configures capacity and associates it with the specified base or fine-tuned model. This operator supports asynchronous monitoring through Airflow sensors and triggers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprovision_throughput_task = BedrockCreateProvisionedModelThroughputOperator(\n    task_id=\"provision_model_throughput\",\n    model_id=\"amazon.titan-text-lite-v1\",\n    provisioned_model_name=\"titan-provisioned\",\n    model_units=1,\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Monitoring Data Quality Scan Job State via Sensor - Python\nDESCRIPTION: Uses DataplexDataQualityJobStatusSensor to poll and wait for a Data Quality scan job to reach a final state. Dependencies include a valid scan ID and Airflow sensor mechanism. Sensor completes when the job is finished, otherwise continues polling.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n\"dq_job_state_sensor = DataplexDataQualityJobStatusSensor(\\n    task_id=\\\"dq_job_state_sensor\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    data_quality_scan_id=DATA_QUALITY_SCAN_ID,\\n    job_id=JOB_ID,\\n    target_states=[\\\"SUCCEEDED\\\"],\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Including SqlToSlackWebhookOperator Example in Python\nDESCRIPTION: This reStructuredText directive includes an external Python code snippet demonstrating the usage of the SqlToSlackWebhookOperator. The referenced code, located in '/../../slack/tests/system/slack/example_sql_to_slack_webhook.py', shows how to instantiate the operator within an Airflow DAG. It specifies the SQL connection, the query to execute, the Slack webhook connection, and the message format for posting results to Slack, using specific markers ([START/END howto_operator_sql_to_slack_webhook]) to delimit the relevant section and applying dedentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/operators/sql_to_slack_webhook.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../slack/tests/system/slack/example_sql_to_slack_webhook.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_sql_to_slack_webhook]\n    :end-before: [END howto_operator_sql_to_slack_webhook]\n```\n\n----------------------------------------\n\nTITLE: Implementing BashSensor in Apache Airflow DAG\nDESCRIPTION: This code snippet demonstrates how to use the BashSensor in an Apache Airflow DAG. The BashSensor is used to execute arbitrary bash commands for sensing, where the command should return 0 on success and any other value on failure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/bash.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nbash_sensor_task = BashSensor(\n    task_id=\"bash_sensor_task\",\n    bash_command=\"if [ $(( ( RANDOM % 10 ) + 1 )) -ge 6 ]; then exit 0; else exit 1; fi\",\n    poke_interval=10,\n    timeout=60 * 10,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Cloud Composer Environment in Deferrable Mode\nDESCRIPTION: This snippet shows how to delete a Cloud Composer environment using the CloudComposerDeleteEnvironmentOperator in deferrable mode. It includes additional parameters for deferrable execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndelete_environment_task_deferrable = CloudComposerDeleteEnvironmentOperator(\n    task_id=\"delete-environment-deferrable-mode\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    environment_name=ENVIRONMENT_NAME,\n    deferrable=True,\n    poll_interval=10,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Cloud SQL Database in Python\nDESCRIPTION: Example of using CloudSQLDeleteInstanceDatabaseOperator to delete a database from a Cloud SQL instance. Shows usage with and without project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsql_delete_database = CloudSQLDeleteInstanceDatabaseOperator(\n    instance=INSTANCE_NAME,\n    database=DB_NAME,\n    task_id=\"sql_delete_database\",\n)\n\nsql_delete_database_with_project = CloudSQLDeleteInstanceDatabaseOperator(\n    project_id=GCP_PROJECT_ID,\n    instance=INSTANCE_NAME,\n    database=DB_NAME,\n    task_id=\"sql_delete_database_with_project\",\n)\n```\n\n----------------------------------------\n\nTITLE: Storing Datadog Connection in Secret Management System\nDESCRIPTION: Example JSON structure for storing Datadog connection details in a secret management system. The secret should be named with the default connection ID 'airflow/connections/datadog_default' and include connection type, description, API configuration details, and host information.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/datadog/docs/connections/datadog.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"conn_type\": \"datadog\",\n  \"description\": \"Datadog connection for my app\",\n  \"extra\": {\n    \"api_host\": \"https://api.datadoghq.com\",\n    \"api_key\": \"my api key\",\n    \"app_key\": \"my app key\",\n    \"source_type_name\": \"apache\"\n  },\n  \"host\": \"environment-region-application.domain.com\"\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting AlloyDB Backup with Airflow Operator\nDESCRIPTION: Uses AlloyDBDeleteBackupOperator to delete a backup of an AlloyDB instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/alloy_db.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndelete_backup = AlloyDBDeleteBackupOperator(\n    task_id=\"delete_backup\",\n    project_id=GCP_PROJECT_ID,\n    cluster_id=CLUSTER_ID,\n    backup_id=BACKUP_ID,\n    region=GCP_ALLOYDB_REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Filesystem Secrets Backend in Airflow\nDESCRIPTION: This snippet shows how to configure the Local Filesystem Secrets Backend in the airflow.cfg file. It specifies the backend class and file paths for variables and connections.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/secrets-backend/local-filesystem-secrets-backend.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.secrets.local_filesystem.LocalFilesystemBackend\nbackend_kwargs = {\"variables_file_path\": \"/files/var.json\", \"connections_file_path\": \"/files/conn.json\"}\n```\n\n----------------------------------------\n\nTITLE: Migrating Airflow Auth Manager Import Paths in Python\nDESCRIPTION: Migration paths for deprecated authentication manager imports in Airflow. The old paths from airflow.auth.managers.fab need to be updated to use the new airflow.providers.fab.auth_manager path structure.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41708.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old imports (deprecated)\nfrom airflow.auth.managers.fab.fab_auth_manager import *\nfrom airflow.auth.managers.fab.security_manager.override import *\n\n# New imports\nfrom airflow.providers.fab.auth_manager.security_manager.override import *\n```\n\n----------------------------------------\n\nTITLE: Moving Specific Files from GCS to Samba Using Wildcard (Python)\nDESCRIPTION: This Python snippet showcases moving specific files, selected by a wildcard pattern, from GCS to a specified directory on a Samba server using GCSToSambaOperator. It sets 'move_object=True' so each matching file is removed from the GCS source bucket after successful transfer. The 'destination_path' defines a path prefix on Samba, and the operator iterates all matching files. Inputs are a GCS bucket and a wildcard path; output is files moved to Samba under the given path, with deletion from GCS.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/transfer/gcs_to_samba.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nGCSToSambaOperator(\n    task_id=\"move_gcs_to_samba_specific_files\",\n    source_bucket=\"example-source-bucket\",\n    source_path=\"folder/subfolder/*.csv\",\n    samba_server=\"example.samba.server\",\n    share_name=\"SHARE\",\n    destination_path=\"destination_on_samba/\",\n    samba_username=\"airflow\",\n    samba_password=\"airflow\",\n    move_object=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS SAML Metadata in Airflow via INI File - ini\nDESCRIPTION: This snippet shows how to configure the base URL for Airflow's API and set the AWS IAM Identity Center SAML metadata URL within the Airflow ini configuration file. Both keys must be set correctly for the Airflow AWS auth manager to interact with the configured Identity Center instance. '<base_url>' should be replaced with your Airflow instance's public address, and '<saml_metadata_file_url>' should be the URL to the AWS SAML metadata file obtained during application setup.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/setup/identity-center.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[api]\nbase_url = <base_url>\n\n[aws_auth_manager]\nsaml_metadata_url = <saml_metadata_file_url>\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS SSM Parameter Store for Connections Only\nDESCRIPTION: This configuration example shows how to set up the AWS SSM Parameter Store backend to look up only connections, excluding variables and config. It sets the connections_prefix and nullifies the variables_prefix and config_prefix.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-ssm-parameter-store.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.amazon.aws.secrets.systems_manager.SystemsManagerParameterStoreBackend\nbackend_kwargs = {\n  \"connections_prefix\": \"airflow/connections\",\n  \"variables_prefix\": null,\n  \"config_prefix\": null,\n  \"profile_name\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Java Dataflow Job with Local JAR\nDESCRIPTION: Example showing how to create and run a Java Dataflow pipeline using a locally stored JAR file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n_ = BeamRunJavaPipelineOperator(\n    task_id=\"start-java-job-local\",\n    jar=LOCAL_JAR_PATH,\n    pipeline_options=DATAFLOW_JAVA_OPTIONS,\n    job_class=\"org.apache.beam.examples.WordCount\",\n    location=location,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Batch Prediction Job in Vertex AI\nDESCRIPTION: Example showing how to create a batch prediction job using the CreateBatchPredictionJobOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nCreateBatchPredictionJobOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    job_display_name=f\"temp_batch_job_test_{SUFFIX}\",\n    model_name=MODEL_ID,\n    predictions_format=\"jsonl\",\n    gcs_source=f\"gs://{BUCKET_NAME}/{FILENAME}\",\n    gcs_destination_prefix=f\"gs://{BUCKET_NAME}/batch_prediction_test_{SUFFIX}\",\n    task_id=\"batch_prediction_job\",)\n```\n\n----------------------------------------\n\nTITLE: Configuring Oracle Connection with Host and SID in Python\nDESCRIPTION: Example of configuring an Oracle connection using a host and SID as schema. This approach is an alternative to using service_name for connecting to an Oracle database.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/connections/oracle.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nHost = \"dbhost.example.com\"\nSchema = \"orcl\"\n```\n\n----------------------------------------\n\nTITLE: Publishing Messages to Amazon SNS Topic using SnsPublishOperator\nDESCRIPTION: Example demonstrating how to publish a message to an Amazon SNS topic using the SnsPublishOperator in Airflow. The operator handles the connection to AWS and message delivery to the specified SNS topic.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sns.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npublish_task = SnsPublishOperator(\n    task_id=\"publish_message\",\n    target_arn=SNS_TOPIC_ARN,\n    message=\"This is a sample message\",\n    subject=\"This is the subject line\",\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Object Storage for Airflow XComs (INI)\nDESCRIPTION: This configuration snippet demonstrates how to set up Airflow to use an S3 bucket for storing XComs larger than 1MB, applying gzip compression. It requires setting the `xcom_backend` to `XComObjectStorageBackend`, specifying the S3 path including the connection ID in `xcom_objectstorage_path`, setting the size threshold in `xcom_objectstorage_threshold`, and defining the compression method in `xcom_objectstorage_compression`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/io/docs/xcom_backend.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nxcom_backend = airflow.providers.common.io.xcom.backend.XComObjectStorageBackend\n\n[common.io]\nxcom_objectstorage_path = s3://conn_id@mybucket/key\nxcom_objectstorage_threshold = 1048576\nxcom_objectstorage_compression = gzip\n```\n\n----------------------------------------\n\nTITLE: Configuring a SparkR Job for Google Cloud Dataproc\nDESCRIPTION: This code defines a configuration for a SparkR job to be submitted to a Dataproc cluster. It specifies the main R file and arguments for the SparkR execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nSPARKR_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"spark_r_job\": {\n        \"main_r_file_uri\": f\"gs://{BUCKET_NAME}/{SPARKR_MAIN}\",\n        \"args\": [f\"gs://{BUCKET_NAME}/{SPARKR_FILE}\"],\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Defining botocore.config.Config Parameters - Python\nDESCRIPTION: This snippet shows an example Python dictionary for configuring botocore.config.Config, which controls AWS SDK behavior in Airflow integrations. The dictionary includes fields for signature versioning, S3 endpoint specification, retry policies, connection and read timeouts, and TCP keepalive, allowing detailed tuning of client interactions. This structure must be passed as part of the 'botocore_config' parameter in Airflow AWS connections, and relies on the botocore library being available.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/_partials/generic_parameters.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"signature_version\": \"unsigned\",\n    \"s3\": {\n        \"us_east_1_regional_endpoint\": True,\n    },\n    \"retries\": {\n      \"mode\": \"standard\",\n      \"max_attempts\": 10,\n    },\n    \"connect_timeout\": 300,\n    \"read_timeout\": 300,\n    \"tcp_keepalive\": True,\n}\n```\n\n----------------------------------------\n\nTITLE: Running a Dataplex Data Profile Scan with Deferrable Airflow Operator\nDESCRIPTION: Uses the DataplexRunDataProfileScanOperator in deferrable mode to execute a Dataplex Data Profile scan. This provides a more efficient execution model for long-running data profile scan jobs in Google Cloud Dataplex.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nrun_data_profile_def = DataplexRunDataProfileScanOperator(\n    task_id=\"run_data_profile_def\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    data_scan_id=DATA_PROFILE_SCAN_ID,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Using CloudTextToSpeechSynthesizeOperator in Airflow DAG (Python)\nDESCRIPTION: Provides a complete example of using the CloudTextToSpeechSynthesizeOperator within an Airflow DAG. It demonstrates how to instantiate the operator, passing the required configuration dictionaries (input_data, voice_config, audio_config), the target GCS bucket, and the output filename.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/text_to_speech.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsynthesize_text = CloudTextToSpeechSynthesizeOperator(\n    project_id=GCP_PROJECT_ID,\n    input_data=input_data,\n    voice_config=voice_config,\n    audio_config=audio_config,\n    target_bucket_name=GCP_TTS_BUCKET,\n    target_filename=filename,\n    task_id=\"synthesize_text\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing MultipleCronTriggerTimetable in Airflow DAG\nDESCRIPTION: Example of using MultipleCronTriggerTimetable with multiple cron expressions to schedule a DAG at 1:10 and 2:40 each day. This is useful when a schedule cannot be expressed by a single cron expression.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timetable.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.timetables.trigger import MultipleCronTriggerTimetable\n\n\n# At 1:10 and 2:40 each day.\n@dag(schedule=MultipleCronTriggerTimetable(\"10 1 * * *\", \"40 2 * * *\", timezone=\"UTC\"), ...)\ndef example_dag():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Implementing DayOfWeekSensor in Apache Airflow\nDESCRIPTION: This snippet demonstrates how to use the DayOfWeekSensor to sense for a specific day of the week. It creates a sensor that waits for Wednesday (week_day=2) to start the task.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/datetime.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nDayOfWeekSensor(\n    task_id=\"wait_for_wednesday\",\n    week_day=2,  # 0 is Monday, 6 is Sunday\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Command on Remote GCE Instance without Project ID\nDESCRIPTION: This snippet shows how to create a ComputeEngineRemoteInstanceSSHOperator without specifying a project ID. The project ID will be automatically retrieved from the Google credentials used.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute_ssh.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineSSHOperator(\n    task_id=\"ssh_operator_task_id\",\n    instance_name=\"instance_name\",\n    zone=\"us-central1-a\",\n    command=\"echo 'Hello World!'\",\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Storing Connections as JSON in AWS SSM Parameter Store\nDESCRIPTION: These JSON snippets demonstrate how to store connection information in AWS SSM Parameter Store using the JSON format. They show examples for HTTP and Spark connections, including conn_type, host, and port.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-ssm-parameter-store.rst#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"conn_type\": \"http\", \"host\": \"https://example.com\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"conn_type\": \"spark\", \"host\": \"spark://spark-main-0.spark-main.spark\", \"port\": 7077}\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Auth Manager via INI in Airflow\nDESCRIPTION: This snippet shows how to configure Apache Airflow to use the AWS authentication manager by setting the `auth_manager` and `region_name` parameters within the `[core]` section of the `airflow.cfg` configuration file. Ensure `<region_name>` is replaced with your specific AWS region. This configuration needs to be consistent across all Airflow components.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/setup/config.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nauth_manager = airflow.providers.amazon.aws.auth_manager.aws_auth_manager.AwsAuthManager\nregion_name = <region_name>\n```\n\n----------------------------------------\n\nTITLE: Enabling Alert Policies with StackdriverEnableAlertPoliciesOperator in Python\nDESCRIPTION: This example shows how to use the StackdriverEnableAlertPoliciesOperator to enable Alert Policies identified by a given filter. The operator can be used with or without a project ID, which will be retrieved from the Google Cloud connection if not provided.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/stackdriver.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nStackdriverEnableAlertPoliciesOperator(\n    task_id='enable_alert_policies',\n    filter_='',\n    project_id=None,\n    gcp_conn_id='google_cloud_default'\n)\n```\n\n----------------------------------------\n\nTITLE: Polling with HttpSensor in Python\nDESCRIPTION: This snippet demonstrates using `HttpSensor` to poll the `/get` endpoint of `httpbin` via the `http_default` connection. The sensor waits until the response text contains the string 'httpbin', using a lambda function defined in `response_check`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntask_http_sensor_check = HttpSensor(\n    task_id=\"http_sensor_check\",\n    http_conn_id=\"http_default\",\n    endpoint=\"get\",\n    request_params={},\n    response_check=lambda response: \"httpbin\" in response.text,\n    poke_interval=5,\n)\n```\n\n----------------------------------------\n\nTITLE: Controlling DAG Run State with Teardown\nDESCRIPTION: Shows how to configure teardown tasks to affect DAG run state using on_failure_fail_dagrun parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster >> run_query >> delete_cluster.as_teardown(setups=create_cluster, on_failure_fail_dagrun=True)\n```\n\n----------------------------------------\n\nTITLE: Listing Translation Glossaries with TranslateListGlossariesOperator in Python\nDESCRIPTION: This code snippet shows how to use TranslateListGlossariesOperator to list all available translation glossaries on a Google Cloud project. It demonstrates the basic usage of the operator for retrieving glossary information.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlist_glossaries = TranslateListGlossariesOperator(\n    task_id=\"list_glossaries\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n)\n```\n\n----------------------------------------\n\nTITLE: Using XCom in Jinja Template\nDESCRIPTION: Illustrates how to use XCom values within Jinja templates in Airflow, allowing for dynamic SQL queries based on task outputs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/xcoms.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nSELECT * FROM {{ task_instance.xcom_pull(task_ids='foo', key='table_name') }}\n```\n\n----------------------------------------\n\nTITLE: Wait for DV360 Query - Python Example\nDESCRIPTION: Demonstrates using GoogleDisplayVideo360RunQuerySensor to wait for a report query to complete execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/display_video.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n[START howto_google_display_video_wait_run_query_sensor]\n[END howto_google_display_video_wait_run_query_sensor]\n```\n\n----------------------------------------\n\nTITLE: Deleting AlloyDB Instance with Airflow Operator\nDESCRIPTION: Uses AlloyDBDeleteInstanceOperator to delete an AlloyDB instance from Google Cloud.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/alloy_db.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndelete_instance = AlloyDBDeleteInstanceOperator(\n    task_id=\"delete_instance\",\n    project_id=GCP_PROJECT_ID,\n    cluster_id=CLUSTER_ID,\n    instance_id=INSTANCE_ID,\n    region=GCP_ALLOYDB_REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Executing multi-query string in one Databricks session\nDESCRIPTION: Modifies Databricks run to execute related multi-query strings in a single session.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n\"Making Databricks run related multi-query string in one session again (#31898) (#31899)\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Executor Aliases in Airflow INI File\nDESCRIPTION: Shows how to set up aliases for executors in the Airflow configuration file to make it easier to specify executors on tasks and DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/index.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nexecutor = LocalExecutor,ShortName:my.custom.module.ExecutorClass\n```\n\n----------------------------------------\n\nTITLE: Configuring AllowListRoundRobinPolicy for Cassandra Load Balancing in Airflow\nDESCRIPTION: Example JSON configuration for the Extra field to specify AllowListRoundRobinPolicy as the load balancing policy. This policy restricts the driver to only use a specific list of hosts for Cassandra connections.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/cassandra/docs/connections/cassandra.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"load_balancing_policy\": \"AllowListRoundRobinPolicy\",\n  \"load_balancing_policy_args\": {\n    \"hosts\": [\"HOST1\", \"HOST2\", \"HOST3\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Item Schedule in Fabric with MSGraphAsyncOperator (Python)\nDESCRIPTION: This snippet shows how to use the MSGraphAsyncOperator to create a scheduled item in Microsoft Fabric using the Graph API. The task requires specifying the correct endpoint and request payload to define the schedule. A POST method is used, and the operator must have access to the Fabric service with appropriate permissions. The expected output is confirmation and details about the created schedule.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/msgraph.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncreate_item_schedule = MSGraphAsyncOperator(\n    task_id=\"create_item_schedule\",\n    endpoint=\"fabric/items/{item_id}/schedules\",\n    method=\"POST\",\n    payload={\n        # Define schedule parameters here\n    },\n    msgraph_conn_id=\"msgraph_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Updating a Cloud Composer Environment in Python\nDESCRIPTION: This snippet demonstrates how to use the CloudComposerUpdateEnvironmentOperator to update a Cloud Composer environment. It specifies the project ID, region, environment name, update configuration, and update mask.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nupdate_environment_task = CloudComposerUpdateEnvironmentOperator(\n    task_id=\"update-environment\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    environment_name=ENVIRONMENT_NAME,\n    update_mask=UPDATE_MASK,\n    environment=UPDATE_CONFIG,\n)\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Queries with SQLExecuteQueryOperator in Apache Drill\nDESCRIPTION: Example demonstrating how to use SQLExecuteQueryOperator to execute SQL queries on an Apache Drill server. The operator requires a configured Drill connection ID and supports templated SQL queries.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\nexecute_query = SQLExecuteQueryOperator(\n    task_id=\"execute_drill_query\",\n    conn_id=\"drill_conn_id\",\n    sql=\"SELECT * FROM cp.`employee.json` LIMIT 1\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Airflow Client API in Python\nDESCRIPTION: Demonstrates how to initialize the Airflow client, configure authentication, and make an API call to retrieve configuration settings. The example includes error handling and shows how to use the ConfigApi endpoint with basic authentication.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport airflow_client.client\nfrom pprint import pprint\nfrom airflow_client.client.api import config_api\nfrom airflow_client.client.model.config import Config\nfrom airflow_client.client.model.error import Error\n\n# Defining the host is optional and defaults to /api/v1\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = client.Configuration(host=\"/api/v1\")\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure HTTP basic authorization: Basic\nconfiguration = client.Configuration(username=\"YOUR_USERNAME\", password=\"YOUR_PASSWORD\")\n\n\n# Enter a context with an instance of the API client\nwith client.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = config_api.ConfigApi(api_client)\n\n    try:\n        # Get current configuration\n        api_response = api_instance.get_config()\n        pprint(api_response)\n    except client.ApiException as e:\n        print(\"Exception when calling ConfigApi->get_config: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Constraints File\nDESCRIPTION: Commands to install Airflow with custom dependencies and generate a new constraints file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-pypi.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install \"apache-airflow[celery]==|version|\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-|version|/constraints-3.9.txt\"\npip install \"apache-airflow==|version|\" dbt-core==0.20.0\npip freeze > my-constraints.txt\n```\n\n----------------------------------------\n\nTITLE: Running an Azure Data Factory Pipeline Deferrably with Airflow Operator (Python)\nDESCRIPTION: This example shows how to use `AzureDataFactoryRunPipelineOperator` with the `deferrable=True` flag. This mode triggers the pipeline and then defers polling for completion to the Airflow Triggerer, freeing up the worker slot. Requires an Azure Data Factory connection configured in Airflow (`azure_data_factory_conn_id`).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/adf_run_pipeline.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../tests/system/microsoft/azure/example_adf_run_pipeline.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_adf_run_pipeline_with_deferrable_flag]\n#     :end-before: [END howto_operator_adf_run_pipeline_with_deferrable_flag]\n\n# Example (simulated based on description with dedent: 4):\n    from airflow.providers.microsoft.azure.operators.data_factory import AzureDataFactoryRunPipelineOperator\n\n    run_pipeline_defer = AzureDataFactoryRunPipelineOperator(\n        task_id=\"run_pipeline_defer\",\n        pipeline_name=\"pipeline1\",\n        azure_data_factory_conn_id=\"azure_data_factory_default\",\n        deferrable=True,\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Inserting a Compute Engine Instance from Template with ComputeEngineInsertInstanceFromTemplateOperator in Python\nDESCRIPTION: Creates a new Google Compute Engine instance based on a specified instance template using the ComputeEngineInsertInstanceFromTemplateOperator. This snippet shows how to use the operator with and without specifying a project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineInsertInstanceFromTemplateOperator(\n    project_id=GCP_PROJECT_ID,\n    zone=GCE_ZONE,\n    resource_id=INSTANCE_NAME,\n    source_instance_template=SOURCE_INSTANCE_TEMPLATE,\n    task_id=\"gcp_compute_create_instance_from_template\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineInsertInstanceFromTemplateOperator(\n    zone=GCE_ZONE,\n    resource_id=INSTANCE_NAME,\n    source_instance_template=SOURCE_INSTANCE_TEMPLATE,\n    task_id=\"gcp_compute_create_instance_from_template\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using XCom Result from GKEStartPodOperator\nDESCRIPTION: Example of accessing the XCom result pushed by a GKEStartPodOperator task in a subsequent task using the 'xcom_pull' method.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nBashOperator(\n    task_id=\"xcom_result\",\n    bash_command=\"echo \"{{ task_instance.xcom_pull('pod_task')[0] }}\"\",\n)\n```\n\n----------------------------------------\n\nTITLE: Getting Dataproc Metastore Service Details using Airflow Operator in Python\nDESCRIPTION: Illustrates how to use the `DataprocMetastoreGetServiceOperator` in an Airflow DAG to retrieve details about an existing Google Cloud Dataproc Metastore service. Requires specifying the `region`, `project_id`, and `service_id` of the target service.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nget_service = DataprocMetastoreGetServiceOperator(\n    task_id=\"get_service\",\n    region=REGION,\n    project_id=PROJECT_ID,\n    service_id=SERVICE_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Sending Files via SlackAPIFileOperator in Python (by File Path)\nDESCRIPTION: This code sample demonstrates using the SlackAPIFileOperator in Airflow (Python) to upload a file to a Slack channel by path. It requires the Airflow Slack provider, a valid Slack connection, and appropriate Slack permissions (files:write, possibly files:read and channels:read as outlined). Main parameters include the list of target channels and the path to the local file to send. Output is the uploaded file appearing in the specified Slack channels. Slack API method versions may affect usage and limitations based on your configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/operators/slack_api.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.providers.slack.operators.slack import SlackAPIFileOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2021, 1, 1),\n}\n\ndag = DAG(\n    'example_slack_file_upload',\n    default_args=default_args,\n    schedule_interval=None,\n    catchup=False,\n)\n\nupload = SlackAPIFileOperator(\n    task_id='upload_file',\n    channels=['#general'],\n    file='/path/to/myfile.txt',\n    initial_comment='Please see attached file.',\n    dag=dag,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Listing Data Fusion Pipelines in Python with Airflow\nDESCRIPTION: This snippet demonstrates how to list Data Fusion pipelines using the CloudDataFusionListPipelinesOperator in Airflow. It specifies the instance name, location, and project ID required to identify where to list the pipelines from.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datafusion.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlist_pipelines = CloudDataFusionListPipelinesOperator(\n    task_id=\"list_pipelines\",\n    instance_name=INSTANCE_NAME,\n    location=LOCATION,\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Bigtable Cluster with BigtableUpdateClusterOperator in Python\nDESCRIPTION: This snippet shows how to use the BigtableUpdateClusterOperator to modify the number of nodes in a Cloud Bigtable cluster. It demonstrates creating the operator with and without specifying the project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigtable.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nupdate_cluster_task = BigtableUpdateClusterOperator(\n    project_id=GCP_PROJECT_ID,\n    instance_id=INSTANCE_ID,\n    cluster_id=CLUSTER_ID,\n    nodes=4,\n    task_id=\"update_cluster\",\n)\n\n# The same operator can be created without project_id:\nupdate_cluster_task_no_project_id = BigtableUpdateClusterOperator(\n    instance_id=INSTANCE_ID,\n    cluster_id=CLUSTER_ID,\n    nodes=4,\n    task_id=\"update_cluster\",\n)\n```\n\n----------------------------------------\n\nTITLE: Getting DataFusion Instance with CloudDataFusionGetInstanceOperator in Python\nDESCRIPTION: This code shows how to use CloudDataFusionGetInstanceOperator to retrieve information about a Google Cloud DataFusion instance. It specifies the instance name, project ID, and location.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datafusion.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nCloudDataFusionGetInstanceOperator(\n    task_id=\"get_instance\",\n    instance_name=INSTANCE_NAME,\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    retry=RETRY,\n    timeout=TIMEOUT,\n    metadata=METADATA,\n)\n```\n\n----------------------------------------\n\nTITLE: Running WinRMOperator with Hook in Apache Airflow (Python)\nDESCRIPTION: This example illustrates how to use the initialized WinRM hook in conjunction with the WinRMOperator to execute commands on a remote machine from within an Airflow DAG. The operator is configured with the previously created hook and the command to execute. Prerequisites include the hook instance, a valid Airflow DAG context, and the Airflow WinRM provider installed. Key parameters include the WinRM hook and the command to be run; the operator will output the resulting status and any standard output/error from command execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/winrm/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nrun_cmd = WinRMOperator(\n    task_id=\"run_cmd\",\n    winrm_hook=hook,\n    command=\"dir\"\n)\n```\n\n----------------------------------------\n\nTITLE: Closing Alerts with OpsgenieCloseAlertOperator in Airflow\nDESCRIPTION: Demonstrates the usage of OpsgenieCloseAlertOperator to close an existing alert in Opsgenie. This operator is useful for automatically resolving alerts when issues are fixed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opsgenie/docs/operators/opsgenie_alert.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Upserting Notification Channels with StackdriverUpsertNotificationChannelOperator in Python\nDESCRIPTION: This snippet illustrates how to use the StackdriverUpsertNotificationChannelOperator to upsert Notification Channels identified by a given channel JSON string. If a channel with the given name already exists, the operator updates the existing channel; otherwise, it creates a new one.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/stackdriver.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nStackdriverUpsertNotificationChannelOperator(\n    task_id='upsert_notification_channel',\n    channels=NOTIFICATION_CHANNEL,\n    project_id=None,\n    gcp_conn_id='google_cloud_default'\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring SSH Connection Extras in JSON\nDESCRIPTION: Example of configuring extra parameters for an SSH connection using JSON format. This includes settings for key file, connection timeout, compression, and other SSH-specific options.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/docs/connections/ssh.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"key_file\": \"/home/airflow/.ssh/id_rsa\",\n   \"conn_timeout\": \"10\",\n   \"compress\": \"false\",\n   \"look_for_keys\": \"false\",\n   \"allow_host_key_change\": \"false\",\n   \"host_key\": \"AAAHD...YDWwq==\",\n   \"disabled_algorithms\": {\"pubkeys\": [\"rsa-sha2-256\", \"rsa-sha2-512\"]},\n   \"ciphers\": [\"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Monitoring S3 Prefix Changes with S3KeysUnchangedSensor in Python\nDESCRIPTION: This snippet demonstrates the use of S3KeysUnchangedSensor to monitor changes in the number of objects at a specific prefix in an S3 bucket. It waits until the inactivity period has passed with no increase in the number of objects.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/s3.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nS3KeysUnchangedSensor(\n    task_id=\"wait_for_keys_unchanged\",\n    bucket_name=\"my-bucket\",\n    prefix=\"path/to/my/keys/\",\n    inactivity_period=60 * 60,\n    poke_interval=60,\n    min_objects=1,\n    allow_delete=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Python Pipeline with DirectRunner using GCS File in Deferrable Mode\nDESCRIPTION: This example shows how to run a Python pipeline using DirectRunner with a GCS file in deferrable (async) mode. This approach allows freeing up Airflow workers while the pipeline executes by deferring task completion.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_direct_runner_gcs_file_async = BeamRunPythonPipelineOperator(\n    task_id=\"beam_task_direct_runner_gcs_file_async\",\n    py_file=\"{{ var.json.beam_variables.gcs_file_path }}\",\n    py_options=[],\n    pipeline_options={\n        \"output\": \"/tmp/apache_beam/direct_runner_output_gcs_async\",\n    },\n    py_requirements=[\"apache-beam[gcp]==2.44.0\"],\n    py_interpreter=\"python3\",\n    py_system_site_packages=False,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Teradata Stored Procedure with IN/INOUT/OUT Parameters (SQL)\nDESCRIPTION: This SQL code defines a Teradata stored procedure named `TEST_PROCEDURE`. It accepts an integer input (`val_in`), modifies an integer input/output parameter (`val_in_out`), and returns an integer output (`val_out`) and a string output (`value_str_out`). This procedure is used as an example to illustrate calling stored procedures from Airflow using `TeradataStoredProcedureOperator` with various parameter passing methods.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/operators/teradata.rst#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nREPLACE PROCEDURE TEST_PROCEDURE (\n    IN val_in INTEGER,\n    INOUT val_in_out INTEGER,\n    OUT val_out INTEGER,\n    OUT value_str_out varchar(100)\n)\n    BEGIN\n        set val_out = val_in * 2;\n        set val_in_out = val_in_out * 4;\n        set value_str_out = 'string output';\n    END;\n/\n```\n\n----------------------------------------\n\nTITLE: Deleting GCS Bucket using Python\nDESCRIPTION: Shows how to use the GCSDeleteBucketOperator to remove a bucket from Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndelete_bucket = GCSDeleteBucketOperator(\n    task_id=\"delete_bucket\", bucket_name=BUCKET_NAME\n)\n```\n\n----------------------------------------\n\nTITLE: Copying a Single File in GCS Using GCSToGCSOperator in Python\nDESCRIPTION: Example demonstrating how to copy a single file from one GCS bucket to another using the GCSToGCSOperator. When exact_match is set to False, the source_object will be considered as a prefix for searching objects in the source bucket.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncopy_single_file = GCSToGCSOperator(\n    task_id=\"gcs_to_gcs_single_file\",\n    source_bucket=BUCKET_1_SRC,\n    source_object=OBJECT_1,\n    destination_bucket=BUCKET_1_DST,\n    exact_match=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Copying Single File from Google Drive to Google Cloud Storage using Airflow\nDESCRIPTION: This snippet demonstrates how to use the GoogleDriveToGCSOperator to copy a single file from a shared Google Drive folder to a Google Cloud Storage bucket. It includes parameters for specifying the source drive, folder, and file, as well as the destination bucket and object name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gdrive_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nGoogleDriveToGCSOperator(\n    task_id=\"upload_gdrive_to_gcs\",\n    folder_id=\"{{ var.value.get('folder_id') }}\",\n    file_name=\"{{ var.value.get('file_name') }}\",\n    drive_id=\"{{ var.value.get('drive_id') }}\",\n    bucket_name=\"{{ var.value.get('bucket_name') }}\",\n    object_name=\"{{ var.value.get('object_name') }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataplex Entry Type using Airflow Python\nDESCRIPTION: This snippet demonstrates how to create a new Entry Type in a specific location within Google Cloud Dataplex Catalog using the `DataplexCatalogCreateEntryTypeOperator` in an Airflow DAG. It utilizes a predefined configuration dictionary (like the one shown previously) and references an external example file for the complete operator usage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_41\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_create_entry_type]\n#     :end-before: [END howto_operator_dataplex_catalog_create_entry_type]\n\n# This example uses DataplexCatalogCreateEntryTypeOperator with a configuration dictionary.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Monitoring SageMaker Pipeline Execution State in Python\nDESCRIPTION: This code snippet demonstrates the use of SageMakerPipelineSensor to check the state of an Amazon SageMaker pipeline execution until it reaches a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_sensor_sagemaker_pipeline]\n# Code snippet not provided in the original text\n# [END howto_sensor_sagemaker_pipeline]\n```\n\n----------------------------------------\n\nTITLE: Evaluating a Vertex AI Model using Airflow Operator in Python\nDESCRIPTION: Illustrates how to use the `RunEvaluationOperator` from `airflow.providers.google.cloud.operators.vertex_ai.generative_model` to evaluate a Vertex AI model. The evaluation summary metrics are returned via XCom under the 'summary_metrics' key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_51\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/vertex_ai/example_vertex_ai_generative_model.py\n    :language: python\n    :dedent: 4\n    :start-after: [START how_to_cloud_vertex_ai_run_evaluation_operator]\n    :end-before: [END how_to_cloud_vertex_ai_run_evaluation_operator]\n```\n\n----------------------------------------\n\nTITLE: Database Session Handling with create_session\nDESCRIPTION: Shows proper database session management using create_session helper, emphasizing explicit session handling and avoiding commits within functions.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sqlalchemy.orm import Session\n\nfrom airflow.utils.session import create_session\n\n\ndef my_call(x, y, *, session: Session):\n    ...\n    # You MUST not commit the session here.\n\n\nwith create_session() as session:\n    my_call(x, y, session=session)\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Logging with Custom Logging Configuration in Airflow\nDESCRIPTION: This configuration shows how to enable both remote logging and custom logging configuration in the airflow.cfg file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/advanced-logging-configuration.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\nremote_logging = True\nlogging_config_class = log_config.LOGGING_CONFIG\n```\n\n----------------------------------------\n\nTITLE: Deploying a Google Cloud Function using CloudFunctionDeployFunctionOperator in Python\nDESCRIPTION: This snippet shows how to use the CloudFunctionDeployFunctionOperator to deploy a function to Google Cloud Functions. It includes different variants for specifying the function source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/functions.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndeploy_task = CloudFunctionDeployFunctionOperator(\n    task_id=\"deploy_function\",\n    location=GCP_LOCATION,\n    body=body,\n    project_id=PROJECT_ID,\n    gcp_conn_id=GCP_CONN_ID,\n)\n\n# Alternative: Deploy from filesystem\ndeploy_task = CloudFunctionDeployFunctionOperator(\n    task_id=\"deploy_function\",\n    location=GCP_LOCATION,\n    body=body,\n    project_id=PROJECT_ID,\n    gcp_conn_id=GCP_CONN_ID,\n    zip_path=zip_path,\n)\n\n# Deploy without explicit project ID\ndeploy_task = CloudFunctionDeployFunctionOperator(\n    task_id=\"deploy_function\",\n    location=GCP_LOCATION,\n    body=body,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Starting BigQuery Data Transfer Run Manually in Python\nDESCRIPTION: This snippet demonstrates how to manually start a BigQuery Data Transfer run using the BigQueryDataTransferServiceStartTransferRunsOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery_dts.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstart_transfer = BigQueryDataTransferServiceStartTransferRunsOperator(\n    task_id=\"start_transfer\",\n    transfer_config_id=\"{{ task_instance.xcom_pull('create_transfer')['transfer_config_id'] }}\",\n    requested_run_time=requested_run_time,\n    project_id=PROJECT_ID,\n    location=LOCATION,\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Image Safe Search Detection Result from XCom - Airflow Python Operator\nDESCRIPTION: This code fetches the output from a CloudVisionDetectImageSafeSearchOperator via XCom in Airflow, making safe search details available for workflow routing or alerts. The input is the task instance for safe search detection, and the expected result is the scan output for compliance or moderation logic.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nresult = detect_safe_search.output\n\n```\n\n----------------------------------------\n\nTITLE: Transforming S3 Object with S3FileTransformOperator\nDESCRIPTION: Demonstrates how to transform data from one S3 object and save it to another using the S3FileTransformOperator. Supports optional S3 Select expressions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/s3.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntransform_object = S3FileTransformOperator(\n    task_id=\"transform_object\",\n    source_s3_key=f\"s3://{BUCKET_NAME}/{KEY}\",\n    dest_s3_key=f\"s3://{BUCKET_NAME}/{NEW_KEY}\",\n    transform_script=\"transform.py\",\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving PowerBI Workspaces with MSGraphAsyncOperator (Python)\nDESCRIPTION: This code snippet illustrates invoking the MSGraphAsyncOperator to collect a list of PowerBI workspaces through the Microsoft Graph API. Requires the Airflow Microsoft Azure provider and corresponding connection setup for PowerBI access. The API endpoint used (e.g., '/groups') should reference the PowerBI workspaces context, and the operator's output will consist of metadata about available workspaces.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/msgraph.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npowerbi_workspaces = MSGraphAsyncOperator(\n    task_id=\"get_powerbi_workspaces\",\n    endpoint=\"groups\",\n    msgraph_conn_id=\"msgraph_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: AQLSensor Example - Monitoring Document in Collection\nDESCRIPTION: Shows how to implement an AQLSensor to wait for a specific document in a collection. The example monitors the students collection for a document with student name 'judy'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/operators/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwait_for_student = AQLSensor(\n    task_id=\"wait_for_student\",\n    arangodb_conn_id=\"arangodb_default\",\n    query=\"FOR doc IN students FILTER doc.name == 'judy' RETURN doc\"\n```\n\n----------------------------------------\n\nTITLE: Templating Fields for GCE Instance Template Insertion in Python\nDESCRIPTION: Defines the template fields available for the ComputeEngineInsertInstanceTemplateOperator, which allow for runtime parameter substitution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"project_id\",\n    \"body\",\n    \"request_id\",\n    \"gcp_conn_id\",\n    \"api_version\",\n    \"impersonation_chain\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS SAML Metadata in Airflow via Environment Variables - bash\nDESCRIPTION: This snippet demonstrates setting the necessary Airflow configuration options for AWS IAM Identity Center SAML via environment variables. 'AIRFLOW__API__BASE_URL' and 'AIRFLOW__AWS_AUTH_MANAGER__SAML_METADATA_URL' must be exported with values matching your Airflow deployment and the metadata file URL. This approach is commonly used when deploying Airflow via Docker or similar environments where environment variable configuration is preferable to ini file editing.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/setup/identity-center.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__API__BASE_URL='<base_url>'\nexport AIRFLOW__AWS_AUTH_MANAGER__SAML_METADATA_URL='<saml_metadata_file_url>'\n```\n\n----------------------------------------\n\nTITLE: Defining Connection Keyword Arguments as Booleans in Airflow Provider - JSON\nDESCRIPTION: This snippet shows the correct and updated approach for specifying connect_kwargs in an Airflow provider connection, where keyword arguments are now provided as JSON booleans rather than strings. For instance, 'autocommit' is set to false and 'ansi' is set to true, directly as Boolean values. This aligns with newer Airflow provider versions, which may reject string values for boolean fields. The input is a JSON dictionary with appropriate boolean values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\\n   \\\"connect_kwargs\\\": {\\n      \\\"autocommit\\\": false,\\n      \\\"ansi\\\": true\\n   }\\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an EMR Job Flow using EmrCreateJobFlowOperator in Python\nDESCRIPTION: Demonstrates instantiating the `EmrCreateJobFlowOperator` within an Airflow DAG to create a new EMR cluster. It takes the `job_flow_overrides` dictionary (like `JOB_FLOW_OVERRIDES` defined previously) and an `aws_conn_id` for AWS credentials as parameters. The `task_id` uniquely identifies this operation. The operator can run in deferrable mode (`deferrable=True`) for better resource utilization, requiring the Airflow triggerer.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncreate_job_flow_task = EmrCreateJobFlowOperator(\n    task_id=\"create_job_flow_task\",\n    job_flow_overrides=JOB_FLOW_OVERRIDES,\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Enabling Postgres Client Message Logging (Python)\nDESCRIPTION: Configures `SQLExecuteQueryOperator` to enable logging of database messages sent to the client (e.g., from `RAISE` statements in a stored procedure `proc()`). This is achieved by setting `enable_log_db_messages` to `True` within the `hook_params` dictionary, which passes the setting to the underlying DbApiHook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/operators.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncall_proc = SQLExecuteQueryOperator(\n    task_id=\"call_proc\",\n    conn_id=\"postgres_default\",\n    sql=\"call proc();\",\n    hook_params={\"enable_log_db_messages\": True},\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring TaskGroup with Default Arguments\nDESCRIPTION: Example of configuring TaskGroups with default arguments that override DAG-level defaults. Shows how to set retry configurations at different levels.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nfrom airflow.sdk import DAG\nfrom airflow.sdk import task_group\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.empty import EmptyOperator\n\nwith DAG(\n    dag_id=\"dag1\",\n    start_date=datetime.datetime(2016, 1, 1),\n    schedule=\"@daily\",\n    default_args={\"retries\": 1},\n):\n\n    @task_group(default_args={\"retries\": 3})\n    def group1():\n        \"\"\"This docstring will become the tooltip for the TaskGroup.\"\"\"\n        task1 = EmptyOperator(task_id=\"task1\")\n        task2 = BashOperator(task_id=\"task2\", bash_command=\"echo Hello World!\", retries=2)\n        print(task1.retries)  # 3\n        print(task2.retries)  # 2\n```\n\n----------------------------------------\n\nTITLE: Creating BigQuery External Table from GCS Data in Python\nDESCRIPTION: Example of using BigQueryCreateExternalTableOperator to create an external table in BigQuery that reads data directly from Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/presto_to_gcs.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntask_create_external_table_multiple_types = BigQueryCreateExternalTableOperator(\n    task_id=\"create_external_table_multiple_types\",\n    table_resource={\n        \"tableReference\": {\n            \"projectId\": PROJECT_ID,\n            \"datasetId\": DATASET,\n            \"tableId\": \"external_table_multiple_types\",\n        },\n        \"externalDataConfiguration\": {\n            \"sourceFormat\": \"NEWLINE_DELIMITED_JSON\",\n            \"compression\": \"NONE\",\n            \"sourceUris\": [f\"gs://{BUCKET}/{DATASET_PREFIX}/multiple_types/data*.json\"],\n            \"schema\": {\"fields\": SCHEMA_FIELDS},\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Tag Template with CloudDataCatalogCreateTagTemplateOperator in Python\nDESCRIPTION: Example of creating a tag template in Google Cloud DataCatalog using the CloudDataCatalogCreateTagTemplateOperator. The result is saved to XCom, allowing it to be used by other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_create_tag_template]\ncreate_tag_template = CloudDataCatalogCreateTagTemplateOperator(\n    task_id=\"create_tag_template\",\n    location=LOCATION,\n    tag_template_id=TAG_TEMPLATE_ID,\n    tag_template={\n        \"display_name\": \"Example Template\",\n        \"fields\": {\n            \"source\": {\n                \"display_name\": \"Source of data asset\",\n                \"type\": {\"primitive_type\": \"STRING\"},\n            }\n        },\n    },\n)\n# [END howto_operator_gcp_datacatalog_create_tag_template]\n```\n\n----------------------------------------\n\nTITLE: Running MyPy Check for All Files in a Specific Folder using Pre-commit in Bash\nDESCRIPTION: This command runs the MyPy check for all files in a specific folder (e.g., 'airflow') using pre-commit.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --hook-stage manual mypy-airflow --all-files\n```\n\n----------------------------------------\n\nTITLE: Customizing UI Field Behavior in Airflow Connections\nDESCRIPTION: Example demonstrating how to customize connection field behavior in the UI including hiding fields, relabeling, and adding placeholder text using the get_ui_field_behaviour method.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\ndef get_ui_field_behaviour() -> dict[str, Any]:\n    \"\"\"Returns custom field behaviour\"\"\"\n    return {\n        \"hidden_fields\": [\"port\", \"host\", \"login\", \"schema\"],\n        \"relabeling\": {},\n        \"placeholders\": {\n            \"password\": \"Asana personal access token\",\n            \"workspace\": \"My workspace gid\",\n            \"project\": \"My project gid\",\n        },\n    }\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Undeprecate private_key option in SFTPHook\nDESCRIPTION: Associated with commit 15e044c7e4, this message indicates a reversal of a previous deprecation for the `private_key` option within the SFTPHook, making it non-deprecated again. Tracked via issue #15348.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_25\n\nLANGUAGE: text\nCODE:\n```\n``Undeprecate private_key option in SFTPHook (#15348)``\n```\n\n----------------------------------------\n\nTITLE: Extracting Data from Salesforce to Amazon S3 using SalesforceToS3Operator in Python\nDESCRIPTION: This example demonstrates how to extract account data from Salesforce and upload it to an Amazon S3 bucket using the SalesforceToS3Operator. The operator queries Salesforce for all account data and stores it in the specified S3 location.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/salesforce_to_s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsalesforce_to_s3 = SalesforceToS3Operator(\n    task_id=\"salesforce_to_s3\",\n    salesforce_query=\"SELECT Id, Name, Account.Name, LeadSource FROM Contact\",\n    salesforce_conn_id=\"salesforce_conn\",\n    aws_conn_id=\"aws_default\",\n    s3_bucket_name=\"{{ dag_run.conf['bucket_name'] if dag_run and dag_run.conf and 'bucket_name' in dag_run.conf else 'test-airflow-bucket' }}\",\n    s3_key=\"{{ dag_run.conf['s3_key'] if dag_run and dag_run.conf and 's3_key' in dag_run.conf else 'salesforce/contacts_\" + ts_nodash + \".csv' }}\",\n    include_deleted=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Postgres Table via External SQL File (Python)\nDESCRIPTION: Instantiates `SQLExecuteQueryOperator` to execute INSERT statements stored in `sql/pet_schema.sql` to populate the `pet` table. Requires the `conn_id` for the PostgreSQL connection and the path to the SQL file containing the INSERT statements.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/operators.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npopulate_pet_table = SQLExecuteQueryOperator(\n    task_id=\"populate_pet_table\",\n    conn_id=\"postgres_default\",\n    sql=\"sql/pet_schema.sql\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating AutoML Image Object Detection Training Job in Vertex AI\nDESCRIPTION: Example showing how to create an AutoML image object detection training job for detecting objects in images.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nCreateAutoMLImageTrainingJobOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    display_name=f\"temp_image_training_job_test_{SUFFIX}\",\n    dataset_id=DATASET_ID,\n    prediction_type=\"object_detection\",\n    multi_label=False,\n    model_type=\"CLOUD\",\n    training_budget_milli_node_hours=8000,\n    task_id=\"image_training_job\",)\n```\n\n----------------------------------------\n\nTITLE: Selecting data into a file with DatabricksSqlOperator\nDESCRIPTION: Example of using DatabricksSqlOperator to execute a SELECT query and store the results in a file. This demonstrates how to output query results to a specified file path.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/sql.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndatabricks_select_file = DatabricksSqlOperator(\n    task_id=\"databricks_sql_select_file\",\n    sql=\"SELECT 1 as One\",\n    sql_warehouse_name=\"my_warehouse\",\n    output_path=\"my_file.txt\",\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Spanner Database in Python\nDESCRIPTION: Example of using SpannerQueryDatabaseInstanceOperator to execute an arbitrary DML query (INSERT, UPDATE, DELETE) in a Cloud Spanner database. The operator can be created with or without a project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/spanner.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_spanner_query]\n# Without project_id\nspanner_query_database = SpannerQueryDatabaseInstanceOperator(\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    database_id=\"data_database\",\n    query=[\"DELETE FROM data WHERE test_id = 1\"],\n    task_id=\"query_database\",\n)\n\n# With project_id\nspanner_query_database = SpannerQueryDatabaseInstanceOperator(\n    project_id=\"{{ var.value.spanner_project }}\",\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    database_id=\"data_database\",\n    query=[\"DELETE FROM data WHERE test_id = 1\"],\n    task_id=\"query_database\",\n)\n# [END howto_operator_spanner_query]\n```\n\n----------------------------------------\n\nTITLE: Accessing TaskInstance and DagRun Info in Airflow Task\nDESCRIPTION: This code defines an Airflow task that prints information about the current TaskInstance and DagRun. It demonstrates how to access run ID, duration, and queued time within a task context.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/docs/shared/template-examples/taskflow-kwargs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models.taskinstance import TaskInstance\nfrom airflow.models.dagrun import DagRun\n\n\n@task\ndef print_ti_info(**kwargs):\n    ti: TaskInstance = kwargs[\"task_instance\"]\n    print(f\"Run ID: {ti.run_id}\")  # Run ID: scheduled__2023-08-09T00:00:00+00:00\n    print(f\"Duration: {ti.duration}\")  # Duration: 0.972019\n\n    dr: DagRun = kwargs[\"dag_run\"]\n    print(f\"DAG Run queued at: {dr.queued_at}\")  # 2023-08-10 00:00:01+02:20\n```\n\n----------------------------------------\n\nTITLE: Using DatabricksPartitionSensor to monitor partition existence\nDESCRIPTION: Example of using DatabricksPartitionSensor to check for the existence of partitions in a Databricks table. The sensor waits until specified partitions with given values exist in the target table.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/sql.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npartition_sensor = DatabricksPartitionSensor(\n    task_id=\"databricks_partition_sensor\",\n    table_name=\"db_sensor.partition_table\",\n    partitions={\"p\": \"test\"},\n    sql_warehouse_name=endpoint,\n    conn_id=DATABRICKS_SYSTEM_TEST_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating SageMaker Unified Studio Workflow using Notebook Operator\nDESCRIPTION: Example demonstrating how to use the SageMakerNotebookOperator to create and orchestrate a SageMaker Unified Studio workflow. The operator allows execution of notebooks within a SageMaker Studio environment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemakerunifiedstudio.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsagemaker_notebook_task = SageMakerNotebookOperator(\n    task_id=\"sagemaker_notebook_task\",\n    aws_conn_id=\"aws_default\",\n    notebook_name=\"example_notebook\",\n    project_name=\"example_project\",\n    notebook_params={\"param1\": \"value1\"},\n    region_name=\"us-west-2\"\n)\n```\n\n----------------------------------------\n\nTITLE: Terminating EC2 Instances with Airflow AWS Operator - Python\nDESCRIPTION: This snippet illustrates usage of Airflow's EC2TerminateInstanceOperator to terminate one or more EC2 instances by specifying their IDs in an Airflow DAG task. Required dependencies include Airflow AWS provider and Boto3, and the operator emits information about terminated instances. Ensure that proper AWS credentials, instance IDs, and terminate-instance permissions are provided, as the operation is irreversible.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/ec2.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# There is no code in the provided input itself, only includes and code references to other files.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Notification Channels with StackdriverDeleteNotificationChannelOperator in Python\nDESCRIPTION: This example demonstrates how to use the StackdriverDeleteNotificationChannelOperator to delete a Notification Channel identified by a given name. The name of the channel to be deleted should be in the format 'projects/<PROJECT_NAME>/notificationChannels/<CHANNEL_NAME>'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/stackdriver.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nStackdriverDeleteNotificationChannelOperator(\n    task_id='delete_notification_channel',\n    name='',\n    gcp_conn_id='google_cloud_default'\n)\n```\n\n----------------------------------------\n\nTITLE: Waiting for a Google Campaign Manager Report in Airflow\nDESCRIPTION: Example of using GoogleCampaignManagerReportSensor to wait for a report to be ready for downloading. Reports are generated asynchronously, and this sensor helps manage the workflow timing.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/campaign_manager.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwait_for_report = GoogleCampaignManagerReportSensor(\n    profile_id=PROFILE_ID,\n    report_id=FILE_ID,\n    file_id=FILE_ID,\n    task_id=\"wait_for_report\",\n)\n```\n\n----------------------------------------\n\nTITLE: Reference Image Creation with Generated ID\nDESCRIPTION: Creates a ReferenceImage with API-generated product_set_id\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_vision_reference_image_create]\\n[END howto_operator_vision_reference_image_create]\n```\n\n----------------------------------------\n\nTITLE: Using Secret Backend for Airflow Database Connection in INI Configuration\nDESCRIPTION: This configuration shows how to use a secret backend to retrieve the database connection string in the airflow.cfg file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-config.rst#2025-04-22_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n[database]\nsql_alchemy_conn_secret = sql_alchemy_conn\n```\n\n----------------------------------------\n\nTITLE: Configuring DAG File Import Timeout in Airflow Local Settings\nDESCRIPTION: Implementation of custom DAG file parsing timeout control through the airflow_local_settings.py file. Allows setting different timeout values based on DAG file paths.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/faq.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_dagbag_import_timeout(dag_file_path: str) -> Union[int, float]:\n    \"\"\"\n    This setting allows to dynamically control the DAG file parsing timeout.\n\n    It is useful when there are a few DAG files requiring longer parsing times, while others do not.\n    You can control them separately instead of having one value for all DAG files.\n\n    If the return value is less than or equal to 0, it means no timeout during the DAG parsing.\n    \"\"\"\n    if \"slow\" in dag_file_path:\n        return 90\n    if \"no-timeout\" in dag_file_path:\n        return 0\n    return conf.getfloat(\"core\", \"DAGBAG_IMPORT_TIMEOUT\")\n```\n\n----------------------------------------\n\nTITLE: Checking Current Auth Manager Configuration in Airflow CLI\nDESCRIPTION: Command to check which auth manager is currently configured in the Airflow environment by retrieving the value from the core section of the configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/auth-manager/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ airflow config get-value core auth_manager\nairflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager\n```\n\n----------------------------------------\n\nTITLE: Creating PubSub Topic with PubSubCreateTopicOperator\nDESCRIPTION: Example showing how to create a Google Cloud PubSub topic using the PubSubCreateTopicOperator in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/pubsub.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_topic = PubSubCreateTopicOperator(\n    task_id=\"create_topic\", topic=TOPIC_ID, project_id=PROJECT_ID\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Job Trigger in Google Cloud DLP\nDESCRIPTION: Shows how to update an existing job trigger using CloudDLPUpdateJobTriggerOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/data_loss_prevention.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_dlp_update_job_trigger]\n[END howto_operator_dlp_update_job_trigger]\n```\n\n----------------------------------------\n\nTITLE: Creating a Cloud Composer Environment in Deferrable Mode\nDESCRIPTION: This snippet demonstrates how to create a Cloud Composer environment using the CloudComposerCreateEnvironmentOperator in deferrable mode. It includes additional parameters for deferrable execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncreate_environment_task_deferrable = CloudComposerCreateEnvironmentOperator(\n    task_id=\"create-environment-deferrable-mode\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    environment_name=ENVIRONMENT_NAME,\n    environment=environment_config,\n    deferrable=True,\n    poll_interval=10,\n)\n```\n\n----------------------------------------\n\nTITLE: S3 to SQL Transfer with Generator Parser\nDESCRIPTION: Example demonstrating the use of S3ToSqlOperator with a generator-based parser for memory-efficient processing. The generator approach allows handling large files by processing them row by row instead of loading the entire file into memory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/s3_to_sql.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef s3_to_sql_generator_task():\n    def process_input_generator(input_file):\n        with input_file as file:\n            reader = csv.reader(file)\n            for row in reader:\n                yield row\n\n    task = S3ToSqlOperator(\n        task_id='insert_from_s3_to_db_generator',\n        s3_key='s3://bucket/path/sample.csv',\n        aws_conn_id='s3_default',\n        sql_conn_id='sql_default',\n        table_name='sample_table',\n        parser=process_input_generator,\n    )\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Flink Provider Package for Airflow\nDESCRIPTION: Command to install the Apache Flink provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/flink/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-flink\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL Options for Cassandra Connection in Airflow\nDESCRIPTION: Example JSON configuration for the Extra field to specify SSL options when connecting to an SSL-enabled Cassandra cluster. This configuration passes parameters to ssl.wrap_socket() to establish a secure connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/cassandra/docs/connections/cassandra.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ssl_options\": {\n    \"ca_certs\": \"PATH/TO/CA_CERTS\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Secret Backend in airflow.cfg\nDESCRIPTION: Configuration settings for enabling Google Cloud Secret Manager backend in Airflow's configuration file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/secrets-backends/google-cloud-secret-manager-backend.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend\n```\n\n----------------------------------------\n\nTITLE: Creating IAM Role for Service Account (IRSA) using Terraform\nDESCRIPTION: This Terraform module creates an IAM role and associates it with a Kubernetes service account. It uses the Amazon EKS Blueprints for Terraform module to set up IRSA, allowing fine-grained AWS permissions for pods running on EKS.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_29\n\nLANGUAGE: terraform\nCODE:\n```\nmodule \"airflow_irsa\" {\n  source = \"github.com/aws-ia/terraform-aws-eks-blueprints//modules/irsa\"\n\n  eks_cluster_id             = \"<EKS_CLUSTER_ID>\"\n  eks_oidc_provider_arn      = \"<EKS_CLUSTER_OIDC_PROVIDER_ARN>\"\n  irsa_iam_policies          = [\"<IAM_POLICY_ARN>\"]\n  kubernetes_namespace       = \"<NAMESPACE>\"\n  kubernetes_service_account = \"<SERVICE_ACCOUNT_NAME>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Deferrable Dataflow Flex Template Job with Airflow\nDESCRIPTION: Illustrates running the `DataflowStartFlexTemplateOperator` in deferrable mode (`deferrable=True`). Similar to the Classic Template deferrable operator, this frees up Airflow worker resources while awaiting job initiation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_template.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_start_flex_template_job_deferrable]\n    :end-before: [END howto_operator_start_flex_template_job_deferrable]\n```\n\n----------------------------------------\n\nTITLE: Creating a Google Cloud Run Job Object in Python\nDESCRIPTION: Demonstrates how to create a Google Cloud Run Job object using Python. This object defines the configuration for a Cloud Run job.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_run.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud.run_v2.types import Job\n\njob = Job(\n    launch_stage=\"BETA\",\n    template=Job.ExecutionTemplate(\n        template=Job.ExecutionTemplate.Template(\n            containers=[\n                Job.ExecutionTemplate.Template.Container(\n                    image=\"python:3.9-slim\",\n                    command=[\"python\"],\n                    args=[\"-c\", \"print('Hello World!')\"]\n                )\n            ]\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Connection in Airflow\nDESCRIPTION: The connection configuration is handled through a JSON serializable string in the 'extra' field. It requires at minimum the 'bootstrap.servers' parameter and can include callback functions specified through the 'error_cb' parameter using module path notation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/connections/kafka.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"bootstrap.servers\": \"localhost:9092\",\n  \"error_cb\": \"module.callback_func\"\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Objects in GCS Bucket using Python\nDESCRIPTION: Shows how to use the GCSListObjectsOperator to list objects in a Google Cloud Storage bucket, with options for prefix and delimiter.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlist_task = GCSListObjectsOperator(\n    task_id=\"list_buckets\", bucket=BUCKET_NAME, prefix=SOURCE_PREFIX_TO_COPY\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Loggers for Specific Operators and Hooks in Airflow\nDESCRIPTION: This code demonstrates how to create custom logging configurations for specific operators and hooks by updating the default logging configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/advanced-logging-configuration.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom copy import deepcopy\nfrom pydantic.utils import deep_update\nfrom airflow.config_templates.airflow_local_settings import DEFAULT_LOGGING_CONFIG\n\nLOGGING_CONFIG = deep_update(\n    deepcopy(DEFAULT_LOGGING_CONFIG),\n    {\n        \"loggers\": {\n            \"airflow.task.operators.airflow.providers.common.sql.operators.sql.SQLExecuteQueryOperator\": {\n                \"handlers\": [\"task\"],\n                \"level\": \"DEBUG\",\n                \"propagate\": True,\n            },\n            \"airflow.task.hooks.airflow.providers.http.hooks.http.HttpHook\": {\n                \"handlers\": [\"task\"],\n                \"level\": \"WARNING\",\n                \"propagate\": False,\n            },\n        }\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Getting File via FTP using FTPFileTransmitOperator in Python\nDESCRIPTION: This snippet shows the usage of Airflow's `FTPFileTransmitOperator` to download ('get') a file from a remote FTP server to the local filesystem. It utilizes an existing Airflow FTP connection (`ftp_conn_id`) and defines the `remote_filepath` (source) and `local_filepath` (destination) parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ftp/docs/operators/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../ftp/tests/system/ftp/example_ftp.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_ftp_get]\n    :end-before: [END howto_operator_ftp_get]\n```\n\n----------------------------------------\n\nTITLE: Setup Scope Example\nDESCRIPTION: Demonstrates setup scope with multiple tasks and dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ns1 >> w1 >> w2 >> t1.as_teardown(setups=s1) >> w3\nw2 >> w4\n```\n\n----------------------------------------\n\nTITLE: Deleting Dataproc Metastore Backup using Airflow Operator in Python\nDESCRIPTION: Shows how to use the `DataprocMetastoreDeleteBackupOperator` within an Airflow DAG to delete a specific backup associated with a Dataproc Metastore service. Requires `backup_id`, `service_id`, `location`, and `project_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndelete_backup_service = DataprocMetastoreDeleteBackupOperator(\n    task_id=\"delete_backup_service\",\n    backup_id=BACKUP_ID,\n    service_id=SERVICE_ID,\n    location=REGION,\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Schema Validation for Params\nDESCRIPTION: Shows how to use JSON Schema validation with Param objects, including type constraints, multiple types, enums, and formatting rules.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/params.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    \"my_dag\",\n    params={\n        # an int with a default value\n        \"my_int_param\": Param(10, type=\"integer\", minimum=0, maximum=20),\n\n        # a required param which can be of multiple types\n        # a param must have a default value\n        \"multi_type_param\": Param(5, type=[\"null\", \"number\", \"string\"]),\n\n        # an enum param, must be one of three values\n        \"enum_param\": Param(\"foo\", enum=[\"foo\", \"bar\", 42]),\n\n        # a param which uses json-schema formatting\n        \"email\": Param(\n            default=\"example@example.com\",\n            type=\"string\",\n            format=\"idn-email\",\n            minLength=5,\n            maxLength=255,\n        ),\n    },\n):\n```\n\n----------------------------------------\n\nTITLE: Updating and Applying Parameters to Memcached Instance\nDESCRIPTION: Example of using CloudMemorystoreMemcachedUpdateParametersOperator and CloudMemorystoreMemcachedApplyParametersOperator to update and apply parameters to a Memcached instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore_memcached.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nupdate_parameters = CloudMemorystoreMemcachedUpdateParametersOperator(\n    task_id=\"update-parameters\",\n    location=LOCATION,\n    instance_id=MEMCACHED_INSTANCE_ID,\n    parameters=PARAMETERS,\n    project_id=PROJECT_ID,\n)\n\napply_parameters = CloudMemorystoreMemcachedApplyParametersOperator(\n    task_id=\"apply-parameters\",\n    location=LOCATION,\n    instance_id=MEMCACHED_INSTANCE_ID,\n    node_ids=[\"node-1\"],\n    apply_all=False,\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Dataplex Task - Python\nDESCRIPTION: Demonstrates the use of DataplexDeleteTaskOperator to delete a previously created Dataplex task. Prerequisites include specifying the correct project_id, region, lake_id, and task_id. Output is the status of the deletion operation; operator may raise errors if identifiers are incorrect or the task does not exist.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"delete_task = DataplexDeleteTaskOperator(\\n    task_id=\\\"delete_task\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    lake_id=LAKE_ID,\\n    task_id=TASK_ID,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Looking up a Single Entry in Dataplex Catalog with Python\nDESCRIPTION: This example demonstrates the use of DataplexCatalogLookupEntryOperator to look up a single Entry by name in the Dataplex Catalog, using the permission on the source system. It requires the appropriate Airflow provider and Google Cloud credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_58\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_dataplex_catalog_lookup_entry]\nlookup_entry = DataplexCatalogLookupEntryOperator(\n    task_id=\"lookup_entry\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    entry_id=ENTRY_ID,\n    location=LOCATION,\n)\n# [END howto_operator_dataplex_catalog_lookup_entry]\n```\n\n----------------------------------------\n\nTITLE: Skipping Tests for Lowest Dependencies\nDESCRIPTION: Python code snippet showing how to mark a test to be skipped when running with force lowest dependencies. This is useful for tests that depend on multiple providers or specific configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nfrom tests_common.pytest_plugin import skip_if_force_lowest_dependencies_marker\n\n\n@skip_if_force_lowest_dependencies_marker\ndef test_my_test_that_should_be_skipped():\n    assert 1 == 1\n```\n\n----------------------------------------\n\nTITLE: Listing Google Cloud Tasks Queues in Python\nDESCRIPTION: This snippet shows how to list all Google Cloud Tasks queues using the CloudTasksQueuesListOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# [START list_queue]\nCloudTasksQueuesListOperator(\n    task_id=\"list_queue\",\n    location=LOCATION,\n    results_filter=\"state:RUNNING\",\n).execute(context=context)\n# [END list_queue]\n```\n\n----------------------------------------\n\nTITLE: Adding Version Aliases to Model using Vertex AI Model Service Operator - Python\nDESCRIPTION: Demonstrates how to add aliases to a model version in Vertex AI using AddVersionAliasesOnModelOperator. Requires configuration of model ID, version, and aliases array. Aliases help reference specific versions more conveniently in code and deployment scripts.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nadd_version_aliases_task = AddVersionAliasesOnModelOperator(\n    task_id=\"add_version_aliases_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    model_id=MODEL_ID,\n    version_id=VERSION_ID,\n    version_aliases=[\"alias1\", \"alias2\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Updating DataFusion Instance with CloudDataFusionUpdateInstanceOperator in Python\nDESCRIPTION: This snippet illustrates how to use CloudDataFusionUpdateInstanceOperator to update an existing Google Cloud DataFusion instance. It includes parameters for instance name, update mask, and instance details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datafusion.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nCloudDataFusionUpdateInstanceOperator(\n    task_id=\"update_instance\",\n    instance_name=INSTANCE_NAME,\n    update_mask={\"paths\": [\"labels\"]},\n    instance={\n        \"name\": INSTANCE_NAME,\n        \"labels\": {\"label1\": \"test2\"},\n    },\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    retry=RETRY,\n    timeout=TIMEOUT,\n    metadata=METADATA,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting Azure Service Bus Topic in Python\nDESCRIPTION: This example shows how to use the AzureServiceBusTopicDeleteOperator to delete an Azure Service Bus topic.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/asb.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_delete_service_bus_topic]\n# [END howto_operator_delete_service_bus_topic]\n```\n\n----------------------------------------\n\nTITLE: DAG Run Failure State Listener\nDESCRIPTION: Example implementation of a listener that monitors when a DAG run changes to failed state. This code would be part of the listener.py file referenced in the plugin registration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/listener-plugin.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.example_dags.plugins.event_listener import *\n```\n\n----------------------------------------\n\nTITLE: Using Deferrable FileSensor in Apache Airflow\nDESCRIPTION: This snippet shows how to use the FileSensor in deferrable mode. It allows for asynchronous file detection, which can be more efficient for long-running sensors. The 'deferrable' parameter is set to True to enable this mode.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/file.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nFileSensor(\n    task_id=\"wait_for_file_async\",\n    filepath=\"/tmp/file.txt\",\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting Files from Azure DataLake Storage with ADLSDeleteOperator in Python\nDESCRIPTION: This example shows how to use the ADLSDeleteOperator to remove files from Azure DataLake Storage. It requires the airflow.providers.microsoft.azure.operators.adls module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/adls.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_adls_delete]\n# Code snippet for ADLSDeleteOperator\n# [END howto_operator_adls_delete]\n```\n\n----------------------------------------\n\nTITLE: Creating a New Model Version via Custom Training Job (V2) in VertexAI using Airflow (Python)\nDESCRIPTION: Demonstrates creating a new version of an existing VertexAI Model rather than a new model, by specifying the parent_model parameter in the CreateCustomTrainingJobOperator. Inputs include all standard training parameters plus parent_model. Outputs the updated model as XCom. Useful for updating/rolling versions of production models.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n    create_training_job_v2 = CreateCustomTrainingJobOperator(\n        task_id=\"train_model_v2\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        display_name=DISPLAY_NAME,\n        script_path=SCRIPT_PATH,\n        dataset_id=DATASET_ID,\n        parent_model=PARENT_MODEL,\n        gcp_conn_id=GCP_CONN_ID,\n    )\n```\n\n----------------------------------------\n\nTITLE: Stopping DataFusion Pipeline with CloudDataFusionStopPipelineOperator in Python\nDESCRIPTION: This snippet illustrates the use of CloudDataFusionStopPipelineOperator to stop a running pipeline in Google Cloud DataFusion. It specifies the pipeline name, instance name, and other relevant parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datafusion.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nCloudDataFusionStopPipelineOperator(\n    task_id=\"stop_pipeline\",\n    pipeline_name=PIPELINE_NAME,\n    instance_name=INSTANCE_NAME,\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    namespace=\"default\",\n    retry=RETRY,\n    timeout=TIMEOUT,\n    metadata=METADATA,\n)\n```\n\n----------------------------------------\n\nTITLE: Parallel Setup and Teardown Tasks\nDESCRIPTION: Shows how to configure multiple setup and teardown tasks to run in parallel.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n[\n    [create_cluster, create_bucket]\n    >> run_query\n    >> [delete_cluster.as_teardown(setups=create_cluster), delete_bucket.as_teardown(setups=create_bucket)]\n]\n```\n\n----------------------------------------\n\nTITLE: Using KubernetesInstallKueueOperator in Python\nDESCRIPTION: Example of using KubernetesInstallKueueOperator to install the Kueue component in a Kubernetes cluster. It demonstrates how to configure the operator for Kueue installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nKubernetesInstallKueueOperator(\n    task_id=\"install_kueue\",\n    namespace=\"kueue-system\",\n    release_name=\"kueue\",\n)\n```\n\n----------------------------------------\n\nTITLE: Sensing Vertex AI Feature View Sync Completion using Airflow Sensor in Python\nDESCRIPTION: Shows the usage of `FeatureViewSyncSensor` from `airflow.providers.google.cloud.sensors.vertex_ai` to wait for a Vertex AI Feature View synchronization job to complete successfully. This sensor polls the status of the sync job and succeeds when the job finishes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_56\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/vertex_ai/example_vertex_ai_feature_store.py\n    :language: python\n    :dedent: 4\n    :start-after: [START how_to_cloud_vertex_ai_feature_store_feature_view_sync_sensor]\n    :end-before: [END how_to_cloud_vertex_ai_feature_store_feature_view_sync_sensor]\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Database Connection Secret via Environment Variable in Bash\nDESCRIPTION: This command demonstrates how to set a secret key for retrieving the database connection string using an environment variable in Bash.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-config.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__DATABASE__SQL_ALCHEMY_CONN_SECRET=sql_alchemy_conn\n```\n\n----------------------------------------\n\nTITLE: Template Fields for Google Cloud SQL Query Operator in Python\nDESCRIPTION: This code snippet lists the template fields available for the Google Cloud SQL Query Operator. These fields can be templated using Jinja templating engine in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# [START gcp_sql_query_template_fields]\n# [END gcp_sql_query_template_fields]\n```\n\n----------------------------------------\n\nTITLE: Creating a Hyperparameter Tuning Job using Vertex AI HyperparameterTuningJob Operator - Python\nDESCRIPTION: Shows how to create a Vertex AI hyperparameter tuning job using the CreateHyperparameterTuningJobOperator in Airflow. Requires detailed tuning job configuration and project/location context. The operator returns the tuning job ID in XCom under 'hyperparameter_tuning_job_id'; proper role permissions and job specs are required.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ncreate_hpt_job_task = CreateHyperparameterTuningJobOperator(\n    task_id=\"create_hyperparameter_tuning_job_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    display_name=\"your_hptuning_job_name\",\n    custom_job_spec=YOUR_CUSTOM_JOB_SPEC,\n    max_trial_count=MAX_TRIALS,\n    parallel_trial_count=PARALLEL_TRIALS,\n    study_spec=YOUR_STUDY_SPEC,\n    labels={\"your_label\": \"label_value\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Import Operation Body for CloudSQL (Python)\nDESCRIPTION: Demonstrates how to construct the body of a Cloud SQL import operation when using the CloudSQLImportInstanceOperator in Airflow. The snippet must include required parameters such as import context, file URI, and database type. Expected input is a Python dictionary matching the Google Cloud SQL API structure, and the output is suitable for use as an operator argument in Airflow DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"import_body = {\\n    \\\"importContext\\\": {\\n        \\\"kind\\\": \\\"sql#importContext\\\",\\n        \\\"fileType\\\": \\\"SQL\\\",\\n        \\\"uri\\\": \\\"gs://bucketName/file.sql\\\",\\n        \\\"database\\\": \\\"my_database\\\",\\n        \\\"importUser\\\": \\\"myuser\\\"\\n    }\\n}\"\n```\n\n----------------------------------------\n\nTITLE: Specifying MySQL Connection String for Airflow\nDESCRIPTION: This code snippet shows the recommended format for specifying a MySQL connection string in Airflow using the mysqlclient driver.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-up-database.rst#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nmysql+mysqldb://<user>:<password>@<host>[:<port>]/<dbname>\n```\n\n----------------------------------------\n\nTITLE: Configuring LoadBalancer Service for Airflow Webserver\nDESCRIPTION: Configuration to set up a LoadBalancer service type for the Airflow webserver.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nwebserver:\n  service:\n    type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Including AzureCosmosDocumentSensor Example (Python)\nDESCRIPTION: This reStructuredText directive includes a Python code example demonstrating the usage of `AzureCosmosDocumentSensor`. The referenced example, located in `/../tests/system/microsoft/azure/example_azure_cosmosdb.py`, shows how to configure the sensor within an Airflow DAG to check for a document in a specific Azure Cosmos DB database and collection matching a query. Requires `airflow.providers.microsoft.azure.sensors.cosmos.AzureCosmosDocumentSensor` and a configured Azure Cosmos DB connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/sensors/cosmos_document_sensor.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../tests/system/microsoft/azure/example_azure_cosmosdb.py\n    :language: python\n    :dedent: 4\n    :start-after: [START cosmos_document_sensor]\n    :end-before: [END cosmos_document_sensor]\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Apache Hive to Amazon DynamoDB using HiveToDynamoDBOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the HiveToDynamoDBOperator to transfer data from an Apache Hive table to an Amazon DynamoDB table. It includes the operator configuration and task definition within an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/hive_to_dynamodb.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nhive_to_dynamodb = HiveToDynamoDBOperator(\n    task_id=\"hive_to_dynamodb\",\n    hive_table=\"hive_table\",\n    dynamodb_table=\"dynamodb_table\",\n    partition=\"{{ ds }}\",\n    hql=\"SELECT * FROM hive_table WHERE ds = '{{ ds }}'\",\n    aws_conn_id=\"aws_default\",\n    hiveserver2_conn_id=\"hiveserver2_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Nullable Parameter Configuration in Python\nDESCRIPTION: Example of configuring a parameter that accepts either null or string values, making the field optional.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/params.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nParam(None, type=[\"null\", \"string\"])\n```\n\n----------------------------------------\n\nTITLE: Creating a GCP Transfer Job in Python\nDESCRIPTION: Example of creating a Google Cloud Platform transfer job using the CloudDataTransferServiceCreateJobOperator in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nbody = {\n    \"description\": \"This is an example GCP transfer job\",\n    \"status\": \"ENABLED\",\n    \"projectId\": PROJECT_ID,\n    \"schedule\": {\n        \"scheduleStartDate\": {\n            \"day\": 1,\n            \"month\": 1,\n            \"year\": 2015\n        },\n        \"scheduleEndDate\": {\n            \"day\": 1,\n            \"month\": 1,\n            \"year\": 2030\n        },\n        \"startTimeOfDay\": {\n            \"hours\": 0,\n            \"minutes\": 0,\n            \"seconds\": 0\n        }\n    },\n    \"transferSpec\": {\n        \"gcsDataSource\": {\n            \"bucketName\": GCP_SOURCE_BUCKET\n        },\n        \"gcsDataSink\": {\n            \"bucketName\": GCP_DESTINATION_BUCKET\n        },\n        \"transferOptions\": {\n            \"overwriteObjectsAlreadyExistingInSink\": True,\n            \"deleteObjectsFromSourceAfterTransfer\": False\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow with KEDA Support\nDESCRIPTION: Helm commands to install Apache Airflow with KEDA autoscaling enabled. Configures the CeleryExecutor and enables KEDA worker scaling.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/keda.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create namespace airflow\nhelm repo add apache-airflow https://airflow.apache.org\nhelm install airflow apache-airflow/airflow \\\n    --namespace airflow \\\n    --set executor=CeleryExecutor \\\n    --set workers.keda.enabled=true\n```\n\n----------------------------------------\n\nTITLE: Deleting a Dataplex Lake - Python\nDESCRIPTION: Uses DataplexDeleteLakeOperator to delete an existing Dataplex lake. Inputs include project_id, region, and lake_id; operator initiates lake removal and returns operation status, potentially raising an error if the lake does not exist.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"delete_lake = DataplexDeleteLakeOperator(\\n    task_id=\\\"delete_lake\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    lake_id=LAKE_ID,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: UV Package Synchronization Commands\nDESCRIPTION: Commands for synchronizing project dependencies using uv, including core Airflow dependencies and provider-specific installations.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/07_local_virtualenv.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv sync\nuv sync --package apache-airflow-providers-amazon\nuv sync --all-packages\n```\n\n----------------------------------------\n\nTITLE: Configuring Datadog Agent Integration\nDESCRIPTION: Configuration for enabling Airflow metrics export to Datadog agent.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nstatsd:\n  enabled: false\nconfig:\n  metrics: # or 'scheduler' for Airflow 1\n    statsd_on: true\n    statsd_port: 8125\nextraEnv: |-\n  - name: AIRFLOW__METRICS__STATSD_HOST\n    valueFrom:\n      fieldRef:\n        fieldPath: status.hostIP\n```\n\n----------------------------------------\n\nTITLE: Configuring SendGrid SMTP Settings (INI)\nDESCRIPTION: This snippet shows how to configure SMTP settings for SendGrid in the Airflow configuration file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\n[smtp]\nsmtp_host=smtp.sendgrid.net\nsmtp_starttls=False\nsmtp_ssl=False\nsmtp_port=587\nsmtp_mail_from=<your-from-email>\n```\n\n----------------------------------------\n\nTITLE: Parsing Airflow Connection URIs\nDESCRIPTION: Example showing how to parse connection URI strings into Connection objects to verify proper formatting.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> from airflow.sdk import Connection\n\n>>> c = Connection(uri=\"my-conn-type://my-login:my-password@my-host:5432/my-schema?param1=val1&param2=val2\")\n>>> print(c.login)\nmy-login\n>>> print(c.password)\nmy-password\n```\n\n----------------------------------------\n\nTITLE: Storing Airflow Connection in AWS Secrets Manager\nDESCRIPTION: AWS CLI command to store an Airflow connection as a secret in AWS Secrets Manager. The secret is stored as a JSON string containing login, password, host, and port information.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-secrets-manager.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naws secretsmanager put-secret-value \\\n    --secret-id airflow/connections/smtp_default \\\n    --secret-string '{\"login\": \"nice_user\", \"password\": \"this_is_the_password\", \"host\": \"ec2.8399.com\", \"port\": \"999\"}'\n```\n\n----------------------------------------\n\nTITLE: Creating an Index in OpenSearch using Apache Airflow\nDESCRIPTION: This snippet demonstrates how to use the OpenSearchCreateIndexOperator to create a new index in an OpenSearch domain. It requires the OpenSearch provider for Airflow and proper configuration of the OpenSearch connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/operators/opensearch.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_opensearch_create_index]\n# [END howto_operator_opensearch_create_index]\n```\n\n----------------------------------------\n\nTITLE: Fetching Data from BigQuery Table - Python\nDESCRIPTION: Example of using BigQueryGetDataOperator to fetch data from a BigQuery table.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nget_data = BigQueryGetDataOperator(\n    task_id=\"get_data_from_bq\",\n    dataset_id=DATASET_NAME,\n    table_id=TABLE_NAME,\n    max_results=10,\n    selected_fields=\"value\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Python and Airflow with Homebrew\nDESCRIPTION: This script demonstrates how to install Python using Homebrew, create a virtual environment, and install Apache Airflow to resolve issues related to incompatible Python versions or missing dylib files.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-pypi.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# Note: these instructions are for python3.9 but can be loosely modified for other versions\nbrew install python@3.9\nvirtualenv -p /usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/bin/python3 .toy-venv\nsource .toy-venv/bin/activate\npip install apache-airflow\npython\n>>> import setproctitle\n# Success!\n```\n\n----------------------------------------\n\nTITLE: Migrating Basic Auth Backend Import Path in Python\nDESCRIPTION: Update instructions for changing the deprecated basic authentication backend import path to the new provider-based path. The old path 'airflow.api.auth.backend.basic_auth' should be replaced with 'airflow.providers.fab.auth_manager.api.auth.backend.basic_auth'.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41663.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old import path (deprecated)\nairflow.api.auth.backend.basic_auth\n\n# New import path\nairflow.providers.fab.auth_manager.api.auth.backend.basic_auth\n```\n\n----------------------------------------\n\nTITLE: Configuring External Database in Helm Values\nDESCRIPTION: YAML configuration to disable default Postgres and configure external database credentials in values file.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npostgresql:\n  enabled: false\n\ndata:\n  metadataConnection:\n    user: <username>\n    pass: <password>\n    protocol: postgresql\n    host: <hostname>\n    port: 5432\n    db: <database name>\n```\n\n----------------------------------------\n\nTITLE: Submitting a Google Cloud Batch Job in Deferrable Mode\nDESCRIPTION: Shows how to use the CloudBatchSubmitJobOperator in deferrable mode to submit a job to Google Cloud Batch. This version of the operator is suitable for asynchronous execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_batch.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsubmit_job_async = CloudBatchSubmitJobOperator(\n    task_id=\"submit_job_async\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    job=job,\n    gcp_conn_id=GCP_CONN_ID,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Apache Kafka Consumer Group with ManagedKafkaUpdateConsumerGroupOperator in Python\nDESCRIPTION: This code shows how to update an Apache Kafka consumer group using the ManagedKafkaUpdateConsumerGroupOperator. It requires the project ID, region, cluster, consumer group, and update mask.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nupdate_consumer_group = ManagedKafkaUpdateConsumerGroupOperator(\n    task_id=\"update_consumer_group\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n    consumer_group=CONSUMER_GROUP,\n    update_mask={\"paths\": [\"some_field\"]},\n)\n```\n\n----------------------------------------\n\nTITLE: Running Database Migration for Apache Airflow\nDESCRIPTION: This command creates the database schema if it doesn't exist or migrates it to the latest version. It's important to ensure that Airflow components are not running during migration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/setting-up-the-database.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairflow db migrate\n```\n\n----------------------------------------\n\nTITLE: Setting Docker Cache to Local via Environment Variable\nDESCRIPTION: Setting an environment variable to use local Docker caching for image builds. This affects all subsequent Breeze commands.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_CACHE=\"local\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Salesforce Connection via Environment Variable in Bash\nDESCRIPTION: This Bash command demonstrates how to configure the default Salesforce connection (`SALESFORCE_DEFAULT`) using an environment variable. It follows the standard URI format for database connections, encoding necessary components like the host URL and including credentials (username, password) and security token as part of the URI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/connections/salesforce.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SALESFORCE_DEFAULT='http://your_username:your_password@https%3A%2F%2Fyour_host.lightning.force.com?security_token=your_token'\n```\n\n----------------------------------------\n\nTITLE: Disabling Source Code Collection for OpenLineage in airflow.cfg\nDESCRIPTION: Configuration to prevent OpenLineage from collecting source code from operators like Python and Bash, which is included by default in events.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_13\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\ndisable_source_code = true\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Client with Extra Parameters via JSON in Apache Airflow\nDESCRIPTION: This JSON snippet demonstrates how to structure the 'extra' field in an Airflow connection to pass custom parameters, such as timeout and api_key, to the OpenAI client upon instantiation. The key 'openai_client_kwargs' holds client options as key-value pairs. Required dependencies include Apache Airflow and openai-python; this configuration is intended to be placed inside the Airflow connection's 'extra' JSON field. Parameters like 'timeout' control connection behavior; 'api_key' provides alternate API credentials, if desired.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openai/docs/connections.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"openai_client_kwargs\": {\n    \"timeout\": 10,\n    \"api_key\": \"YOUR_API_KEY\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a GKE Cluster with GKECreateClusterOperator\nDESCRIPTION: Example of using the GKECreateClusterOperator to create a Google Kubernetes Engine cluster based on the provided cluster definition.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster = GKECreateClusterOperator(\n    task_id=\"create_cluster\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    body=CLUSTER,\n)\n```\n\n----------------------------------------\n\nTITLE: Moving Specific Files from GCS to SFTP using Wildcards in Python\nDESCRIPTION: This example shows how to move specific files from Google Cloud Storage to SFTP by using wildcards in the source_path parameter. The destination_path defines the path prefix for all copied files, and move_object=True ensures the files are deleted from GCS after copying.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_sftp.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmove_specific_files_from_gcs_to_sftp = GCSToSFTPOperator(\n    task_id=\"move-specific-files-from-gcs-to-sftp\",\n    source_bucket=BUCKET_NAME,\n    source_object=GCS_SRC_PATH + \"/*.txt\",\n    destination_path=SFTP_DST_PATH,\n    move_object=True,\n    sftp_conn_id=\"sftp_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Connection Extras with SSL\nDESCRIPTION: JSON configuration example for MySQL connection extras, including charset, cursor type, UNIX socket, and SSL certificate parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/connections/mysql.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"charset\": \"utf8\",\n   \"cursor\": \"sscursor\",\n   \"unix_socket\": \"/var/socket\",\n   \"ssl\": {\n     \"cert\": \"/tmp/client-cert.pem\",\n     \"ca\": \"/tmp/server-ca.pem\",\n     \"key\": \"/tmp/client-key.pem\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow FTP Connection via Environment Variable (Bash)\nDESCRIPTION: This snippet demonstrates how to configure the default FTP connection (`ftp_default`) in Apache Airflow using an environment variable (`AIRFLOW_CONN_FTP_DEFAULT`). It uses URI syntax, specifying the user ('user'), password ('pass'), host ('localhost'), and disables passive mode ('passive=false'). All components of the URI must be URL-encoded.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ftp/docs/connections/ftp.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_FTP_DEFAULT='ftp://user:pass@localhost?passive=false'\n```\n\n----------------------------------------\n\nTITLE: Creating __init__.py for Custom Airflow Operators Package\nDESCRIPTION: Provides an example of the content for the __init__.py file in a custom Airflow operators package, which prints a message when imported.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/modules_management.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Hello from airflow_operators\")\n```\n\n----------------------------------------\n\nTITLE: Defining Dataproc Metastore Metadata Import Configuration in Python\nDESCRIPTION: Illustrates creating a Python dictionary that defines a metadata import operation for Dataproc Metastore. This configuration specifies details like the name of the import and the GCS path to the database dump (`database_dump`). It is used by the `DataprocMetastoreCreateMetadataImportOperator`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nMETADATA_IMPORT = {\n    \"name\": METADATA_IMPORT_ID,\n    \"database_dump\": {\n        \"database_type\": \"MYSQL\",\n        \"gcs_uri\": GCS_PATH,\n        \"type\": \"MYSQL\",\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Installing PagerDuty Provider with Cross-provider Dependencies\nDESCRIPTION: Command to install the PagerDuty provider package with additional common compatibility dependencies. This ensures all features of the package can be used properly.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pagerduty/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-pagerduty[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Running MongoDB Integration Test in Breeze\nDESCRIPTION: Command to start a Breeze shell with MongoDB integration enabled for testing\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/integration_tests.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze --integration mongo\n```\n\n----------------------------------------\n\nTITLE: Installing Providers with Specific Airflow Version Check (Bash)\nDESCRIPTION: Installs provider distributions from the `dist` folder while checking compatibility against a specified older Airflow version (e.g., 2.4.0) using the `--use-airflow-version` flag with the `breeze release-management install-provider-distributions` command.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management install-provider-distributions --use-airflow-version 2.4.0\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Task Decorator in Provider's get_provider_info Function\nDESCRIPTION: This snippet shows how to register a custom task decorator in the get_provider_info function of an Airflow provider package. It adds a 'task-decorators' key-value pair to the returned dictionary.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/create-custom-decorator.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_provider_info():\n    return {\n        \"package-name\": \"foo-provider-airflow\",\n        \"name\": \"Foo\",\n        \"task-decorators\": [\n            {\n                \"name\": \"foo\",\n                # \"Import path\" and function name of the `foo_task`\n                \"class-name\": \"name.of.python.package.foo_task\",\n            }\n        ],\n        # ...\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring Console Transport for OpenLineage in airflow.cfg\nDESCRIPTION: Configuration for setting up a console transport in airflow.cfg, which outputs OpenLineage events to task logs rather than sending them to a backend.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"console\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Hook Lineage Reader Plugin\nDESCRIPTION: Example demonstrating how to create a custom HookLineageReader and register it as an Airflow plugin to access collected lineage data.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/lineage.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.lineage.hook_lineage import HookLineageReader\nfrom airflow.plugins_manager import AirflowPlugin\n\n\nclass CustomHookLineageReader(HookLineageReader):\n    def get_inputs(self):\n        return self.lineage_collector.collected_assets.inputs\n\n\nclass HookLineageCollectionPlugin(AirflowPlugin):\n    name = \"HookLineageCollectionPlugin\"\n    hook_lineage_readers = [CustomHookLineageReader]\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Filesystem (local://) for Airflow XComs (INI)\nDESCRIPTION: This configuration snippet provides an alternative way to configure Airflow to use the local filesystem for XComs using the `local://` scheme, which is handled by `fsspec` and is functionally equivalent to `file://`. It requires setting the `xcom_backend` and specifying the local path in `xcom_objectstorage_path`. A `xcom_objectstorage_threshold` greater than -1 is necessary for the object storage backend to activate.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/io/docs/xcom_backend.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nxcom_backend = airflow.providers.common.io.xcom.backend.XComObjectStorageBackend\n\n[common.io]\nxcom_objectstorage_path = local://airflow/xcoms\n```\n\n----------------------------------------\n\nTITLE: AQLOperator with Template File Example\nDESCRIPTION: Demonstrates how to use AQLOperator with a template file for loading queries. The template file path is relative to the dags/ folder unless a custom template_searchpath is specified in the DAG configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/operators/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlist_students = AQLOperator(\n    task_id=\"list_students_in_collection\",\n    arangodb_conn_id=\"arangodb_default\",\n    template_file=\"template_file.sql\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataplex Catalog Entry Group with Airflow Operator\nDESCRIPTION: Uses the DataplexCatalogCreateEntryGroupOperator to create a Dataplex Catalog Entry Group. This operator creates an entry group in Google Cloud Dataplex Catalog with the specified configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ncreate_entry_group = DataplexCatalogCreateEntryGroupOperator(\n    task_id=\"create_entry_group\",\n    project_id=PROJECT_ID,\n    location=REGION,\n    entry_group_id=ENTRY_GROUP_ID,\n    entry_group=ENTRY_GROUP,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFusion Instance with CloudDataFusionCreateInstanceOperator in Python\nDESCRIPTION: This code demonstrates the use of CloudDataFusionCreateInstanceOperator to create a new Google Cloud DataFusion instance. It specifies various parameters including instance name, project ID, location, instance type, and options.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datafusion.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nCloudDataFusionCreateInstanceOperator(\n    task_id=\"create_instance\",\n    instance_name=INSTANCE_NAME,\n    instance={\n        \"type\": InstanceType.BASIC,\n        \"displayName\": INSTANCE_NAME,\n        \"labels\": {\"label1\": \"test\"},\n        \"options\": {\"jupyterlab-conda-extension\": \"true\"},\n    },\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    retry=RETRY,\n    timeout=TIMEOUT,\n    metadata=METADATA,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataproc Metastore Metadata Import using Airflow Operator in Python\nDESCRIPTION: Shows the usage of `DataprocMetastoreCreateMetadataImportOperator` in an Airflow DAG. This operator initiates a metadata import job into a specified Dataproc Metastore service using the provided configuration. Requires `metadata_import` dictionary, `service_id`, `location`, `project_id`, and `metadata_import_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncreate_metadata_import = DataprocMetastoreCreateMetadataImportOperator(\n    task_id=\"create_metadata_import\",\n    metadata_import=METADATA_IMPORT,\n    service_id=SERVICE_ID,\n    location=REGION,\n    project_id=PROJECT_ID,\n    metadata_import_id=METADATA_IMPORT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Apache Hadoop Stack for Airflow\nDESCRIPTION: This command builds a custom Airflow Docker image with the Apache Hadoop Stack installed. The image is based on a specified Airflow version and tagged for later use.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/recipes.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . \\\n  --pull \\\n  --build-arg BASE_AIRFLOW_IMAGE=\"apache/airflow:2.0.2\" \\\n  --tag my-airflow-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Executing a Google Cloud Run Job in Deferrable Mode using CloudRunExecuteJobOperator in Airflow\nDESCRIPTION: Shows how to use the CloudRunExecuteJobOperator in deferrable mode to execute a Cloud Run job in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_run.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nexecute_job_async = CloudRunExecuteJobOperator(\n    task_id=\"execute_job_async\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    job_id=JOB_NAME,\n    deferrable=True\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a DLP Job Trigger with Airflow Operator - Python\nDESCRIPTION: This snippet demonstrates how to use the CloudDLPDeleteJobTriggerOperator in an Airflow DAG to delete a Data Loss Prevention (DLP) job trigger on Google Cloud. It requires the airflow.providers.google.cloud.operators.cloud.dlp.CloudDLPDeleteJobTriggerOperator and a configured Airflow environment with Google credentials. The main parameter is the name of the DLP job trigger to remove. The operator executes synchronously and returns result status or error upon completion. Inputs include Airflow context and Google Cloud connection; outputs indicate deletion outcome.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/data_loss_prevention.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    'example_gcp_dlp_delete_job_trigger',\n    default_args=default_args,\n    schedule_interval=None,  # Set your schedule interval\n    catchup=False,\n) as dag:\n    delete_job_trigger = CloudDLPDeleteJobTriggerOperator(\n        task_id='delete_job_trigger',\n        project_id=PROJECT_ID,\n        job_trigger_id=JOB_TRIGGER_ID,\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Product Instance Creation for Vision API\nDESCRIPTION: Creates a Product instance for use with Google Cloud Vision API\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_vision_product]\\n[END howto_operator_vision_product]\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with aiobotocore Extra\nDESCRIPTION: This command installs Apache Airflow with the aiobotocore extra, which enables support for asynchronous (deferrable) operators for Amazon integration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[aiobotocore]'\n```\n\n----------------------------------------\n\nTITLE: Using TableauOperator in Airflow DAG (Python)\nDESCRIPTION: This Python snippet demonstrates the usage of the TableauOperator within an Airflow DAG to interact with Tableau server resources (such as datasources or workbooks). The operator is configured with parameters like resource, method, find, and tableau_conn_id to specify what action is to be taken on which Tableau object. Requires the 'apache-airflow-providers-tableau' package and a valid Tableau connection configured in Airflow. Inputs are operator parameters defining the resource and action. The output is the completion of the specified Tableau action, with optional blocking refresh support.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/tableau/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example usage of TableauOperator in an Airflow DAG\nfrom airflow import DAG\nfrom airflow.providers.tableau.operators.tableau import TableauOperator\nfrom datetime import datetime\n\nwith DAG(dag_id=\"example_tableau\", start_date=datetime(2023, 1, 1), schedule_interval=None, catchup=False) as dag:\n    refresh_workbook = TableauOperator(\n        task_id=\"refresh_workbook\",\n        resource=\"workbooks\",\n        method=\"refresh\",\n        find=\"SampleWorkbook\",\n        match_with=\"name\",\n        site_id=None,\n        blocking_refresh=True,\n        check_interval=20.0,\n        tableau_conn_id=\"tableau_default\"\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Variables via Environment Variables in Bash\nDESCRIPTION: Shows how to set Airflow variables using environment variables. Variables can be stored as plain strings or JSON strings. The naming convention requires AIRFLOW_VAR_ prefix followed by the variable name in uppercase.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/variable.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_VAR_FOO=BAR\n\n# To use JSON, store them as JSON strings\nexport AIRFLOW_VAR_FOO_BAZ='{\"hello\":\"world\"}'\n```\n\n----------------------------------------\n\nTITLE: Enabling Pre-push Git Hooks\nDESCRIPTION: Command to install and enable pre-commit hooks for git push operations.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install -t pre-push\n```\n\n----------------------------------------\n\nTITLE: Download DV360 Line Items - Python Example\nDESCRIPTION: Shows how to download line items in CSV format using GoogleDisplayVideo360DownloadLineItemsOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/display_video.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n[START howto_google_display_video_download_line_items_operator]\n[END howto_google_display_video_download_line_items_operator]\n```\n\n----------------------------------------\n\nTITLE: Disabling OpenLineage for Specific Airflow Operators via Environment Variable\nDESCRIPTION: Sets the `AIRFLOW__OPENLINEAGE__DISABLED_FOR_OPERATORS` environment variable to disable OpenLineage event emission for the specified operators using a semicolon-separated list. This provides an alternative method to the INI configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_16\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__DISABLED_FOR_OPERATORS='airflow.providers.standard.operators.bash.BashOperator;airflow.providers.standard.operators.python.PythonOperator'\n```\n\n----------------------------------------\n\nTITLE: Configuring a Dataplex Data Quality Scan - Python\nDESCRIPTION: Defines the configuration payload for creating or updating a Dataplex Data Quality scan, specifying required rules and datasets. This dictionary is used as input to the create or update operator. Field requirements are as per the Dataplex API.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"data_quality_scan_config = {\\n    'name': 'sample-dq-scan',\\n    'data': {\\n        'entity': 'sample-entity',\\n    },\\n    'rules': [\\n        # List of data quality rules\\n    ],\\n    # Additional configuration fields...\\n}\"\n```\n\n----------------------------------------\n\nTITLE: Monitoring QuickSight Ingestion Status with Airflow Sensor\nDESCRIPTION: Example demonstrating the use of QuickSightSensor to monitor the status of a QuickSight ingestion until it reaches a terminal state. The sensor helps ensure data ingestion is completed successfully.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/quicksight.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nquicksight_sensor = QuickSightSensor(\n    task_id=\"check_quicksight_status\",\n    aws_conn_id=\"aws_default\",\n    data_set_id=\"{{ data.PlaceholderDatasetId }}\",\n    ingestion_id=\"{{ data.PlaceholderId }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Stopping a Compute Engine Instance with ComputeEngineStopInstanceOperator in Python\nDESCRIPTION: Stops a Google Compute Engine instance using the ComputeEngineStopInstanceOperator. This snippet demonstrates how to use the operator with and without specifying a project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineStopInstanceOperator(\n    project_id=GCP_PROJECT_ID,\n    zone=GCE_ZONE,\n    resource_id=INSTANCE_NAME,\n    task_id=\"gcp_compute_stop_instance\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineStopInstanceOperator(\n    zone=GCE_ZONE,\n    resource_id=INSTANCE_NAME,\n    task_id=\"gcp_compute_stop_instance\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataproc Batch with Persistent History Server in Python\nDESCRIPTION: This snippet demonstrates using the `DataprocCreateBatchOperator` to create a Dataproc Batch job configured to use a Persistent History Server (PHS). This requires a pre-configured Dataproc cluster (see previous snippet) and specific configuration within the `batch` parameter to link to the PHS cluster.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# Code extracted from: /../../google/tests/system/google/cloud/dataproc/example_dataproc_batch_persistent.py\n# Between markers: [START how_to_cloud_dataproc_create_batch_operator_with_persistent_history_server] and [END how_to_cloud_dataproc_create_batch_operator_with_persistent_history_server]\n# \n# Example using DataprocCreateBatchOperator with PHS configuration\n# ... (actual Python code would be here)\n\n```\n\n----------------------------------------\n\nTITLE: Complete Airflow DAG using S3ToTeradataOperator in Python\nDESCRIPTION: This Python snippet provides a complete example of an Apache Airflow DAG that incorporates the `S3ToTeradataOperator` to demonstrate a full workflow involving data transfer from S3 to Teradata.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/operators/s3_to_teradata.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../teradata/tests/system/teradata/example_s3_to_teradata_transfer.py\n    :language: python\n    :start-after: [START s3_to_teradata_transfer_operator_howto_guide]\n    :end-before: [END s3_to_teradata_transfer_operator_howto_guide]\n```\n\n----------------------------------------\n\nTITLE: Deleting an Amazon EMR Serverless Application with Airflow - Python\nDESCRIPTION: Describes the process for deleting an EMR Serverless Application using EmrServerlessDeleteApplicationOperator in Airflow. The example depends on airflow.providers.amazon.aws and (optionally) aiobotocore for deferrable operation. The key input is application_id from a prior step, and deletion is final. Ensure that the application is not needed before deletion and proper IAM permissions are present.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr_serverless.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.amazon.aws.operators.emr import EmrServerlessDeleteApplicationOperator\n\ndelete_emr_serverless_app = EmrServerlessDeleteApplicationOperator(\n    task_id=\"delete_emr_serverless_application\",\n    application_id=\"{{ task_instance.xcom_pull(task_ids='create_emr_serverless_application') }}\",\n    deferrable=True,  # Optional\n    dag=dag,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring a Presto Job for Google Cloud Dataproc\nDESCRIPTION: This code defines a configuration for a Presto job to be submitted to a Dataproc cluster. It specifies query file and client output format for the Presto execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nPRESTO_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"presto_job\": {\n        \"query_file_uri\": f\"gs://{BUCKET_NAME}/{PRESTO_SCRIPT}\",\n        \"client_output_format\": f\"TSV\",\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Amazon Glacier Job with GlacierCreateJobOperator\nDESCRIPTION: Example demonstrating how to initiate an Amazon Glacier inventory retrieval job using the GlacierCreateJobOperator. The operator returns job information including jobId for tracking.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/glacier.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_job = GlacierCreateJobOperator(\n    task_id=\"create_glacier_job\",\n    aws_conn_id=\"aws_default\",\n    vault_name=\"airflow\",\n    account_id=\"-\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS IAM Authentication for Redshift in Airflow (JSON)\nDESCRIPTION: This JSON snippet illustrates configuring the 'Extra' field for an Airflow PostgreSQL connection to use AWS IAM database authentication specifically for Amazon Redshift. It enables IAM ('iam': true), specifies the AWS connection ('aws_conn_id'), indicates Redshift usage ('redshift': true - note: source has '/tmp/server-ca.pem' which might be a typo), and provides the Redshift cluster identifier.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/connections/postgres.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"iam\": true,\n   \"aws_conn_id\": \"aws_awesome_redshift_conn\",\n   \"redshift\": \"/tmp/server-ca.pem\",\n   \"cluster-identifier\": \"awesome-redshift-identifier\"\n}\n```\n\n----------------------------------------\n\nTITLE: Mocking Airflow Connections in Tests\nDESCRIPTION: Demonstrates mocking Airflow connections in unit tests by setting environment variables with connection URIs. This approach allows testing code that uses connections without requiring database access.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nconn = Connection(\n    conn_type=\"gcpssh\",\n    login=\"cat\",\n    host=\"conn-host\",\n)\nconn_uri = conn.get_uri()\nwith mock.patch.dict(\"os.environ\", AIRFLOW_CONN_MY_CONN=conn_uri):\n    assert \"cat\" == Connection.get_connection_from_secrets(\"my_conn\").login\n```\n\n----------------------------------------\n\nTITLE: Configuring GitDagBundle Environment Variable in Bash\nDESCRIPTION: Example showing how to set up the DAG_BUNDLE_CONFIG_LIST environment variable for Git bundle configuration in Airflow. The configuration includes bundle name, classpath, and parameters like subdirectory, tracking reference, and refresh interval.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/git/docs/bundles/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__DAG_PROCESSOR__DAG_BUNDLE_CONFIG_LIST='[\n {\n     \"name\": \"my-git-repo\",\n     \"classpath\": \"airflow.providers.git.bundles.git.GitDagBundle\",\n     \"kwargs\": {\n         \"subdir\": \"dags\",\n         \"tracking_ref\": \"main\",\n         \"refresh_interval\": 3600\n     }\n }\n]'\n```\n\n----------------------------------------\n\nTITLE: Updating a Google Cloud Dataproc Cluster with Python\nDESCRIPTION: This code uses the DataprocUpdateClusterOperator to update an existing Dataproc cluster with new specifications and configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nupdate_cluster = DataprocUpdateClusterOperator(\n    task_id=\"update_cluster\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster_name=CLUSTER_NAME,\n    cluster=CLUSTER_CONFIG,\n    update_mask=UPDATE_MASK,\n    graceful_decommission_timeout={\n        \"seconds\": GRACEFUL_DECOMISSION_TIMEOUT,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading dbt Cloud Job Run Artifacts in Python\nDESCRIPTION: This example shows how to use the DbtCloudGetJobRunArtifactOperator to download artifacts generated by a dbt Cloud job run. The path should be rooted at the target/ directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/operators.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndbt_cloud_get_artifact = DbtCloudGetJobRunArtifactOperator(\n    task_id=\"dbt_cloud_get_artifact\",\n    run_id=dbt_cloud_run_job_async.output[\"id\"],\n    path=\"manifest.json\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom AWS Session Factory for Federation (Python)\nDESCRIPTION: This Python code provides an example of a custom AWS Session Factory (`MyCustomSessionFactory`) extending Airflow's `BaseSessionFactory`. It demonstrates overriding `_create_basic_session` to handle a custom federation scenario. It checks for a 'federation' key in the connection's extra config and uses a helper function (`get_federated_aws_credentials`) to retrieve temporary AWS credentials using the provided username and password before creating the Boto3 session.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef get_federated_aws_credentials(username: str, password: str):\n    \"\"\"\n    Mock interaction with federation endpoint/process and returns AWS credentials.\n    \"\"\"\n    return {\n        \"Version\": 1,\n        \"AccessKeyId\": \"key\",\n        \"SecretAccessKey\": \"secret\",\n        \"SessionToken\": \"token\",\n        \"Expiration\": \"2050-12-31T00:00:00.000Z\",\n    }\n\n\nclass MyCustomSessionFactory(BaseSessionFactory):\n    @property\n    def federated(self):\n        return \"federation\" in self.extra_config\n\n    def _create_basic_session(self, session_kwargs: dict[str, Any]) -> boto3.session.Session:\n        if self.federated:\n            return self._create_federated_session(session_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Basic Setup and Teardown Task Dependencies\nDESCRIPTION: Shows how to create basic dependencies between tasks without setup/teardown functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster >> run_query >> delete_cluster\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenLineage Debug Mode via Environment Variable\nDESCRIPTION: Sets the `AIRFLOW__OPENLINEAGE__DEBUG_MODE` environment variable to `true` to enable OpenLineage debug mode. This provides an alternative way to activate detailed logging and event information for troubleshooting.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_24\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__DEBUG_MODE=true\n```\n\n----------------------------------------\n\nTITLE: Thread-safety Fix for JdbcHook get_conn Method\nDESCRIPTION: Bug fix that made the get_conn method in JdbcHook threadsafe to avoid OSError when the JVM is already started. Resolves issues with concurrent connection creation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nMade get_conn in JdbcHook threadsafe to avoid OSError: JVM is already started (#44718)\n```\n\n----------------------------------------\n\nTITLE: Setting ODBC Connection URI via Environment Variable\nDESCRIPTION: Example of setting up an ODBC connection string as an environment variable for MS SQL Server connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/connections/odbc.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_MSSQL_DEFAULT='mssql-odbc://my_user:XXXXXXXXXXXX@1.1.1.1:1433/my_database?Driver=ODBC+Driver+18+for+SQL+Server&ApplicationIntent=ReadOnly&TrustedConnection=Yes'\n```\n\n----------------------------------------\n\nTITLE: Deleting Dataform Repository in Python\nDESCRIPTION: This snippet demonstrates how to delete a repository using the DataformDeleteRepositoryOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_delete_repository]\n# [END howto_operator_delete_repository]\n```\n\n----------------------------------------\n\nTITLE: Creating Long-Running Test DAG in Python\nDESCRIPTION: Example DAG implementation with a long-running task that sleeps for 10 minutes, used to test task instance heartbeat timeout functionality. Uses the BashOperator to execute a sleep command.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/tasks.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import dag\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom datetime import datetime\n\n\n@dag(start_date=datetime(2021, 1, 1), schedule=\"@once\", catchup=False)\ndef sleep_dag():\n    t1 = BashOperator(\n        task_id=\"sleep_10_minutes\",\n        bash_command=\"sleep 600\",\n    )\n\n\nsleep_dag()\n```\n\n----------------------------------------\n\nTITLE: Example DAG File Structure with External DAG Creation\nDESCRIPTION: Demonstrates DAG file organization pattern where the DAG definition is split between a main file and a loader module.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/faq.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dag_loader import create_dag\n\nglobals()[dag.dag_id] = create_dag(dag_id, schedule, dag_number, default_args)\n```\n\n----------------------------------------\n\nTITLE: Monitoring Amazon Bedrock Customize Model Job with Python\nDESCRIPTION: This example shows how to use the BedrockCustomizeModelCompletedSensor to wait for an Amazon Bedrock customize model job to reach a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_sensor_customize_model]\n# Example code not provided in the original text\n# [END howto_sensor_customize_model]\n```\n\n----------------------------------------\n\nTITLE: Installing NPM Packages in Dataform Workspace in Python\nDESCRIPTION: This snippet shows how to install npm packages for a specified workspace using the DataformInstallNpmPackagesOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_install_npm_packages]\n# [END howto_operator_install_npm_packages]\n```\n\n----------------------------------------\n\nTITLE: Setting OpenLineage Transport Using Environment Variable\nDESCRIPTION: Example of configuring OpenLineage transport using an environment variable, which is an alternative to using the airflow.cfg file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__TRANSPORT='{\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}'\n```\n\n----------------------------------------\n\nTITLE: Deleting Dataform Workspace in Python\nDESCRIPTION: This snippet shows how to delete a workspace using the DataformDeleteWorkspaceOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_delete_workspace]\n# [END howto_operator_delete_workspace]\n```\n\n----------------------------------------\n\nTITLE: Starting DataFusion Pipeline with CloudDataFusionStartPipelineOperator in Python\nDESCRIPTION: This code shows how to use CloudDataFusionStartPipelineOperator to start a pipeline in Google Cloud DataFusion. It includes options for synchronous and asynchronous execution, as well as deferrable mode.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datafusion.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nCloudDataFusionStartPipelineOperator(\n    task_id=\"start_pipeline\",\n    pipeline_name=PIPELINE_NAME,\n    instance_name=INSTANCE_NAME,\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    namespace=\"default\",\n    runtime_args={\"arg1\": \"a\", \"arg2\": \"b\"},\n    retry=RETRY,\n    timeout=TIMEOUT,\n    metadata=METADATA,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nCloudDataFusionStartPipelineOperator(\n    task_id=\"start_pipeline_async\",\n    pipeline_name=PIPELINE_NAME,\n    instance_name=INSTANCE_NAME,\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    namespace=\"default\",\n    runtime_args={\"arg1\": \"a\", \"arg2\": \"b\"},\n    asynchronous=True,\n    retry=RETRY,\n    timeout=TIMEOUT,\n    metadata=METADATA,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nCloudDataFusionStartPipelineOperator(\n    task_id=\"start_pipeline_def\",\n    pipeline_name=PIPELINE_NAME,\n    instance_name=INSTANCE_NAME,\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    namespace=\"default\",\n    runtime_args={\"arg1\": \"a\", \"arg2\": \"b\"},\n    deferrable=True,\n    retry=RETRY,\n    timeout=TIMEOUT,\n    metadata=METADATA,\n)\n```\n\n----------------------------------------\n\nTITLE: Using Google Cloud Secret Manager for SSL Certificates in Python\nDESCRIPTION: This snippet shows how to use Google Cloud Secret Manager to store SSL certificates for the Cloud SQL Query Operator. It demonstrates the format for storing the certificates and how to reference them using a secret ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n{\"sslcert\": \"\", \"sslkey\": \"\", \"sslrootcert\": \"\"}\n```\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_cloudsql_query_operators_ssl_secret_id]\n# [END howto_operator_cloudsql_query_operators_ssl_secret_id]\n```\n\n----------------------------------------\n\nTITLE: RDS Export Task Status Sensor\nDESCRIPTION: Monitors Amazon RDS snapshot export task status using RdsExportTaskExistenceSensor. Default status is 'available'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nwait_for_export = RdsExportTaskExistenceSensor(\n    task_id=\"wait_for_export\",\n    export_task_identifier=EXPORT_TASK_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from FTP to Amazon S3 using FTPToS3Operator in Python\nDESCRIPTION: This code snippet demonstrates how to use the FTPToS3Operator in Apache Airflow to transfer data from an FTP server to an Amazon S3 bucket. It includes the operator configuration with necessary parameters such as task_id, ftp_path, s3_bucket, and s3_key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/ftp_to_s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nFTPToS3Operator(\n    task_id=\"ftp_to_s3_task\",\n    ftp_path=\"/path/to/ftp/file.txt\",\n    s3_bucket=\"{{ dag_run.conf['s3_bucket'] }}\",\n    s3_key=\"path/to/s3/file.txt\",\n    ftp_conn_id=\"ftp_default\",\n    aws_conn_id=\"aws_default\",\n    replace=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Uploading File to Azure Data Lake using LocalFilesystemToADLSOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the LocalFilesystemToADLSOperator to upload a local file to Azure Data Lake Storage. It specifies the local file path, the Azure Data Lake destination path, and Azure connection details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/transfer/local_to_adls.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nupload_to_adls = LocalFilesystemToADLSOperator(\n    task_id=\"upload_to_adls\",\n    local_path=local_path,\n    remote_path=remote_path,\n    azure_data_lake_conn_id=CONNECTION_ID,\n    # Run as a BashOperator\n    store_name=AZURE_DATA_LAKE_STORE_NAME,\n    create_intermediate_dirs=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Querying an OpenSearch Index using Apache Airflow\nDESCRIPTION: This code demonstrates how to use the OpenSearchQueryOperator to run a query against an OpenSearch index. It requires the OpenSearch provider for Airflow and assumes an existing index with documents to query.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/operators/opensearch.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_opensearch_query]\n# [END howto_operator_opensearch_query]\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Stat Name Handler in Python\nDESCRIPTION: Example Python function for customizing metric names in Airflow. This function can be referenced in the metrics configuration to transform metric names before they are sent.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/metrics.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef my_custom_stat_name_handler(stat_name: str) -> str:\n    return stat_name.lower()[:32]\n```\n\n----------------------------------------\n\nTITLE: Waiting for EKS Nodegroup State using EksNodegroupStateSensor in Python\nDESCRIPTION: This Python snippet shows how to use the `EksNodegroupStateSensor` to wait until a specific EKS managed node group reaches a target state (e.g., 'ACTIVE') or a terminal state. Requires `cluster_name`, `nodegroup_name`, and optionally `target_state`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eks.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Example using EksNodegroupStateSensor\n# Assumes necessary imports and DAG context\n\nwait_for_nodegroup_active = EksNodegroupStateSensor(\n    task_id=\"wait_for_nodegroup_active\",\n    cluster_name=\"my-existing-eks-cluster\",\n    nodegroup_name=\"my-managed-nodegroup\",\n    target_state=NodegroupStates.ACTIVE, # Or other NodegroupStates enum value\n)\n\n# [END howto_sensor_eks_nodegroup]\n```\n\n----------------------------------------\n\nTITLE: Beginning a Transaction in Google Cloud Datastore using Python\nDESCRIPTION: This example shows how to use CloudDatastoreBeginTransactionOperator to begin a new transaction in Google Cloud Datastore. It specifies the project ID and transaction options for the operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nbegin_transaction1 = CloudDatastoreBeginTransactionOperator(\n    task_id=\"begin_transaction1\",\n    project_id=GCP_PROJECT_ID,\n    transaction_options=TRANSACTION_OPTIONS,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Airflow CLI Commands in Cloud Composer (Deferrable Mode)\nDESCRIPTION: This snippet demonstrates how to run Airflow CLI commands in a Cloud Composer environment using the CloudComposerRunAirflowCLICommandOperator in deferrable mode. It includes additional parameters for deferrable execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nrun_airflow_command_deferrable = CloudComposerRunAirflowCLICommandOperator(\n    task_id=\"run-command-deferrable-mode\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    environment_name=ENVIRONMENT_NAME,\n    command=[\"dags\", \"list\"],\n    deferrable=True,\n    poll_interval=10,\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying Spanner Database in Python\nDESCRIPTION: Example of using SpannerDeployDatabaseInstanceOperator to create a new Cloud Spanner database in a specified instance. The operator can be created with or without a project ID, which will be retrieved from the Google Cloud connection if not provided.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/spanner.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_spanner_database_deploy]\n# Without project_id\nspanner_database_deploy = SpannerDeployDatabaseInstanceOperator(\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    database_id=\"data_database\",\n    ddl_statements=[\n        \"CREATE TABLE data (test_id INT64 NOT NULL) PRIMARY KEY (test_id)\"\n    ],\n    task_id=\"deploy_database\",\n)\n\n# With project_id\nspanner_database_deploy = SpannerDeployDatabaseInstanceOperator(\n    project_id=\"{{ var.value.spanner_project }}\",\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    database_id=\"data_database\",\n    ddl_statements=[\n        \"CREATE TABLE data (test_id INT64 NOT NULL) PRIMARY KEY (test_id)\"\n    ],\n    task_id=\"deploy_database\",\n)\n# [END howto_operator_spanner_database_deploy]\n```\n\n----------------------------------------\n\nTITLE: Copying Multiple Files from GCS to Google Drive using Wildcards\nDESCRIPTION: This snippet demonstrates how to copy multiple files from Google Cloud Storage to Google Drive using wildcards in the source object path. It allows batch processing of files that match a specific pattern.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gdrive.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntransfer_gcs_to_gdrive_wildcard = GCSToGoogleDriveOperator(\n    task_id=\"gcs_to_drive_wildcard\",\n    source_bucket=BUCKET_NAME,\n    source_object=GCS_WILDCARD_PATH,\n    destination_object=DESTINATION_PATH,\n)\n```\n\n----------------------------------------\n\nTITLE: Waiting on EMR Job Flow State using EmrJobFlowSensor in Python\nDESCRIPTION: Illustrates the use of `EmrJobFlowSensor` to wait for an EMR job flow (cluster) to reach a specified state. Requires the `job_flow_id` to monitor and an `aws_conn_id`. The `target_states` parameter defines the desired cluster state(s) to wait for (e.g., 'WAITING', 'TERMINATED').\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwait_for_job_flow_completion = EmrJobFlowSensor(\n    task_id=\"wait_for_job_flow_completion\",\n    job_flow_id=cluster_id,\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Checking Airflow Release Files in SVN with Python Script - Shell Script\nDESCRIPTION: This shell script snippet runs a Python script ('check_files.py') to verify expected release files are present in an SVN checkout. It checks both file completeness and validity, and can also help check package installations. It requires Python, the 'check_files.py' script, and the appropriate versions and paths. Inputs are version and path variables; the output is a validation log or report.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\npython check_files.py airflow -v ${VERSION} -p {PATH_TO_SVN}\n```\n\n----------------------------------------\n\nTITLE: Using DataflowJobMessagesSensor\nDESCRIPTION: Example of using the DataflowJobMessagesSensor to monitor Dataflow job messages. This sensor waits for specific messages in the job's output before proceeding.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Code snippet not provided in the input text\n```\n\n----------------------------------------\n\nTITLE: Analyzing Sentiment with Google Cloud Natural Language in Apache Airflow\nDESCRIPTION: This snippet demonstrates how to use the CloudNaturalLanguageAnalyzeSentimentOperator to perform sentiment analysis on a given text.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/natural_language.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nanalyze_sentiment = CloudNaturalLanguageAnalyzeSentimentOperator(\n    task_id=\"analyze_sentiment\",\n    document=document,\n)\n```\n\n----------------------------------------\n\nTITLE: Base HelloOperator Implementation\nDESCRIPTION: Example showing basic operator implementation with template fields.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass HelloOperator(BaseOperator):\n    template_fields: Sequence[str] = (\"name\",)\n\n    def __init__(self, name: str, world: str, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.name = name\n        self.world = world\n\n    def execute(self, context):\n        message = f\"Hello {self.world} it's {self.name}!\"\n        print(message)\n        return message\n```\n\n----------------------------------------\n\nTITLE: Basic DingDing Message Sending in Python\nDESCRIPTION: Example showing how to send a basic message using the DingdingOperator in Airflow. Requires a configured DingTalk Custom Robot webhook token.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsend_dingding_msg = DingdingOperator(\n    task_id=\"send_dingding_msg\",\n    message_type=\"text\",\n    message=\"This is a test message sent from Apache Airflow\",\n    at_all=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow using pip with constraints\nDESCRIPTION: Command to install Apache Airflow using pip with version-specific constraint files. This ensures a reproducible installation with compatible dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/generated/PYPI_README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow==2.10.5' \\\n --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.5/constraints-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Submitting Databricks Job with JSON Payload in Python\nDESCRIPTION: Example of using DatabricksSubmitRunOperator with a JSON payload to submit a Databricks notebook job. This method provides full control over the API payload but lacks type checking.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/submit_run.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\njson = {\n    \"new_cluster\": {\"spark_version\": \"2.1.0-db3-scala2.11\", \"num_workers\": 2},\n    \"notebook_task\": {\n        \"notebook_path\": \"/Users/airflow@example.com/PrepareData\",\n    },\n}\nnotebook_run = DatabricksSubmitRunOperator(task_id=\"notebook_run\", json=json)\n```\n\n----------------------------------------\n\nTITLE: Waiting on EMR Step State using EmrStepSensor in Python\nDESCRIPTION: Shows how to use `EmrStepSensor` to wait for a specific step within an EMR job flow to reach a target state (e.g., 'COMPLETED'). It requires the `job_flow_id`, the `step_id` of the step to monitor, and an `aws_conn_id`. The `target_states` parameter specifies the desired step state(s).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwait_for_step_completion = EmrStepSensor(\n    task_id=\"wait_for_step_completion\",\n    job_flow_id=cluster_id,\n    step_id=\"{{ task_instance.xcom_pull(task_ids='add_steps_task', key='return_value')[0] }}\",\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Committing a Transaction in Google Cloud Datastore using Python\nDESCRIPTION: This example shows how to use CloudDatastoreCommitOperator to commit a transaction in Google Cloud Datastore. It specifies the project ID, namespace, and commit information for the operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncommit_task = CloudDatastoreCommitOperator(\n    task_id=\"commit_task\",\n    project_id=GCP_PROJECT_ID,\n    namespace=None,\n    commit=COMMIT,\n)\n```\n\n----------------------------------------\n\nTITLE: Task Expansion Example\nDESCRIPTION: Example showing how to expand WaitHoursSensor tasks with different hour parameters using partial and expand methods.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/deferring.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nWaitHoursSensor.partial(task_id=\"wait_for_n_hours\", start_from_trigger=True).expand(\n    trigger_kwargs=[{\"hours\": 1}, {\"hours\": 2}]\n)\n```\n\n----------------------------------------\n\nTITLE: Producing Data to Apache Kafka Topic with ProduceToTopicOperator in Python\nDESCRIPTION: This snippet demonstrates how to produce data to an Apache Kafka topic using the ProduceToTopicOperator. It specifies the topic, kafka_config, and data to be produced.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nproduce_to_topic = ProduceToTopicOperator(\n    task_id=\"produce_to_topic\",\n    topic=TOPIC,\n    kafka_config=KAFKA_CONFIG,\n    producer_function=\"tests.providers.apache.kafka.operators.test_produce_consume.basic_producer\",\n    producer_function_kwargs={\"output_list\": OUTPUT_LIST},\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced Airflow Variable Configuration with Templates\nDESCRIPTION: YAML configuration showing templated environment variables and ConfigMap references in Helm chart. Includes examples of using Release name in variable names and external ConfigMap references.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/adding-connections-and-variables.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nextraEnv: |\n  - name: AIRFLOW_VAR_HELM_RELEASE_NAME\n    value: '{{ .Release.Name }}'\n\nextraEnvFrom: |\n  - configMapRef:\n      name: '{{ .Release.Name }}-airflow-variables'\n\nextraConfigMaps:\n  '{{ .Release.Name }}-airflow-variables':\n    data: |\n      AIRFLOW_VAR_HELLO_MESSAGE: \"Hi!\"\n```\n\n----------------------------------------\n\nTITLE: Creating Slack Webhook Connection in Python\nDESCRIPTION: Python code snippet demonstrating how to create a Slack Webhook connection using the Connection class. The code sets up connection parameters including the webhook token and timeout, then generates the corresponding environment variable.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/connections/slack-incoming-webhook.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models.connection import Connection\n\nconn = Connection(\n    conn_id=\"slack_default\",\n    conn_type=\"slackwebhook\",\n    password=\"T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX\",\n    extra={\n        # Specify extra parameters here\n        \"timeout\": \"42\",\n    },\n)\n\n# Generate Environment Variable Name\nenv_key = f\"AIRFLOW_CONN_{conn.conn_id.upper()}\"\nprint(f\"{env_key}='{conn.get_uri()}'\")\n```\n\n----------------------------------------\n\nTITLE: Checking Table and Partition Existence with Airflow BigQueryTableExistenceSensor (Sync and Async) - Python\nDESCRIPTION: These snippets utilize BigQueryTableExistenceSensor in Airflow to wait for a table (or partition) to exist before allowing downstream tasks to run, supporting both regular and deferrable (async) execution. Requirements include airflow.providers.google.cloud.sensors.bigquery. Input parameters are project_id, dataset_id, and table_id (or partition), with optional use of macros for sharded tables. Outputs are task pass (when table exists) or failure (timeout). Using deferrable mode can free up Airflow workers while waiting. Limitations relate to BigQuery resource availability and proper macro resolution for dynamic table naming.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nbq_table_sensor = BigQueryTableExistenceSensor(\n    task_id=\"bq_table_sensor\",\n    project_id=GCP_PROJECT_ID,\n    dataset_id=DATASET_NAME,\n    table_id=TABLE_NAME,\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Deferrable mode: frees up worker slots while waiting\nbq_table_sensor_deferred = BigQueryTableExistenceSensor(\n    task_id=\"bq_table_sensor_deferred\",\n    project_id=GCP_PROJECT_ID,\n    dataset_id=DATASET_NAME,\n    table_id=TABLE_NAME,\n    deferrable=True,\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Async usage of table existence sensor\nbq_table_sensor_async = BigQueryTableExistenceSensor(\n    task_id=\"bq_table_sensor_async\",\n    project_id=GCP_PROJECT_ID,\n    dataset_id=DATASET_NAME,\n    table_id=TABLE_NAME,\n    deferrable=True,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into YDB Table\nDESCRIPTION: SQL statements for inserting sample pet data using UPSERT operations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/docs/operators/ydb_operator_howto_guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nUPSERT INTO pet (pet_id, name, pet_type, birth_date, owner)\nVALUES (1, 'Max', 'Dog', '2018-07-05', 'Jane');\n\nUPSERT INTO pet (pet_id, name, pet_type, birth_date, owner)\nVALUES (2, 'Susie', 'Cat', '2019-05-01', 'Phil');\n\nUPSERT INTO pet (pet_id, name, pet_type, birth_date, owner)\nVALUES (3, 'Lester', 'Hamster', '2020-06-23', 'Lily');\n\nUPSERT INTO pet (pet_id, name, pet_type, birth_date, owner)\nVALUES (4, 'Quincy', 'Parrot', '2013-08-11', 'Anne');\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables in UNIX-based Systems\nDESCRIPTION: Commands to set and verify environment variables required for running Google provider system tests in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/tests/system/google/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport NAME_OF_ENV_VAR=value\necho $NAME_OF_ENV_VAR\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Cloud Composer Environment in Python\nDESCRIPTION: This snippet demonstrates how to define a simple Cloud Composer environment configuration. It includes settings for the image version, node count, environment size, and scheduler CPU count.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nenvironment_config = {\n    \"node_count\": 3,\n    \"node_config\": {\"machine_type\": \"e2-standard-2\"},\n    \"software_config\": {\n        \"image_version\": \"composer-2.0.11-airflow-2.2.3\",\n        \"env_variables\": {\n            \"FOO\": \"bar\"\n        }\n    },\n    \"web_server_config\": {\n        \"machine_type\": \"composer-n1-webserver-2\"\n    },\n    \"environment_size\": \"ENVIRONMENT_SIZE_SMALL\",\n    \"workloads_config\": {\n        \"scheduler\": {\n            \"cpu\": 0.5,\n            \"memory_gb\": 1.875,\n            \"storage_gb\": 1,\n            \"count\": 1,\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Sentry Default Integrations in Airflow\nDESCRIPTION: Configuration to disable Sentry's default integrations, particularly to prevent environment variable changes in subprocess behavior. This setting affects how the SubprocessHook works with environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/errors.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[sentry]\ndefault_integrations = False\n```\n\n----------------------------------------\n\nTITLE: Configuring Google OpenID Authentication Backend in Airflow\nDESCRIPTION: This snippet shows how to set the authentication backend to use Google OpenID in the Airflow configuration file. It's a crucial step in enabling Google OpenID authentication for Airflow 2.x.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/api-auth-backend/google-openid.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[api]\nauth_backends = airflow.providers.google.common.auth_backend.google_openid\n```\n\n----------------------------------------\n\nTITLE: Suspending a Job on GKE using GKESuspendJobOperator in Python\nDESCRIPTION: This snippet demonstrates using the `GKESuspendJobOperator` to suspend an active Job within a specified GKE cluster. This is useful for temporarily pausing job execution without deleting the job.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/kubernetes_engine/example_kubernetes_engine_job.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_gke_suspend_job]\n    :end-before: [END howto_operator_gke_suspend_job]\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Database Connection in INI Configuration\nDESCRIPTION: This snippet shows how to set the metadata database connection string in the airflow.cfg file using INI format.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-config.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[database]\nsql_alchemy_conn = my_conn_string\n```\n\n----------------------------------------\n\nTITLE: Implementing DAG Test Main Block in Python\nDESCRIPTION: Code snippet showing how to add a main block to a DAG file to make it runnable for testing purposes. This enables running the DAG in a single process using the test() method.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/dag_testing.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    dag.test()\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Connection with IAM Keys via Environment Variable (URI Format) in Bash\nDESCRIPTION: This snippet shows how to configure an Airflow AWS connection using an environment variable with the URI format. It includes the AWS Access Key ID and the URL-encoded Secret Access Key. Note the required trailing '@' symbol to distinguish it from a host:port format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AWS_DEFAULT=aws://AKIAIOSFODNN7EXAMPLE:wJalrXUtnFEMI%2FK7MDENG%2FbPxRfiCYEXAMPLEKEY@\n```\n\n----------------------------------------\n\nTITLE: Waiting for a Single DynamoDB Attribute Value Match using DynamoDBValueSensor in Python\nDESCRIPTION: This Python code snippet demonstrates how to use the `DynamoDBValueSensor` from the Airflow AWS provider to wait until a specific attribute in a DynamoDB item matches a single defined value. It requires specifying the table name, partition key name and value, the attribute name to check, and the expected value for that attribute. An AWS connection ID (`aws_conn_id`) is also needed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dynamodb.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwait_for_value = DynamoDBValueSensor(\n    task_id=\"wait_for_value_sensor\",\n    table_name=TABLE_NAME,\n    partition_key_name=PARTITION_KEY_NAME,\n    partition_key_value=ROW_KEY,\n    attribute_name=ATTRIBUTE_NAME,\n    attribute_value=ROW_ITEM_VALUE,\n    aws_conn_id=AWS_CONN_ID,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Migration Code Changes for BaseOperator\nDESCRIPTION: Code changes required for migrating from deprecated to new features in BaseOperator. Replace task_concurrency with max_active_tis_per_dag and update trigger rule NONE_FAILED_OR_SKIPPED to NONE_FAILED_MIN_ONE_SUCCESS.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41761.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old code\ntask = BaseOperator(\n    task_concurrency=5,\n    trigger_rule='none_failed_or_skipped'\n)\n\n# New code\ntask = BaseOperator(\n    max_active_tis_per_dag=5,\n    trigger_rule='none_failed_min_one_success'\n)\n```\n\n----------------------------------------\n\nTITLE: Getting Apache Kafka Cluster Info with ManagedKafkaGetClusterOperator in Python\nDESCRIPTION: This snippet demonstrates how to retrieve information about an Apache Kafka cluster using the ManagedKafkaGetClusterOperator. It specifies the project ID, region, and cluster name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nget_cluster = ManagedKafkaGetClusterOperator(\n    task_id=\"get_cluster\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Parameter for Papermill in Python\nDESCRIPTION: This code sets a parameter 'msgs' with a string value. The cell is tagged for papermill to identify variables it needs to set when executing the notebook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/tests/system/papermill/input_notebook.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmsgs = \"Hello!\"\n```\n\n----------------------------------------\n\nTITLE: Including Full Task Information via Environment Variable\nDESCRIPTION: Sets the `AIRFLOW__OPENLINEAGE__INCLUDE_FULL_TASK_INFO` environment variable to `true`, enabling the inclusion of full task details in OpenLineage events. This serves as an alternative to the INI configuration for enabling verbose task information.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_18\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__INCLUDE_FULL_TASK_INFO=true\n```\n\n----------------------------------------\n\nTITLE: Starting a Job on GKE using GKEStartJobOperator in Python\nDESCRIPTION: This snippet demonstrates using the `GKEStartJobOperator` to run a Kubernetes Job on a GKE cluster. This operator extends `KubernetesJobOperator`, automatically handling Google Cloud authentication and eliminating the need to manage kube_config files. All parameters of `KubernetesJobOperator` (except `config_file`) are applicable.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/kubernetes_engine/example_kubernetes_engine_job.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_gke_start_job]\n    :end-before: [END howto_operator_gke_start_job]\n```\n\n----------------------------------------\n\nTITLE: Creating Amazon Redshift Cluster with RedshiftCreateClusterOperator\nDESCRIPTION: Example demonstrating how to create an Amazon Redshift cluster using the RedshiftCreateClusterOperator with specified parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/redshift/redshift_cluster.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster = RedshiftCreateClusterOperator(\n    task_id='create_cluster',\n    cluster_identifier=redshift_cluster_identifier,\n    node_type='dc2.large',\n    master_username='adminuser',\n    master_user_password='Test123$',\n    cluster_type='single-node',\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Airflow Dockerfile\nDESCRIPTION: Example Dockerfile for creating a custom Airflow image with additional requirements\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_9\n\nLANGUAGE: docker\nCODE:\n```\nFROM apache/airflow:|version|\nADD requirements.txt .\nRUN pip install apache-airflow==${AIRFLOW_VERSION} -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Sample YAML Format for Exported Airflow Connections\nDESCRIPTION: This YAML snippet demonstrates the structure of exported Airflow connections, including connection IDs, types, and configuration details.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nairflow_db:\n  conn_type: mysql\n  extra: null\n  host: mysql\n  login: root\n  password: plainpassword\n  port: null\n  schema: airflow\ndruid_broker_default:\n  conn_type: druid\n  extra: '{\"endpoint\": \"druid/v2/sql\"}'\n  host: druid-broker\n  login: null\n  password: null\n  port: 8082\n  schema: null\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Core with Providers Separately\nDESCRIPTION: This command demonstrates how to install Apache Airflow core and a specific provider package separately, without using constraints. This approach allows for managing providers independently from the Airflow core package.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-pypi.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install \"apache-airflow==|version|\" \"apache-airflow-providers-google==8.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Adding PySpark Decorator Feature\nDESCRIPTION: A feature added in version 4.4.0 that provides a decorator for PySpark operations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"Add pyspark decorator (#35247)\"\n```\n\n----------------------------------------\n\nTITLE: Creating Airflow Admin User\nDESCRIPTION: Command to create an administrative user in Airflow with specified credentials and roles\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_gitpod.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairflow users create \\\n   --role Admin \\\n   --username admin \\\n   --password admin \\\n   --email admin@example.com \\\n   --firstname foo \\\n   --lastname bar\n```\n\n----------------------------------------\n\nTITLE: Installing Apache WebHDFS Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with WebHDFS integration, enabling HDFS hooks and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[apache-webhdfs]'\n```\n\n----------------------------------------\n\nTITLE: Deleting a Cloud Composer Environment in Python\nDESCRIPTION: This snippet demonstrates how to use the CloudComposerDeleteEnvironmentOperator to delete a Cloud Composer environment. It specifies the project ID, region, and environment name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndelete_environment_task = CloudComposerDeleteEnvironmentOperator(\n    task_id=\"delete-environment\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    environment_name=ENVIRONMENT_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Google Ads API and Generating CSV Report with GoogleAdsToGcsOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the GoogleAdsToGcsOperator to query the Google Ads API and generate a CSV report of the results. The operator allows for dynamic value determination using Jinja templating for various parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/ads.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nGoogleAdsToGcsOperator(\n    task_id=\"get_ads_data\",\n    client_ids=[\"123\"],\n    query=\"SELECT campaign.id, campaign.name FROM campaign\",\n    attributes=[\"campaign.id\", \"campaign.name\"],\n    obj=\"ads_data.csv\",\n    bucket=\"my-bucket\",\n    google_ads_conn_id=ADS_CONN_ID,\n    gcp_conn_id=GCS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Starting Python Dataflow Job\nDESCRIPTION: Example showing how to create and run a Python Dataflow pipeline using a Python file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n_ = BeamRunPythonPipelineOperator(\n    task_id=\"start-python-job\",\n    py_file=f\"gs://{BUCKET_NAME}/{PY_FILE}\",\n    py_options=[],\n    pipeline_options=DATAFLOW_PYTHON_OPTIONS,\n    py_requirements=[\"apache-beam[gcp]\"],\n    py_interpreter=\"python3\",\n    py_system_site_packages=False,\n    location=location,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Google Cloud Dataproc Cluster Asynchronously\nDESCRIPTION: This code shows how to delete a Dataproc cluster using deferrable mode for asynchronous execution, which improves Airflow's resource utilization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndelete_cluster_async = DataprocDeleteClusterOperator(\n    task_id=\"delete_cluster_async\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster_name=CLUSTER_NAME,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Dataplex Catalog Entry Group with Airflow Operator\nDESCRIPTION: Uses the DataplexCatalogDeleteEntryGroupOperator to delete a Dataplex Catalog Entry Group. This operator removes a specified entry group from Google Cloud Dataplex Catalog.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndelete_entry_group = DataplexCatalogDeleteEntryGroupOperator(\n    task_id=\"delete_entry_group\",\n    project_id=PROJECT_ID,\n    location=REGION,\n    entry_group_id=ENTRY_GROUP_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Polling dbt Cloud Job Run Status Asynchronously in Python\nDESCRIPTION: This snippet demonstrates how to use the DbtCloudJobRunSensor in deferrable mode to poll for the status of a job run asynchronously, freeing up worker slots while the sensor is running.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/operators.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndbt_cloud_job_run_sensor_deferred = DbtCloudJobRunSensor(\n    task_id=\"dbt_cloud_job_run_sensor_deferred\",\n    run_id=dbt_cloud_run_job_async.output[\"id\"],\n    timeout=TIMEOUT,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting Trino Query Results and BigQuery Schema to Google Cloud Storage using Airflow Operator in Python\nDESCRIPTION: Illustrates the usage of Airflow's TrinoToGCSOperator to both export data and generate a BigQuery-compatible schema file, by specifying the schema_filename parameter. The operator executes the provided SQL against Trino and uploads results in the desired export format (default is JSON Lines) as well as a JSON schema file for BigQuery ingestion. Requires Airflow with Trino and Google provider packages. Useful in automated ETL workflows to provide both data and schema in sync for BigQuery.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/trino_to_gcs.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrino_to_gcs_with_schema = TrinoToGCSOperator(\n    task_id=\"trino_to_gcs_with_schema\",\n    sql=\"SELECT column1, column2, column3 FROM my_table;\",\n    bucket=\"my-gcs-bucket\",\n    filename=\"data_types_{}.json\",\n    schema_filename=\"schema_file.json\",\n)\n```\n\n----------------------------------------\n\nTITLE: Monitoring MWAA DAG Run State\nDESCRIPTION: Example demonstrating how to use MwaaDagRunSensor to wait for a DAG run to reach a specific state in Amazon MWAA. The sensor monitors the DAG run created by the previous task until completion.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/mwaa.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwait_for_dag_run = MwaaDagRunSensor(\n    task_id=\"wait_for_dag_run\",\n    environment_name=\"MyAirflowEnvironment\",\n    dag_name=\"hello_world\",\n    dag_run_id=trigger_dag_run.output,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Cloud Memorystore Instance\nDESCRIPTION: Example showing how to create a new Cloud Memorystore instance using the CreateInstanceOperator\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncreate_instance = CloudMemorystoreCreateInstanceOperator(task_id=\"create-instance\", location=\"europe-north1\", instance_id=INSTANCE_NAME, project_id=PROJECT_ID, instance=INSTANCE)\n```\n\n----------------------------------------\n\nTITLE: Avoiding 'Jinja template not found' Error with @task.bash in Python\nDESCRIPTION: Illustrates the workaround for the 'Jinja template not found' error when using `@task.bash` to execute a script that doesn't require templating. Adding a space after the script path in the return statement prevents Airflow from trying to render it as a Jinja template.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n@task.bash\ndef bash_example():\n    # This fails with 'Jinja template not found' error\n    # return \"/home/batcher/test.sh\",\n    # This works (has a space after)\n    return \"/home/batcher/test.sh \"\n```\n\n----------------------------------------\n\nTITLE: Installing Zendesk Integration for Apache Airflow\nDESCRIPTION: Command to install Zendesk integration package that provides hooks for Apache Airflow\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_45\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[zendesk]'\n```\n\n----------------------------------------\n\nTITLE: Presto to GCS CSV Export in Python\nDESCRIPTION: Example showing how to configure PrestoToGCSOperator to output query results in CSV format instead of the default JSON Lines.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/presto_to_gcs.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntask_presto_to_gcs_csv = PrestoToGCSOperator(\n    task_id=\"presto_to_gcs_csv\",\n    sql=PRESTO_SQL_BASIC_TYPES,\n    bucket=BUCKET,\n    filename=f\"{DATASET_PREFIX}/csv/data*.csv\",\n    export_format=\"csv\",\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Secret Masking in Tests\nDESCRIPTION: Python code snippet demonstrating how to enable secret masking for specific tests using a pytest marker. This is useful for testing logging and output redaction.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_43\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.enable_redact\ndef test_masking(capsys):\n    mask_secret(\"eggs\")\n    RedactedIO().write(\"spam eggs and potatoes\")\n    assert \"spam *** and potatoes\" in capsys.readouterr().out\n```\n\n----------------------------------------\n\nTITLE: Presto to GCS with Schema Generation in Python\nDESCRIPTION: Example demonstrating how to generate BigQuery schema files alongside the data export using the schema_filename parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/presto_to_gcs.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntask_presto_to_gcs_multiple_types = PrestoToGCSOperator(\n    task_id=\"presto_to_gcs_multiple_types\",\n    sql=PRESTO_SQL_MULTIPLE_TYPES,\n    bucket=BUCKET,\n    filename=f\"{DATASET_PREFIX}/multiple_types/data*.json\",\n    schema_filename=f\"{DATASET_PREFIX}/multiple_types/schema.json\",\n)\n```\n\n----------------------------------------\n\nTITLE: Registering Custom OpenLineage Run Facets in Airflow via INI\nDESCRIPTION: Configures Airflow to inject custom run facets into OpenLineage events by specifying a semicolon-separated list of facet function import paths using the `custom_run_facets` key in the `[openlineage]` section of the Airflow configuration. This allows adding custom metadata to run-level events.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_21\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\ncustom_run_facets = full.path.to.get_my_custom_facet;full.path.to.another_custom_facet_function\n```\n\n----------------------------------------\n\nTITLE: Listing dbt Cloud Jobs in Python\nDESCRIPTION: This snippet demonstrates how to use the DbtCloudListJobsOperator to list all jobs tied to a specified dbt Cloud account. If a project_id is supplied, only jobs for that project will be retrieved.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/operators.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndbt_cloud_list_jobs = DbtCloudListJobsOperator(\n    task_id=\"dbt_cloud_list_jobs\",\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying GCS Output Filename in Python\nDESCRIPTION: Demonstrates how to define the 'filename' argument for the CloudTextToSpeechSynthesizeOperator. This simple string specifies the path within the target Google Cloud Storage bucket where the synthesized audio file will be saved.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/text_to_speech.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# The name of the audio file / GCS Object\nfilename = \"synthesize-text-audio.wav\"\n```\n\n----------------------------------------\n\nTITLE: Deleting a Dataplex Asset with Airflow Operator\nDESCRIPTION: Uses the DataplexDeleteAssetOperator to delete a Dataplex asset. This operator removes a specified asset from a zone in Google Cloud Dataplex.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndelete_asset = DataplexDeleteAssetOperator(\n    task_id=\"delete_asset\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    lake_id=LAKE_ID,\n    zone_id=ZONE_ID,\n    asset_id=ASSET_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Chime Notifications in Apache Airflow DAG (Python)\nDESCRIPTION: This Python example shows how to configure an Airflow DAG and associated tasks to send notifications to an Amazon Chime chat room via the send_chime_notification utility. It requires the airflow.providers.amazon.aws.notifications.chime package and an established Chime webhook connection (chime_conn_id). The DAG is set up with on_success_callback and the BashOperator task has its own on_failure_callback, each sending templated messages to Chime. Inputs include DAG/task context, webhook connection ID, and notification messages; outputs are Chime chat messages. The setup assumes Airflow 2.x and requires the specified provider packages.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/notifications/chime_notifier_howto_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.amazon.aws.notifications.chime import send_chime_notification\n\nwith DAG(\n    dag_id=\"mydag\",\n    schedule=\"@once\",\n    start_date=datetime(2023, 6, 27),\n    on_success_callback=[\n        send_chime_notification(chime_conn_id=\"my_chime_conn\", message=\"The DAG {{ dag.dag_id }} succeeded\")\n    ],\n    catchup=False,\n):\n    BashOperator(\n        task_id=\"mytask\",\n        on_failure_callback=[\n            send_chime_notification(chime_conn_id=\"my_chime_conn\", message=\"The task {{ ti.task_id }} failed\")\n        ],\n        bash_command=\"fail\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with DAGs Loaded from S3 Bucket for Airflow (bash)\nDESCRIPTION: This snippet demonstrates building a Docker image where DAG files are synced from an S3 bucket. Build-time arguments include AWS credentials and s3_uri. You must have S3 read permissions and replace placeholders with real values. Suitable for AWS Executor task distribution of DAGs inside containers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/executors/general.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t my-airflow-image \\\n --build-arg aws_access_key_id=YOUR_ACCESS_KEY \\\n --build-arg aws_secret_access_key=YOUR_SECRET_KEY \\\n --build-arg aws_default_region=YOUR_DEFAULT_REGION \\\n --build-arg aws_session_token=YOUR_SESSION_TOKEN \\\n --build-arg s3_uri=YOUR_S3_URI .\n```\n\n----------------------------------------\n\nTITLE: Exporting ElasticSearch Connection URI - Bash\nDESCRIPTION: This snippet demonstrates how to configure the ElasticSearch connection for Apache Airflow by setting an environment variable using URI syntax. It requires properly URL-encoded values for the login credentials, host, port, and scheme. 'AIRFLOW_CONN_ELASTICSEARCH_DEFAULT' must be exported with the connection string to enable ElasticSearch hooks and operators to use the specified instance by default. The expected input is a valid ElasticSearch URI and no output is produced by the export command beyond environment variable assignment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/connections/elasticsearch.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_ELASTICSEARCH_DEFAULT='elasticsearch://elasticsearchlogin:elasticsearchpassword@elastic.co:80/http'\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Workflow Template in Google Cloud Dataproc\nDESCRIPTION: This code demonstrates how to create a workflow template in Dataproc using DataprocCreateWorkflowTemplateOperator. Workflow templates allow defining multi-step jobs that can be executed repeatedly.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ncreate_workflow_template = DataprocCreateWorkflowTemplateOperator(\n    task_id=\"create_workflow_template\",\n    template=WORKFLOW_TEMPLATE,\n    project_id=PROJECT_ID,\n    region=REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating Airflow's ObjectStoragePath with DuckDB for S3 Parquet Reading\nDESCRIPTION: This snippet demonstrates how to use Airflow's ObjectStoragePath to read a Parquet file from S3 using DuckDB. It showcases the integration between Airflow's connection handling and external data processing libraries.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/objectstorage.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nfrom airflow.sdk import ObjectStoragePath\n\npath = ObjectStoragePath(\"s3://my-bucket/my-table.parquet\", conn_id=\"aws_default\")\nconn = duckdb.connect(database=\":memory:\")\nconn.register_filesystem(path.fs)\nconn.execute(f\"CREATE OR REPLACE TABLE my_table AS SELECT * FROM read_parquet('{path}');\"\n```\n\n----------------------------------------\n\nTITLE: Defining De-identification Config for DLP Content - Python\nDESCRIPTION: This snippet shows the creation of a DeidentifyConfig object and related configuration for de-identifying sensitive information using Google Cloud DLP. It depends on the google.cloud.dlp_v2 library and is typically used before passing the configuration to an operator. Key parameters specify how sensitive info is transformed, using info_types and transformation rules. Inputs are the sensitive content and the DeidentifyConfig; outputs are the configured objects for downstream processing.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/data_loss_prevention.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndeidentify_config = {\n    'info_type_transformations': {\n        'transformations': [\n            {\n                'info_types': [{'name': 'EMAIL_ADDRESS'}],\n                'primitive_transformation': {\n                    'replace_with_info_type_config': {}\n                },\n            },\n        ]\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring External StatsD for Airflow Metrics\nDESCRIPTION: Configuration for connecting Airflow to an external StatsD instance for metrics collection.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nstatsd:\n  enabled: false\nconfig:\n  metrics:  # or 'scheduler' for Airflow 1\n    statsd_on: true\n    statsd_host: ...\n    statsd_port: ...\n```\n\n----------------------------------------\n\nTITLE: Listing Datasets using Google Cloud VertexAI ListDatasetsOperator in Airflow (Python)\nDESCRIPTION: Utilizes the ListDatasetsOperator to retrieve a list of datasets from VertexAI. Requires project ID, location, Airflow Google provider, and GCP credentials. No mandatory input except project/location; outputs a dataset list with associated metadata in an XCom.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    list_datasets = ListDatasetsOperator(\n        task_id=\"list_datasets\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        gcp_conn_id=GCP_CONN_ID,\n    )\n```\n\n----------------------------------------\n\nTITLE: Listing Tags on an Entry with CloudDataCatalogListTagsOperator in Python\nDESCRIPTION: Example of listing tags on an entry in Google Cloud DataCatalog using the CloudDataCatalogListTagsOperator. The result is saved to XCom for use in other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_list_tags]\nlist_tags = CloudDataCatalogListTagsOperator(\n    task_id=\"list_tags\",\n    location=LOCATION,\n    entry_group=ENTRY_GROUP_ID,\n    entry=ENTRY_ID,\n)\n# [END howto_operator_gcp_datacatalog_list_tags]\n```\n\n----------------------------------------\n\nTITLE: Reading Files from Object Storage\nDESCRIPTION: Demonstrates how to read the contents of a file from object storage using the open() method with a context manager.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/objectstorage.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef read_file(path: ObjectStoragePath) -> str:\n    with path.open() as f:\n        return f.read()\n```\n\n----------------------------------------\n\nTITLE: Writing to Task Logs using Python Logging\nDESCRIPTION: Example of using Python's standard logging module to write to the Airflow task log by creating a logger using the Python module name.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/logging-tasks.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogger.info(\"This is a log message\")\n```\n\n----------------------------------------\n\nTITLE: Describing AWS DMS Serverless Replication Config using DmsDescribeReplicationConfigsOperator\nDESCRIPTION: This snippet illustrates using DmsDescribeReplicationConfigsOperator to retrieve details about AWS DMS serverless replication configurations. Filters can be applied to specify which configurations to describe. Depends on the airflow.providers.amazon.aws.operators.dms module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dms.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Example using DmsDescribeReplicationConfigsOperator\n# from airflow.providers.amazon.aws.operators.dms import DmsDescribeReplicationConfigsOperator\n\n# describe_repl_config = DmsDescribeReplicationConfigsOperator(\n#     task_id=\"describe_dms_serverless_replication_config\",\n#     describe_replication_configs_kwargs={\n#         'Filters': [\n#             {'Name': 'replication-config-identifier', 'Values': ['my-serverless-config']}\n#         ]\n#     }\n#     # ... other parameters\n# )\n```\n\n----------------------------------------\n\nTITLE: Uploading Archive to Amazon Glacier with GlacierUploadArchiveOperator\nDESCRIPTION: Example showing how to upload an archive to an Amazon S3 Glacier vault using the GlacierUploadArchiveOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/glacier.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nupload_archive = GlacierUploadArchiveOperator(\n    task_id=\"upload_glacier_archive\",\n    aws_conn_id=\"aws_default\",\n    vault_name=\"airflow\",\n    account_id=\"-\"\n)\n```\n\n----------------------------------------\n\nTITLE: Pulling XCom Value in Python Task\nDESCRIPTION: Shows how to pull a value from XCom that was previously pushed by another task. This enables data sharing between tasks in a DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/xcoms.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntask_instance.xcom_pull(key=\"identifier as string\", task_ids=\"task-1\")\n```\n\n----------------------------------------\n\nTITLE: Configuring a Dataplex Catalog Entry Group in Python\nDESCRIPTION: Defines the configuration for a Google Cloud Dataplex Catalog Entry Group. This configuration specifies the properties needed before creating an entry group with the DataplexCatalogCreateEntryGroupOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n# Configuration to create a simple Dataplex Catalog Entry Group\nENTRY_GROUP_ID = \"test-entry-group\"\n\nENTRY_GROUP = {\n    \"display_name\": \"Test Entry Group\",\n    \"description\": \"A simple Entry Group for testing purposes\",\n}\n```\n\n----------------------------------------\n\nTITLE: Moving Specific Files from SFTP to GCS using Wildcard\nDESCRIPTION: Example demonstrating how to move specific files from SFTP to Google Cloud Storage using wildcards to match file patterns. Only one wildcard is allowed in the path.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/sftp_to_gcs.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmove_specific_files = SFTPToGCSOperator(\n    task_id=\"move-specific-files\",\n    source_path=\"path/to/{{ds}}/*.bin\",\n    destination_bucket=BUCKET_NAME,\n    destination_path=\"specific_files\",\n    move_object=True,\n    sftp_conn_id=\"sftp_default\",\n    gcp_conn_id=\"google_cloud_default\",\n    dag=dag\n)\n```\n\n----------------------------------------\n\nTITLE: Using Inspection Template in Google Cloud DLP\nDESCRIPTION: Demonstrates how to use an inspection template to find sensitive information using CloudDLPInspectContentOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/data_loss_prevention.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_dlp_use_inspect_template]\n[END howto_operator_dlp_use_inspect_template]\n```\n\n----------------------------------------\n\nTITLE: Skipping Specific Pre-commit Checks in Bash\nDESCRIPTION: This command demonstrates how to skip specific pre-commit checks by setting the SKIP environment variable.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nSKIP=mypy-airflow-core,ruff pre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Deleting a Google Cloud Function using CloudFunctionDeleteFunctionOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the CloudFunctionDeleteFunctionOperator to delete a function from Google Cloud Functions. It includes the operator initialization with necessary parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/functions.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nCloudFunctionDeleteFunctionOperator(\n    task_id=\"delete_function\",\n    name=GCF_NAME,\n    location=GCP_LOCATION,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Using KubernetesJobOperator in Python\nDESCRIPTION: Example of how to use the KubernetesJobOperator to create and run Jobs on a Kubernetes cluster. It demonstrates the basic usage and configuration options.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nKubernetesJobOperator(\n    task_id=\"kubernetes_job_task\",\n    namespace=\"default\",\n    image=\"ubuntu\",\n    cmds=[\"bash\", \"-cx\"],\n    arguments=[\"echo\", \"10\"],\n    labels={\"foo\": \"bar\"},\n    name=\"airflow-job-test\",\n    on_finish_action=\"delete_pod\",\n)\n```\n\n----------------------------------------\n\nTITLE: Incremental Point-in-Time DynamoDB to S3 Export in Python using Airflow\nDESCRIPTION: This example demonstrates an incremental point-in-time export from DynamoDB to S3. It uses the point_in_time_export parameter along with dynamodb_scan_kwargs to perform an incremental export based on a specific attribute and time range.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/dynamodb_to_s3.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndynamodb_to_s3_incremental_export_task = DynamoDBToS3Operator(\n    task_id=\"dynamodb_to_s3_incremental_export\",\n    dynamodb_table_name=\"{{ redshift_cluster_identifier }}_table\",\n    s3_bucket_name=\"{{ s3_bucket }}\",\n    file_size=1000,\n    aws_conn_id=\"aws_default\",\n    point_in_time_export=True,\n    export_time=datetime(2023, 1, 1, 0, 0, 0),\n    dynamodb_scan_kwargs={\n        \"FilterExpression\": \"#date_attr between :start_date and :end_date\",\n        \"ExpressionAttributeNames\": {\"#date_attr\": \"date\"},\n        \"ExpressionAttributeValues\": {\n            \":start_date\": {\"S\": \"2022-12-31\"},\n            \":end_date\": {\"S\": \"2023-01-01\"}\n        }\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Installing ODBC Dependencies with pip\nDESCRIPTION: Command to install the required pyodbc module and ODBC dependencies for Apache Airflow\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow[odbc]\n```\n\n----------------------------------------\n\nTITLE: Deleting Product with Explicit ID in Google Cloud Vision Operator\nDESCRIPTION: Shows how to use CloudVisionDeleteProductOperator with an explicitly specified product ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndelete_product = CloudVisionDeleteProductOperator(\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    product_id=PRODUCT_ID,\n    task_id=\"delete_product\",\n)\n```\n\n----------------------------------------\n\nTITLE: Migrate Airflow Database\nDESCRIPTION: Performs the database migration process for upgrading to Airflow 3.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading_to_airflow3.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairflow db migrate\n```\n\n----------------------------------------\n\nTITLE: Disabling Alert Policies with StackdriverDisableAlertPoliciesOperator in Python\nDESCRIPTION: This snippet illustrates the usage of StackdriverDisableAlertPoliciesOperator to disable Alert Policies identified by a given filter. The operator can be used with or without a project ID, which will be retrieved from the Google Cloud connection if not provided.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/stackdriver.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nStackdriverDisableAlertPoliciesOperator(\n    task_id='disable_alert_policies',\n    filter_='',\n    project_id=None,\n    gcp_conn_id='google_cloud_default'\n)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Priority Weight Strategy in DAG Definition\nDESCRIPTION: Example DAG showing how to apply a custom priority weight strategy to tasks. Demonstrates both direct class instantiation and string path reference methods.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/priority-weight.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_custom_weight\",\n    start_date=datetime(2023, 1, 1),\n    schedule=None,\n    catchup=False,\n) as dag:\n    # Directly provide instance of the DecreasingPriorityWeightStrategy class\n    task1 = EmptyOperator(\n        task_id=\"task1\",\n        weight_rule=DecreasingPriorityWeightStrategy(base_weight=10),\n    )\n\n    # Reference the path to the DecreasingPriorityWeightStrategy class\n    task2 = EmptyOperator(\n        task_id=\"task2\",\n        weight_rule=\"airflow.example_dags.plugins.decreasing_priority_weight_strategy.DecreasingPriorityWeightStrategy\",\n    )\n\n    task1 >> task2\n```\n\n----------------------------------------\n\nTITLE: Deleting PubSub Topic with PubSubDeleteTopicOperator\nDESCRIPTION: Example showing how to delete a PubSub topic using the PubSubDeleteTopicOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/pubsub.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndelete_topic = PubSubDeleteTopicOperator(\n    task_id=\"delete_topic\", topic=TOPIC_ID, project_id=PROJECT_ID\n)\n```\n\n----------------------------------------\n\nTITLE: Updating a Cloud Build Trigger Using Airflow Operator - Python\nDESCRIPTION: Shows an Airflow example with CloudBuildUpdateBuildTriggerOperator for updating properties of a Cloud Build trigger, such as its definition or substitutions. Requires the Google provider, trigger details, project/trigger IDs, and new trigger configuration. Results are available to future tasks through XCom, supporting workflow-driven trigger management.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_build.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nupdate_build_trigger = CloudBuildUpdateBuildTriggerOperator(\n    task_id=\"update_build_trigger\",\n    project_id=PROJECT_ID,\n    trigger_id=TRIGGER_ID,\n    trigger=TRIGGER,\n)\n```\n\n----------------------------------------\n\nTITLE: S3 to SQL Transfer with CSV Parser\nDESCRIPTION: Example showing how to use S3ToSqlOperator with a basic CSV parser that loads the entire file into memory and returns a list of rows. The operator transfers data from an S3 bucket to a SQL table using specified connection IDs and parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/s3_to_sql.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef s3_to_sql_task():\n    def process_input(input_file):\n        with input_file as file:\n            reader = csv.reader(file)\n            rows = [row for row in reader]\n            return rows\n\n    task = S3ToSqlOperator(\n        task_id='insert_from_s3_to_db',\n        s3_key='s3://bucket/path/sample.csv',\n        aws_conn_id='s3_default',\n        sql_conn_id='sql_default',\n        table_name='sample_table',\n        parser=process_input,\n    )\n```\n\n----------------------------------------\n\nTITLE: Deleting S3 Objects with S3DeleteObjectsOperator\nDESCRIPTION: Shows how to delete one or multiple objects from an S3 bucket using the S3DeleteObjectsOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/s3.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndelete_objects = S3DeleteObjectsOperator(\n    task_id=\"delete_objects\",\n    bucket=BUCKET_NAME,\n    keys=[KEY, NEW_KEY],\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Ingesting Weaviate Data with Custom Vectors from XCom using Python\nDESCRIPTION: This snippet demonstrates how to use the `WeaviateIngestOperator` to ingest data into a Weaviate collection (`ClassName`) using custom vectors. The input data, including vectors, is retrieved from an Airflow XCom pushed by a previous task (`get_data_with_vectors.output`). It requires a configured Weaviate connection (`weaviate_default`).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/weaviate/docs/operators/weaviate.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ningest_data_with_vectors_from_xcom = WeaviateIngestOperator(\n    task_id=\"ingest_data_with_vectors_from_xcom\",\n    conn_id=\"weaviate_default\",\n    class_name=\"ClassName\",\n    input_data=get_data_with_vectors.output,\n)\n```\n\n----------------------------------------\n\nTITLE: Jenkins Pipeline Stage for Checking DAG Import Errors\nDESCRIPTION: This Groovy snippet defines a Jenkins pipeline stage that checks for DAG import errors using the Airflow CLI and fails if any errors are found.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_14\n\nLANGUAGE: groovy\nCODE:\n```\nstage('All dags are loadable') {\n    steps {\n        sh 'airflow dags list-import-errors | tee import_errors.txt && jq -e \\'select(type==\"array\" and length == 0)\\' import_errors.txt'\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using DataflowJobAutoScalingEventsSensor\nDESCRIPTION: Example of using the DataflowJobAutoScalingEventsSensor to monitor autoscaling events in a Dataflow job. This sensor waits for specific autoscaling events before proceeding.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Code snippet not provided in the input text\n```\n\n----------------------------------------\n\nTITLE: Fetching SQL Data as Polars DataFrames using DbApiHook in Airflow (Python)\nDESCRIPTION: This snippet illustrates using the `get_df` and `get_df_by_chunks` methods of Airflow's `DbApiHook` to execute SQL queries and return results as Polars DataFrames. `get_df` fetches the complete dataset, while `get_df_by_chunks` enables chunked processing for large datasets. Requires the `polars` extra to be installed. The `df_type` parameter must be set to `\"polars\"`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/dataframes.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Get complete DataFrame in a single operation\ndf = hook.get_df(\n    sql=\"SELECT * FROM my_table WHERE date_column >= %s\",\n    parameters={\"date_column\": \"2023-01-01\"},\n    df_type=\"polars\",\n)\n\n# Get DataFrame in chunks for memory-efficient processing of large results\nfor chunk_df in hook.get_df_by_chunks(sql=\"SELECT * FROM large_table\", chunksize=10000, df_type=\"polars\"):\n    process_chunk(chunk_df)\n```\n\n----------------------------------------\n\nTITLE: Configuring Boto3 Retry Strategy in Connection Extra Field (JSON)\nDESCRIPTION: This JSON snippet for the 'Extra' field configures the Boto3 retry strategy for an Airflow AWS connection to help avoid throttling exceptions. It sets the retry 'mode' to 'standard' (an exponential backoff strategy) and increases the 'max_attempts' to 10, overriding the default 'legacy' mode with 5 attempts.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"config_kwargs\": {\n    \"retries\": {\n      \"mode\": \"standard\",\n      \"max_attempts\": 10\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Apache Kafka Clusters with ManagedKafkaListClustersOperator in Python\nDESCRIPTION: This code shows how to list Apache Kafka clusters using the ManagedKafkaListClustersOperator. It requires the project ID and region.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlist_clusters = ManagedKafkaListClustersOperator(\n    task_id=\"list_clusters\",\n    project_id=PROJECT_ID,\n    region=REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Fetching AWS Step Functions Execution Output in Airflow\nDESCRIPTION: This snippet shows how to use the StepFunctionGetExecutionOutputOperator to fetch the output from an AWS Step Function state machine execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/step_functions.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_step_function_get_execution_output]\n# [END howto_operator_step_function_get_execution_output]\n```\n\n----------------------------------------\n\nTITLE: Starting Dataflow YAML Job with Airflow\nDESCRIPTION: Demonstrates using the `DataflowStartYamlJobOperator` to run a Dataflow pipeline defined in a Beam YAML file. This allows defining pipelines without writing Python or Java code but requires the `gcloud` SDK on the Airflow worker.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_yaml.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_dataflow_start_yaml_job]\n    :end-before: [END howto_operator_dataflow_start_yaml_job]\n```\n\n----------------------------------------\n\nTITLE: Requesting JWT Token via cURL (Bash)\nDESCRIPTION: This Bash script uses the cURL command to send a POST request to the Airflow FAB authentication endpoint (`/auth/token`). It includes the username and password within a JSON payload in the request body to obtain a JWT token necessary for subsequent Airflow API authentication. Users need to replace `<username>` and `<password>` with their valid credentials and ensure the `ENDPOINT_URL` points to their Airflow webserver instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/token.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nENDPOINT_URL=\"http://localhost:8080/\"\ncurl -X 'POST' \\\n    \"${ENDPOINT_URL}/auth/token\" \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n    \"username\": \"<username>\",\n    \"password\": \"<password>\"\n    }'\n```\n\n----------------------------------------\n\nTITLE: Upserting Alert Policies with StackdriverUpsertAlertOperator in Python\nDESCRIPTION: This example demonstrates how to use the StackdriverUpsertAlertOperator to upsert Alert Policies identified by a given filter JSON string. If an alert with the given name already exists, the operator updates the existing policy; otherwise, it creates a new one.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/stackdriver.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nStackdriverUpsertAlertOperator(\n    task_id='upsert_alert_policy',\n    alerts=ALERT_POLICY,\n    project_id=None,\n    gcp_conn_id='google_cloud_default'\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Model Versions using Vertex AI Model Service Operator - Python\nDESCRIPTION: Explains how to list all versions for a specific Vertex AI model using Airflow's ListModelVersionsOperator. Required parameters include project, region, and model IDs; the output is a set of available model versions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nlist_versions_task = ListModelVersionsOperator(\n    task_id=\"list_model_versions_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    model_id=MODEL_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Connection with Kerberos in JSON\nDESCRIPTION: JSON configuration examples for Hive connections using Kerberos authentication in Airflow, including options for proxy users.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/kerberos.rst#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"use_beeline\": true, \"principal\": \"hive/_HOST@EXAMPLE.COM\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"use_beeline\": true, \"principal\": \"hive/_HOST@EXAMPLE.COM\", \"proxy_user\": \"login\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"use_beeline\": true, \"principal\": \"hive/_HOST@EXAMPLE.COM\", \"proxy_user\": \"owner\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Redshift IAM Authentication (Serverless) via Extra JSON\nDESCRIPTION: This JSON configuration, placed in the 'Extra' field of an Airflow Redshift connection, enables IAM authentication specifically for Redshift Serverless. It requires setting both `iam` and `is_serverless` to true, and providing the `serverless_work_group`. Connection details like `port`, `region`, and `database` are also needed. The `serverless_token_duration_seconds` parameter (optional, default 3600) controls the temporary credential expiry. An optional AWS `profile` can be specified.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/redshift.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"iam\": true,\n  \"is_serverless\": true,\n  \"serverless_work_group\": \"default\",\n  \"serverless_token_duration_seconds\": 3600,\n  \"port\": 5439,\n  \"region\": \"us-east-1\",\n  \"database\": \"dev\",\n  \"profile\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running a Past-Records-Only AppFlow Flow Using AppflowRunBeforeOperator - Apache Airflow - Python\nDESCRIPTION: Explains use of AppflowRunBeforeOperator to trigger flows that select records before a certain cutoff (e.g., now), discarding future data. For use cases requiring historical snapshots or backfills. Requires Amazon provider and relevant AWS/AppFlow configuration. The main parameters are flow_name and filter configuration, usually using the DAG run context. Output is limited to records dated before the DAG run time.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/appflow.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_appflow_run_before\",\n    schedule_interval=None,\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"appflow\"],\n) as dag:\n\n    run_flow_before = AppflowRunBeforeOperator(\n        task_id=\"appflow_run_before_task\",\n        flow_name=\"your_appflow_flow_name\",\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Enhancing Remote Logging with AWS S3 in Python\nDESCRIPTION: Extends task context logging support for remote logging using AWS S3.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"Extend task context logging support for remote logging using AWS S3 (#32950)\"\n```\n\n----------------------------------------\n\nTITLE: Using partial() for Non-Expanding Parameters in Airflow Tasks\nDESCRIPTION: Shows how to use the partial() function to provide fixed parameters alongside mapped parameters. This is useful for passing connection IDs, table names, or other constants to mapped tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef add(x: int, y: int):\n    return x + y\n\n\nadded_values = add.partial(y=10).expand(x=[1, 2, 3])\n# This results in add function being expanded to\n# add(x=1, y=10)\n# add(x=2, y=10)\n# add(x=3, y=10)\n```\n\n----------------------------------------\n\nTITLE: Configuring Neo4j Connection Extras in JSON\nDESCRIPTION: JSON configuration for Neo4j connection extras, including encryption settings, scheme configuration, and certificate settings for secure connections.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/neo4j/docs/connections/neo4j.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"encrypted\": true,\n   \"neo4j_scheme\": true,\n   \"certs_self_signed\": true,\n   \"certs_trusted_ca\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Dynamic Task Mapping with Non-TaskFlow Operators in Airflow\nDESCRIPTION: Example showing how to use partial() and expand() with traditional (non-TaskFlow) operators in Airflow. Demonstrates passing non-mappable parameters to partial() while using expand() for dynamic parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../src/airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py\n```\n\n----------------------------------------\n\nTITLE: Listing Datasets with Vertex AI in Apache Airflow\nDESCRIPTION: This snippet demonstrates how to use the ListDatasetsOperator from Vertex AI to list datasets in Apache Airflow. It specifies the project, region, and encryption spec for the operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/automl.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlist_datasets = ListDatasetsOperator(\n    task_id=\"list_datasets\",\n    region=REGION,\n    project_id=PROJECT_ID,\n    encryption_spec=CMEK_RESOURCE_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Counting Input Tokens for Vertex AI Gemini API using Airflow Operator in Python\nDESCRIPTION: Demonstrates using the `CountTokensOperator` from `airflow.providers.google.cloud.operators.vertex_ai.generative_model` to calculate the number of input tokens before calling the Gemini API. The total token count is pushed to XCom under the 'total_tokens' key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_50\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/vertex_ai/example_vertex_ai_generative_model.py\n    :language: python\n    :dedent: 4\n    :start-after: [START how_to_cloud_vertex_ai_count_tokens_operator]\n    :end-before: [END how_to_cloud_vertex_ai_count_tokens_operator]\n```\n\n----------------------------------------\n\nTITLE: Deleting a Google Cloud Run Job using CloudRunDeleteJobOperator in Airflow\nDESCRIPTION: Shows how to use the CloudRunDeleteJobOperator to delete a Cloud Run job in an Airflow DAG. The operator waits for the job to be deleted before completing.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_run.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndelete_job = CloudRunDeleteJobOperator(\n    task_id=\"delete_job\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    job_id=JOB_NAME\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with graphviz Extra\nDESCRIPTION: This command installs Apache Airflow with the graphviz extra, which provides a Graphviz renderer for converting DAGs to graphical output.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[graphviz]'\n```\n\n----------------------------------------\n\nTITLE: Transferring Multiple Keys from Amazon S3 to Amazon Redshift using S3ToRedshiftOperator in Python\nDESCRIPTION: This code snippet shows how to use the S3ToRedshiftOperator to ingest multiple keys from Amazon S3 into an Amazon Redshift table. It demonstrates the usage of a list comprehension to generate S3 keys and specifies additional COPY options.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/s3_to_redshift.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nS3ToRedshiftOperator(\n    task_id=\"transfer_s3_to_redshift_multiple_keys\",\n    schema=\"public\",\n    table=\"s3_to_redshift_multiple_keys\",\n    s3_bucket=\"{{ BUCKET_NAME }}\",\n    s3_key=[f\"prefix_{i}.json\" for i in range(2)],\n    redshift_conn_id=\"aws_default\",\n    aws_conn_id=\"aws_default\",\n    copy_options=[\"json\", \"auto\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Using BigQueryTablePartitionExistenceSensor in Standard Mode\nDESCRIPTION: Demonstrates how to use the BigQueryTablePartitionExistenceSensor to check that a table and its partition exist in BigQuery. For DAY partitioned tables, the partition_id parameter is a string in the \"%Y%m%d\" format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nbigquery_table_partition_sensor = BigQueryTablePartitionExistenceSensor(\n    task_id=\"bq_table_partition_sensor\",\n    project_id=GCP_PROJECT_ID,\n    dataset_id=DATASET_NAME,\n    table_id=TABLE_NAME,\n    partition_id=PARTITION_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: CloudSQL Create Operator Templated Fields (Python)\nDESCRIPTION: Shows which fields in CloudSQLCreateInstanceOperator are Jinja-templated in Airflow, supporting parameterization for deployments. Useful when defining DAGs for multi-environment or dynamic provisioning. Presents the operator's template_fields tuple, requiring the Airflow Google provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"template_fields = (\\\"project_id\\\", \\\"body\\\")\"\n```\n\n----------------------------------------\n\nTITLE: Submitting Spark Batch Jobs with AnalyticDBSparkBatchOperator - Python\nDESCRIPTION: This code demonstrates how to define and configure Airflow DAG tasks that use AnalyticDBSparkBatchOperator to submit Spark Pi and Logistic Regression jobs to Alibaba Cloud AnalyticDB Spark. It requires the Airflow installation with Alibaba Cloud provider package, and an appropriately configured connection to AnalyticDB Spark. Key parameters include the application name, job configuration, and resource details. Expected input is the job definition, and the output is the asynchronous submission and management of Spark jobs. Users must configure credentials and connections to successfully run these tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/operators/analyticdb_spark.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_adb_spark_batch\",\n    start_date=datetime(2021, 1, 1),\n    schedule_interval=None,\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    submit_spark_pi = AnalyticDBSparkBatchOperator(\n        task_id=\"submit_spark_pi\",\n        application=\"org.apache.spark.examples.SparkPi\",\n        main_class=\"org.apache.spark.examples.SparkPi\",\n        arguments=[\"1000\"],\n        resource_config={\n            \"executor_count\": 2,\n            \"executor_memory\": \"2G\",\n        },\n        conn_id=\"adb_spark_default\",\n    )\n\n    submit_logistic_regression = AnalyticDBSparkBatchOperator(\n        task_id=\"submit_logistic_regression\",\n        application=\"org.apache.spark.examples.mllib.LogisticRegressionWithSGDExample\",\n        main_class=\"org.apache.spark.examples.mllib.LogisticRegressionWithSGDExample\",\n        arguments=[],\n        resource_config={\n            \"executor_count\": 2,\n            \"executor_memory\": \"2G\",\n        },\n        conn_id=\"adb_spark_default\",\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL Database and User for Airflow\nDESCRIPTION: This SQL script creates a PostgreSQL database and user for Airflow, granting necessary privileges.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-up-database.rst#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE DATABASE airflow_db;\nCREATE USER airflow_user WITH PASSWORD 'airflow_pass';\nGRANT ALL PRIVILEGES ON DATABASE airflow_db TO airflow_user;\n\n-- PostgreSQL 15 requires additional privileges:\n-- Note: Connect to the airflow_db database before running the following GRANT statement\n-- You can do this in psql with: \\c airflow_db\nGRANT ALL ON SCHEMA public TO airflow_user;\n```\n\n----------------------------------------\n\nTITLE: Generating SHA512 Checksums and ASC Signatures in Shell\nDESCRIPTION: Navigates into the 'dist' directory, runs a signing script (`dev/sign.sh`) to generate SHA512 checksums and ASC signature files for the Helm chart tarball and source tarball, and then returns to the previous directory. Assumes `${AIRFLOW_REPO_ROOT}` points to the repository root.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npushd ${AIRFLOW_REPO_ROOT}/dist\n${AIRFLOW_REPO_ROOT}/dev/sign.sh airflow-*.tgz airflow-*-source.tar.gz\npopd\n```\n\n----------------------------------------\n\nTITLE: Installing System Dependencies for Apache Airflow on Debian Bookworm\nDESCRIPTION: This bash command installs the necessary system-level dependencies for Apache Airflow on Debian Bookworm (12). It includes various libraries and tools required for Airflow's functionality, such as SSL support, LDAP, Kerberos, and database connectors.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/dependencies.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y --no-install-recommends apt-utils ca-certificates \\\n    curl dumb-init freetds-bin krb5-user libgeos-dev \\\n    ldap-utils libsasl2-2 libsasl2-modules libxmlsec1 locales libffi8 libldap-2.5-0 libssl3 netcat-openbsd \\\n    lsb-release openssh-client python3-selinux rsync sasl2-bin sqlite3 sudo unixodbc\n```\n\n----------------------------------------\n\nTITLE: Using Deferrable ADF Pipeline Status Sensor in Airflow (Python)\nDESCRIPTION: This example demonstrates using the `AzureDataFactoryPipelineRunStatusSensor` in deferrable mode (`deferrable=True`). When used with an asynchronously triggered pipeline, this sensor polls for the pipeline status using the Airflow Triggerer, freeing up worker slots. It requires the pipeline `run_id` and an Azure Data Factory connection (`azure_data_factory_conn_id`).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/adf_run_pipeline.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../tests/system/microsoft/azure/example_adf_run_pipeline.py\n#     :language: python\n#     :dedent: 0\n#     :start-after: [START howto_operator_adf_run_pipeline_async]\n#     :end-before: [END howto_operator_adf_run_pipeline_async]\n\n# Example (simulated based on description - highlighting deferrable sensor):\nfrom airflow.providers.microsoft.azure.operators.data_factory import AzureDataFactoryRunPipelineOperator\nfrom airflow.providers.microsoft.azure.sensors.data_factory import AzureDataFactoryPipelineRunStatusSensor\n\nrun_pipeline_async = AzureDataFactoryRunPipelineOperator(\n    task_id=\"run_pipeline_async\",\n    pipeline_name=\"pipeline1\",\n    azure_data_factory_conn_id=\"azure_data_factory_default\",\n    wait_for_termination=False,\n)\n\npipeline_run_sensor_defer = AzureDataFactoryPipelineRunStatusSensor(\n    task_id=\"pipeline_run_sensor_defer\",\n    run_id=run_pipeline_async.output[\"run_id\"],\n    azure_data_factory_conn_id=\"azure_data_factory_default\",\n    deferrable=True, # Key aspect for this example context\n)\n\nrun_pipeline_async >> pipeline_run_sensor_defer\n\n```\n\n----------------------------------------\n\nTITLE: Updating Dataproc Metastore Service using Airflow Operator in Python\nDESCRIPTION: Demonstrates using the `DataprocMetastoreUpdateServiceOperator` in an Airflow DAG to modify an existing Google Cloud Dataproc Metastore service. Requires the `service_id`, `location` (region), `project_id`, the `service` dictionary containing updates, and the `update_mask` specifying fields to change.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nupdate_service = DataprocMetastoreUpdateServiceOperator(\n    task_id=\"update_service\",\n    service_id=SERVICE_ID,\n    location=REGION,\n    project_id=PROJECT_ID,\n    service=SERVICE_TO_UPDATE,\n    update_mask=UPDATE_MASK,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Translation Glossary with TranslateCreateGlossaryOperator in Python\nDESCRIPTION: Example of using the `TranslateCreateGlossaryOperator` in an Airflow DAG to create a translation glossary using the Cloud Translate V3 API. It requires `project_id`, `location`, `glossary_id`, and `glossary_input_config` (specifying the GCS path to the glossary file).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncreate_glossary = TranslateCreateGlossaryOperator(\n    task_id=\"create_glossary\",\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n    glossary_id=GLOSSARY_ID,\n    glossary_input_config=GLOSSARY_INPUT_CONFIG,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Custom Timetable Plugin Structure\nDESCRIPTION: Basic skeleton for implementing a custom timetable as an Airflow plugin. Shows the required class structure and plugin registration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/timetable.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.plugins_manager import AirflowPlugin\nfrom airflow.timetables.base import Timetable\n\n\nclass AfterWorkdayTimetable(Timetable):\n    pass\n\n\nclass WorkdayTimetablePlugin(AirflowPlugin):\n    name = \"workday_timetable_plugin\"\n    timetables = [AfterWorkdayTimetable]\n```\n\n----------------------------------------\n\nTITLE: Configuring GCP Project for System Tests\nDESCRIPTION: Command to set the GCP project for running system tests using gcloud CLI. This is required if the environment is not already configured to use a particular GCP project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/tests/system/google/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngcloud config set project <project-id>\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow from PyPI with Constraints\nDESCRIPTION: This command installs Apache Airflow version 2.10.5 using pip, with constraints specified to ensure a repeatable installation. The constraints file is fetched from the official Airflow GitHub repository.\nSOURCE: https://github.com/apache/airflow/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow==2.10.5' \\\n --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.5/constraints-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Forcing Breeze Autocomplete Setup\nDESCRIPTION: Command to force reinstall Breeze autocomplete configuration\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/01_installation.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbreeze setup autocomplete --force\n```\n\n----------------------------------------\n\nTITLE: Installing Edge3 Provider Package for Apache Airflow\nDESCRIPTION: This command installs the Edge3 provider package on top of an existing Airflow 2 installation. It requires a minimum Airflow version of 2.10.0 and supports Python versions 3.9, 3.10, 3.11, and 3.12.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/edge3/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-edge3\n```\n\n----------------------------------------\n\nTITLE: Implementing CLI Commands in a Custom Auth Manager\nDESCRIPTION: Pseudo-code example showing how an auth manager can provide CLI commands to the Airflow command line tool by implementing the get_cli_commands method.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/auth-manager/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\ndef get_cli_commands() -> list[CLICommand]:\n    sub_commands = [\n        ActionCommand(\n            name=\"command_name\",\n            help=\"Description of what this specific command does\",\n            func=lazy_load_command(\"path.to.python.function.for.command\"),\n            args=(),\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Google Cloud Workflow Execution using Airflow Operator in Python\nDESCRIPTION: Demonstrates fetching the details and status of a specific workflow execution using the `WorkflowsGetExecutionOperator`. Requires `workflow_id`, `location_id`, and `execution_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/workflows.rst#2025-04-22_snippet_8\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/workflows/example_workflows.py\n      :language: python\n      :dedent: 4\n      :start-after: [START how_to_get_execution]\n      :end-before: [END how_to_get_execution]\n```\n\n----------------------------------------\n\nTITLE: KubernetesPodOperator Configuration Example\nDESCRIPTION: Code example showing core configuration parameters supported by KubernetesPodOperator including active_deadline_seconds, cluster_context, and termination_message_policy\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nKubernetesPodOperator(\n    active_deadline_seconds=600,\n    cluster_context='my-context',\n    termination_message_policy='File',\n    on_finish_action='delete_pod',\n    get_logs=True,\n    container_logs=True\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting Google Cloud VertexAI Dataset Operator Usage in Airflow (Python)\nDESCRIPTION: Provides an example of how to use DeleteDatasetOperator to delete a VertexAI dataset within an Airflow DAG. Needs proper GCP connection, with required dataset_id, project, and location. The operator takes dataset_id (from earlier creation or listing) and removes the corresponding VertexAI dataset. No output is produced except deletion confirmation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    delete_dataset = DeleteDatasetOperator(\n        task_id=\"delete_dataset\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        dataset_id=DATASET_ID,\n        gcp_conn_id=GCP_CONN_ID,\n    )\n```\n\n----------------------------------------\n\nTITLE: Removing Product from Product Set - Google Cloud Vision Airflow Operator - Python\nDESCRIPTION: This Python snippet demonstrates how to explicitly remove a product from a product set using the Airflow Google Cloud Vision operator by referencing an example system test. Dependencies include the Airflow provider for Google Cloud and the proper configuration of Google credentials. Key parameters are the product set and product identifiers; the input consists of the relevant resource names and the output is a confirmation or status of the removal operation. The snippet is designed for integration in an Airflow pipeline and requires access to Google Cloud Vision APIs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nremove_product_from_product_set = CloudVisionRemoveProductFromProductSetOperator(\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n    product_set_id=GCP_VISION_PRODUCT_SET_ID,\n    product_id=GCP_VISION_PRODUCT_ID,\n    task_id=\"remove_product_from_product_set\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Granting Standard Airflow User Permissions to a User Group in Cedar (User Role)\nDESCRIPTION: This Cedar policy provides standard user permissions to members of a specific AWS IAM Identity Center group (ID `aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee`). It includes read access (GET) to various resources and modification permissions (POST, PUT, DELETE) specifically for DAGs. This set of permissions corresponds to the 'User' role in the default Flask AppBuilder authentication manager.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/manage/index.rst#2025-04-22_snippet_3\n\nLANGUAGE: cedar\nCODE:\n```\npermit(\n  principal in Airflow::Group::\"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\",\n  action in [\n    Airflow::Action::\"Configuration.GET\",\n    Airflow::Action::\"Connection.GET\",\n    Airflow::Action::\"Custom.GET\",\n    Airflow::Action::\"Dag.GET\",\n    Airflow::Action::\"Menu.MENU\",\n    Airflow::Action::\"Pool.GET\",\n    Airflow::Action::\"Variable.GET\",\n    Airflow::Action::\"Asset.GET\",\n    Airflow::Action::\"View.GET\",\n    Airflow::Action::\"Dag.POST\",\n    Airflow::Action::\"Dag.PUT\",\n    Airflow::Action::\"Dag.DELETE\",\n  ],\n  resource\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow to Use Custom Auth Manager in INI\nDESCRIPTION: This snippet shows how to configure Airflow to use a custom auth manager by setting the core.auth_manager configuration value in the Airflow configuration file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/auth-manager/index.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n    [core]\n    auth_manager = my_company.auth_managers.MyCustomAuthManager\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom AWS Session Factory in Airflow Settings (INI)\nDESCRIPTION: This INI snippet demonstrates how to configure Airflow's AWS provider to use a custom session factory implementation. By setting the `session_factory` key under the `[aws]` section (typically in `airflow.cfg` or via environment variable `AIRFLOW__AWS__SESSION_FACTORY`), you specify the full Python path to your custom class derived from `BaseSessionFactory`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_16\n\nLANGUAGE: ini\nCODE:\n```\n[aws]\nsession_factory = my_company.aws.MyCustomSessionFactory\n```\n\n----------------------------------------\n\nTITLE: Starting AWS DMS Serverless Replication using DmsStartReplicationOperator\nDESCRIPTION: This snippet shows how to use the DmsStartReplicationOperator to initiate an AWS DMS serverless replication based on a previously created configuration. It requires the ARN of the replication configuration and specifies the start type (e.g., 'start-replication'). Depends on the airflow.providers.amazon.aws.operators.dms module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dms.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Example using DmsStartReplicationOperator\n# from airflow.providers.amazon.aws.operators.dms import DmsStartReplicationOperator\n\n# start_replication = DmsStartReplicationOperator(\n#     task_id=\"start_dms_serverless_replication\",\n#     replication_config_arn=\"arn:aws:dms:us-east-1:123456789012:replication-config:XYZABC123DEF\",\n#     start_replication_type=\"start-replication\",\n#     # ... other parameters\n# )\n```\n\n----------------------------------------\n\nTITLE: Setting Snowflake Connection with URI in Bash Environment Variable\nDESCRIPTION: This Bash command sets the Airflow Snowflake connection as an environment variable using a URI. It specifies user credentials, schema, and various Snowflake parameters as query arguments. All URI components should be URL-encoded to avoid parsing errors. This method is suitable for Airflow versions prior to 2.3.0 or environments requiring URI-style connection configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/connections/snowflake.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SNOWFLAKE_DEFAULT='snowflake://user:password@/db-schema?account=account&database=snow-db&region=us-east&warehouse=snow-warehouse'\n```\n\n----------------------------------------\n\nTITLE: Configuring a Spark Job for Google Cloud Dataproc\nDESCRIPTION: This code defines a configuration for a Spark job to be submitted to a Dataproc cluster. It specifies the main class, jar file, and arguments for the Spark job.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nSPARK_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"spark_job\": {\n        \"main_class\": \"org.apache.spark.examples.SparkPi\",\n        \"jar_file_uris\": [\"file:///usr/lib/spark/examples/jars/spark-examples.jar\"],\n        \"args\": [\"1000\"],\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting a Google Cloud Workflow using Airflow Operator in Python\nDESCRIPTION: Illustrates how to delete a workflow using the `WorkflowsDeleteWorkflowOperator`. Requires identifying the workflow to be deleted using `workflow_id` and `location_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/workflows.rst#2025-04-22_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/workflows/example_workflows.py\n      :language: python\n      :dedent: 4\n      :start-after: [START how_to_delete_workflow]\n      :end-before: [END how_to_delete_workflow]\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Allow List in Airflow\nDESCRIPTION: Configuration for specifying an allow list of metric prefixes in Airflow. This limits metrics collection to only those that start with the elements in the list.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/metrics.rst#2025-04-22_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\n[metrics]\nmetrics_allow_list = scheduler,executor,dagrun,pool,triggerer,celery\n```\n\n----------------------------------------\n\nTITLE: Creating Connection with Unencoded Special Characters in Python\nDESCRIPTION: This snippet shows an incorrect way of creating a Connection object with a password containing a '/' character. It results in a ValueError due to improper parsing of the URI.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_12\n\nLANGUAGE: pycon\nCODE:\n```\n>>> c = Connection(uri=\"my-conn-type://my-login:my-pa/ssword@my-host:5432/my-schema?param1=val1&param2=val2\")\nValueError: invalid literal for int() with base 10: 'my-pa'\n```\n\n----------------------------------------\n\nTITLE: Creating Databricks Job Using Named Parameters\nDESCRIPTION: Example demonstrating the use of DatabricksCreateJobsOperator with named parameters instead of JSON payload\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/jobs_create.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncreate_job_named = DatabricksCreateJobsOperator(\n    task_id=\"create_named\",\n    name=\"{{ task_instance.task_id }}\",\n    description=\"Created via the REST API v2.1\",\n    tasks=[\n        {\n            \"task_key\": \"task1\",\n            \"notebook_task\": {\n                \"notebook_path\": \"/Shared/MyNotebook\",\n            },\n            \"new_cluster\": DEFAULT_CLUSTER,\n        }\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Per-Service Endpoint URLs in Connection Extra Field (JSON)\nDESCRIPTION: This JSON snippet for the 'Extra' field allows setting custom AWS service endpoint URLs. A global 'endpoint_url' can be set (not used by STS client for assume role/test connection), and specific endpoints can be defined per service (e.g., S3, STS) within 'service_config'. Setting a service endpoint to `null` enforces default Boto3 behavior for that service. Per-service settings override the global setting.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"endpoint_url\": \"s3.amazonaws.com\"\n  \"service_config\": {\n    \"s3\": {\n      \"endpoint_url\": \"https://s3.eu-west-1.amazonaws.com\"\n    },\n    \"sts\": {\n      \"endpoint_url\": \"https://sts.eu-west-2.amazonaws.com\"\n    },\n    \"ec2\": {\n      \"endpoint_url\": null\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Python Client Package\nDESCRIPTION: Command to build the Python client package using Breeze release management tools. Builds both distribution formats with a dev suffix for PyPI.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/python_client_tests.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-python-client --distribution-format both\\n          --version-suffix-for-pypi dev0 --python-client-repo ./airflow-client-python\n```\n\n----------------------------------------\n\nTITLE: Installing Kueue in a GKE Cluster with GKEStartKueueInsideClusterOperator\nDESCRIPTION: Example of using the GKEStartKueueInsideClusterOperator to install a specific version of Kueue, a Cloud Native Job scheduler, inside a Google Kubernetes Engine cluster.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstart_kueue = GKEStartKueueInsideClusterOperator(\n    task_id=\"start_kueue\",\n    cluster_name=CLUSTER_NAME,\n    location=LOCATION,\n    namespace=\"default\",\n    version=\"v0.5.1\",\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Default Extras for Apache Airflow Docker Image\nDESCRIPTION: This snippet provides a list of default extras included in the production Dockerfile for Apache Airflow. These extras represent additional features and integrations that are pre-installed in the Docker image.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build-arg-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n* aiobotocore\n* amazon\n* async\n* celery\n* cncf-kubernetes\n* common-io\n* common-messaging\n* docker\n* elasticsearch\n* fab\n* ftp\n* git\n* google\n* google-auth\n* graphviz\n* grpc\n* hashicorp\n* http\n* ldap\n* microsoft-azure\n* mysql\n* odbc\n* openlineage\n* pandas\n* postgres\n* redis\n* sendgrid\n* sftp\n* slack\n* snowflake\n* ssh\n* statsd\n* uv\n```\n\n----------------------------------------\n\nTITLE: Implementing Asset Alias DAGs in Airflow Python\nDESCRIPTION: Demonstrates the implementation of producer and consumer DAGs using asset aliases. Shows how to create DAGs that produce regular assets, produce asset aliases, consume specific assets, and consume asset aliases. The example illustrates the relationship between asset aliases and their resolved assets.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/asset-scheduling.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(dag_id=\"asset-producer\"):\n\n    @task(outlets=[Asset(\"example-alias\")])\n    def produce_asset_events():\n        pass\n\n\nwith DAG(dag_id=\"asset-alias-producer\"):\n\n    @task(outlets=[AssetAlias(\"example-alias\")])\n    def produce_asset_events(*, outlet_events):\n        outlet_events[AssetAlias(\"example-alias\")].add(Asset(\"s3://bucket/my-task\"))\n\n\nwith DAG(dag_id=\"asset-consumer\", schedule=Asset(\"s3://bucket/my-task\")):\n    ...\n\nwith DAG(dag_id=\"asset-alias-consumer\", schedule=AssetAlias(\"example-alias\")):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Creating AutoML Video Training Job in Vertex AI\nDESCRIPTION: Example showing how to create an AutoML video training job using the CreateAutoMLVideoTrainingJobOperator. Requires a pre-created Video dataset.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nCreateAutoMLVideoTrainingJobOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    display_name=TRAINING_JOB_NAME,\n    dataset_id=VIDEO_DATASET,\n    prediction_type=\"classification\",\n    training_budget_milli_node_hours=8000,\n    sync=True,\n    task_id=\"training_job\",)\n```\n\n----------------------------------------\n\nTITLE: Defining PostgreSQL Connection URI with SSL Extras in Bash\nDESCRIPTION: This Bash command demonstrates how to set an environment variable (AIRFLOW_CONN_POSTGRES_DEFAULT) to define an Airflow PostgreSQL connection using the URI format. The example includes standard connection details (user, password, host, port, database) and passes extra parameters like SSL settings (sslmode, sslcert, sslkey, sslrootcert) as URL-encoded query parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/connections/postgres.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_POSTGRES_DEFAULT='postgresql://postgres_user:XXXXXXXXXXXX@1.1.1.1:5432/postgresdb?sslmode=verify-ca&sslcert=%2Ftmp%2Fclient-cert.pem&sslkey=%2Ftmp%2Fclient-key.pem&sslrootcert=%2Ftmp%2Fserver-ca.pem'\n```\n\n----------------------------------------\n\nTITLE: Monitoring Dataplex Data Profile Job Status with Airflow Sensor\nDESCRIPTION: Uses the DataplexDataProfileJobStatusSensor to monitor the status of a Dataplex Data Profile scan job. This sensor checks if a data profile scan job has successfully completed in Google Cloud Dataplex.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nwait_for_dp_job = DataplexDataProfileJobStatusSensor(\n    task_id=\"wait_for_dp_job\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    data_scan_id=DATA_PROFILE_SCAN_ID,\n    job_id=\"{{ task_instance.xcom_pull('run_data_profile')['job']['name'].split('/')[-1] }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using JWT Token for Airflow API Request\nDESCRIPTION: This bash snippet demonstrates how to use the obtained JWT token to make an authenticated GET request to the Airflow API. The token is included in the Authorization header as a Bearer token.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/api.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nENDPOINT_URL=\"http://localhost:8080/\"\ncurl -X GET ${ENDPOINT_URL}/api/v2/dags \\\n  -H \"Authorization: Bearer <JWT-TOKEN>\"\n```\n\n----------------------------------------\n\nTITLE: Creating Text Embeddings with CohereEmbeddingOperator in Apache Airflow (Python)\nDESCRIPTION: This code demonstrates how to use the CohereEmbeddingOperator within an Apache Airflow DAG to generate text embeddings using Cohere's API. It requires specifying the 'input_text' parameter with the input string or list, and setting 'conn_id' to reference your Cohere connection in Airflow. The output is an embedding vector response from the Cohere API; ensure the Cohere connection exists in your Airflow environment and that the input text meets Cohere API constraints.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cohere/docs/operators/embedding.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"# Example Airflow DAG using CohereEmbeddingOperator\\nfrom airflow import DAG\\nfrom airflow.providers.cohere.operators.embedding import CohereEmbeddingOperator\\nfrom datetime import datetime\\n\\ndefault_args = {\\n    'start_date': datetime(2024, 1, 1),\\n}\\n\\nwith DAG(\\n    'example_cohere_embedding_operator',\\n    schedule_interval=None,\\n    catchup=False,\\n    default_args=default_args,\\n    tags=['example'],\\n) as dag:\\n\\n    create_embedding = CohereEmbeddingOperator(\\n        task_id='create_embedding',\\n        conn_id='cohere_default',\\n        input_text=[\\\"Apache Airflow is a platform to programmatically author, schedule and monitor workflows.\\\"],\\n    )\\n\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Databricks Repo using DatabricksReposCreateOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the DatabricksReposCreateOperator to create a Databricks Repo. It specifies the Git URL, provider, and other optional parameters such as branch and repo path.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/repos_create.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_repo = DatabricksReposCreateOperator(\n    task_id='create_repo',\n    git_url='https://github.com/apache/airflow-dev-tools',\n    git_provider='github',\n    branch='main',\n    repo_path='/Repos/user@example.com/airflow-dev-tools',\n    ignore_existing_repo=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Cloud Build from Storage\nDESCRIPTION: Example showing how to create a new Cloud Build from Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_build.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncreate_build_from_storage = CloudBuildCreateBuildOperator(\n    task_id=\"create_build_from_storage\",\n    project_id=PROJECT_ID,\n    build=CREATE_BUILD_FROM_STORAGE_BODY,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Auth Manager via Environment Variables in Airflow (Bash)\nDESCRIPTION: This snippet demonstrates configuring Apache Airflow's AWS authentication manager using environment variables in a Bash shell. Set `AIRFLOW__CORE__AUTH_MANAGER` to specify the auth manager class and `AIRFLOW__AWS_AUTH_MANAGER__REGION_NAME` for the AWS region, replacing `<region_name>` accordingly. This method is an alternative to INI configuration and often used in containerized environments.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/setup/config.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__CORE__AUTH_MANAGER='airflow.providers.amazon.aws.auth_manager.aws_auth_manager.AwsAuthManager'\nexport AIRFLOW__AWS_AUTH_MANAGER__REGION_NAME='<region_name>'\n```\n\n----------------------------------------\n\nTITLE: DAG Loading Time Measurement Results\nDESCRIPTION: Sample output from the time command showing real, user, and system time for DAG loading. The real time metric (0m0.699s) indicates the total wall-clock time taken to process the DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\nreal    0m0.699s\nuser    0m0.590s\nsys     0m0.108s\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Connection with JSON in Apache Airflow\nDESCRIPTION: This JSON snippet demonstrates how to configure an AWS connection in Airflow using a role ARN and assume role method with web identity federation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"role_arn\": \"arn:aws:iam::240057002457:role/WebIdentity-Role\",\n  \"assume_role_method\": \"assume_role_with_web_identity\",\n  \"assume_role_with_web_identity_federation\": \"google\",\n  \"assume_role_with_web_identity_federation_audience\": \"service_a.apache.com\"\n}\n```\n\n----------------------------------------\n\nTITLE: Transferring Large Trino Query Results in Chunks to Google Cloud Storage using Airflow Operator in Python\nDESCRIPTION: Demonstrates configuring Airflow's TrinoToGCSOperator to split large exports into multiple files, using the approx_max_file_size_bytes parameter. This facilitates managing GCS file size limits and parallel processing in downstream tasks. The code sets a small chunk size (e.g., 10 MB) for finer splitting. Requires Airflow, proper provider packages, and permissions to write to GCS. The operator will automatically create multiple numbered files in the bucket as required.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/trino_to_gcs.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrino_to_gcs_chunked = TrinoToGCSOperator(\n    task_id=\"trino_to_gcs_chunked\",\n    sql=\"SELECT * FROM large_table;\",\n    bucket=\"my-gcs-bucket\",\n    filename=\"large_export_{}.json\",\n    approx_max_file_size_bytes=10 * 1024 * 1024,  # 10 MB\n)\n```\n\n----------------------------------------\n\nTITLE: Creating RDS Database Snapshot\nDESCRIPTION: Creates a snapshot of an Amazon RDS database instance or cluster using RDSCreateDBSnapshotOperator. The source database must be in 'available' or 'storage-optimization' state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_snapshot = RDSCreateDBSnapshotOperator(\n    task_id=\"create_snapshot\",\n    db_type=\"instance\",\n    db_identifier=DB_INSTANCE_NAME,\n    db_snapshot_identifier=DB_SNAPSHOT_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting AWS DMS Serverless Replication Config using DmsDeleteReplicationConfigOperator\nDESCRIPTION: This snippet demonstrates using the DmsDeleteReplicationConfigOperator to remove an AWS DMS serverless replication configuration. It requires the ARN of the configuration to be deleted. The associated replication must be stopped first. Depends on the airflow.providers.amazon.aws.operators.dms module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dms.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Example using DmsDeleteReplicationConfigOperator\n# from airflow.providers.amazon.aws.operators.dms import DmsDeleteReplicationConfigOperator\n\n# delete_repl_config = DmsDeleteReplicationConfigOperator(\n#     task_id=\"delete_dms_serverless_replication_config\",\n#     replication_config_arn=\"arn:aws:dms:us-east-1:123456789012:replication-config:XYZABC123DEF\",\n#     # ... other parameters\n# )\n```\n\n----------------------------------------\n\nTITLE: Starting Celery Worker for Apache Airflow\nDESCRIPTION: Command to start a Celery worker for Apache Airflow, which will begin picking up tasks as they are assigned.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/celery_executor.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairflow celery worker\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Secrets Manager Backend with IAM Role\nDESCRIPTION: Example configuration for AWS Secrets Manager backend using IAM role authentication. It specifies prefixes for connections, variables, and config, along with the IAM role ARN.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-secrets-manager.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend\nbackend_kwargs = {\n  \"connections_prefix\": \"airflow/connections\",\n  \"variables_prefix\": \"airflow/variables\",\n  \"config_prefix\": \"airflow/config\",\n  \"role_arn\": \"arn:aws:iam::123456789098:role/role-name\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining OpenLineage Facet Methods in Airflow Operators (Python)\nDESCRIPTION: Defines the required method signatures (`get_openlineage_facets_on_start`, `get_openlineage_facets_on_complete`, `get_openlineage_facets_on_failure`) within an Airflow Operator. These methods are called at specific task lifecycle stages (RUNNING, SUCCESS, FAILED respectively) to gather lineage information. They are expected to return an `OperatorLineage` object. Implementing at least `get_openlineage_facets_on_start` or `get_openlineage_facets_on_complete` is mandatory for lineage extraction via this mechanism.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/developer.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_openlineage_facets_on_start() -> OperatorLineage: ...\n\n\ndef get_openlineage_facets_on_complete(ti: TaskInstance) -> OperatorLineage: ...\n\n\ndef get_openlineage_facets_on_failure(ti: TaskInstance) -> OperatorLineage: ...\n```\n\n----------------------------------------\n\nTITLE: Using Google Cloud SQL Query Operator in Python\nDESCRIPTION: This snippet illustrates how to use the Google Cloud SQL Query Operator in Apache Airflow. It shows how to reference a previously prepared connection, either from the Airflow database or configured via environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_cloudsql_query_operators]\n# [END howto_operator_cloudsql_query_operators]\n```\n\n----------------------------------------\n\nTITLE: Overriding Operator Links of Existing Operators in Python\nDESCRIPTION: This snippet demonstrates how to override a built-in link on an existing operator (BigQueryExecuteQueryOperator) using an Airflow plugin. It defines a custom BigQueryDatasetLink class and registers it through an AirflowExtraLinkPlugin.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/define-extra-link.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import BaseOperator, BaseOperatorLink\nfrom airflow.models.taskinstancekey import TaskInstanceKey\nfrom airflow.plugins_manager import AirflowPlugin\nfrom airflow.providers.google.cloud.operators.bigquery import BigQueryOperator\n\n# Change from https to http just to display the override\nBIGQUERY_JOB_DETAILS_LINK_FMT = \"http://console.cloud.google.com/bigquery?j={job_id}\"\n\n\nclass BigQueryDatasetLink(BaseGoogleLink):\n    \"\"\"\n    Helper class for constructing BigQuery Dataset Link.\n    \"\"\"\n\n    name = \"BigQuery Dataset\"\n    key = \"bigquery_dataset\"\n    format_str = BIGQUERY_DATASET_LINK\n\n    @staticmethod\n    def persist(\n        context: Context,\n        task_instance: BaseOperator,\n        dataset_id: str,\n        project_id: str,\n    ):\n        task_instance.xcom_push(\n            context,\n            key=BigQueryDatasetLink.key,\n            value={\"dataset_id\": dataset_id, \"project_id\": project_id},\n        )\n\n\n# Defining the plugin class\nclass AirflowExtraLinkPlugin(AirflowPlugin):\n    name = \"extra_link_plugin\"\n    operator_extra_links = [\n        BigQueryDatasetLink(),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Managing Build Triggers\nDESCRIPTION: Examples showing how to create and delete Cloud Build triggers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_build.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncreated_trigger = CloudBuildCreateBuildTriggerOperator(\n    task_id=\"create_build_trigger\",\n    project_id=PROJECT_ID,\n    trigger=TRIGGER_BODY,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nCloudBuildDeleteBuildTriggerOperator(\n    task_id=\"delete_build_trigger\", \n    project_id=PROJECT_ID,\n    trigger_id=created_trigger[\"id\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Dataplex Entries using Airflow Python\nDESCRIPTION: This snippet shows how to list all Entries within a specific Entry Group in Google Cloud Dataplex Catalog using the `DataplexCatalogListEntriesOperator` in an Airflow DAG. Filtering and ordering are supported. It references an external example file for implementation details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_55\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_list_entries]\n#     :end-before: [END howto_operator_dataplex_catalog_list_entries]\n\n# This example uses DataplexCatalogListEntriesOperator.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Verifying Apache Airflow Package Signatures - Bash\nDESCRIPTION: Commands for verifying package signatures using gpg, pgpv, or pgp after downloading the signature (.asc) files.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/installing-helm-chart-from-sources.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngpg --verify airflow-********.asc airflow-*********\n```\n\nLANGUAGE: bash\nCODE:\n```\npgpv airflow-********.asc\n```\n\nLANGUAGE: bash\nCODE:\n```\npgp airflow-********.asc\n```\n\n----------------------------------------\n\nTITLE: Deleting a Tag Template with CloudDataCatalogDeleteTagTemplateOperator in Python\nDESCRIPTION: Example of deleting a tag template in Google Cloud DataCatalog using the CloudDataCatalogDeleteTagTemplateOperator. Jinja templating can be used to dynamically determine parameter values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_delete_tag_template]\ndelete_tag_template = CloudDataCatalogDeleteTagTemplateOperator(\n    task_id=\"delete_tag_template\",\n    location=LOCATION,\n    tag_template=\"{tag_template}\",\n    force=True,\n)\n# [END howto_operator_gcp_datacatalog_delete_tag_template]\n```\n\n----------------------------------------\n\nTITLE: Waiting on AWS Batch Job State using BatchSensor in Python\nDESCRIPTION: Illustrates using the Airflow BatchSensor (:class:`~airflow.providers.amazon.aws.sensors.batch.BatchSensor`) to pause DAG execution until a specific AWS Batch job reaches a terminal state (e.g., SUCCEEDED, FAILED). This sensor polls the job status. Using the 'deferrable=True' parameter enables asynchronous monitoring, releasing the worker slot, but requires the triggerer component.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/batch.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../amazon/tests/system/amazon/aws/example_batch.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_sensor_batch]\n    :end-before: [END howto_sensor_batch]\n```\n\n----------------------------------------\n\nTITLE: Deleting Vertex AI Model using Python\nDESCRIPTION: This code snippet demonstrates how to delete a Vertex AI model using the DeleteModelOperator in Airflow. It specifies the project ID, region, and model ID for the delete operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/automl.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nDeleteModelOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    model_id=model_id,\n    task_id=\"delete_model\",\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced Asset Scheduling with Conditional Expressions in Python\nDESCRIPTION: Illustrates how to use logical operators (AND, OR) to create complex asset-based scheduling conditions for DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/asset-scheduling.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndag1_asset = Asset(\"s3://dag1/output_1.txt\")\ndag2_asset = Asset(\"s3://dag2/output_1.txt\")\n\nwith DAG(\n    # Consume asset 1 and 2 with asset expressions\n    schedule=(dag1_asset & dag2_asset),\n    ...,\n):\n    ...\n\nwith DAG(\n    # Consume asset 1 or 2 with asset expressions\n    schedule=(dag1_asset | dag2_asset),\n    ...,\n):\n    ...\n\ndag3_asset = Asset(\"s3://dag3/output_3.txt\")\n\nwith DAG(\n    # Consume asset 1 or both 2 and 3 with asset expressions\n    schedule=(dag1_asset | (dag2_asset & dag3_asset)),\n    ...,\n):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Starting Dataflow Classic Template Job with Airflow\nDESCRIPTION: Illustrates using the `DataflowTemplatedJobStartOperator` in Airflow to run a Dataflow job based on a pre-staged Classic Template stored in Cloud Storage. This separates pipeline development from execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_template.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_start_template_job]\n    :end-before: [END howto_operator_start_template_job]\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiple Cluster Policy Rules\nDESCRIPTION: A comprehensive approach to applying multiple policy rules by aggregating them in a separate module and collecting all violations before raising an exception.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/cluster-policies.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport importlib\n\nfrom airflow.exceptions import AirflowClusterPolicyViolation\n\n# Create a list to collect errors\npolicy_errors = []\n\n# List all your custom policy functions here. These are imported from a separate file.\nRULES_MODULE = \"my_company.airflow.policy_rules\"\n\nTASK_RULES = [\n    \"task_must_have_owners\",\n    \"task_must_have_correct_pool\",\n    \"task_must_have_retries\",\n]\n\n\ndef task_policy(task):\n    \"\"\"\n    Apply all task rules from the rules module to the task, and raise a single exception if there are any\n    violations.\n    \"\"\"\n    # Local errors for this task\n    errors = []\n\n    # Dynamically load all the rules from the rules module\n    rules_module = importlib.import_module(RULES_MODULE)\n    for rule_name in TASK_RULES:\n        rule = getattr(rules_module, rule_name)\n        # Apply the rule to the task\n        try:\n            rule(task)\n        except AirflowClusterPolicyViolation as ex:\n            # Collect the error message\n            errors.append(str(ex))\n\n    # If we had any errors, combine them and raise a single exception\n    if errors:\n        error_str = \"\\n\".join(errors)\n        raise AirflowClusterPolicyViolation(f\"Task policy violations:\\n{error_str}\")\n```\n\n----------------------------------------\n\nTITLE: Waiting for an EMR Serverless Job State with Airflow Sensor - Python\nDESCRIPTION: Demonstrates use of EmrServerlessJobSensor in Airflow to monitor the status of an EMR Serverless job until completion or failure. Requires airflow.providers.amazon.aws, and aiobotocore if deferrable=True. Accepts application_id and job_run_id as parameters; emits an event or exception based on the observed state. Useful for synchronizing downstream tasks on AWS job completion.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr_serverless.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.amazon.aws.sensors.emr import EmrServerlessJobSensor\n\nwait_for_emr_serverless_job = EmrServerlessJobSensor(\n    task_id=\"wait_for_emr_serverless_job\",\n    application_id=\"{{ task_instance.xcom_pull(task_ids='create_emr_serverless_application') }}\",\n    job_run_id=\"{{ task_instance.xcom_pull(task_ids='start_emr_serverless_job') }}\",\n    deferrable=True,  # Optional\n    dag=dag,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Granting GCS Permissions for Cloud SQL Export in Python\nDESCRIPTION: Example of using GCSBucketCreateAclEntryOperator to grant the Cloud SQL instance's service account WRITE permissions on the GCS bucket for exporting data.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ngrant_bucket_access = GCSBucketCreateAclEntryOperator(\n    bucket=BUCKET_NAME,\n    entity=f\"user-{CLOUD_SQL_SERVICE_ACCOUNT_EMAIL}\",\n    role=\"WRITER\",\n    task_id=\"grant_bucket_access\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow for Stdout JSON Logging\nDESCRIPTION: Configuration in airflow.cfg to enable writing task logs to stdout in JSON format. This is useful for containerized environments and log forwarding.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/logging/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\nremote_logging = True\n\n[opensearch]\nwrite_stdout = True\njson_format = True\n```\n\n----------------------------------------\n\nTITLE: Creating AlloyDB Backup with Airflow Operator\nDESCRIPTION: Uses AlloyDBCreateBackupOperator to create a backup of an AlloyDB instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/alloy_db.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncreate_backup = AlloyDBCreateBackupOperator(\n    task_id=\"create_backup\",\n    project_id=GCP_PROJECT_ID,\n    cluster_id=CLUSTER_ID,\n    backup_id=BACKUP_ID,\n    body=ALLOYDB_BACKUP,\n    region=GCP_ALLOYDB_REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Dataproc Cluster Config with ClusterGenerator in Python\nDESCRIPTION: This snippet shows how to generate a Dataproc cluster configuration using the ClusterGenerator class. It demonstrates creating a config with specific node types and counts.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nCLUSTER_CONFIG = ClusterGenerator(\n    project_id=PROJECT_ID,\n    zone=ZONE,\n    master_machine_type=\"n1-standard-4\",\n    worker_machine_type=\"n1-standard-4\",\n    num_workers=2,\n    worker_disk_size=300,\n    master_disk_size=300,\n    storage_bucket=f\"{PROJECT_ID}-airflow-gke-session\",\n).make()\n```\n\n----------------------------------------\n\nTITLE: Running Go Pipeline with DataflowRunner in Async Mode using GCS File\nDESCRIPTION: This example demonstrates running a Go pipeline on Google Cloud Dataflow with asynchronous execution using a GCS file. It configures both pipeline options and Dataflow-specific settings through the dataflow_config parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_go_dataflow_runner_async_gcs_file = BeamRunGoPipelineOperator(\n    task_id=\"beam_task_go_dataflow_runner_async_gcs_file\",\n    go_file=\"{{ var.json.beam_variables.gcs_go_file_path }}\",\n    runner=\"DataflowRunner\",\n    pipeline_options={\n        \"output\": \"{{ var.json.beam_variables.output_path }}\",\n    },\n    dataflow_config={\n        \"job_name\": \"{{task.task_id}}\",\n        \"project_id\": \"{{ var.json.beam_variables.gcp_project }}\",\n        \"location\": \"{{ var.json.beam_variables.gcp_region }}\",\n        \"staging_location\": \"{{ var.json.beam_variables.gcp_dataflow_staging }}\",\n        \"temp_location\": \"{{ var.json.beam_variables.gcp_dataflow_temp }}\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Sharepoint Site with MSGraphAsyncOperator (Python)\nDESCRIPTION: This snippet demonstrates how to use the MSGraphAsyncOperator in Airflow to retrieve data about a Sharepoint site via the Microsoft Graph API. It requires the Airflow Microsoft Azure provider, appropriate credentials, and network access to Microsoft Graph. The main parameter is the API endpoint for the Sharepoint site, and the output is the response content of the specified site's metadata. No output transformation is applied; the raw API response is returned to Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/msgraph.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsite = MSGraphAsyncOperator(\n    task_id=\"get_sharepoint_site\",\n    endpoint=\"sites/root\",\n    msgraph_conn_id=\"msgraph_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Submitting Multiple Databricks Tasks with Python\nDESCRIPTION: Example of using DatabricksSubmitRunOperator to submit multiple Databricks tasks in a single run using the 'tasks' parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/submit_run.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntasks = [\n    {\n        \"new_cluster\": {\"spark_version\": \"2.1.0-db3-scala2.11\", \"num_workers\": 2},\n        \"notebook_task\": {\"notebook_path\": \"/Users/airflow@example.com/PrepareData\"},\n    }\n]\nnotebook_run = DatabricksSubmitRunOperator(task_id=\"notebook_run\", tasks=tasks)\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Secrets Manager for Specific Lookups\nDESCRIPTION: Configuration example for AWS Secrets Manager backend that only looks up connections and excludes variables and config. It sets the connections prefix and nullifies the variables and config prefixes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-secrets-manager.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend\nbackend_kwargs = {\n  \"connections_prefix\": \"airflow/connections\",\n  \"variables_prefix\": null,\n  \"config_prefix\": null,\n  \"profile_name\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Spanner Database in Python\nDESCRIPTION: Examples of using SpannerUpdateDatabaseInstanceOperator to run DDL queries in a Cloud Spanner database. The operator can be created with or without a project ID and with or without an operation_id for idempotency checks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/spanner.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_spanner_database_update]\n# Without project_id\nspanner_database_update = SpannerUpdateDatabaseInstanceOperator(\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    database_id=\"data_database\",\n    ddl_statements=[\"CREATE TABLE data (test_id INT64 NOT NULL) PRIMARY KEY (test_id)\"],\n    task_id=\"update_database\",\n)\n\n# With project_id\nspanner_database_update = SpannerUpdateDatabaseInstanceOperator(\n    project_id=\"{{ var.value.spanner_project }}\",\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    database_id=\"data_database\",\n    ddl_statements=[\"CREATE TABLE data (test_id INT64 NOT NULL) PRIMARY KEY (test_id)\"],\n    task_id=\"update_database\",\n)\n# [END howto_operator_spanner_database_update]\n\n# [START howto_operator_spanner_database_update_idempotent]\n# With project_id and operation_id\nspanner_database_update = SpannerUpdateDatabaseInstanceOperator(\n    project_id=\"{{ var.value.spanner_project }}\",\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    database_id=\"data_database\",\n    operation_id=\"update_table_schema\",\n    ddl_statements=[\"CREATE TABLE data (test_id INT64 NOT NULL) PRIMARY KEY (test_id)\"],\n    task_id=\"update_database\",\n)\n# [END howto_operator_spanner_database_update_idempotent]\n```\n\n----------------------------------------\n\nTITLE: Creating SageMaker Transform Job\nDESCRIPTION: Example showing how to use SageMakerTransformOperator to create a batch transform job in Amazon SageMaker.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntransform_task = SageMakerTransformOperator(\n    task_id='transform_task',\n    config={\n        \"TransformJobName\": \"demo-transform-job\",\n        \"ModelName\": \"demo-model\",\n        \"TransformInput\": {},\n        \"TransformOutput\": {},\n        \"TransformResources\": {},\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Processing Pull Request Data into DataFrame in Python\nDESCRIPTION: This snippet iterates through the loaded pull request data, extracting various metrics and scores for each PR. It creates a pandas DataFrame with these metrics and concatenates them into a single DataFrame 'rows'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/stats/explore_pr_candidates.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrows = pd.DataFrame()\n\nfor pr_stat in selected_prs:\n    data = {\n        \"number\": [pr_stat.pull_request.number],\n        \"url\": [pr_stat.pull_request.html_url],\n        \"title\": [pr_stat.pull_request.title],\n        \"overall_score\": [pr_stat.score],\n        \"label_score\": [pr_stat.label_score],\n        \"length_score\": [pr_stat.length_score],\n        \"body_length\": [pr_stat.body_length],\n        \"comment_length\": [pr_stat.comment_length],\n        \"interaction_score\": [pr_stat.interaction_score],\n        \"comments\": [pr_stat.num_comments],\n        \"reactions\": [pr_stat.num_reactions],\n        \"reviews\": [pr_stat.num_reviews],\n        \"num_interacting_users\": [pr_stat.num_interacting_users],\n        \"change_score\": [pr_stat.change_score],\n        \"additions\": [pr_stat.num_additions],\n        \"deletions\": [pr_stat.num_deletions],\n        \"num_changed_files\": [pr_stat.num_changed_files],\n    }\n    df = pd.DataFrame(data)\n    rows = pd.concat([df, rows]).reset_index(drop=True)\n```\n\n----------------------------------------\n\nTITLE: Deleting Alerts with OpsgenieDeleteAlertOperator in Airflow\nDESCRIPTION: Shows how to use OpsgenieDeleteAlertOperator to completely remove an alert from Opsgenie. This operator helps manage the alert lifecycle by allowing permanent removal of alerts.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opsgenie/docs/operators/opsgenie_alert.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Ingesting Weaviate Data without Vectors from Callable using Python\nDESCRIPTION: This snippet illustrates using the `WeaviateIngestOperator` to ingest data from a Python callable (`get_data_without_vectors_callable`) into a Weaviate collection (`ClassName`). The operator will automatically generate embedding vectors for the text found in the specified `vector_col` ('content'). It relies on the `weaviate_default` connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/weaviate/docs/operators/weaviate.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ningest_data_without_vectors_from_callable = WeaviateIngestOperator(\n    task_id=\"ingest_data_without_vectors_from_callable\",\n    conn_id=\"weaviate_default\",\n    class_name=\"ClassName\",\n    input_data=get_data_without_vectors_callable(),\n    vector_col=\"content\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using BashOperator for Bash Commands in Airflow\nDESCRIPTION: Shows how to use the classic BashOperator to execute Bash commands in Airflow. This method is an alternative to the @task.bash decorator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nt1 = BashOperator(\n    task_id=\"print_date\",\n    bash_command=\"date\",\n)\n\nt2 = BashOperator(\n    task_id=\"sleep\",\n    bash_command=\"sleep 5\",\n)\n\nt3 = BashOperator(\n    task_id=\"print_hello\",\n    bash_command='echo \"hello world!!\"',\n)\n\nt1 >> [t2, t3]\n```\n\n----------------------------------------\n\nTITLE: Using CloudVideoIntelligenceDetectVideoExplicitContentOperator to Detect Explicit Content (Python)\nDESCRIPTION: This snippet demonstrates execution of the CloudVideoIntelligenceDetectVideoExplicitContentOperator in an Airflow DAG for explicit content annotation on a GCS video. It references the shared OTHER_ARGS for input parameters and specifies the result_path for annotation results. Prerequisites include Airflow with Google integration and a GCS video URI. The output is pushed to XCom or result_path for further analysis.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/video_intelligence.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndetect_explicit_content = CloudVideoIntelligenceDetectVideoExplicitContentOperator(\n    task_id=\"detect_explicit_content\",\n    location=GCP_LOCATION,\n    **OTHER_ARGS,\n    result_path=RESULT_PATH,\n)\n```\n\n----------------------------------------\n\nTITLE: Migrating Apache Airflow Transfer Operator Import Paths\nDESCRIPTION: This code snippet illustrates the migration of import paths for transfer operators in Apache Airflow. It covers various data transfer operations between different systems such as Hive, MySQL, S3, and Redshift.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41368.significant.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Old import path  New import path\n\"airflow.operators.hive_to_mysql.HiveToMySqlTransfer\"  \"airflow.providers.apache.hive.transfers.hive_to_mysql.HiveToMySqlOperator\"\n\"airflow.operators.mssql_to_hive.MsSqlToHiveTransfer\"  \"airflow.providers.apache.hive.transfers.mssql_to_hive.MsSqlToHiveOperator\"\n\"airflow.operators.mysql_to_hive.MySqlToHiveTransfer\"  \"airflow.providers.apache.hive.transfers.mysql_to_hive.MySqlToHiveOperator\"\n\"airflow.operators.presto_to_mysql.PrestoToMySqlTransfer\"  \"airflow.providers.mysql.transfers.presto_to_mysql.PrestoToMySqlOperator\"\n\"airflow.operators.s3_to_hive_operator.S3ToHiveTransfer\"  \"airflow.providers.apache.hive.transfers.s3_to_hive.S3ToHiveOperator\"\n\"airflow.operators.s3_to_redshift_operator.S3ToRedshiftTransfer\"  \"airflow.providers.amazon.aws.transfers.s3_to_redshift.S3ToRedshiftOperator\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Airflow Variables in Python\nDESCRIPTION: Demonstrates how to import and use the Variable model to retrieve values from Airflow's variable store. Shows basic retrieval, JSON deserialization, and default value handling.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/variables.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import Variable\n\n# Normal call style\nfoo = Variable.get(\"foo\")\n\n# Auto-deserializes a JSON value\nbar = Variable.get(\"bar\", deserialize_json=True)\n\n# Returns the value of default (None) if the variable is not set\nbaz = Variable.get(\"baz\", default=None)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Google Cloud Dataproc Cluster\nDESCRIPTION: This code demonstrates how to delete a Dataproc cluster using the DataprocDeleteClusterOperator. This is typically done when the cluster is no longer needed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndelete_cluster = DataprocDeleteClusterOperator(\n    task_id=\"delete_cluster\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster_name=CLUSTER_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Using BigQueryTablePartitionExistenceSensor in Deferrable Mode\nDESCRIPTION: Shows how to use the BigQueryTablePartitionExistenceSensor in deferrable mode to free up worker slots while the sensor is running.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nbigquery_table_partition_sensor_defered = BigQueryTablePartitionExistenceSensor(\n    task_id=\"bq_table_partition_sensor_defered\",\n    project_id=GCP_PROJECT_ID,\n    dataset_id=DATASET_NAME,\n    table_id=TABLE_NAME,\n    partition_id=PARTITION_NAME,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying gRPC Authentication Types in Airflow\nDESCRIPTION: Lists the possible values for the 'Auth Type' parameter when configuring a gRPC connection in Airflow. `NO_AUTH` uses an insecure channel. `SSL` or `TLS` requires a credential pem file. `JWT_GOOGLE` and `OATH_GOOGLE` use Google default credentials (OAuth requires scopes). `CUSTOM` allows providing a custom function to return a gRPC channel.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/connections/grpc.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nNO_AUTH\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nSSL\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nTLS\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nJWT_GOOGLE\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nOATH_GOOGLE\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nCUSTOM\n```\n\n----------------------------------------\n\nTITLE: Sending Opsgenie Alert using OpsgenieNotifier in Apache Airflow\nDESCRIPTION: This code snippet demonstrates how to use the OpsgenieNotifier class to send an alert to Opsgenie with a specific message. It shows the setup of the notifier with required parameters and how to trigger the alert within a task.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opsgenie/docs/notifications/opsgenie_notifier.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.decorators import task\nfrom airflow.providers.opsgenie.notifications.opsgenie import OpsgenieNotifier\n\nopsgenie_notifier = OpsgenieNotifier(\n    opsgenie_conn_id=\"opsgenie_conn_id\",\n    message=\"Example Opsgenie Message\",\n    description=\"Example Opsgenie Description\",\n)\n\n@task(on_failure_callback=opsgenie_notifier.notify)\ndef opsgenie_notifier_task():\n    raise Exception(\"Example fail\")\n\nopsgenie_notifier_task()\n```\n\n----------------------------------------\n\nTITLE: Running a Daily-Filtered AppFlow Flow Using AppflowRunDailyOperator - Apache Airflow - Python\nDESCRIPTION: Shows how to schedule an AppFlow run to load only daily records using AppflowRunDailyOperator within an Airflow DAG. Useful for incremental data loads from supported sources, mainly Salesforce. Must configure schedule, flow name, and ensure dependencies including Airflow Amazon provider and boto3. Inputs include the flow_name and run date context (inferred from DAG). Output is execution that only processes records corresponding to the current day.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/appflow.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_appflow_run_daily\",\n    schedule_interval=None,\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"appflow\"],\n) as dag:\n\n    run_flow_daily = AppflowRunDailyOperator(\n        task_id=\"appflow_run_daily_task\",\n        flow_name=\"your_appflow_flow_name\",\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-common-sql via pip (Bash)\nDESCRIPTION: This code installs the 'apache-airflow-providers-common-sql' provider for Apache Airflow using pip. It should be run in a terminal with an existing compatible Airflow installation. The command pulls the latest version from PyPI that matches local requirements and dependencies. Python 3.9 through 3.12 are supported for this provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-common-sql\n```\n\n----------------------------------------\n\nTITLE: Creating Dataproc Batch with Airflow Operator in Python\nDESCRIPTION: This snippet demonstrates how to use the `DataprocCreateBatchOperator` in an Airflow DAG to submit a new batch job to Google Cloud Dataproc. Key parameters include `project_id`, `region`, `batch` (containing the batch configuration), and `batch_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# Code extracted from: /../../google/tests/system/google/cloud/dataproc/example_dataproc_batch.py\n# Between markers: [START how_to_cloud_dataproc_create_batch_operator] and [END how_to_cloud_dataproc_create_batch_operator]\n# \n# Example using DataprocCreateBatchOperator(...)\n# ... (actual Python code would be here)\n\n```\n\n----------------------------------------\n\nTITLE: Installing Additional Dependencies with Pinned Airflow Version\nDESCRIPTION: Example of installing additional providers while pinning Airflow version to prevent accidental upgrades/downgrades.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-pypi.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"apache-airflow==|version|\" apache-airflow-providers-google==10.1.0\n```\n\n----------------------------------------\n\nTITLE: Configuring sudo permissions for Airflow impersonation\nDESCRIPTION: This snippet shows a simple sudoers file entry to allow the Airflow user to impersonate other users. It grants the Airflow user full sudo privileges without requiring a password.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/workload.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nairflow ALL=(ALL) NOPASSWD: ALL\n```\n\n----------------------------------------\n\nTITLE: Configuring SQLite Connection in JSON Format for Airflow\nDESCRIPTION: This example demonstrates how to configure a SQLite connection using JSON format in Airflow. It includes the connection type, host (relative path), and extra parameters for read-only mode.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/connections/sqlite.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SQLITE_DEFAULT='{\\n    \"conn_type\": \"sqlite\",\\n    \"host\": \"relative/path/to/db\",\\n    \"extra\": {\\n        \"mode\": \"ro\"\\n    }\\n}'\n```\n\n----------------------------------------\n\nTITLE: Updating Dataproc Cluster with Python\nDESCRIPTION: This snippet shows how to update a Dataproc cluster by providing a new cluster configuration and an update mask. It demonstrates scaling the cluster by modifying the number of worker instances.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nCLUSTER_CONFIG = {\n    \"config\": {\n        \"worker_config\": {\n            \"num_instances\": 3,\n        },\n    }\n}\n\nUPDATE_MASK = {\"paths\": [\"config.worker_config.num_instances\"]}\n```\n\n----------------------------------------\n\nTITLE: Task Dependencies with Chain Method in Python\nDESCRIPTION: Example of defining task dependencies using the chain() method with section comments to distinguish between setup, body, and teardown stages of the test.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/tests/system/amazon/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nchain(\n    # TEST SETUP\n    test_context,\n    create_database,\n    create_table,\n    # TEST BODY\n    copy_selected_data,\n    # TEST TEARDOWN\n    delete_database,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Language Name Specification to PapermillOperator\nDESCRIPTION: Feature (Version 3.0.0): Enhances PapermillOperator to allow specifying the language name for the notebook execution, referencing pull request #23916.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Add support to specify language name in PapermillOperator (#23916)``\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS SSM Parameter Store Backend in Airflow\nDESCRIPTION: This snippet shows how to configure the AWS SSM Parameter Store as a secrets backend in the Airflow configuration file. It specifies the backend class and various prefixes for connections, variables, and config.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-ssm-parameter-store.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.amazon.aws.secrets.systems_manager.SystemsManagerParameterStoreBackend\nbackend_kwargs = {\n  \"connections_prefix\": \"airflow/connections\",\n  \"connections_lookup_pattern\": null,\n  \"variables_prefix\": \"airflow/variables\",\n  \"variables_lookup_pattern\": null,\n  \"config_prefix\": \"airflow/config\",\n  \"config_lookup_pattern\": null,\n  \"profile_name\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Remote Logging in Airflow via OSS Backend - INI Configuration\nDESCRIPTION: This snippet demonstrates how to configure 'airflow.cfg' to enable remote log storage in Alibaba OSS for Apache Airflow. Required dependencies include an Airflow installation with logging enabled and a valid connection (referenced by 'remote_log_conn_id') set up in your Airflow connections. The key parameters are 'remote_logging' (enables the feature), 'remote_base_log_folder' (OSS URL as log storage backend), and 'remote_log_conn_id' (Airflow connection ID granting OSS access). The system expects a valid OSS URL and access credentials; logs will be unavailable remotely if the configuration or connection is missing.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/logging/oss-task-handler.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\\n# Airflow can store logs remotely in Alibaba OSS. Users must supply a remote\\n# location URL (starting with either 'oss://...') and an Airflow connection\\n# id that provides access to the storage location.\\nremote_logging = True\\nremote_base_log_folder = oss://my-bucket/path/to/logs\\nremote_log_conn_id = oss_default\n```\n\n----------------------------------------\n\nTITLE: Exporting Data from Cloud SQL in Python\nDESCRIPTION: Example of using CloudSQLExportInstanceOperator to export data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump or CSV file. Shows both regular and deferrable mode usage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsql_export = CloudSQLExportInstanceOperator(\n    instance=INSTANCE_NAME,\n    body=export_body,\n    task_id=\"sql_export\",\n)\n\nsql_export_with_project = CloudSQLExportInstanceOperator(\n    project_id=GCP_PROJECT_ID,\n    instance=INSTANCE_NAME,\n    body=export_body,\n    task_id=\"sql_export_with_project\",\n)\n\nsql_export_async = CloudSQLExportInstanceOperator(\n    instance=INSTANCE_NAME,\n    body=export_body,\n    task_id=\"sql_export_async\",\n    deferrable=True,\n)\n\nsql_export_with_project_async = CloudSQLExportInstanceOperator(\n    project_id=GCP_PROJECT_ID,\n    instance=INSTANCE_NAME,\n    body=export_body,\n    task_id=\"sql_export_with_project_async\",\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Templating Fields for GCE Instance Template Copy Operator in Python\nDESCRIPTION: Defines the template fields available for the ComputeEngineCopyInstanceTemplateOperator, which allow for runtime parameter substitution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"project_id\",\n    \"source_instance_template\",\n    \"destination_instance_template\",\n    \"update_mask\",\n    \"request_id\",\n    \"gcp_conn_id\",\n    \"api_version\",\n    \"impersonation_chain\",\n)\n```\n\n----------------------------------------\n\nTITLE: Running a Databricks Notebook on an Existing Cluster in Python\nDESCRIPTION: This example shows how to use the DatabricksNotebookOperator to run a Databricks notebook on an existing cluster. It includes configuration for specifying the existing cluster ID and notebook parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/notebook.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../databricks/tests/system/databricks/example_databricks.py\n    :language: python\n    :start-after: [START howto_operator_databricks_notebook_existing_cluster]\n    :end-before: [END howto_operator_databricks_notebook_existing_cluster]\n```\n\n----------------------------------------\n\nTITLE: Defining Policy Functions with Pluggy Entrypoints\nDESCRIPTION: Example of creating a policy function using the Pluggy interface with setuptools entrypoints, allowing for more modular and maintainable policy implementations.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/cluster-policies.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.policies import hookimpl\n\n\n@hookimpl\ndef task_policy(task) -> None:\n    # Mutate task in place\n    # ...\n    print(f\"Hello from {__file__}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Trino Provider via pip for Airflow (Bash)\nDESCRIPTION: This snippet demonstrates how to install the apache-airflow-providers-trino package with the common.sql extra using pip. It requires an existing Apache Airflow 2 installation that supports pip package installations. The command specifies the installation of optional cross-provider dependencies needed for common SQL features. Expected input is the pip command, with the output being the updated Python environment. Ensure pip is available and the Airflow version matches requirement constraints before running this command.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/trino/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-trino[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Speech-to-Text Operator Template Fields\nDESCRIPTION: Definition of template fields available for the Speech-to-Text operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/speech_to_text.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n[START gcp_speech_to_text_synthesize_template_fields]\n[END gcp_speech_to_text_synthesize_template_fields]\n```\n\n----------------------------------------\n\nTITLE: Correcting Template Fields Renderer for HiveOperator\nDESCRIPTION: This code fixes a key typo in the 'template_fields_renderers' for the 'HiveOperator', ensuring correct template rendering.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n\"Fix key typo in 'template_fields_renderers' for 'HiveOperator' (#21525)\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataplex Task Synchronously - Python\nDESCRIPTION: Uses the DataplexCreateTaskOperator to create a Dataplex task synchronously. Requires Airflow's Google Dataplex provider and a properly defined task_config. Inputs are operator parameters such as lake_id, body, and project_id; output is the created task resource or execution status.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"create_task = DataplexCreateTaskOperator(\\n    task_id=\\\"create_task\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    lake_id=LAKE_ID,\\n    body=task_config,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Google Cloud Run Job using CloudRunCreateJobOperator in Airflow\nDESCRIPTION: Demonstrates how to use the CloudRunCreateJobOperator to create a Cloud Run job in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_run.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncreate_job = CloudRunCreateJobOperator(\n    task_id=\"create_job\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    job=job,\n    job_id=JOB_NAME\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Yandex Lockbox Backend via Airflow Connection ID (INI)\nDESCRIPTION: Shows configuring `backend_kwargs` in `airflow.cfg` to leverage an existing Yandex Cloud connection defined within Airflow for authentication. The `yc_connection_id` parameter specifies the ID of the connection to use. If omitted, `yandexcloud_default` is used.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_8\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend_kwargs = {\"yc_connection_id\": \"my_yc_connection\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring .airflowignore File with Glob Syntax\nDESCRIPTION: Example .airflowignore file using glob syntax to exclude specific DAG files and directories from Airflow processing. The pattern matches will ignore project_a related files and tenant numbered directories.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dags.rst#2025-04-22_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\n**/*project_a*\ntenant_[0-9]*\n```\n\n----------------------------------------\n\nTITLE: Installing Core Airflow without Provider Constraints\nDESCRIPTION: Command to upgrade Airflow core while keeping existing provider dependencies using no-providers constraint file.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/07_local_virtualenv.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install -e \".[devel]\" \\\n  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-main/constraints-no-providers-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Installing Kubernetes Provider Package via PIP\nDESCRIPTION: Command to install the Kubernetes provider package for Apache Airflow 2.x\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-cncf-kubernetes\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Beam Provider with Google Extra via Pip (Shell)\nDESCRIPTION: This shell command shows how to install the Apache Airflow Apache Beam provider including its optional 'google' extra using pip. This command installs the necessary dependencies for integrating Apache Beam workflows with Google Cloud services managed via the Google provider. Similar to the corresponding Google provider command, the documentation warns that using pip <= 20.2.4 might lead to dependency conflicts affecting BigQuery functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-apache-beam[google]\n```\n\n----------------------------------------\n\nTITLE: Building Docs for a Specific Provider Package with Breeze\nDESCRIPTION: This command builds the documentation for a specific provider package using its PACKAGE_ID in the Breeze development environment.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/docs/README.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs PACKAGE_ID\n```\n\n----------------------------------------\n\nTITLE: Setting Webserver Configuration in Airflow Helm Chart\nDESCRIPTION: Example showing how to expose the Airflow configuration in the webserver UI by setting the expose_config parameter to True in the Helm chart values. This demonstrates setting custom configuration options under the config key.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/airflow-configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  webserver:\n    expose_config: 'True'  # by default this is 'False'\n```\n\n----------------------------------------\n\nTITLE: Deleting Product with Google Cloud Vision Operator in Airflow\nDESCRIPTION: Demonstrates how to use CloudVisionDeleteProductOperator to delete a product by retrieving the ID from XCOM.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndelete_product = CloudVisionDeleteProductOperator(\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    product_id=\"{{ task_instance.xcom_pull('create_product')['name'].split('/')[-1] }}\",\n    retry=Retry(maximum=10.0),\n    timeout=5,\n    task_id=\"delete_product\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating AlloyDB Instance with Airflow Operator\nDESCRIPTION: Uses AlloyDBCreateInstanceOperator to create an AlloyDB instance (primary and secondary) in Google Cloud.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/alloy_db.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncreate_instance = AlloyDBCreateInstanceOperator(\n    task_id=\"create_instance\",\n    project_id=GCP_PROJECT_ID,\n    cluster_id=CLUSTER_ID,\n    instance_id=INSTANCE_ID,\n    instance=ALLOYDB_INSTANCE,\n    region=GCP_ALLOYDB_REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Cross-Provider Dependencies for OpenLineage (Bash)\nDESCRIPTION: This command demonstrates how to install the 'apache-airflow-providers-common-sql' provider package along with optional dependencies for OpenLineage functionality. The '[openlineage]' extra ensures all necessary packages for OpenLineage integration are installed. This is required for workflows that utilize monitoring or lineage tracking with OpenLineage and should be run in a supporting Python environment with Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-common-sql[openlineage]\n```\n\n----------------------------------------\n\nTITLE: Listing Apache Kafka Topics with ManagedKafkaListTopicsOperator in Python\nDESCRIPTION: This snippet demonstrates how to list Apache Kafka topics using the ManagedKafkaListTopicsOperator. It specifies the project ID, region, and cluster.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlist_topics = ManagedKafkaListTopicsOperator(\n    task_id=\"list_topics\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n)\n```\n\n----------------------------------------\n\nTITLE: Updating an Entry in Dataplex Catalog using Python\nDESCRIPTION: This code snippet shows how to use the DataplexCatalogUpdateEntryOperator to update an existing Entry in a specific location in the Dataplex Catalog. It requires the appropriate Airflow provider and Google Cloud credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_57\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_dataplex_catalog_update_entry]\nupdate_entry = DataplexCatalogUpdateEntryOperator(\n    task_id=\"update_entry\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    entry_id=ENTRY_ID,\n    entry=ENTRY,\n    location=LOCATION,\n)\n# [END howto_operator_dataplex_catalog_update_entry]\n```\n\n----------------------------------------\n\nTITLE: Transferring JSON Data from S3 to Teradata using S3ToTeradataOperator in Python\nDESCRIPTION: This Python snippet shows an example configuration of the `S3ToTeradataOperator` in an Airflow DAG for transferring JSON formatted data from AWS S3 to a Teradata table. The operator leverages Teradata's READ_NOS functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/operators/s3_to_teradata.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../teradata/tests/system/teradata/example_s3_to_teradata_transfer.py\n    :language: python\n    :start-after: [START s3_to_teradata_transfer_operator_howto_guide_transfer_data_s3_to_teradata_json]\n    :end-before: [END s3_to_teradata_transfer_operator_howto_guide_transfer_data_s3_to_teradata_json]\n```\n\n----------------------------------------\n\nTITLE: Setting S3 Bucket Tags with S3PutBucketTaggingOperator\nDESCRIPTION: Demonstrates how to set tags for an S3 bucket using the S3PutBucketTaggingOperator. Tags are key-value pairs that help in bucket organization and management.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/s3.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nput_bucket_tagging = S3PutBucketTaggingOperator(\n    task_id=\"put_bucket_tagging\",\n    bucket_name=BUCKET_NAME,\n    key_values={\"key1\": \"value1\", \"key2\": \"value2\"},\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Dataplex Entry Types using Airflow Python\nDESCRIPTION: This snippet shows how to list all Entry Types within a specific location in Google Cloud Dataplex Catalog using the `DataplexCatalogListEntryTypesOperator` in an Airflow DAG. The operator supports filtering and ordering of results. It references an external example file for the implementation details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_43\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_list_entry_types]\n#     :end-before: [END howto_operator_dataplex_catalog_list_entry_types]\n\n# This example uses DataplexCatalogListEntryTypesOperator.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Moving Hive Macros to Provider in Python\nDESCRIPTION: Relocates Hive macros from core Airflow to the Apache Hive provider, centralizing Hive-related functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n\"Move Hive macros to the provider (#28538)\"\n```\n\n----------------------------------------\n\nTITLE: DingDing User Mentions in Messages\nDESCRIPTION: Demonstrates how to mention specific users or all users in a DingDing message using at_mobiles and at_all parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nremind_specific_user = DingdingOperator(\n    task_id=\"remind_specific_user\",\n    message_type=\"text\",\n    message=\"This is a test message sent from Apache Airflow\",\n    at_mobiles=[\"156XXXXXXXX\"],\n    at_all=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Docker Image with DAGs\nDESCRIPTION: Dockerfile commands to create an Airflow image that includes DAG files. This is the basic approach for Airflow 2.0.2 and later versions.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build --pull --tag \"my-company/airflow:8a0da78\" . -f - <<EOF\nFROM apache/airflow\n\nCOPY ./dags/ \\${AIRFLOW_HOME}/dags/\n\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Livy Connection Using Environment Variables in Bash\nDESCRIPTION: Example of how to set up a Livy connection through an environment variable using URI syntax. The connection includes username, password, host, port, and optional headers which should be URL-encoded.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/livy/docs/connections.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_LIVY_DEFAULT='http://username:password@livy-server.com:80?headers=header'\n```\n\n----------------------------------------\n\nTITLE: Updating Kylin Provider Version in Python\nDESCRIPTION: This code snippet shows how to update the version number of the Apache Kylin provider package for Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kylin/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"apache-airflow-providers-apache-kylin\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Updating Dataplex Entry Type using Airflow Python\nDESCRIPTION: This snippet shows how to update an existing Entry Type in a specific location within Google Cloud Dataplex Catalog using the `DataplexCatalogUpdateEntryTypeOperator` in an Airflow DAG. It requires providing the updated configuration and references an external example file for the implementation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_45\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_update_entry_type]\n#     :end-before: [END howto_operator_dataplex_catalog_update_entry_type]\n\n# This example uses DataplexCatalogUpdateEntryTypeOperator.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Setting Kubernetes Connection Using URI Format in Bash\nDESCRIPTION: Example of setting a Kubernetes connection string using environment variable in URI format. Demonstrates configuration of in-cluster mode, kubeconfig path, and namespace settings.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/connections/kubernetes.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_CONN_KUBERNETES_DEFAULT='kubernetes://?in_cluster=True&kube_config_path=~%2F.kube%2Fconfig&kube_config=kubeconfig+json&namespace=namespace'\n```\n\n----------------------------------------\n\nTITLE: Listing Dataplex Catalog Entry Groups with Airflow Operator\nDESCRIPTION: Uses the DataplexCatalogListEntryGroupsOperator to list all Entry Groups in a specific location. This operator supports filtering and ordering the results of entry groups in Google Cloud Dataplex Catalog.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nlist_entry_groups = DataplexCatalogListEntryGroupsOperator(\n    task_id=\"list_entry_groups\",\n    project_id=PROJECT_ID,\n    location=REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Cloud Build Triggers Using Airflow Operator - Python\nDESCRIPTION: Utilizes CloudBuildListBuildTriggersOperator in an Airflow DAG to list all existing build triggers for a given Google Cloud project. Must be run with proper Google provider extras and valid credentials. Accepts project ID as a parameter and outputs the result to XCom, supporting further automation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_build.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlist_build_triggers = CloudBuildListBuildTriggersOperator(\n    task_id=\"list_build_triggers\",\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Using CloudVideoIntelligenceDetectVideoLabelsOperator to Detect Video Labels (Python)\nDESCRIPTION: This snippet shows how to invoke the CloudVideoIntelligenceDetectVideoLabelsOperator in an Airflow DAG to perform label annotation on a video file stored in Google Cloud Storage. It leverages pre-defined OTHER_ARGS for input parameters, and the output is pushed to XCom for downstream consumption. Required dependencies are Airflow with Google provider, and a proper service account with access rights. The operator accepts parameters such as input_uri and optionally video_context, and outputs annotation results.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/video_intelligence.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndetect_video_labels = CloudVideoIntelligenceDetectVideoLabelsOperator(\n    task_id=\"detect_video_labels\",\n    location=GCP_LOCATION,\n    **OTHER_ARGS,\n    result_path=RESULT_PATH,\n)\n```\n\n----------------------------------------\n\nTITLE: Moving Hive Configuration to Apache Hive Provider in Python\nDESCRIPTION: Relocates Hive configuration settings from the core Airflow to the dedicated Apache Hive provider for better modularity.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n\"Move Hive configuration to Apache Hive provider (#32777)\"\n```\n\n----------------------------------------\n\nTITLE: Implementing TimeDeltaSensorAsync in Apache Airflow\nDESCRIPTION: This snippet shows the usage of TimeDeltaSensorAsync, an asynchronous version of TimeDeltaSensor. It creates a sensor that waits for 5 days from the start date and requires a Triggerer to run.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/datetime.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nTimeDeltaSensorAsync(\n    task_id=\"wait_for_5_days_async\",\n    delta=timedelta(days=5),\n)\n```\n\n----------------------------------------\n\nTITLE: Using Cluster Resources with KubernetesPodOperator in Python\nDESCRIPTION: Shows how to add ConfigMaps, Volumes, and other Kubernetes native objects to a KubernetesPodOperator using the Kubernetes model API.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kubernetes.client import models as k8s\n```\n\n----------------------------------------\n\nTITLE: Wait for AWS Glue Data Quality Recommendation Example\nDESCRIPTION: Example showing how to wait for an AWS Glue Data Quality recommendation run to reach terminal state using GlueDataQualityRuleRecommendationRunSensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/glue.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n[START howto_sensor_glue_data_quality_rule_recommendation_run]\n[END howto_sensor_glue_data_quality_rule_recommendation_run]\n```\n\n----------------------------------------\n\nTITLE: Resuming a Transfer Operation with Airflow and Google Cloud Storage Transfer Service in Python\nDESCRIPTION: This code illustrates the use of CloudDataTransferServiceResumeOperationOperator in Airflow to resume a previously paused transfer operation in Google Cloud Storage Transfer Service. Required fields include the operation name, project ID, and connection information. Like other operators, it supports template fields for dynamic parameterization. The output confirms the successful resumption of an operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresume_operation = CloudDataTransferServiceResumeOperationOperator(\n    task_id=\"resume_operation\",\n    operation_name=OPERATION_NAME,\n    project_id=PROJECT_ID,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"operation_name\",\n    \"project_id\",\n    \"gcp_conn_id\",\n)\n```\n\n----------------------------------------\n\nTITLE: Consuming Data from Apache Kafka Topic with ConsumeFromTopicOperator in Python\nDESCRIPTION: This code shows how to consume data from an Apache Kafka topic using the ConsumeFromTopicOperator. It requires the topic, kafka_config, and consumer function configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nconsume_from_topic = ConsumeFromTopicOperator(\n    task_id=\"consume_from_topic\",\n    topics=[TOPIC],\n    kafka_config=KAFKA_CONFIG,\n    consumer_function=\"tests.providers.apache.kafka.operators.test_produce_consume.basic_consumer\",\n    consumer_function_kwargs={\"output_list\": OUTPUT_LIST},\n    max_messages=2,\n    max_batch_size=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting RDS Database Snapshot\nDESCRIPTION: Deletes a snapshot of an Amazon RDS database using RDSDeleteDBSnapshotOperator. The snapshot must be in 'available' state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndelete_snapshot = RDSDeleteDBSnapshotOperator(\n    task_id=\"delete_snapshot\",\n    db_type=\"instance\",\n    db_snapshot_identifier=DB_SNAPSHOT_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Entity Analysis Results in Apache Airflow\nDESCRIPTION: This snippet shows how to retrieve and process the results of entity analysis using XCom in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/natural_language.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nentities = analyze_entities.output[\"entities\"]\nfor entity in entities:\n    print(f\"{entity.name}: {entity.type_.name}\")\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with OpenAI integration, enabling OpenAI hooks and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[openai]'\n```\n\n----------------------------------------\n\nTITLE: Fixing Empty Table Read in HiveHook\nDESCRIPTION: This code fixes an issue in HiveHook where get_pandas_df() would fail when trying to read an empty table.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n\"HiveHook fix get_pandas_df() failure when it tries to read an empty table (#17777)\"\n```\n\n----------------------------------------\n\nTITLE: Defining the OperatorLineage Data Structure in Airflow (Python)\nDESCRIPTION: Defines the `OperatorLineage` class using the `@define` decorator (likely from the `attrs` library, based on `Factory`). This data structure is returned by the OpenLineage facet methods (`get_openlineage_facets_on_start`, etc.) within an operator. It encapsulates the inputs (list of `Dataset`), outputs (list of `Dataset`), run-specific facets (`RunFacet`), and job-level facets (`BaseFacet`) to be included in the OpenLineage event. The core OpenLineage integration uses this information to construct the final `RunEvent`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/developer.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@define\nclass OperatorLineage:\n    inputs: list[Dataset] = Factory(list)\n    outputs: list[Dataset] = Factory(list)\n    run_facets: dict[str, RunFacet] = Factory(dict)\n    job_facets: dict[str, BaseFacet] = Factory(dict)\n```\n\n----------------------------------------\n\nTITLE: Moving a Single File to a Specific Destination with Airflow GCSToSambaOperator (Python)\nDESCRIPTION: This Python snippet illustrates moving a file from GCS to a specific location on a Samba server using Airflow's GCSToSambaOperator. Setting 'move_object=True' causes the original GCS file to be deleted after transfer. The 'destination_path' parameter specifies the exact file path and name on the Samba share. This requires permissions to delete files in GCS. Input is a GCS file path; output is the file relocated on Samba with deletion from GCS.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/transfer/gcs_to_samba.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nGCSToSambaOperator(\n    task_id=\"move_gcs_to_samba_single_file_with_destination\",\n    source_bucket=\"example-source-bucket\",\n    source_path=\"folder/test_object.txt\",\n    samba_server=\"example.samba.server\",\n    share_name=\"SHARE\",\n    destination_path=\"folder_on_samba/test_object_moved.txt\",\n    samba_username=\"airflow\",\n    samba_password=\"airflow\",\n    move_object=True,\n)\n```\n\n----------------------------------------\n\nTITLE: TaskFlow External Python Decorator\nDESCRIPTION: Decorator syntax for using external Python environments in TaskFlow API\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n@task.external_python\n```\n\n----------------------------------------\n\nTITLE: Canceling Cloud Build Operation\nDESCRIPTION: Example showing how to cancel an in-progress Cloud Build operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_build.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncancel_build = CloudBuildCancelBuildOperator(\n    task_id=\"cancel_build\",\n    id_=create_build_from_storage_result[\"id\"],\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Waiting for Amazon Comprehend PII Entities Detection Job Completion with Airflow Sensor - Python\nDESCRIPTION: Illustrates the use of ComprehendStartPiiEntitiesDetectionJobCompletedSensor in Airflow to poll for readiness of a PII detection job on Amazon Comprehend. This sensor requires the job ARN as input and repeatedly checks its AWS state until the job is completed or fails. Relies on the AWS provider hook and is suitable for managing dependent downstream tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/comprehend.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwait_for_pii_entities_detection_job = ComprehendStartPiiEntitiesDetectionJobCompletedSensor(\n    task_id=\"wait_for_pii_entities_detection_job\",\n    job_arn=comprehend_start_pii_entities_detection_job.output,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Flink Provider with Kubernetes Support\nDESCRIPTION: Command to install the Apache Flink provider package with additional CNCF Kubernetes provider dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/flink/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-flink[cncf.kubernetes]\n```\n\n----------------------------------------\n\nTITLE: Deleting a Tag with CloudDataCatalogDeleteTagOperator in Python\nDESCRIPTION: Example of deleting a tag in Google Cloud DataCatalog using the CloudDataCatalogDeleteTagOperator. Jinja templating can be used to dynamically determine parameter values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_delete_tag]\ndelete_tag = CloudDataCatalogDeleteTagOperator(\n    task_id=\"delete_tag\",\n    location=LOCATION,\n    entry_group=ENTRY_GROUP_ID,\n    entry=ENTRY_ID,\n    tag=\"{{ task_instance.xcom_pull('create_tag', key='tag_id') }}\",\n)\n# [END howto_operator_gcp_datacatalog_delete_tag]\n```\n\n----------------------------------------\n\nTITLE: Import Retry Object - Google API Core - Python\nDESCRIPTION: This Python code snippet demonstrates how to import the 'Retry' object from the 'google.api_core.retry' module for usage with Airflow Google Vision operators. 'Retry' enables configuration of retry behavior for API calls, which is critical for robust, production-ready task execution. This import is a prerequisite for any vision-related operator usage that leverages failover or error-handling policies.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom google.api_core.retry import Retry\n\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Teradata Provider via pip (Bash)\nDESCRIPTION: This Bash code snippet demonstrates installing the apache-airflow-providers-teradata package using pip with an optional extra (in this example, amazon). Requires pip and Python. The extra flag (in square brackets) adds additional dependencies for Amazon provider integration. This command should be run in a terminal within the appropriate Python environment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-teradata[amazon]\n```\n\n----------------------------------------\n\nTITLE: Implementing Task Instance Failure Listener in Python\nDESCRIPTION: Code example showing how to implement a listener for task instance failure events.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/listeners.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@hookimpl\ndef on_task_instance_failed(previous_state, task_instance):\n    \"\"\"Listen for task instance failure events.\"\"\"\n    print(f\"Task instance {task_instance} switched from {previous_state} to failed\")\n```\n\n----------------------------------------\n\nTITLE: Canceling RDS Export Task\nDESCRIPTION: Cancels an Amazon RDS export task using RDSCancelExportTaskOperator. Already written data in S3 is not removed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncancel_export = RDSCancelExportTaskOperator(\n    task_id=\"cancel_export\",\n    export_task_identifier=EXPORT_TASK_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Starting Neptune Database Cluster with Airflow Operator\nDESCRIPTION: Example showing how to use the StartNeptuneDbClusterOperator to start an existing Neptune database cluster. The operator can be run in deferrable mode by setting deferrable=True and requires aiobotocore module for deferrable execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/neptune.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nstart_cluster = StartNeptuneDbClusterOperator(\n    task_id=\"start_cluster\",\n    db_cluster_identifier=\"neptune-cluster-demo\",\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Cloud Build Trigger Information Using Airflow Operator - Python\nDESCRIPTION: Shows how to use CloudBuildGetBuildTriggerOperator to fetch information about a specific build trigger in Google Cloud Build from an Airflow workflow. Requires Airflow with Google provider, credentials, the project ID, and trigger ID. Results are made available to later tasks via XCom.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_build.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nget_build_trigger = CloudBuildGetBuildTriggerOperator(\n    task_id=\"get_build_trigger\",\n    project_id=PROJECT_ID,\n    trigger_id=TRIGGER_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow Scheduler\nDESCRIPTION: Command to start the Airflow scheduler service which monitors and triggers tasks based on their dependencies and schedules.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/scheduler.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairflow scheduler\n```\n\n----------------------------------------\n\nTITLE: Using GCSObjectUpdateSensor in Python\nDESCRIPTION: Demonstrates how to use the GCSObjectUpdateSensor to check if an object is updated in Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsensor_update_task = GCSObjectUpdateSensor(\n    task_id=\"sensor_update_task\",\n    bucket=BUCKET_NAME,\n    object=OBJECT_1,\n)\n```\n\n----------------------------------------\n\nTITLE: Airflow Environment Configuration for AWS Batch Executor\nDESCRIPTION: Required environment variables for configuring Airflow to use AWS Batch Executor, including database connection, executor settings, and AWS Batch resource specifications.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/executors/batch-executor.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW__CORE__EXECUTOR='airflow.providers.amazon.aws.executors.batch.batch_executor.AwsBatchExecutor'\n\nAIRFLOW__DATABASE__SQL_ALCHEMY_CONN=<postgres-connection-string>\n\nAIRFLOW__AWS_BATCH_EXECUTOR__REGION_NAME=<executor-region>\n\nAIRFLOW__AWS_BATCH_EXECUTOR__JOB_QUEUE=<batch-job-queue>\n\nAIRFLOW__AWS_BATCH_EXECUTOR__JOB_DEFINITION=<batch-job-definition>\n\nAIRFLOW__AWS_BATCH_EXECUTOR__JOB_NAME=<batch-job-name>\n```\n\n----------------------------------------\n\nTITLE: Using Notifier in Airflow DAG Definition\nDESCRIPTION: Example showing how to use a custom notifier in a DAG definition with success and failure callbacks at both DAG and task levels.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/notifications.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom airflow.sdk import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nfrom myprovider.notifier import MyNotifier\n\nwith DAG(\n    dag_id=\"example_notifier\",\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    on_success_callback=MyNotifier(message=\"Success!\"),\n    on_failure_callback=MyNotifier(message=\"Failure!\"),\n):\n    task = BashOperator(\n        task_id=\"example_task\",\n        bash_command=\"exit 1\",\n        on_success_callback=MyNotifier(message=\"Task Succeeded!\"),\n    )\n```\n\n----------------------------------------\n\nTITLE: Getting S3 Bucket Tags with S3GetBucketTaggingOperator\nDESCRIPTION: Shows how to retrieve tags associated with an S3 bucket using the S3GetBucketTaggingOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/s3.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nget_bucket_tagging = S3GetBucketTaggingOperator(\n    task_id=\"get_bucket_tagging\",\n    bucket_name=BUCKET_NAME,\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Teradata Stored Procedure Returning Cursors (SQL)\nDESCRIPTION: This SQL code defines a Teradata stored procedure named `TEST_PROCEDURE`. It accepts an integer input (`val_in`) and returns an integer output (`val_out`). Critically, it also declares and opens two cursors (`cur1`, `cur2`) using `WITH RETURN`, which return result sets from queries on the `DBC.DBCINFO` table. This demonstrates handling stored procedures that return dynamic result sets via cursors when called using `TeradataStoredProcedureOperator` in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/operators/teradata.rst#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nREPLACE PROCEDURE\nTEST_PROCEDURE (IN val_in INTEGER, OUT val_out INTEGER)\n  BEGIN\n    DECLARE cur1 CURSOR WITH RETURN FOR SELECT * from DBC.DBCINFO ORDER BY 1 ;\n    DECLARE cur2 CURSOR WITH RETURN FOR SELECT infodata, infokey from DBC.DBCINFO order by 1 ;\n    open cur1 ;\n    open cur2 ;\n    set val_out = val_in * 2;\n  END;\n/\n```\n\n----------------------------------------\n\nTITLE: Implementing Addition Function in Python\nDESCRIPTION: A simple function that adds two numbers together and returns the result. It takes two numeric parameters and returns their sum.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/tests/unit/amazon/aws/operators/test_notebook.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef add(num1, num2):\n    return num1 + num2\n```\n\n----------------------------------------\n\nTITLE: Using CloudDataTransferServiceCancelOperationOperator in Python\nDESCRIPTION: Example of using the CloudDataTransferServiceCancelOperationOperator to cancel a transfer operation in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncancel_operation = CloudDataTransferServiceCancelOperationOperator(\n    task_id=\"cancel_operation\",\n    operation_name=\"{{ task_instance.xcom_pull('get_operation', key='name') }}\",\n    project_id=PROJECT_ID,\n    gcp_conn_id=GCP_CONN_ID\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing External Python Task with Classic Operator\nDESCRIPTION: Example showing how to execute Python code in a pre-defined environment using the ExternalPythonOperator. The task runs in a specific Python environment and requires numpy.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef print_numpy_version():\n    import numpy\n    print(f\"Numpy version: {numpy.__version__}\")\n    return \"done\"\n\nexternal_python = ExternalPythonOperator(\n    task_id=\"external_python\",\n    python=PATH_TO_PYTHON_BINARY,\n    requirements=[\"numpy\"],\n    python_callable=print_numpy_version,\n)\n```\n\n----------------------------------------\n\nTITLE: Getting Vertex AI Model using Python\nDESCRIPTION: This code snippet shows how to retrieve a Vertex AI model using the GetModelOperator in Airflow. It specifies the project ID, region, and model ID for the get operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/automl.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nGetModelOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    model_id=model_id,\n    task_id=\"get_model\",\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Conversions in Google Campaign Manager via Airflow\nDESCRIPTION: Example of using GoogleCampaignManagerBatchUpdateConversionsOperator to update Campaign Manager conversions. The operator supports Jinja templating and saves results to XCom for use by other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/campaign_manager.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nupdate_conversion = GoogleCampaignManagerBatchUpdateConversionsOperator(\n    profile_id=PROFILE_ID,\n    conversions=[\n        {\n            \"conversion\": {\n                \"floodlightActivityId\": 1234,\n                \"floodlightConfigurationId\": 1234,\n                \"gclid\": \"KJH3S499-DSFKM-MV35-CSADG-5WAEG\",\n                \"ordinal\": \"1\",\n                \"quantity\": 10,\n                \"timestampMicros\": 1639410000000000,\n                \"value\": 500.0,\n            }\n        }\n    ],\n    encryption_entity_type=\"CONVERSION\",\n    encryption_entity_id=10,\n    encryption_source=\"ADWORDS\",\n    task_id=\"update_conversion\",\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Custom Container Training Job in VertexAI with CreateCustomContainerTrainingJobOperator (Python)\nDESCRIPTION: Shows how to run a custom container-based training job on VertexAI using Airflow's operator. Requires a GCP project, pre-built container image (container_uri), dataset_id, and other job parameters. Inputs include command (for container), arguments, and region. Trains and returns a model, whose ID is pushed as an XCom. User must build and upload container per VertexAI docs; deferrable mode available for longer jobs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwith models.DAG(\n    \"vertex_ai_custom_container_training_job\",\n    default_args=default_args,\n    schedule_interval=None,\n    start_date=days_ago(1),\n    catchup=False,\n) as dag:\n    create_training_job = CreateCustomContainerTrainingJobOperator(\n        task_id=\"train_model\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        display_name=DISPLAY_NAME,\n        container_uri=CONTAINER_URI,\n        command=[\"python3\", \"train.py\"],\n        model_serving_container_image_uri=SERVING_IMAGE_URI,\n        dataset_id=DATASET_ID,\n        gcp_conn_id=GCP_CONN_ID,\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring CORS for Airflow API in airflow.cfg\nDESCRIPTION: This INI snippet shows how to configure Cross-Origin Resource Sharing (CORS) settings for the Airflow API in the airflow.cfg file. It includes settings for allowed headers, methods, and origins.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/api.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[api]\naccess_control_allow_headers = origin, content-type, accept\naccess_control_allow_methods = POST, GET, OPTIONS, DELETE\naccess_control_allow_origins = https://exampleclientapp1.com https://exampleclientapp2.com\n```\n\n----------------------------------------\n\nTITLE: Importing BaseAuthManager in Python for Custom Auth Manager\nDESCRIPTION: Shows how to import the BaseAuthManager class, which is the foundation for creating custom authentication and authorization managers in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/public-airflow-interface.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.api_fastapi.auth.managers.base_auth_manager import BaseAuthManager\n```\n\n----------------------------------------\n\nTITLE: Setting PYTHONPATH for Apache Airflow in Bash\nDESCRIPTION: Demonstrates how to set the PYTHONPATH environment variable to include a custom directory for Airflow operators before starting a Python shell.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/modules_management.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nPYTHONPATH=/home/arch/projects/airflow_operators python\n```\n\n----------------------------------------\n\nTITLE: Accessing Context Parameters in Python\nDESCRIPTION: Shows how to access parameters via a task's context kwarg. This provides an alternative way to access parameters within task functions.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/params.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef print_my_int_param(**context):\n    print(context[\"params\"][\"my_int_param\"])\n\nPythonOperator(\n    task_id=\"print_my_int_param\",\n    python_callable=print_my_int_param,\n    params={\"my_int_param\": 12345},\n)\n```\n\n----------------------------------------\n\nTITLE: Start AWS Glue Data Quality Evaluation Example\nDESCRIPTION: Example demonstrating how to start an AWS Glue Data Quality ruleset evaluation run using GlueDataQualityRuleSetEvaluationRunOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/glue.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_glue_data_quality_ruleset_evaluation_run_operator]\n[END howto_operator_glue_data_quality_ruleset_evaluation_run_operator]\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Resource Operator Examples\nDESCRIPTION: Demonstrates usage of KubernetesCreateResourceOperator and KubernetesDeleteResourceOperator for managing Kubernetes resources\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nKubernetesCreateResourceOperator()\nKubernetesDeleteResourceOperator()\n```\n\n----------------------------------------\n\nTITLE: Releasing Production Images\nDESCRIPTION: Commands for release managers to push production images. Supports both regular and slim images with optional latest tags.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management release-prod-images --airflow-version 2.4.0\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management release-prod-images --airflow-version 2.4.0 --slim-images\n```\n\n----------------------------------------\n\nTITLE: Generating Kerberos Keytab for Airflow in Bash\nDESCRIPTION: Commands to create a Kerberos principal and keytab file for Airflow using kadmin.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/kerberos.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# in the kadmin.local or kadmin shell, create the airflow principal\nkadmin:  addprinc -randkey airflow/fully.qualified.domain.name@YOUR-REALM.COM\n\n# Create the Airflow keytab file that will contain the Airflow principal\nkadmin:  xst -norandkey -k airflow.keytab airflow/fully.qualified.domain.name\n```\n\n----------------------------------------\n\nTITLE: Triggering DAG Run via CLI in Airflow\nDESCRIPTION: This snippet demonstrates how to manually trigger a DAG run using the Airflow CLI. It allows specifying a logical date for the run.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dag-run.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairflow dags trigger --logical-date logical_date run_id\n```\n\n----------------------------------------\n\nTITLE: Stopping AWS DMS Replication Task using DmsStopTaskOperator in Python\nDESCRIPTION: This snippet demonstrates the use of DmsStopTaskOperator to stop a running AWS DMS replication task. It requires the ARN of the replication task to be stopped. Depends on the airflow.providers.amazon.aws.operators.dms module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dms.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Example using DmsStopTaskOperator\n# from airflow.providers.amazon.aws.operators.dms import DmsStopTaskOperator\n\n# stop_task = DmsStopTaskOperator(\n#     task_id='stop_dms_task',\n#     replication_task_arn='arn:aws:dms:us-east-1:123456789012:task:ABCDEF123GHIJKL',\n#     # ... other parameters\n# )\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Account Impersonation Chain with Terraform\nDESCRIPTION: Terraform configuration that creates four service accounts and sets up an impersonation chain where each account can impersonate the next one in the sequence. This enables delegated access through multiple service accounts.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp.rst#2025-04-22_snippet_4\n\nLANGUAGE: terraform\nCODE:\n```\nterraform {\n  required_version = \"> 0.11.14\"\n}\nprovider \"google\" {\n}\nvariable \"project_id\" {\n  type = \"string\"\n}\nresource \"google_service_account\" \"sa_1\" {\n  account_id   = \"impersonation-chain-1\"\n  project = \"${var.project_id}\"\n}\nresource \"google_service_account\" \"sa_2\" {\n  account_id   = \"impersonation-chain-2\"\n  project = \"${var.project_id}\"\n}\nresource \"google_service_account\" \"sa_3\" {\n  account_id   = \"impersonation-chain-3\"\n  project = \"${var.project_id}\"\n}\nresource \"google_service_account\" \"sa_4\" {\n  account_id   = \"impersonation-chain-4\"\n  project = \"${var.project_id}\"\n}\nresource \"google_service_account_iam_member\" \"sa_4_member\" {\n  service_account_id = \"${google_service_account.sa_4.name}\"\n  role               = \"roles/iam.serviceAccountTokenCreator\"\n  member             = \"serviceAccount:${google_service_account.sa_3.email}\"\n}\nresource \"google_service_account_iam_member\" \"sa_3_member\" {\n  service_account_id = \"${google_service_account.sa_3.name}\"\n  role               = \"roles/iam.serviceAccountTokenCreator\"\n  member             = \"serviceAccount:${google_service_account.sa_2.email}\"\n}\nresource \"google_service_account_iam_member\" \"sa_2_member\" {\n  service_account_id = \"${google_service_account.sa_2.name}\"\n  role               = \"roles/iam.serviceAccountTokenCreator\"\n  member             = \"serviceAccount:${google_service_account.sa_1.email}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Slack API Connection via Environment Variable\nDESCRIPTION: Demonstrates how to set up a Slack API connection using an environment variable with URI format. The connection includes the API token and timeout parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/connections/slack.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SLACK_API_DEFAULT='slack://:xoxb-1234567890123-09876543210987-AbCdEfGhIjKlMnOpQrStUvWx@/?timeout=42'\n```\n\n----------------------------------------\n\nTITLE: Deleting an Endpoint using Vertex AI Endpoint Service Operator - Python\nDESCRIPTION: Shows how to delete a Vertex AI endpoint via Airflow's DeleteEndpointOperator. Dependencies include configuration of the GCP project and necessary Airflow provider operators. Required parameters are project_id, endpoint_id, and location; upon successful execution, the specified endpoint is permanently deleted from Vertex AI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndelete_endpoint_task = DeleteEndpointOperator(\n    task_id=\"delete_endpoint_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    endpoint_id=ENDPOINT_ID,\n    location=REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Connection via CLI with Individual Parameters\nDESCRIPTION: Command line example showing how to add an Airflow connection by specifying individual connection parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nairflow connections add 'my_prod_db' \\\n    --conn-type 'my-conn-type' \\\n    --conn-login 'login' \\\n    --conn-password 'password' \\\n    --conn-host 'host' \\\n    --conn-port 'port' \\\n    --conn-schema 'schema' \\\n    ...\n```\n\n----------------------------------------\n\nTITLE: Starting SageMaker Hyperparameter Tuning Job\nDESCRIPTION: Example showing how to use SageMakerTuningOperator to start a hyperparameter optimization job for a SageMaker model.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntuning_task = SageMakerTuningOperator(\n    task_id='tuning_task',\n    config={\n        \"HyperParameterTuningJobName\": \"demo-tuning-job\",\n        \"HyperParameterTuningJobConfig\": {},\n        \"TrainingJobDefinition\": {},\n        \"Tags\": [],\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Running Python Pipeline with DirectRunner using Local File in Apache Beam\nDESCRIPTION: This example demonstrates how to use BeamRunPythonPipelineOperator to execute a Python pipeline using DirectRunner with a local file. The operator expects a local file path and specifies pipeline options including the runner type.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_direct_runner_local_file = BeamRunPythonPipelineOperator(\n    task_id=\"beam_task_direct_runner_local_file\",\n    py_file=\"/tmp/apache_beam/wordcount.py\",\n    py_options=[],\n    pipeline_options={\n        \"output\": \"/tmp/apache_beam/direct_runner_output_local\",\n    },\n    py_interpreter=\"python3\",\n    py_system_site_packages=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Migrating Kubernetes Pod Generator Functions in Python\nDESCRIPTION: Migration rules for Kubernetes Pod Generator functions and classes, moving from the deprecated airflow.kubernetes module to airflow.providers.cncf.kubernetes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41735.significant.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"airflow.kubernetes.pod_generator.datetime_to_label_safe_datestring\"  \"airflow.providers.cncf.kubernetes.pod_generator.datetime_to_label_safe_datestring\"\n\"airflow.kubernetes.pod_generator.extend_object_field\"  \"airflow.kubernetes.airflow.providers.cncf.kubernetes.pod_generator.extend_object_field\"\n\"airflow.kubernetes.pod_generator.label_safe_datestring_to_datetime\"  \"airflow.providers.cncf.kubernetes.pod_generator.label_safe_datestring_to_datetime\"\n\"airflow.kubernetes.pod_generator.make_safe_label_value\"  \"airflow.providers.cncf.kubernetes.pod_generator.make_safe_label_value\"\n\"airflow.kubernetes.pod_generator.merge_objects\"  \"airflow.providers.cncf.kubernetes.pod_generator.merge_objects\"\n\"airflow.kubernetes.pod_generator.PodGenerator\"  \"airflow.providers.cncf.kubernetes.pod_generator.PodGenerator\"\n\"airflow.kubernetes.pod_generator.PodGeneratorDeprecated\"  \"airflow.providers.cncf.kubernetes.pod_generator.PodGenerator\"\n\"airflow.kubernetes.pod_generator.PodDefaults\"  \"airflow.providers.cncf.kubernetes.pod_generator_deprecated.PodDefaults\"\n\"airflow.kubernetes.pod_generator.add_pod_suffix\"  \"airflow.providers.cncf.kubernetes.kubernetes_helper_functions.add_pod_suffix\"\n\"airflow.kubernetes.pod_generator.rand_str\"  \"airflow.providers.cncf.kubernetes.kubernetes_helper_functions.rand_str\"\n\"airflow.kubernetes.pod_generator_deprecated.make_safe_label_value\"  \"airflow.providers.cncf.kubernetes.pod_generator_deprecated.make_safe_label_value\"\n\"airflow.kubernetes.pod_generator_deprecated.PodDefaults\"  \"airflow.providers.cncf.kubernetes.pod_generator_deprecated.PodDefaults\"\n\"airflow.kubernetes.pod_generator_deprecated.PodGenerator\"  \"airflow.providers.cncf.kubernetes.pod_generator_deprecated.PodGenerator\"\n```\n\n----------------------------------------\n\nTITLE: Hibernating EC2 Instances with Airflow AWS Operator - Python\nDESCRIPTION: This snippet covers Airflow's EC2HibernateInstanceOperator, showing how to hibernate an EC2 instance from an Airflow task for cost-saving or workflow orchestration. Necessary dependencies include Airflow AWS provider, Boto3, and sufficient instance privileges; not all EC2 types/volumes support hibernation. Key parameters are the instance IDs, and output is a confirmation of hibernated instances.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/ec2.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# There is no code in the provided input itself, only includes and code references to other files.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Running DV360 Query - Python Example\nDESCRIPTION: Example demonstrating how to run a Display & Video 360 query using GoogleDisplayVideo360RunQueryOperator. Supports Jinja templating for dynamic value determination.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/display_video.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[START howto_google_display_video_run_query_report_operator]\n[END howto_google_display_video_run_query_report_operator]\n```\n\n----------------------------------------\n\nTITLE: Creating BigQuery Table from JSON Schema - Python\nDESCRIPTION: Example of creating a BigQuery table using a schema defined in a JSON file stored in Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncreate_table_schema_json_task = BigQueryCreateTableOperator(\n    task_id=\"create_table_schema_json\",\n    dataset_id=DATASET_NAME,\n    table_id=TABLE_NAME,\n    schema_object=f\"gs://{BUCKET_NAME}/{GCS_SCHEMA_OBJECT}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Docker Context Size\nDESCRIPTION: Command to measure the total size of the Docker build context in human-readable format\nSOURCE: https://github.com/apache/airflow/blob/main/dev/MANUALLY_BUILDING_IMAGES.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nprintf 'FROM scratch\\nCOPY . /' | DOCKER_BUILDKIT=1 docker build -q -f- -o- . | wc -c | numfmt --to=iec --suffix=B\n```\n\n----------------------------------------\n\nTITLE: Setting OpenLineage Extractors via Airflow Environment Variable (INI Syntax)\nDESCRIPTION: This snippet configures OpenLineage extractors for Airflow using the AIRFLOW__OPENLINEAGE__EXTRACTORS environment variable, in INI syntax. Class names of custom extractors are separated by semicolons. The extractors must be available on the Python path for the worker and scheduler processes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/developer.rst#2025-04-22_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__EXTRACTORS='full.path.to.ExtractorClass;full.path.to.AnotherExtractorClass'\\n\n```\n\n----------------------------------------\n\nTITLE: Generating CLI Documentation with Argparse Directive in reStructuredText\nDESCRIPTION: This reStructuredText directive leverages the `argparse` Sphinx extension to automatically generate documentation for command-line arguments derived from a Python module. It targets the `get_parser` function within the `airflow.providers.fab.auth_manager.fab_auth_manager` module, which is expected to return an `ArgumentParser` instance. The `prog` parameter sets the program name displayed in the generated help output to `airflow`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/cli-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. argparse::\n   :module: airflow.providers.fab.auth_manager.fab_auth_manager\n   :func: get_parser\n   :prog: airflow\n```\n\n----------------------------------------\n\nTITLE: Passing Parameters to MSSQL Query using Airflow SQLExecuteQueryOperator (Reference)\nDESCRIPTION: References a Python code example illustrating how to use the `parameters` argument of `SQLExecuteQueryOperator` to dynamically pass values into an SQL query targeting an MSSQL database. This allows for parameterized queries, enhancing security and flexibility.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/operators.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../tests/system/microsoft/mssql/example_mssql.py\n    :language: python\n    :start-after: [START mssql_operator_howto_guide_params_passing_get_query]\n    :end-before: [END mssql_operator_howto_guide_params_passing_get_query]\n```\n\n----------------------------------------\n\nTITLE: Listing Google Cloud Batch Job Tasks using CloudBatchListTasksOperator\nDESCRIPTION: Demonstrates the use of CloudBatchListTasksOperator to list tasks of a specific job in Google Cloud Batch. Optional parameters include 'limit' to restrict the number of tasks returned and 'filter' to list only tasks matching specific criteria.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_batch.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlist_tasks = CloudBatchListTasksOperator(\n    task_id=\"list_tasks\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    job_name=JOB_NAME,\n    gcp_conn_id=GCP_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Bash Scripts from Files with @task.bash in Airflow\nDESCRIPTION: Illustrates how to execute Bash scripts stored in files using the @task.bash decorator in Airflow. The scripts must have a .sh or .bash extension.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@task.bash\ndef bash_example():\n    # \"scripts\" folder is under \"/usr/local/airflow/dags\"\n    return \"scripts/test.sh\"\n```\n\n----------------------------------------\n\nTITLE: Migrating Kubernetes Pod Launcher Classes in Python\nDESCRIPTION: Migration rules for Kubernetes Pod Launcher classes, moving from the deprecated airflow.kubernetes module to airflow.providers.cncf.kubernetes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41735.significant.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"airflow.kubernetes.pod_launcher.PodLauncher\"  \"airflow.providers.cncf.kubernetes.pod_launcher.PodLauncher\"\n\"airflow.kubernetes.pod_launcher.PodStatus\"  \"airflow.providers.cncf.kubernetes.pod_launcher.PodStatus\"\n\"airflow.kubernetes.pod_launcher_deprecated.PodLauncher\"  \"airflow.providers.cncf.kubernetes.pod_launcher_deprecated.PodLauncher\"\n\"airflow.kubernetes.pod_launcher_deprecated.PodStatus\"  \"airflow.providers.cncf.kubernetes.pod_launcher_deprecated.PodStatus\"\n```\n\n----------------------------------------\n\nTITLE: Tagging Release Candidates\nDESCRIPTION: Commands to create and push release candidate tags in both repositories.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncd ${AIRFLOW_REPO_ROOT}\ngit tag -s python-client-${VERSION}${VERSION_SUFFIX} -m \"Airflow Python Client ${VERSION}${VERSION_SUFFIX}\"\ngit push apache python-client-${VERSION}${VERSION_SUFFIX}\ncd ${CLIENT_REPO_ROOT}\ngit tag -s ${VERSION}${VERSION_SUFFIX} -m \"Airflow Python Client ${VERSION}${VERSION_SUFFIX}\"\ngit push apache tag ${VERSION}${VERSION_SUFFIX}\n```\n\n----------------------------------------\n\nTITLE: Installing ODBC Provider with Common SQL Extras using Pip (Bash)\nDESCRIPTION: Installs the `apache-airflow-providers-odbc` package along with its optional dependencies specified by the `common.sql` extra. This typically includes the `apache-airflow-providers-common-sql` package, enabling features that rely on common SQL functionalities.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-odbc[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare docs for Aug 2nd wave of providers\nDESCRIPTION: This text is a commit message summary detailing the preparation of documentation for the August 2nd wave of Apache Airflow provider releases, linked to pull request #41559.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs for Aug 2nd wave of providers (#41559)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Multiple Fields Metadata with GoogleSearchAdsSearchFieldsOperator\nDESCRIPTION: This snippet demonstrates how to retrieve metadata for multiple fields using the GoogleSearchAdsSearchFieldsOperator. The operator supports Jinja templating for dynamic parameter determination.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/search_ads.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[START howto_search_ads_search_fields]\n[END howto_search_ads_search_fields]\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow Microsoft MSSQL Provider via pip - Bash\nDESCRIPTION: Demonstrates the installation of the apache-airflow-providers-microsoft-mssql package with extra dependencies using pip. This Bash command ensures cross-provider dependency support, such as common.sql, is installed alongside the main provider. Requires an existing Airflow 2 environment and pip; the extra must match desired additional feature support.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-microsoft-mssql[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Configuring a Trino Job for Google Cloud Dataproc\nDESCRIPTION: This code defines a configuration for a Trino job to be submitted to a Dataproc cluster. It specifies query file and client output format for the Trino execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nTRINO_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"trino_job\": {\n        \"query_file_uri\": f\"gs://{BUCKET_NAME}/{TRINO_SCRIPT}\",\n        \"client_output_format\": f\"TSV\",\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Cloud Provider with Common Compat Dependencies\nDESCRIPTION: Command to install the dbt Cloud provider package with common compatibility dependencies from PyPI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-dbt-cloud[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Using KubernetesStartKueueJobOperator in Python\nDESCRIPTION: Shows how to use KubernetesStartKueueJobOperator to start a Kueue job in a Kubernetes cluster. It includes configuration for the job specification and resource requirements.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nKubernetesStartKueueJobOperator(\n    task_id=\"start_kueue_job\",\n    namespace=\"default\",\n    image=\"hello-world\",\n    job_name=\"example-job\",\n    resources={\n        \"requests\": {\n            \"cpu\": \"1\",\n            \"memory\": \"1Gi\",\n        },\n        \"limits\": {\n            \"cpu\": \"1\",\n            \"memory\": \"1Gi\",\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Waiting for Dataplex Task Completion Using Sensor - Python\nDESCRIPTION: Shows use of DataplexTaskStateSensor to monitor and wait for a Dataplex task's completion. The sensor requires project_id, region, lake_id, and task_id as inputs, and periodically checks the task's execution state, only succeeding when the target state is reached.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"wait_for_task = DataplexTaskStateSensor(\\n    task_id=\\\"wait_for_task\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    lake_id=LAKE_ID,\\n    task_id=TASK_ID,\\n    target_states=[\\\"SUCCEEDED\\\"],\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Listing Cloud Composer Image Versions in Python\nDESCRIPTION: This snippet demonstrates how to use the CloudComposerListImageVersionsOperator to list all supported Cloud Composer images for a specific project and region.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlist_image_versions_task = CloudComposerListImageVersionsOperator(\n    task_id=\"list-image-versions\",\n    project_id=PROJECT_ID,\n    region=REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Executing External SQL File for Table Creation (Python)\nDESCRIPTION: Shows how to configure `SQLExecuteQueryOperator` to run SQL commands stored in an external file (`sql/pet_schema.sql`) for creating a PostgreSQL table. This approach improves code organization compared to embedding SQL directly. Requires the `conn_id` for the PostgreSQL connection and the relative path to the SQL file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_pet_table = SQLExecuteQueryOperator(\n    task_id=\"create_pet_table\",\n    conn_id=\"postgres_default\",\n    sql=\"sql/pet_schema.sql\",\n)\n```\n\n----------------------------------------\n\nTITLE: Replacing Data in PostgreSQL with BigQuery Data using BigQueryToPostgresOperator\nDESCRIPTION: This example shows how to use the BigQueryToPostgresOperator to replace data in a PostgreSQL table with matching data from a BigQuery table. It demonstrates the use of the 'replace' parameter and specifies selected fields and a replace index.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/bigquery_to_postgres.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nBigQueryToPostgresOperator(\n    task_id=\"bigquery_to_postgres_upsert\",\n    dataset_table=\"{{ DATASET_NAME }}.{{ TABLE_NAME }}\",\n    postgres_conn_id=\"postgres_default\",\n    postgres_table=\"postgres_table\",\n    replace=True,\n    selected_fields=[\"id\", \"name\", \"age\"],\n    replace_index=[\"id\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Establishing ODBC Connection in Python\nDESCRIPTION: Example code demonstrating how to establish an ODBC connection using pyodbc with SQL Server. Shows connection string construction and basic connection setup.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pyodbc\n\ndriver = \"{ODBC Driver 17 for SQL Server}\"\nserver = \"localhost\"\ndatabase = \"testdb\"\nusername = \"user\"\npassword = \"password\"\n\nconn_str = (\n    f\"DRIVER={driver};\" f\"SERVER={server};\" f\"DATABASE={database};\" f\"UID={username};\" f\"PWD={password};\"\n)\n\nconn = pyodbc.connect(conn_str)\n```\n\n----------------------------------------\n\nTITLE: Listing AutoML Translation Datasets with TranslateDatasetsListOperator in Python\nDESCRIPTION: Example of using `TranslateDatasetsListOperator` in an Airflow DAG to retrieve a list of AutoML translation datasets. It requires the `project_id` and `location` for filtering the datasets within a specific Google Cloud project and region using the V3 API.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlist_datasets = TranslateDatasetsListOperator(\n    task_id=\"list_datasets\",\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n)\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Queries with SQLExecuteQueryOperator in Apache Kylin\nDESCRIPTION: Example of using SQLExecuteQueryOperator to connect to Apache Kylin and execute SQL queries. The operator requires proper connection configuration including host, schema, credentials, and port settings. Additional configuration can be provided through the Extra field in JSON format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kylin/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\nwith DAG(\n    'kylin_example',\n    start_date=datetime(2021, 1, 1),\n    description='Example DAG for Apache Kylin operator',\n    schedule_interval=None,\n    tags=['example'],\n) as dag:\n    # [START howto_operator_kylin]\n    kylin_sql = SQLExecuteQueryOperator(\n        task_id='kylin_task',\n        sql='select count(*) from kylin_sales',\n        conn_id='kylin_default',\n    )\n    # [END howto_operator_kylin]\n```\n\n----------------------------------------\n\nTITLE: Fix for SQLAlchemy Engine Connection in JdbcHook\nDESCRIPTION: Fixed issue to only pass connection to SQLAlchemy engine in JdbcHook. Ensures proper connection handling and prevents potential connection leaks or errors.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nFIX: Only pass connection to sqlalchemy engine in JdbcHook (#42705)\n```\n\n----------------------------------------\n\nTITLE: Fixing File Ownership Command in Breeze\nDESCRIPTION: Command to fix ownership of files created within Docker containers that are owned by root instead of the host user. This is particularly useful on Linux systems to prevent permission issues when switching branches.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/08_ci_tasks.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci fix-ownership\n```\n\n----------------------------------------\n\nTITLE: String Parameter Configuration in Python\nDESCRIPTION: Examples of configuring string parameters with validation rules like maxLength and date format.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/params.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nParam(\"default\", type=\"string\", maxLength=10)\nParam(f\"{datetime.date.today()}\", type=\"string\", format=\"date\")\n```\n\n----------------------------------------\n\nTITLE: Publishing Airflow Image to Registry\nDESCRIPTION: Command to push the custom Airflow image to a Docker registry.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker push my-company/airflow:8a0da78\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add Docs for July 2022 Provider Release\nDESCRIPTION: This commit message, associated with commit d2459a241b dated 2022-07-13, notes the addition of documentation for the July 2022 release of Apache Airflow Providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_29\n\nLANGUAGE: text\nCODE:\n```\n``Add documentation for July 2022 Provider's release (#25030)``\n```\n\n----------------------------------------\n\nTITLE: Monitoring AWS Lambda Function State with Airflow Sensor\nDESCRIPTION: Example showing how to use LambdaFunctionStateSensor to monitor the deployment state of an AWS Lambda function until it reaches the target state or another terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/lambda.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlambda_sensor_task = LambdaFunctionStateSensor(\n    task_id=\"wait_for_function_state\",\n    function_name=\"test_function\",\n    target_states=\"Active\",\n    aws_conn_id=\"aws_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Migrating BaseNotifier Import in Python Airflow Code\nDESCRIPTION: The import statement for BaseNotifier must be updated from 'airflow.notifications.basenotifier.BaseNotifier' to 'airflow.sdk.BaseNotifier' as indicated by the ruff linting rule AIR302.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/48008.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old import path (deprecated)\nfrom airflow.notifications.basenotifier import BaseNotifier\n\n# New import path\nfrom airflow.sdk import BaseNotifier\n```\n\n----------------------------------------\n\nTITLE: Running Pytest in Breeze Kubernetes Shell Session - Bash\nDESCRIPTION: Within the activated Breeze Kubernetes shell, this command runs tests in `test_kubernetes_executor.py` using pytest directly. Assumes that Airflow is deployed and the test environment is fully set up. Results, warnings, and platform details are displayed as part of pytest output.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n(kind-airflow-python-3.9-v1.24.0:KubernetesExecutor)> pytest test_kubernetes_executor.py\n================================================= test session starts =================================================\nplatform linux -- Python 3.10.6, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /home/jarek/code/airflow/kubernetes-tests/.venv/bin/python\ncachedir: .pytest_cache\nrootdir: /home/jarek/code/airflow, configfile: pytest.ini\nplugins: anyio-3.6.1\ncollected 2 items\n\ntest_kubernetes_executor.py::TestKubernetesExecutor::test_integration_run_dag PASSED           [ 50%]\ntest_kubernetes_executor.py::TestKubernetesExecutor::test_integration_run_dag_with_scheduler_failure PASSED [100%]\n\n================================================== warnings summary ===================================================\nkubernetes-tests/.venv/lib/python3.10/site-packages/_pytest/config/__init__.py:1233\n  /home/jarek/code/airflow/kubernetes-tests/.venv/lib/python3.10/site-packages/_pytest/config/__init__.py:1233: PytestConfigWarning: Unknown config option: asyncio_mode\n\n    self._warn_or_fail_if_strict(f\"Unknown config option: {key}\\n\")\n\n-- Docs: https://docs.pytest.org/en/stable/warnings.html\n============================================ 2 passed, 1 warning in 38.62s ============================================\n(kind-airflow-python-3.9-v1.24.0:KubernetesExecutor)>\n```\n\n----------------------------------------\n\nTITLE: Inserting a Key-Value Pair with LevelDBOperator in Python (Airflow)\nDESCRIPTION: This Python snippet demonstrates using the `LevelDBOperator` within an Apache Airflow DAG to insert a key-value pair into a LevelDB database. It requires a pre-configured LevelDB connection (`leveldb_default`), specifies the database name (`\"test\"`), the key (`b\"key\"`), the value (`b\"value\"`), and the operation type (`\"put\"`). The `plyvel` library must be installed via the `apache-airflow-providers-google[leveldb]` extra.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/leveldb/leveldb.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nput_key = LevelDBOperator(\n    task_id=\"put_key\",\n    leveldb_conn_id=LEVELDB_CONN_ID,\n    db=DB_NAME,\n    key=b\"key\",\n    value=b\"value\",\n    command=\"put\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing DeltaDataIntervalTimetable in Airflow DAG\nDESCRIPTION: Example of using a timedelta directly as the schedule parameter, which is automatically converted to a DeltaDataIntervalTimetable. This schedules a DAG to run every 30 minutes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timetable.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dag(schedule=datetime.timedelta(minutes=30))\ndef example_dag():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Creating a Google Cloud Workflow using Airflow Operator in Python\nDESCRIPTION: Demonstrates the usage of the `WorkflowsCreateWorkflowOperator` to create a new workflow in Google Cloud Workflows. This operator requires parameters such as `workflow_id`, `location_id`, and the `workflow` definition object.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/workflows.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/workflows/example_workflows.py\n      :language: python\n      :dedent: 4\n      :start-after: [START how_to_create_workflow]\n      :end-before: [END how_to_create_workflow]\n```\n\n----------------------------------------\n\nTITLE: Deleting a Google Campaign Manager Report in Airflow\nDESCRIPTION: Example of using GoogleCampaignManagerDeleteReportOperator to delete a report by its unique ID. The operator supports Jinja templating for dynamic value determination.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/campaign_manager.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndelete_report = GoogleCampaignManagerDeleteReportOperator(\n    profile_id=PROFILE_ID,\n    report_id=REPORT_ID,\n    task_id=\"delete_report\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Tag Template Field with CloudDataCatalogCreateTagTemplateFieldOperator in Python\nDESCRIPTION: Example of creating a tag template field in Google Cloud DataCatalog using the CloudDataCatalogCreateTagTemplateFieldOperator. The result is saved to XCom for use in other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_create_tag_template_field]\ncreate_tag_template_field = CloudDataCatalogCreateTagTemplateFieldOperator(\n    task_id=\"create_tag_template_field\",\n    location=LOCATION,\n    tag_template=TAG_TEMPLATE_ID,\n    tag_template_field_id=TAG_TEMPLATE_FIELD_ID,\n    tag_template_field={\n        \"display_name\": \"Required Source\",\n        \"type\": {\"primitive_type\": \"STRING\"},\n        \"is_required\": True,\n    },\n)\n# [END howto_operator_gcp_datacatalog_create_tag_template_field]\n```\n\n----------------------------------------\n\nTITLE: Airflow Provider Metadata Configuration - YAML - yaml\nDESCRIPTION: Illustrates the recommended format for a provider.yaml file required in every Airflow provider package. This YAML document defines provider metadata, dependencies, versioning, integration information, operator and hook modules, sensor modules, and connection types. It is parsed by Airflow tools for packaging, documentation generation, and provider registration. Replace placeholders such as <PROVIDER> and ensure listed Python modules exist before using.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/MANAGING_PROVIDERS_LIFECYCLE.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\npackage-name: apache-airflow-providers-<PROVIDER>\nname: <PROVIDER>\ndescription: |\n  `<PROVIDER> <https://example.io/>`__\nversions:\n  - 1.0.0\n\nintegrations:\n  - integration-name: <PROVIDER>\n    external-doc-url: https://www.example.io/\n    logo: /docs/integration-logos/<PROVIDER>.png\n    how-to-guide:\n      - /docs/apache-airflow-providers-<PROVIDER>/operators/<PROVIDER>.rst\n    tags: [service]\n\noperators:\n  - integration-name: <PROVIDER>\n    python-modules:\n      - airflow.providers.<PROVIDER>.operators.<PROVIDER>\n\nhooks:\n  - integration-name: <PROVIDER>\n    python-modules:\n      - airflow.providers.<PROVIDER>.hooks.<PROVIDER>\n\nsensors:\n  - integration-name: <PROVIDER>\n    python-modules:\n      - airflow.providers.<PROVIDER>.sensors.<PROVIDER>\n\nconnection-types:\n  - hook-class-name: airflow.providers.<PROVIDER>.hooks.<PROVIDER>.NewProviderHook\n  - connection-type: provider-connection-type\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Trino to Google Cloud Storage using Airflow Operator in Python\nDESCRIPTION: Demonstrates a basic use of Airflow's TrinoToGCSOperator for exporting query results from Trino to a specified Google Cloud Storage bucket and object. Requires Airflow and the corresponding provider packages for Trino and GCP. The example shows minimum required parameters such as SQL query, bucket, and destination filename, typically invoked within an Airflow DAG. The operator runs the SQL, exports results, and uploads them to GCS.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/trino_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrino_to_gcs = TrinoToGCSOperator(\n    task_id=\"trino_to_gcs\",\n    sql=\"SELECT * FROM my_table;\",\n    bucket=\"my-gcs-bucket\",\n    filename=\"data_{}.json\",\n)\n```\n\n----------------------------------------\n\nTITLE: Monitoring Amazon Bedrock Batch Inference Job with Python\nDESCRIPTION: This example demonstrates how to use the BedrockBatchInferenceScheduledSensor to wait for an Amazon Bedrock batch inference job to reach the \"Scheduled\" or \"Completed\" state. It allows waiting for job completion or just confirmation of being in the queue.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_sensor_bedrock_batch_inference_scheduled]\n# Example code not provided in the original text\n# [END howto_sensor_bedrock_batch_inference_scheduled]\n```\n\n----------------------------------------\n\nTITLE: In-Process Job Runner Sequence Flow in Apache Airflow\nDESCRIPTION: Sequence diagram showing the interaction between CLI component, JobRunner, and Database for in-process job execution. Details the complete lifecycle including job creation, execution, heartbeat monitoring, and completion phases.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/jobs/JOB_LIFECYCLE.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant CLI component\n    participant JobRunner\n    participant DB\n\n    activate CLI component\n    CLI component-->>DB: Create Session\n\n    activate DB\n\n    CLI component->>DB: Create Job\n    DB->>CLI component: Job object\n\n    CLI component->>JobRunner: Create Job Runner\n    JobRunner ->> CLI component: JobRunner object\n\n    CLI component->>JobRunner: Run Job\n\n    activate JobRunner\n\n    JobRunner->>DB: prepare_for_execution [Job]\n    DB->>JobRunner: prepared\n\n    par\n        JobRunner->>JobRunner: execute_job\n    and\n        JobRunner->>DB: access DB (Variables/Connections etc.)\n        DB ->> JobRunner: returned data\n    and\n        JobRunner-->>DB: create heartbeat session\n        Note over DB: Note: During heartbeat<br> two DB sessions <br>are opened in parallel(!)\n        JobRunner->>DB: perform_heartbeat [Job]\n        JobRunner ->> JobRunner: Heartbeat Callback [Job]\n        DB ->> JobRunner: heartbeat response\n        DB -->> JobRunner: close heartbeat session\n    end\n\n    JobRunner->>DB: complete_execution [Job]\n    DB ->> JobRunner: completed\n\n    JobRunner ->> CLI component: completed\n    deactivate JobRunner\n\n    deactivate DB\n    deactivate CLI component\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Helm Chart Test Class in Python\nDESCRIPTION: Example showing the basic structure of a Helm chart test class using Python\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/helm_unit_tests.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass TestBaseChartTest: ...\n```\n\n----------------------------------------\n\nTITLE: Putting File via FTP using FTPFileTransmitOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the Airflow `FTPFileTransmitOperator` to upload ('put') a file from the local filesystem to a remote FTP server. It requires an FTP connection configured in Airflow (identified by `ftp_conn_id`) and specifies the `local_filepath` and `remote_filepath` parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ftp/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../ftp/tests/system/ftp/example_ftp.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_ftp_put]\n    :end-before: [END howto_operator_ftp_put]\n```\n\n----------------------------------------\n\nTITLE: DAG Creation Function Implementation\nDESCRIPTION: Example implementation of a DAG creation function showing task definition and DAG configuration patterns.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/faq.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import DAG\nfrom airflow.sdk import task\n\nimport pendulum\n\n\ndef create_dag(dag_id, schedule, dag_number, default_args):\n    dag = DAG(\n        dag_id,\n        schedule=schedule,\n        default_args=default_args,\n        pendulum.datetime(2021, 9, 13, tz=\"UTC\"),\n    )\n\n    with dag:\n\n        @task()\n        def hello_world():\n            print(\"Hello World\")\n            print(f\"This is DAG: {dag_number}\")\n\n        hello_world()\n\n    return dag\n```\n\n----------------------------------------\n\nTITLE: Creating AlloyDB User with Airflow Operator\nDESCRIPTION: Uses AlloyDBCreateUserOperator to create a new user in an AlloyDB instance. Requires the primary instance to be created in the cluster.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/alloy_db.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncreate_user = AlloyDBCreateUserOperator(\n    task_id=\"create_user\",\n    project_id=GCP_PROJECT_ID,\n    cluster_id=CLUSTER_ID,\n    instance_id=INSTANCE_ID,\n    user=ALLOYDB_USER,\n    region=GCP_ALLOYDB_REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying Model to Vertex AI Endpoint\nDESCRIPTION: Example demonstrating how to deploy a model to an endpoint using the DeployModelOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nDeployModelOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    endpoint_id=ENDPOINT_ID,\n    deployed_model={\n        \"model\": MODEL_ID,\n        \"display_name\": f\"temp_deploy_model_test_{SUFFIX}\",\n        \"dedicated_resources\": {\n            \"machine_spec\": {\n                \"machine_type\": \"n1-standard-4\",\n            },\n            \"min_replica_count\": 1,\n            \"max_replica_count\": 1,\n        },\n    },\n    traffic_split={\"0\": 100},\n    task_id=\"deploy_model\",)\n```\n\n----------------------------------------\n\nTITLE: Installing Discord Provider via PIP\nDESCRIPTION: Command to install the Discord provider package on top of an existing Airflow 2 installation using pip package manager.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/discord/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-discord\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Run Facet Class and Function in Python\nDESCRIPTION: Example of creating a custom run facet class inheriting from RunFacet and implementing a function that returns custom facets based on TaskInstance and TaskInstanceState. The function demonstrates conditional logic for different operator types and task states.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/developer.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport attrs\nfrom airflow.models.taskinstance import TaskInstance, TaskInstanceState\nfrom airflow.providers.common.compat.openlineage.facet import RunFacet\n\n\n@attrs.define\nclass MyCustomRunFacet(RunFacet):\n    \"\"\"Define a custom facet.\"\"\"\n\n    name: str\n    jobState: str\n    uniqueName: str\n    displayName: str\n    dagId: str\n    taskId: str\n    cluster: str\n    custom_metadata: dict\n\n\ndef get_my_custom_facet(\n    task_instance: TaskInstance, ti_state: TaskInstanceState\n) -> dict[str, RunFacet] | None:\n    operator_name = task_instance.task.operator_name\n    custom_metadata = {}\n    if operator_name == \"BashOperator\":\n        return None\n    if ti_state == TaskInstanceState.FAILED:\n        custom_metadata[\"custom_key_failed\"] = \"custom_value\"\n    job_unique_name = f\"TEST.{task_instance.dag_id}.{task_instance.task_id}\"\n    return {\n        \"additional_run_facet\": MyCustomRunFacet(\n            name=\"test-lineage-namespace\",\n            jobState=task_instance.state,\n            uniqueName=job_unique_name,\n            displayName=f\"{task_instance.dag_id}.{task_instance.task_id}\",\n            dagId=task_instance.dag_id,\n            taskId=task_instance.task_id,\n            cluster=\"TEST\",\n            custom_metadata=custom_metadata,\n        )\n    }\n```\n\n----------------------------------------\n\nTITLE: Adding Connection ID Parameter to SQL Operators in Python\nDESCRIPTION: Adds a new parameter to SQL operators to specify the connection ID field. This allows more flexibility in configuring database connections.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nAdd a new parameter to SQL operators to specify conn id field (#30784)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Notifier Class in Python\nDESCRIPTION: Example of creating a custom notifier by extending BaseNotifier class. The implementation includes template fields and a notify method that sends messages using a custom provider.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/notifications.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import BaseNotifier\nfrom my_provider import send_message\n\n\nclass MyNotifier(BaseNotifier):\n    template_fields = (\"message\",)\n\n    def __init__(self, message):\n        self.message = message\n\n    def notify(self, context):\n        # Send notification here, below is an example\n        title = f\"Task {context['task_instance'].task_id} failed\"\n        send_message(title, self.message)\n```\n\n----------------------------------------\n\nTITLE: Using Airflow Variables in Jinja Templates\nDESCRIPTION: Shows how to access Airflow variables within Jinja templates. Demonstrates retrieving raw values and auto-deserialized JSON values.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/variables.rst#2025-04-22_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n# Raw value\necho {{ var.value.<variable_name> }}\n\n# Auto-deserialize JSON value\necho {{ var.json.<variable_name> }}\n```\n\n----------------------------------------\n\nTITLE: Configuring a Dataplex Zone - Python\nDESCRIPTION: Defines the configuration dictionary necessary to create a Dataplex zone resource. Includes both mandatory identifiers and optional metadata or properties, per the Dataplex create zone API documentation. Used as an input body for corresponding operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n\"zone_config = {\\n    'name': 'sample-zone',\\n    'type': 'RAW',\\n    # Optional and required fields...\\n}\"\n```\n\n----------------------------------------\n\nTITLE: Running Go Pipeline with DirectRunner using Local File in Apache Beam\nDESCRIPTION: This example demonstrates how to use BeamRunGoPipelineOperator to execute a Go pipeline using DirectRunner with a local file. It specifies the Go file path and pipeline options including output location.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_go_direct_runner_local_file = BeamRunGoPipelineOperator(\n    task_id=\"beam_task_go_direct_runner_local_file\",\n    go_file=\"/tmp/apache_beam/wordcount.go\",\n    pipeline_options={\n        \"output\": \"/tmp/apache_beam/direct_runner_go_output_local\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Including Python Example for LocalFilesystemToWasbOperator\nDESCRIPTION: This reStructuredText directive includes a Python code example from an external file (`/../tests/system/microsoft/azure/example_local_to_wasb.py`). The example demonstrates the usage of the `LocalFilesystemToWasbOperator` to upload a file from the local filesystem to Azure Blob Storage. The specific code block is identified by the `START` and `END` markers within the referenced file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/transfer/local_to_wasb.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../tests/system/microsoft/azure/example_local_to_wasb.py\n    :language: python\n    :dedent: 0\n    :start-after: [START howto_operator_local_to_wasb]\n    :end-before: [END howto_operator_local_to_wasb]\n```\n\n----------------------------------------\n\nTITLE: Deleting Memcached Instance using Airflow Operator\nDESCRIPTION: Example of using CloudMemorystoreMemcachedDeleteInstanceOperator to delete an existing Memcached instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore_memcached.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndelete_instance = CloudMemorystoreMemcachedDeleteInstanceOperator(\n    task_id=\"delete-instance\",\n    location=LOCATION,\n    instance=MEMCACHED_INSTANCE_ID,\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with Extras from PyPI\nDESCRIPTION: This command installs Apache Airflow version 2.10.5 with additional extras (postgres and google) using pip. It also applies constraints to ensure compatibility and a repeatable installation.\nSOURCE: https://github.com/apache/airflow/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[postgres,google]==2.10.5' \\\n --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.5/constraints-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Printing Task Instance and DAG Run Information in Apache Airflow (Python)\nDESCRIPTION: This code snippet defines a task function that prints information about a task instance and its associated DAG run. It demonstrates how to access the run ID, duration, and queued time using the Airflow Python API.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/docs/shared/template-examples/taskflow.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models.taskinstance import TaskInstance\nfrom airflow.models.dagrun import DagRun\n\n\n@task\ndef print_ti_info(task_instance: TaskInstance, dag_run: DagRun):\n    print(f\"Run ID: {task_instance.run_id}\")  # Run ID: scheduled__2023-08-09T00:00:00+00:00\n    print(f\"Duration: {task_instance.duration}\")  # Duration: 0.972019\n    print(f\"DAG Run queued at: {dag_run.queued_at}\")  # 2023-08-10 00:00:01+02:20\n```\n\n----------------------------------------\n\nTITLE: Deleting a Dataplex Data Quality Scan - Python\nDESCRIPTION: Invokes DataplexDeleteDataQualityScanOperator to delete a specific Data Quality scan resource. Inputs are project_id, region, and data quality scan ID; the operator schedules resource deletion and returns the result.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n\"delete_data_quality_scan = DataplexDeleteDataQualityScanOperator(\\n    task_id=\\\"delete_data_quality_scan\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    data_quality_scan_id=DATA_QUALITY_SCAN_ID,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: TaskFlow Virtual Environment Decorator\nDESCRIPTION: Decorator syntax for using virtual environments in TaskFlow API to handle dependency conflicts\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n@task.virtualenv\n```\n\n----------------------------------------\n\nTITLE: Running a Pod on GKE Asynchronously with XCom Support using GKEStartPodOperator\nDESCRIPTION: Example of using the GKEStartPodOperator in deferrable mode to asynchronously run a pod on a Google Kubernetes Engine cluster with XCom enabled, freeing up worker resources during pod execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npod_task_async = GKEStartPodOperator(\n    task_id=\"pod_task_async\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    cluster_name=CLUSTER_NAME,\n    namespace=\"default\",\n    image=\"perl\",\n    name=\"test-pod-async\",\n    cmds=[\"perl\"],\n    arguments=[\"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"],\n    is_delete_operator_pod=True,\n    startup_check_interval_seconds=120,\n    get_logs=True,\n    xcom_push=True,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Wait for AWS Glue Catalog Partition Example\nDESCRIPTION: Example demonstrating how to wait for a partition to appear in AWS Glue Catalog using GlueCatalogPartitionSensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/glue.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n[START howto_sensor_glue_catalog_partition]\n[END howto_sensor_glue_catalog_partition]\n```\n\n----------------------------------------\n\nTITLE: Deleting Alert Policies with StackdriverDeleteAlertOperator in Python\nDESCRIPTION: This snippet shows how to use the StackdriverDeleteAlertOperator to delete an Alert Policy identified by a given name. The name of the alert to be deleted should be in the format 'projects/<PROJECT_NAME>/alertPolicies/<ALERT_NAME>'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/stackdriver.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nStackdriverDeleteAlertOperator(\n    task_id='delete_alert_policy',\n    name='',\n    gcp_conn_id='google_cloud_default'\n)\n```\n\n----------------------------------------\n\nTITLE: DingDing Task Failure Callback Implementation\nDESCRIPTION: Implementation of a failure callback function that sends notifications via DingDing when a task fails.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/operators.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef failure_callback(context):\n    Hook = DingdingHook(\n        message_type=\"text\",\n        message=\"Task Failed, dag_id: {}, task_id: {}, execution_time: {}\".format(\n            context[\"task_instance\"].dag_id,\n            context[\"task_instance\"].task_id,\n            context[\"execution_date\"],\n        ),\n        at_mobiles=[\"156XXXXXXXX\"],\n        at_all=False,\n    )\n    Hook.send()\n```\n\n----------------------------------------\n\nTITLE: Deleting Google Analytics Property with GoogleAnalyticsAdminDeletePropertyOperator in Python\nDESCRIPTION: This snippet shows how to use the GoogleAnalyticsAdminDeletePropertyOperator to delete a property in Google Analytics. It provides an example of the operator usage and mentions that Jinja templating can be applied to the operator's template fields.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/analytics_admin.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nGoogleAnalyticsAdminDeletePropertyOperator(\n    task_id=\"delete_property\",\n    property_id=\"{{ task_instance.xcom_pull('create_property')['name'].split('/')[-1] }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Monitoring Dataform Workflow Invocation Action in Python\nDESCRIPTION: This snippet demonstrates how to use a sensor to check the status of a particular action for a workflow invocation triggered asynchronously.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_create_workflow_invocation_action_async]\n# [END howto_operator_create_workflow_invocation_action_async]\n```\n\n----------------------------------------\n\nTITLE: Getting a Dataplex Task - Python\nDESCRIPTION: Uses DataplexGetTaskOperator to retrieve the details of a specified Dataplex task. Necessary parameters include project_id, region, lake_id, and task_id. Outputs the full task resource JSON object; this operation is read-only and useful for task introspection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"get_task = DataplexGetTaskOperator(\\n    task_id=\\\"get_task\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    lake_id=LAKE_ID,\\n    task_id=TASK_ID,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Create DV360 SDF Download Task - Python Example\nDESCRIPTION: Example showing how to create an SDF download task using GoogleDisplayVideo360CreateSDFDownloadTaskOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/display_video.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n[START howto_google_display_video_create_sdf_download_task_operator]\n[END howto_google_display_video_create_sdf_download_task_operator]\n```\n\n----------------------------------------\n\nTITLE: Using OpenSearchServerlessCollectionActiveSensor in Python\nDESCRIPTION: Example showing how to use the OpenSearchServerlessCollectionActiveSensor to wait for an Amazon OpenSearch Serverless Collection to become active. The sensor monitors the collection state until it reaches a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/opensearchserverless.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nopensearch_serverless_collection_sensor = OpenSearchServerlessCollectionActiveSensor(\n    task_id=\"wait_for_collection\",\n    collection_name=\"test-collection\",\n    aws_conn_id=\"aws_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Including Python Example for CopyFromExternalStageToSnowflakeOperator Usage\nDESCRIPTION: This reStructuredText directive dynamically includes a Python code snippet from the specified file path. It extracts the code block between the start and end markers, applies dedentation, and formats it as Python code. The referenced Python code demonstrates initializing and using the `CopyFromExternalStageToSnowflakeOperator` to copy data from a configured external stage (like S3) into a Snowflake table.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/operators/copy_into_snowflake.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../snowflake/tests/system/snowflake/example_copy_into_snowflake.py\n    :language: python\n    :start-after: [START howto_operator_s3_copy_into_snowflake]\n    :end-before: [END howto_operator_s3_copy_into_snowflake]\n    :dedent: 4\n```\n\n----------------------------------------\n\nTITLE: Deleting Cloud Memorystore Instance\nDESCRIPTION: Example showing how to delete a Cloud Memorystore instance\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndelete_instance = CloudMemorystoreDeleteInstanceOperator(task_id=\"delete-instance\", location=\"europe-north1\", instance=INSTANCE_NAME, project_id=PROJECT_ID)\n```\n\n----------------------------------------\n\nTITLE: Start AWS Glue Data Quality Recommendation Example\nDESCRIPTION: Example showing how to start an AWS Glue Data Quality rule recommendation run using GlueDataQualityRuleRecommendationRunOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/glue.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_glue_data_quality_rule_recommendation_run]\n[END howto_operator_glue_data_quality_rule_recommendation_run]\n```\n\n----------------------------------------\n\nTITLE: Implementing Serialization for Airflow Object in Python\nDESCRIPTION: This snippet demonstrates how to implement serialization and deserialization methods for a custom Airflow object. It includes version control and handles both primitive and complex data types.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/serializers.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, ClassVar\n\n\nclass Foo:\n    __version__: ClassVar[int] = 1\n\n    def __init__(self, a, v) -> None:\n        self.a = a\n        self.b = {\"x\": v}\n\n    def serialize(self) -> dict[str, Any]:\n        return {\n            \"a\": self.a,\n            \"b\": self.b,\n        }\n\n    @staticmethod\n    def deserialize(data: dict[str, Any], version: int):\n        f = Foo(a=data[\"a\"])\n        f.b = data[\"b\"]\n        return f\n```\n\n----------------------------------------\n\nTITLE: Defining Cloud Memorystore Instance Configuration in Python\nDESCRIPTION: Example showing how to define a Cloud Memorystore instance configuration object that can be used with operators\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nINSTANCE = {\"tier\": \"STANDARD_HA\", \"memory_size_gb\": 5, \"region\": \"europe-north1\", \"redis_version\": \"REDIS_4_0\", \"display_name\": \"presto-test-instance\", \"labels\": {\"environment\": \"test\"}, \"redis_configs\": {\"maxmemory-policy\": \"allkeys-lru\"}}\n```\n\n----------------------------------------\n\nTITLE: Task Configuration with Volume Override\nDESCRIPTION: Python code demonstrating how to override Kubernetes pod configuration for a specific task with volume mounts.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/kubernetes_executor.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nexecutor_config_template = {\n    \"pod_template_file\": os.path.join(AIRFLOW_HOME, \"pod_templates/basic_template.yaml\"),\n    \"pod_override\": k8s.V1Pod(metadata=k8s.V1ObjectMeta(labels={\"release\": \"stable\"})),\n}\n\n@task(executor_config=executor_config_template)\ndef task_with_template():\n    print_stuff()\n```\n\n----------------------------------------\n\nTITLE: Sending Events to Amazon EventBridge using PutEventsOperator\nDESCRIPTION: Example of using EventBridgePutEventsOperator to send custom events to Amazon EventBridge. The operator allows sending event data to specified EventBridge buses.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eventbridge.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nput_events = EventBridgePutEventsOperator(\n    task_id=\"put_events\",\n    entries=[{\n        \"Detail\": json.dumps({\n            \"message\": \"Test message\",\n        }),\n        \"DetailType\": \"TestMessage\",\n        \"Source\": \"test.source\",\n        \"EventBusName\": \"test-bus\",\n    }],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring DCAwareRoundRobinPolicy for Cassandra Load Balancing in Airflow\nDESCRIPTION: Example JSON configuration for the Extra field to specify DCAwareRoundRobinPolicy as the load balancing policy. This policy prioritizes nodes in the local datacenter and allows configuring how many hosts to use in remote datacenters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/cassandra/docs/connections/cassandra.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"load_balancing_policy\": \"DCAwareRoundRobinPolicy\",\n  \"load_balancing_policy_args\": {\n    \"local_dc\": \"LOCAL_DC_NAME\",\n    \"used_hosts_per_remote_dc\": \"SOME_INT_VALUE\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Account Impersonation\nDESCRIPTION: Configuration for using project ID and service account impersonation with Application Default Credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/secrets-backends/google-cloud-secret-manager-backend.rst#2025-04-22_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend\nbackend_kwargs = {\"project_id\": \"example-project\", \"impersonation_chain\": \"impersonated_account@example_project.iam.gserviceaccount.com\"}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Build Details Using Airflow Cloud Build Operator - Python\nDESCRIPTION: Demonstrates how to use the CloudBuildGetBuildOperator to retrieve details about a specific Google Cloud Build job in an Airflow DAG. Depends on Airflow with Google provider extras and appropriate Google Cloud credentials. Takes parameters such as the build ID and saves the result to XCom for downstream tasks. Requires Google Cloud Build API access.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_build.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nget_build = CloudBuildGetBuildOperator(\n    task_id=\"get_build\",\n    project_id=PROJECT_ID,\n    id=build_result[\"id\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Populating YDB Table Using Operator\nDESCRIPTION: Python code demonstrating how to use YDBExecuteQueryOperator to populate the pet table using an SQL file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/docs/operators/ydb_operator_howto_guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npopulate_pet_table = YDBExecuteQueryOperator(\n    task_id=\"populate_pet_table\",\n    sql=\"sql/pet_schema.sql\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataplex Asset with Airflow Operator\nDESCRIPTION: Uses the DataplexCreateAssetOperator to create a Dataplex asset with the specified configuration. This operator creates an asset within a zone in Google Cloud Dataplex.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ncreate_asset = DataplexCreateAssetOperator(\n    task_id=\"create_asset\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    lake_id=LAKE_ID,\n    zone_id=ZONE_ID,\n    asset_id=ASSET_ID,\n    body=ASSET,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive CLI Connection using Environment Variable in Bash\nDESCRIPTION: This snippet demonstrates how to set up a Hive CLI connection using an environment variable in Bash. It includes parameters for Beeline username, password, host, port, database, and additional configuration options.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/connections/hive_cli.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_HIVE_CLI_DEFAULT='hive-cli://beeline-username:beeline-password@jdbc-hive-host:80/hive-database?hive_cli_params=params&use_beeline=True&auth=noSasl&principal=hive%2F_HOST%40EXAMPLE.COM'\n```\n\n----------------------------------------\n\nTITLE: Transferring CSV Data from S3 to Teradata using S3ToTeradataOperator in Python\nDESCRIPTION: This Python snippet demonstrates how to configure and use the `S3ToTeradataOperator` within an Airflow DAG to transfer data in CSV format from an AWS S3 bucket to a Teradata table. It utilizes Teradata's READ_NOS feature for the transfer.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/operators/s3_to_teradata.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../teradata/tests/system/teradata/example_s3_to_teradata_transfer.py\n    :language: python\n    :start-after: [START s3_to_teradata_transfer_operator_howto_guide_transfer_data_public_s3_to_teradata_csv]\n    :end-before: [END s3_to_teradata_transfer_operator_howto_guide_transfer_data_public_s3_to_teradata_csv]\n```\n\n----------------------------------------\n\nTITLE: Configuring Postgres Keepalives in Python for Airflow Database Connection\nDESCRIPTION: This snippet demonstrates how to set up keepalive parameters for PostgreSQL connections in Airflow to prevent idle connection timeouts. It defines a dictionary of keepalive settings that can be used in the Airflow configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-up-database.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nkeepalive_kwargs = {\n    \"keepalives\": 1,\n    \"keepalives_idle\": 30,\n    \"keepalives_interval\": 5,\n    \"keepalives_count\": 5,\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Variable from Lockbox in Airflow\nDESCRIPTION: This command shows how to use Airflow CLI to retrieve a variable that is stored in Yandex Cloud Lockbox Secret Backend.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_16\n\nLANGUAGE: console\nCODE:\n```\n$ airflow variables get my_variable\nsome_secret_data\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Profile Name in Airflow Connection Extra Field (JSON)\nDESCRIPTION: This JSON snippet is intended for the 'Extra' field of an Airflow AWS connection. It instructs Airflow to use credentials and settings defined within a specific profile ('my_profile') located in the standard AWS credential files (~/.aws/credentials and ~/.aws/config). This assumes other credential fields in the connection are left empty.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"profile_name\": \"my_profile\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting a Job on GKE in Deferrable Mode using GKEStartJobOperator in Python\nDESCRIPTION: This snippet shows how to use `GKEStartJobOperator` in deferrable mode. This mode is useful when the `wait_until_job_complete` parameter is set to `True`, allowing the operator to release the worker slot while waiting for the GKE job to finish.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/kubernetes_engine/example_kubernetes_engine_job.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_gke_start_job_def]\n    :end-before: [END howto_operator_gke_start_job_def]\n```\n\n----------------------------------------\n\nTITLE: Getting Vertex AI Feature View Sync Job using Airflow Operator in Python\nDESCRIPTION: Example of using `GetFeatureViewSyncOperator` from `airflow.providers.google.cloud.operators.vertex_ai.feature_store` to retrieve details of a feature view sync job in Vertex AI Feature Store. The sync job results are pushed to XCom under the 'return_value' key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_54\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/vertex_ai/example_vertex_ai_feature_store.py\n    :language: python\n    :dedent: 4\n    :start-after: [START how_to_cloud_vertex_ai_feature_store_get_feature_view_sync_operator]\n    :end-before: [END how_to_cloud_vertex_ai_feature_store_get_feature_view_sync_operator]\n```\n\n----------------------------------------\n\nTITLE: Creating a Google Cloud Run Service Configuration in Python\nDESCRIPTION: Shows how to define a configuration for a Google Cloud Run Service using Python in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_run.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nservice = {\n    \"template\": {\n        \"containers\": [\n            {\n                \"image\": \"us-docker.pkg.dev/cloudrun/container/hello\"\n            }\n        ]\n    },\n    \"traffic\": [\n        {\n            \"type\": \"TRAFFIC_TARGET_ALLOCATION_TYPE_LATEST\",\n            \"percent\": 100\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: DAG Definition with Custom Timetable\nDESCRIPTION: Example of how to use a custom timetable when defining a DAG, showing the SometimeAfterWorkdayTimetable implementation with a specific time parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/timetable.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    schedule=SometimeAfterWorkdayTimetable(Time(8)),  # 8am.\n    ...,\n):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow SQLite Provider with Optional Dependencies - Bash\nDESCRIPTION: Demonstrates how to install the apache-airflow-providers-sqlite Python package with the optional extra 'common.sql' using pip. This allows users to pull in cross-provider dependencies required for advanced SQL-related features. It assumes a compatible version of Airflow is already present. The key parameter is '[common.sql]' which triggers installation of associated extras. The command should be run in a shell environment where Python and pip are properly configured, and output or errors are handled by the terminal.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-sqlite[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Using Deferrable GCSObjectExistenceSensor in Python\nDESCRIPTION: Shows how to use the GCSObjectExistenceSensor in deferrable mode to free up worker slots while the sensor is running.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsensor_task_defered = GCSObjectExistenceSensor(\n    task_id=\"sensor_task_defered\",\n    bucket=BUCKET_NAME,\n    object=f\"{OBJECT_1}\",\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Running an Inline Workflow Template in Google Cloud Dataproc\nDESCRIPTION: This code uses DataprocInstantiateInlineWorkflowTemplateOperator to create, run, and then delete a workflow template in a single operation, which is useful for one-time workflow executions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ninstantiate_inline_workflow_template = DataprocInstantiateInlineWorkflowTemplateOperator(\n    task_id=\"instantiate_inline_workflow_template\",\n    template=WORKFLOW_TEMPLATE,\n    project_id=PROJECT_ID,\n    region=REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Wait for DV360 SDF Operation - Python Example\nDESCRIPTION: Example demonstrating how to wait for an SDF operation using GoogleDisplayVideo360GetSDFDownloadOperationSensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/display_video.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n[START howto_google_display_video_wait_for_operation_sensor]\n[END howto_google_display_video_wait_for_operation_sensor]\n```\n\n----------------------------------------\n\nTITLE: CloudSQL Instance Creation Operator Initialization Example (Python)\nDESCRIPTION: Demonstrates the creation and configuration of the CloudSQLCreateInstanceOperator in Airflow, optionally using a project ID or deriving it from the Google Cloud connection. The snippet requires the operator import and a prepared instance body. Inputs include the instance body, instance name, and optional project ID. Resulting operator is suitable for inclusion in Airflow DAGs to automate Cloud SQL instance provisioning.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"create_instance = CloudSQLCreateInstanceOperator(\\n    task_id=\\\"create_cloudsql_instance\\\",\\n    project_id=\\\"my-gcp-project\\\",  # pass None to use project from connection\\n    body=create_body,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Git Commit History for Apache Airflow SFTP Provider\nDESCRIPTION: This code snippet shows a formatted list of Git commits related to the Apache Airflow SFTP provider. Each line includes the commit hash, date, and commit message.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_28\n\nLANGUAGE: Markdown\nCODE:\n```\n`f6bd817a3a <https://github.com/apache/airflow/commit/f6bd817a3aac0a16430fc2e3d59c1f17a69a15ac>`__  2020-06-16   ``Introduce 'transfers' packages (#9320)``\n`0b0e4f7a4c <https://github.com/apache/airflow/commit/0b0e4f7a4cceff3efe15161fb40b984782760a34>`__  2020-05-26   ``Preparing for RC3 relase of backports (#9026)``\n`00642a46d0 <https://github.com/apache/airflow/commit/00642a46d019870c4decb3d0e47c01d6a25cb88c>`__  2020-05-26   ``Fixed name of 20 remaining wrongly named operators. (#8994)``\n`375d1ca229 <https://github.com/apache/airflow/commit/375d1ca229464617780623c61c6e8a1bf570c87f>`__  2020-05-19   ``Release candidate 2 for backport packages 2020.05.20 (#8898)``\n`12c5e5d8ae <https://github.com/apache/airflow/commit/12c5e5d8ae25fa633efe63ccf4db389e2b796d79>`__  2020-05-17   ``Prepare release candidate for backport packages (#8891)``\n`f3521fb0e3 <https://github.com/apache/airflow/commit/f3521fb0e36733d8bd356123e56a453fd37a6dca>`__  2020-05-16   ``Regenerate readme files for backport package release (#8886)``\n`92585ca4cb <https://github.com/apache/airflow/commit/92585ca4cb375ac879f4ab331b3a063106eb7b92>`__  2020-05-15   ``Added automated release notes generation for backport operators (#8807)``\n`bac0ab27cf <https://github.com/apache/airflow/commit/bac0ab27cfc89e715efddc97214fcd7738084361>`__  2020-03-30   ``close sftp connection without error (#7953)``\n`42eef38217 <https://github.com/apache/airflow/commit/42eef38217e709bc7a7f71bf0286e9e61293a43e>`__  2020-03-07   ``[AIRFLOW-6877] Add cross-provider dependencies as extras (#7506)``\n`97a429f9d0 <https://github.com/apache/airflow/commit/97a429f9d0cf740c5698060ad55f11e93cb57b55>`__  2020-02-02   ``[AIRFLOW-6714] Remove magic comments about UTF-8 (#7338)``\n`ceea293c16 <https://github.com/apache/airflow/commit/ceea293c1652240e7e856c201e4341a87ef97a0f>`__  2020-01-28   ``[AIRFLOW-6656] Fix AIP-21 moving (#7272)``\n`9a04013b0e <https://github.com/apache/airflow/commit/9a04013b0e40b0d744ff4ac9f008491806d60df2>`__  2020-01-27   ``[AIRFLOW-6646][AIP-21] Move protocols classes to providers package (#7268)``\n`69629a5a94 <https://github.com/apache/airflow/commit/69629a5a948ab2c4ac04a4a4dca6ac86d19c11bd>`__  2019-12-09   ``[AIRFLOW-5807] Move SFTP from contrib to providers. (#6464)``\n```\n\n----------------------------------------\n\nTITLE: Running a Dataplex Data Quality Scan Asynchronously - Python\nDESCRIPTION: Demonstrates usage of DataplexRunDataQualityScanOperator to start a Data Quality scan in asynchronous mode, enabling non-blocking execution. The operator returns immediately, and job execution is monitored using a sensor. Inputs include scan IDs and configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"run_dq_scan = DataplexRunDataQualityScanOperator(\\n    task_id=\\\"run_dq_scan\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    data_quality_scan_id=DATA_QUALITY_SCAN_ID,\\n    asynchronous=True,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow JDBC Provider with Extras Using pip (bash)\nDESCRIPTION: This snippet demonstrates how to install the 'apache-airflow-providers-jdbc' Python package with the 'common.sql' extra dependency using pip. The 'common.sql' extra ensures that any additional required provider integrations, such as for cross-provider functionality, are installed. The command must be run in a shell where an appropriate version of Python (3.9, 3.10, 3.11, or 3.12) and pip are available. Inputs include the package string; output is an updated Python environment with the provider and extras installed. Requires prior installation of Apache Airflow 2.9.0 or newer.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-jdbc[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Migrating BaseOperator Chain Functions Import in Python\nDESCRIPTION: Example showing how to update imports for chain, chain_linear, and cross_downstream functions which have been moved from airflow.models.baseoperator to airflow.sdk.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/aip-72.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models.baseoperator import chain, chain_linear, cross_downstream\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import chain, chain_linear, cross_downstream\n```\n\n----------------------------------------\n\nTITLE: Deleting Databricks Repo using DatabricksReposDeleteOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the DatabricksReposDeleteOperator to delete a Databricks Repo by specifying its path. It includes the necessary import and task definition within an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/repos_delete.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.databricks.operators.databricks import DatabricksReposDeleteOperator\n\ndelete_repo = DatabricksReposDeleteOperator(\n    task_id=\"delete_repo\",\n    databricks_conn_id=databricks_conn_id,\n    repo_path=\"/Repos/user@domain.com/demo-repo\",\n)\n```\n\n----------------------------------------\n\nTITLE: Writing XCom Values with KubernetesPodOperator in Python\nDESCRIPTION: Illustrates how to pass XCom values from a Pod using KubernetesPodOperator by writing to a specific file path.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Example code not provided in the original text\n```\n\n----------------------------------------\n\nTITLE: Defining Dataplex Entry Type Configuration in Python\nDESCRIPTION: This snippet shows a sample Python dictionary configuration for defining a Dataplex Entry Type. This configuration object is typically passed to the `DataplexCatalogCreateEntryTypeOperator` or `DataplexCatalogUpdateEntryTypeOperator`. The structure follows the `EntryType` resource definition.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 0\n#     :start-after: [START howto_dataplex_entry_type_configuration]\n#     :end-before: [END howto_dataplex_entry_type_configuration]\n\n# Example Entry Type configuration dictionary (structure shown in linked example)\n```\n\n----------------------------------------\n\nTITLE: Package naming for Celery Provider\nDESCRIPTION: Package identifier for the Apache Airflow Celery provider that enables Celery-based task execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n``apache-airflow-providers-celery``\n```\n\n----------------------------------------\n\nTITLE: Downloading and Verifying PyPI Releases of Apache Airflow Provider Packages\nDESCRIPTION: Bash script to download an Apache Airflow provider package from PyPI along with its signature and SHA512 checksum files. This script facilitates the verification of PyPI releases.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nPACKAGE_VERSION={{ package_version }}\nPACKAGE_NAME={{ package_name }}\nprovider_download_dir=$(mktemp -d)\npip download --no-deps \"${PACKAGE_NAME}==${PACKAGE_VERSION}\" --dest \"${provider_download_dir}\"\ncurl \"{{ base_url }}/{{ package_name_underscores }}-{{ package_version }}-py3-none-any.whl.asc\" \\\n    -L -o \"${provider_download_dir}/{{ package_name_underscores }}-{{ package_version }}-py3-none-any.whl.asc\"\ncurl \"{{ base_url }}/{{ package_name_underscores }}-{{ package_version }}-py3-none-any.whl.sha512\" \\\n    -L -o \"${provider_download_dir}/{{ package_name_underscores }}-{{ package_version }}-py3-none-any.whl.sha512\"\necho\necho \"Please verify files downloaded to ${provider_download_dir}\"\nls -la \"${provider_download_dir}\"\necho\n```\n\n----------------------------------------\n\nTITLE: Installing SSH Provider Package for Apache Airflow\nDESCRIPTION: Command to install the SSH provider package for Apache Airflow using pip. This package supports Python versions 3.9, 3.10, 3.11, and 3.12.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-ssh\n```\n\n----------------------------------------\n\nTITLE: Including Security Information in reStructuredText\nDESCRIPTION: This snippet includes an external file containing security information for the Apache Airflow project using reStructuredText syntax.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Defining an Update for a Cloud Composer Environment in Python\nDESCRIPTION: This snippet shows how to define an update configuration for a Cloud Composer environment. It includes changes to the environment size and web server machine type.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nUPDATE_CONFIG = {\n    \"environment_size\": \"ENVIRONMENT_SIZE_MEDIUM\",\n    \"web_server_config\": {\n        \"machine_type\": \"composer-n1-webserver-4\"\n    }\n}\n\nUPDATE_MASK = {\n    \"paths\": [\"environment_size\", \"web_server_config.machine_type\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Google Cloud Workflow Executions using Airflow Operator in Python\nDESCRIPTION: Shows how to list executions for a specific workflow using the `WorkflowsListExecutionsOperator`. Requires `workflow_id` and `location_id`. By default, it lists executions from the last 60 minutes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/workflows.rst#2025-04-22_snippet_9\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/workflows/example_workflows.py\n      :language: python\n      :dedent: 4\n      :start-after: [START how_to_list_executions]\n      :end-before: [END how_to_list_executions]\n```\n\n----------------------------------------\n\nTITLE: Granting Operational Airflow Permissions to a User Group in Cedar (Op Role)\nDESCRIPTION: This Cedar policy grants operational permissions to users within a specified AWS IAM Identity Center group (ID `aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee`). It includes read access (GET) and modification permissions (POST, PUT, DELETE) for DAGs, Connections, Pools, Variables, Assets, and Backfills. This policy aligns with the permissions of the 'Op' (Operator) role in the default Flask AppBuilder authentication manager.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/manage/index.rst#2025-04-22_snippet_4\n\nLANGUAGE: cedar\nCODE:\n```\npermit(\n  principal in Airflow::Group::\"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\",\n  action in [\n    Airflow::Action::\"Configuration.GET\",\n    Airflow::Action::\"Connection.GET\",\n    Airflow::Action::\"Custom.GET\",\n    Airflow::Action::\"Dag.GET\",\n    Airflow::Action::\"Menu.MENU\",\n    Airflow::Action::\"Pool.GET\",\n    Airflow::Action::\"Variable.GET\",\n    Airflow::Action::\"Asset.GET\",\n    Airflow::Action::\"View.GET\",\n    Airflow::Action::\"Dag.POST\",\n    Airflow::Action::\"Dag.PUT\",\n    Airflow::Action::\"Dag.DELETE\",\n    Airflow::Action::\"Connection.POST\",\n    Airflow::Action::\"Connection.PUT\",\n    Airflow::Action::\"Connection.DELETE\",\n    Airflow::Action::\"Pool.POST\",\n    Airflow::Action::\"Pool.PUT\",\n    Airflow::Action::\"Pool.DELETE\",\n    Airflow::Action::\"Variable.POST\",\n    Airflow::Action::\"Variable.PUT\",\n    Airflow::Action::\"Variable.DELETE\",\n    Airflow::Action::\"Asset.POST\",\n    Airflow::Action::\"Asset.PUT\",\n    Airflow::Action::\"Asset.DELETE\",\n    Airflow::Action::\"Backfill.POST\",\n    Airflow::Action::\"Backfill.PUT\",\n\n  ],\n  resource\n);\n```\n\n----------------------------------------\n\nTITLE: Building Custom Airflow Operators Package with Hatch\nDESCRIPTION: Demonstrates how to build a custom Airflow operators package using the Hatch build tool, creating a wheel file for distribution.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/modules_management.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhatch build -t wheel\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow with Kerberos Support\nDESCRIPTION: Pip command to install Apache Airflow with Kerberos extras group for Kerberos authentication support.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/kerberos.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[kerberos]'\n```\n\n----------------------------------------\n\nTITLE: Getting Operation State in Google Cloud Datastore using Python\nDESCRIPTION: This snippet demonstrates how to use CloudDatastoreGetOperationOperator to get the current state of a long-running operation in Google Cloud Datastore. It specifies the name of the operation to retrieve.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nget_operation = CloudDatastoreGetOperationOperator(\n    task_id=\"get_operation\",\n    name=\"{{ task_instance.xcom_pull('export_task')['name'] }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataplex Task Asynchronously - Python\nDESCRIPTION: Invokes DataplexCreateTaskOperator with asynchronous mode enabled to initiate task creation without waiting for completion. Requires Airflow's Dataplex provider and a proper configuration dictionary. The operator will return immediately; task completion should be monitored with an Airflow sensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"create_task_async = DataplexCreateTaskOperator(\\n    task_id=\\\"create_task_async\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    lake_id=LAKE_ID,\\n    body=task_config,\\n    asynchronous=True,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Deleting a Pipeline Job using Vertex AI Pipeline Job Operator - Python\nDESCRIPTION: Illustrates deletion of a Vertex AI pipeline job using DeletePipelineJobOperator. Users must specify job, project, and region identifiers. Care must be taken, as this task permanently removes the pipeline job.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndelete_pipeline_job_task = DeletePipelineJobOperator(\n    task_id=\"delete_pipeline_job_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    pipeline_job_id=PIPELINE_JOB_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding PyPI Packages from requirements.txt to Airflow Docker Image\nDESCRIPTION: This Dockerfile example demonstrates how to add multiple PyPI packages from a requirements.txt file to the Airflow image. It uses the airflow user and explicitly installs the matching Airflow version.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_2\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apache/airflow:2.7.1\nCOPY requirements.txt /\nUSER airflow\nRUN pip install --no-cache-dir apache-airflow==2.7.1 -r /requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Provider with Optional Extras (bash)\nDESCRIPTION: This bash code snippet demonstrates how to install the 'apache-airflow-providers-common-compat' Python package along with the optional 'openlineage' extra via pip. Prerequisites include an installed Apache Airflow version >=2.9.0 and Python 3.9 or newer. The key parameter is the pip install command, where the optional extras are specified in square brackets. Input: the snippet is a shell command; output: successful installation of the compatibility provider and any specified extra dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/compat/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-common-compat[openlineage]\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Breeze Dependencies\nDESCRIPTION: Command to synchronize Breeze dependencies using UV package manager with the latest dependencies from the lock file.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nuv sync\n```\n\n----------------------------------------\n\nTITLE: Defining Postgres Table Schema (SQL)\nDESCRIPTION: SQL script, intended to be saved as `dags/sql/pet_schema.sql`, to create the `pet` table if it doesn't exist. Defines columns like `pet_id`, `name`, `pet_type`, `birth_date`, and `OWNER` with appropriate data types and constraints (SERIAL PRIMARY KEY, NOT NULL, DATE).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- create pet table\nCREATE TABLE IF NOT EXISTS pet (\n    pet_id SERIAL PRIMARY KEY,\n    name VARCHAR NOT NULL,\n    pet_type VARCHAR NOT NULL,\n    birth_date DATE NOT NULL,\n    OWNER VARCHAR NOT NULL);\n```\n\n----------------------------------------\n\nTITLE: Copying a Directory from GCS to Samba with Wildcard (Python)\nDESCRIPTION: This Python snippet demonstrates how to copy all files matching a wildcard pattern from a GCS directory to Samba using Airflow's GCSToSambaOperator. The 'source_path' can include wildcards (e.g., '*') to select multiple files. The operator loops over matching objects and copies each to the destination Samba path while preserving file structure as needed. Inputs include the GCS bucket and wildcard-based path, and output is the bulk-copied files on Samba.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/transfer/gcs_to_samba.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nGCSToSambaOperator(\n    task_id=\"copy_gcs_to_samba_directory\",\n    source_bucket=\"example-source-bucket\",\n    source_path=\"folder/subfolder/*\",\n    samba_server=\"example.samba.server\",\n    share_name=\"SHARE\",\n    destination_path=\"folder_on_samba/subfolder/\",\n    samba_username=\"airflow\",\n    samba_password=\"airflow\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache HDFS Provider Package for Airflow\nDESCRIPTION: Command to install the Apache HDFS provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-hdfs\n```\n\n----------------------------------------\n\nTITLE: Updating GCE Instance Group Manager Template Without Project ID in Python\nDESCRIPTION: Creates a ComputeEngineInstanceGroupUpdateManagerTemplateOperator without specifying a project ID, which will be automatically retrieved from the Google Cloud connection used.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nupdate_template_no_project_id = ComputeEngineInstanceGroupUpdateManagerTemplateOperator(\n    task_id=\"update_template_no_project_id\",\n    resource_id=IGM_NAME,\n    zone=GCE_ZONE,\n    source_template=f\"global/instanceTemplates/{TEMPLATE_NAME}\",\n    destination_template=f\"global/instanceTemplates/{UPDATED_TEMPLATE_NAME}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Example for Identifying Top-Level Code in Airflow DAGs\nDESCRIPTION: Code snippet showing how to identify whether code is executed at top-level during DAG parsing. This example shows the beginning of a DAG file with functions that may or may not be executed during parsing.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import DAG\nfrom airflow.providers.standard.operators.python import PythonOperator\nimport pendulum\n\n\ndef get_task_id():\n    return \"print_array_task\"  # <- is that code going to be executed?\n\n\ndef get_array():\n    return [1, 2, 3]  # <- is that code going to be executed?\n\n\nwith DAG(\n    dag_id=\"example_python_operator\",\n    schedule=None,\n```\n\n----------------------------------------\n\nTITLE: Updating AlloyDB Cluster with Airflow Operator\nDESCRIPTION: Uses AlloyDBUpdateClusterOperator to update an existing AlloyDB cluster in Google Cloud.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/alloy_db.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nupdate_cluster = AlloyDBUpdateClusterOperator(\n    task_id=\"update_cluster\",\n    project_id=GCP_PROJECT_ID,\n    cluster_id=CLUSTER_ID,\n    instance=ALLOYDB_CLUSTER,\n    region=GCP_ALLOYDB_REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Getting Cloud Memorystore Instance Details\nDESCRIPTION: Example showing how to retrieve details of a Cloud Memorystore instance\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nget_instance = CloudMemorystoreGetInstanceOperator(task_id=\"get-instance\", location=\"europe-north1\", instance=INSTANCE_NAME, project_id=PROJECT_ID)\n```\n\n----------------------------------------\n\nTITLE: Exporting Cloud Memorystore Instance\nDESCRIPTION: Example showing how to export a Cloud Memorystore instance data to Cloud Storage\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nexport_instance = CloudMemorystoreExportInstanceOperator(task_id=\"export-instance\", location=\"europe-north1\", instance=INSTANCE_NAME, project_id=PROJECT_ID, output_config={\"gcs_destination\": {\"uri\": \"gs://test-memorystore/my-export.rdb\"}})\n```\n\n----------------------------------------\n\nTITLE: Upgrading Apache Airflow Helm Chart\nDESCRIPTION: Command to upgrade an existing Airflow installation using Helm.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade airflow apache-airflow/airflow --namespace airflow\n```\n\n----------------------------------------\n\nTITLE: Templating with @task Decorator in Airflow\nDESCRIPTION: Shows how to use Jinja templating with a Python function decorated with @task in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/python.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@task(task_id=\"render_sql\")\ndef render_sql(sql_template: str):\n    # Jinja template variables are rendered automatically\n    return sql_template\n\nrendered_sql = render_sql(\n    \"SELECT * FROM {{ params.table }} WHERE DATE = {{ data_interval_start.strftime('%Y-%m-%d') }}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Complete Example DAG using Teradata Operators in Python\nDESCRIPTION: This snippet presents a complete Airflow DAG showcasing the integration of Teradata operators, likely including the `TeradataToTeradataOperator`. It illustrates how to set up a workflow involving Teradata tasks within the broader context of an Airflow DAG definition. Dependencies include Apache Airflow and the `apache-airflow-providers-teradata` package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/operators/teradata_to_teradata.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../teradata/tests/system/teradata/example_teradata.py\n#    :language: python\n#    :start-after: [START teradata_operator_howto_guide]\n#    :end-before: [END teradata_operator_howto_guide]\n\n# This placeholder represents the complete Python DAG code included from the specified file.\n# The actual code would define a DAG object and instantiate various tasks,\n# potentially including TeradataOperator for executing SQL and \n# TeradataToTeradataOperator for data transfers, showing task dependencies.\n# Example structure:\n# from airflow import DAG\n# from airflow.providers.teradata.operators.teradata import TeradataOperator\n# from airflow.providers.teradata.transfers.teradata_to_teradata import TeradataToTeradataOperator\n# from datetime import datetime\n# \n# with DAG(...) as dag:\n#     # ... task definitions using Teradata operators ...\n\n```\n\n----------------------------------------\n\nTITLE: Monitoring SageMaker Processing Job State in Python\nDESCRIPTION: This code snippet demonstrates the use of SageMakerProcessingSensor to check the state of an Amazon SageMaker processing job until it reaches a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_sensor_sagemaker_processing]\n# Code snippet not provided in the original text\n# [END howto_sensor_sagemaker_processing]\n```\n\n----------------------------------------\n\nTITLE: Installing MySQL Provider Package via pip\nDESCRIPTION: Command to install the MySQL provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-mysql[amazon]\n```\n\n----------------------------------------\n\nTITLE: Manual Docker Compose Installation\nDESCRIPTION: Script to manually download and install latest version of Docker Compose.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nCOMPOSE_VERSION=\"$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep '\"tag_name\":'\\\n| cut -d '\"' -f 4)\"\n\nCOMPOSE_URL=\"https://github.com/docker/compose/releases/download/${COMPOSE_VERSION}/\\\ndocker-compose-$(uname -s)-$(uname -m)\"\n\nsudo curl -L \"${COMPOSE_URL}\" -o /usr/local/bin/docker-compose\n\nsudo chmod +x /usr/local/bin/docker-compose\n```\n\n----------------------------------------\n\nTITLE: Defining Airflow Variables via Environment Variables\nDESCRIPTION: Sets Airflow variables directly through environment variables, providing a way to configure variables without using the Airflow UI.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/cli-and-env-variables-ref.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_VAR_{KEY}\n```\n\n----------------------------------------\n\nTITLE: Verifying Papermill Notebook Output with Remote Jupyter Kernel in Python\nDESCRIPTION: This snippet demonstrates how to verify the output of a Papermill-executed notebook using a remote Jupyter kernel. It includes setting up the remote kernel connection and executing code remotely.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/operators.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef verify_notebook_output(ti, **kwargs):\n    from jupyter_client import BlockingKernelClient\n    import json\n\n    connection_file = kwargs['dag_run'].conf['connection_file']\n    with open(connection_file) as f:\n        config = json.load(f)\n\n    client = BlockingKernelClient()\n    client.load_connection_file(connection_file)\n    client.start_channels()\n\n    code = '''\n    import json\n    from pathlib import Path\n\n    exec_date = \"{{ execution_date }}\"\n    notebook_path = f\"/tmp/out-{exec_date}.ipynb\"\n    notebook_output = Path(notebook_path).read_text()\n\n    if 'Hello Airflow!' not in notebook_output:\n        raise ValueError(\"Unexpected output in notebook\")\n    '''\n\n    msg_id = client.execute(code)\n    reply = client.get_shell_msg(timeout=60)\n    if reply['content']['status'] != 'ok':\n        raise ValueError(f\"Error executing code: {reply['content']}\")\n\n    client.stop_channels()\n\nverify_notebook = PythonOperator(\n    task_id='verify_notebook',\n    python_callable=verify_notebook_output,\n    provide_context=True,\n    dag=dag,\n)\n\nexecute_notebook >> verify_notebook\n```\n\n----------------------------------------\n\nTITLE: Installing Celery Provider with CNCF Kubernetes Dependency\nDESCRIPTION: Command to install the Celery provider package with cross-provider dependency for CNCF Kubernetes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-celery[cncf.kubernetes]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenLineage Extractors in Airflow with INI Syntax\nDESCRIPTION: This example demonstrates how to register custom Extractors with Airflow's OpenLineage integration using the Airflow configuration (INI format). The 'extractors' setting lists fully qualified Python class paths, separated by semicolons, specifying which custom extractor classes should be loaded by the plugin. Dependencies include Airflow and OpenLineage packages, and the extractors should be importable from scheduler and worker environments.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/developer.rst#2025-04-22_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\\ntransport = {\\\"type\\\": \\\"http\\\", \\\"url\\\": \\\"http://example.com:5000\\\"}\\nextractors = full.path.to.ExtractorClass;full.path.to.AnotherExtractorClass\\n\n```\n\n----------------------------------------\n\nTITLE: Setting Azure Container Volume Connection Environment Variable in Bash\nDESCRIPTION: Example of how to set an environment variable for Azure Container Volume connection using token credentials. The connection string is formatted as a URI with URL-encoded components.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/azure_container_volume.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_WASP_DEFAULT='azure_container_volume://blob%20username:blob%20password@myblob.com'\n```\n\n----------------------------------------\n\nTITLE: Granting Full Airflow Permissions to a User Group in Cedar (Admin Role)\nDESCRIPTION: This Cedar policy grants all possible actions (`action`) on all Airflow resources (`resource`) to all users belonging to a specific group defined in AWS IAM Identity Center. The group is identified by its unique ID (`aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee`). This policy effectively replicates the permissions of the 'Admin' role found in the default Flask AppBuilder authentication manager.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/manage/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: cedar\nCODE:\n```\npermit(\n  principal in Airflow::Group::\"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\",\n  action,\n  resource\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS IAM Authentication for RDS/Aurora PostgreSQL in Airflow (JSON)\nDESCRIPTION: This JSON snippet shows how to configure the 'Extra' field for an Airflow PostgreSQL connection to use AWS IAM database authentication for Amazon RDS or Aurora. Setting 'iam' to true enables this feature, and 'aws_conn_id' specifies the Airflow AWS connection to use for retrieving temporary credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/connections/postgres.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"iam\": true,\n   \"aws_conn_id\": \"aws_awesome_rds_conn\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Druid Provider Package via pip\nDESCRIPTION: Command to install the Apache Druid provider package with optional Hive dependencies using pip package manager.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/druid/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-druid[apache.hive]\n```\n\n----------------------------------------\n\nTITLE: Configuring Vault Connection Extras in JSON - JSON\nDESCRIPTION: This JSON snippet provides an example configuration for the 'extras' field used when setting up a Hashicorp Vault connection in Airflow. It specifies 'auth_type' as 'kubernetes' and sets 'kubernetes_role' to 'vault_role'. Required dependencies are Airflow's Vault integration and appropriate authentication setup on the Vault server. Inputs are JSON key-value pairs, and the output is a configuration object attached to the Airflow Vault connection. Only specified fields will be interpreted for connection setup; missing incompatible fields may cause authentication to fail.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/hashicorp/docs/connections/vault.rst#2025-04-22_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"auth_type\": \"kubernetes\",\n  \"kubernetes_role\": \"vault_role\",\n}\n```\n\n----------------------------------------\n\nTITLE: Asynchronous PDT Build Operation with Sensor in Looker\nDESCRIPTION: Example of using LookerStartPdtBuildOperator with LookerCheckPdtBuildSensor for asynchronous PDT materialization job execution\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/looker.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nbuild_pdt_async = LookerStartPdtBuildOperator(\n    task_id=\"build_pdt_async\",\n    model=\"model_name\",\n    view=\"view_name\",\n    asynchronous=True,\n)\n\ncheck_async_pdt_state = LookerCheckPdtBuildSensor(\n    task_id=\"check_async_pdt_state\",\n    materialization_id=build_pdt_async.output,\n)\n```\n\n----------------------------------------\n\nTITLE: Fixing Usage of range(len()) Pattern\nDESCRIPTION: Bug Fix (Version 2.1.0): Replaces the anti-pattern `range(len(sequence))` with `enumerate(sequence)` for cleaner and more Pythonic iteration, referencing pull request #18174.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_30\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Fix usage of range(len() to enumerate (#18174)``\n```\n\n----------------------------------------\n\nTITLE: Deleting SageMaker Model\nDESCRIPTION: Example showing how to use SageMakerDeleteModelOperator to delete a model from Amazon SageMaker.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndelete_model_task = SageMakerDeleteModelOperator(\n    task_id='delete_model_task',\n    config={\n        \"ModelName\": \"demo-model\"\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Instantiating ObjectStoragePath with Separate Connection ID Parameter\nDESCRIPTION: Shows how to create an ObjectStoragePath instance by providing the connection ID as a separate parameter instead of embedding it in the URI.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/objectstorage.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Equivalent to the previous example.\nbase = ObjectStoragePath(\"s3://my-bucket/\", conn_id=\"aws_default\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Boto3 Retry Strategy in AWS Config File (INI)\nDESCRIPTION: This snippet shows how to configure the Boto3 retry strategy within an AWS profile ('awesome_aws_profile') in the `~/.aws/config` file. It sets the `retry_mode` to `standard` and `max_attempts` to 10. This configuration can then be referenced by an Airflow connection using the 'profile_name' in the Extra field.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_13\n\nLANGUAGE: ini\nCODE:\n```\n[profile awesome_aws_profile]\nretry_mode = standard\nmax_attempts = 10\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Extraction in Python\nDESCRIPTION: Function to extract order data from a JSON string and convert it to a Python dictionary.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef extract():\n    data_string = '{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}'\n    return json.loads(data_string)\n```\n\n----------------------------------------\n\nTITLE: Using Command to Set Airflow Database Connection in INI Configuration\nDESCRIPTION: This configuration snippet demonstrates how to use a command to set the database connection string at runtime in the airflow.cfg file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-config.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[database]\nsql_alchemy_conn_cmd = bash_command_to_run\n```\n\n----------------------------------------\n\nTITLE: Getting a Tag Template with CloudDataCatalogGetTagTemplateOperator in Python\nDESCRIPTION: Example of retrieving a tag template from Google Cloud DataCatalog using the CloudDataCatalogGetTagTemplateOperator. The result is saved to XCom for use in other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_get_tag_template]\nget_tag_template = CloudDataCatalogGetTagTemplateOperator(\n    task_id=\"get_tag_template\",\n    location=LOCATION,\n    tag_template=\"{tag_template}\",\n)\n# [END howto_operator_gcp_datacatalog_get_tag_template]\n```\n\n----------------------------------------\n\nTITLE: Setting Default Arguments in DAG Definition\nDESCRIPTION: Example of creating a DAG with timezone-aware start date using pendulum and setting default arguments.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timezone.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndag = DAG(\n    \"my_dag\",\n    start_date=pendulum.datetime(2017, 1, 1, tz=\"UTC\"),\n    default_args={\"retries\": 3},\n)\nop = BashOperator(task_id=\"hello_world\", bash_command=\"Hello World!\", dag=dag)\nprint(op.retries)  # 3\n```\n\n----------------------------------------\n\nTITLE: Inserting a New Google Campaign Manager Report in Airflow\nDESCRIPTION: Example of using GoogleCampaignManagerInsertReportOperator to create a new report. The operator supports Jinja templating and can provide report definition using a .json file. Results are saved to XCom for use by other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/campaign_manager.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninsert_report = GoogleCampaignManagerInsertReportOperator(\n    profile_id=PROFILE_ID,\n    report={\n        \"name\": \"Test report\",\n        \"type\": \"STANDARD\",\n        \"fileName\": \"test_report\",\n        \"format\": \"CSV\",\n    },\n    task_id=\"insert_report\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing TimeDeltaSensor in Apache Airflow\nDESCRIPTION: This snippet demonstrates how to use the TimeDeltaSensor to end sensing after a specific time delta. It creates a sensor that waits for 5 days from the start date.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/datetime.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nTimeDeltaSensor(\n    task_id=\"wait_for_5_days\",\n    delta=timedelta(days=5),\n)\n```\n\n----------------------------------------\n\nTITLE: Receiving Message from Azure Service Bus Subscription in Python\nDESCRIPTION: This example shows how to use the ASBReceiveSubscriptionMessageOperator to receive a batch of messages from an Azure Service Bus subscription under a specific topic.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/asb.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_receive_message_service_bus_subscription]\n# [END howto_operator_receive_message_service_bus_subscription]\n```\n\n----------------------------------------\n\nTITLE: Creating and Executing AWS DataSync Task Dynamically Using Airflow DataSyncOperator - Python\nDESCRIPTION: This code example shows how Airflow's DataSyncOperator can be configured to create a new DataSync task and its source/destination locations if no existing matching task is found, execute it, and then optionally delete it after execution. Dependencies include Airflow's AWS provider, AWS permission to create/delete DataSync resources, and properly specified create_*_kwargs dictionaries. Parameters: create_task_kwargs, create_source_location_kwargs, create_destination_location_kwargs, delete_task_after_execution (controls post-execution cleanup).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/datasync.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nfrom airflow.providers.amazon.aws.operators.datasync import DataSyncOperator\n\ndatasync_create_and_execute_task = DataSyncOperator(\n    task_id='datasync_create_and_execute_task',\n    source_location_uri='smb://onprem/server/path',\n    destination_location_uri='s3://my-bucket/path',\n    create_source_location_kwargs={\n        'SmbParameters': { ... }  # specify SMB parameters here\n    },\n    create_destination_location_kwargs={\n        'S3Parameters': { ... }  # specify S3 parameters here\n    },\n    create_task_kwargs={\n        'Name': 'MyNewSyncTask',\n        # any additional DataSync Task params\n    },\n    delete_task_after_execution=True,\n)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Synchronous PDT Build Operation in Looker\nDESCRIPTION: Example of using LookerStartPdtBuildOperator to submit a PDT materialization job in synchronous/blocking mode\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/looker.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbuild_pdt = LookerStartPdtBuildOperator(\n    task_id=\"build_pdt\",\n    model=\"model_name\",\n    view=\"view_name\",)\n```\n\n----------------------------------------\n\nTITLE: Waiting for Google Cloud Workflow Execution Completion using Airflow Sensor in Python\nDESCRIPTION: Shows how to use the `WorkflowExecutionSensor` to pause a DAG run until a specific workflow execution completes. It requires `workflow_id`, `location_id`, and `execution_id` to monitor the execution status.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/workflows.rst#2025-04-22_snippet_7\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/workflows/example_workflows.py\n      :language: python\n      :dedent: 4\n      :start-after: [START how_to_wait_for_execution]\n      :end-before: [END how_to_wait_for_execution]\n```\n\n----------------------------------------\n\nTITLE: Templating Document Text Detection Field - Google Cloud Vision Airflow Operator - Python\nDESCRIPTION: This field definition snippet lists template fields for the CloudVisionTextDetectOperator in Airflow, allowing for the dynamic assignment of project ID, location, image, and retry parameters in workflow templates. This supports flexible workflow management and makes the operator reusable in different GCP contexts.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"project_id\",\n    \"location\",\n    \"image\",\n    \"retry\",\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting AutoML Translation Dataset with TranslateDeleteDatasetOperator in Python\nDESCRIPTION: Demonstrates how to use the `TranslateDeleteDatasetOperator` in an Airflow DAG to delete a specific AutoML translation dataset using the V3 API. It requires the `dataset_id`, `project_id`, and `location` to identify the dataset to be deleted.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndelete_dataset = TranslateDeleteDatasetOperator(\n    task_id=\"delete_dataset\",\n    dataset_id=DATASET_ID,\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n)\n```\n\n----------------------------------------\n\nTITLE: Verifying a Built Docker Image with Airflow Utility Script - Shell\nDESCRIPTION: This shell command runs the Airflow-provided utility script to check if a custom-built Docker image is production-ready. It requires the Airflow source code checked out locally and the path to the verify_docker_image.sh script. The key parameters are 'PROD' (build type) and 'my-image:0.0.1' (the tag of your image to test). The script outputs diagnostic results indicating image suitability. Limitations include the necessity of local Airflow sources and compatibility with the targeted Airflow version.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n./scripts/ci/tools/verify_docker_image.sh PROD my-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Exporting Tableau Connection via Environment Variable in Bash\nDESCRIPTION: This snippet demonstrates how to configure a Tableau connection for Airflow by exporting the connection URI as an environment variable using Bash. It requires the URI to be URL-encoded and includes username, password, server URL, and an optional 'site_id' parameter. Inputs are the connection details, and the expected effect is that Airflow recognizes 'tableau_default' as the Tableau connection when reading environment variables. Dependencies include a running Airflow environment and correct user credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/tableau/docs/connections/tableau.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_TABLEAU_DEFAULT='tableau://username:password@https%3A%2F%2FMY-SERVER%2F?site_id=example-id'\n```\n\n----------------------------------------\n\nTITLE: Triggering Cloud Memorystore Instance Failover\nDESCRIPTION: Example showing how to trigger a failover for a Cloud Memorystore instance\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfailover_instance = CloudMemorystoreFailoverInstanceOperator(task_id=\"failover-instance\", location=\"europe-north1\", instance=INSTANCE_NAME, project_id=PROJECT_ID)\n```\n\n----------------------------------------\n\nTITLE: Listing Tags in a Repository with GithubOperator in Airflow\nDESCRIPTION: Example of using GithubOperator to list all tags in a specific repository. This snippet shows how to implement the PyGithub client.get_repo(full_name_or_id='apache/airflow').get_tags() method.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/github/docs/operators/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlist_repo_tags = GithubOperator(\n    task_id=\"list_repo_tags\",\n    github_method=\"get_repo\",\n    github_method_args={\"full_name_or_id\": \"apache/airflow\"},\n    result_processor=lambda repo: [tag.name for tag in repo.get_tags()],\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting Objects from GCS Bucket in Python\nDESCRIPTION: Demonstrates the use of GCSDeleteObjectsOperator to delete one or more objects from a Google Cloud Storage bucket.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndelete_task = GCSDeleteObjectsOperator(\n    task_id=\"delete_files\", bucket_name=BUCKET_NAME, objects=delete_list\n)\n```\n\n----------------------------------------\n\nTITLE: Running Spellcheck Only with Breeze\nDESCRIPTION: This command runs only the spellcheck on the documentation using the Breeze development environment.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/docs/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs --spellcheck-only\n```\n\n----------------------------------------\n\nTITLE: Updating Dataplex Entry Group using Airflow Python\nDESCRIPTION: This snippet demonstrates how to update an existing Entry Group in Google Cloud Dataplex Catalog using the `DataplexCatalogUpdateEntryGroupOperator` in an Airflow DAG. It references an external example file for the specific implementation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_39\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_update_entry_group]\n#     :end-before: [END howto_operator_dataplex_catalog_update_entry_group]\n\n# This example uses DataplexCatalogUpdateEntryGroupOperator\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Implementing Pod Mutation Hook in Airflow Local Settings (Python)\nDESCRIPTION: This code snippet demonstrates how to define a pod_mutation_hook function in the Airflow local settings file. The function modifies Kubernetes pod objects before they are sent for scheduling, adding an annotation to indicate the pod was launched by tests.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/kubernetes.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kubernetes.client.models import V1Pod\n\n\ndef pod_mutation_hook(pod: V1Pod):\n    pod.metadata.annotations[\"airflow.apache.org/launched-by\"] = \"Tests\"\n```\n\n----------------------------------------\n\nTITLE: Defining Teradata Extras (JSON) in Airflow UI\nDESCRIPTION: This JSON snippet demonstrates how to configure extra parameters for a Teradata connection within the Airflow UI's 'Extras' field. It shows examples for setting the transaction mode ('tmode'), SSL mode ('sslmode'), and specifying paths to SSL certificate files ('sslcert', 'sslca', 'sslkey'). These parameters allow for fine-grained control over the database connection behavior, particularly security settings.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/connections/teradata.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"tmode\": \"TERA\",\n   \"sslmode\": \"verify-ca\",\n   \"sslcert\": \"/tmp/client-cert.pem\",\n   \"sslca\": \"/tmp/server-ca.pem\",\n   \"sslkey\": \"/tmp/client-key.pem\"\n}\n```\n\n----------------------------------------\n\nTITLE: Getting a Pipeline Job using Vertex AI Pipeline Job Operator - Python\nDESCRIPTION: Shows how to fetch details of a specific Vertex AI pipeline job using GetPipelineJobOperator. Needs valid project, region, and job ID inputs. Returns the pipeline job configuration and status, useful for workflow introspection and conditional branching.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nget_pipeline_job_task = GetPipelineJobOperator(\n    task_id=\"get_pipeline_job_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    pipeline_job_id=PIPELINE_JOB_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Slack Connection Using Python\nDESCRIPTION: Shows how to programmatically create a Slack connection using Python code. Creates a Connection object with connection ID, type, password (API token), and extra parameters, then generates the corresponding environment variable URI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/connections/slack.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models.connection import Connection\n\nconn = Connection(\n    conn_id=\"slack_api_default\",\n    conn_type=\"slack\",\n    password=\"xoxb-1234567890123-09876543210987-AbCdEfGhIjKlMnOpQrStUvWx\",\n    extra={\n        # Specify extra parameters here\n        \"timeout\": \"42\",\n    },\n)\n\n# Generate Environment Variable Name\nenv_key = f\"AIRFLOW_CONN_{conn.conn_id.upper()}\"\n\nprint(f\"{env_key}='{conn.get_uri()}'\")\n# AIRFLOW_CONN_SLACK_API_DEFAULT='slack://:xoxb-1234567890123-09876543210987-AbCdEfGhIjKlMnOpQrStUvWx@/?timeout=42'\n```\n\n----------------------------------------\n\nTITLE: Retrieving Configuration Values from Secrets Backend\nDESCRIPTION: Configuration values can be retrieved from a configured secrets backend. The returned value is used for the corresponding Airflow configuration setting.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/cli-and-env-variables-ref.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW__{SECTION}__{KEY}_SECRET\n```\n\n----------------------------------------\n\nTITLE: Creating Yandex Lockbox Secret for Airflow Connection (JSON) using YC CLI\nDESCRIPTION: Shows how to create a secret in Yandex Lockbox using the `yc` CLI, storing an Airflow connection named `my_sql_db_json`. The connection details are provided as an escaped JSON string within the payload's `text_value`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_12\n\nLANGUAGE: console\nCODE:\n```\n$ yc lockbox secret create \\\n    --name airflow/connections/my_sql_db_json \\\n    --payload '[{\"key\": \"value\", \"text_value\": \"{\\\"conn_type\\\": \\\"mysql\\\", \\\"host\\\": \\\"host.com\\\", \\\"login\\\": \\\"myname\\\", \\\"password\\\": \\\"mypassword\\\", \\\"extra\\\": {\\\"this_param\\\": \\\"some val\\\", \\\"that_param\\\": \\\"other val*\\\"}}\"}]'\ndone (1s)\nname: airflow/connections/my_sql_db_json\n```\n\n----------------------------------------\n\nTITLE: Configuring SQLite Connection with Absolute Path in Airflow URI Format\nDESCRIPTION: This example shows how to configure a SQLite connection using an absolute path in the Airflow URI format. Note the three slashes after the connection type for absolute paths.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/connections/sqlite.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SQLITE_DEFAULT='sqlite:///absolute/path/to/db?mode=ro'\n```\n\n----------------------------------------\n\nTITLE: Resuming a Job on GKE using GKEResumeJobOperator in Python\nDESCRIPTION: This snippet shows how to use the `GKEResumeJobOperator` to resume a previously suspended Job within a specified GKE cluster. This operator allows restarting the execution of a paused job.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/kubernetes_engine/example_kubernetes_engine_job.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_gke_resume_job]\n    :end-before: [END howto_operator_gke_resume_job]\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS SES Email Backend (INI)\nDESCRIPTION: This snippet shows how to configure the AWS SES email backend in the Airflow configuration file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_12\n\nLANGUAGE: ini\nCODE:\n```\n[email]\nemail_backend = airflow.providers.amazon.aws.utils.emailer.send_email\nemail_conn_id = aws_default\nfrom_email = From email <email@example.com>\n```\n\n----------------------------------------\n\nTITLE: Template Fields for CloudVisionAddProductToProductSetOperator\nDESCRIPTION: Lists the template fields available for the CloudVisionAddProductToProductSetOperator for dynamic field resolution at runtime.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"location\",\n    \"project_id\",\n    \"product_set_id\",\n    \"product_id\",\n    \"gcp_conn_id\",\n    \"impersonation_chain\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Product with Google Cloud Vision Operator in Airflow\nDESCRIPTION: Demonstrates how to use CloudVisionCreateProductOperator to create a new product with API-generated ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncreate_product = CloudVisionCreateProductOperator(\n    product=product,\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    retry=Retry(maximum=10.0),\n    timeout=5,\n    task_id=\"create_product\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using CloudDataTransferServiceCreateJobOperator in Python\nDESCRIPTION: Example of using the CloudDataTransferServiceCreateJobOperator to create a transfer job in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncreate_transfer = CloudDataTransferServiceCreateJobOperator(\n    task_id=\"create_transfer\",\n    body=body,\n    aws_conn_id=AWS_CONN_ID,\n    gcp_conn_id=GCP_CONN_ID\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Google Cloud Run Service using CloudRunCreateServiceOperator in Airflow\nDESCRIPTION: Demonstrates how to use the CloudRunCreateServiceOperator to create a Cloud Run service in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_run.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncreate_service = CloudRunCreateServiceOperator(\n    task_id=\"create_service\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    service_id=SERVICE_NAME,\n    service=service\n)\n```\n\n----------------------------------------\n\nTITLE: Submitting Databricks Job with Named Parameters in Python\nDESCRIPTION: Example of using DatabricksSubmitRunOperator with named parameters to submit a Databricks notebook job. This method provides better type checking and error detection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/submit_run.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnew_cluster = {\"spark_version\": \"10.1.x-scala2.12\", \"num_workers\": 2}\nnotebook_task = {\n    \"notebook_path\": \"/Users/airflow@example.com/PrepareData\",\n}\nnotebook_run = DatabricksSubmitRunOperator(\n    task_id=\"notebook_run\", new_cluster=new_cluster, notebook_task=notebook_task\n)\n```\n\n----------------------------------------\n\nTITLE: Migrating BashOperator Import Path\nDESCRIPTION: Migration path for updating BashOperator import statement from core Airflow to standard provider package. This change is enforced by the ruff linting rule AIR303.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/42252.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nairflow.operators.bash.BashOperator  airflow.providers.standard.operators.bash.BashOperator\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Task SDK with pip - Bash\nDESCRIPTION: This snippet provides the bash command required to install the Apache Airflow Task SDK via pip. The system must have Python and pip already installed as prerequisites. The key parameter is the package name 'apache-airflow-task-sdk'; using this command will download and install the package from PyPI into the active Python environment. The expected result is a working installation of the SDK; if there are dependency conflicts or missing permissions, installation may fail.\nSOURCE: https://github.com/apache/airflow/blob/main/task-sdk/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-task-sdk\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into MSSQL Table using Airflow SQLExecuteQueryOperator (Reference)\nDESCRIPTION: References an example `SQLExecuteQueryOperator` task configured to insert data into the 'Users' table in an MSSQL database. It likely uses an inline `INSERT` statement within the `sql` parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/operators.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../tests/system/microsoft/mssql/example_mssql.py\n    :language: python\n    :start-after: [START mssql_operator_howto_guide_populate_user_table]\n    :end-before: [END mssql_operator_howto_guide_populate_user_table]\n```\n\n----------------------------------------\n\nTITLE: Using load_connections_dict for Loading Connections in Airflow\nDESCRIPTION: Example showing how to use the new load_connections_dict function to load connections from a JSON file in Airflow's local filesystem backend. This replaces the deprecated load_connections function.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41533.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconnection_by_conn_id = local_filesystem.load_connections_dict(file_path=\"a.json\")\n```\n\n----------------------------------------\n\nTITLE: Configuring IAM Role Assumption in Airflow Connection Extra Field (JSON)\nDESCRIPTION: This JSON snippet, used in the 'Extra' field of an Airflow AWS connection, configures the connection to assume a specific IAM role. It requires specifying the 'role_arn' of the target role and optionally the 'region_name'. Airflow will use the base credentials (or instance profile) to call AssumeRole.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"role_arn\": \"arn:aws:iam::112223334444:role/my_role\",\n  \"region_name\": \"ap-southeast-2\"\n}\n```\n\n----------------------------------------\n\nTITLE: Upload DV360 Line Items - Python Example\nDESCRIPTION: Demonstrates uploading line items using GoogleDisplayVideo360UploadLineItemsOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/display_video.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n[START howto_google_display_video_upload_line_items_operator]\n[END howto_google_display_video_upload_line_items_operator]\n```\n\n----------------------------------------\n\nTITLE: Create AWS Glue Crawler Example\nDESCRIPTION: Example showing how to create and run an AWS Glue crawler using the GlueCrawlerOperator. Requires proper IAM role configuration with access to data source and AWSGlueServiceRole policy.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/glue.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_glue_crawler]\n[END howto_operator_glue_crawler]\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in reStructuredText\nDESCRIPTION: This snippet includes an external security documentation file using the reStructuredText include directive. It references a file located in the project's development common source directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cloudant/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Installing Apprise Provider with Cross-Provider Dependencies\nDESCRIPTION: Command to install the Apprise provider package along with its cross-provider dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apprise[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Timetable Summary in Python\nDESCRIPTION: Shows how to override the summary property in a custom timetable class to provide a customized schedule display in the Airflow UI.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/timetable.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef summary(self) -> str:\n    return f\"after each workday, at {self._schedule_at}\"\n```\n\n----------------------------------------\n\nTITLE: Implementing DeltaTriggerTimetable with timedelta in Airflow DAG\nDESCRIPTION: Example of using DeltaTriggerTimetable with a timedelta to run a DAG once every week. This timetable accepts a datetime.timedelta or dateutil.relativedelta and runs the DAG once that delta passes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timetable.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\n\nfrom airflow.timetables.trigger import DeltaTriggerTimetable\n\n\n@dag(schedule=DeltaTriggerTimetable(timedelta(days=7)), ...)  # Once every week.\ndef example_dag():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Adding Template Fields Renderers to Airflow\nDESCRIPTION: This commit message details the addition of multiple template_fields_renderers to the Airflow codebase. It references pull request #15130.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_38\n\nLANGUAGE: text\nCODE:\n```\nA bunch of template_fields_renderers additions (#15130)\n```\n\n----------------------------------------\n\nTITLE: Checking SQLite Version in Python\nDESCRIPTION: This Python code snippet demonstrates how to check the version of SQLite being used by the Python interpreter.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-up-database.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import sqlite3\n>>> sqlite3.sqlite_version\n'3.27.2'\n```\n\n----------------------------------------\n\nTITLE: Installing Microsoft PSRP Provider via pip\nDESCRIPTION: This shell command installs the `apache-airflow-providers-microsoft-psrp` package using the Python package installer, pip. This package provides operators and hooks for interacting with systems via the PowerShell Remoting Protocol (PSRP). Installation requires pip and a compatible Apache Airflow version (>=2.9.0).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/psrp/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-microsoft-psrp\n```\n\n----------------------------------------\n\nTITLE: Including External RST Content using Sphinx\nDESCRIPTION: This snippet uses the Sphinx `include` directive within a reStructuredText (RST) file. It instructs the Sphinx build system to insert the content of the specified file (`/../../../../devel-common/src/sphinx_exts/includes/security.rst`) at this location during documentation generation. This is commonly used for reusing content like licenses, warnings, or common sections across multiple documents.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/messaging/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Setting template_searchpath with @dag Decorator in Python\nDESCRIPTION: This snippet demonstrates how to specify a custom directory for finding template files (like Bash scripts) when using the TaskFlow API. The `template_searchpath` argument is passed directly to the `@dag` decorator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@dag(..., template_searchpath=\"/opt/scripts\")\ndef example_bash_dag():\n    @task.bash\n    def bash_example():\n        return \"test.sh \"\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow YDB Provider with common.sql Dependency (Bash)\nDESCRIPTION: This command installs the `apache-airflow-providers-ydb` package along with its optional `common.sql` dependency. This installs the `apache-airflow-providers-common-sql` package, which might be required for specific features within the YDB provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-ydb[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Multiple Asset Event Emission with AssetAlias in Python using Airflow\nDESCRIPTION: This snippet illustrates how to emit multiple asset events using AssetAlias. It demonstrates that duplicate events are not emitted, but events with different extra values will be emitted separately.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk.definitions.asset import AssetAlias\n\n\n@task(\n    outlets=[\n        AssetAlias(\"my-task-outputs-1\"),\n        AssetAlias(\"my-task-outputs-2\"),\n        AssetAlias(\"my-task-outputs-3\"),\n    ]\n)\ndef my_task_with_outlet_events(*, outlet_events):\n    outlet_events[AssetAlias(\"my-task-outputs-1\")].add(Asset(\"s3://bucket/my-task\"), extra={\"k\": \"v\"})\n    # This line won't emit an additional asset event as the asset and extra are the same as the previous line.\n    outlet_events[AssetAlias(\"my-task-outputs-2\")].add(Asset(\"s3://bucket/my-task\"), extra={\"k\": \"v\"})\n    # This line will emit an additional asset event as the extra is different.\n    outlet_events[AssetAlias(\"my-task-outputs-3\")].add(Asset(\"s3://bucket/my-task\"), extra={\"k2\": \"v2\"})\n```\n\n----------------------------------------\n\nTITLE: Listing Files in Object Storage\nDESCRIPTION: Demonstrates how to list files in an object storage location using the iterdir() method and filtering for file objects.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/objectstorage.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef list_files() -> list[ObjectStoragePath]:\n    files = [f for f in base.iterdir() if f.is_file()]\n    return files\n```\n\n----------------------------------------\n\nTITLE: Granting Full Airflow Permissions to a Specific User in Cedar\nDESCRIPTION: This Cedar policy, used within Amazon Verified Permissions, grants all possible actions (`action`) on all Airflow resources (`resource`) to a specific user. The user must be identified by their unique AWS IAM Identity Center user ID (`aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee`). This policy provides unrestricted access to the specified user.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/manage/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: cedar\nCODE:\n```\npermit(\n  principal == Airflow::User::\"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\",\n  action,\n  resource\n);\n```\n\n----------------------------------------\n\nTITLE: Monitoring SageMaker Endpoint State in Python\nDESCRIPTION: This code snippet shows how to use SageMakerEndpointSensor to check the state of an Amazon SageMaker endpoint until it reaches a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_sensor_sagemaker_endpoint]\n# Code snippet not provided in the original text\n# [END howto_sensor_sagemaker_endpoint]\n```\n\n----------------------------------------\n\nTITLE: Applying Advanced Elasticsearch Client Settings via INI\nDESCRIPTION: This configuration demonstrates passing advanced parameters directly to the underlying Python Elasticsearch client via the `[elasticsearch_configs]` section in `airflow.cfg`. Any valid keyword argument for the `elasticsearch.Elasticsearch` constructor can be used, such as `http_compress`, `ca_certs`, `api_key`, and `verify_certs`. This allows for fine-grained control over the Elasticsearch connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/logging/index.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[elasticsearch_configs]\nhttp_compress = True\nca_certs = /root/ca.pem\napi_key = \"SOMEAPIKEY\"\nverify_certs = True\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch TLS Settings via INI\nDESCRIPTION: This snippet shows how to configure TLS settings for reading logs from Elasticsearch within the `[elasticsearch_configs]` section of `airflow.cfg`. The `verify_certs=True` setting enables SSL certificate verification, and `ca_certs` specifies the path to the Certificate Authority (CA) certificate file. These settings apply both when reading logs and when writing logs directly to Elasticsearch is enabled.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/logging/index.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\nremote_logging = True\n\n[elasticsearch_configs]\nverify_certs=True\nca_certs=/path/to/CA_certs\n```\n\n----------------------------------------\n\nTITLE: Creating or Updating a Dataplex Data Profile Scan with Airflow Operator\nDESCRIPTION: Uses the DataplexCreateOrUpdateDataProfileScanOperator to create or update a Dataplex Data Profile scan. This operator manages data profile scans in Google Cloud Dataplex.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ncreate_data_profile = DataplexCreateOrUpdateDataProfileScanOperator(\n    task_id=\"create_data_profile\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    data_scan_id=DATA_PROFILE_SCAN_ID,\n    body=DATA_PROFILE_SCAN,\n)\n```\n\n----------------------------------------\n\nTITLE: Rolling Back a Transaction in Google Cloud Datastore using Python\nDESCRIPTION: This example shows how to use CloudDatastoreRollbackOperator to roll back a transaction in Google Cloud Datastore. It specifies the project ID and transaction ID for the rollback operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nrollback_transaction = CloudDatastoreRollbackOperator(\n    task_id=\"rollback_transaction\",\n    transaction=\"{{ task_instance.xcom_pull('begin_transaction1')[0] }}\",\n    project_id=GCP_PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Connection via CLI with URI\nDESCRIPTION: Command line example for adding an Airflow connection using URI format. Demonstrates connection string structure.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairflow connections add 'my_prod_db' \\\n    --conn-uri '<conn-type>://<login>:<password>@<host>:<port>/<schema>?param1=val1&param2=val2&...'\n```\n\n----------------------------------------\n\nTITLE: Getting a Google Cloud Tasks Queue in Python\nDESCRIPTION: This snippet shows how to get information about a Google Cloud Tasks queue using the CloudTasksQueueGetOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# [START get_queue]\nCloudTasksQueueGetOperator(\n    task_id=\"get_queue\",\n    location=LOCATION,\n    queue_name=QUEUE_ID,\n).execute(context=context)\n# [END get_queue]\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Spark Transport Info Injection via INI\nDESCRIPTION: Configures Airflow's OpenLineage integration to automatically inject its transport configuration (e.g., URL, endpoint) into Spark application properties for supported operators by setting `spark_inject_transport_info` to `true` in the `[openlineage]` section. This helps ensure Spark jobs send lineage data to the same backend as Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_31\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\nspark_inject_transport_info = true\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Spark Provider with Kubernetes Support\nDESCRIPTION: Command to install the Apache Spark provider package with optional CNCF Kubernetes support using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-spark[cncf.kubernetes]\n```\n\n----------------------------------------\n\nTITLE: Preparing Helm Chart Packages\nDESCRIPTION: Commands to prepare and optionally sign Helm chart packages and tarballs.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-helm-chart-tarball --version 1.12.0 --version-suffix rc1\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-helm-chart-package --sign myemail@apache.org\n```\n\n----------------------------------------\n\nTITLE: Checking for DAG Import Errors in Airflow CLI\nDESCRIPTION: This bash command uses the Airflow CLI to list DAG import errors in JSON format and checks if the output is an empty array, indicating no errors.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nairflow dags list-import-errors --output=json | jq -e 'select(type==\"array\" and length == 0)'\n```\n\n----------------------------------------\n\nTITLE: Installing Microsoft PSRP Extras for Apache Airflow\nDESCRIPTION: Command to install PowerShell Remoting Protocol (PSRP) hooks and operators for Apache Airflow. This enables PowerShell remoting integration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_63\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[microsoft-psrp]'\n```\n\n----------------------------------------\n\nTITLE: Including External RST File using Sphinx Directive\nDESCRIPTION: This reStructuredText directive includes the content of the specified external file ('/../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst') into the current document during the Sphinx build process. This allows for modular documentation structure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/trino/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Importing Entities from Cloud Storage to Google Cloud Datastore in Python\nDESCRIPTION: This snippet shows how to use the CloudDatastoreImportEntitiesOperator to import entities from Cloud Storage to Google Cloud Datastore. It specifies the project ID, input URL, and entity filter for the import operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport_task = CloudDatastoreImportEntitiesOperator(\n    task_id=\"import_task\",\n    project_id=GCP_PROJECT_ID,\n    input_url=f\"gs://{BUCKET_NAME}/{EXPORT_BUCKET_FOLDER}outputFile\",\n    entity_filter={},\n)\n```\n\n----------------------------------------\n\nTITLE: Checking out ASF Dev SVN Repository in Shell\nDESCRIPTION: Clones the Apache Airflow development distribution Subversion (SVN) repository. This is used by PMC members to verify the presence and contents of release candidate artifacts.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\nsvn co https://dist.apache.org/repos/dist/dev/airflow\n```\n\n----------------------------------------\n\nTITLE: Using Custom Waiters in Airflow Operators with Python\nDESCRIPTION: Comparison between using an official AWS waiter and a custom waiter in Airflow operators. Shows how to invoke both types of waiters with the appropriate parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/waiters/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nEksHook().conn.get_waiter(\"nodegroup_deleted\").wait(clusterName=cluster_name, nodegroupName=nodegroup_name)\nEksHook().get_waiter(\"all_nodegroups_deleted\").wait(clusterName=cluster_name)\n```\n\n----------------------------------------\n\nTITLE: CloudSQL Instance Patch Operator Initialization Example (Python)\nDESCRIPTION: Demonstrates how to initialize the CloudSQLInstancePatchOperator for updating (patching) instance settings in Airflow. Can use explicit or connection-derived project_id. Requires the patch body and operator import. Inputs are the patch body, instance name, and optional project ID. Produces an operator suitable for Airflow DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"patch_instance = CloudSQLInstancePatchOperator(\\n    task_id=\\\"patch_cloudsql_instance\\\",\\n    project_id=\\\"my-gcp-project\\\",  # pass None to use project from connection\\n    body=patch_body,\\n    instance=\\\"my-instance\\\",\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Terraform Configuration for AWS Web Identity Federation\nDESCRIPTION: Terraform script to configure AWS roles for Google Cloud federation. It creates an IAM role with appropriate assume role policy and attaches permissions for S3 bucket access using web identity federation with Google.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_23\n\nLANGUAGE: terraform\nCODE:\n```\nlocals {\n  google_service_account = \"<NAME>@<PROJECT>.iam.gserviceaccount.com\"\n  google_openid_audience = \"<SERVICE_NAME>.<DOMAIN>\"\n  aws_role_name          = \"WebIdentity-Role\"\n  aws_policy_name        = \"WebIdentity-Role\"\n}\n\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 3.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = \"us-east-1\"\n}\n\ndata \"aws_iam_policy_document\" \"assume_role_policy\" {\n  statement {\n    actions = [\n      \"sts:AssumeRoleWithWebIdentity\"\n    ]\n    effect = \"Allow\"\n\n    condition {\n      test = \"StringEquals\"\n      variable = \"accounts.google.com:aud\"\n      values = [local.google_service_account]\n    }\n\n    condition {\n      test = \"StringEquals\"\n      variable = \"accounts.google.com:oaud\"\n      values = [local.google_openid_audience]\n    }\n\n    principals {\n      identifiers = [\"accounts.google.com\"]\n      type = \"Federated\"\n    }\n  }\n}\n\nresource \"aws_iam_role\" \"role_web_identity\" {\n  name               = local.aws_role_name\n  description        = \"Terraform managed policy\"\n  path               = \"/\"\n  assume_role_policy = data.aws_iam_policy_document.assume_role_policy.json\n}\n# terraform import aws_iam_role.role_web_identity \"WebIdentity-Role\"\n\ndata \"aws_iam_policy_document\" \"web_identity_bucket_policy_document\" {\n  statement {\n    effect = \"Allow\"\n    actions = [\n      \"s3:ListAllMyBuckets\"\n    ]\n    resources = [\"*\"]\n  }\n}\n\nresource \"aws_iam_policy\" \"web_identity_bucket_policy\" {\n  name = local.aws_policy_name\n  path = \"/\"\n  description = \"Terraform managed policy\"\n  policy = data.aws_iam_policy_document.web_identity_bucket_policy_document.json\n}\n# terraform import aws_iam_policy.web_identity_bucket_policy arn:aws:iam::240057002457:policy/WebIdentity-S3-Policy\n\n\nresource \"aws_iam_role_policy_attachment\" \"policy-attach\" {\n  role       = aws_iam_role.role_web_identity.name\n  policy_arn = aws_iam_policy.web_identity_bucket_policy.arn\n}\n# terraform import aws_iam_role_policy_attachment.policy-attach WebIdentity-Role/arn:aws:iam::240057002457:policy/WebIdentity-S3-Policy\n```\n\n----------------------------------------\n\nTITLE: Callable Solution for Data Transformation\nDESCRIPTION: Solution using a callable function to properly handle XCom data retrieval and transformation.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef render_transform_op_kwargs(context, jinja_env):\n    order_data = context[\"ti\"].xcom_pull(\"extract\")\n    return {\"order_data\": order_data}\n\n\ntransform = PythonOperator(\n    task_id=\"transform\",\n    op_kwargs=render_transform_op_kwargs,\n    python_callable=transform,\n)\n```\n\n----------------------------------------\n\nTITLE: Encoding Private Key for Environment Variable\nDESCRIPTION: Command to encode a private SSH key into a one-liner format suitable for use in an environment variable.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/docs/connections/ssh.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -c 'from urllib.parse import quote_plus, sys; print(quote_plus(sys.stdin.read()))' < /path/to/your/key\n```\n\n----------------------------------------\n\nTITLE: Deleting Bigtable Table with BigtableDeleteTableOperator in Python\nDESCRIPTION: This snippet shows how to use the BigtableDeleteTableOperator to delete a table in Google Cloud Bigtable. It demonstrates creating the operator with and without specifying the project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigtable.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndelete_table_task = BigtableDeleteTableOperator(\n    project_id=GCP_PROJECT_ID,\n    instance_id=INSTANCE_ID,\n    table_id=TABLE_ID,\n    task_id=\"delete_table\",\n)\n\n# The same operator can be created without project_id:\ndelete_table_task_no_project_id = BigtableDeleteTableOperator(\n    instance_id=INSTANCE_ID,\n    table_id=TABLE_ID,\n    task_id=\"delete_table\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setup and Teardown with Context Manager\nDESCRIPTION: Demonstrates using teardown as a context manager to wrap multiple tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith delete_cluster().as_teardown(setups=create_cluster()):\n    [RunQueryOne(), RunQueryTwo()] >> DoSomeOtherStuff()\n    WorkOne() >> [do_this_stuff(), do_other_stuff()]\n```\n\n----------------------------------------\n\nTITLE: Running a Google Dataprep Flow in Python\nDESCRIPTION: Example usage of the DataprepRunFlowOperator to run a flow in Google Dataprep. A flow contains imported datasets, recipes, output objects, and references.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataprep.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# [START how_to_dataprep_dataprep_run_flow_operator]\n# Example usage code would be here\n# [END how_to_dataprep_dataprep_run_flow_operator]\n```\n\n----------------------------------------\n\nTITLE: Monitoring SageMaker Training Job State in Python\nDESCRIPTION: This code snippet demonstrates the use of SageMakerTrainingSensor to check the state of an Amazon SageMaker training job until it reaches a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_sensor_sagemaker_training]\n# Code snippet not provided in the original text\n# [END howto_sensor_sagemaker_training]\n```\n\n----------------------------------------\n\nTITLE: Installing Trino Provider with Extra Dependencies using pip (Bash)\nDESCRIPTION: This bash command demonstrates how to install the apache-airflow-providers-trino package along with the common.sql extra dependency using pip. This enables additional functionality that may rely on the common-sql provider. Prerequisites include an existing Airflow 2 installation and access to pip. The command expects that your Python environment supports one of the specified versions (3.9, 3.10, 3.11, or 3.12). The expected output is successful installation of the Trino provider and any specified extras.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/trino/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-trino[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Installing Presto Provider with Extras for Cross-Provider SQL Support - Bash\nDESCRIPTION: This bash snippet extends the core installation by including the \"common.sql\" extra. By running this pip command, users add dependencies from the Airflow \"common-sql\" provider, enabling broader SQL interoperability within Airflow DAGs and Presto-related workflows. This is useful for projects requiring features that rely on cross-provider SQL operators and hooks. All prerequisites are similar to the standard installation, with \"common.sql\" dependencies also being fetched.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/presto/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-presto[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Configuring GCSToBigQueryOperator in Python\nDESCRIPTION: Allows for overriding of 'stringify_dict' for json export format on BaseSQLToGCSOperator, providing more control over JSON formatting.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n\"Allow for the overriding of 'stringify_dict' for json export format on BaseSQLToGCSOperator (#26277)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Google Cloud Dependencies in Airflow\nDESCRIPTION: Command to install the Google Cloud provider package as an extra dependency for Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/secrets-backends/google-cloud-secret-manager-backend.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow[google]\n```\n\n----------------------------------------\n\nTITLE: Deleting AlloyDB User with Airflow Operator\nDESCRIPTION: Uses AlloyDBDeleteUserOperator to delete a user from an AlloyDB instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/alloy_db.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndelete_user = AlloyDBDeleteUserOperator(\n    task_id=\"delete_user\",\n    project_id=GCP_PROJECT_ID,\n    cluster_id=CLUSTER_ID,\n    instance_id=INSTANCE_ID,\n    user_id=USER_ID,\n    region=GCP_ALLOYDB_REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Docker Compose Tests with Breeze\nDESCRIPTION: Commands to build Airflow production image and run Docker Compose tests using Breeze. This performs a complete test of the Docker Compose deployment with Python 3.9.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/docker_compose_tests.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze prod-image build --python 3.9\nbreeze testing docker-compose-tests\n```\n\n----------------------------------------\n\nTITLE: Defining Operator Classnames for OpenLineage Extractor in Python\nDESCRIPTION: This Python snippet shows a classmethod that lists supported Operator names for an OpenLineage Extractor, which will trigger this Extractor when matching Airflow Operators are encountered. The method should return a list of operator class names as strings, such as ['CustomPostgresOperator']. No external dependencies are required other than Airflow and OpenLineage Extractor infrastructure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/developer.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\\ndef get_operator_classnames(cls) -> List[str]:\\n  return ['CustomPostgresOperator']\\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Connection Parameters using Jinja Templates in Airflow\nDESCRIPTION: Demonstrates how to access connection parameters using Jinja templating in Airflow workflows. This snippet shows how to retrieve the host parameter from a connection using the conn_id reference.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/connections.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\necho {{ conn.<conn_id>.host }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Backend Parameters with Custom Prefixes\nDESCRIPTION: Example configuration for setting custom connection and variable prefixes in the secret backend.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/secrets-backends/google-cloud-secret-manager-backend.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend\nbackend_kwargs = {\"connections_prefix\": \"example-connections-prefix\", \"variables_prefix\": \"example-variables-prefix\"}\n```\n\n----------------------------------------\n\nTITLE: Adding PyPI Packages to Airflow Docker Image\nDESCRIPTION: This Dockerfile example shows how to add new PyPI packages (lxml in this case) to the Airflow image. It uses the airflow user to install packages and explicitly installs the matching Airflow version to avoid conflicts.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_1\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apache/airflow:2.7.1\nUSER airflow\nRUN pip install --no-cache-dir apache-airflow==2.7.1 lxml\n```\n\n----------------------------------------\n\nTITLE: Setting Azure Data Lake Connection Environment Variable in Bash\nDESCRIPTION: Example of how to set up the Azure Data Lake connection using an environment variable with URI syntax. The connection string includes client ID, secret, tenant ID, and account name, all properly URL-encoded.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/adl.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AZURE_DATA_LAKE_DEFAULT='azure-data-lake://client%20id:secret@?tenant=tenant+id&account_name=store+name'\n```\n\n----------------------------------------\n\nTITLE: Deep Nested Field Templating in Python\nDESCRIPTION: Shows how to use templating with deeply nested fields in custom classes used as arguments for PythonOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass MyDataTransformer:\n    template_fields: Sequence[str] = (\"reader\",)\n\n    def __init__(self, my_reader):\n        self.reader = my_reader\n\n    # [additional code here...]\n\n\nclass MyDataReader:\n    template_fields: Sequence[str] = (\"path\",)\n\n    def __init__(self, my_path):\n        self.path = my_path\n\n    # [additional code here...]\n\n\nt = PythonOperator(\n    task_id=\"transform_data\",\n    python_callable=transform_data,\n    op_args=[MyDataTransformer(MyDataReader(\"/tmp/{{ ds }}/my_file\"))],\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Preparing Documentation for May 2023 Providers Release\nDESCRIPTION: Excluded Change (Version > 3.1.1): Involves documentation preparation tasks for the May 2023 wave of provider releases, referencing pull request #31252.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Prepare docs for May 2023 wave of Providers (#31252)``\n```\n\n----------------------------------------\n\nTITLE: Disabling OpenLineage for Specific Airflow Operators via INI\nDESCRIPTION: Configures OpenLineage to skip event emission for listed operators (BashOperator, PythonOperator) using the `disabled_for_operators` key in the `[openlineage]` section of Airflow's configuration file. Requires a semicolon-separated string of full operator import paths.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_15\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\ndisabled_for_operators = 'airflow.providers.standard.operators.bash.BashOperator;airflow.providers.standard.operators.python.PythonOperator'\n```\n\n----------------------------------------\n\nTITLE: Registering Custom OpenLineage Extractors in Airflow via INI\nDESCRIPTION: Configures Airflow to use custom OpenLineage extractors by providing a semicolon-separated list of their full import paths to the `extractors` key in the `[openlineage]` section of the Airflow configuration file. This allows extending lineage extraction capabilities for specific operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_19\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\nextractors = full.path.to.ExtractorClass;full.path.to.AnotherExtractorClass\n```\n\n----------------------------------------\n\nTITLE: Defining a Teradata Stored Procedure Returning a Timestamp (SQL)\nDESCRIPTION: This SQL script defines a Teradata stored procedure `GetTimestampOutParameter`. It takes no input parameters and returns a single `TIMESTAMP` output parameter (`out_timestamp`) containing the current timestamp when the procedure is executed. This example demonstrates handling timestamp data types with the `TeradataStoredProcedureOperator` in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/operators/teradata.rst#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nREPLACE PROCEDURE GetTimestampOutParameter (OUT out_timestamp TIMESTAMP)\n   BEGIN\n       -- Assign current timestamp to the OUT parameter\n       SET out_timestamp = CURRENT_TIMESTAMP;\n   END;\n /\n```\n\n----------------------------------------\n\nTITLE: Requirements for apache-airflow-providers-git\nDESCRIPTION: Specifies the minimum required versions for core dependencies needed by the `apache-airflow-providers-git` package. It requires `apache-airflow` version 3.0.0 or higher and `GitPython` version 3.1.44 or higher. This information is presented in a reStructuredText table format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/git/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n``apache-airflow``  ``>=3.0.0``\n``GitPython``       ``>=3.1.44``\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Updating Azure Data Explorer Hook in Python\nDESCRIPTION: Adds managed identity support to the Azure Data Explorer (ADX) hook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfeat(provider/azure): add managed identity support to adx hook (#35325)\n```\n\n----------------------------------------\n\nTITLE: Force Deleting an EKS Cluster and Attached Resources using EksDeleteClusterOperator in Python\nDESCRIPTION: This Python snippet demonstrates using the `EksDeleteClusterOperator` with the `force=True` parameter. This attempts to delete any attached resources (like nodegroups or Fargate profiles) before deleting the EKS cluster itself.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eks.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Example using EksDeleteClusterOperator with force=True\n# Assumes necessary imports and DAG context\n\nforce_delete_cluster = EksDeleteClusterOperator(\n    task_id=\"force_delete_eks_cluster\",\n    cluster_name=\"my-eks-cluster-with-nodegroup\", # Specify the cluster to delete\n    force=True, # Attempt to delete attached resources first\n)\n\n# [END howto_operator_eks_force_delete_cluster]\n```\n\n----------------------------------------\n\nTITLE: Updating an Entry with CloudDataCatalogUpdateEntryOperator in Python\nDESCRIPTION: Uses CloudDataCatalogUpdateEntryOperator to update an existing entry in Google Cloud Data Catalog.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nupdate_entry = CloudDataCatalogUpdateEntryOperator(\n    task_id=\"update_entry\",\n    location=LOCATION,\n    entry_group=ENTRY_GROUP_ID,\n    entry=ENTRY_ID,\n    entry_dict={\"display_name\": \"The updated Cloud Storage entry\"},\n    update_mask={\"paths\": [\"display_name\"]},\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Granting DAG Permissions to User Group\nDESCRIPTION: Policy that grants all DAG-related permissions for 'financial-1' and 'financial-2' DAGs to a specific user group identified by UUID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/manage/index.rst#2025-04-22_snippet_6\n\nLANGUAGE: cedar\nCODE:\n```\npermit(\n  principal in Airflow::Group::\"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\",\n  action,\n  resource in [Airflow::Dag::\"financial-1\", Airflow::Dag::\"financial-2\"]\n);\n```\n\n----------------------------------------\n\nTITLE: Inserting GCE Instance Group Manager Without Project ID in Python\nDESCRIPTION: Creates a ComputeEngineInsertInstanceGroupManagerOperator without specifying a project ID, which will be automatically retrieved from the Google Cloud connection used.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ninsert_igm_no_project_id = ComputeEngineInsertInstanceGroupManagerOperator(\n    task_id=\"insert_igm_no_project_id\",\n    zone=GCE_ZONE,\n    body={\n        \"name\": IGM_NAME,\n        \"baseInstanceName\": \"instances\",\n        \"instanceTemplate\": f\"global/instanceTemplates/{TEMPLATE_NAME}\",\n        \"targetSize\": 1,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Basic WaitHoursSensor Implementation\nDESCRIPTION: Basic implementation of a WaitHoursSensor that uses triggers for task deferral. Shows initialization and execution completion handling.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/deferring.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\n\nfrom airflow.sdk import BaseSensorOperator\nfrom airflow.triggers.base import StartTriggerArgs\n\nif TYPE_CHECKING:\n    from airflow.utils.context import Context\n\n\nclass WaitHoursSensor(BaseSensorOperator):\n    start_trigger_args = StartTriggerArgs(\n        trigger_cls=\"airflow.providers.standard.triggers.temporal.TimeDeltaTrigger\",\n        trigger_kwargs={\"moment\": timedelta(hours=1)},\n        next_method=\"execute_complete\",\n        next_kwargs=None,\n        timeout=None,\n    )\n    start_from_trigger = True\n\n    def __init__(self, *args: list[Any], **kwargs: dict[str, Any]) -> None:\n        super().__init__(*args, **kwargs)\n        self.start_trigger_args.trigger_kwargs = {\"hours\": 2}\n        self.start_from_trigger = True\n\n    def execute_complete(self, context: Context, event: dict[str, Any] | None = None) -> None:\n        # We have no more work to do here. Mark as complete.\n        return\n```\n\n----------------------------------------\n\nTITLE: Deleting S3 Bucket Tags with S3DeleteBucketTaggingOperator\nDESCRIPTION: Demonstrates how to remove all tags from an S3 bucket using the S3DeleteBucketTaggingOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/s3.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndelete_bucket_tagging = S3DeleteBucketTaggingOperator(\n    task_id=\"delete_bucket_tagging\",\n    bucket_name=BUCKET_NAME,\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Getting a Cloud Composer Environment in Python\nDESCRIPTION: This snippet shows how to use the CloudComposerGetEnvironmentOperator to retrieve information about a Cloud Composer environment. It specifies the project ID, region, and environment name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nget_environment_task = CloudComposerGetEnvironmentOperator(\n    task_id=\"get-environment\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    environment_name=ENVIRONMENT_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Azure Service Bus Topic in Python\nDESCRIPTION: This code demonstrates how to use the AzureServiceBusTopicCreateOperator to create an Azure Service Bus topic with specific parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/asb.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_create_service_bus_topic]\n# [END howto_operator_create_service_bus_topic]\n```\n\n----------------------------------------\n\nTITLE: Wait for AWS Glue Data Quality Evaluation Example\nDESCRIPTION: Example demonstrating how to wait for an AWS Glue Data Quality RuleSet Evaluation Run to reach terminal state using GlueDataQualityRuleSetEvaluationRunSensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/glue.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n[START howto_sensor_glue_data_quality_ruleset_evaluation_run]\n[END howto_sensor_glue_data_quality_ruleset_evaluation_run]\n```\n\n----------------------------------------\n\nTITLE: Checking DAG Run Completion in Cloud Composer (Deferrable Mode)\nDESCRIPTION: This snippet demonstrates how to check DAG run completion in a Cloud Composer environment using the CloudComposerDAGRunSensor in deferrable mode. It includes additional parameters for deferrable execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncheck_dag_run_task_deferrable = CloudComposerDAGRunSensor(\n    task_id=\"check-dag-run-deferrable-mode\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    environment_name=ENVIRONMENT_NAME,\n    dag_id=DAG_ID,\n    execution_date=\"2022-01-01T00:00:00+00:00\",\n    deferrable=True,\n    poke_interval=10,\n)\n```\n\n----------------------------------------\n\nTITLE: Using Domain-Wide Delegation with Google Sheets Operator\nDESCRIPTION: Example of using domain-wide delegation with Google Sheets operator in Airflow. Shows how to create a spreadsheet while impersonating a service account using domain-wide delegation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nPROJECT_ID = os.environ.get(\"TF_VAR_project_id\", \"your_project_id\")\n\nSPREADSHEET = {\n    \"properties\": {\"title\": \"Test1\"},\n    \"sheets\": [{\"properties\": {\"title\": \"Sheet1\"}}],\n}\n\nfrom airflow.providers.google.suite.operators.sheets import (\n    GoogleSheetsCreateSpreadsheetOperator,\n)\n\ncreate_spreadsheet_operator = GoogleSheetsCreateSpreadsheetOperator(\n    task_id=\"create-spreadsheet\",\n    gcp_conn_id=\"google_cloud_default\",\n    spreadsheet=SPREADSHEET,\n    impersonation_chain=f\"projects/-/serviceAccounts/SA@{PROJECT_ID}.iam.gserviceaccount.com\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring a Dataplex Asset in Python\nDESCRIPTION: Defines the configuration for a Google Cloud Dataplex asset. This configuration specifies the necessary properties before creating an asset with the DataplexCreateAssetOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Configuration to create a simple Dataplex asset.\nASSET_ID = \"test-bucket\"\nZONE_ID = \"test-zone\"\nLAKE_ID = \"test-lake\"\n\nASSET = {\n    \"resource_spec\": {\n        \"name\": f\"projects/{PROJECT_ID}/buckets/{BUCKET_NAME}\",\n        \"type_\": \"STORAGE_BUCKET\",\n    },\n    \"discovery_spec\": {\"enabled\": True},\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Google Provider with Apache Beam Extra via Pip (Shell)\nDESCRIPTION: This shell command demonstrates how to install the Apache Airflow Google provider along with its optional 'apache.beam' extra using pip. This command ensures that dependencies required for Apache Beam integration (like Dataflow operators) are installed. The surrounding text notes that using this method with pip versions <= 20.2.4 might revert to older dependency resolution behavior, potentially causing issues with BigQuery operator functionality due to conflicting Google client library versions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-google[apache.beam]\n```\n\n----------------------------------------\n\nTITLE: Deleting DV360 Report - Python Example\nDESCRIPTION: Shows how to delete a Display & Video 360 report using GoogleDisplayVideo360DeleteReportOperator. Supports Jinja templating for parameter values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/display_video.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[START howto_google_display_video_delete_query_report_operator]\n[END howto_google_display_video_delete_query_report_operator]\n```\n\n----------------------------------------\n\nTITLE: Checking DAG Run Completion in Cloud Composer\nDESCRIPTION: This snippet shows how to use the CloudComposerDAGRunSensor to check if a DAG run has completed in a Cloud Composer environment. It specifies the project ID, region, environment name, DAG ID, and execution date.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncheck_dag_run_task = CloudComposerDAGRunSensor(\n    task_id=\"check-dag-run\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    environment_name=ENVIRONMENT_NAME,\n    dag_id=DAG_ID,\n    execution_date=\"2022-01-01T00:00:00+00:00\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Pod-Based Pinecone Index using CreatePodIndexOperator in Python\nDESCRIPTION: This snippet demonstrates using the `CreatePodIndexOperator` within an Airflow DAG to create a new pod-based index in Pinecone. It requires parameters for the index configuration like `index_name` and `dimension`, as well as pod configuration details like `environment`, `pod_type`, and `pods`. The Pinecone `api_key` and `environment` can be provided either directly as arguments or through the Airflow connection. The example code is included from a separate file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pinecone/docs/operators/pinecone.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../pinecone/tests/system/pinecone/example_create_pod_index.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_create_pod_index]\n    :end-before: [END howto_operator_create_pod_index]\n```\n\n----------------------------------------\n\nTITLE: Exporting SFTP Connection with Key File - Bash\nDESCRIPTION: This Bash snippet demonstrates how to set an Airflow environment variable to define the default SFTP connection using URI syntax, including a URL-encoded path to the key_file for private key authentication. It should be run in the shell where Airflow is started; dependencies are Airflow installed and access to the specified user's private key. Key parameter: AIRFLOW_CONN_SFTP_DEFAULT, set to an encoded URI that includes user, password, host, port, and the key_file path in extras. The export ensures Airflow processes the SFTP connection with the provided authentication. Limitation: Careful handling and encoding of sensitive information is required.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/connections/sftp.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SFTP_DEFAULT='sftp://user:pass@localhost:22?key_file=%2Fhome%2Fairflow%2F.ssh%2Fid_rsa'\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow from GitHub Archive via Pip (Bash)\nDESCRIPTION: Shows the pip command used to install Airflow directly from a GitHub archive (e.g., a tag or branch) rather than PyPI. It uses the `#egg=apache-airflow` syntax to specify the package name and applies constraints from the 'constraints-main' branch, suitable for development builds.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install \"https://github.com/apache/airflow/archive/<tag>.tar.gz#egg=apache-airflow\" \\\n  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-main/constraints-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Renaming Dataset to Asset in Apache Airflow Codebase\nDESCRIPTION: This code snippet represents a comprehensive list of renaming operations to change 'Dataset' to 'Asset' across various modules, classes, methods, and variables in the Apache Airflow project. It includes changes to API schemas, SDK definitions, models, views, listeners, timetables, serialization, jobs, and providers.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41348.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n* list of renamed objects\n\n  * Rename module ``airflow.api_connexion.schemas.dataset_schema`` as ``airflow.api_connexion.schemas.asset_schema``\n\n    * Rename variable ``create_dataset_event_schema`` as ``create_asset_event_schema``\n    * Rename variable ``dataset_collection_schema`` as ``asset_collection_schema``\n    * Rename variable ``dataset_event_collection_schema`` as ``asset_event_collection_schema``\n    * Rename variable ``dataset_event_schema`` as ``asset_event_schema``\n    * Rename variable ``dataset_schema`` as ``asset_schema``\n    * Rename class ``TaskOutletDatasetReferenceSchema`` as ``TaskOutletAssetReferenceSchema``\n    * Rename class ``DagScheduleDatasetReferenceSchema`` as ``DagScheduleAssetReferenceSchema``\n    * Rename class ``DatasetAliasSchema`` as ``AssetAliasSchema``\n    * Rename class ``DatasetSchema`` as ``AssetSchema``\n    * Rename class ``DatasetCollection`` as ``AssetCollection``\n    * Rename class ``DatasetEventSchema`` as ``AssetEventSchema``\n    * Rename class ``DatasetEventCollection`` as ``AssetEventCollection``\n    * Rename class ``DatasetEventCollectionSchema`` as ``AssetEventCollectionSchema``\n    * Rename class ``CreateDatasetEventSchema`` as ``CreateAssetEventSchema``\n\n  * Move module ``airflow.datasets`` to ``airflow.sdk.definitions.asset``\n\n    * Rename class ``DatasetAlias`` as ``AssetAlias``\n    * Rename class ``DatasetAll`` as ``AssetAll``\n    * Rename class ``DatasetAny`` as ``AssetAny``\n    * Rename function ``expand_alias_to_datasets`` as ``expand_alias_to_assets``\n    * Rename class ``DatasetAliasEvent`` as ``AssetAliasEvent``\n\n      * Rename attribute ``dest_dataset_uri`` as ``dest_asset_uri``\n\n    * Rename class ``BaseDataset`` as ``BaseAsset``\n\n      * Rename method ``iter_datasets`` as ``iter_assets``\n      * Rename method ``iter_dataset_aliases`` as ``iter_asset_aliases``\n\n    * Rename class ``Dataset`` as ``Asset``\n\n      * Rename method ``iter_datasets`` as ``iter_assets``\n      * Rename method ``iter_dataset_aliases`` as ``iter_asset_aliases``\n\n    * Rename class ``_DatasetBooleanCondition`` as ``_AssetBooleanCondition``\n\n      * Rename method ``iter_datasets`` as ``iter_assets``\n      * Rename method ``iter_dataset_aliases`` as ``iter_asset_aliases``\n\n  * Rename module ``airflow.datasets.manager`` as ``airflow.assets.manager``\n\n    * Rename variable ``dataset_manager`` as ``asset_manager``\n    * Rename function ``resolve_dataset_manager`` as ``resolve_asset_manager``\n    * Rename class ``DatasetManager`` as ``AssetManager``\n\n        * Rename method ``register_dataset_change`` as ``register_asset_change``\n        * Rename method ``create_datasets`` as ``create_assets``\n        * Rename method ``register_dataset_change`` as ``notify_asset_created``\n        * Rename method ``notify_dataset_changed`` as ``notify_asset_changed``\n        * Rename method ``notify_dataset_alias_created`` as ``notify_asset_alias_created``\n\n  * Rename module ``airflow.models.dataset`` as ``airflow.models.asset``\n\n      * Rename class ``DatasetDagRunQueue`` as ``AssetDagRunQueue``\n      * Rename class ``DatasetEvent`` as ``AssetEvent``\n      * Rename class ``DatasetModel`` as ``AssetModel``\n      * Rename class ``DatasetAliasModel`` as ``AssetAliasModel``\n      * Rename class ``DagScheduleDatasetReference`` as ``DagScheduleAssetReference``\n      * Rename class ``TaskOutletDatasetReference`` as ``TaskOutletAssetReference``\n      * Rename class ``DagScheduleDatasetAliasReference`` as ``DagScheduleAssetAliasReference``\n\n  * Rename module ``airflow.api_ui.views.datasets`` as ``airflow.api_ui.views.assets``\n\n      * Rename variable ``dataset_router`` as ``asset_rounter``\n\n  * Rename module ``airflow.listeners.spec.dataset`` as ``airflow.listeners.spec.asset``\n\n      * Rename function ``on_dataset_created`` as ``on_asset_created``\n      * Rename function ``on_dataset_changed`` as ``on_asset_changed``\n\n  * Rename module ``airflow.timetables.datasets`` as ``airflow.timetables.assets``\n\n      * Rename class ``DatasetOrTimeSchedule`` as ``AssetOrTimeSchedule``\n\n  * Rename module ``airflow.serialization.pydantic.dataset`` as ``airflow.serialization.pydantic.asset``\n\n      * Rename class ``DagScheduleDatasetReferencePydantic`` as ``DagScheduleAssetReferencePydantic``\n      * Rename class ``TaskOutletDatasetReferencePydantic`` as ``TaskOutletAssetReferencePydantic``\n      * Rename class ``DatasetPydantic`` as ``AssetPydantic``\n      * Rename class ``DatasetEventPydantic`` as ``AssetEventPydantic``\n\n  * Rename module ``airflow.datasets.metadata`` as ``airflow.sdk.definitions.asset.metadata``\n\n  * In module ``airflow.jobs.scheduler_job_runner``\n\n      * and its class ``SchedulerJobRunner``\n\n          * Rename method ``_create_dag_runs_dataset_triggered`` as ``_create_dag_runs_asset_triggered``\n          * Rename method ``_orphan_unreferenced_datasets`` as ``_orphan_unreferenced_datasets``\n\n  * In module ``airflow.api_connexion.security``\n\n      * Rename decorator ``requires_access_dataset`` as ``requires_access_asset``\n\n  * In module ``airflow.api_fastapi.auth.managers.models.resource_details``\n\n      * Rename class ``DatasetDetails`` as ``AssetDetails``\n\n  * In module ``airflow.api_fastapi.auth.managers.base_auth_manager``\n\n      * Rename function ``is_authorized_dataset`` as ``is_authorized_asset``\n\n  * In module ``airflow.timetables.simple``\n\n      * Rename class ``DatasetTriggeredTimetable`` as ``AssetTriggeredTimetable``\n\n  * In module ``airflow.lineage.hook``\n\n      * Rename class ``DatasetLineageInfo`` as ``AssetLineageInfo``\n\n          * Rename attribute ``dataset`` as ``asset``\n\n      * In its class ``HookLineageCollector``\n\n          * Rename method ``create_dataset`` as ``create_asset``\n          * Rename method ``add_input_dataset`` as ``add_input_asset``\n          * Rename method ``add_output_dataset`` as ``add_output_asset``\n          * Rename method ``collected_datasets`` as ``collected_assets``\n\n  * In module ``airflow.models.dag``\n\n      * Rename function ``get_dataset_triggered_next_run_info`` as ``get_asset_triggered_next_run_info``\n\n      * In its class ``DagModel``\n\n          * Rename method ``get_dataset_triggered_next_run_info`` as ``get_asset_triggered_next_run_info``\n\n  * In module ``airflow.models.taskinstance``\n\n      * and its class ``TaskInstance``\n\n          * Rename method ``_register_dataset_changes`` as ``_register_asset_changes``\n\n  * In module ``airflow.providers_manager``\n\n      * and its class ``ProvidersManager``\n\n          * Rename method ``initialize_providers_dataset_uri_resources`` as ``initialize_providers_asset_uri_resources``\n          * Rename attribute ``_discover_dataset_uri_resources`` as ``_discover_asset_uri_resources``\n          * Rename property ``dataset_factories`` as ``asset_factories``\n          * Rename property ``dataset_uri_handlers`` as ``asset_uri_handlers``\n          * Rename property ``dataset_to_openlineage_converters`` as ``asset_to_openlineage_converters``\n\n  * In module ``airflow.security.permissions``\n\n      * Rename constant ``RESOURCE_DATASET`` as ``RESOURCE_ASSET``\n\n  * In module ``airflow.serialization.enums``\n\n      * and its class DagAttributeTypes\n\n          * Rename attribute ``DATASET_EVENT_ACCESSORS`` as ``ASSET_EVENT_ACCESSORS``\n          * Rename attribute ``DATASET_EVENT_ACCESSOR`` as ``ASSET_EVENT_ACCESSOR``\n          * Rename attribute ``DATASET`` as ``ASSET``\n          * Rename attribute ``DATASET_ALIAS`` as ``ASSET_ALIAS``\n          * Rename attribute ``DATASET_ANY`` as ``ASSET_ANY``\n          * Rename attribute ``DATASET_ALL`` as ``ASSET_ALL``\n\n  * In module ``airflow.serialization.pydantic.taskinstance``\n\n      * and its class ``TaskInstancePydantic``\n\n          * Rename method ``_register_dataset_changes`` as ``_register_dataset_changes``\n\n  * In module ``airflow.serialization.serialized_objects``\n\n      * Rename function ``encode_dataset_condition`` as ``encode_asset_condition``\n      * Rename function ``decode_dataset_condition`` as ``decode_asset_condition``\n\n  * In module ``airflow.timetables.base``\n\n      * Rename class ```_NullDataset``` as ```_NullAsset```\n\n          * Rename method ``iter_datasets`` as ``iter_assets``\n          * Rename method ``iter_dataset_aliases`` as ``iter_assets_aliases``\n\n  * In module ``airflow.utils.context``\n\n      * Rename class ``LazyDatasetEventSelectSequence`` as ``LazyAssetEventSelectSequence``\n\n  * In module ``airflow.www.auth``\n\n      * Rename function ``has_access_dataset`` as ``has_access_asset``\n\n  * Rename configuration ``core.dataset_manager_class`` as ``core.asset_manager_class`` and ``core.dataset_manager_kwargs`` as ``core.asset_manager_kwargs``\n  * Rename example dags  ``example_dataset_alias.py``, ``example_dataset_alias_with_no_taskflow.py``, ``example_datasets.py`` as ``example_asset_alias.py``, ``example_asset_alias_with_no_taskflow.py``, ``example_assets.py``\n  * Rename DagDependency name ``dataset-alias``, ``dataset`` as ``asset-alias``, ``asset``\n  * Rename context key ``triggering_dataset_events`` as ``triggering_asset_events``\n  * Rename resource key ``dataset-uris`` as ``asset-uris`` for providers amazon, common.io, mysql, fab, postgres, trino\n\n  * In provider ``airflow.providers.amazon.aws``\n\n      * Rename package ``datasets`` as ``assets``\n\n          * In its module ``s3``\n\n              * Rename method ``create_dataset`` as ``create_asset``\n              * Rename method ``convert_dataset_to_openlineage`` as ``convert_asset_to_openlineage``\n\n    * and its module ``auth_manager.avp.entities``\n\n      * Rename attribute ``AvpEntities.DATASET`` as ``AvpEntities.ASSET``\n\n    * and its module ``auth_manager.auth_manager.aws_auth_manager``\n\n      * Rename function ``is_authorized_dataset`` as ``is_authorized_asset``\n\n  * In provider ``airflow.providers.common.io``\n\n    * Rename package ``datasets``  as ``assets``\n\n      * in its module ``file``\n\n          * Rename method ``create_dataset`` as ``create_asset``\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from BigQuery to MySQL using BigQueryToMySqlOperator\nDESCRIPTION: This snippet demonstrates how to use the BigQueryToMySqlOperator to copy data from a Google Cloud BigQuery table to a MySQL database. It includes configuration for the operator with dataset, table, and connection parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/bigquery_to_mysql.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_bigquery_to_mysql]\nbigquery_to_mysql = BigQueryToMySqlOperator(\n    task_id=\"bigquery_to_mysql\",\n    dataset_table=\"{{var.value.dataset_table}}\",\n    mysql_table=\"example_table\",\n    replace=False,\n)\n# [END howto_operator_bigquery_to_mysql]\n```\n\n----------------------------------------\n\nTITLE: Initializing Airflow with Database Migration and Admin User\nDESCRIPTION: Docker run command that starts Airflow webserver with database migration and admin user creation, useful for quick testing purposes.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it -p 8080:8080 \\\n  --env \"_AIRFLOW_DB_MIGRATE=true\" \\\n  --env \"_AIRFLOW_WWW_USER_CREATE=true\" \\\n  --env \"_AIRFLOW_WWW_USER_PASSWORD=admin\" \\\n    apache/airflow:3.1.0.dev0-python3.9 webserver\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Azure FileShare to Google Cloud Storage using Airflow\nDESCRIPTION: Example of using AzureFileShareToGCSOperator to transfer files from Azure FileShare to Google Cloud Storage. The operator requires a share_name parameter to specify the Azure FileShare share to transfer files from.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/azure_fileshare_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nazure_fileshare_to_gcs_operator = AzureFileShareToGCSOperator(\n    task_id=\"azure_fileshare_to_gcs_operator_task\",\n    share_name=\"my_share\"\n)\n```\n\n----------------------------------------\n\nTITLE: Building a Custom Docker Image with Docker CLI - Shell\nDESCRIPTION: This shell command builds a Docker image from a provided Dockerfile, tagging it with a custom version name. Dependencies include Docker being installed and a Dockerfile present in the build context. The key parameters are the build context '.' (current directory), '-f Dockerfile' to specify the Dockerfile, '--pull' to refresh base image layers, and '--tag my-image:0.0.1' to assign a versioned tag. The output is a locally built Docker image, which can be subsequently tested or deployed. Ensure that your Dockerfile starts with an appropriate FROM line and contains all necessary RUN statements for dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndocker build . -f Dockerfile --pull --tag my-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Checking Authentication Backend Configuration\nDESCRIPTION: Command to check the currently configured authentication backend in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ airflow config get-value api auth_backends\nairflow.providers.fab.auth_manager.api.auth.backend.basic_auth\n```\n\n----------------------------------------\n\nTITLE: Creating a Config Value in Yandex Cloud Lockbox using CLI\nDESCRIPTION: This command creates a secret in Yandex Cloud Lockbox to store an Airflow configuration value. It demonstrates storing a Sentry DSN as a sensitive configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_17\n\nLANGUAGE: console\nCODE:\n```\n$ yc lockbox secret create \\\n    --name airflow/config/sentry_dsn_value \\\n    --payload '[{\"key\": \"value\", \"text_value\": \"https://public@sentry.example.com/1\"}]'\ndone (1s)\nname: airflow/config/sentry_dsn_value\n```\n\n----------------------------------------\n\nTITLE: Creating Google Cloud VertexAI Dataset Operator Usage in Airflow (Python)\nDESCRIPTION: Demonstrates how to use Airflow's CreateDatasetOperator to create a dataset in Google Cloud VertexAI. Requires the airflow.providers.google Python package, and access to a Google Cloud project with VertexAI enabled. You specify project, location, and dataset metadata as parameters. Returns the dataset_id as an XCom output. Input parameters include dataset display_name and optionally dataset_type; creates a dataset and outputs its ID. Limitations: user must set up cloud access and relevant Airflow connections.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith models.DAG(\n    \"example_vertex_ai_dataset\",\n    default_args=default_args,\n    schedule_interval=None,\n    start_date=days_ago(1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    create_dataset = CreateDatasetOperator(\n        task_id=\"create_dataset\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        metadata=dataset,\n        gcp_conn_id=GCP_CONN_ID,\n    )\n```\n\n----------------------------------------\n\nTITLE: Deleting Google Analytics Data Stream with GoogleAnalyticsAdminDeleteDataStreamOperator in Python\nDESCRIPTION: This snippet illustrates the use of GoogleAnalyticsAdminDeleteDataStreamOperator to delete a data stream in Google Analytics. It provides an example of the operator usage and mentions that Jinja templating can be applied to the operator's template fields.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/analytics_admin.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nGoogleAnalyticsAdminDeleteDataStreamOperator(\n    task_id=\"delete_data_stream\",\n    property_id=\"{{ task_instance.xcom_pull('create_property')['name'].split('/')[-1] }}\",\n    data_stream_id=\"{{ task_instance.xcom_pull('create_data_stream')['name'].split('/')[-1] }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Running a Google Campaign Manager Report in Airflow\nDESCRIPTION: Example of using GoogleCampaignManagerRunReportOperator to execute a Campaign Manager report. The operator supports Jinja templating for dynamic value determination and saves results to XCom.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/campaign_manager.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrun_report = GoogleCampaignManagerRunReportOperator(\n    profile_id=PROFILE_ID,\n    report_id=REPORT_ID,\n    synchronous=True,\n    task_id=\"run_report\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Cloud Build Config from Storage\nDESCRIPTION: Example configuration for creating a Cloud Build from Google Cloud Storage source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_build.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nCREATE_BUILD_FROM_STORAGE_BODY = {\n    \"source\": {\"storageSource\": {\"bucket\": \"{{ var.value.gcp_cloud_build_bucket }}\", \"object\": \"files.tgz\"}},\n    \"steps\": [{\"name\": \"ubuntu\", \"args\": [\"echo\", \"Hello Cloud Build!\"]}],\n    \"status\": \"QUEUED\",\n}\n```\n\n----------------------------------------\n\nTITLE: Migrating Cross Downstream Function Usage in Python\nDESCRIPTION: Replace deprecated airflow.utils.helpers.cross_downstream with airflow.sdk.cross_downstream method for DAG task dependencies\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41520.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Old usage (deprecated)\nfrom airflow.utils.helpers import cross_downstream\n\n# New usage\nfrom airflow.sdk import cross_downstream\n```\n\n----------------------------------------\n\nTITLE: Adding a Document to an OpenSearch Index using Apache Airflow\nDESCRIPTION: This example shows how to use the OpenSearchAddDocumentOperator to add a single document to an OpenSearch index. It requires the OpenSearch provider for Airflow and assumes an existing index in the OpenSearch domain.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/operators/opensearch.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_opensearch_add_document]\n# [END howto_operator_opensearch_add_document]\n```\n\n----------------------------------------\n\nTITLE: Updating GCE Instance Group Manager Template in Python\nDESCRIPTION: Creates a ComputeEngineInstanceGroupUpdateManagerTemplateOperator to update the instance template used by an instance group manager.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nupdate_template = ComputeEngineInstanceGroupUpdateManagerTemplateOperator(\n    task_id=\"update_template\",\n    project_id=GCP_PROJECT_ID,\n    resource_id=IGM_NAME,\n    zone=GCE_ZONE,\n    source_template=f\"global/instanceTemplates/{TEMPLATE_NAME}\",\n    destination_template=f\"global/instanceTemplates/{UPDATED_TEMPLATE_NAME}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Cloud SQL Instance in Python\nDESCRIPTION: Example of using CloudSQLDeleteInstanceOperator to delete a Cloud SQL instance in Google Cloud. Can also be used for deleting read and failover replicas.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsql_delete_instance = CloudSQLDeleteInstanceOperator(\n    instance=INSTANCE_NAME,\n    task_id=\"sql_delete_instance\",\n)\n\nsql_delete_instance_with_project = CloudSQLDeleteInstanceOperator(\n    project_id=GCP_PROJECT_ID,\n    instance=INSTANCE_NAME,\n    task_id=\"sql_delete_instance_with_project\",\n)\n```\n\n----------------------------------------\n\nTITLE: Including External Documentation in reStructuredText\nDESCRIPTION: This snippet uses the reStructuredText 'include' directive to incorporate external documentation about installing Apache Airflow provider packages from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Running Airflow CTL Tests\nDESCRIPTION: Command to run all Airflow CTL tests without database initialization\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing airflow-ctl-tests\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Connection via Environment Variable in Apache Airflow\nDESCRIPTION: This bash command shows how to set an AWS connection in Airflow using the AIRFLOW_CONN environment variable. It includes parameters for role ARN, assume role method, and web identity federation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AWS_DEFAULT=\"aws://\\\n?role_arn=arn%3Aaws%3Aiam%3A%3A240057002457%3Arole%2FWebIdentity-Role&\\\nassume_role_method=assume_role_with_web_identity&\\\nassume_role_with_web_identity_federation=google&\\\nassume_role_with_web_identity_federation_audience=aaa.polidea.com\"\n```\n\n----------------------------------------\n\nTITLE: Getting Memcached Instance Details using Airflow Operator\nDESCRIPTION: Example of using CloudMemorystoreMemcachedGetInstanceOperator to retrieve details of a Memcached instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore_memcached.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nget_instance = CloudMemorystoreMemcachedGetInstanceOperator(\n    task_id=\"get-instance\",\n    location=LOCATION,\n    instance=MEMCACHED_INSTANCE_ID,\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Postgres Provider with Amazon Extras using Pip\nDESCRIPTION: This bash command demonstrates how to install the `apache-airflow-providers-postgres` package using pip, specifically including the optional 'amazon' extras. This installation method enables features that require integration with the `apache-airflow-providers-amazon` package. It requires pip to be available in the execution environment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-postgres[amazon]\n```\n\n----------------------------------------\n\nTITLE: Avoiding Asserts in Production Code\nDESCRIPTION: Demonstrates the recommended approach of using explicit conditional checks instead of assertions in production code, with the exception of type checking.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nassert some_predicate()\n```\n\nLANGUAGE: python\nCODE:\n```\nif not some_predicate():\n    handle_the_case()\n```\n\nLANGUAGE: python\nCODE:\n```\nif TYPE_CHECKING:\n    assert isinstance(x, MyClass)\n```\n\n----------------------------------------\n\nTITLE: Fixing soft_fail Handling in Alibaba Provider\nDESCRIPTION: Corrects an issue in the Alibaba provider where the `soft_fail` argument was not properly respected when an exception occurred during task execution, as detailed in issue #34157.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_20\n\nLANGUAGE: text\nCODE:\n```\nfix(providers/alibaba): respect soft_fail argument when exception is raised (#34157)\n```\n\n----------------------------------------\n\nTITLE: Defining Airflow DAG Configuration in Python\nDESCRIPTION: This snippet sets up the DAG configuration including its ID, description, schedule interval, and default arguments.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/.latest-doc-only-change.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_task_group\",\n    schedule=\"0 0 * * *\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    dagrun_timeout=timedelta(minutes=60),\n    tags=[\"example\", \"example2\"],\n    params={\"example_key\": \"example_value\"},\n) as dag:\n    # [rest of the code...]\n```\n\n----------------------------------------\n\nTITLE: Loading CI Image from PR\nDESCRIPTION: Command to load a CI image from a specific Pull Request using Breeze CLI with GitHub token authentication\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/06_managing_docker_images.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image load --from-pr 12345 --python 3.9 --github-token <your_github_token>\n```\n\n----------------------------------------\n\nTITLE: Conditionally Skipping Tests When Running Against Airflow Packages (Python)\nDESCRIPTION: This Python snippet imports a feature-detection constant (RUNNING_TESTS_AGAINST_AIRFLOW_PACKAGES) and uses pytest's skipif marker to conditionally skip a test if Airflow is installed as a package (as opposed to from sources). This prevents test execution in environments where initialization or plugin issues are expected due to packaging differences.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nfrom tests_common import RUNNING_TESTS_AGAINST_AIRFLOW_PACKAGES\n\n\n@pytest.mark.skipif(\n    RUNNING_TESTS_AGAINST_AIRFLOW_PACKAGES, reason=\"Plugin initialization is done early in case of packages\"\n)\ndef test_plugin():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Accessing Google Spreadsheet URL via XCom\nDESCRIPTION: Shows how to retrieve and print the URL of a newly created Google Spreadsheet using XCom values from the previous task.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/suite/sheets.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nspreadsheet_url = f\"https://docs.google.com/spreadsheets/d/{task_create_spreadsheet.output}\"\nprint(f\"Spreadsheet URL: {spreadsheet_url}\")\n```\n\n----------------------------------------\n\nTITLE: Creating an Entry Group with CloudDataCatalogCreateEntryGroupOperator in Python\nDESCRIPTION: Uses CloudDataCatalogCreateEntryGroupOperator to create a new entry group in Google Cloud Data Catalog. The newly created entry group ID is saved to XCom for use by other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncreate_entry_group = CloudDataCatalogCreateEntryGroupOperator(\n    task_id=\"create_entry_group\",\n    location=LOCATION,\n    entry_group_id=ENTRY_GROUP_ID,\n    entry_group={\n        \"display_name\": \"Entry group\",\n        \"description\": \"Entry group description\",\n    },\n    project_id=PROJECT_ID,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ncreate_entry_group_result = create_entry_group.output[\"entry_group_id\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring base_url in airflow.cfg for Reverse Proxy\nDESCRIPTION: Sets the base URL for Airflow when running behind a reverse proxy. This configuration allows Airflow to generate correct URLs when accessed through the proxy.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/run-behind-proxy.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\nbase_url = http://my_host/myorg/airflow\n```\n\n----------------------------------------\n\nTITLE: Retrieving Variable from Secret Backend\nDESCRIPTION: Command to verify and retrieve variable value from the configured secret backend.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/secrets-backends/google-cloud-secret-manager-backend.rst#2025-04-22_snippet_9\n\nLANGUAGE: console\nCODE:\n```\n$ airflow variables get first-variable\nsecret_content\n```\n\n----------------------------------------\n\nTITLE: Example of Creating IRSA with Full S3 Access using eksctl\nDESCRIPTION: This bash command demonstrates creating an IAM role for service account with full S3 access. It uses a managed policy, but it's recommended to create a more restricted custom policy for production use.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\neksctl create iamserviceaccount --cluster=airflow-eks-cluster --name=airflow-sa --namespace=airflow --attach-policy-arn=arn:aws:iam::aws:policy/AmazonS3FullAccess --approve\n```\n\n----------------------------------------\n\nTITLE: Searching AWS DataSync Task by Locations and Executing Using Airflow DataSyncOperator - Python\nDESCRIPTION: This snippet demonstrates using Airflow's DataSyncOperator to find and execute a DataSync task by specifying the source and destination location URIs instead of a TaskArn. It requires prior setup of locations in AWS DataSync and Airflow's AWS integration. Parameters: source_location_uri and destination_location_uri (location URIs); allow_random_task_choice toggles exception behavior if multiple matches. If a unique match is found, the task is executed and monitored.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/datasync.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nfrom airflow.providers.amazon.aws.operators.datasync import DataSyncOperator\n\ndatasync_search_and_execute_task = DataSyncOperator(\n    task_id='datasync_search_and_execute_task',\n    source_location_uri='smb://onprem/server/path',\n    destination_location_uri='s3://my-bucket/path',\n    allow_random_task_choice=False,  # if multiple tasks found, raise Exception\n)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Starting AWS DMS Replication Task using DmsStartTaskOperator in Python\nDESCRIPTION: This snippet shows how to use the DmsStartTaskOperator to begin an existing AWS DMS replication task. It requires the ARN of the replication task to start and optionally the start type (e.g., 'start-replication', 'resume-processing'). Depends on the airflow.providers.amazon.aws.operators.dms module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dms.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Example using DmsStartTaskOperator\n# from airflow.providers.amazon.aws.operators.dms import DmsStartTaskOperator\n\n# start_task = DmsStartTaskOperator(\n#     task_id='start_dms_task',\n#     replication_task_arn='arn:aws:dms:us-east-1:123456789012:task:ABCDEF123GHIJKL',\n#     start_replication_task_type='start-replication',\n#     # ... other parameters\n# )\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Data Factory Connection with Environment Variables in Bash\nDESCRIPTION: Examples of how to set up Azure Data Factory connections using environment variables in URI syntax. Shows both full connection string with factory name and resource group, and a minimal connection with just authentication details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/adf.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AZURE_DATA_FACTORY_DEFAULT='azure-data-factory://applicationid:serviceprincipalpassword@?tenantId=tenant+id&subscriptionId=subscription+id&resource_group_name=group+name&factory_name=factory+name'\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AZURE_DATA_FACTORY_DEFAULT='azure-data-factory://applicationid:serviceprincipalpassword@?tenantId=tenant+id&subscriptionId=subscription+id'\n```\n\n----------------------------------------\n\nTITLE: Installing GitHub Provider Package for Apache Airflow\nDESCRIPTION: Command to install the GitHub provider package on an existing Airflow 2 installation. This package allows integration with GitHub services.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/github/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-github\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting Kubernetes Clusters\nDESCRIPTION: Commands to list and delete Kind clusters manually from within the breeze virtual environment.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nkind get clusters\nkind delete clusters <NAME_OF_THE_CLUSTER>\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Describe Clusters Method for Redshift in Python\nDESCRIPTION: Demonstrates how to implement an asynchronous method to describe Redshift clusters using aiobotocore client when custom or built-in waiters are not suitable.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/triggers/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync with await self.get_async_conn() as client:\n    response = client.describe_clusters(ClusterIdentifier=self.cluster_identifier)\n```\n\n----------------------------------------\n\nTITLE: Installing OpsGenie Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with OpsGenie integration, enabling OpsGenie hooks and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[opsgenie]'\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx as Reverse Proxy for Airflow\nDESCRIPTION: Nginx configuration for proxying requests to Airflow. This setup passes the URL and HTTP headers to the Airflow webserver without any rewrite, ensuring proper functionality behind the proxy.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/run-behind-proxy.rst#2025-04-22_snippet_1\n\nLANGUAGE: nginx\nCODE:\n```\nserver {\n  listen 80;\n  server_name lab.mycompany.com;\n\n  location /myorg/airflow/ {\n      proxy_pass http://localhost:8080;\n      proxy_set_header Host $http_host;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header X-Forwarded-Proto $scheme;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n      proxy_redirect off;\n      proxy_http_version 1.1;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building CI Image Locally using Breeze\nDESCRIPTION: Command to build the CI image locally using Breeze. This is useful when working with a specific PR branch.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/07_running_ci_locally.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image build\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow to Use Custom Logging Configuration\nDESCRIPTION: This configuration snippet shows how to update the airflow.cfg file to use a custom logging configuration class.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/advanced-logging-configuration.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\nlogging_config_class = log_config.LOGGING_CONFIG\n```\n\n----------------------------------------\n\nTITLE: Adding AWS Connection via Airflow CLI (CLI)\nDESCRIPTION: Shows the Airflow CLI command to add an AWS connection suitable for IRSA or instance profile authentication. The connection ID is set to `aws_conn` (matching the Helm chart example). The connection URI `aws://@/?region_name=eu-west-1` specifies the AWS connection type and the region (`eu-west-1`) where the S3 bucket is located. The `@` symbol is used as a placeholder because the URI format requires it, even though no user/password credentials are provided, implying reliance on ambient credentials like IRSA.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/logging/s3-task-handler.rst#2025-04-22_snippet_4\n\nLANGUAGE: cli\nCODE:\n```\nairflow connections add aws_conn --conn-uri aws://@/?region_name=eu-west-1\n```\n\n----------------------------------------\n\nTITLE: Setting Google Cloud SQL Connection via Environment Variable\nDESCRIPTION: Example of configuring a Google Cloud SQL connection using an environment variable with URL-encoded parameters, including authentication credentials and connection details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp_sql.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_GOOGLE_CLOUD_SQL_DEFAULT='gcpcloudsql://user:XXXXXXXXX@1.1.1.1:3306/mydb?database_type=mysql&project_id=example-project&location=europe-west1&instance=testinstance&use_proxy=True&sql_proxy_use_tcp=False'\n```\n\n----------------------------------------\n\nTITLE: Listing Dataproc Metastore Backups using Airflow Operator in Python\nDESCRIPTION: Demonstrates using the `DataprocMetastoreListBackupsOperator` in an Airflow DAG to retrieve a list of backups for a given Dataproc Metastore service. Requires `service_id`, `location`, and `project_id` to identify the target service.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nlist_backups = DataprocMetastoreListBackupsOperator(\n    task_id=\"list_backups\", service_id=SERVICE_ID, location=REGION, project_id=PROJECT_ID\n)\n```\n\n----------------------------------------\n\nTITLE: Update Airflow Configuration Automatically\nDESCRIPTION: Updates the Airflow configuration to be compatible with version 3.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading_to_airflow3.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nairflow config update --fix\n```\n\n----------------------------------------\n\nTITLE: Allocating IDs for Incomplete Keys in Google Cloud Datastore using Python\nDESCRIPTION: This example demonstrates the use of CloudDatastoreAllocateIdsOperator to allocate IDs for incomplete keys in Google Cloud Datastore. It specifies the project ID, namespace, and partial keys for the allocation operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nallocate_ids = CloudDatastoreAllocateIdsOperator(\n    task_id=\"allocate_ids\",\n    project_id=GCP_PROJECT_ID,\n    namespace=None,\n    partial_keys=KEYS,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Databricks Job Using JSON Parameters\nDESCRIPTION: Example showing how to use the DatabricksCreateJobsOperator with JSON payload to create a Databricks job\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/jobs_create.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_job_json = DatabricksCreateJobsOperator(\n    task_id=\"create_json\",\n    json={\n        \"name\": \"{{ task_instance.task_id }}\",\n        \"description\": \"Created via the REST API v2.1\",\n        \"tasks\": [\n            {\n                \"task_key\": \"task1\",\n                \"notebook_task\": {\n                    \"notebook_path\": \"/Shared/MyNotebook\",\n                },\n                \"new_cluster\": DEFAULT_CLUSTER,\n            }\n        ],\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Home Directory\nDESCRIPTION: Defines the root directory for Airflow content, which serves as the default parent directory for assets like DAGs and logs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/cli-and-env-variables-ref.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_HOME\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Amazon S3 to FTP using S3ToFTPOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the S3ToFTPOperator in Apache Airflow to transfer data from an Amazon S3 bucket to an FTP server. It shows the basic setup and configuration of the operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/s3_to_ftp.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../amazon/tests/system/amazon/aws/example_s3_to_ftp.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_transfer_s3_to_ftp]\n    :end-before: [END howto_transfer_s3_to_ftp]\n```\n\n----------------------------------------\n\nTITLE: Creating Airflow Admin User via CLI (Bash)\nDESCRIPTION: Demonstrates using the `airflow users create` command-line interface command to add a new administrative user to the Airflow metadata database. This command requires specifying username, first name, last name, role (Admin), and email.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/webserver-authentication.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# create an admin user\nairflow users create \\\n    --username admin \\\n    --firstname Peter \\\n    --lastname Parker \\\n    --role Admin \\\n    --email spiderman@superhero.org\n```\n\n----------------------------------------\n\nTITLE: Installing Oracle Provider Package via pip\nDESCRIPTION: Command to install the Oracle provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-oracle\n```\n\n----------------------------------------\n\nTITLE: Getting Product Information using CloudVisionGetProductOperator\nDESCRIPTION: Example showing how to retrieve information about a Product using the product_id from XCOM\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nproduct_get_operator = CloudVisionGetProductOperator(\n    location=location,\n    product_id=product_create_operator.output[\"name\"].split(\"/\")[-1],\n    task_id=\"product_get_operator\",\n)\n```\n\n----------------------------------------\n\nTITLE: Excluding Views from CSRF Protection in Python\nDESCRIPTION: This code snippet shows how to use a decorator to exclude specific views from CSRF protection in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/plugins.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.www.app import csrf\n\n@csrf.exempt\ndef my_handler():\n    # ...\n    return \"ok\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating ObjectStoragePath with Connection ID in URI\nDESCRIPTION: Demonstrates how to create an ObjectStoragePath instance by specifying the URI including the connection ID in the username part of the URI to connect to an S3 bucket.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/objectstorage.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import ObjectStoragePath\n\nbase = ObjectStoragePath(\"s3://aws_default@my-bucket/\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Key Vault Backend with Selective Lookup\nDESCRIPTION: This configuration demonstrates how to set up Azure Key Vault backend with selective lookup. It shows how to exclude variables from being looked up in Azure Key Vault by setting the variables_prefix to null.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/secrets-backends/azure-key-vault.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.microsoft.azure.secrets.key_vault.AzureKeyVaultBackend\nbackend_kwargs = {\"connections_prefix\": \"airflow-connections\", \"variables_prefix\": null, \"vault_url\": \"https://example-akv-resource-name.vault.azure.net/\"}\n```\n\n----------------------------------------\n\nTITLE: Storing Google Cloud Connection in AWS Secrets Manager\nDESCRIPTION: Example of storing a Google Cloud connection in AWS Secrets Manager using a key file. The connection details are stored in the 'extra' field as a JSON string.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-secrets-manager.rst#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"extra\": {\"key_path\": \"/opt/airflow/service_account.json\",\n\"scope\": \"https://www.googleapis.com/auth/devstorage.read_only\"}}\n```\n\n----------------------------------------\n\nTITLE: Enabling Remote Logging to Elasticsearch via INI\nDESCRIPTION: This configuration snippet demonstrates the basic settings required in `airflow.cfg` to enable remote logging and specify the Elasticsearch host and port. Setting `remote_logging` to `True` activates the remote logging feature, and the `host` parameter under the `[elasticsearch]` section defines the connection endpoint for the Elasticsearch cluster.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/logging/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\nremote_logging = True\n\n[elasticsearch]\nhost = <host>:<port>\n```\n\n----------------------------------------\n\nTITLE: Testing JDBC Connection in Python\nDESCRIPTION: Python snippet to test JDBC connection using jaydebeapi. It demonstrates how to connect to a database using a JDBC driver, connection URL, and credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndriver_class = \"com.exasol.jdbc.EXADriver\"\ndriver_path = \"/opt/airflow/drivers/exasol/EXASolution_JDBC-7.0.2/exajdbc.jar\"\nconnection_url = \"jdbc:exa:localhost\"\ncredentials = [\"\", \"\"]\n\nconn = jaydebeapi.connect(\n    driver_class,\n    connection_url,\n    credentials,\n    driver_path,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Dataplex Entry Configuration in Python\nDESCRIPTION: This snippet shows a sample Python dictionary configuration for defining a Dataplex Entry. This configuration object is used with the `DataplexCatalogCreateEntryOperator` or `DataplexCatalogUpdateEntryOperator`. The structure follows the `Entry` resource definition.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_52\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 0\n#     :start-after: [START howto_dataplex_entry_configuration]\n#     :end-before: [END howto_dataplex_entry_configuration]\n\n# Example Entry configuration dictionary (structure shown in linked example)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare RC2 docs for May 2023 wave of Providers (#31416)\nDESCRIPTION: This commit message, associated with version 3.2.0, indicates the preparation of documentation for the second release candidate (RC2) of the May 2023 wave of Apache Airflow Providers. Commit hash: 45548b9451, Date: 2023-05-19.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n``Prepare RC2 docs for May 2023 wave of Providers (#31416)``\n```\n\n----------------------------------------\n\nTITLE: Adding Apt Package to Airflow Image (Dockerfile)\nDESCRIPTION: Simple Dockerfile example illustrating how to add an arbitrary system package (vim) using apt-get to the Airflow image. Requires switching to the root user for installation.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_10\n\nLANGUAGE: Dockerfile\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/add-apt-packages/Dockerfile\n    :language: Dockerfile\n    :start-after: [START Dockerfile]\n    :end-before: [END Dockerfile]\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow Plugin Entrypoint in TOML\nDESCRIPTION: This snippet shows how to configure the entrypoint for an Airflow plugin in a pyproject.toml file using the setuptools entrypoint mechanism.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/plugins.rst#2025-04-22_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[project.entry-points.\"airflow.plugins\"]\nmy_plugin = \"my_package.my_plugin:MyAirflowPlugin\"\n```\n\n----------------------------------------\n\nTITLE: Example OpenLineage YAML Configuration with API Key Authentication\nDESCRIPTION: Example content of an OpenLineage YAML configuration file that defines HTTP transport with API key authentication for secure event transmission.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ntransport:\n  type: http\n  url: https://backend:5000\n  endpoint: events/receive\n  auth:\n    type: api_key\n    apiKey: f048521b-dfe8-47cd-9c65-0cb07d57591e\n```\n\n----------------------------------------\n\nTITLE: Adding Packages from requirements.txt (Dockerfile)\nDESCRIPTION: Dockerfile example showing how to install multiple Python packages specified in a `requirements.txt` file using pip. It highlights the importance of installing as the airflow user and optionally pinning the `apache-airflow` package itself to prevent accidental upgrades/downgrades due to conflicting dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_14\n\nLANGUAGE: Dockerfile\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/add-requirement-packages/Dockerfile\n    :language: Dockerfile\n    :start-after: [START Dockerfile]\n    :end-before: [END Dockerfile]\n```\n\n----------------------------------------\n\nTITLE: Caching Dependencies Configuration in Dockerfile\nDESCRIPTION: Configuration for dependency caching in Airflow Docker builds using mount-type cache volumes and DEPENDENCY_CACHE_EPOCH for cache invalidation. The caching system utilizes uv package installer and is optimized for both CI and local development environments.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build-arg-ref.rst#2025-04-22_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nAIRFLOW_SOURCES_TO=/Dockerfile\nAIRFLOW_VERSION_SPECIFICATION=\nAIRFLOW_CONSTRAINTS_LOCATION=\nDOCKER_CONTEXT_FILES=Dockerfile\nINSTALL_DISTRIBUTIONS_FROM_CONTEXT=false\nDEPENDENCY_CACHE_EPOCH=\"0\"\n```\n\n----------------------------------------\n\nTITLE: Creating Dataplex Entry using Airflow Python\nDESCRIPTION: This snippet demonstrates how to create a new Entry within a specific Entry Group in Google Cloud Dataplex Catalog using the `DataplexCatalogCreateEntryOperator` in an Airflow DAG. It utilizes a predefined configuration dictionary and references an external example file for the complete operator usage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_53\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_create_entry]\n#     :end-before: [END howto_operator_dataplex_catalog_create_entry]\n\n# This example uses DataplexCatalogCreateEntryOperator with a configuration dictionary.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Creating YDB Table Using Operator\nDESCRIPTION: Python code showing how to use YDBExecuteQueryOperator to create a table using an external SQL file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/docs/operators/ydb_operator_howto_guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncreate_pet_table = YDBExecuteQueryOperator(\n    task_id=\"create_pet_table\",\n    sql=\"sql/pet_schema.sql\",\n)\n```\n\n----------------------------------------\n\nTITLE: Updating a Google Cloud Workflow using Airflow Operator in Python\nDESCRIPTION: Shows how to use the `WorkflowsUpdateWorkflowOperator` to modify an existing workflow in Google Cloud Workflows. It requires the updated `workflow` object containing the changes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/workflows.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/workflows/example_workflows.py\n      :language: python\n      :dedent: 4\n      :start-after: [START how_to_update_workflow]\n      :end-before: [END how_to_update_workflow]\n```\n\n----------------------------------------\n\nTITLE: Writing API Response to File in Python\nDESCRIPTION: A code snippet that writes API response text to a file. This represents the first part of the data ingestion process in the DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/pipeline.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith open(data_path, \"w\") as file:\n    file.write(response.text)\n```\n\n----------------------------------------\n\nTITLE: Updating Apache Kafka Topic with ManagedKafkaUpdateTopicOperator in Python\nDESCRIPTION: This code shows how to update an Apache Kafka topic using the ManagedKafkaUpdateTopicOperator. It requires the project ID, region, cluster, topic name, and update mask.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nupdate_topic = ManagedKafkaUpdateTopicOperator(\n    task_id=\"update_topic\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n    topic=TOPIC,\n    update_mask={\"paths\": [\"partition_count\"]},\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Deferrable SFTP Sensor\nDESCRIPTION: Shows how to use the SFTP sensor in deferrable mode for improved resource utilization. This mode allows the sensor to yield its resources while waiting for the file to appear.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/sensors/sftp_sensor.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.sftp.sensors.sftp import SFTPSensor\n\nsensor_deferrable = SFTPSensor(\n    task_id=\"sftp_sensor_deferrable\",\n    path=\"/path/to/file/file.txt\",\n    sftp_conn_id=\"sftp_default\",\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Sentry Integration for Apache Airflow\nDESCRIPTION: Command to install the Sentry integration package for Apache Airflow using pip. This is the first step required to enable error tracking with Sentry in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/errors.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[sentry]'\n```\n\n----------------------------------------\n\nTITLE: Executing Hive Commands with SQLExecuteQueryOperator in Python\nDESCRIPTION: Example demonstrating how to create a Hive connection and execute queries using SQLExecuteQueryOperator. The operator connects to HiveServer2 using specified connection parameters and executes the provided SQL query.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\nwith DAG(\n    dag_id='example_hive_dag',\n    schedule_interval=None,\n    start_date=datetime(2021, 1, 1),\n    tags=['example'],\n    catchup=False,\n) as dag:\n\n    hql_query = SQLExecuteQueryOperator(\n        task_id='hql_query',\n        query=\"\"\"DROP TABLE IF EXISTS apahce_hive_table;\n             CREATE TABLE IF NOT EXISTS apahce_hive_table (\n             col1 string );\"\"\",\n        conn_id='hive_conn_id',\n    )\n```\n\n----------------------------------------\n\nTITLE: Skipping Tasks with @task.bash in Airflow\nDESCRIPTION: Shows how to use exit code 99 to skip a task when using the @task.bash decorator in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@task.bash\ndef bash_skip():\n    return \"echo 'hello world' && exit 99\"\n```\n\n----------------------------------------\n\nTITLE: Rebooting EC2 Instances with Airflow AWS Operator - Python\nDESCRIPTION: This code snippet details invoking the EC2RebootInstanceOperator within Airflow to programmatically reboot specific EC2 instances. You must provide the operator with a list of EC2 instance IDs, and have AWS credentials configured along with reboot-instance permissions. The operator executes a soft reboot and returns the affected instance IDs; care should be taken that instance OS/applications are tolerant to reboots in an automated context.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/ec2.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# There is no code in the provided input itself, only includes and code references to other files.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow Connections and Secrets in Helm\nDESCRIPTION: YAML configuration for setting up Airflow connections and sensitive environment variables using secret and extraSecret sections in Helm chart. Shows how to reference external secrets and provide base64 encoded connection strings.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/adding-connections-and-variables.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# override.yaml\n\nsecret:\n  - envName: \"AIRFLOW_CONN_GCP\"\n    secretName: \"my-airflow-connections\"\n    secretKey: \"AIRFLOW_CONN_GCP\"\n  - envName: \"my-env\"\n    secretName: \"my-secret-name\"\n    secretKey: \"my-secret-key\"\n\nextraSecrets:\n  my-airflow-connections:\n    data: |\n      AIRFLOW_CONN_GCP: 'base64_encoded_gcp_conn_string'\n  my-secret-name:\n    stringData: |\n      my-secret-key: my-secret\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Account and S3 Logging in Airflow Helm Chart (YAML)\nDESCRIPTION: Illustrates how to configure the official Apache Airflow Helm chart (`values.yaml`) to use an existing Kubernetes service account (created via IRSA, e.g., `airflow-sa`) for worker and webserver pods. It sets `serviceAccount.create` to `false` and specifies the `serviceAccount.name`. Additionally, it configures remote S3 logging parameters under `config.logging`, including enabling it (`remote_logging: 'True'`), setting the S3 bucket/path (`remote_base_log_folder`), defining the Airflow connection ID (`remote_log_conn_id: 'aws_conn'`), and enabling S3 encryption (`encrypt_s3_logs: 'True'`). Assumes the specified service account exists and has the necessary annotations provided by `eksctl`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/logging/s3-task-handler.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nworkers:\n  serviceAccount:\n    create: false\n    name: airflow-sa\n    # Annotations are automatically added by **Step1** to serviceAccount. So, you dont need to mention the annotations. We have added this for information purpose\n    annotations:\n      eks.amazonaws.com/role-arn: <ENTER_IAM_ROLE_ARN_CREATED_BY_EKSCTL_COMMAND>\n\nwebserver:\n  serviceAccount:\n    create: false\n    name: airflow-sa\n    # Annotations are automatically added by **Step1** to serviceAccount. So, you dont need to mention the annotations. We have added this for information purpose\n    annotations:\n      eks.amazonaws.com/role-arn: <ENTER_IAM_ROLE_ARN_CREATED_BY_EKSCTL_COMMAND>\n\nconfig:\n  logging:\n    remote_logging: 'True'\n    logging_level: 'INFO'\n    remote_base_log_folder: 's3://<ENTER_YOUR_BUCKET_NAME>/<FOLDER_PATH' # Specify the S3 bucket used for logging\n    remote_log_conn_id: 'aws_conn' # Notice that this name is used in Step3 for creating connections through Airflow UI\n    delete_worker_pods: 'False'\n    encrypt_s3_logs: 'True'\n```\n\n----------------------------------------\n\nTITLE: Getting Google Ads Link with GoogleAnalyticsAdminGetGoogleAdsLinkOperator in Python\nDESCRIPTION: This snippet shows how to use the GoogleAnalyticsAdminGetGoogleAdsLinkOperator to retrieve a specific Google Ads link in Google Analytics. It provides an example of the operator usage and mentions that Jinja templating can be applied to the operator's template fields.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/analytics_admin.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nGoogleAnalyticsAdminGetGoogleAdsLinkOperator(\n    task_id=\"get_google_ad_link\",\n    name=\"properties/{{ task_instance.xcom_pull('create_property')['name'].split('/')[-1] }}/googleAdsLinks/{{ task_instance.xcom_pull('list_google_ads_links')[0]['name'].split('/')[-1] }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Purging a Google Cloud Tasks Queue in Python\nDESCRIPTION: This snippet demonstrates how to purge a Google Cloud Tasks queue using the CloudTasksQueuePurgeOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# [START purge_queue]\nCloudTasksQueuePurgeOperator(\n    task_id=\"purge_queue\",\n    location=LOCATION,\n    queue_name=QUEUE_ID,\n).execute(context=context)\n# [END purge_queue]\n```\n\n----------------------------------------\n\nTITLE: Exporting HTTP Connection Environment Variable - Bash\nDESCRIPTION: This snippet demonstrates how to set the AIRFLOW_CONN_HTTP_DEFAULT environment variable in Bash to configure an HTTP connection in Apache Airflow using URI syntax. It should be run in the shell before starting Airflow, with URL components properly URL-encoded. The variable value specifies the HTTP(S) scheme, authentication credentials, server address, port, and connection parameters, such as headers, required for establishing an HTTP connection in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/connections/http.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_HTTP_DEFAULT='http://username:password@servvice.com:80/https?headers=header'\n```\n\n----------------------------------------\n\nTITLE: Setting Default Namespace in @task.kubernetes (Configuration)\nDESCRIPTION: This configuration snippet demonstrates how to explicitly set the `namespace` parameter to `\"default\"` within the `@task.kubernetes` decorator in Airflow. This is necessary to retain the behavior prior to version 9.0.0, overriding the new default where `namespace=None` would use the cluster's default namespace if running `in_cluster`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: Configuration\nCODE:\n```\nnamespace=\"default\"\n```\n\n----------------------------------------\n\nTITLE: Updating a Google Cloud Run Job using CloudRunUpdateJobOperator in Airflow\nDESCRIPTION: Shows how to use the CloudRunUpdateJobOperator to update an existing Cloud Run job in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_run.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nupdate_job = CloudRunUpdateJobOperator(\n    task_id=\"update_job\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    job_id=JOB_NAME,\n    job=job_update\n)\n```\n\n----------------------------------------\n\nTITLE: Disabling EventBridge Rules using DisableRuleOperator\nDESCRIPTION: Demonstrates using EventBridgeDisableRuleOperator to disable an existing rule in Amazon EventBridge. This operator deactivates rules to stop event processing.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eventbridge.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndisable_rule = EventBridgeDisableRuleOperator(\n    task_id=\"disable_rule\",\n    name=\"test-rule\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing MySQL Provider with Amazon Extra Dependencies\nDESCRIPTION: Command to install the MySQL provider package with additional Amazon provider dependencies\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-mysql[amazon]\n```\n\n----------------------------------------\n\nTITLE: RDS Database Status Sensor\nDESCRIPTION: Waits for an Amazon RDS instance or cluster to reach a specific status using RdsDbSensor. Default status is 'available'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwait_for_db = RdsDbSensor(\n    task_id=\"wait_for_db\",\n    db_identifier=DB_INSTANCE_NAME,\n    target_statuses=[\"available\"],\n    db_type=\"instance\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Qdrant Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Qdrant integration, enabling Qdrant Operators and Hooks.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[qdrant]'\n```\n\n----------------------------------------\n\nTITLE: Final Cleanup for Airflow 2020.6.23rc1 Release Preparation\nDESCRIPTION: This commit message signifies the final cleanup steps performed in preparation for the Airflow release candidate version 2020.6.23rc1. It references pull request #9404.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_83\n\nLANGUAGE: text\nCODE:\n```\nFinal cleanup for 2020.6.23rc1 release preparation (#9404)\n```\n\n----------------------------------------\n\nTITLE: Setting JWT Token in Auth Manager Response Cookie\nDESCRIPTION: Example of how an auth manager should set the JWT token in a cookie when redirecting to Airflow UI. The token is stored in a cookie named '_token' with secure setting based on SSL configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/auth-manager/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.api_fastapi.auth.managers.base_auth_manager import COOKIE_NAME_JWT_TOKEN\n\nresponse = RedirectResponse(url=\"/\")\n\nsecure = conf.has_option(\"api\", \"ssl_cert\")\nresponse.set_cookie(COOKIE_NAME_JWT_TOKEN, token, secure=secure)\nreturn response\n```\n\n----------------------------------------\n\nTITLE: Checking Google Dataprep Job Group Completion in Python\nDESCRIPTION: Example usage of the DataprepJobGroupIsFinishedSensor to check if a started job group in Google Dataprep has finished, regardless of success or failure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataprep.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# [START how_to_dataprep_job_group_finished_sensor]\n# Example usage code would be here\n# [END how_to_dataprep_job_group_finished_sensor]\n```\n\n----------------------------------------\n\nTITLE: Querying YDB Table\nDESCRIPTION: Example of using YDBExecuteQueryOperator to fetch all records from a table.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/docs/operators/ydb_operator_howto_guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nget_all_pets = YDBExecuteQueryOperator(\n    task_id=\"get_all_pets\",\n    sql=\"SELECT * FROM pet;\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using CloudDataTransferServiceDeleteJobOperator in Python\nDESCRIPTION: Example of using the CloudDataTransferServiceDeleteJobOperator to delete a transfer job in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndelete_transfer = CloudDataTransferServiceDeleteJobOperator(\n    task_id=\"delete_transfer\",\n    job_name=\"{{ task_instance.xcom_pull('create_transfer', key='name') }}\",\n    project_id=PROJECT_ID,\n    gcp_conn_id=GCP_CONN_ID\n)\n```\n\n----------------------------------------\n\nTITLE: Using GithubTagSensor in Airflow\nDESCRIPTION: Example of using GithubTagSensor to wait for the creation of a specific tag ('v1.0') in a GitHub repository. This sensor polls GitHub until the tag is found or a timeout is reached.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/github/docs/operators/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntag_sensor = GithubTagSensor(\n    task_id=\"tag_sensor\",\n    tag_name=\"v1.0\",\n    repository_name=\"apache/airflow\",\n    github_conn_id=\"github_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-mongo via pip\nDESCRIPTION: This shell command uses the Python package installer `pip` to install the `apache-airflow-providers-mongo` package. This installation enables Apache Airflow to interact with MongoDB. It should be executed in an environment with an existing Airflow 2 installation (>=2.9.0) and a compatible Python version (3.9, 3.10, 3.11, or 3.12).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mongo/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-mongo\n```\n\n----------------------------------------\n\nTITLE: Adding Kernel Name Specification to PapermillOperator\nDESCRIPTION: Feature (Version 2.2.0): Enhances PapermillOperator to allow specifying the kernel name for notebook execution, referencing pull request #20035.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_25\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Add support to specify kernel name in PapermillOperator (#20035)``\n```\n\n----------------------------------------\n\nTITLE: Defining FastAPI Route for DAGs Endpoint in Python\nDESCRIPTION: Example of implementing a FastAPI route for retrieving DAGs with query parameters, including pagination, filtering, and sorting options. The endpoint is defined with proper type annotations and return type specification.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/16_adding_api_endpoints.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dags_router.get(\"/dags\")  # permissions go in the dependencies parameter here\nasync def get_dags(\n    *,\n    limit: int = 100,\n    offset: int = 0,\n    tags: Annotated[list[str] | None, Query()] = None,\n    dag_id_pattern: str | None = None,\n    only_active: bool = True,\n    paused: bool | None = None,\n    order_by: str = \"dag_id\",\n    session: SessionDep,\n) -> DAGCollectionResponse:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Encoding Snowflake Private Key Content Using base64 in Python\nDESCRIPTION: This Python snippet reads a private key file from disk and encodes its contents into base64 format suitable for use in Airflow Snowflake connections. It assumes the 'base64' module is available in the Python environment. The input is the path to the PEM-encoded private key file. The output is a base64 string, which can then be placed in the 'private_key_content' connection parameter. Be sure that the key file is accessible and handle secrets securely.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/connections/snowflake.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport base64\\n\\nwith open(\\\"path/to/private_key.pem\\\", \\\"rb\\\") as key_file:\\n    private_key_content = base64.b64encode(key_file.read()).decode(\\\"utf-8\\\")\\n    print(private_key_content)\n```\n\n----------------------------------------\n\nTITLE: Using KubernetesDeleteJobOperator in Python\nDESCRIPTION: Example of using the KubernetesDeleteJobOperator to delete Jobs on a Kubernetes cluster. It shows how to configure the operator for job deletion.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nKubernetesDeleteJobOperator(\n    task_id=\"delete_k8s_job\",\n    namespace=\"default\",\n    job_name=\"airflow-job-test\",\n)\n```\n\n----------------------------------------\n\nTITLE: Dry Run of Pre-commit Ruff Check in Bash\nDESCRIPTION: This command performs a dry run of the Ruff pre-commit check, showing what would be executed without actually running the commands.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nDRY_RUN=\"true\" pre-commit run --verbose ruff\n```\n\n----------------------------------------\n\nTITLE: AWS IAM Role Policy for Web Identity Federation\nDESCRIPTION: JSON policy document defining an IAM role for web identity federation with Google. It allows the sts:AssumeRoleWithWebIdentity action with conditions to validate the Google service account and audience.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"accounts.google.com\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"accounts.google.com:aud\": \"<NAME>@<PROJECT_ID>.iam.gserviceaccount.com\"\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an AWS Transfer Job in Python\nDESCRIPTION: Example of creating an AWS transfer job using the CloudDataTransferServiceCreateJobOperator in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbody = {\n    \"description\": \"This is an example AWS transfer job\",\n    \"status\": \"ENABLED\",\n    \"projectId\": PROJECT_ID,\n    \"schedule\": {\n        \"scheduleStartDate\": {\n            \"day\": 1,\n            \"month\": 1,\n            \"year\": 2015\n        },\n        \"scheduleEndDate\": {\n            \"day\": 1,\n            \"month\": 1,\n            \"year\": 2030\n        },\n        \"startTimeOfDay\": {\n            \"hours\": 0,\n            \"minutes\": 0,\n            \"seconds\": 0\n        }\n    },\n    \"transferSpec\": {\n        \"awsS3DataSource\": {\n            \"bucketName\": AWS_SOURCE_BUCKET\n        },\n        \"gcsDataSink\": {\n            \"bucketName\": GCP_DESTINATION_BUCKET\n        },\n        \"transferOptions\": {\n            \"overwriteObjectsAlreadyExistingInSink\": True,\n            \"deleteObjectsFromSourceAfterTransfer\": False\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Launching Deferrable Custom Container Training Job in VertexAI using Deferrable Mode (Python)\nDESCRIPTION: Provides sample code for deferrable mode of CreateCustomContainerTrainingJobOperator, suitable for long-running VertexAI jobs. Functionality is akin to the standard job but utilizes async execution to free resources while waiting for completion. Inputs and outputs are similar to the standard version; requires setting 'deferrable=True' when instantiating the operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    create_training_job_deferrable = CreateCustomContainerTrainingJobOperator(\n        task_id=\"train_model_deferrable\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        display_name=DISPLAY_NAME,\n        container_uri=CONTAINER_URI,\n        command=[\"python3\", \"train.py\"],\n        model_serving_container_image_uri=SERVING_IMAGE_URI,\n        dataset_id=DATASET_ID,\n        gcp_conn_id=GCP_CONN_ID,\n        deferrable=True,\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Sentry Transport in Airflow\nDESCRIPTION: Configuration for using a custom Transport class for Sentry in Airflow. This allows customizing how events are sent to Sentry by providing a dotted path to a Transport class implementation.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/errors.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[sentry]\ntransport = path.to.my.sentry.Transport\n```\n\n----------------------------------------\n\nTITLE: Query Parameter Format - Previous Implementation\nDESCRIPTION: Shows the old format for passing list parameters in API requests using comma-separated values in a single parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43102.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nhttp://<URL>:<PORT>/<PATH>?param=item1,item2\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Updated docs for RC3 wave of providers (#27937)\nDESCRIPTION: This commit message, associated with version 3.2.0, details the update of documentation for the third release candidate (RC3) wave of Apache Airflow Providers. Commit hash: 25bdbc8e67, Date: 2022-11-26.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\n``Updated docs for RC3 wave of providers (#27937)``\n```\n\n----------------------------------------\n\nTITLE: Fixing Oracle Provider Bind Variable Handling in RST\nDESCRIPTION: This commit (a8c6451e61, committed on 2022-01-07) corrects an issue in the Oracle Provider related to handling bind variables when no parameters are provided. Refers to issue #20720.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_7\n\nLANGUAGE: rst\nCODE:\n```\nOracle Provider: Fix handling of bindvars with no parameters (#20720)\n```\n\n----------------------------------------\n\nTITLE: Generating Kubernetes Executor CLI Documentation using argparse - reStructuredText\nDESCRIPTION: This code snippet uses the 'argparse' reStructuredText directive to auto-generate documentation for the Kubernetes Executor commands in Airflow, sourcing the parser function ('_get_parser') from the specified Python module. It requires the Airflow 'providers.cncf.kubernetes.executors.kubernetes_executor' module to be available at doc generation time, as well as support for the 'sphinx-argparse' extension (or a custom 'argparse' RST directive) in the Sphinx documentation toolchain. The ':prog: airflow' parameter ensures that the generated CLI docs reflect the correct program entry point. Inputs are extracted from the referenced Python module, and outputs comprise detailed documentation blocks describing CLI arguments and usage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/cli-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. argparse::\n   :module: airflow.providers.cncf.kubernetes.executors.kubernetes_executor\n   :func: _get_parser\n   :prog: airflow\n```\n\n----------------------------------------\n\nTITLE: Setting Docker Compose Port Environment Variable in Airflow (Python)\nDESCRIPTION: This Python snippet shows how to register the environment variable for the Drill host port in Airflow's global constants by calling the _set_var function. This ensures that Docker Compose can substitute the port mapping from a Python-defined variable. Requires Airflow's dev/breeze utility environment, DRILL_HOST_PORT variable, and supporting functions imported. Takes variable name, environment, default, and value as parameters; outputs updated environment config for Docker.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/integration_tests.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n_set_var(_env, \"DRILL_HOST_PORT\", None, DRILL_HOST_PORT)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Iceberg Provider Package for Airflow\nDESCRIPTION: Command to install the Apache Iceberg provider package for Apache Airflow using pip. This package can be installed on top of an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/iceberg/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-iceberg\n```\n\n----------------------------------------\n\nTITLE: Updating Memcached Instance using Airflow Operator\nDESCRIPTION: Example of using CloudMemorystoreMemcachedUpdateInstanceOperator to update a Memcached instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore_memcached.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nupdate_instance = CloudMemorystoreMemcachedUpdateInstanceOperator(\n    task_id=\"update-instance\",\n    location=LOCATION,\n    instance_id=MEMCACHED_INSTANCE_ID,\n    instance=MEMCACHED_INSTANCE,\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Amazon Bedrock Batch Inference Job with Python\nDESCRIPTION: This snippet demonstrates how to use the BedrockBatchInferenceOperator to create a batch inference job in Amazon Bedrock. It requires input formatted in jsonl and uploaded to an Amazon S3 bucket. Deferrable mode is recommended due to potential wait times.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_bedrock_batch_inference]\n# Example code not provided in the original text\n# [END howto_operator_bedrock_batch_inference]\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Cloud Provider Package for Apache Airflow\nDESCRIPTION: Command to install the dbt Cloud provider package on top of an existing Apache Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-dbt-cloud\n```\n\n----------------------------------------\n\nTITLE: Creating or Updating a Dataplex Data Quality Scan - Python\nDESCRIPTION: Utilizes DataplexCreateOrUpdateDataQualityScanOperator to create or update a Data Quality scan resource in Dataplex. Inputs include the scan_config dictionary, project_id, region, and other scan identifiers. Outputs the operation status or resulting resource.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"create_or_update_dq_scan = DataplexCreateOrUpdateDataQualityScanOperator(\\n    task_id=\\\"create_or_update_dq_scan\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    body=data_quality_scan_config,\\n    data_quality_scan_id=DATA_QUALITY_SCAN_ID,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Static Timetable Description\nDESCRIPTION: Demonstrates how to add a static description property to a custom timetable class for UI display.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/timetable.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndescription = \"Schedule: after each workday\"\n```\n\n----------------------------------------\n\nTITLE: Deleting a Google Cloud Tasks Task in Python\nDESCRIPTION: This snippet demonstrates how to delete a task from a Google Cloud Tasks queue using the CloudTasksTaskDeleteOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# [START create_task]\nCloudTasksTaskCreateOperator(\n    task_id=\"create_task\",\n    location=LOCATION,\n    queue_name=QUEUE_ID,\n    task={\n        \"app_engine_http_request\": {\n            \"http_method\": \"POST\",\n            \"relative_uri\": \"/example_task_handler\",\n        }\n    },\n).execute(context=context)\n# [END create_task]\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom OpenLineage Extractor and Operator in Airflow (Python)\nDESCRIPTION: This Python example creates a custom Airflow Operator and a corresponding OpenLineage Extractor. The Extractor implements required interface methods (_execute_extraction, extract_on_complete, extract_on_failure), each returning an OperatorLineage object. Additional job and run facets are added based on the operation's outcome, including BigQuery job info and failure error messages. Dependencies include Airflow, OpenLineage provider modules, and facet models. Inputs are Operator context and task instances; outputs are structured lineage metadata. Ensure correct import paths and Python type checks if required.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/developer.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models.baseoperator import BaseOperator\\nfrom airflow.providers.openlineage.extractors.base import BaseExtractor, OperatorLineage\\nfrom airflow.providers.common.compat.openlineage.facet import (\\n    Dataset,\\n    ExternalQueryRunFacet,\\n    ErrorMessageRunFacet,\\n    SQLJobFacet,\\n)\\n\\n\\nclass ExampleOperator(BaseOperator):\\n    def __init__(self, query, bq_table_reference, s3_path) -> None:\\n        self.bq_table_reference = bq_table_reference\\n        self.s3_path = s3_path\\n        self.s3_file_name = s3_file_name\\n        self._job_id = None\\n\\n    def execute(self, context) -> Any:\\n        self._job_id, self._error_message = run_query(query=self.query)\\n\\n\\nclass ExampleExtractor(BaseExtractor):\\n    @classmethod\\n    def get_operator_classnames(cls):\\n        return [\\\"ExampleOperator\\\"]\\n\\n    def _execute_extraction(self) -> OperatorLineage:\\n        \\\"\\\"\\\"Define what we know before Operator's extract is called.\\\"\\\"\\\"\\n        return OperatorLineage(\\n            inputs=[Dataset(namespace=\\\"bigquery\\\", name=self.operator.bq_table_reference)],\\n            outputs=[Dataset(namespace=self.operator.s3_path, name=self.operator.s3_file_name)],\\n            job_facets={\\n                \\\"sql\\\": SQLJobFacet(\\n                    query=\\\"EXPORT INTO ... OPTIONS(FORMAT=csv, SEP=';' ...) AS SELECT * FROM ... \\\"\\n                )\\n            },\\n        )\\n\\n    def extract_on_complete(self, task_instance) -> OperatorLineage:\\n        \\\"\\\"\\\"Add what we received after Operator's extract call.\\\"\\\"\\\"\\n        lineage_metadata = self.extract()\\n        lineage_metadata.run_facets = {\\n            \\\"parent\\\": ExternalQueryRunFacet(externalQueryId=task_instance.task._job_id, source=\\\"bigquery\\\")\\n        }\\n        return lineage_metadata\\n\\n    def extract_on_failure(self, task_instance) -> OperatorLineage:\\n        \\\"\\\"\\\"Add any failure-specific information.\\\"\\\"\\\"\\n        lineage_metadata = self.extract_on_complete(task_instance)\\n        lineage_metadata.run_facets = {\\n            \\\"error\\\": ErrorMessageRunFacet(\\n                message=task_instance.task._error_message, programmingLanguage=\\\"python\\\"\\n            )\\n        }\\n        return lineage_metadata\\n\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Environment Variables in systemd\nDESCRIPTION: This snippet shows how to configure environment variables for Airflow when using systemd. The configuration file is located at /etc/sysconfig/airflow and can include settings for AIRFLOW_HOME and AIRFLOW_CONFIG.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/run-with-systemd.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Example content for /etc/sysconfig/airflow\nAIRFLOW_HOME=/path/to/airflow/home\nAIRFLOW_CONFIG=/path/to/airflow/config\n```\n\n----------------------------------------\n\nTITLE: Installing Papermill Provider with Cross-Provider Dependencies\nDESCRIPTION: Command to install the Papermill provider package along with its cross-provider dependencies, specifically the common.compat extra.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-papermill[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Kafka Provider with Google Dependencies\nDESCRIPTION: Command to install the Apache Kafka provider package with additional Google provider dependencies via pip\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-kafka[google]\n```\n\n----------------------------------------\n\nTITLE: AWS IAM Role Policy with Additional Audience Condition\nDESCRIPTION: JSON condition block for an IAM role policy that adds an additional audience restriction to prevent misuse of the Google OpenID token by specifying both the service account and an audience value.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Condition\": {\n    \"StringEquals\": {\n      \"accounts.google.com:aud\": \"<NAME>@<PROJECT_ID>.iam.gserviceaccount.com\",\n      \"accounts.google.com:oaud\": \"service-amp.my-company.com\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Discord Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Discord integration, enabling Discord hooks and sensors.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[discord]'\n```\n\n----------------------------------------\n\nTITLE: Executing SQL statements from a file with DatabricksSqlOperator\nDESCRIPTION: Example of using DatabricksSqlOperator to execute SQL statements from a file. This demonstrates reading SQL statements from an external .sql file where each statement must end with a semicolon followed by a new line.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/sql.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndatabricks_multiple_file = DatabricksSqlOperator(\n    task_id=\"databricks_sql_multiple_file\",\n    sql=\"include/queries.sql\",\n    sql_warehouse_name=\"my_warehouse\",\n)\n```\n\n----------------------------------------\n\nTITLE: Testing RC with Downloaded Distributions\nDESCRIPTION: Example of testing Airflow RC with downloaded distributions for Celery and Kubernetes providers\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/testing_packages.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nrm dist/*\npip download apache-airflow==2.9.0rc1 --dest dist --no-deps\npip download apache-airflow-providers-celery==3.6.2rc1 --dest dist --no-deps\npip download apache-airflow-providers-cncf-kubernetes==8.1.0rc1 --dest dist --no-deps\nbreeze start-airflow --mount-sources remove --use-distributions-from-dist --use-airflow-version sdist --executor CeleryExecutor --backend postgres --load-default-connections --load-example-dags\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Batch Executor Configuration via Environment Variable (Bash)\nDESCRIPTION: Example demonstrating how to configure an AWS Batch Executor option (specifically JOB_QUEUE) using the standard Airflow environment variable format `AIRFLOW__<SECTION>__<OPTION>`. This method allows overriding settings specified in the airflow.cfg file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/executors/batch-executor.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW__AWS_BATCH_EXECUTOR__JOB_QUEUE = \"myJobQueue\"\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Docker provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/docker/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-docker\n```\n\n----------------------------------------\n\nTITLE: Deploying Airflow to Kubernetes Cluster\nDESCRIPTION: Command to deploy Airflow to a Kubernetes cluster using the Airflow Helm Chart. This creates a complete Airflow environment with webserver, scheduler, and workers in the cluster.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s deploy-airflow\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow API Authentication Backends - ini\nDESCRIPTION: This snippet sets the 'auth_backend' property in the [api] section of airflow.cfg to enable both session and basic HTTP authentication backends for Airflow's REST API. It is required to ensure the test scripts can authenticate against the Airflow webserver. The configuration must be added to airflow.cfg or set via environment variable for both backends to be enabled.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_9\n\nLANGUAGE: ini\nCODE:\n```\n[api]\nauth_backend = airflow.providers.fab.auth_manager.api.auth.backend.session,airflow.providers.fab.auth_manager.api.auth.backend.basic_auth\n\n```\n\n----------------------------------------\n\nTITLE: Waiting for Amazon Comprehend Document Classifier Completion with Airflow Sensor - Python\nDESCRIPTION: Explains how to use ComprehendCreateDocumentClassifierCompletedSensor in Airflow to wait for a document classifier training job on Amazon Comprehend to finish. Requires the classifier ARN and checks completion status at configurable intervals through AWS API. Ensures DAG coordination for tasks that are dependent on classifier readiness.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/comprehend.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwait_for_document_classifier = ComprehendCreateDocumentClassifierCompletedSensor(\n    task_id=\"wait_for_document_classifier\",\n    document_classifier_arn=create_document_classifier.output,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings with Vertex AI using Airflow Operator in Python\nDESCRIPTION: Illustrates how to use the `TextEmbeddingModelGetEmbeddingsOperator` from `airflow.providers.google.cloud.operators.vertex_ai.generative_model` to generate text embeddings using a Vertex AI generative model. The operator returns the model's response via XCom under the 'model_response' key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_47\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/vertex_ai/example_vertex_ai_generative_model.py\n    :language: python\n    :dedent: 4\n    :start-after: [START how_to_cloud_vertex_ai_text_embedding_model_get_embeddings_operator]\n    :end-before: [END how_to_cloud_vertex_ai_text_embedding_model_get_embeddings_operator]\n```\n\n----------------------------------------\n\nTITLE: Parameterized Test Example\nDESCRIPTION: Example of properly implementing parameterized tests with sorted parameters for xdist compatibility\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize(\"url, expected_dag_run_ids\",\n    [\n        pytest.param(\n            f\"api/v1/dags/TEST_DAG_ID/dagRuns?end_date_gte=\"\n            f\"{urllib.parse.quote((timezone.utcnow() + timedelta(days=1)).isoformat())}\",\n            []\n```\n\n----------------------------------------\n\nTITLE: Package Installation Command for Apache Airflow Git Provider\nDESCRIPTION: Command to install the Git provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/git/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-git\n```\n\n----------------------------------------\n\nTITLE: Listing Previous Builds with Airflow Cloud Build Operator - Python\nDESCRIPTION: Demonstrates how to use CloudBuildListBuildsOperator within Airflow to enumerate previous Google Cloud Build jobs for a project. Requires Airflow's Google provider extras, credentials, and the project ID. Outputs to XCom for flexible task chaining, with typical filters like page size and status supported.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_build.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlist_builds = CloudBuildListBuildsOperator(\n    task_id=\"list_builds\",\n    project_id=PROJECT_ID,\n    page_size=5,\n    filter_=\"status=SUCCESS\",\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced Kerberos Configuration Options in Airflow.cfg\nDESCRIPTION: Additional INI configuration options for fine-tuning Kerberos settings in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/kerberos.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[kerberos]\nccache = /tmp/airflow_krb5_ccache\nprincipal = airflow\nreinit_frequency = 3600\nkinit_path = kinit\nkeytab = airflow.keytab\nforwardable = True\ninclude_ip = True\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-alibaba via pip\nDESCRIPTION: This command installs the Apache Airflow provider package for Alibaba Cloud using the pip package manager. It should be executed in an environment where Apache Airflow version 2.9.0 or higher is already installed. This package enables interaction with Alibaba Cloud services within Airflow DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-alibaba\n```\n\n----------------------------------------\n\nTITLE: Populating Postgres Table with Data (SQL)\nDESCRIPTION: SQL script containing multiple INSERT statements to add sample records into the `pet` table. This script is intended to be part of (or used alongside) `dags/sql/pet_schema.sql`. Assumes the `pet` table structure defined previously.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/operators.rst#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- populate pet table\nINSERT INTO pet VALUES ( 'Max', 'Dog', '2018-07-05', 'Jane');\nINSERT INTO pet VALUES ( 'Susie', 'Cat', '2019-05-01', 'Phil');\nINSERT INTO pet VALUES ( 'Lester', 'Hamster', '2020-06-23', 'Lily');\nINSERT INTO pet VALUES ( 'Quincy', 'Parrot', '2013-08-11', 'Anne');\n```\n\n----------------------------------------\n\nTITLE: Fixing Language Override in PapermillOperator\nDESCRIPTION: Feature (Version 3.0.0): Corrects an issue with overriding the language setting in the PapermillOperator, referencing pull request #24301. This complements the feature added in #23916.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Fix language override in papermill operator (#24301)``\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Blob Storage Connection Using Environment Variable in Bash\nDESCRIPTION: This snippet demonstrates how to set up an Azure Blob Storage connection using an environment variable in Bash. It uses token credentials, specifying the username, password, host, and tenant ID in a URL-encoded format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/wasb.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_WASB_DEFAULT='wasb://blob%20username:blob%20password@myblob.com?tenant_id=tenant+id'\n```\n\n----------------------------------------\n\nTITLE: Storing Connections in YAML Format\nDESCRIPTION: This snippet shows how to store connections in a YAML file. It includes examples of connections defined as URI strings and as YAML objects with connection parameters and extra JSON data.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/secrets-backend/local-filesystem-secrets-backend.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nCONN_A: 'mysql://host_a'\n\nCONN_B:\n  - 'mysql://host_a'\n  - 'mysql://host_b'\n\nCONN_C:\n  conn_type: scheme\n  host: host\n  schema: lschema\n  login: Login\n  password: None\n  port: 1234\n  extra_dejson:\n    a: b\n    nestedblock_dict:\n      x: y\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Variables in Helm\nDESCRIPTION: YAML configuration for defining Airflow variables through environment variables in Helm chart. Demonstrates setting simple key-value pairs for Airflow variables.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/adding-connections-and-variables.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n  - name: \"AIRFLOW_VAR_KEY\"\n    value: \"value_1\"\n  - name: \"AIRFLOW_VAR_ANOTHER_KEY\"\n    value: \"value_2\"\n```\n\n----------------------------------------\n\nTITLE: PgBouncer Basic Configuration\nDESCRIPTION: YAML configuration for enabling PgBouncer connection pooling.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npgbouncer:\n  enabled: true\n```\n\n----------------------------------------\n\nTITLE: Migrating Weekday Import Path\nDESCRIPTION: Example of the required import path change for the weekday utility module. Users need to update their imports from the core airflow.utils location to the new standard provider location.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/47892.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nairflow.utils.weekday  airflow.providers.standard.utils.weekday\n```\n\n----------------------------------------\n\nTITLE: Configuring Celery Worker Log Persistence\nDESCRIPTION: Helm command to configure Airflow with CeleryExecutor and persistent worker logs using a volume claim template with 10Gi storage.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-logs.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airflow apache-airflow/airflow \\\n  --set executor=CeleryExecutor \\\n  --set workers.persistence.size=10Gi\n```\n\n----------------------------------------\n\nTITLE: Deleting RDS Event Subscription\nDESCRIPTION: Deletes an Amazon RDS event subscription using RDSDeleteEventSubscriptionOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndelete_event_subscription = RDSDeleteEventSubscriptionOperator(\n    task_id=\"delete_event_subscription\",\n    subscription_name=EVENT_SUBSCRIPTION_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Add docs for RC2 wave of providers for Jan 2024\nDESCRIPTION: This text is a commit message summary for adding documentation related to the Release Candidate 2 wave of Apache Airflow provider releases for the second round in January 2024, linked to pull request #37019.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_22\n\nLANGUAGE: plaintext\nCODE:\n```\nAdd docs for RC2 wave of providers for 2nd round of Jan 2024 (#37019)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Entrypoint in Docker for Apache Airflow\nDESCRIPTION: Docker configuration that copies a custom entrypoint script and sets it as the container entrypoint with dumb-init for proper signal handling.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_5\n\nLANGUAGE: Dockerfile\nCODE:\n```\nCOPY my_entrypoint.sh /\nENTRYPOINT [\"/usr/bin/dumb-init\", \"--\", \"/my_entrypoint.sh\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Cloud Proxy Settings in JSON\nDESCRIPTION: JSON configuration for specifying HTTP and HTTPS proxy settings when connecting to dbt Cloud through a proxy server.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/connections.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"proxies\": {\n    \"http\": \"http://myproxy.mycompany.local:8080\",\n    \"https\": \"http://myproxy.mycompany.local:8080\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Alibaba Cloud Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Alibaba Cloud integration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[alibaba]'\n```\n\n----------------------------------------\n\nTITLE: Example DAG for Embedding in Airflow Image (Python)\nDESCRIPTION: A simple example Python script defining an Airflow DAG (`test_dag.py`). This file is intended to be copied into the Airflow image using the corresponding Dockerfile example.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/embedding-dags/test_dag.py\n    :language: Python\n    :start-after: [START dag]\n    :end-before: [END dag]\n```\n\n----------------------------------------\n\nTITLE: Adding Run Job Parameters to AWS Glue Job Operator in Python\nDESCRIPTION: Inclusion of run_job_kwargs parameter in the AWS Glue Job Operator, allowing for more flexible job configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nAwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796)\n```\n\n----------------------------------------\n\nTITLE: Clearing Tasks via Airflow CLI\nDESCRIPTION: Command line interface command for clearing task instances within a specified date range and matching a task regex pattern.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dag-run.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairflow tasks clear dag_id \\\\\\n    --task-regex task_regex \\\\\\n    --start-date START_DATE \\\\\\n    --end-date END_DATE\n```\n\n----------------------------------------\n\nTITLE: CloudSQL Clone Operator Initialization Example (Python)\nDESCRIPTION: Shows how to initialize the CloudSQLCloneInstanceOperator for cloning a Cloud SQL instance, optionally specifying the project ID. The clone_context dictionary must match Google Cloud SQL API requirements. Inputs are the clone_context, instance names, and optional project ID/value. The result is an operator for Airflow DAG inclusion.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n\"clone_instance = CloudSQLCloneInstanceOperator(\\n    task_id=\\\"clone_cloudsql_instance\\\",\\n    project_id=\\\"my-gcp-project\\\",  # pass None to use project from connection\\n    instance=\\\"my-instance\\\",\\n    destination_clone_name=\\\"my-instance-clone\\\",\\n    body=clone_context\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Dynamic Task Mapping WaitHoursSensor\nDESCRIPTION: Enhanced WaitHoursSensor implementation with support for dynamic task mapping through trigger arguments.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/deferring.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\n\nfrom airflow.sdk import BaseSensorOperator\nfrom airflow.triggers.base import StartTriggerArgs\n\nif TYPE_CHECKING:\n    from airflow.utils.context import Context\n\n\nclass WaitHoursSensor(BaseSensorOperator):\n    start_trigger_args = StartTriggerArgs(\n        trigger_cls=\"airflow.providers.standard.triggers.temporal.TimeDeltaTrigger\",\n        trigger_kwargs={\"moment\": timedelta(hours=1)},\n        next_method=\"execute_complete\",\n        next_kwargs=None,\n        timeout=None,\n    )\n    start_from_trigger = True\n\n    def __init__(\n        self,\n        *args: list[Any],\n        trigger_kwargs: dict[str, Any] | None,\n        start_from_trigger: bool,\n        **kwargs: dict[str, Any],\n    ) -> None:\n        # This whole method will be skipped during dynamic task mapping.\n\n        super().__init__(*args, **kwargs)\n        self.start_trigger_args.trigger_kwargs = trigger_kwargs\n        self.start_from_trigger = start_from_trigger\n\n    def execute_complete(self, context: Context, event: dict[str, Any] | None = None) -> None:\n        # We have no more work to do here. Mark as complete.\n        return\n```\n\n----------------------------------------\n\nTITLE: Creating ProductSet using CloudVisionCreateProductSetOperator\nDESCRIPTION: Example showing how to create a new ProductSet using the ProductSet object with auto-generated ID\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud import vision\nfrom google.api_core import retry\n\nPRODUCT_SET = vision.ProductSet(\n    display_name=\"My product set\"\n)\n\nproduct_set_create = CloudVisionCreateProductSetOperator(\n    location=location,\n    product_set=PRODUCT_SET,\n    retry=retry.Retry(),\n    task_id=\"product_set_create\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring StatsD in Airflow Configuration File\nDESCRIPTION: Basic configuration settings for enabling StatsD metrics in Airflow. These settings need to be added to the airflow.cfg file to enable metrics collection via StatsD.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/metrics.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[metrics]\nstatsd_on = True\nstatsd_host = localhost\nstatsd_port = 8125\nstatsd_prefix = airflow\n```\n\n----------------------------------------\n\nTITLE: Checking for DAG Import Errors with Output in Airflow CLI\nDESCRIPTION: This bash command uses the Airflow CLI to list DAG import errors, saves the output to a file, and checks if the output is an empty array, indicating no errors.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nairflow dags list-import-errors | tee import_errors.txt && jq -e 'select(type==\"array\" and length == 0)' import_errors.txt\n```\n\n----------------------------------------\n\nTITLE: Implementing Start-from-Trigger Sensor in Python\nDESCRIPTION: Example of a sensor that starts directly from the triggerer without going through a worker, using start_trigger_args configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/deferring.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\n\nfrom airflow.sdk import BaseSensorOperator\nfrom airflow.triggers.base import StartTriggerArgs\n\nif TYPE_CHECKING:\n    from airflow.utils.context import Context\n\n\nclass WaitOneHourSensor(BaseSensorOperator):\n    start_trigger_args = StartTriggerArgs(\n        trigger_cls=\"airflow.providers.standard.triggers.temporal.TimeDeltaTrigger\",\n        trigger_kwargs={\"moment\": timedelta(hours=1)},\n        next_method=\"execute_complete\",\n        next_kwargs=None,\n        timeout=None,\n    )\n    start_from_trigger = True\n\n    def execute_complete(self, context: Context, event: dict[str, Any] | None = None) -> None:\n        # We have no more work to do here. Mark as complete.\n        return\n```\n\n----------------------------------------\n\nTITLE: Handling Complex JSON in Airflow Connection Extras\nDESCRIPTION: Example demonstrating how complex JSON structures are handled in connection URIs using the __extra__ parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> extra_dict = {\"my_val\": [\"list\", \"of\", \"values\"], \"extra\": {\"nested\": {\"json\": \"val\"}}}\n>>> c = Connection(\n...     conn_type=\"scheme\",\n...     host=\"host/location\",\n...     schema=\"schema\",\n...     login=\"user\",\n...     password=\"password\",\n...     port=1234,\n...     extra=json.dumps(extra_dict),\n... )\n>>> uri = c.get_uri()\n>>> uri\n'scheme://user:password@host%2Flocation:1234/schema?__extra__=%7B%22my_val%22%3A+%5B%22list%22%2C+%22of%22%2C+%22values%22%5D%2C+%22extra%22%3A+%7B%22nested%22%3A+%7B%22json%22%3A+%22val%22%7D%7D%7D'\n\n>>> new_c = Connection(uri=uri)\n>>> new_c.extra_dejson == extra_dict\nTrue\n```\n\n----------------------------------------\n\nTITLE: Listing Dataplex Aspect Types using Airflow Python\nDESCRIPTION: This snippet shows how to list all Aspect Types within a specific location in Google Cloud Dataplex Catalog using the `DataplexCatalogListAspectTypesOperator` in an Airflow DAG. Filtering and ordering are supported. It references an external example file for the implementation details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_49\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_list_aspect_types]\n#     :end-before: [END howto_operator_dataplex_catalog_list_aspect_types]\n\n# This example uses DataplexCatalogListAspectTypesOperator.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub OAuth Authentication in Airflow (Python)\nDESCRIPTION: Example configuration within `webserver_config.py` for enabling GitHub OAuth authentication. It sets `AUTH_TYPE` to `AUTH_OAUTH`, enables role synchronization on login (`AUTH_ROLES_SYNC_AT_LOGIN`), allows new user registration (`AUTH_USER_REGISTRATION`), defines a mapping between GitHub teams/attributes and Airflow roles (`AUTH_ROLES_MAPPING`), configures the GitHub OAuth provider details including client ID/secret (fetched from environment variables), API endpoints, and required scopes. It also specifies a custom security manager class (`CustomSecurityManager`) for handling authorization logic. Requires `airflow.providers.fab`, Flask-AppBuilder, and `os` module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/webserver-authentication.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.fab.auth_manager.security_manager.override import FabAirflowSecurityManagerOverride\nfrom flask_appbuilder.security.manager import AUTH_OAUTH\nimport os\n\nAUTH_TYPE = AUTH_OAUTH\nAUTH_ROLES_SYNC_AT_LOGIN = True  # Checks roles on every login\nAUTH_USER_REGISTRATION = True  # allow users who are not already in the FAB DB to register\n\nAUTH_ROLES_MAPPING = {\n    \"Viewer\": [\"Viewer\"],\n    \"Admin\": [\"Admin\"],\n}\n# If you wish, you can add multiple OAuth providers.\nOAUTH_PROVIDERS = [\n    {\n        \"name\": \"github\",\n        \"icon\": \"fa-github\",\n        \"token_key\": \"access_token\",\n        \"remote_app\": {\n            \"client_id\": os.getenv(\"OAUTH_APP_ID\"),\n            \"client_secret\": os.getenv(\"OAUTH_APP_SECRET\"),\n            \"api_base_url\": \"https://api.github.com\",\n            \"client_kwargs\": {\"scope\": \"read:user, read:org\"},\n            \"access_token_url\": \"https://github.com/login/oauth/access_token\",\n            \"authorize_url\": \"https://github.com/login/oauth/authorize\",\n            \"request_token_url\": None,\n        },\n    },\n]\n\n\nclass CustomSecurityManager(FabAirflowSecurityManagerOverride):\n    pass\n\n\n# Make sure to replace this with your own implementation of AirflowSecurityManager class\nSECURITY_MANAGER_CLASS = CustomSecurityManager\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkKubernetesOperator Job Template in YAML\nDESCRIPTION: A sample YAML template file (`spark_job_template.yaml`) used to configure a Spark application run via Airflow's `SparkKubernetesOperator`. It defines both Spark-specific settings (like version, mode, driver/executor resources) under the `spark` key and Kubernetes resource configurations (like environment variables, volumes, affinity, tolerations) under the `kubernetes` key. This template serves as a base configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n.. code-block:: yaml\n\n    spark:\n      apiVersion: sparkoperator.k8s.io/v1beta2\n      version: v1beta2\n      kind: SparkApplication\n      apiGroup: sparkoperator.k8s.io\n      metadata:\n        namespace: ds\n      spec:\n        type: Python\n        pythonVersion: \"3\"\n        mode: cluster\n        sparkVersion: 3.0.0\n        successfulRunHistoryLimit: 1\n        restartPolicy:\n          type: Never\n        imagePullPolicy: Always\n        hadoopConf: {}\n        imagePullSecrets: []\n        dynamicAllocation:\n          enabled: false\n          initialExecutors: 1\n          minExecutors: 1\n          maxExecutors: 1\n        labels: {}\n        driver:\n          serviceAccount: default\n          container_resources:\n            gpu:\n              name: null\n              quantity: 0\n            cpu:\n              request: null\n              limit: null\n            memory:\n              request: null\n              limit: null\n        executor:\n          instances: 1\n          container_resources:\n            gpu:\n              name: null\n              quantity: 0\n            cpu:\n              request: null\n              limit: null\n            memory:\n              request: null\n              limit: null\n    kubernetes:\n      # example:\n      # env_vars:\n      # - name: TEST_NAME\n      #   value: TEST_VALUE\n      env_vars: []\n\n      # example:\n      # env_from:\n      # - name: test\n      #   valueFrom:\n      #     secretKeyRef:\n      #       name: mongo-secret\n      #       key: mongo-password\n      env_from: []\n\n      # example:\n      # node_selector:\n      #   karpenter.sh/provisioner-name: spark\n      node_selector: {}\n\n      # example: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/\n      # affinity:\n      #   nodeAffinity:\n      #     requiredDuringSchedulingIgnoredDuringExecution:\n      #       nodeSelectorTerms:\n      #       - matchExpressions:\n      #         - key: beta.kubernetes.io/instance-type\n      #           operator: In\n      #           values:\n      #           - r5.xlarge\n      affinity:\n        nodeAffinity: {}\n        podAffinity: {}\n        podAntiAffinity: {}\n\n      # example: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n      # type: list\n      # tolerations:\n      # - key: \"key1\"\n      #   operator: \"Equal\"\n      #   value: \"value1\"\n      #   effect: \"NoSchedule\"\n      tolerations: []\n\n      # example:\n      # config_map_mounts:\n      #   snowflake-default: /mnt/tmp\n      config_map_mounts: {}\n\n      # example:\n      # volume_mounts:\n      # - name: config\n      #   mountPath: /airflow\n      volume_mounts: []\n\n      # https://kubernetes.io/docs/concepts/storage/volumes/\n      # example:\n      # volumes:\n      # - name: config\n      #   persistentVolumeClaim:\n      #     claimName: airflow\n      volumes: []\n\n      # read config map into an env variable\n      # example:\n      # from_env_config_map:\n      # - configmap_1\n      # - configmap_2\n      from_env_config_map: []\n\n      # load secret into an env variable\n      # example:\n      # from_env_secret:\n      # - secret_1\n      # - secret_2\n      from_env_secret: []\n\n      in_cluster: true\n      conn_id: kubernetes_default\n      kube_config_file: null\n      cluster_context: null\n```\n\n----------------------------------------\n\nTITLE: Configuring Redshift IAM Authentication (Cluster) via Extra JSON\nDESCRIPTION: This JSON configuration, placed in the 'Extra' field of an Airflow Redshift connection, enables IAM authentication for a standard Redshift cluster. It requires specifying `iam` as true, the `cluster_identifier`, `port`, `region`, `db_user`, and `database`. An optional AWS `profile` can be specified; otherwise, the default credentials/role associated with the Airflow environment are used. This method leverages AWS IAM to generate temporary database credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/redshift.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"iam\": true,\n  \"cluster_identifier\": \"redshift-cluster-1\",\n  \"port\": 5439,\n  \"region\": \"us-east-1\",\n  \"db_user\": \"awsuser\",\n  \"database\": \"dev\",\n  \"profile\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing the Zendesk Provider Package using pip\nDESCRIPTION: This shell command installs the `apache-airflow-providers-zendesk` package using pip, the Python package installer. This adds Zendesk integration capabilities to an existing Apache Airflow installation (version 2.9.0 or newer is required).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/zendesk/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-zendesk\n```\n\n----------------------------------------\n\nTITLE: Defining User Permissions - Flask AppBuilder (FAB) - Python\nDESCRIPTION: This snippet details the incremental permissions granted to the User role on top of the Viewer role in Airflow's RBAC using FAB. It illustrates which new resource-action pairs become available to Users, notably some edit and create/delete privileges on core Airflow objects and access to certain menu actions. Dependencies: same as Viewer, with required configurations for extendable permissions. Focuses on higher-level access as roles inherit lower permissions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/access-control.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nuser_perms = viewer_perms + [\n    (\"DAGs\", \"can_edit\"),\n    (\"DAG Runs\", \"can_create\"),\n    (\"DAG Runs\", \"can_delete\"),\n    (\"Task Instances\", \"can_edit\"),\n    (\"Task Instances\", \"can_clear\"),\n    (\"XCom Entries\", \"can_edit\"),\n    (\"Connections\", \"can_create\"),\n    (\"Connections\", \"can_edit\"),\n    (\"Connections\", \"can_delete\"),\n    (\"Event Logs\", \"can_create\"),\n    (\"Pools\", \"can_edit\"),\n    (\"Pools\", \"can_create\"),\n    (\"Pools\", \"can_delete\"),\n    (\"Variables\", \"can_edit\"),\n    (\"Variables\", \"can_create\"),\n    (\"Variables\", \"can_delete\"),\n    (\"SLA Misses\", \"can_edit\"),\n    (\"Dag Warnings\", \"can_edit\"),\n    (\"Trigger Requests\", \"can_create\"),\n    (\"Provider\", \"can_edit\"),\n    (\"Provider\", \"can_create\"),\n    (\"Provider\", \"can_delete\"),\n]\n\n```\n\n----------------------------------------\n\nTITLE: Getting Dataproc Batch Details with Airflow Operator in Python\nDESCRIPTION: This snippet demonstrates using the `DataprocGetBatchOperator` to retrieve the details and status of a specific Dataproc Batch job. It requires `batch_id`, `region`, and `project_id` to identify the target batch.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n# Code extracted from: /../../google/tests/system/google/cloud/dataproc/example_dataproc_batch.py\n# Between markers: [START how_to_cloud_dataproc_get_batch_operator] and [END how_to_cloud_dataproc_get_batch_operator]\n# \n# Example using DataprocGetBatchOperator(...)\n# ... (actual Python code would be here)\n\n```\n\n----------------------------------------\n\nTITLE: Using DataflowJobAutoScalingEventsSensor in Deferrable Mode\nDESCRIPTION: Example of using the DataflowJobAutoScalingEventsSensor in deferrable mode. This allows the sensor to release its worker slot while waiting for the specified autoscaling events.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# Code snippet not provided in the input text\n```\n\n----------------------------------------\n\nTITLE: Retrieving Video Label Detection Results from XCom in Airflow (Python)\nDESCRIPTION: This snippet retrieves the annotation results produced by the CloudVideoIntelligenceDetectVideoLabelsOperator via Airflow's XCom mechanism. It uses airflow.models.XCom.get_value to fetch and print results from the 'detect_video_labels' task. The input is the task_id and dag context, while the expected output is the annotated labels from the video. Dependencies include a running Airflow environment with accessible XCom records.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/video_intelligence.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndetect_video_labels_result = XCom.get_value(\n    key=\"return_value\",\n    task_id=\"detect_video_labels\",\n    dag_id=dag.dag_id,\n    execution_date=kwargs['execution_date'],\n)\nprint(f\"Label detection result: {detect_video_labels_result}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Hive Provider with Amazon Dependencies\nDESCRIPTION: Command to install the Apache Hive provider package with Amazon provider dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-hive[amazon]\n```\n\n----------------------------------------\n\nTITLE: Using Command Execution for Configuration Values\nDESCRIPTION: For specific configuration options, commands can be executed to provide values. The command result is used as the value for the corresponding configuration setting.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/cli-and-env-variables-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW__{SECTION}__{KEY}_CMD\n```\n\n----------------------------------------\n\nTITLE: Downloading a Google Campaign Manager Report in Airflow\nDESCRIPTION: Example of using GoogleCampaignManagerDownloadReportOperator to download a Campaign Manager report to Google Cloud Storage bucket. Supports Jinja templating for dynamic values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/campaign_manager.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nget_report = GoogleCampaignManagerDownloadReportOperator(\n    profile_id=PROFILE_ID,\n    report_id=FILE_ID,\n    file_id=FILE_ID,\n    bucket_name=BUCKET,\n    report_name=REPORT_NAME,\n    task_id=\"get_report\",\n)\n```\n\n----------------------------------------\n\nTITLE: Resource-Specific DAG Access Control in Python\nDESCRIPTION: Shows how to configure access control for specific resources like DAGs and DAG Runs. This allows for more granular permission settings per resource type.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/access-control.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nDAG(\n    dag_id=\"example_fine_grained_access\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    access_control={\n        \"Viewer\": {\"DAGs\": {\"can_edit\", \"can_read\", \"can_delete\"}, \"DAG Runs\": {\"can_create\"}},\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow MSSQL Provider with Common SQL Extras\nDESCRIPTION: Demonstrates how to install the `apache-airflow-providers-microsoft-mssql` package along with optional cross-provider dependencies using pip. This specific command includes the `common.sql` extra, which installs the `apache-airflow-providers-common-sql` package required for certain features. This allows users to leverage functionalities shared across different SQL-based providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-microsoft-mssql[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Using Asynchronous GCSObjectsWithPrefixExistenceSensor in Python\nDESCRIPTION: Shows how to use the GCSObjectsWithPrefixExistenceSensor in asynchronous mode for more efficient resource utilization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsensor_prefix_async_task = GCSObjectsWithPrefixExistenceSensor(\n    task_id=\"sensor_prefix_async_task\",\n    bucket=BUCKET_NAME,\n    prefix=PREFIX,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Drill Provider Package for Airflow\nDESCRIPTION: Command to install the Apache Drill provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-drill\n```\n\n----------------------------------------\n\nTITLE: Fixing type annotation\nDESCRIPTION: Fixes a type annotation issue.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"Fix type annotation (#31888)\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Yandex Lockbox Backend with OAuth Token in Airflow (INI)\nDESCRIPTION: Shows how to configure the `backend_kwargs` in `airflow.cfg` to authenticate with Yandex Lockbox using a user account's OAuth token. Both `folder_id` and the `yc_oauth_token` must be specified.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend_kwargs = {\"folder_id\": \"b1g66mft1vo1n4vbn57j\", \"yc_oauth_token\": \"y3_Vd3eub7w9bIut67GHeL345gfb5GAnd3dZnf08FR1vjeUFve7Yi8hGvc\"}\n```\n\n----------------------------------------\n\nTITLE: Detect Image Labels - Google Cloud Vision Airflow Operator - Python\nDESCRIPTION: This code block shows how to configure the CloudVisionDetectImageLabelsOperator in Airflow for automated label detection in images using Google Cloud Vision. It requires setting up the GCP project and location, specifying the input image, and optionally a retry policy. The expected output is a set of detected labels applicable for image classification or search workflows.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndetect_labels = CloudVisionDetectImageLabelsOperator(\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n    image=GCP_VISION_IMAGE,\n    retry=Retry(),\n    task_id=\"detect_labels\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow CLI for Remote API Requests\nDESCRIPTION: This snippet shows how to configure the Airflow CLI to send requests to a remote API instead of querying a local database. It specifies the API client and the endpoint URL to use.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/api-auth-backend/google-openid.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[cli]\napi_client = airflow.api.client.json_client\nendpoint_url = http://remote-host.example.org/\n```\n\n----------------------------------------\n\nTITLE: Creating Named Asset in Python\nDESCRIPTION: Shows how to create an Asset with both URI and human-readable name.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import Asset\n\nexample_asset = Asset(uri=\"s3://asset-bucket/example.csv\", name=\"bucket-1\")\n```\n\n----------------------------------------\n\nTITLE: Executing Backfill Command in Airflow CLI\nDESCRIPTION: Command line interface command for performing backfill operations on a DAG for a specified date range.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dag-run.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairflow backfill create --dag-id DAG_ID \\\\\\n    --start-date START_DATE \\\\\\n    --end-date END_DATE \\\\\n```\n\n----------------------------------------\n\nTITLE: Enabling Yandex Lockbox Backend in Airflow Configuration (INI)\nDESCRIPTION: Configures the `[secrets]` section in the `airflow.cfg` file to specify `LockboxSecretBackend` as the secrets backend, enabling integration with Yandex Lockbox.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.yandex.secrets.lockbox.LockboxSecretBackend\n```\n\n----------------------------------------\n\nTITLE: Entering the Breeze Interactive Shell - Bash\nDESCRIPTION: This command launches an interactive Breeze shell (containerized airflow development environment) for running Pytest or Airflow commands. No arguments are required. Output is an interactive terminal environment with all dependencies pre-installed. Assumes Docker, Breeze, and Airflow prerequisites are met.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nbreeze\n```\n\n----------------------------------------\n\nTITLE: Transferring Parquet Data from S3 to Teradata using S3ToTeradataOperator in Python\nDESCRIPTION: This Python code example illustrates using the `S3ToTeradataOperator` within an Airflow DAG to transfer data stored in Parquet format from AWS S3 directly into a Teradata table, utilizing the READ_NOS feature.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/operators/s3_to_teradata.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../teradata/tests/system/teradata/example_s3_to_teradata_transfer.py\n    :language: python\n    :start-after: [START s3_to_teradata_transfer_operator_howto_guide_transfer_data_s3_to_teradata_parquet]\n    :end-before: [END s3_to_teradata_transfer_operator_howto_guide_transfer_data_s3_to_teradata_parquet]\n```\n\n----------------------------------------\n\nTITLE: Defining Instance Patch Body for CloudSQL Partial Update (Python)\nDESCRIPTION: Provides an example of the body needed for a partial update to a Cloud SQL instance using CloudSQLInstancePatchOperator. Requires a schema-matching dictionary with updated settings. Input is a Python dictionary for fields to patch. It's used as the 'body' argument for the patch operator in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n\"patch_body = {\\n    \\\"settings\\\": {\\n        \\\"activationPolicy\\\": \\\"ALWAYS\\\"\\n    }\\n}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Yandex Lockbox with Connection ID and Folder ID Override (INI)\nDESCRIPTION: Configures `backend_kwargs` in `airflow.cfg` to use a specific Yandex Cloud connection (`yc_connection_id`) while also explicitly defining the `folder_id` where secrets should be searched. This overrides any folder ID specified within the connection itself.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_9\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend_kwargs = {\"folder_id\": \"b1g66mft1vo1n4vbn57j\", \"yc_connection_id\": \"my_yc_connection\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Logging in Airflow\nDESCRIPTION: INI configuration for enabling remote logging in Airflow with the option to delete local log files after uploading to a remote location.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/logging-tasks.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\nremote_logging = True\nremote_base_log_folder = schema://path/to/remote/log\ndelete_local_logs = True\n```\n\n----------------------------------------\n\nTITLE: Directory Paths for Ownership Fix in Linux Containers\nDESCRIPTION: List of directory paths where file ownership needs to be fixed when exiting the container. These paths represent common locations where files might be created during container execution.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0006-using-root-user-and-fixing-ownership-for-ci-container.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"/files\"\n\"/root/.aws\"\n\"/root/.azure\"\n\"/root/.config/gcloud\"\n\"/root/.docker\"\n\"/opt/airflow/logs\"\n\"/opt/airflow/docs\"\n\"/opt/airflow/dags\"\n\"${AIRFLOW_SOURCES}\"\n```\n\n----------------------------------------\n\nTITLE: Importing TaskDecorator in Python for Custom Decorator\nDESCRIPTION: Demonstrates the import of TaskDecorator class, which is used as the base for creating custom decorators in Airflow for TaskFlow-based DAG authoring.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/public-airflow-interface.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.decorators.base import TaskDecorator\n```\n\n----------------------------------------\n\nTITLE: Diagnosing Dataproc Cluster with Python\nDESCRIPTION: This snippet demonstrates how to diagnose a Dataproc cluster using the DataprocDiagnoseClusterOperator. It shows both synchronous and asynchronous (deferrable) modes of operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndiagnose_cluster = DataprocDiagnoseClusterOperator(\n    task_id=\"diagnose_cluster\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster_name=CLUSTER_NAME,\n    storage_uri=DIAGNOSTIC_LOCATION,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ndiagnose_cluster_async = DataprocDiagnoseClusterOperator(\n    task_id=\"diagnose_cluster\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster_name=CLUSTER_NAME,\n    storage_uri=DIAGNOSTIC_LOCATION,\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Celery Integration for Apache Airflow\nDESCRIPTION: Command to install Celery integration package that enables CeleryExecutor and CeleryKubernetesExecutor with dependencies and sensor\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_47\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[celery]'\n```\n\n----------------------------------------\n\nTITLE: Preparing Airflow Backport Release Candidate 2020.6.23rc1\nDESCRIPTION: This commit message details the preparation steps taken specifically for the backport release candidate version 2020.6.23rc1. It references pull request #9370.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_84\n\nLANGUAGE: text\nCODE:\n```\nPrepare backport release candidate for 2020.6.23rc1 (#9370)\n```\n\n----------------------------------------\n\nTITLE: Refactoring Provider Import Time Consolidation\nDESCRIPTION: Optimizes provider loading by consolidating imports, potentially reducing import times and improving performance, tracked in issue #34402.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nRefactor: consolidate import time in providers (#34402)\n```\n\n----------------------------------------\n\nTITLE: Implementing SHA-pinned GitHub Action Reference\nDESCRIPTION: Example showing how to reference a GitHub Action using its full-length commit SHA for security purposes, including a comment with the version number for documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0013-get-rid-of-submodules.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nuses: aws-actions/configure-aws-credentials@010d0da01d0b5a38af31e9c3470dbfdabdecca3a  # v4.0.1\n```\n\n----------------------------------------\n\nTITLE: Checking Kubernetes Cluster Status with Breeze\nDESCRIPTION: This command checks and displays the status of the current KinD cluster managed by Breeze. It provides information about the cluster configuration, control plane, running pods, storage classes, and checks the availability of the Airflow webserver.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s status\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow Authentication with Keycloak in Python\nDESCRIPTION: This snippet sets up the authentication configuration for Apache Airflow using Keycloak as the OIDC provider. It defines role mappings, OAuth provider settings, and fetches the public key for token verification.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/webserver-authentication.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nAUTH_USER_REGISTRATION_ROLE = \"Viewer\"\nOIDC_ISSUER = \"https://sso.keycloak.me/realms/airflow\"\n\n# Make sure you create these role on Keycloak\nAUTH_ROLES_MAPPING = {\n    \"Viewer\": [\"Viewer\"],\n    \"Admin\": [\"Admin\"],\n    \"User\": [\"User\"],\n    \"Public\": [\"Public\"],\n    \"Op\": [\"Op\"],\n}\n\nOAUTH_PROVIDERS = [\n    {\n        \"name\": \"keycloak\",\n        \"icon\": \"fa-key\",\n        \"token_key\": \"access_token\",\n        \"remote_app\": {\n            \"client_id\": \"airflow\",\n            \"client_secret\": \"xxx\",\n            \"server_metadata_url\": \"https://sso.keycloak.me/realms/airflow/.well-known/openid-configuration\",\n            \"api_base_url\": \"https://sso.keycloak.me/realms/airflow/protocol/openid-connect\",\n            \"client_kwargs\": {\"scope\": \"email profile\"},\n            \"access_token_url\": \"https://sso.keycloak.me/realms/airflow/protocol/openid-connect/token\",\n            \"authorize_url\": \"https://sso.keycloak.me/realms/airflow/protocol/openid-connect/auth\",\n            \"request_token_url\": None,\n        },\n    }\n]\n\n# Fetch public key\nreq = requests.get(OIDC_ISSUER)\nkey_der_base64 = req.json()[\"public_key\"]\nkey_der = b64decode(key_der_base64.encode())\npublic_key = serialization.load_der_public_key(key_der)\n```\n\n----------------------------------------\n\nTITLE: Creating ACL Entry for GCS Object in Python\nDESCRIPTION: Demonstrates the usage of GCSObjectCreateAclEntryOperator to create a new ACL entry on a specified object in Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncreate_object_acl_entry = GCSObjectCreateAclEntryOperator(\n    task_id=\"create_object_acl_entry\",\n    bucket=BUCKET_NAME,\n    object_name=FILE_NAME,\n    entity=\"user-{{ USER_EMAIL }}\",\n    role=\"OWNER\",\n)\n```\n\n----------------------------------------\n\nTITLE: Removing File from Dataform Workspace in Python\nDESCRIPTION: This snippet demonstrates how to remove a file from a workspace using the DataformRemoveFileOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_remove_file]\n# [END howto_operator_remove_file]\n```\n\n----------------------------------------\n\nTITLE: Installing Redis Extras for Apache Airflow\nDESCRIPTION: Command to install Redis hooks and sensors for Apache Airflow. This provides functionality for interacting with Redis services.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_49\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[redis]'\n```\n\n----------------------------------------\n\nTITLE: Configuring a Hadoop Job for Google Cloud Dataproc\nDESCRIPTION: This code defines a configuration for a Hadoop job to be submitted to a Dataproc cluster. It specifies the main jar file, class, and arguments for the job.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nHADOOP_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"hadoop_job\": {\n        \"main_jar_file_uri\": f\"file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\",\n        \"args\": [\"wordcount\", f\"gs://{BUCKET_NAME}/{HADOOP_INPUT}\", f\"gs://{BUCKET_NAME}/{HADOOP_OUTPUT_DIR}\"],\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Branching with Time Range Across Days - Airflow Python\nDESCRIPTION: This snippet demonstrates usage of the BranchDateTimeOperator where target_upper is set to a time value before target_lower, causing the considered time range to span across midnight to the next day. This enables flexible scheduling for operations that must handle overnight periods. The snippet assumes Airflow's core packages. Inputs are times for the range, and the operator will automatically adjust the upper bound into the next day if needed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/datetime.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbranch_overnight = BranchDateTimeOperator(\n    task_id='branch_overnight',\n    follow_task_ids_if_true=['overnight_process'],\n    follow_task_ids_if_false=['day_process'],\n    target_lower=time(22, 0),\n    target_upper=time(6, 0),\n    use_task_logical_date=False,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Setting Azure Data Explorer Connection Using Environment Variable in Bash\nDESCRIPTION: Example of how to set the Azure Data Explorer connection using an environment variable with URI syntax in Bash. This example shows how to configure an AAD_APP authentication method with username, password, and tenant ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/adx.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AZURE_DATA_EXPLORER_DEFAULT='azure-data-explorer://add%20username:add%20password@mycluster.com?auth_method=AAD_APP&tenant=tenant+id'\n```\n\n----------------------------------------\n\nTITLE: Setting SQL Dialect Name in Airflow Connection Extras (YAML)\nDESCRIPTION: This YAML code snippet demonstrates how to specify the SQL dialect name (e.g., 'mssql') within the 'extra' configuration of an Apache Airflow database connection. By setting the 'dialect_name' key in the connection's extra field, Airflow will use the specified dialect implementation when connecting to a database. The key parameter is 'dialect_name', and the input is a string representing the desired dialect, such as 'mssql'. The output is the correct routing of database operations to the appropriate dialect handler. This feature allows users to override default auto-detection of database dialects; if an invalid or unavailable dialect is specified, Airflow will fallback to the default implementation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/dialects.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndialect_name: 'mssql'\n```\n\n----------------------------------------\n\nTITLE: Installing PGVector Provider with Common SQL Dependencies\nDESCRIPTION: This command installs the PGVector provider package along with its common SQL dependencies. This is useful for accessing all features of the package that rely on common SQL functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-pgvector[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Building and Publishing Documentation\nDESCRIPTION: Series of commands to build, preview, and publish Airflow documentation including SBOM generation.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\ncd \"${AIRFLOW_REPO_ROOT}\"\nbreeze build-docs apache-airflow docker-stack --clean-build\n```\n\n----------------------------------------\n\nTITLE: Defining SFTP Connection Extras With Key File - JSON\nDESCRIPTION: This snippet illustrates configuring the \"extras\" field for an Airflow SFTP connection to use a private key file for authentication. It shows setting the key_file parameter (path to SSH private key) and disabling host key checking for simplified access. The context assumes an SFTP connection managed within Airflow's UI or configuration file. Inputs: JSON object with key_file and no_host_key_check; outputs: extras used during SFTP authentication. Limitations include security consideration when setting no_host_key_check to true.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/connections/sftp.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"key_file\": \"path/to/private_key\",\n   \"no_host_key_check\": \"true\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Cloudant Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Cloudant integration, enabling the Cloudant hook.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[cloudant]'\n```\n\n----------------------------------------\n\nTITLE: Retrieving Text Classification Results in Apache Airflow\nDESCRIPTION: This snippet shows how to retrieve and process the results of content classification using XCom in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/natural_language.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncategories = classify_text.output[\"categories\"]\nfor category in categories:\n    print(f\"{category.name}: {category.confidence}\")\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Enable enforcing pydocstyle rule D213 in ruff\nDESCRIPTION: This text is a commit message summary describing the enablement of pydocstyle rule D213 (multi-line docstring summary should start at the second line) enforcement using the Ruff linter, linked to pull request #40448.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nEnable enforcing pydocstyle rule D213 in ruff. (#40448)\n```\n\n----------------------------------------\n\nTITLE: Configuring Direct Log Writing to Elasticsearch via INI\nDESCRIPTION: This configuration snippet details how to set up Airflow to write task logs directly to an Elasticsearch index. Key settings include `remote_logging = True` to enable, `delete_local_logs` (optional) to remove local log copies after successful upload, `host` for the Elasticsearch endpoint, `write_stdout = False` to disable stdout logging, `json_format = True` for the log format (required for direct writing), `write_to_es = True` to enable direct writing, and `target_index` to specify the destination Elasticsearch index.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/logging/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\nremote_logging = True\ndelete_local_logs = False\n\n[elasticsearch]\nhost = <host>:<port>\nwrite_stdout = False\njson_format = True\nwrite_to_es = True\ntarget_index = [name of the index to store logs]\n```\n\n----------------------------------------\n\nTITLE: Building CI Image from Sources with Breeze (Bash)\nDESCRIPTION: This command uses the Breeze tool to build the Airflow CI Docker image. The image is built using the current Airflow source code and includes all dependencies required for CI tests and local development. It's optimized for rebuild speed.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image build\n```\n\n----------------------------------------\n\nTITLE: Configuring AssumeRoleWithSAML in Airflow Connection Extra Field (JSON)\nDESCRIPTION: This complex JSON snippet for the 'Extra' field configures Airflow for AWS authentication using AssumeRoleWithSAML. It details parameters like region, role ARN, SAML provider ARN, IDP URL, authentication method (e.g., SPNEGO via requests_gssapi), IDP request parameters (headers, verification), retry logic for IDP requests, and how to parse the SAML response. Additional arguments can be passed directly to the STS client.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"region_name\":\"eu-west-1\",\n  \"role_arn\":\"arn:aws:iam::112223334444:role/my_role\",\n  \"assume_role_method\":\"assume_role_with_saml\",\n  \"assume_role_with_saml\":{\n    \"principal_arn\":\"arn:aws:iam::112223334444:saml-provider/my_saml_provider\",\n    \"idp_url\":\"https://idp.mycompany.local/.../saml/clients/amazon-aws\",\n    \"idp_auth_method\":\"http_spegno_auth\",\n    \"mutual_authentication\":\"OPTIONAL\",\n    \"idp_request_kwargs\":{\n      \"headers\":{\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"},\n      \"verify\":false\n    },\n    \"idp_request_retry_kwargs\": {\n      \"total\": 10,\n      \"backoff_factor\":1,\n      \"status\":10,\n      \"status_forcelist\": [400, 429, 500, 502, 503, 504]\n    },\n    \"log_idp_response\":false,\n    \"saml_response_xpath\":\"////INPUT[@NAME='SAMLResponse']/@VALUE\",\n  },\n  \"assume_role_kwargs\": { \"something\":\"something\" }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Custom Logging Configuration in Airflow\nDESCRIPTION: This code snippet shows how to create a basic custom logging configuration by copying the default configuration that can be modified for custom needs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/advanced-logging-configuration.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom copy import deepcopy\nfrom airflow.config_templates.airflow_local_settings import DEFAULT_LOGGING_CONFIG\n\nLOGGING_CONFIG = deepcopy(DEFAULT_LOGGING_CONFIG)\n```\n\n----------------------------------------\n\nTITLE: Disabling OpenLineage Integration in airflow.cfg\nDESCRIPTION: Configuration to disable the OpenLineage integration without uninstalling the provider package, which can be useful for temporary deactivation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_11\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\ndisabled = true\n```\n\n----------------------------------------\n\nTITLE: Implementing Git Sync Helm Chart Test in Python\nDESCRIPTION: Example demonstrating how to test Helm chart rendering using Python with git sync configuration. Shows usage of render_chart and render_k8s_object functions for testing Kubernetes objects.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/helm_unit_tests.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom chart_utils.helm_template_generator import render_chart, render_k8s_object\n\ngit_sync_basic = \"\"\"\ndags:\n  gitSync:\n  enabled: true\n\"\"\"\n\n\nclass TestGitSyncScheduler:\n    def test_basic(self):\n        helm_settings = yaml.safe_load(git_sync_basic)\n        res = render_chart(\n            \"GIT-SYNC\",\n            helm_settings,\n            show_only=[\"templates/scheduler/scheduler-deployment.yaml\"],\n        )\n        dep: k8s.V1Deployment = render_k8s_object(res[0], k8s.V1Deployment)\n        assert \"dags\" == dep.spec.template.spec.volumes[1].name\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Auth Manager Group Commands in Python\nDESCRIPTION: This snippet demonstrates how to create a custom auth manager by implementing group commands. It returns a list containing a GroupCommand object with subcommands.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/auth-manager/index.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n        return [\n            GroupCommand(\n                name=\"my_cool_auth_manager\",\n                help=\"Description of what this group of commands do\",\n                subcommands=sub_commands,\n            ),\n        ]\n```\n\n----------------------------------------\n\nTITLE: Deleting Apache Kafka Consumer Group with ManagedKafkaDeleteConsumerGroupOperator in Python\nDESCRIPTION: This snippet demonstrates how to delete an Apache Kafka consumer group using the ManagedKafkaDeleteConsumerGroupOperator. It specifies the project ID, region, cluster, and consumer group.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndelete_consumer_group = ManagedKafkaDeleteConsumerGroupOperator(\n    task_id=\"delete_consumer_group\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n    consumer_group=CONSUMER_GROUP,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Worker Resources in Airflow YAML\nDESCRIPTION: Example configuration for setting CPU resource requests and limits for Airflow workers using YAML configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/customizing-workers.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nworkers:\n  resources:\n    requests:\n      cpu: 1\n    limits:\n      cpu: 1\n```\n\n----------------------------------------\n\nTITLE: Restarting DataFusion Instance with CloudDataFusionRestartInstanceOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the CloudDataFusionRestartInstanceOperator to restart a Google Cloud DataFusion instance. It specifies the instance name, project ID, location, and retry parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datafusion.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nCloudDataFusionRestartInstanceOperator(\n    task_id=\"restart_instance\",\n    instance_name=INSTANCE_NAME,\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    retry=RETRY,\n    timeout=TIMEOUT,\n    metadata=METADATA,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Yandex Lockbox with Service Account Key Path in Airflow (INI)\nDESCRIPTION: Demonstrates configuring `backend_kwargs` in `airflow.cfg` to authenticate using a service account key stored in a JSON file. The path to the file is specified via `yc_sa_key_json_path`, along with the `folder_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_7\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend_kwargs = {\"folder_id\": \"b1g66mft1vo1n4vbn57j\", \"yc_sa_key_json_path\": \"/home/airflow/authorized_key.json\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Spark Provider with Kubernetes Support\nDESCRIPTION: Command to install the Apache Spark provider package with additional Kubernetes support dependencies\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-spark[cncf.kubernetes]\n```\n\n----------------------------------------\n\nTITLE: AQLSensor with Template File Example\nDESCRIPTION: Example of using AQLSensor with a template file for loading the monitoring query. Similar to AQLOperator, the template file can be used to keep queries separate from the code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/operators/index.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwait_for_student = AQLSensor(\n    task_id=\"wait_for_student\",\n    arangodb_conn_id=\"arangodb_default\",\n    template_file=\"template_file.sql\"\n```\n\n----------------------------------------\n\nTITLE: Version 7.10.0 Changelog Entries\nDESCRIPTION: List of commits and changes for version 7.10.0 of the Kubernetes provider, including features like CRD support, pod annotations, and executor improvements.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n7.10.0\n......\n\nLatest change: 2023-11-24\n\nCommit                                                                                              Committed    Subject\n--------------------------------------------------------------------------------------------------  -----------  ------------------------------------------------------------------------------\n`0b23d5601c <https://github.com/apache/airflow/commit/0b23d5601c6f833392b0ea816e651dcb13a14685>`__  2023-11-24   \"Prepare docs 2nd wave of Providers November 2023 (#35836)\"\n```\n\n----------------------------------------\n\nTITLE: Advanced ODBC Configuration with Connect Arguments\nDESCRIPTION: Extended JSON configuration example showing how to specify both ODBC driver parameters and additional pyodbc connection arguments.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/connections/odbc.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Driver\": \"ODBC Driver 18 for SQL Server\",\n  \"ApplicationIntent\": \"ReadOnly\",\n  \"TrustedConnection\": \"Yes\",\n  \"connect_kwargs\": {\n    \"autocommit\": false,\n    \"ansi\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example SQL Script for Creating MSSQL Users Table\nDESCRIPTION: Provides the SQL code for creating a 'Users' table in MSSQL. This script defines columns for `user_id` (primary key, auto-incrementing), `username`, and `description`. It is intended to be saved in a file like `create_table.sql` and executed via `SQLExecuteQueryOperator`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/operators.rst#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- create Users table\nCREATE TABLE Users (\n    user_id INT NOT NULL IDENTITY(1,1) PRIMARY KEY,\n    username TEXT,\n    description TEXT\n);\n```\n\n----------------------------------------\n\nTITLE: Formatting Private Key for Airflow UI Extras\nDESCRIPTION: Command to format a private SSH key for use in the Airflow UI extras field by replacing newlines with '\\n'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/docs/connections/ssh.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -c 'import re, sys; print(re.sub(\"\\r?\\n\", \"\\\\n\", sys.stdin.read()))' < /path/to/your/key\n```\n\n----------------------------------------\n\nTITLE: Updating BaseOperatorLink Method Signature in Python\nDESCRIPTION: The BaseOperatorLink.get_link method signature has been updated to accept ti_key instead of execution_date for identifying task instances. This change affects code interface and requires migration using ruff linter rule AIR302.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/46415.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nBaseOperatorLink.get_link\n```\n\n----------------------------------------\n\nTITLE: Building and Tagging CI Images Using Breeze in Bash\nDESCRIPTION: This bash code demonstrates how to explicitly override the GITHUB_REPOSITORY variable when building a CI image with the breeze CLI tool. It ensures that the image build uses the main Apache Airflow repository cache, resulting in faster builds by leveraging shared images. The snippet then shows how to tag the built docker image for local use. Dependencies include the breeze CLI, Docker, and appropriate permissions/tokens for the referenced GitHub repository. Parameters include the Python version (e.g., --python 3.10) and repository override. Inputs are CLI arguments; outputs are the locally tagged Docker images. Intended for use in a GitHub Actions workflow or similar CI setup; requires access to breeze and docker environments.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/03_github_variables.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# GITHUB_REPOSITORY is set automatically in GitHub Actions so we need to override it with flag\\n#\\nbreeze ci-image build --github-repository apache/airflow --python 3.10\\ndocker tag ghcr.io/apache/airflow/main/ci/python3.10 your-image-name:tag\n```\n\n----------------------------------------\n\nTITLE: CI/CD Configuration for Airflow Helm Chart\nDESCRIPTION: YAML configuration required when installing Airflow using CI/CD tools like Argo CD, Flux, Rancher, or Terraform to handle database migrations properly.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/index.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncreateUserJob:\n  useHelmHooks: false\n  applyCustomEnv: false\nmigrateDatabaseJob:\n  useHelmHooks: false\n  applyCustomEnv: false\n```\n\n----------------------------------------\n\nTITLE: Retrieving Google Dataprep Job Group Information in Python\nDESCRIPTION: Example usage of the DataprepGetJobGroupOperator to get information about a specified job group in Google Dataprep.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataprep.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# [START how_to_dataprep_get_job_group_operator]\n# Example usage code would be here\n# [END how_to_dataprep_get_job_group_operator]\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare documentation for July release of providers.\nDESCRIPTION: Linked to commit b916b75079, this message marks the preparation of documentation for the main July 2021 release of Airflow providers, as noted in issue #17015.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n``Prepare documentation for July release of providers. (#17015)``\n```\n\n----------------------------------------\n\nTITLE: Setting Kubernetes Connection Using JSON Format in Bash\nDESCRIPTION: Example of setting a Kubernetes connection string using environment variable in JSON format. Shows how to specify connection type, in-cluster mode, kubeconfig path, and namespace settings.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/connections/kubernetes.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_CONN_KUBERNETES_DEFAULT='{\"conn_type\": \"kubernetes\", \"extra\": {\"in_cluster\": true, \"kube_config_path\": \"~/.kube/config\", \"namespace\": \"my-namespace\"}}'\n```\n\n----------------------------------------\n\nTITLE: REST API Request Example\nDESCRIPTION: Example curl command to send a request to Airflow's REST API using basic authentication\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nENDPOINT_URL=\"http://localhost:8080/\"\ncurl -X GET  \\\n    --user \"airflow:airflow\" \\\n    \"${ENDPOINT_URL}/api/v1/pools\"\n```\n\n----------------------------------------\n\nTITLE: Disabling OpenLineage Using Environment Variable\nDESCRIPTION: Example of disabling the OpenLineage integration using an environment variable instead of airflow.cfg configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_12\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__DISABLED=true\n```\n\n----------------------------------------\n\nTITLE: Installing the Core ODBC Provider Package using Pip (Bash)\nDESCRIPTION: Installs the `apache-airflow-providers-odbc` package using the pip package manager. This command should be executed in a compatible Python environment (3.9-3.12) with Apache Airflow version 2.9.0 or higher already installed or being installed alongside.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-odbc\n```\n\n----------------------------------------\n\nTITLE: Formatting Git Commit Hash and Message in Markdown\nDESCRIPTION: This code snippet demonstrates how to format a Git commit hash as a hyperlink along with the commit date and message in a markdown table row.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n`d8ec1ec8ec <https://github.com/apache/airflow/commit/d8ec1ec8ecae64dbe8591496e8bec71d5e3ca25a>`__  2022-06-28   ``'WebHDFSHook' Bugfix/optional port (#24550)``\n```\n\n----------------------------------------\n\nTITLE: Emitting Asset Events using AssetAlias and outlet_events in Python with Airflow\nDESCRIPTION: This snippet shows how to use AssetAlias to emit asset events during task execution. It creates an asset event for an S3 URI with additional metadata using the outlet_events parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk.definitions.asset import AssetAlias\n\n\n@task(outlets=[AssetAlias(\"my-task-outputs\")])\ndef my_task_with_outlet_events(*, outlet_events):\n    outlet_events[AssetAlias(\"my-task-outputs\")].add(Asset(\"s3://bucket/my-task\"), extra={\"k\": \"v\"})\n```\n\n----------------------------------------\n\nTITLE: Configuring Oracle Connection Extra Parameters in JSON\nDESCRIPTION: Example JSON configuration for the 'extras' field of an Oracle connection, showing how to set events mode, connection mode, and session purity settings.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/connections/oracle.rst#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"events\": false,\n   \"mode\": \"sysdba\",\n   \"purity\": \"new\"\n}\n```\n\n----------------------------------------\n\nTITLE: Updating a Tag Template with CloudDataCatalogUpdateTagTemplateOperator in Python\nDESCRIPTION: Example of updating a tag template in Google Cloud DataCatalog using the CloudDataCatalogUpdateTagTemplateOperator. Jinja templating can be used to dynamically determine parameter values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_update_tag_template]\nupdate_tag_template = CloudDataCatalogUpdateTagTemplateOperator(\n    task_id=\"update_tag_template\",\n    tag_template=TAG_TEMPLATE,\n    update_mask={\"paths\": [\"display_name\"]},\n)\n# [END howto_operator_gcp_datacatalog_update_tag_template]\n```\n\n----------------------------------------\n\nTITLE: Updating AVP Policy Store Schema using Airflow CLI (Bash)\nDESCRIPTION: This command updates the schema of an existing Amazon Verified Permissions (AVP) policy store used by Airflow to the latest version defined in the provider. This is typically needed if the policy store was created manually, if manual schema modifications need reverting, or if upgrading to a newer schema version is required due to changes in the Airflow AWS provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/setup/amazon-verified-permissions.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairflow aws-auth-manager update-avp-schema\n```\n\n----------------------------------------\n\nTITLE: Accessing CloudTranslateTextOperator Results via XCom in Python\nDESCRIPTION: Shows how to retrieve the translation results from a preceding `CloudTranslateTextOperator` task using Airflow's XCom mechanism. The results, identified by the `task_id` 'translate_text_task', are accessed within a `BashOperator` and printed to the logs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nget_translation_task = BashOperator(\n    bash_command=\"echo \"{{ task_instance.xcom_pull('translate_text_task') }}\"\",\n    task_id=\"get_translation_task\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Mask in Task Function\nDESCRIPTION: Example of implementing custom masking in an Airflow task function using the mask_secret utility to hide sensitive values in logs and output.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/mask-sensitive-values.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef my_func():\n    from airflow.sdk.execution_time.secrets_masker import mask_secret\n\n    mask_secret(\"custom_value\")\n\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Pod Template Dictionary - KubernetesPodOperator\nDESCRIPTION: Example of using pod_template_dict field in KubernetesPodOperator for Kubernetes pod configuration\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npod_template_dict = {\n    'pod_name': 'example',\n    'host_aliases': [...],\n    'annotations': {...}\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Lineage Collection in Custom Hook\nDESCRIPTION: Example showing how to implement lineage collection in a custom hook class by using HookLineageCollector to track input and output assets.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/lineage.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.lineage.hook import get_hook_lineage_collector\n\n\nclass CustomHook(BaseHook):\n    def run(self):\n        # run actual code\n        collector = get_hook_lineage_collector()\n        collector.add_input_asset(self, asset_kwargs={\"scheme\": \"file\", \"path\": \"/tmp/in\"})\n        collector.add_output_asset(self, asset_kwargs={\"scheme\": \"file\", \"path\": \"/tmp/out\"})\n```\n\n----------------------------------------\n\nTITLE: Configuring a Deferrable Spark Job for Google Cloud Dataproc\nDESCRIPTION: This code defines a configuration for a Spark job to be submitted in deferrable mode, which allows for asynchronous execution to improve Airflow resource utilization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nSPARK_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"spark_job\": {\n        \"main_class\": \"org.apache.spark.examples.SparkPi\",\n        \"jar_file_uris\": [\"file:///usr/lib/spark/examples/jars/spark-examples.jar\"],\n        \"args\": [\"1000\"],\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Getting Dataplex Aspect Type using Airflow Python\nDESCRIPTION: This snippet demonstrates how to retrieve details of a specific Aspect Type from a location in Google Cloud Dataplex Catalog using the `DataplexCatalogGetAspectTypeOperator` in an Airflow DAG. It references an external example file for the specific implementation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_50\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_get_aspect_type]\n#     :end-before: [END howto_operator_dataplex_catalog_get_aspect_type]\n\n# This example uses DataplexCatalogGetAspectTypeOperator.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Implementing a Task Instance Mutation Hook for Queue Routing\nDESCRIPTION: A hook that changes the queue for task instances that are being retried, sending them to a dedicated 'retry' queue for better resource management.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/cluster-policies.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef task_instance_mutation_hook(task_instance):\n    \"\"\"\n    This is a function that modifies the task instance before execution by the executor.\n    It is used for instances of changing the task's queue when the task is on it's retry.\n    \"\"\"\n    if task_instance.try_number > 1:\n        task_instance.queue = \"retry_queue\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Corrupted DAGs in Airflow Unit Tests with Python\nDESCRIPTION: Code snippet demonstrating how to access intentionally corrupted DAG files in unit tests. The code first sets the TEST_DAG_FOLDER path to the 'dags_corrupted' directory, then creates a DagBag instance with this folder, and finally retrieves a specific DAG by its ID.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/tests/unit/dags_corrupted/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nTEST_DAG_FOLDER = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"dags_corrupted\")\n\ndagbag = DagBag(dag_folder=TEST_DAG_FOLDER)\ndag = dagbag.get_dag(dag_id)\n```\n\n----------------------------------------\n\nTITLE: Concatenating Multiple Upstreams in Python for Apache Airflow\nDESCRIPTION: Demonstrates how to combine input sources to run the same task against multiple iterables using the concat() function, improving scalability and ease of inspection.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nlist_filenames_a = S3ListOperator(\n    task_id=\"list_files_in_a\",\n    bucket=\"bucket\",\n    prefix=\"incoming/provider_a/{{ data_interval_start|ds }}\",\n)\nlist_filenames_b = S3ListOperator(\n    task_id=\"list_files_in_b\",\n    bucket=\"bucket\",\n    prefix=\"incoming/provider_b/{{ data_interval_start|ds }}\",\n)\n\n\n@task\ndef download_file(filename):\n    S3Hook().download_file(filename)\n    # process file...\n\n\nlist_filenames_concat = list_filenames_a.concat(list_filenames_b)\ndownload_file.expand(filename=list_filenames_concat)\n```\n\n----------------------------------------\n\nTITLE: Exporting Entities from Google Cloud Datastore to Cloud Storage in Python\nDESCRIPTION: This snippet demonstrates how to use the CloudDatastoreExportEntitiesOperator to export entities from Google Cloud Datastore to Cloud Storage. It specifies the project ID, namespace, and output URL prefix for the export operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nexport_task = CloudDatastoreExportEntitiesOperator(\n    task_id=\"export_task\",\n    project_id=GCP_PROJECT_ID,\n    namespace=None,\n    output_url_prefix=f\"gs://{BUCKET_NAME}/{EXPORT_BUCKET_FOLDER}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using DataflowJobStatusSensor in Airflow\nDESCRIPTION: Shows how to use the `DataflowJobStatusSensor` to wait for a Dataflow job to reach a specific status (e.g., DONE, FAILED, CANCELLED). This is useful for managing dependencies on asynchronously triggered jobs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_native_python_async.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_sensor_wait_for_job_status]\n    :end-before: [END howto_sensor_wait_for_job_status]\n```\n\n----------------------------------------\n\nTITLE: Running Airflow Database Migrations\nDESCRIPTION: Shell command to execute Apache Airflow database migrations using the Airflow CLI. This command is mentioned in the context of the 3.0.0 breaking changes, advising users to run it manually if an automatic Airflow core upgrade occurs due to the provider requiring Airflow 2.1.0 or higher.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nairflow upgrade db\n```\n\n----------------------------------------\n\nTITLE: Adding Packages Requiring Compilation (Dockerfile)\nDESCRIPTION: Dockerfile example showing how to install a Python package (mpi4py) that requires compilation. This involves installing build dependencies (like `build-essential` and `mpi compiler`) using apt before installing the PyPI package.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_17\n\nLANGUAGE: Dockerfile\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/add-build-essential-extend/Dockerfile\n    :language: Dockerfile\n    :start-after: [START Dockerfile]\n    :end-before: [END Dockerfile]\n```\n\n----------------------------------------\n\nTITLE: Listing Notification Channels with StackdriverListNotificationChannelsOperator in Python\nDESCRIPTION: This example illustrates how to use the StackdriverListNotificationChannelsOperator to fetch all Notification Channels identified by a given filter. The operator can be used with or without a project ID, which will be retrieved from the Google Cloud connection if not provided.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/stackdriver.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nStackdriverListNotificationChannelsOperator(\n    task_id='list_notification_channels',\n    filter_='',\n    project_id=None,\n    gcp_conn_id='google_cloud_default'\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Presto Provider with pip for Apache Airflow - Bash\nDESCRIPTION: This bash code snippet demonstrates how to install the \"apache-airflow-providers-presto\" package using pip. It is intended for users who already have an existing Airflow 2 installation and need to add Presto integration. The command automatically pulls compatible versions required by the provider; make sure you meet the minimum Airflow version and Python requirements as listed previously. No code dependencies beyond pip are necessary.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/presto/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-presto\n```\n\n----------------------------------------\n\nTITLE: Executing Command on Remote GCE Instance with Cloud OS Login\nDESCRIPTION: This snippet demonstrates how to create a ComputeEngineRemoteInstanceSSHOperator with Cloud OS Login enabled. It requires the service account to have 'compute.osAdminLogin' IAM role and the instance metadata to have Cloud OS Login enabled.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute_ssh.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineSSHOperator(\n    task_id=\"ssh_operator_task_id\",\n    instance_name=\"instance_name\",\n    zone=\"us-central1-a\",\n    project_id=\"example-project\",\n    command=\"echo 'Hello World!'\",\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Common IO Provider Package via pip - Bash\nDESCRIPTION: This snippet shows the Bash command to install the 'apache-airflow-providers-common-io' package and its extras using pip. It assumes you have an existing Airflow 2 installation that meets the minimum version requirements. No Python code is required; just run the command in your terminal to install the provider, including optional features with the '[common.compat]' extra.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/io/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-common-io[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Updating Provider YAML Link Information\nDESCRIPTION: Modifies the `provider.yaml` files to update information regarding links, likely related to documentation or external resources, tracked in issue #35837.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nUpdate information about links into the provider.yaml files (#35837)\n```\n\n----------------------------------------\n\nTITLE: Fixing Placeholder Handling in SQLite Hook\nDESCRIPTION: This code snippet addresses issues with placeholder handling in the SQLiteHook, along with similar fixes for TrinoHook and PrestoHook. It improves the reliability of SQL query execution with placeholders.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"Fix placeholders in 'TrinoHook', 'PrestoHook', 'SqliteHook' (#25939)\"\n```\n\n----------------------------------------\n\nTITLE: Making Kerberos Authenticated API Request using cURL (Bash)\nDESCRIPTION: Shows an example of authenticating with Kerberos to access an Airflow API endpoint (`/api/v1/pools`) using cURL. It requires obtaining a Kerberos ticket using `kinit` first, then uses `curl` with the `--negotiate` flag for SPNEGO authentication and specifies the correct service principal name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/api-authentication.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkinit user_name@REALM\nENDPOINT_URL=\"http://localhost:8080/\"\ncurl -X GET  \\\n    --negotiate \\  # enables Negotiate (SPNEGO) authentication\n    --service airflow \\  # matches the `airflow` service name in the `airflow/fully.qualified.domainname@REALM` principal\n    --user : \\\n    \"${ENDPOINT_URL}/api/v1/pools\"\n```\n\n----------------------------------------\n\nTITLE: Adding Airflow Configuration via Environment Variables\nDESCRIPTION: This Dockerfile example demonstrates how to add Airflow configuration to the image using environment variables. It sets the AIRFLOW__CORE__LOAD_EXAMPLES variable to false.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_4\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apache/airflow:2.7.1\nENV AIRFLOW__CORE__LOAD_EXAMPLES=false\n```\n\n----------------------------------------\n\nTITLE: Deleting Stored Info-Type from Google Cloud DLP\nDESCRIPTION: Shows how to delete a stored info-type using CloudDLPDeleteStoredInfoTypeOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/data_loss_prevention.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_dlp_delete_info_type]\n[END howto_operator_dlp_delete_info_type]\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Azure Blob Storage to Google Cloud Storage using Python\nDESCRIPTION: This code snippet demonstrates how to use the AzureBlobStorageToGCSOperator to transfer data from Azure Blob Storage to Google Cloud Storage. It includes the operator configuration with necessary parameters such as task ID, Azure credentials, source blob details, and GCS destination details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/azure_blob_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START how_to_azure_blob_to_gcs]\nAzureBlobStorageToGCSOperator(\n    task_id='azure_blob_to_gcs',\n    # [START how_to_azure_blob_to_gcs_params]\n    azure_conn_id='azure_blob_default',\n    container_name='azure_container',\n    blob_name='azure_blob',\n    file_path='azure_file',\n    bucket_name='{{ var.value.gcp_bucket }}',\n    object_name='file',\n    filename='file',\n    gzip=False,\n    impersonation_chain={{ var.json.GCP_IMPERSONATION_CHAIN }},\n    # [END how_to_azure_blob_to_gcs_params]\n)\n# [END how_to_azure_blob_to_gcs]\n```\n\n----------------------------------------\n\nTITLE: Task Group Branching Implementation\nDESCRIPTION: Shows how to implement branching logic within a mapped task group based on input values.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ninputs = [\"a\", \"b\", \"c\"]\n\n\n@task_group(group_id=\"my_task_group\")\ndef my_task_group(input):\n    @task.branch\n    def branch(element):\n        if \"a\" in element:\n            return \"my_task_group.a\"\n        elif \"b\" in element:\n            return \"my_task_group.b\"\n        else:\n            return \"my_task_group.c\"\n\n    @task\n    def a():\n        print(\"a\")\n\n    @task\n    def b():\n        print(\"b\")\n\n    @task\n    def c():\n        print(\"c\")\n\n    branch(input) >> [a(), b(), c()]\n\n\nmy_task_group.expand(input=inputs)\n```\n\n----------------------------------------\n\nTITLE: Deleting BigQuery Dataset - Python\nDESCRIPTION: Example of using BigQueryDeleteDatasetOperator to remove a dataset from BigQuery.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndelete_dataset_task = BigQueryDeleteDatasetOperator(\n    task_id=\"delete_dataset\",\n    dataset_id=DATASET_NAME,\n    delete_contents=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Outbound HTTP Proxy in Airflow Connection Extra Field (JSON)\nDESCRIPTION: This JSON snippet, placed in the 'Extra' field of an Airflow AWS connection, configures Boto3 (the underlying AWS SDK) to route AWS API calls through a specified HTTP/HTTPS proxy. The proxy URLs are provided within the 'config_kwargs' dictionary under 'proxies'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"config_kwargs\": {\n    \"proxies\": {\n      \"http\": \"http://myproxy.mycompany.local:8080\",\n      \"https\": \"http://myproxy.mycompany.local:8080\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Polling dbt Cloud Job Run Status in Python\nDESCRIPTION: This example shows how to use the DbtCloudJobRunSensor to periodically check the status of a dbt Cloud job run. The run_id is obtained from the output of a previous DbtCloudRunJobOperator task.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/operators.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndbt_cloud_job_run_sensor = DbtCloudJobRunSensor(\n    task_id=\"dbt_cloud_job_run_sensor\",\n    run_id=dbt_cloud_run_job_async.output[\"id\"],\n    timeout=TIMEOUT,\n)\n```\n\n----------------------------------------\n\nTITLE: DAG Testing Configuration\nDESCRIPTION: Python code block for adding a test execution entry point to DAG files for debugging purposes.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_pycharm.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    dag.test()\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: D401 Support in Providers (simple)\nDESCRIPTION: This text is a commit message summary describing the addition of support for pydocstyle rule D401 (first line should be in imperative mood) within providers, noted as a 'simple' implementation, linked to pull request #37258.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_21\n\nLANGUAGE: plaintext\nCODE:\n```\nD401 Support in Providers (simple) (#37258)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Created Tag Template ID from XCom in Python\nDESCRIPTION: Example of retrieving the newly created tag template ID from XCom after creating a tag template with CloudDataCatalogCreateTagTemplateOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_create_tag_template_result]\ntask_instance = kwargs[\"ti\"]\ncreate_tag_template_result = task_instance.xcom_pull(\n    task_ids=\"create_tag_template\", key=\"tag_template_id\"\n)\nprint(create_tag_template_result)\n# [END howto_operator_gcp_datacatalog_create_tag_template_result]\n```\n\n----------------------------------------\n\nTITLE: Listing Provider Documentation Structure - Bash - bash\nDESCRIPTION: Demonstrates the expected directory and file structure for an Airflow provider package using a tree-style listing. Describes important locations for configuration, documentation, and logo assets, guiding contributors in maintaining a consistent layout. There are no runtime dependencies, as this is intended for structural reference in documentation or terminal output.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/MANAGING_PROVIDERS_LIFECYCLE.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n pyproject.toml\n providers/<PROVIDER>/src/airflow/providers/\n                                               provider.yaml\n                                               pyproject.toml\n                                               CHANGELOG.rst\n                                              \n                                               docs/\n                                                   integration-logos\n                                                                      <PROVIDER>.png\n                                                   index.rst\n                                                   commits.rst\n                                                   connections.rst\n                                                   operators/\n                                                       <PROVIDER>.rst\n```\n\n----------------------------------------\n\nTITLE: Displaying GitHub Provider Package Changelog\nDESCRIPTION: ReStructuredText markup documenting the version history and changes for the GitHub provider package, including commit hashes, dates and descriptions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/github/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nPackage apache-airflow-providers-github\n------------------------------------------------------\n\n`GitHub <https://www.github.com/>`__\n\n\nThis is detailed commit list of changes for versions provider package: ``github``.\nFor high-level changelog, see :doc:`package information including changelog <index>`.\n```\n\n----------------------------------------\n\nTITLE: Displaying Airflow Provider Commits (Jan 2024) using RST Table\nDESCRIPTION: This reStructuredText (RST) snippet formats a list of recent commits for Apache Airflow providers (specifically the January 2024, 2nd wave) into a table. It includes the commit hash (linked to the specific GitHub commit), the commit date, and the commit subject, utilizing RST's table structure, hyperlink syntax (`<link>`__), and inline code formatting (``code``).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ====================================================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ====================================================================================================================\n`cead3da4a6 <https://github.com/apache/airflow/commit/cead3da4a6f483fa626b81efd27a24dcb5a36ab0>`__  2024-01-26   ``Add docs for RC2 wave of providers for 2nd round of Jan 2024 (#37019)``\n`0b680c9492 <https://github.com/apache/airflow/commit/0b680c94922e3f7ca1f3ada8328e315bbae37dc8>`__  2024-01-26   ``Revert \"Provide the logger_name param in providers hooks in order to override the logger name (#36675)\" (#37015)``\n`2b4da0101f <https://github.com/apache/airflow/commit/2b4da0101f0314989d148c3c8a02c87e87048974>`__  2024-01-22   ``Prepare docs 2nd wave of Providers January 2024 (#36945)``\n`6bd450da1e <https://github.com/apache/airflow/commit/6bd450da1eb6cacc2ccfd4544d520ae059b75c3b>`__  2024-01-10   ``Provide the logger_name param in providers hooks in order to override the logger name (#36675)``\n`ecb2c9f24d <https://github.com/apache/airflow/commit/ecb2c9f24d1364642604c14f0deb681ab4894135>`__  2024-01-09   ``Set min pandas dependency to 1.2.5 for all providers and airflow (#36698)``\n`19ebcac239 <https://github.com/apache/airflow/commit/19ebcac2395ef9a6b6ded3a2faa29dc960c1e635>`__  2024-01-07   ``Prepare docs 1st wave of Providers January 2024 (#36640)``\n`6937ae7647 <https://github.com/apache/airflow/commit/6937ae76476b3bc869ef912d000bcc94ad642db1>`__  2023-12-30   ``Speed up autocompletion of Breeze by simplifying provider state (#36499)``\n==================================================================================================  ===========  ====================================================================================================================\n```\n\n----------------------------------------\n\nTITLE: Unifying DbApiHook.run() Method in Python\nDESCRIPTION: This code snippet unifies the implementation of the run() method in DbApiHook and its subclasses. It standardizes the behavior of database API hooks across different database types.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"Unify DbApiHook.run() method with the methods which override it (#23971)\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Executors in Airflow INI File\nDESCRIPTION: Demonstrates how to configure multiple executors using a comma-separated list in the Airflow configuration file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/index.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nexecutor = LocalExecutor,CeleryExecutor\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Spark Parent Job Info Injection via INI\nDESCRIPTION: Configures Airflow's OpenLineage integration to automatically inject parent job information (namespace, job name, run ID) into Spark application properties for supported operators by setting `spark_inject_parent_job_info` to `true` in the `[openlineage]` section. This facilitates linking Airflow tasks and Spark jobs in lineage data.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_29\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\nspark_inject_parent_job_info = true\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Query on Apache Impala using SQLExecuteQueryOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the SQLExecuteQueryOperator to connect to an Apache Impala instance and execute a SQL query. It includes setting up the connection, defining the SQL query, and creating the operator task.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/impala/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\n# [START howto_operator_impala]\nimpala_task = SQLExecuteQueryOperator(\n    task_id='impala_query',\n    conn_id='impala_default',\n    sql='SELECT * FROM my_table LIMIT 10',\n    hook_params={\n        'use_ssl': False,\n        'auth_mechanism': 'NOSASL'\n    }\n)\n# [END howto_operator_impala]\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Logging to Stdout via INI\nDESCRIPTION: This configuration snippet shows how to configure Airflow to output task logs to standard output (stdout) in JSON format, intended for collection by external tools like Fluentd or Logstash. `remote_logging` enables the feature, `host` specifies the Elasticsearch endpoint (still required for context), `write_stdout = True` directs logs to stdout, and `json_format = True` ensures the output is JSON.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/logging/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\nremote_logging = True\n\n[elasticsearch]\nhost = <host>:<port>\nwrite_stdout = True\njson_format = True\n```\n\n----------------------------------------\n\nTITLE: UV Virtual Environment Creation Commands\nDESCRIPTION: Commands for creating and managing virtual environments using the uv tool, including specific Python version selection and custom environment naming.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/07_local_virtualenv.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv venv\nuv venv --python 3.9.7\nuv venv .my-venv\n```\n\n----------------------------------------\n\nTITLE: Enabling Git-Sync with Persistence\nDESCRIPTION: Helm command to enable git-sync with persistence for DAG files using ReadWriteMany PVC.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airflow apache-airflow/airflow \\\n  --set dags.persistence.enabled=true \\\n  --set dags.gitSync.enabled=true\n```\n\n----------------------------------------\n\nTITLE: Initializing Databricks Service Principal OAuth in Python\nDESCRIPTION: Adds support for Service Principal OAuth authentication for Databricks. This allows using service principal credentials instead of username/password.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nAdd Service Principal OAuth for Databricks. (#33005)\n```\n\n----------------------------------------\n\nTITLE: Multi-line OpenLineage Extractor Registration in INI/YAML Style\nDESCRIPTION: This snippet illustrates how to register multiple OpenLineage extractors using Airflow configuration, with separator flexibility and YAML-style multi-line syntax. Each extractor class path is on a separate line, and semicolons divide the entries. Supports easier parsing in YAML or multi-line INI configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/developer.rst#2025-04-22_snippet_7\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__EXTRACTORS: >-\\n  full.path.to.FirstExtractor;\\n  full.path.to.SecondExtractor\\n\n```\n\n----------------------------------------\n\nTITLE: Updating AlloyDB User with Airflow Operator\nDESCRIPTION: Uses AlloyDBUpdateUserOperator to update an existing user in an AlloyDB instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/alloy_db.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nupdate_user = AlloyDBUpdateUserOperator(\n    task_id=\"update_user\",\n    project_id=GCP_PROJECT_ID,\n    cluster_id=CLUSTER_ID,\n    instance_id=INSTANCE_ID,\n    user=ALLOYDB_USER,\n    region=GCP_ALLOYDB_REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Connection via Environment Variable in Bash\nDESCRIPTION: Example showing how to configure a Spark connection using an environment variable with URI syntax. The URI includes cluster address, deploy mode, spark binary command, and Kubernetes namespace parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/connections/spark-submit.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SPARK_DEFAULT='spark://mysparkcluster.com:80?deploy-mode=cluster&spark_binary=command&namespace=kube+namespace'\n```\n\n----------------------------------------\n\nTITLE: Configuring Instance Name in Airflow Configuration File\nDESCRIPTION: This snippet shows how to add the 'instance_name' configuration option under the [webserver] section in the airflow.cfg file. This setting allows customization of the DAG home page header and page title.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/customize-ui.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n[webserver]\n\ninstance_name = \"DevEnv\"\n```\n\n----------------------------------------\n\nTITLE: Installing JDBC Dependencies for Apache Airflow\nDESCRIPTION: Command to install the required Python module jaydebeapi for using JDBC connections in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow[jdbc]\n```\n\n----------------------------------------\n\nTITLE: Generating Helm Chart Source Tarball using Breeze in Shell\nDESCRIPTION: Uses the 'breeze' development tool to prepare the source tarball for the Helm chart release. It requires the target version (`${VERSION}`) and version suffix (`${VERSION_SUFFIX}`) as arguments.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nbreeze release-management prepare-helm-chart-tarball --version ${VERSION} --version-suffix ${VERSION_SUFFIX}\n```\n\n----------------------------------------\n\nTITLE: Installing Cohere Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Cohere integration, enabling Cohere hook and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[cohere]'\n```\n\n----------------------------------------\n\nTITLE: Installing Papermill Extras for Apache Airflow\nDESCRIPTION: Command to install Papermill hooks and operators for Apache Airflow. This enables the execution of Jupyter notebooks via Papermill.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_67\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[papermill]'\n```\n\n----------------------------------------\n\nTITLE: Accessing Airflow Configuration After Deprecation (Python)\nDESCRIPTION: This code snippet shows the updated method for accessing configuration values after the deprecation. It imports the 'conf' object from airflow.configuration and uses its 'get' method to retrieve a configuration value.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43530.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.configuration import conf\n\nvalue = conf.get(\"section\", \"key\")\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Speed up autocompletion of Breeze\nDESCRIPTION: This text is a commit message summary indicating an optimization to speed up the autocompletion feature in the Breeze development environment by simplifying provider state handling, linked to pull request #36499.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_27\n\nLANGUAGE: plaintext\nCODE:\n```\nSpeed up autocompletion of Breeze by simplifying provider state (#36499)\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Provider Configuration via Environment Variable in Bash\nDESCRIPTION: This example shows how to set a configuration parameter for a provider with a dot in its name using an environment variable in Bash.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-config.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__PROVIDERS_SOME_PROVIDER__THIS_PARAM=true\n```\n\n----------------------------------------\n\nTITLE: Specifying SQL Dialect in Airflow Connection Extras\nDESCRIPTION: Demonstrates how to explicitly configure the SQL dialect for an Airflow connection by adding the 'dialect_name' key to the connection's extra options. This example sets the dialect to 'mssql', overriding any dialect that might be automatically derived from the connection string. This is useful when auto-detection fails or a specific dialect implementation is required.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/dialects.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndialect_name: 'mssql'\n```\n\n----------------------------------------\n\nTITLE: Building Local Provider Package with Flit\nDESCRIPTION: Command to build a local provider package using the Flit build tool. This allows testing the provider in a local Airflow environment. Flit is recommended due to its minimal dependencies and fast execution.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/12_provider_distributions.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nflit build\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow API Auth Backends via Environment Variable - bash\nDESCRIPTION: This snippet sets the AIRFLOW__API__AUTH_BACKENDS environment variable for Airflow, equivalent to the airflow.cfg configuration, enabling session and basic authentication for the API. This is useful for containerized or transient environments where configuration via files is not practical. The variable expects a comma-separated list of backend classes as value.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__API__AUTH_BACKENDS=airflow.providers.fab.auth_manager.api.auth.backend.session,airflow.providers.fab.auth_manager.api.auth.backend.basic_auth\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Executor Configuration in Airflow\nDESCRIPTION: Configuration snippet showing how to set a custom executor class in Airflow's core configuration. The executor is specified using the full module path to the custom executor class implementation.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/index.rst#2025-04-22_snippet_10\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nexecutor = my_company.executors.MyCustomExecutor\n```\n\n----------------------------------------\n\nTITLE: Storing Connections in JSON Format\nDESCRIPTION: This snippet demonstrates how to store connections in a JSON file. It shows two connection definitions: one as a URI string and another as a JSON object with connection parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/secrets-backend/local-filesystem-secrets-backend.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"CONN_A\": \"mysql://host_a\",\n    \"CONN_B\": {\n        \"conn_type\": \"scheme\",\n        \"host\": \"host\",\n        \"schema\": \"schema\",\n        \"login\": \"Login\",\n        \"password\": \"None\",\n        \"port\": \"1234\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Validating Connection Prefix for Spark JDBC Hook\nDESCRIPTION: A bug fix in version 4.1.3 to validate the connection prefix in the extra field for the Spark JDBC hook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"Validate conn_prefix in extra field for Spark JDBC hook (#32946)\"\n```\n\n----------------------------------------\n\nTITLE: Installing LocalKubernetes Executor Dependencies\nDESCRIPTION: Instructions for installing the required CNCF Kubernetes provider package to use the LocalKubernetes Executor. Can be installed either directly or through Airflow extras.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/local_kubernetes_executor.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-cncf-kubernetes>=7.4.0\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[cncf.kubernetes]'\n```\n\n----------------------------------------\n\nTITLE: Deleting Apache Kafka Topic with ManagedKafkaDeleteTopicOperator in Python\nDESCRIPTION: This snippet demonstrates how to delete an Apache Kafka topic using the ManagedKafkaDeleteTopicOperator. It specifies the project ID, region, cluster, and topic name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndelete_topic = ManagedKafkaDeleteTopicOperator(\n    task_id=\"delete_topic\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n    topic=TOPIC,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting Amazon Bedrock Knowledge Base with TaskFlow API (Python)\nDESCRIPTION: Performs deletion of a specified Bedrock Knowledge Base using a simple AWS boto3 API call inside an Airflow TaskFlow task. This approach offers fine-grained control and integrates deletion directly into an Airflow workflow step.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@task\n    def delete_knowledge_base(kb_id: str):\n        import boto3\n        client = boto3.client(\"bedrock\")\n        client.delete_knowledge_base(knowledgeBaseId=kb_id)\n\n```\n\n----------------------------------------\n\nTITLE: Installing Google Cloud Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Google Cloud integration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[google]'\n```\n\n----------------------------------------\n\nTITLE: Setting PIP Default Timeout in Bash\nDESCRIPTION: Sets extended timeout for pip package installations when facing slow internet connections.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/04_troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport PIP_DEFAULT_TIMEOUT=1000\n```\n\n----------------------------------------\n\nTITLE: Deleting Dataproc Metastore Service using Airflow Operator in Python\nDESCRIPTION: Shows how to use the `DataprocMetastoreDeleteServiceOperator` within an Airflow DAG to remove a Google Cloud Dataproc Metastore service. The operator requires the `region`, `project_id`, and `service_id` of the service to be deleted.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndelete_service = DataprocMetastoreDeleteServiceOperator(\n    task_id=\"delete_service\", region=REGION, project_id=PROJECT_ID, service_id=SERVICE_ID\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow Database Connection\nDESCRIPTION: This snippet demonstrates how to set the Airflow database connection using an environment variable. If not set, a SQLite database is created by default.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n```\n\n----------------------------------------\n\nTITLE: Installing the Salesforce Provider via Pip\nDESCRIPTION: Installs the `apache-airflow-providers-salesforce` package using pip. This command should be executed in an environment with an existing Apache Airflow installation (version 2.9.0 or higher is required as per the requirements section). This package enables interaction with Salesforce from within Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-salesforce\n```\n\n----------------------------------------\n\nTITLE: Deleting PubSub Subscription with PubSubDeleteSubscriptionOperator\nDESCRIPTION: Example showing how to delete a PubSub subscription using the PubSubDeleteSubscriptionOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/pubsub.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndelete_subscription = PubSubDeleteSubscriptionOperator(\n    task_id=\"delete_subscription\",\n    project_id=PROJECT_ID,\n    subscription=SUBSCRIPTION_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Connection Caching Improvement for DbApiHook\nDESCRIPTION: Performance enhancement that generalizes caching of connections in DbApiHook. This improves performance by reusing database connections when possible.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nGeneralize caching of connection in DbApiHook to improve performance (#40751)\n```\n\n----------------------------------------\n\nTITLE: Configuring ODBC Connection Parameters in JSON\nDESCRIPTION: Example JSON configuration for ODBC connection parameters including driver specification and connection attributes for MS SQL Server.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/connections/odbc.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Driver\": \"ODBC Driver 18 for SQL Server\",\n  \"ApplicationIntent\": \"ReadOnly\",\n  \"TrustedConnection\": \"Yes\"\n}\n```\n\n----------------------------------------\n\nTITLE: Fixing Connection Extra Parameter in Hive Hooks\nDESCRIPTION: This code snippet fixes the 'auth_mechanism' connection extra parameter in 'HiveMetastoreHook' and 'HiveServer2Hook'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfix connection extra parameter 'auth_mechanism' in 'HiveMetastoreHook' and 'HiveServer2Hook' (#24713)\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA with Helm\nDESCRIPTION: Commands to add KEDA Helm repository and install KEDA in a Kubernetes cluster. Creates a dedicated namespace and installs KEDA version 2.0.0.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/keda.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n\nhelm repo update\n\nkubectl create namespace keda\n\nhelm install keda kedacore/keda \\\n    --namespace keda \\\n    --version \"v2.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Configuring GCSToBigQueryOperator in Python\nDESCRIPTION: Allows autodetect to be None and infers schema in GCSToBigQueryOperator, providing more options for schema handling.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"GCSToBigQueryOperator allows autodetect None and infers schema (#28564)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Breeze with UV Tool\nDESCRIPTION: Command to install Breeze in editable/development mode using the UV package manager. The --force flag ensures reinstallation when dependencies change.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nuv tool install -e ./dev/breeze --force\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with UV\nDESCRIPTION: Commands to set up virtual environment and install Airflow dependencies using UV package manager. Includes options for installing all provider packages or specific provider dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_pycharm.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ uv sync\n$ uv sync --all-packages\n$ uv sync --packages apache-airflow-provider-amazon\n```\n\n----------------------------------------\n\nTITLE: Creating Redshift Cluster Snapshot with RedshiftCreateClusterSnapshotOperator\nDESCRIPTION: Example showing how to create a snapshot of a Redshift cluster using RedshiftCreateClusterSnapshotOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/redshift/redshift_cluster.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster_snapshot = RedshiftCreateClusterSnapshotOperator(\n    task_id='create_cluster_snapshot',\n    cluster_identifier=redshift_cluster_identifier,\n    snapshot_identifier=redshift_cluster_snapshot_identifier,\n    aws_conn_id='aws_default',\n)\n```\n\n----------------------------------------\n\nTITLE: WebHDFS License Header\nDESCRIPTION: Apache License 2.0 header for the WebHDFS operators documentation file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/operators/webhdfs.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n```\n\n----------------------------------------\n\nTITLE: Google Cloud Connection with JSON Format\nDESCRIPTION: Example showing how to configure a Google Cloud connection using JSON format specifying connection type and extra parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT='{\"conn_type\": \"google_cloud_platform\", \"extra\": {\"key_path\": \"/keys/key.json\", \"scope\": \"https://www.googleapis.com/auth/cloud-platform\", \"project\": \"airflow\", \"num_retries\": 5}}'\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Pig Provider Package for Airflow\nDESCRIPTION: Command to install the Apache Pig provider package for Apache Airflow using pip. This package supports Python versions 3.9, 3.10, 3.11, and 3.12.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pig/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-pig\n```\n\n----------------------------------------\n\nTITLE: Relocating ImportError Class in Airflow Models - Python\nDESCRIPTION: The ImportError class has been moved from airflow.models to airflow.models.errors.ParseImportError. This change affects import statements in code that previously used the class from airflow.models.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41367.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models.errors import ParseImportError  # New import location\n# from airflow.models import ImportError  # Old deprecated import\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Livy Provider with Common Compatibility Dependencies\nDESCRIPTION: Command to install the Apache Livy provider package along with its common compatibility dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/livy/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-livy[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Updating Listener API and TaskSDK/OpenLineage Support\nDESCRIPTION: Commit message describing changes to the listener API, adding basic support for task instance listeners in TaskSDK, and updating the OpenLineage provider for Airflow 3's listener interface. References pull request #45294.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_30\n\nLANGUAGE: plaintext\nCODE:\n```\nchange listener API, add basic support for task instance listeners in TaskSDK, make OpenLineage provider support Airflow 3's listener interface (#45294)\n```\n\n----------------------------------------\n\nTITLE: Attaching Alternative Backend for Object Storage\nDESCRIPTION: Demonstrates how to configure an alternative backend implementation for a specific protocol by attaching a custom filesystem class to the protocol.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/objectstorage.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import ObjectStoragePath\nfrom airflow.sdk.io import attach\n\nfrom fsspec.implementations.dbfs import DBFSFileSystem\n\nattach(protocol=\"dbfs\", fs=DBFSFileSystem(instance=\"myinstance\", token=\"mytoken\"))\nbase = ObjectStoragePath(\"dbfs://my-location/\")\n```\n\n----------------------------------------\n\nTITLE: Pod Template with DAGs in Volume\nDESCRIPTION: YAML configuration for a pod template that mounts DAGs from a persistent volume in Kubernetes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/kubernetes_executor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n    - args: []\n      command: []\n      env: []\n      image: dummy-image\n      name: base\n      volumeMounts:\n        - mountPath: /opt/airflow/dags\n          name: dags\n          readOnly: true\n  volumes:\n    - name: dags\n      persistentVolumeClaim:\n        claimName: airflow-dags\n```\n\n----------------------------------------\n\nTITLE: Running Airflow Initialization\nDESCRIPTION: Command to initialize Airflow database and create first user account\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up airflow-init\n```\n\n----------------------------------------\n\nTITLE: Clearing DAG Access Control in Python\nDESCRIPTION: Example showing how to remove all DAG-level permissions by setting an empty access_control dictionary.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/access-control.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nDAG(\n    dag_id=\"example_no_fine_grained_access\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    access_control={},\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Teradata QueryBand (JSON) in Airflow UI\nDESCRIPTION: This JSON snippet illustrates how to set a QueryBand for a Teradata session using the 'Extras' field in the Airflow UI. The 'query_band' key holds a string value containing key-value pairs separated by semicolons. This QueryBand is applied to the database session initiated by Airflow, allowing for session tracking or workload management within Teradata.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/connections/teradata.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"query_band\": \"appname=airflow;org=test;\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Tags to Airflow DAG Definition\nDESCRIPTION: Example showing how to add tags to an Airflow DAG object during definition. Tags can be used for filtering DAGs in the UI, with the filter preference being saved in a cookie.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/add-dag-tags.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndag = DAG(dag_id=\"example_dag_tag\", schedule=\"0 0 * * *\", tags=[\"example\"])\n```\n\n----------------------------------------\n\nTITLE: Resuming a Google Cloud Tasks Queue in Python\nDESCRIPTION: This snippet demonstrates how to resume a paused Google Cloud Tasks queue using the CloudTasksQueueResumeOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [START resume_queue]\nCloudTasksQueueResumeOperator(\n    task_id=\"resume_queue\",\n    location=LOCATION,\n    queue_name=QUEUE_ID,\n).execute(context=context)\n# [END resume_queue]\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Celery Provider with Constraints\nDESCRIPTION: Command to install the Apache Airflow Celery provider package from PyPI using pip with version-specific constraints. The command ensures compatibility by applying constraints from a specific Airflow version (2.10.2) and Python version (3.9).\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/installing-from-pypi.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"apache-airflow-providers-celery\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.2/constraints-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Synchronizing from GCS Bucket Subdirectory\nDESCRIPTION: Demonstrates synchronizing files from a subdirectory in the source bucket to the destination bucket root. The operation preserves existing files and doesn't delete extra files in the destination.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gcs.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsync_from_subdir = GCSToGCSOperator(\n    task_id=\"sync_from_subdir\",\n    source_bucket=BUCKET_1_SRC,\n    destination_bucket=BUCKET_1_DST,\n    source_object=\"subdir/*\",\n    allow_overwrite=False,\n    delete_extra_files=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting Azure Service Bus Subscription in Python\nDESCRIPTION: This snippet demonstrates how to use the AzureServiceBusSubscriptionDeleteOperator to delete an Azure Service Bus subscription under a topic.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/asb.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_delete_service_bus_subscription]\n# [END howto_operator_delete_service_bus_subscription]\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom REMOTE_USER Authentication Middleware (Python)\nDESCRIPTION: Provides a Python example of a custom WSGI middleware (`CustomMiddleware`) for handling specific authentication scenarios. The middleware intercepts requests, performs custom authentication logic, and sets the `REMOTE_USER` environment variable. This variable is then used by Flask-AppBuilder when `AUTH_TYPE` is configured to `AUTH_REMOTE_USER` in `webserver_config.py`. Requires Flask and Flask-AppBuilder.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/webserver-authentication.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Callable\n\nfrom flask import current_app\nfrom flask_appbuilder.const import AUTH_REMOTE_USER\n\n\nclass CustomMiddleware:\n    def __init__(self, wsgi_app: Callable) -> None:\n        self.wsgi_app = wsgi_app\n\n    def __call__(self, environ: dict, start_response: Callable) -> Any:\n        # Custom authenticating logic here\n        # ...\n        environ[\"REMOTE_USER\"] = \"username\"\n        return self.wsgi_app(environ, start_response)\n\n\ncurrent_app.wsgi_app = CustomMiddleware(current_app.wsgi_app)\n\nAUTH_TYPE = AUTH_REMOTE_USER\n```\n\n----------------------------------------\n\nTITLE: Installing Custom Airflow Operators Package with pip\nDESCRIPTION: Shows the command to install a custom Airflow operators package using pip and the generated wheel file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/modules_management.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install dist/airflow_operators-0.0.0-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Configuring a Dataplex Data Profile Scan in Python\nDESCRIPTION: Defines the configuration for a Google Cloud Dataplex Data Profile scan. This specifies the properties required before creating a data profile scan with the appropriate operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# Configuration to create a simple Dataplex Data Profile scan.\nDATA_PROFILE_SCAN_ID = \"test-data-profile\"\n\nDATA_PROFILE_SCAN = {\n    \"data_profile_spec\": {\"sampling_percent\": 100.0},\n    \"data_source\": {\n        \"entity\": f\"projects/{PROJECT_ID}/locations/{REGION}/lakes/{LAKE_ID}/zones/{ZONE_ID}/entities/{ENTITY_ID}\"\n    },\n    \"description\": \"Data Profile Scan\",\n    \"display_name\": \"test-profile\",\n}\n```\n\n----------------------------------------\n\nTITLE: Task Group Mapping Examples\nDESCRIPTION: Demonstrates various ways to implement task group mapping, including simple expansion and value handling.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@task_group\ndef file_transforms(filename):\n    return convert_to_yaml(filename)\n\n\nfile_transforms.expand(filename=[\"data1.json\", \"data2.json\"])\n```\n\n----------------------------------------\n\nTITLE: Creating Dataform Workspace in Python\nDESCRIPTION: This snippet shows how to create a workspace for storing code in Dataform service using the DataformCreateWorkspaceOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_create_workspace]\n# [END howto_operator_create_workspace]\n```\n\n----------------------------------------\n\nTITLE: Monitoring Glacier Job Status with GlacierJobOperationSensor\nDESCRIPTION: Example demonstrating how to wait for an Amazon Glacier job to reach a terminal state using the GlacierJobOperationSensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/glacier.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwait_for_job = GlacierJobOperationSensor(\n    task_id=\"wait_for_glacier_job\",\n    aws_conn_id=\"aws_default\",\n    vault_name=\"airflow\",\n    account_id=\"-\",\n    job_id=create_job.output\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Jenkins Connection via Environment Variable in Bash\nDESCRIPTION: Example of how to set up a Jenkins connection using an environment variable with URI syntax. All components of the URI should be URL-encoded, and the connection includes username, password, server address, and port.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/connections.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_JENKINS_DEFAULT='http://username:password@server.com:443'\n```\n\n----------------------------------------\n\nTITLE: Installing Kubernetes Integration for Apache Airflow\nDESCRIPTION: Command to install Kubernetes client libraries, KubernetesPodOperator and related components that enable KubernetesExecutor and LocalKubernetesExecutor\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_48\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[cncf-kubernetes]'\n```\n\n----------------------------------------\n\nTITLE: Updating Azure Data Factory Hook in Python\nDESCRIPTION: Adds managed identity support to the Azure Data Factory hook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfeat(provider/azure): add managed identity support to data_factory hook (#35328)\n```\n\n----------------------------------------\n\nTITLE: Running an AppFlow Flow with All Filters Removed Using AppflowRunFullOperator - Apache Airflow - Python\nDESCRIPTION: Illustrates how to use AppflowRunFullOperator to execute an AppFlow run where all filters are removed, typically fetching the complete dataset from the chosen source. Requires Airflow Amazon provider and boto3 dependencies, and assumes setup of the relevant AppFlow connection and permissions. The primary parameter is the flow_name passed to the operator. Expected output is a full extraction of available records at time of run.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/appflow.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_appflow_run_full\",\n    schedule_interval=None,\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"appflow\"],\n) as dag:\n\n    run_flow_full = AppflowRunFullOperator(\n        task_id=\"appflow_run_full_task\",\n        flow_name=\"your_appflow_flow_name\",\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Granting DAG-Specific Airflow Permissions to a User Group in Cedar\nDESCRIPTION: This Cedar policy grants all actions (`action`) related specifically to the Airflow DAG named 'test' (`Airflow::Dag::\"test\"`) to all users within a specified AWS IAM Identity Center group (ID `aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee`). This allows fine-grained control, limiting a group's permissions to operations only on a particular DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/manage/index.rst#2025-04-22_snippet_5\n\nLANGUAGE: cedar\nCODE:\n```\npermit(\n  principal in Airflow::Group::\"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\",\n  action,\n  resource == Airflow::Dag::\"test\"\n);\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Pinot Provider Package for Airflow via pip\nDESCRIPTION: This command installs the Apache Pinot provider package on top of an existing Airflow 2 installation using pip. It's used to add Apache Pinot integration capabilities to Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-pinot\n```\n\n----------------------------------------\n\nTITLE: Deploying Spanner Instance in Python\nDESCRIPTION: Example of using SpannerDeployInstanceOperator to create or update a Cloud Spanner instance. The operator can be created with or without a project ID, which will be retrieved from the Google Cloud connection if not provided.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/spanner.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_spanner_deploy]\n# Without project_id\nspanner_instance = SpannerDeployInstanceOperator(\n    project_id=None,\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    configuration_name=\"{{ var.value.spanner_configuration }}\",\n    node_count=1,\n    display_name=\"Test Instance\",\n    task_id=\"deploy_spanner_instance\",\n)\n\n# With project_id\nspanner_instance = SpannerDeployInstanceOperator(\n    project_id=\"{{ var.value.spanner_project }}\",\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    configuration_name=\"{{ var.value.spanner_configuration }}\",\n    node_count=1,\n    display_name=\"Test Instance\",\n    task_id=\"deploy_spanner_instance\",\n)\n# [END howto_operator_spanner_deploy]\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-common-io via pip - Bash\nDESCRIPTION: This snippet demonstrates how to install the apache-airflow-providers-common-io provider package and its optional extras using pip. It is intended for users who have an existing Apache Airflow 2.9.0+ installation and want to extend its functionality by including this provider and any cross-provider dependencies, such as common.compat. The command should be run in a shell with the appropriate Python and pip environment configured. Input parameter is the extras keyword to specify optional feature groups.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/io/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-common-io[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Update Old Style Typing\nDESCRIPTION: This commit message, associated with commit 9ab1a6a3e7 dated 2022-10-27, notes updates made to the codebase to replace older Python type hinting styles with newer ones.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_23\n\nLANGUAGE: text\nCODE:\n```\n``Update old style typing (#26872)``\n```\n\n----------------------------------------\n\nTITLE: Building PROD Image from PyPI Package with Breeze (Bash)\nDESCRIPTION: Uses Breeze to build the Airflow PROD image for Python 3.9. It installs Airflow version 2.0.0 directly from PyPI instead of building from source. It also adds the 'trino' extra package to the default set.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbreeze prod-image build --python 3.9 --additional-airflow-extras=trino --install-airflow-version=2.0.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Timezone in Airflow\nDESCRIPTION: Configuration example showing how to set the default timezone in Airflow's core settings.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timezone.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[core]\ndefault_timezone = utc\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow for Opensearch Logging\nDESCRIPTION: Basic configuration in airflow.cfg to enable remote logging to Opensearch. Specifies host, port, username, and password for Opensearch connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/logging/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\nremote_logging = True\n\n[opensearch]\nhost = <host>\nport = <port>\nusername = <username>\npassword = <password>\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Python Debug Service\nDESCRIPTION: YAML configuration for adding a Python debug service to docker-compose for PyCharm integration\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nairflow-python:\n  <<: *airflow-common\n  profiles:\n      - debug\n  environment:\n      <<: *airflow-common-env\n  user: \"50000:0\"\n  entrypoint: [ \"/bin/bash\", \"-c\" ]\n```\n\n----------------------------------------\n\nTITLE: Setting BigQuery SQL Mode in Python\nDESCRIPTION: Fixes the setting of the 'use_legacy_sql' parameter in BigQuery job configuration to ensure correct SQL mode is used.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"Set bigquery 'use_legacy_sql' param in job config correctly (#28522)\"\n```\n\n----------------------------------------\n\nTITLE: Updating BigQuery Hook Function in Python\nDESCRIPTION: Renames the BigQuery hook function '_bq_cast' to 'bq_cast' for consistency and clarity.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"Rename  hook bigquery function '_bq_cast' to 'bq_cast' (#27543)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Providers Apache Druid Package\nDESCRIPTION: Command to install the Apache Airflow provider package for Apache Druid using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/druid/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-druid\n```\n\n----------------------------------------\n\nTITLE: MySQL SSL mode configuration\nDESCRIPTION: Code showing how to enable and configure SSL mode in MySQL provider\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nAllow SSL mode in MySQL provider (#27717)\n```\n\n----------------------------------------\n\nTITLE: Updating Stored Info-Type in Google Cloud DLP\nDESCRIPTION: Demonstrates how to update an existing info-type using CloudDLPUpdateStoredInfoTypeOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/data_loss_prevention.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_dlp_update_info_type]\n[END howto_operator_dlp_update_info_type]\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs for July 2023 wave of Providers (#32298)\nDESCRIPTION: This commit message, associated with version 3.2.1, signifies the preparation of documentation for the July 2023 release wave of Apache Airflow Providers. Commit hash: cb4927a018, Date: 2023-07-05.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs for July 2023 wave of Providers (#32298)``\n```\n\n----------------------------------------\n\nTITLE: Initializing Backend Configuration for Apache Airflow in Python\nDESCRIPTION: This code snippet sets up a dictionary of backend configuration parameters for Apache Airflow. It specifies paths for connections and variables, as well as other settings like URL and SSL verification.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/hashicorp/docs/secrets-backends/hashicorp-vault.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nbackend_kwargs = {\"connections_path\": \"dags_mount_point/airflow/connections\", \"variables_path\": \"dags_mount_point/airflow/variables\", \"config_path\": \"\", \"mount_point\": null, \"url\": \"http://127.0.0.1:8200\", \"verify\": \"/etc/ssl/certs/ca-certificates\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring toctree for Standard Sensors in reStructuredText\nDESCRIPTION: Sets up a table of contents (toctree) for standard sensors documentation. It uses maxdepth of 1 and glob pattern to include all files in the current directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/sensors/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Configuring RestructuredText Documentation Toctree\nDESCRIPTION: Sets up a toctree directive to organize documentation structure with maxdepth of 1 and glob pattern to include all files in directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Migrating from ExternalTaskSensorLink to ExternalDagLink in Apache Airflow\nDESCRIPTION: This code snippet demonstrates the migration rule for replacing the deprecated ExternalTaskSensorLink class with the ExternalDagLink class. It shows the old import statement and the new recommended import statement.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41391.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old (deprecated)\nfrom airflow.sensors.external_task import ExternalTaskSensorLink\n\n# New (recommended)\nfrom airflow.sensors.external_task import ExternalDagLink\n```\n\n----------------------------------------\n\nTITLE: Hook Deprecations in Google Provider\nDESCRIPTION: Documents removed hook classes and methods, including their replacements, primarily affecting Google Kubernetes Engine and BigQuery functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nGKEDeploymentHook -> GKEKubernetesHook\nGKECustomResourceHook -> GKEKubernetesHook\nSecretsManagerHook -> GoogleCloudSecretManagerHook\nBigQueryHook methods updates:\nrun_table_delete() -> delete_table()\nget_tabledata() -> list_rows()\ncancel_query() -> cancel_job()\n```\n\n----------------------------------------\n\nTITLE: List CI and Development Tool Changes\nDESCRIPTION: Shell commands to identify changes related to CI and development tools that haven't been merged, excluding previously merged changes.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit fetch apache\ngit log --oneline apache/v2-2-test | sed -n 's/.*\\((#[0-9]*)\\)$/\\1/p' > /tmp/merged\ngit log --oneline --decorate apache/v2-2-stable..apache/main -- Dockerfile* scripts breeze* .github/ setup* dev | grep -vf /tmp/merged\n```\n\n----------------------------------------\n\nTITLE: Listing Hyperparameter Tuning Jobs using Vertex AI Operator - Python\nDESCRIPTION: Describes how to list available hyperparameter tuning jobs using Vertex AI's ListHyperparameterTuningJobOperator in Airflow. Requires region and project ID. The result is typically pushed to XCom or consumed by follow-up tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nlist_hpt_jobs_task = ListHyperparameterTuningJobOperator(\n    task_id=\"list_hptuning_jobs_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Stopping Dataflow Job with Airflow\nDESCRIPTION: Illustrates using the `DataflowStopJobOperator` to stop a running Dataflow job. It can target a specific `job_id` or jobs matching a `job_name_prefix`. Streaming jobs are drained by default unless `drain_pipeline` is set to `False`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_native_python.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_stop_dataflow_job]\n    :end-before: [END howto_operator_stop_dataflow_job]\n```\n\n----------------------------------------\n\nTITLE: Installing Atlassian Jira Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Atlassian Jira integration, enabling Jira hooks and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[atlassian-jira]'\n```\n\n----------------------------------------\n\nTITLE: JWT Token Response for Airflow API Authentication\nDESCRIPTION: This JSON snippet shows the expected response format when successfully generating a JWT token for Airflow API authentication. The response contains an access_token field with the JWT token.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/api.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"access_token\": \"<JWT-TOKEN>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using @asset.multi for Multiple Asset Emission in Python with Airflow\nDESCRIPTION: This code demonstrates the use of @asset.multi decorator as a shorthand for emitting multiple assets from a single function. It processes an input asset and splits it into two output assets.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import Asset, asset\n\ninput_asset = Asset(\"input_asset\")\nout_asset_1 = Asset(\"out_asset_1\")\nout_asset_2 = Asset(\"out_asset_2\")\n\n\n@asset.multi(schedule=None, outlets=[out_asset_1, out_asset_2])\ndef process_input(input_asset):\n    \"\"\"Split input into two.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Standard Livy Operator in Python\nDESCRIPTION: Example showing how to create and configure a standard LivyOperator to submit a Spark application via Livy's REST API. This demonstrates the basic usage without deferrable mode.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/livy/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[START create_livy]\n```\n\n----------------------------------------\n\nTITLE: Inserting Previous Log Template for Compatibility via SQL\nDESCRIPTION: This SQL command demonstrates how to manually insert a previous `log_id_template` into the `log_template` table in the Airflow metadata database. This is necessary if, after upgrading to Airflow 2.3.0+, logs associated with the old template become inaccessible. The example inserts the default template from Airflow 2.2.5 with `id=0` to ensure backward compatibility.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/logging/index.rst#2025-04-22_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO log_template (id, filename, elasticsearch_id, created_at) VALUES (0, '{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log', '{dag_id}-{task_id}-{execution_date}-{try_number}', NOW());\n```\n\n----------------------------------------\n\nTITLE: Installing All Extras for Apache Airflow\nDESCRIPTION: Command to install all optional dependencies including all providers for Apache Airflow. This is recommended for development and testing purposes but not for production.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_72\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow[all]\n```\n\n----------------------------------------\n\nTITLE: Installing Tableau Provider via Pip (Bash)\nDESCRIPTION: This command uses pip, the Python package installer, to install the `apache-airflow-providers-tableau` package. This enables integration with Tableau within an Apache Airflow environment. It should be run in a shell environment where pip is configured. The installation requires an existing Airflow installation (version 2.9.0 or higher) and supports Python versions 3.9 through 3.12.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/tableau/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-tableau\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit on Selected Files\nDESCRIPTION: Command to run pre-commit checks on specific files in the repository\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run  --files airflow/utils/decorators.py tests/utils/test_task_group.py\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-http via Pip (Shell)\nDESCRIPTION: This shell command uses pip, the Python package installer, to install the `apache-airflow-providers-http` package. It should be executed in a shell environment on top of an existing Airflow 2 installation (version 2.9.0 or higher).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-http\n```\n\n----------------------------------------\n\nTITLE: Installing Pinecone Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Pinecone integration, enabling Pinecone Operators and Hooks.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[pinecone]'\n```\n\n----------------------------------------\n\nTITLE: Classifying Text with Google Cloud Natural Language in Apache Airflow\nDESCRIPTION: This snippet demonstrates how to use the CloudNaturalLanguageClassifyTextOperator to perform content classification on a given text.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/natural_language.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclassify_text = CloudNaturalLanguageClassifyTextOperator(\n    task_id=\"classify_text\",\n    document=document,\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Airbyte Provider Version 3.3.2 Changelog\nDESCRIPTION: Lists the changes made in version 3.3.2 of the Airbyte provider, including commit hashes, dates, and descriptions of changes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_31\n\nLANGUAGE: markdown\nCODE:\n```\n3.3.2\n.....\n\nLatest change: 2023-09-08\n\n==================================================================================================  ===========  ========================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ========================================================================================\n`21990ed894 <https://github.com/apache/airflow/commit/21990ed8943ee4dc6e060ee2f11648490c714a3b>`__  2023-09-08   ``Prepare docs for 09 2023 - 1st wave of Providers (#34201)``\n`44cb7c6d8d <https://github.com/apache/airflow/commit/44cb7c6d8d516804c95f84accb21b254ce8846cc>`__  2023-09-07   ``fix(providers/airbyte): respect soft_fail argument when exception is raised (#34156)``\n`c077d19060 <https://github.com/apache/airflow/commit/c077d190609f931387c1fcd7b8cc34f12e2372b9>`__  2023-08-26   ``Prepare docs for Aug 2023 3rd wave of Providers (#33730)``\n`c645d8e40c <https://github.com/apache/airflow/commit/c645d8e40c167ea1f6c332cdc3ea0ca5a9363205>`__  2023-08-12   ``D401 Support - Providers: Airbyte to Atlassian (Inclusive) (#33354)``\n`b5a4d36383 <https://github.com/apache/airflow/commit/b5a4d36383c4143f46e168b8b7a4ba2dc7c54076>`__  2023-08-11   ``Prepare docs for Aug 2023 2nd wave of Providers (#33291)``\n`73b90c48b1 <https://github.com/apache/airflow/commit/73b90c48b1933b49086d34176527947bd727ec85>`__  2023-07-21   ``Allow configuration to be contributed by providers (#32604)``\n`225e3041d2 <https://github.com/apache/airflow/commit/225e3041d269698d0456e09586924c1898d09434>`__  2023-07-06   ``Prepare docs for July 2023 wave of Providers (RC2) (#32381)``\n`3878fe6fab <https://github.com/apache/airflow/commit/3878fe6fab3ccc1461932b456c48996f2763139f>`__  2023-07-05   ``Remove spurious headers for provider changelogs (#32373)``\n`cb4927a018 <https://github.com/apache/airflow/commit/cb4927a01887e2413c45d8d9cb63e74aa994ee74>`__  2023-07-05   ``Prepare docs for July 2023 wave of Providers (#32298)``\n`0bc689ee6d <https://github.com/apache/airflow/commit/0bc689ee6d4b6967d7ae99a202031aac14d181a2>`__  2023-06-28   ``D205 Support - Providers: Airbyte and Alibaba (#32214)``\n`09d4718d3a <https://github.com/apache/airflow/commit/09d4718d3a46aecf3355d14d3d23022002f4a818>`__  2023-06-27   ``Improve provider documentation and README structure (#32125)``\n==================================================================================================  ===========  ========================================================================================\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Dataprep Connection in Airflow\nDESCRIPTION: Example of how to set up the connection to Google Dataprep in Airflow's connection configuration. Requires a token and base URL.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataprep.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nConnection Id: \"your_conn_id\"\nExtra: {\"token\": \"TOKEN\", \"base_url\": \"https://api.clouddataprep.com\"}\n```\n\n----------------------------------------\n\nTITLE: Provider Directory Structure\nDESCRIPTION: Shows the standard directory structure for an Airflow provider package, including configuration files, source code, documentation, and test directories.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/12_provider_distributions.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nPROVIDER\n         |- pyproject.toml   # project configuration\n         |- provider.yaml    # additional metadata for provider\n         |- src\n         |.   \\- airflow.providers.PROVIDER\n         |                                \\ # here are hooks, operators, sensors, transfers\n         |- docs   # docs for provider are stored here\n         \\- tests\n                | -unit\n                |      | PROVIDER\n                |               \\ # here unit test code is present\n                | - integration\n                |             | PROVIDER\n                |                      \\ # here integration test code is present\n                \\- system\n                         | PROVIDER\n                                  \\ # here system test code is present\n```\n\n----------------------------------------\n\nTITLE: Enforcing Task Ownership with Cluster Policy\nDESCRIPTION: A policy function that checks if tasks have an owner specified, raising an AirflowClusterPolicyViolation if no owner is found.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/cluster-policies.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef task_must_have_owners(task):\n    \"\"\"\n    Ensure every task has an owner.\n    \"\"\"\n    if not task.owner:\n        raise AirflowClusterPolicyViolation(\n            f\"Task {task.task_id} in DAG {task.dag_id} does not have an owner. All tasks must have an owner.\"\n        )\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom DB Connection Retrieval in Airflow Helm Chart\nDESCRIPTION: This YAML snippet demonstrates how to configure a custom command for retrieving the database connection URL in an Airflow Helm chart. It also shows how to disable the built-in secret environment variable for the database connection.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nextraEnv:\n  AIRFLOW_CONN_AIRFLOW_DB_CMD: \"/usr/local/bin/retrieve_connection_url\"\nenableBuiltInSecretEnvVars:\n  AIRFLOW_CONN_AIRFLOW_DB: false\n```\n\n----------------------------------------\n\nTITLE: Specifying Executor for a Task Decorator in Airflow Python DAG\nDESCRIPTION: Shows how to specify an executor for a task using the @task decorator in an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/index.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@task(executor=\"LocalExecutor\")\ndef hello_world():\n    print(\"hello world!\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Sender's Email Address in Airflow (INI)\nDESCRIPTION: This snippet shows how to set the sender's email address in the Airflow configuration file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[email]\nfrom_email = \"John Doe <johndoe@example.com>\"\n```\n\n----------------------------------------\n\nTITLE: Processing Pulled Messages with BashOperator\nDESCRIPTION: Examples showing how to process pulled PubSub messages using BashOperator and retrieve results from XCom.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/pubsub.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npull_messages_result_cmd = BashOperator(\n    task_id=\"pull_messages_result_cmd\",\n    bash_command=f\"echo {{{{ task_instance.xcom_pull(task_ids='pull_messages') }}}} \",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\npull_messages_result = BashOperator(\n    task_id=\"pull_messages_result\",\n    bash_command=f\"echo {{{{ task_instance.xcom_pull(task_ids='pull_task') }}}} \",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Product with Explicit ID in Google Cloud Vision Operator\nDESCRIPTION: Shows how to use CloudVisionCreateProductOperator with an explicitly specified product ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncreate_product = CloudVisionCreateProductOperator(\n    product=product,\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    product_id=PRODUCT_ID,\n    task_id=\"create_product\",\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Drill Service Health in Airflow CI (Bash)\nDESCRIPTION: This Bash snippet conditionally checks if the Drill integration is enabled, and if so, performs a network connectivity check on the Drill service using a helper (run_nc). It is typically placed within a script block that runs during Airflow's CI environment checks. Dependencies include helper functions like check_service and run_nc. The input is the INTEGRATION_DRILL environment variable; returns script status; assumes drill is running at port 8047 and repeats the check up to 50 times.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/integration_tests.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nif [[ ${INTEGRATION_DRILL} == \"true\" ]]; then\n    check_service \"drill\" \"run_nc drill 8047\" 50\nfi\n\n```\n\n----------------------------------------\n\nTITLE: Getting File via FTPS using FTPSFileTransmitOperator in Python\nDESCRIPTION: This example demonstrates how to use the Airflow `FTPSFileTransmitOperator` to securely download ('get') a file from a remote FTPS server to the local machine. An FTPS connection (`ftp_conn_id`) must be configured in Airflow, and the `remote_filepath` (source) and `local_filepath` (destination) parameters need to be provided.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ftp/docs/operators/index.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../ftp/tests/system/ftp/example_ftp.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_ftps_get]\n    :end-before: [END howto_operator_ftps_get]\n```\n\n----------------------------------------\n\nTITLE: Git-Sync Configuration for Private Repository\nDESCRIPTION: YAML configuration for setting up git-sync with a private GitHub repository, including SSH key authentication setup.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ndags:\n  gitSync:\n    enabled: true\n    repo: git@github.com:<username>/<private-repo-name>.git\n    branch: <branch-name>\n    subPath: \"\"\n    sshKeySecret: airflow-ssh-secret\nextraSecrets:\n  airflow-ssh-secret:\n    data: |\n      gitSshKey: '<base64-converted-ssh-private-key>'\n```\n\n----------------------------------------\n\nTITLE: Copying a Single File with Airflow GCSToSambaOperator (Python)\nDESCRIPTION: This Python snippet demonstrates how to copy a single file from Google Cloud Storage (GCS) to a Samba server using Airflow's GCSToSambaOperator. The operator requires configuration of GCS source bucket/path, Samba server credentials, and destination path. Templating can be used to parameterize file paths at runtime. Expected input is the fully qualified path of a GCS object and output is a file with the same name created on the Samba server.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/transfer/gcs_to_samba.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nGCSToSambaOperator(\n    task_id=\"copy_gcs_to_samba_single_file\",\n    source_bucket=\"example-source-bucket\",\n    source_path=\"folder/test_object.txt\",\n    samba_server=\"example.samba.server\",\n    share_name=\"SHARE\",\n    destination_path=\"test_object_copied.txt\",\n    samba_username=\"airflow\",\n    samba_password=\"airflow\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Pre-instantiated Tasks to Setup Context\nDESCRIPTION: Shows how to add already instantiated tasks to a setup context explicitly.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith my_teardown_task as scope:\n    scope.add_task(work_task)  # work_task was already instantiated elsewhere\n```\n\n----------------------------------------\n\nTITLE: Displaying Airbyte Provider Version 3.5.0 Changelog\nDESCRIPTION: Lists the changes made in version 3.5.0 of the Airbyte provider, including commit hashes, dates, and descriptions of changes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_29\n\nLANGUAGE: markdown\nCODE:\n```\n3.5.0\n.....\n\nLatest change: 2023-12-08\n\n==================================================================================================  ===========  ========================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ========================================================================\n`999b70178a <https://github.com/apache/airflow/commit/999b70178a1f5d891fd2c88af4831a4ba4c2cbc9>`__  2023-12-08   ``Prepare docs 1st wave of Providers December 2023 (#36112)``\n`d0918d77ee <https://github.com/apache/airflow/commit/d0918d77ee05ab08c83af6956e38584a48574590>`__  2023-12-07   ``Bump minimum Airflow version in providers to Airflow 2.6.0 (#36017)``\n`c905fe88de <https://github.com/apache/airflow/commit/c905fe88de6382cbf610b1fffa0159a7a0b5558f>`__  2023-11-25   ``Update information about links into the provider.yaml files (#35837)``\n`0b23d5601c <https://github.com/apache/airflow/commit/0b23d5601c6f833392b0ea816e651dcb13a14685>`__  2023-11-24   ``Prepare docs 2nd wave of Providers November 2023 (#35836)``\n`99534e47f3 <https://github.com/apache/airflow/commit/99534e47f330ce0efb96402629dda5b2a4f16e8f>`__  2023-11-19   ``Use reproducible builds for provider packages (#35693)``\n`99df205f42 <https://github.com/apache/airflow/commit/99df205f42a754aa67f80b5983e1d228ff23267f>`__  2023-11-16   ``Fix and reapply templates for provider documentation (#35686)``\n`1b059c57d6 <https://github.com/apache/airflow/commit/1b059c57d6d57d198463e5388138bee8a08591b1>`__  2023-11-08   ``Prepare docs 1st wave of Providers November 2023 (#35537)``\n`706878ec35 <https://github.com/apache/airflow/commit/706878ec354cf867440c367a95c85753c19e54de>`__  2023-11-04   ``Remove empty lines in generated changelog (#35436)``\n`052e26ad47 <https://github.com/apache/airflow/commit/052e26ad473a9d50f0b96456ed094f2087ee4434>`__  2023-11-04   ``Change security.rst to use includes in providers (#35435)``\n`d1c58d86de <https://github.com/apache/airflow/commit/d1c58d86de1267d9268a1efe0a0c102633c051a1>`__  2023-10-28   ``Prepare docs 3rd wave of Providers October 2023 - FIX (#35233)``\n`3592ff4046 <https://github.com/apache/airflow/commit/3592ff40465032fa041600be740ee6bc25e7c242>`__  2023-10-28   ``Prepare docs 3rd wave of Providers October 2023 (#35187)``\n`dd7ba3cae1 <https://github.com/apache/airflow/commit/dd7ba3cae139cb10d71c5ebc25fc496c67ee784e>`__  2023-10-19   ``Pre-upgrade 'ruff==0.0.292' changes in providers (#35053)``\n==================================================================================================  ===========  ========================================================================\n```\n\n----------------------------------------\n\nTITLE: Breaking Change: JSON Secret Handling in SecretsManagerBackend\nDESCRIPTION: Configuration update to handle JSON secrets in the SecretsManagerBackend. Secrets are no longer interpreted as urlencoded and raw values are used for Connection objects.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# SecretsManagerBackend now handles JSON secrets without urlencoding\nfrom airflow.providers.amazon.aws.secrets.secrets_manager import SecretsManagerBackend\n\n# Raw JSON values are used directly\nbackend = SecretsManagerBackend()\n# No more url encoding inference\n```\n\n----------------------------------------\n\nTITLE: Moving Hive CLI Parameters to Hook in Python\nDESCRIPTION: Transfers hive_cli_params from connection extras to Hook parameters for better configuration management in the Hive provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n\"Move hive_cli_params to hook parameters (#28101)\"\n```\n\n----------------------------------------\n\nTITLE: Installing SMTP Provider with Cross-Provider Dependencies using pip - Bash\nDESCRIPTION: This snippet demonstrates how to install the apache-airflow-providers-smtp Python package along with optional cross-provider dependencies (in this case, the 'common.compat' package) using pip. To execute this command, pip must be available in your environment and Python must be properly installed. Replace 'common.compat' with any other extras as needed. The command takes no additional inputs and outputs standard pip installation progress to the console.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/smtp/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-smtp[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Declaring google Extra Dependency in Apache Beam Provider Setup (Python)\nDESCRIPTION: This Python code snippet illustrates the definition of an optional dependency extra named 'google' within the setup configuration for the Apache Airflow Apache Beam provider. Symmetrically to the Google provider's extra, installing this (`pip install apache-airflow-providers-apache-beam[google]`) ensures that the `apache-airflow-providers-google` package and `apache-beam[gcp]` dependencies are installed. This mechanism, introduced alongside the Google provider's 3.0.0 release, helps manage integration and potential client library version conflicts between the two providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nextras_require = ({\"google\": [\"apache-airflow-providers-google\", \"apache-beam[gcp]\"]},)\n```\n\n----------------------------------------\n\nTITLE: Setting Docker Cache to Disabled via Environment Variable\nDESCRIPTION: Setting an environment variable to disable Docker caching for image builds. This affects all subsequent Breeze commands.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_CACHE=\"disabled\"\n```\n\n----------------------------------------\n\nTITLE: Installing Amazon Web Services Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Amazon Web Services integration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[amazon]'\n```\n\n----------------------------------------\n\nTITLE: Checking Current Executor Configuration in Airflow\nDESCRIPTION: Shows how to use the Airflow CLI to check which executor is currently set in the configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ airflow config get-value core executor\nLocalExecutor\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation - reStructuredText\nDESCRIPTION: This snippet uses the reStructuredText '.. include::' directive to incorporate additional content from a shared file path (devel-common/src/sphinx_exts/includes/security.rst) into the current documentation. This method allows maintainers to import and reuse common security information across multiple documentation sections, promoting DRY (Don't Repeat Yourself) principles. It depends on Sphinx's reStructuredText parser and requires the specified include path to exist and be accessible during the documentation build process.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cohere/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Starting Celery Flower Web UI for Apache Airflow\nDESCRIPTION: Command to start the Celery Flower web UI for monitoring Celery workers in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/celery_executor.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairflow celery flower\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow SendGrid Provider via pip\nDESCRIPTION: This command uses the pip package manager to install the `apache-airflow-providers-sendgrid` package. It should be executed in a shell environment where pip and a compatible Airflow version (>=2.9.0) are installed. This adds SendGrid integration capabilities to the Airflow environment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sendgrid/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-sendgrid\n```\n\n----------------------------------------\n\nTITLE: Enabling Example DAG Loading with Environment Variable - bash\nDESCRIPTION: This environment variable sets the Airflow core configuration to load example DAGs, which are needed for the test scripts to execute workflows in the test environment. The variable AIRFLOW__CORE__LOAD_EXAMPLES must be set to 'True' for serialization and testing of sample DAGs provided by Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__CORE__LOAD_EXAMPLES=True\n```\n\n----------------------------------------\n\nTITLE: Specifying Requirements for Asana Provider Package\nDESCRIPTION: Table showing the required PIP packages and their versions for the Asana provider package. It requires Apache Airflow 2.9.0 or higher and Asana Python library 5.0.0 or higher.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n``apache-airflow``  ``>=2.9.0``\n``asana``           ``>=5.0.0``\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow FTP Provider with Extras using pip (Bash)\nDESCRIPTION: This command demonstrates how to install the `apache-airflow-providers-ftp` package along with optional extra dependencies using pip. The example shows installing the `common.compat` extra, which might be needed for specific features requiring cross-provider compatibility. Other extras like `openlineage` can also be installed similarly.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ftp/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-ftp[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Implementing Pagerduty Notifications in Apache Airflow DAG\nDESCRIPTION: This code snippet demonstrates how to set up Pagerduty notifications for both DAG-level and task-level failure callbacks in an Apache Airflow DAG. It uses the send_pagerduty_notification function to configure the notifications with various parameters such as summary, severity, source, and dedup_key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pagerduty/docs/notifications/pagerduty_notifier_howto_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.pagerduty.notifications.pagerduty import send_pagerduty_notification\n\nwith DAG(\n    \"pagerduty_notifier\",\n    start_date=datetime(2023, 1, 1),\n    on_failure_callback=[\n        send_pagerduty_notification(\n            summary=\"The dag {{ dag.dag_id }} failed\",\n            severity=\"critical\",\n            source=\"airflow dag_id: {{dag.dag_id}}\",\n            dedup_key=\"{{dag.dag_id}}-{{ti.task_id}}\",\n            group=\"{{dag.dag_id}}\",\n            component=\"airflow\",\n            class_type=\"Prod Data Pipeline\",\n        )\n    ],\n):\n    BashOperator(\n        task_id=\"mytask\",\n        bash_command=\"fail\",\n        on_failure_callback=[\n            send_pagerduty_notification(\n                summary=\"The task {{ ti.task_id }} failed\",\n                severity=\"critical\",\n                source=\"airflow dag_id: {{dag.dag_id}}\",\n                dedup_key=\"{{dag.dag_id}}-{{ti.task_id}}\",\n                group=\"{{dag.dag_id}}\",\n                component=\"airflow\",\n                class_type=\"Prod Data Pipeline\",\n            )\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing EventsTimetable in Airflow DAG\nDESCRIPTION: Example of using EventsTimetable with a list of specific datetime events for scheduling a DAG. This is useful for arbitrary but predictable schedules, such as sporting events or planned campaigns.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timetable.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.timetables.events import EventsTimetable\n\n\n@dag(\n    schedule=EventsTimetable(\n        event_dates=[\n            pendulum.datetime(2022, 4, 5, 8, 27, tz=\"America/Chicago\"),\n            pendulum.datetime(2022, 4, 17, 8, 27, tz=\"America/Chicago\"),\n            pendulum.datetime(2022, 4, 22, 20, 50, tz=\"America/Chicago\"),\n        ],\n        description=\"My Team's Baseball Games\",\n        restrict_to_events=False,\n    ),\n    ...,\n)\ndef example_dag():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Database Migration Timeline Table\nDESCRIPTION: Tabular representation of database migrations showing commit hashes, version numbers, and migration descriptions. The table tracks changes from version 2.8.0 to 3.0.0, including schema modifications, column additions, and structural changes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/migrations-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``fb2d4922cd79``        | ``5a5d66100783`` | ``3.0.0``         | Tweak AssetAliasModel to match AssetModel after AIP-76.      |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``5a5d66100783``        | ``c3389cd7793f`` | ``3.0.0``         | Add AssetActive to track orphaning instead of a flag.        |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n[...additional rows omitted for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Example InfluxDB Connection Extra Parameters in JSON\nDESCRIPTION: This JSON object demonstrates the structure for the 'Extra' field when configuring an InfluxDB connection in Airflow. It includes example values for 'token' (authentication credential), 'org' (InfluxDB organization identifier), and 'timeout' (connection timeout in milliseconds). These parameters are specified within a JSON dictionary.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/influxdb/docs/connections/influxdb.rst#2025-04-22_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"token\": \"343434343423234234234343434\",\n  \"org\": \"Test\",\n  \"timeout\": 10000\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Default S3 Bucket Name in Connection Extra Field (JSON)\nDESCRIPTION: This JSON snippet for the 'Extra' field configures a default S3 bucket name ('awesome-bucket') specifically for the S3 service within an Airflow AWS connection. This default bucket will be used by the S3Hook methods unless explicitly overridden by the 'bucket_name' parameter in the hook method call.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"service_config\": {\n    \"s3\": {\n      \"bucket_name\": \"awesome-bucket\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Task Groups and Tasks in Airflow DAG using Python\nDESCRIPTION: This snippet defines task groups and individual tasks within the DAG using various operators such as EmptyOperator, BashOperator, and PythonOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/.latest-doc-only-change.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n    start = EmptyOperator(task_id=\"start\")\n\n    # [task group definitions...]\n\n    end = EmptyOperator(task_id=\"end\")\n\n    chain(start, [section_1, section_2, section_3], end)\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Configuration File Path\nDESCRIPTION: Defines the path to the Airflow configuration file, allowing users to specify a non-default location.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/cli-and-env-variables-ref.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_CONFIG\n```\n\n----------------------------------------\n\nTITLE: Removing Directory from Dataform Workspace in Python\nDESCRIPTION: This snippet shows how to remove a directory from a workspace using the DataformRemoveDirectoryOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_remove_directory]\n# [END howto_operator_remove_directory]\n```\n\n----------------------------------------\n\nTITLE: Debugging Pre-commit Ruff Check with Verbose Output in Bash\nDESCRIPTION: This command runs the Ruff pre-commit check with verbose output, showing the commands being executed.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nVERBOSE=\"true\" pre-commit run --verbose ruff\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis for Shared Rate Limiting (Python)\nDESCRIPTION: Configures Flask-Limiter to use Redis as a shared storage backend for rate limiting authentication requests across multiple webserver processes. Add this line to `webserver_config.py`, replacing `redis_host` with the actual hostname or IP of your Redis instance. This requires the `redis` Python package to be installed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/security.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nRATELIMIT_STORAGE_URI = \"redis://redis_host:6379/0\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SQLAlchemy Logging in Airflow\nDESCRIPTION: Enables detailed query logging in SQLAlchemy by setting the 'echo' parameter to True in the engine configuration. This is done using the database__sql_alchemy_engine_args configuration parameter in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-up-database.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\necho=True\n```\n\n----------------------------------------\n\nTITLE: Fixing Azure Container Instance Operator in Python\nDESCRIPTION: Fixes the remove_on_error functionality in the AzureContainerInstanceOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/changelog.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nFix AzureContainerInstanceOperator remove_on_error (#35212)\n```\n\n----------------------------------------\n\nTITLE: Parameterized YDB Query\nDESCRIPTION: Example of using parameters in YDBExecuteQueryOperator to filter records by date range.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/docs/operators/ydb_operator_howto_guide.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nget_birth_date = YDBExecuteQueryOperator(\n    task_id=\"get_birth_date\",\n    sql=\"sql/birth_date.sql\",\n    params={\"begin_date\": \"2020-01-01\", \"end_date\": \"2020-12-31\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Retrying a Google Cloud Build Using Airflow Operator - Python\nDESCRIPTION: Provides an Airflow task sample using CloudBuildRetryBuildOperator to retry a previously requested Google Cloud Build job. Relies on Airflow Google provider, with build ID, project ID, and credentials as critical parameters. The output is returned to XCom for subsequent task utilization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_build.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nretry_build = CloudBuildRetryBuildOperator(\n    task_id=\"retry_build\",\n    project_id=PROJECT_ID,\n    id=build_result[\"id\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Kerberos Ticket Renewer in Bash\nDESCRIPTION: Commands to start the Kerberos ticket renewer process in Airflow, including standard and one-time modes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/kerberos.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# run ticket renewer\nairflow kerberos\n\n# For one time mode:\nairflow kerberos --one-time\n```\n\n----------------------------------------\n\nTITLE: Identifying the Default Airflow gRPC Connection ID\nDESCRIPTION: Specifies `grpc_default` as the default connection ID used by the `GrpcHook` when interacting with gRPC services if no specific connection ID is provided in the task or operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/connections/grpc.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\ngrpc_default\n```\n\n----------------------------------------\n\nTITLE: Installing Google Cloud Airflow Dependencies\nDESCRIPTION: Command to install the required Google Cloud dependencies for Airflow via pip\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/looker.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[google]'\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Key Vault Backend in Airflow\nDESCRIPTION: This snippet shows the configuration settings for enabling Azure Key Vault as a secrets backend in Airflow's airflow.cfg file. It specifies the backend class and the necessary parameters including connections prefix, variables prefix, and vault URL.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/secrets-backends/azure-key-vault.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.microsoft.azure.secrets.key_vault.AzureKeyVaultBackend\nbackend_kwargs = {\"connections_prefix\": \"airflow-connections\", \"variables_prefix\": \"airflow-variables\", \"vault_url\": \"https://example-akv-resource-name.vault.azure.net/\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Impala Provider Package\nDESCRIPTION: Command to install the Apache Impala provider package for Airflow via pip, including optional SQL dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/impala/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-impala[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Creating RDS Event Subscription\nDESCRIPTION: Creates an Amazon RDS event subscription using RDSCreateEventSubscriptionOperator. Requires an SNS topic ARN and only works with unencrypted topics.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncreate_event_subscription = RDSCreateEventSubscriptionOperator(\n    task_id=\"create_event_subscription\",\n    subscription_name=EVENT_SUBSCRIPTION_NAME,\n    sns_topic_arn=SNS_TOPIC_ARN,\n    source_type=\"db-instance\",\n    source_ids=[DB_INSTANCE_NAME],\n    event_categories=[\"availability\", \"backup\", \"maintenance\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Installing YDB Provider with SQL Dependencies\nDESCRIPTION: Command to install the YDB provider package along with common SQL dependencies needed for full functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-ydb[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: Sphinx directive to include security-related documentation from a common development source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pig/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Building Production Image with Registry Cache\nDESCRIPTION: Command for building a production image using cache pulled from the registry. This is useful when you want to leverage pre-built layers from the registry.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nbreeze prod-image build --python 3.9 --docker-cache registry\n```\n\n----------------------------------------\n\nTITLE: Installing SQLite Extras for Apache Airflow\nDESCRIPTION: Command to install SQLite hooks and operators for Apache Airflow. This enables integration with SQLite databases and is preinstalled with Airflow by default.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_70\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[sqlite]'\n```\n\n----------------------------------------\n\nTITLE: Template Fields for CloudVideoIntelligenceDetectVideoShotsOperator (Python)\nDESCRIPTION: Lists the template fields ('input_uri', 'result_path') for CloudVideoIntelligenceDetectVideoShotsOperator, allowing Airflow users to parameterize important values at runtime. This facilitates efficient pipeline design where video URIs and output paths may change across environments. Airflow's templating functionality and Google provider modules are required.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/video_intelligence.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\"input_uri\", \"result_path\")\n```\n\n----------------------------------------\n\nTITLE: Fixing Parameter Rendering in PapermillOperator\nDESCRIPTION: Bug Fix (Version 3.1.1): Corrects an issue related to rendering parameters within the PapermillOperator, referencing pull request #28979. This ensures parameters are correctly passed to the Papermill execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Fix rendering parameters in PapermillOperator (#28979)``\n```\n\n----------------------------------------\n\nTITLE: Setting Basic Authentication for Celery Flower via Command Line\nDESCRIPTION: Demonstrates how to set up basic authentication for Flower using command line arguments with username:password pairs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/flower.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairflow celery flower --basic-auth=user1:password1,user2:password2\n```\n\n----------------------------------------\n\nTITLE: Installing Slack Provider Package with Dependencies\nDESCRIPTION: Command to install the Slack provider package with common compatibility dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-slack[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Importing BaseSecretsBackend in Python for Custom Secrets Backend\nDESCRIPTION: Demonstrates the import of BaseSecretsBackend class, which is used to create custom secrets backends for retrieving Connections and Variables in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/public-airflow-interface.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.secrets.base_secrets import BaseSecretsBackend\n```\n\n----------------------------------------\n\nTITLE: Templating Fields for GCE Instance Group Manager Template Update in Python\nDESCRIPTION: Defines the template fields available for the ComputeEngineInstanceGroupUpdateManagerTemplateOperator, which allow for runtime parameter substitution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"project_id\",\n    \"zone\",\n    \"resource_id\",\n    \"source_template\",\n    \"destination_template\",\n    \"request_id\",\n    \"gcp_conn_id\",\n    \"api_version\",\n    \"impersonation_chain\",\n)\n```\n\n----------------------------------------\n\nTITLE: Moving Files from GCS to Google Drive with Source Deletion\nDESCRIPTION: This snippet shows how to move files from Google Cloud Storage to Google Drive by setting the move_object parameter to True. This option will delete the source files from the GCS bucket after they are successfully copied to Google Drive.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gdrive.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmove_gcs_to_gdrive = GCSToGoogleDriveOperator(\n    task_id=\"gcs_to_drive_move\",\n    source_bucket=BUCKET_NAME,\n    source_object=GCS_FILE_PATH,\n    destination_object=DRIVE_FILE_PATH,\n    move_object=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Monitoring BigQuery Data Transfer Run Status in Python\nDESCRIPTION: This snippet shows how to monitor the status of a BigQuery Data Transfer run using the BigQueryDataTransferServiceTransferRunSensor in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery_dts.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwait_for_transfer = BigQueryDataTransferServiceTransferRunSensor(\n    task_id=\"wait_for_transfer\",\n    transfer_config_id=\"{{ task_instance.xcom_pull('create_transfer')['transfer_config_id'] }}\",\n    run_id=\"{{ task_instance.xcom_pull('start_transfer')[0] }}\",\n    expected_statuses={BigQueryDataTransferServiceTransferStatus.SUCCEEDED},\n    project_id=PROJECT_ID,\n    location=LOCATION,\n)\n```\n\n----------------------------------------\n\nTITLE: Using GithubSensor for Custom Tag Monitoring in Airflow\nDESCRIPTION: Example of using the more generic GithubSensor to implement tag monitoring. This demonstrates how to directly use GithubSensor with a custom check_function to achieve similar functionality to GithubTagSensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/github/docs/operators/index.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef check_tag(client, tag_name, repository_name):\n    try:\n        repo = client.get_repo(repository_name)\n        tags = [tag.name for tag in repo.get_tags()]\n        return tag_name in tags\n    except Exception as e:\n        return False\n\n\ntag_sensor = GithubSensor(\n    task_id=\"tag_sensor\",\n    github_conn_id=\"github_default\",\n    method_name=\"check_tag\",\n    method_params={\"tag_name\": \"v1.0\", \"repository_name\": \"apache/airflow\"},\n    check_function=check_tag,\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Change Types in Apache Airflow Configuration\nDESCRIPTION: A checklist of different types of changes made to the Apache Airflow project, including config changes and behavior changes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41975.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* Types of change\n\n  * [ ] Dag changes\n  * [x] Config changes\n  * [ ] API changes\n  * [ ] CLI changes\n  * [x] Behaviour changes\n  * [ ] Plugin changes\n  * [ ] Dependency changes\n  * [ ] Code interface changes\n```\n\n----------------------------------------\n\nTITLE: Implementing Unlimited Parallelism in Local Executor\nDESCRIPTION: Strategy for spawning unlimited processes when parallelism is set to 0. Each task gets its own process via LocalWorker class that terminates after task completion.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/local.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.parallelism == 0\n```\n\n----------------------------------------\n\nTITLE: Executing Specific AWS DataSync Task Using Airflow DataSyncOperator - Python\nDESCRIPTION: This snippet demonstrates how to execute a specific AWS DataSync task by passing the Target TaskArn to the Airflow DataSyncOperator. It depends on Airflow's AWS provider and requires pre-existing AWS credentials/configuration as well as the TaskArn for the DataSync task. Key parameter: task_arn (the unique Amazon Resource Name of the DataSync task to execute); expected output: the DataSync TaskExecution is monitored until completion, reporting status to the Airflow task log.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/datasync.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nfrom airflow.providers.amazon.aws.operators.datasync import DataSyncOperator\n\ndatasync_execute_task = DataSyncOperator(\n    task_id='datasync_execute_task',\n    task_arn='arn:aws:datasync:us-east-1:123456789012:task/task-123abcd',  # specify your TaskArn\n)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add support for non-RSA type key for SFTP hook\nDESCRIPTION: Associated with commit 50e334df32, this message describes the addition of support for SSH key types other than RSA (like ECDSA, Ed25519) within the SFTP hook. This enhancement is linked to issue #16314.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\n``Add support for non-RSA type key for SFTP hook (#16314)``\n```\n\n----------------------------------------\n\nTITLE: Mapping Airflow Operator and Transfer File Locations\nDESCRIPTION: This code snippet provides a mapping of file locations for various Apache Airflow operators and transfer operations. It shows the original file paths and their new corresponding locations, primarily for AWS-related services and data transfer operations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/redirects.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\noperators/athena.rst operators/athena/athena_boto.rst\noperators/s3_to_redshift.rst transfer/s3_to_redshift.rst\noperators/google_api_to_s3_transfer.rst transfer/google_api_to_s3.rst\noperators/imap_attachment_to_s3.rst transfer/imap_attachment_to_s3.rst\noperators/salesforce_to_s3.rst transfer/salesforce_to_s3.rst\noperators/sqs_publish.rst operators/sqs.rst\noperators/emr.rst operators/emr/emr.rst\noperators/emr_eks.rst operators/emr/emr_eks.rst\noperators/emr_serverless.rst operators/emr/emr_serverless.rst\noperators/redshift_sql.rst operators/redshift/redshift_sql.rst\noperators/redshift_data.rst operators/redshift/redshift_data.rst\noperators/redshift_cluster.rst operators/redshift/redshift_cluster.rst\noperators/s3.rst operators/s3/s3.rst\noperators/glacier.rst operators/s3/glacier.rst\noperators/transfer/dynamodb_to_s3.rst transfer/dynamodb_to_s3.rst\noperators/transfer/ftp_to_s3.rst transfer/ftp_to_s3.rst\noperators/transfer/gcs_to_s3.rst transfer/gcs_to_s3.rst\noperators/transfer/glacier_to_gcs.rst transfer/glacier_to_gcs.rst\noperators/transfer/google_api_to_s3.rst transfer/google_api_to_s3.rst\noperators/transfer/hive_to_dynamodb.rst transfer/hive_to_dynamodb.rst\noperators/transfer/imap_attachment_to_s3.rst transfer/imap_attachment_to_s3.rst\noperators/transfer/local_to_s3.rst transfer/local_to_s3.rst\noperators/transfer/mongo_to_s3.rst transfer/mongo_to_s3.rst\noperators/transfer/redshift_to_s3.rst transfer/redshift_to_s3.rst\noperators/transfer/s3_to_ftp.rst transfer/s3_to_ftp.rst\noperators/transfer/s3_to_redshift.rst transfer/s3_to_redshift.rst\noperators/transfer/s3_to_sftp.rst transfer/s3_to_sftp.rst\noperators/transfer/s3_to_sql.rst transfer/s3_to_sql.rst\noperators/transfer/salesforce_to_s3.rst transfer/salesforce_to_s3.rst\noperators/transfer/sftp_to_s3.rst transfer/sftp_to_s3.rst\noperators/transfer/sql_to_s3.rst transfer/sql_to_s3.rst\n```\n\n----------------------------------------\n\nTITLE: Protocol Operators Reference Directive\nDESCRIPTION: RST directive to generate documentation for protocol-related operators and hooks\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/operators-and-hooks-ref/protocol.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. operators-hooks-ref::\n   :tags: protocol\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Version 3.2.0 Bug Fix Entry\nDESCRIPTION: RST changelog entry documenting a fix for MsSqlHook.get_uri pymssql driver scheme\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n* ``Fix MsSqlHook.get_uri: pymssql driver to scheme (25092) (#25185)``\n```\n\n----------------------------------------\n\nTITLE: Git Remote Configuration for Apache Airflow\nDESCRIPTION: Commands to add Apache Airflow repository as a remote in your local git setup using either SSH or HTTPS.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/10_working_with_git.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add apache git@github.com:apache/airflow.git\ngit remote add apache https://github.com/apache/airflow.git\n```\n\n----------------------------------------\n\nTITLE: Installing and Running ruff Linter for Airflow\nDESCRIPTION: These snippets show how to install the ruff linter and run it to check Airflow DAGs for potential issues, focusing on Airflow-specific rules.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install \"ruff>=0.9.5\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nruff check dags/ --select AIR301,AIR302,AIR303\n```\n\n----------------------------------------\n\nTITLE: Creating an AWS CloudFormation Stack with Airflow Operator - Python\nDESCRIPTION: This Python code snippet demonstrates how to use the CloudFormationCreateStackOperator in Apache Airflow to create an AWS CloudFormation stack. It requires the Airflow AWS provider package, proper AWS credentials, and the definition of parameters such as 'stack_name' and 'cloudformation_parameters'. The main input is the template and stack configuration, with outputs including the successful creation of the specified stack or relevant error messages upon failure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/cloudformation.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncreate_stack = CloudFormationCreateStackOperator(\n    task_id=\"create_aws_stack\",\n    stack_name=STACK_NAME,\n    cloudformation_parameters=my_stack_params,\n    aws_conn_id=\"aws_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Getting BigQuery Dataset Details - Python\nDESCRIPTION: Example of using BigQueryGetDatasetOperator to retrieve details of an existing dataset.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nget_dataset_task = BigQueryGetDatasetOperator(\n    task_id=\"get_dataset\", dataset_id=DATASET_NAME\n)\n```\n\n----------------------------------------\n\nTITLE: Templated Fields for CloudTextToSpeechSynthesizeOperator (Python)\nDESCRIPTION: Lists the fields within the CloudTextToSpeechSynthesizeOperator that support Jinja templating. This allows dynamic values to be injected into these parameters at runtime using Airflow's templating engine.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/text_to_speech.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields: Sequence[str] = (\n    \"input_data\",\n    \"voice_config\",\n    \"audio_config\",\n    \"target_bucket_name\",\n    \"target_filename\",\n    \"project_id\",\n    \"gcp_conn_id\",\n    \"retry\",\n    \"timeout\",\n    \"impersonation_chain\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring IPv6 StatsD for Airflow Metrics\nDESCRIPTION: Configuration for setting up StatsD with IPv6 support in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nstatsd:\n  enabled: true\nconfig:\n  metrics:  # or 'scheduler' for Airflow 1\n    statsd_on: 'True'\n    statsd_host: ...\n    statsd_ipv6: 'True'\n    statsd_port: ...\n    statsd_prefix: airflow\n```\n\n----------------------------------------\n\nTITLE: Analyzing Entities with Google Cloud Natural Language in Apache Airflow\nDESCRIPTION: This snippet demonstrates how to use the CloudNaturalLanguageAnalyzeEntitiesOperator to perform entity analysis on a given text.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/natural_language.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nanalyze_entities = CloudNaturalLanguageAnalyzeEntitiesOperator(\n    task_id=\"analyze_entities\",\n    document=document,\n)\n```\n\n----------------------------------------\n\nTITLE: Fetching Records from MSSQL Table using Airflow SQLExecuteQueryOperator (Reference)\nDESCRIPTION: References a Python snippet demonstrating fetching records from an MSSQL table using `SQLExecuteQueryOperator`. It executes a `SELECT` query, and the results are typically pushed to XComs by default.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/operators.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../tests/system/microsoft/mssql/example_mssql.py\n    :language: python\n    :start-after: [START mssql_operator_howto_guide_get_all_countries]\n    :end-before: [END mssql_operator_howto_guide_get_all_countries]\n```\n\n----------------------------------------\n\nTITLE: Deleting a Dataset with Vertex AI in Apache Airflow\nDESCRIPTION: This snippet shows how to use the DeleteDatasetOperator from Vertex AI to delete a specific dataset in Apache Airflow. It includes the dataset ID, project, and region for the deletion operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/automl.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndelete_dataset = DeleteDatasetOperator(\n    task_id=\"delete_dataset\",\n    dataset_id=dataset_id,\n    project_id=PROJECT_ID,\n    region=REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Dataplex Aspect Type using Airflow Python\nDESCRIPTION: This snippet shows how to update an existing Aspect Type in a specific location within Google Cloud Dataplex Catalog using the `DataplexCatalogUpdateAspectTypeOperator` in an Airflow DAG. It requires the updated configuration and references an external example file for the implementation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_51\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_update_aspect_type]\n#     :end-before: [END howto_operator_dataplex_catalog_update_aspect_type]\n\n# This example uses DataplexCatalogUpdateAspectTypeOperator.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Running Breeze Tests with Environment Variables in Airflow\nDESCRIPTION: Shows how to use the Breeze testing command to run Airflow core tests while keeping environment variables. This allows for testing with specific environmental configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_45\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-tests --keep-env-variables\n```\n\n----------------------------------------\n\nTITLE: Creating Admin User in Airflow\nDESCRIPTION: Command to create an admin user in Airflow with specified credentials\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairflow users create --role Admin --username admin --password admin --email admin@example.com --firstname foo --lastname bar\n```\n\n----------------------------------------\n\nTITLE: Inspecting and Deleting Tables in SQL for Airflow Database\nDESCRIPTION: SQL commands to inspect and delete tables in the Airflow database. These are used to manage incompatible data tables after a migration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading.rst#2025-04-22_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM <table>;\n```\n\nLANGUAGE: sql\nCODE:\n```\nDROP TABLE <table>;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Airflow Configuration Value in Bash\nDESCRIPTION: This command shows how to retrieve a specific configuration value (in this case, the executor) using the Airflow CLI.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-config.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ airflow config get-value core executor\nLocalExecutor\n```\n\n----------------------------------------\n\nTITLE: Installing Atlassian Jira Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Atlassian Jira provider package for Apache Airflow using pip. This package allows integration with Atlassian Jira in Airflow workflows.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/atlassian/jira/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-atlassian-jira\n```\n\n----------------------------------------\n\nTITLE: Using Custom JSON Library for DAG Serialization\nDESCRIPTION: Example of configuring a custom JSON library like ujson instead of the standard json library in the local Airflow settings file (airflow_local_settings.py).\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/dag-serialization.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ujson\n\njson = ujson\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Documentation in RST\nDESCRIPTION: Sphinx documentation directive to include external RST file containing instructions for installing Airflow providers from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Templating Text Detection Field - Google Cloud Vision Airflow Operator - Python\nDESCRIPTION: This code defines the template fields for text detection in Airflow's CloudVisionDetectTextOperator, specifying which parameters can be templated and thus dynamically set per DAG run. The fields pertain to the specifics of the image, GCP project/location, and retry policy. This snippet ensures parameterization and flexibility for executing the operator on varying datasets.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"project_id\",\n    \"location\",\n    \"image\",\n    \"retry\",\n)\n```\n\n----------------------------------------\n\nTITLE: Basic pnpm Commands for Airflow UI Development\nDESCRIPTION: Essential pnpm commands for managing dependencies, running the development server, building production files, and running tests/linting for the Airflow UI.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/15_node_environment_setup.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# install dependencies\npnpm install\n\n# Run vite dev server for local development.\n# The dev server will run on a different port than Airflow. To use the UI, access it through wherever your Airflow webserver is running, usually 8080 or 28080.\n# Trying to use the UI from the Vite port (5173) will lead to auth errors.\npnpm dev\n\n# Generate production build files will be at airflow/ui/dist\npnpm build\n\n# Format code in .ts, .tsx, .json, .css, .html files\npnpm format\n\n# Check JS/TS code in .ts, .tsx, .html files and report any errors/warnings\npnpm lint\n\n# Check JS/TS code in .ts, .tsx, .html files and report any errors/warnings and fix them if possible\npnpm lint:fix\n\n# Run tests for all .test.ts, test.tsx files\npnpm test\n\n# Run coverage\npnpm coverage\n\n# Generate queries and types from the REST API OpenAPI spec\npnpm codegen\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Spark Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Spark integration, enabling all Spark related operators and hooks.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[apache-spark]'\n```\n\n----------------------------------------\n\nTITLE: Reference Image Creation with Explicit ID\nDESCRIPTION: Creates a ReferenceImage with explicitly specified product_set_id\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_vision_reference_image_create_2]\\n[END howto_operator_vision_reference_image_create_2]\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Transport in airflow.cfg for OpenLineage\nDESCRIPTION: Configuration block for airflow.cfg that defines the HTTP transport settings for OpenLineage, including the target URL and endpoint for sending lineage events.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Executor in Airflow INI File\nDESCRIPTION: Demonstrates how to configure a custom or third-party executor by providing the module path of the executor Python class.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nexecutor = my.custom.executor.module.ExecutorClass\n```\n\n----------------------------------------\n\nTITLE: Setting Kerberos Sidecar Resources in Airflow Worker Container\nDESCRIPTION: Example configuration showing how to set CPU and memory limits and requests for the Kerberos sidecar container in Airflow worker pods using Helm values.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/setting-resources-for-containers.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nworkers:\n  kerberosSidecar:\n    resources:\n      limits:\n        cpu: 200m\n        memory: 256Mi\n      requests:\n        cpu: 100m\n        memory: 128Mi\n```\n\n----------------------------------------\n\nTITLE: Running Full Airflow Test Suite in Parallel or Sequentially - Bash\nDESCRIPTION: This snippet shows how to set the number of parallel jobs for Airflow test execution by exporting the MAX_PARALLEL_TEST_JOBS environment variable before invoking the test runner script. Adjusting the variable allows running tests in parallel or sequentially. Inputs include the environment variable and the test runner script; output is parallelized or serialized test execution. Requires Bash, appropriate test scripts, and all Airflow test dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nMAX_PARALLEL_TEST_JOBS=\"1\" ./scripts/ci/testing/ci_run_airflow_testing.sh\n```\n\n----------------------------------------\n\nTITLE: Executing Bash Script without Jinja Templating using @task.bash in Python\nDESCRIPTION: Demonstrates how to execute an external Bash script using the `@task.bash` decorator while preventing Airflow from attempting Jinja templating. This is achieved by adding a space after the script path in the returned string.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/bash.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@task.bash\ndef run_command_from_script() -> str:\n    return \"$AIRFLOW_HOME/scripts/example.sh \"\n\n\nrun_script = run_command_from_script()\n```\n\n----------------------------------------\n\nTITLE: Describing AWS DMS Replication Tasks using DmsDescribeTasksOperator in Python\nDESCRIPTION: This snippet illustrates using the DmsDescribeTasksOperator to retrieve details about one or more AWS DMS replication tasks. It takes filter parameters to specify which tasks to describe and returns information about their status, configuration, and progress. Depends on the airflow.providers.amazon.aws.operators.dms module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dms.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Example using DmsDescribeTasksOperator\n# from airflow.providers.amazon.aws.operators.dms import DmsDescribeTasksOperator\n\n# describe_tasks = DmsDescribeTasksOperator(\n#     task_id='describe_dms_tasks',\n#     describe_tasks_kwargs={\n#         'Filters': [\n#             {'Name': 'replication-task-arn', 'Values': ['arn:aws:dms:us-east-1:123456789012:task:ABCDEF123GHIJKL']}\n#         ]\n#     }\n#     # ... other parameters\n# )\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: Sphinx documentation directive to include external security documentation from a common development path.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Deleting Dataplex Aspect Type using Airflow Python\nDESCRIPTION: This snippet demonstrates how to delete an existing Aspect Type from a specific location in Google Cloud Dataplex Catalog using the `DataplexCatalogDeleteAspectTypeOperator` within an Airflow DAG. It references an external example file for the specific implementation details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_48\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_delete_aspect_type]\n#     :end-before: [END howto_operator_dataplex_catalog_delete_aspect_type]\n\n# This example uses DataplexCatalogDeleteAspectTypeOperator.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Setting Oracle Connection as Environment Variable in Bash\nDESCRIPTION: Example of setting an Oracle connection string as an environment variable in Bash, including authentication credentials and extra parameters encoded in the URI format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/connections/oracle.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_ORACLE_DEFAULT='oracle://oracle_user:XXXXXXXXXXXX@1.1.1.1:1521?encoding=UTF-8&nencoding=UTF-8&threaded=False&events=False&mode=sysdba&purity=new'\n```\n\n----------------------------------------\n\nTITLE: Processing JSON Output with JQ\nDESCRIPTION: Example of formatting Airflow CLI output as JSON and processing it with jq\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nairflow tasks states-for-dag-run example_complex 2020-11-13T00:00:00+00:00 --output json | jq \".[] | {sd: .start_date, ed: .end_date}\"\n```\n\n----------------------------------------\n\nTITLE: Creating MSSQL Table with Inline SQL using Airflow SQLExecuteQueryOperator (Reference)\nDESCRIPTION: References an Airflow DAG Python code snippet demonstrating the use of `SQLExecuteQueryOperator` to create a table in MSSQL. The SQL `CREATE TABLE` statement is intended to be provided directly within the `sql` parameter. Requires an MSSQL connection ID (`conn_id`) configured in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../tests/system/microsoft/mssql/example_mssql.py\n    :language: python\n    :start-after: [START howto_operator_mssql]\n    :end-before: [END howto_operator_mssql]\n```\n\n----------------------------------------\n\nTITLE: Simplify airflow_version Imports (v1.2.1)\nDESCRIPTION: Refactors the import mechanism for 'airflow_version' to simplify it, referenced by pull request #39497.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\nSimplify 'airflow_version' imports (#39497)\n```\n\n----------------------------------------\n\nTITLE: Importing and Using Pandas in Python\nDESCRIPTION: Consolidates import and usage of pandas library.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Drill Provider with SQL Dependencies in Python\nDESCRIPTION: Command to install the Apache Drill provider package for Apache Airflow with additional SQL dependencies using pip. This includes the common.sql extra.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-drill[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Connection\nDESCRIPTION: Commands for installing MySQL dependencies in the Airflow virtual environment for database connectivity.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_pycharm.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ pyenv activate airflow-env\n$ pip install PyMySQL\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Rename columns and ensure Row.fields retrieval\nDESCRIPTION: This commit message (hash c0f7601391, dated 2024-01-23) details a change where database result columns were renamed to be valid namedtuple attributes and ensures that `Row.fields` are retrieved as a tuple (issue #36949). This likely impacts database interaction hooks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n``Rename columns to valid namedtuple attributes + ensure Row.fields are retrieved as tuple (#36949)``\n```\n\n----------------------------------------\n\nTITLE: Preparing Airflow CTL Distributions\nDESCRIPTION: Command to prepare Airflow CTL distributions in both sdist and wheel formats.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-airflow-ctl-distributions\n```\n\n----------------------------------------\n\nTITLE: Installing Celery Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Celery provider package for Apache Airflow, which is required to use the Celery Executor as of Airflow 2.7.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/celery_executor.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[celery]'\n```\n\n----------------------------------------\n\nTITLE: Adding DatabricksNotebookOperator\nDESCRIPTION: New feature introducing the DatabricksNotebookOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"Add DatabricksNotebookOperator (#39178)\"\n```\n\n----------------------------------------\n\nTITLE: KinD Cluster Deletion Output\nDESCRIPTION: Example output from the cluster deletion command showing the process and confirmation of successful deletion.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_27\n\nLANGUAGE: text\nCODE:\n```\nDeleting KinD cluster airflow-python-3.9-v1.24.2!\nDeleting cluster \"airflow-python-3.9-v1.24.2\" ...\nKinD cluster airflow-python-3.9-v1.24.2 deleted!\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow with Breeze\nDESCRIPTION: Command to start Airflow using the Breeze development environment\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_gitpod.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbreeze start-airflow\n```\n\n----------------------------------------\n\nTITLE: Installing Samba Provider with Google Extra via pip (Bash)\nDESCRIPTION: This command installs the `apache-airflow-providers-samba` package along with optional dependencies required for Google Cloud integration features, such as the `GCSToSambaOperator`. This requires installing the `apache-airflow-providers-google` package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-samba[google]\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Prerequisites on Ubuntu\nDESCRIPTION: Commands to install required packages and set up Docker repository on Ubuntu systems.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\n\nsudo apt-get install \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n\nsudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Providers Apache Pinot Package via pip\nDESCRIPTION: Command to install the Apache Airflow providers package for Apache Pinot using pip. This package can be installed on top of an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-pinot\n```\n\n----------------------------------------\n\nTITLE: Exporting Airflow SMTP Connection URI via Environment Variable - Bash\nDESCRIPTION: Demonstrates how to set the AIRFLOW_CONN_SMTP_DEFAULT environment variable with the SMTP connection URI in Bash. Credentials, hostname, and port must be properly URL-encoded as required. The variable can be exported in shell environments to configure Airflow to use a custom SMTP connection for email notifications and integrations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/smtp/docs/connections/smtp.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SMTP_DEFAULT='smtp://username:password@smtp.sendgrid.net:587'\n```\n\n----------------------------------------\n\nTITLE: Generating Kubernetes Pod YAML for Airflow DAGs (Shell)\nDESCRIPTION: This shell command uses the Airflow CLI to generate the Kubernetes pod manifest (in YAML format) that the KubernetesExecutor would create for tasks within a DAG. This is useful for troubleshooting pod configurations, environment variables, volumes, and commands before or during execution on a Kubernetes cluster. It requires the Airflow Kubernetes provider to be installed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/kubernetes_executor.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nairflow kubernetes generate-dag-yaml\n```\n\n----------------------------------------\n\nTITLE: Loading Test DAGs in Airflow Unit Tests\nDESCRIPTION: Shows how to load a DAG from the test DAGs folder using DagBag in unit test mode. This approach only works when test_mode is enabled, otherwise the standard Airflow DAGS_FOLDER takes precedence.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/tests/unit/dags/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndagbag = DagBag()\ndag = dagbag.get_dag(dag_id)\n```\n\n----------------------------------------\n\nTITLE: Adding PyPI Package with uv to Airflow Image (Dockerfile)\nDESCRIPTION: Experimental Dockerfile example demonstrating how to install a Python package (lxml) from PyPI using the `uv` package installer instead of `pip`. `uv` is known for its speed but is relatively new.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_13\n\nLANGUAGE: Dockerfile\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/add-pypi-packages-uv/Dockerfile\n    :language: Dockerfile\n    :start-after: [START Dockerfile]\n    :end-before: [END Dockerfile]\n```\n\n----------------------------------------\n\nTITLE: Migrating Secret Backend Methods\nDESCRIPTION: Migration path for deprecated secret backend methods showing the transition from old methods to new methods in the BaseSecretsBackend class. The get_conn_uri method should be replaced with get_conn_value, and get_connections should be replaced with get_connection.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41642.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nairflow.secrets.base_secrets.BaseSecretsBackend.get_conn_uri  airflow.secrets.base_secrets.BaseSecretsBackend.get_conn_value\n```\n\nLANGUAGE: python\nCODE:\n```\nairflow.secrets.base_secrets.BaseSecretsBackend.get_connections  airflow.secrets.base_secrets.BaseSecretsBackend.get_connection\n```\n\n----------------------------------------\n\nTITLE: Storing Variables in JSON Format\nDESCRIPTION: This snippet shows how to store variables in a JSON file. The file contains key-value pairs representing variable names and their values.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/secrets-backend/local-filesystem-secrets-backend.rst#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"VAR_A\": \"some_value\",\n    \"var_b\": \"different_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an AWS Batch Compute Environment using BatchCreateComputeEnvironmentOperator in Python\nDESCRIPTION: Shows how to utilize the Airflow BatchCreateComputeEnvironmentOperator (:class:`~airflow.providers.amazon.aws.operators.batch.BatchCreateComputeEnvironmentOperator`) to programmatically create a new AWS Batch compute environment. This operator simplifies the process of setting up the necessary compute resources for Batch jobs within an Airflow DAG.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/batch.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../amazon/tests/system/amazon/aws/example_batch.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_batch_create_compute_environment]\n    :end-before: [END howto_operator_batch_create_compute_environment]\n```\n\n----------------------------------------\n\nTITLE: Creating Deferrable Livy Operator in Python\nDESCRIPTION: Example demonstrating how to create a LivyOperator in deferrable mode, which enables asynchronous job status polling through the Airflow triggerer for more efficient worker utilization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/livy/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[START create_livy_deferrable]\n```\n\n----------------------------------------\n\nTITLE: Running Core Tests with Breeze\nDESCRIPTION: Command for running core tests using Breeze testing framework with specific backend and xdist options.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-tests --skip-db-tests --backend none --use-xdist\n```\n\n----------------------------------------\n\nTITLE: Executing Tasks in Docker Environment with TaskFlow API\nDESCRIPTION: This snippet demonstrates how to use the TaskFlow API to run a task in a Docker container. It's useful for packaging all task dependencies and requires Docker to be available on the worker.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/taskflow.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@task.docker()\ndef transform():\n    import pandas as pd\n    df = pd.read_json(\"/tmp/processed_data.json\")\n    # Do something else\n    return \"Finished task\"\n```\n\n----------------------------------------\n\nTITLE: Installing YDB Integration for Apache Airflow\nDESCRIPTION: Command to install YDB integration package that provides hooks and operators for Apache Airflow\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_44\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[ydb]'\n```\n\n----------------------------------------\n\nTITLE: Async WaitFiveHourSensor Implementation\nDESCRIPTION: Implementation of an async sensor that exits directly from the trigger without worker involvement.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/deferring.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass WaitFiveHourSensorAsync(BaseSensorOperator):\n    # this sensor always exits from trigger.\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.end_from_trigger = True\n\n    def execute(self, context: Context) -> NoReturn:\n        self.defer(\n            method_name=None,\n            trigger=WaitFiveHourTrigger(duration=timedelta(hours=5), end_from_trigger=self.end_from_trigger),\n        )\n```\n\n----------------------------------------\n\nTITLE: Setting Custom IMAP Connection via Environment Variable (Non-SSL)\nDESCRIPTION: Sets a custom Apache Airflow IMAP connection (ID `imap_nonssl`) using a bash environment variable. The connection string follows URI syntax, specifying username, password, host (myimap.com), the standard non-SSL port (143), and explicitly disabling SSL via the `use_ssl=false` query parameter. Ensure username and password are URL-encoded if they contain special characters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/imap/docs/connections/imap.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_IMAP_NONSSL='imap://username:password@myimap.com:143?use_ssl=false'\n```\n\n----------------------------------------\n\nTITLE: Importing SSH Provider Package in Python\nDESCRIPTION: Import statement for the SSH provider package in Apache Airflow. This allows using the SSH-related functionality provided by this package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"apache-airflow-providers-ssh\"\"\n```\n\n----------------------------------------\n\nTITLE: Ingesting Vectors into Pinecone using PineconeIngestOperator in Python\nDESCRIPTION: This snippet shows how to use the `PineconeIngestOperator` in an Airflow DAG to ingest vectors into a specified Pinecone index. It requires a Pinecone connection ID (`conn_id`), the target `index_name`, and the `vectors` data (potentially including metadata). The example code is included from a separate file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pinecone/docs/operators/pinecone.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../pinecone/tests/system/pinecone/example_dag_pinecone.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_pinecone_ingest]\n    :end-before: [END howto_operator_pinecone_ingest]\n```\n\n----------------------------------------\n\nTITLE: Deleting Amazon Redshift Cluster with RedshiftDeleteClusterOperator\nDESCRIPTION: Example showing how to delete a Redshift cluster using RedshiftDeleteClusterOperator. Supports deferrable execution mode.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/redshift/redshift_cluster.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndelete_cluster = RedshiftDeleteClusterOperator(\n    task_id='delete_cluster',\n    cluster_identifier=redshift_cluster_identifier,\n    aws_conn_id='aws_default',\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Logging\nDESCRIPTION: Helm command to enable Elasticsearch integration for log storage and retrieval using a specified secret for authentication.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-logs.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airflow apache-airflow/airflow \\\n  --set elasticsearch.enabled=true \\\n  --set elasticsearch.secretName=my-es-secret\n  # Other choices exist. Please refer to values.yaml for details.\n```\n\n----------------------------------------\n\nTITLE: Installing the Vertica Provider via Pip (Bash)\nDESCRIPTION: Installs the base `apache-airflow-providers-vertica` package using pip. This command is intended for use on top of an existing Airflow 2 installation (version 2.9.0 or higher). It fetches and installs the provider package and its direct dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/vertica/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-vertica\n```\n\n----------------------------------------\n\nTITLE: Referencing Constraints File in Docker Build\nDESCRIPTION: This command demonstrates how to reference a constraints file stored in the docker-context-files folder during the Docker build process for Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-context-files/.README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`--airflow-constraints-location /docker-context-files/constraints.txt`\n```\n\n----------------------------------------\n\nTITLE: Listing Google Analytics Accounts with GoogleAnalyticsAdminListAccountsOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the GoogleAnalyticsAdminListAccountsOperator to list all accounts accessible to the user in Google Analytics. It shows the operator usage and mentions that Jinja templating can be applied to the operator's template fields.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/analytics_admin.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nGoogleAnalyticsAdminListAccountsOperator(\n    task_id=\"list_accounts\",\n)\n```\n\n----------------------------------------\n\nTITLE: Running Java Pipeline with DataflowRunner in Apache Beam\nDESCRIPTION: This example demonstrates using BeamRunJavaPipelineOperator to execute a Java pipeline on Google Cloud Dataflow. It specifies the JAR file, main class, and Dataflow-specific parameters including project ID and region.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_java_dataflow_runner = BeamRunJavaPipelineOperator(\n    task_id=\"beam_task_dataflow_runner\",\n    runner=\"DataflowRunner\",\n    jar=\"{{ var.json.beam_variables.jar }}\",\n    job_class=\"org.apache.beam.examples.WordCount\",\n    pipeline_options={\n        \"tempLocation\": \"{{ var.json.beam_variables.gcp_dataflow_temp }}\",\n        \"output\": \"{{ var.json.beam_variables.output_path }}\",\n        \"inputFile\": \"{{ var.json.beam_variables.input_file }}\",\n        \"project\": \"{{ var.json.beam_variables.gcp_project }}\",\n        \"region\": \"{{ var.json.beam_variables.gcp_region }}\",\n        \"labels\": json.dumps({\"airflow-version\": \"dev\"}),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Atlassian Jira Provider with Common Compat Dependency\nDESCRIPTION: Command to install the Atlassian Jira provider package for Apache Airflow with the common.compat extra, which includes additional cross-provider dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/atlassian/jira/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-atlassian-jira[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Creating Dataform Repository in Python\nDESCRIPTION: This snippet demonstrates how to create a repository for tracking code in Dataform service using the DataformCreateRepositoryOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_create_repository]\n# [END howto_operator_create_repository]\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Providers Apprise Package with Common Compat Dependency\nDESCRIPTION: Command to install the Apache Airflow Providers Apprise package along with its common.compat cross-provider dependency using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apprise[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Including Sections and Options Documentation in ReStructuredText\nDESCRIPTION: This directive includes the sections and options documentation file in the current document. It's used to import common documentation about structuring and configuring options.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/configurations-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst\n```\n\n----------------------------------------\n\nTITLE: Uploading Google Ads Accounts to GCS with GoogleAdsListAccountsOperator in Python\nDESCRIPTION: This snippet shows how to use the GoogleAdsListAccountsOperator to upload Google Ads accounts to a Google Cloud Storage bucket. The operator supports dynamic value determination using Jinja templating for certain parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/ads.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nGoogleAdsListAccountsOperator(\n    task_id=\"list_accounts\",\n    bucket=\"my-bucket\",\n    object_name=\"google_ads_accounts.csv\",\n    query=\"SELECT customer_client.client_customer, customer_client.level\",\n    google_ads_conn_id=ADS_CONN_ID,\n    gcp_conn_id=GCS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Canceling Dataproc Operation with Airflow Operator in Python\nDESCRIPTION: This snippet shows how to use the `DataprocCancelOperationOperator` to cancel a long-running Google Cloud Dataproc operation, which could be related to batch creation or other Dataproc tasks. It requires the `operation_name`, `region`, and `project_id`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n# Code extracted from: /../../google/tests/system/google/cloud/dataproc/example_dataproc_batch.py\n# Between markers: [START how_to_cloud_dataproc_cancel_operation_operator] and [END how_to_cloud_dataproc_cancel_operation_operator]\n# \n# Example using DataprocCancelOperationOperator(...)\n# ... (actual Python code would be here)\n\n```\n\n----------------------------------------\n\nTITLE: Running API Stub Update Pre-Commit for Airflow Providers (Shell)\nDESCRIPTION: This shell snippet demonstrates how contributors can manually force an update of the 'common.sql' provider's API stubs using the pre-commit hook when potentially breaking API changes are reviewed. The environment variable 'UPDATE_COMMON_SQL_API' enables the hook to regenerate and overwrite otherwise protected stub files. Prerequisites include 'pre-commit' installed in the environment, and proper permissions to update stub files; inputs include any modified Python module, and the output is an updated set of compliance stubs in the target Airflow package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/src/airflow/providers/common/sql/README_API.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nUPDATE_COMMON_SQL_API=1 pre-commit run update-common-sql-api-stubs\n```\n\n----------------------------------------\n\nTITLE: Setting Google OAuth2 Audience in Airflow Configuration\nDESCRIPTION: This configuration sets the OAuth2 audience, which is recommended to restrict generated tokens for use by Airflow only. The audience should be a unique identifier for your Airflow instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/api-auth-backend/google-openid.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[api]\ngoogle_oauth2_audience = project-id-random-value.apps.googleusercontent.com\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQueryToGCSOperator in Python\nDESCRIPTION: Adds Export Format to Template Fields in BigQueryToGCSOperator, allowing more flexibility in export configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n\"Add Export Format to Template Fields in BigQueryToGCSOperator (#27910)\"\n```\n\n----------------------------------------\n\nTITLE: Adding Deferred Pagination Mode to GenericTransfer\nDESCRIPTION: Commit message detailing the addition of a deferred pagination mode to the 'GenericTransfer' operator or component. References pull request #44809.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\nAdd deferred pagination mode to GenericTransfer (#44809)\n```\n\n----------------------------------------\n\nTITLE: Creating a Variable in Yandex Cloud Lockbox using CLI\nDESCRIPTION: This command creates a secret in Yandex Cloud Lockbox to store an Airflow variable. The secret is named with a specific format and contains a payload with the variable value.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_15\n\nLANGUAGE: console\nCODE:\n```\n$ yc lockbox secret create \\\n    --name airflow/variables/my_variable \\\n    --payload '[{\"key\": \"value\", \"text_value\": \"some_secret_data\"}]'\ndone (1s)\nname: airflow/variables/my_variable\n```\n\n----------------------------------------\n\nTITLE: Defining Migration Rules for Ruff Linter in Apache Airflow\nDESCRIPTION: Specifies the migration rules for the ruff linter (AIR303) to update import paths for time-related operators and sensors. This ensures that all references to these components are updated to use the new standard provider locations.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41564.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* ruff\n\n  * AIR303\n\n    * [x] ``airflow.operators.datetime.*``  ``airflow.providers.standard.time.operators.datetime.*``\n    * [x] ``airflow.operators.weekday.*``  ``airflow.providers.standard.time.operators.weekday.*``\n    * [x] ``airflow.sensors.date_time.*``  ``airflow.providers.standard.time.sensors.date_time.*``\n    * [x] ``airflow.sensors.time_sensor.*``  ``airflow.providers.standard.time.sensors.time.*``\n    * [x] ``airflow.sensors.time_delta.*``  ``airflow.providers.standard.time.sensors.time_delta.*``\n    * [x] ``airflow.sensors.weekday.*``  ``airflow.providers.standard.time.sensors.weekday.*``\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub Enterprise API URL Format\nDESCRIPTION: Demonstrates the URL format for connecting to GitHub Enterprise API endpoints. The hostname should be replaced with your specific GitHub Enterprise deployment URL.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/github/docs/connections/github.rst#2025-04-22_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\nhttps://{hostname}/api/v3\n```\n\n----------------------------------------\n\nTITLE: Configuring RST Documentation Toctree for HDFS Operators\nDESCRIPTION: RST configuration for structuring the HDFS operators documentation hierarchy, specifying a maximum depth of 1 and including the webhdfs documentation page.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n\n    webhdfs\n```\n\n----------------------------------------\n\nTITLE: Example: Creating AWS IAM Service Account 'airflow-sa' using eksctl (Bash)\nDESCRIPTION: Provides a concrete example of using `eksctl create iamserviceaccount` to set up IRSA for Airflow. It creates a service account named `airflow-sa` within the `airflow` namespace on the EKS cluster `airflow-eks-cluster`. It attaches the broad `AmazonS3FullAccess` policy (recommended to replace with a more restricted policy for production). Requires `eksctl` and appropriate AWS/EKS permissions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/logging/s3-task-handler.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\neksctl create iamserviceaccount --cluster=airflow-eks-cluster --name=airflow-sa --namespace=airflow --attach-policy-arn=arn:aws:iam::aws:policy/AmazonS3FullAccess --approve\n```\n\n----------------------------------------\n\nTITLE: Deleting a Compute Engine Instance with ComputeEngineDeleteInstanceOperator in Python\nDESCRIPTION: Deletes an existing Google Compute Engine instance using the ComputeEngineDeleteInstanceOperator. This snippet demonstrates how to use the operator without specifying a project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineDeleteInstanceOperator(\n    zone=GCE_ZONE,\n    resource_id=INSTANCE_NAME,\n    task_id=\"gcp_compute_delete_instance\",\n)\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST File\nDESCRIPTION: Sphinx documentation directive to include security-related documentation from a common development source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Referencing Git Commits in Markdown\nDESCRIPTION: Shows how to reference specific Git commits using Markdown link syntax with commit hashes and descriptions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n`08729eddbd <https://github.com/apache/airflow/commit/08729eddbd7414b932a654763bf62c6221a0e397>`__  2023-09-22   ``warn level for deprecated set to stacklevel 2 (#34530)``\n`659d94f0ae <https://github.com/apache/airflow/commit/659d94f0ae89f47a7d4b95d6c19ab7f87bd3a60f>`__  2023-09-21   ``Use 'airflow.exceptions.AirflowException' in providers (#34511)``\n`8ecd576de1 <https://github.com/apache/airflow/commit/8ecd576de1043dbea40e5e16b5dc34859cc41725>`__  2023-09-14   ``Refactor shorter defaults in providers (#34347)``\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Hooks on Staged Files in Bash\nDESCRIPTION: This command runs all pre-commit hooks on the staged files in the Git repository.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run\n```\n\n----------------------------------------\n\nTITLE: Migrating Apache Airflow SQL-related Operator Import Paths\nDESCRIPTION: This code snippet shows the migration of import paths for SQL-related operators in Apache Airflow. It includes changes for various SQL check operators, value check operators, and other SQL-specific operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41368.significant.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Old import path  New import path\n\"airflow.operators.presto_check_operator.SQLCheckOperator\"  \"airflow.providers.common.sql.operators.sql.SQLCheckOperator\"\n\"airflow.operators.presto_check_operator.SQLIntervalCheckOperator\"  \"airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator\"\n\"airflow.operators.presto_check_operator.SQLValueCheckOperator\"  \"airflow.providers.common.sql.operators.sql.SQLValueCheckOperator\"\n\"airflow.operators.sql.BaseSQLOperator\"  \"airflow.providers.common.sql.operators.sql.BaseSQLOperator\"\n\"airflow.operators.sql.BranchSQLOperator\"  \"airflow.providers.common.sql.operators.sql.BranchSQLOperator\"\n\"airflow.operators.sql.SQLCheckOperator\"  \"airflow.providers.common.sql.operators.sql.SQLCheckOperator\"\n\"airflow.operators.sql.SQLColumnCheckOperator\"  \"airflow.providers.common.sql.operators.sql.SQLColumnCheckOperator\"\n\"airflow.operators.sql.SQLIntervalCheckOperator\"  \"airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator\"\n\"airflow.operators.sql.SQLTableCheckOperator\"  \"airflow.providers.common.sql.operators.sql.SQLTableCheckOperator\"\n\"airflow.operators.sql.SQLThresholdCheckOperator\"  \"airflow.providers.common.sql.operators.sql.SQLThresholdCheckOperator\"\n\"airflow.operators.sql.SQLValueCheckOperator\"  \"airflow.providers.common.sql.operators.sql.SQLValueCheckOperator\"\n\"airflow.operators.sql._convert_to_float_if_possible\"  \"airflow.providers.common.sql.operators.sql._convert_to_float_if_possible\"\n\"airflow.operators.sql.parse_boolean\"  \"airflow.providers.common.sql.operators.sql.parse_boolean\"\n\"airflow.operators.sql_branch_operator.BranchSQLOperator\"  \"airflow.providers.common.sql.operators.sql.BranchSQLOperator\"\n\"airflow.operators.sqlite_operator.SqliteOperator\"  \"airflow.providers.sqlite.operators.sqlite.SqliteOperator\"\n\"airflow.sensors.sql.SqlSensor\"  \"airflow.providers.common.sql.sensors.sql.SqlSensor\"\n```\n\n----------------------------------------\n\nTITLE: Fetching All Records from Postgres Table (Python)\nDESCRIPTION: Configures `SQLExecuteQueryOperator` to execute a simple `SELECT *` query directly within the operator against the `pet` table in PostgreSQL. Requires the `conn_id` for the connection and the SQL query string.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/operators.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nget_all_pets = SQLExecuteQueryOperator(\n    task_id=\"get_all_pets\",\n    conn_id=\"postgres_default\",\n    sql=\"SELECT * FROM pet;\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Microsoft Azure Provider with Amazon Dependency\nDESCRIPTION: Command to install the Microsoft Azure provider package with cross-provider dependencies. This example shows how to include the Amazon provider dependency when installing from PyPI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-microsoft-azure[amazon]\n```\n\n----------------------------------------\n\nTITLE: Downloading Docker Compose Configuration\nDESCRIPTION: Command to download the docker-compose.yaml file for Airflow setup\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LfO 'docker-compose.yaml'\n```\n\n----------------------------------------\n\nTITLE: Creating AWS IAM Service Account using eksctl (Bash)\nDESCRIPTION: Demonstrates using the `eksctl` command-line tool to create an AWS IAM Role for Service Account (IRSA) on an Amazon EKS cluster. This command links a Kubernetes service account to an AWS IAM role, specified by `attach-policy-arn`, allowing pods using this service account to assume the role. It requires specifying the EKS cluster name, the desired service account name, the Kubernetes namespace, and the ARN of the IAM policy to attach. The `--approve` flag automatically confirms the CloudFormation stack creation. Requires `eksctl` installed and configured AWS credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/logging/s3-task-handler.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\neksctl create iamserviceaccount --cluster=\"<EKS_CLUSTER_ID>\" --name=\"<SERVICE_ACCOUNT_NAME>\" --namespace=\"<NAMESPACE>\" --attach-policy-arn=\"<IAM_POLICY_ARN>\" --approve``\n```\n\n----------------------------------------\n\nTITLE: Basic SQLExecuteQueryOperator Usage with SQLite in Python\nDESCRIPTION: Example of how to use SQLExecuteQueryOperator to connect to a SQLite database and execute a SQL query. This shows the basic implementation with an inline SQL statement.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\nwith DAG(\n    dag_id=\"example_sqlite\",\n    schedule=None,\n    start_date=datetime(2022, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_sqlite]\n\n    create_table_sqlite_task = SQLExecuteQueryOperator(\n        task_id=\"create_table_sqlite\",\n        conn_id=\"sqlite_default\",\n        sql=\"\"\"CREATE TABLE IF NOT EXISTS test_table(custom_id INTEGER NOT NULL, timestamp TIMESTAMP NOT NULL,\n              value FLOAT);\"\"\",\n    )\n\n    # [END howto_operator_sqlite]\n```\n\n----------------------------------------\n\nTITLE: Configuring a Dataplex Lake - Python\nDESCRIPTION: Defines the properties and configuration dictionary for creating a Dataplex lake resource. This is used as the body payload for lake creation operators. Includes all necessary and optional fields as required by the Dataplex API.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"lake_config = {\\n    'name': 'sample-lake',\\n    'displayName': 'Sample Dataplex Lake',\\n    # Additional configuration fields...\\n}\"\n```\n\n----------------------------------------\n\nTITLE: Preparing Single Provider Distribution for Testing (Bash)\nDESCRIPTION: This bash command rebuilds a single provider's distribution wheel, optionally taking a provider identifier as an argument. It uses 'prepare-provider-distributions' with a development suffix and wheel format, and is useful when changes are made to only one provider package during iterative workflows. The output distribution will be located in the 'dist' directory.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-provider-distributions \\\n  --version-suffix-for-pypi dev0 --distribution-format wheel <provider>\n```\n\n----------------------------------------\n\nTITLE: UV Airflow Core Development Commands\nDESCRIPTION: Commands for working specifically on Airflow core components, including dependency installation.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/07_local_virtualenv.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd airflow-core\nuv sync\ncd airflow-core\nuv sync --all-packages\n```\n\n----------------------------------------\n\nTITLE: Deleting PVC Resources in Kubernetes\nDESCRIPTION: Commands for deleting persistent volume claims (PVCs) in the airflow namespace for triggerer, worker, and redis components.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/RELEASE_NOTES.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete pvc -n airflow logs-gta-triggerer-0\nkubectl delete pvc -n airflow logs-gta-worker-0\nkubectl delete pvc -n airflow redis-db-gta-redis-0\n```\n\n----------------------------------------\n\nTITLE: Verifying Secret Backend Configuration\nDESCRIPTION: Command to verify the secret backend configuration in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/secrets-backends/google-cloud-secret-manager-backend.rst#2025-04-22_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ airflow config get-value secrets backend\nairflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend\n```\n\n----------------------------------------\n\nTITLE: Installing the Apache Airflow Amazon Provider via pip (Bash)\nDESCRIPTION: This command installs the core `apache-airflow-providers-amazon` package using pip. It should be executed in a terminal within a Python environment where Apache Airflow (version 2.9.0 or higher) is already installed or will be installed concurrently. This command fetches and installs version 9.6.1 of the provider and its direct dependencies listed in the requirements section.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-amazon\n```\n\n----------------------------------------\n\nTITLE: Getting a Dataplex Data Quality Scan Job in Deferrable Mode - Python\nDESCRIPTION: Runs DataplexGetDataQualityScanResultOperator in deferrable mode to efficiently wait for and retrieve the results of a Data Quality scan job. Inputs are identical to the standard operator, but deferrable mode allows resources to be released while waiting.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n\"get_dq_scan_job_def = DataplexGetDataQualityScanResultOperator(\\n    task_id=\\\"get_dq_scan_job_def\\\",\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    data_quality_scan_id=DATA_QUALITY_SCAN_ID,\\n    job_id=JOB_ID,\\n    deferrable=True,\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: This RST directive includes an external file containing security-related documentation for Apache Airflow. It references a file located in a development-common directory structure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/cassandra/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Task Instance Running State Listener\nDESCRIPTION: Example implementation of a listener that monitors when a task instance enters the running state. This code would be part of the listener.py file referenced in the plugin registration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/listener-plugin.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.example_dags.plugins.event_listener import *\n```\n\n----------------------------------------\n\nTITLE: Dependency Requirements for OpenAI Provider\nDESCRIPTION: Specifies the required package dependencies and their minimum versions needed for using the apache-airflow-providers-openai package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openai/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n===================  ==================\n\"apache-airflow\"   \">=2.9.0\"\n\"openai[datalib]\"  \">=1.66.0\"\n=================== ==================\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Executor for a Task in Airflow Python DAG\nDESCRIPTION: Demonstrates how to specify an executor for a specific task using the executor parameter in an Airflow Operator.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/index.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nBashOperator(\n    task_id=\"hello_world\",\n    executor=\"LocalExecutor\",\n    bash_command=\"echo 'hello world!'\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom StatsD Client in Airflow\nDESCRIPTION: Configuration for using a custom StatsD client instead of the default one provided by Airflow. This requires specifying the module path of the custom client in the configuration file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/metrics.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[metrics]\nstatsd_custom_client_path = x.y.customclient\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Dependencies for Airflow\nDESCRIPTION: Command to install the required OpenTelemetry packages for Apache Airflow using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/traces.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[otel]'\n```\n\n----------------------------------------\n\nTITLE: Publishing Core Airflow Documentation\nDESCRIPTION: Command to publish documentation for apache-airflow core package.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management publish-docs --airflow-site-directory DIRECTORY apache-airflow\n```\n\n----------------------------------------\n\nTITLE: Defining PagerDuty Connection Parameters in RST\nDESCRIPTION: This RST snippet outlines the configuration parameters for a PagerDuty connection in Apache Airflow. It specifies the API token for authentication and the routing key for event routing, with a note about the routing key's deprecation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pagerduty/docs/connections/pagerdurty.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nPagerduty API token\n    The API token that will be used for authentication against the PagerDuty API.\n\nPagerduty Routing key (Integration key)\n    The routing key (also known as Integration key) that will be used to route an event to the corresponding\n    PagerDuty service or rule.\n\n.. note::\n    The Pagerduty Routing key is deprecated.\n    Please use the :ref:`PagerDutyEvents connection <howto/connection:pagerduty-events>` instead.\n```\n\n----------------------------------------\n\nTITLE: Retrieving Tag Template from XCom in Python\nDESCRIPTION: Example of retrieving a tag template from XCom after getting it with CloudDataCatalogGetTagTemplateOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_get_tag_template_result]\nget_tag_template_result = kwargs[\"ti\"].xcom_pull(task_ids=\"get_tag_template\")\nprint(get_tag_template_result)\n# [END howto_operator_gcp_datacatalog_get_tag_template_result]\n```\n\n----------------------------------------\n\nTITLE: Fixing TaskContextLogger Context Setting in Python\nDESCRIPTION: Corrects the attribute check for setting context in TaskContextLogger to use the parent instead of self.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"Check attr on parent not self re TaskContextLogger set_context (#35780)\"\n```\n\n----------------------------------------\n\nTITLE: Running Airflow Unit Tests with Pytest in Bash\nDESCRIPTION: Example command to run specific Airflow unit tests using pytest within the Breeze environment.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nroot@63528318c8b1:/opt/airflow# pytest tests/utils/test_dates.py\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Airbyte integration, enabling Airbyte hooks and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[airbyte]'\n```\n\n----------------------------------------\n\nTITLE: Installing the Apache Airflow Segment Provider via pip\nDESCRIPTION: This command installs the `apache-airflow-providers-segment` package using the pip package installer. It should be executed in a command-line environment with Python and pip installed, on top of an existing Airflow 2.9.0+ installation. This adds Segment integration capabilities to Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/segment/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-segment\n```\n\n----------------------------------------\n\nTITLE: Defining Extra Requirements for Apache Beam Provider Integration in Python\nDESCRIPTION: This code snippet shows the extra requirements definition for the Apache Beam provider to integrate with Google. It includes the Google provider and the GCP extras for Apache Beam.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nextras_require = ({\"google\": [\"apache-airflow-providers-google\", \"apache-beam[gcp]\"]})\n```\n\n----------------------------------------\n\nTITLE: Optimizing Connection Importing for Airflow 2.2.0\nDESCRIPTION: Miscellaneous Change (Version 2.0.1): Improves the efficiency of importing connection objects, specifically optimized for Airflow version 2.2.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_33\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Optimise connection importing for Airflow 2.2.0``\n```\n\n----------------------------------------\n\nTITLE: Building PROD Image with Additional Extras (Bash)\nDESCRIPTION: Builds the Airflow PROD image for Python 3.9 using Breeze. The `--additional-airflow-extras \"all\"` parameter adds the 'all' extras to the default set of extras included in the production image.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze prod-image build --python 3.9 --additional-airflow-extras \"all\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Version-Organized Commit Log Table for Apache Airflow OpenAI Provider (reStructuredText)\nDESCRIPTION: This snippet shows a reStructuredText (reST) table with commit hashes, dates, and summaries for a specific version of the provider. Dependencies include Sphinx or compatible reST renderers for proper formatting. The snippet expects upstream GitHub links for each commit, and all table formatting must be preserved for correct display. Inputs are commit entries; the output is a rendered table in documentation. Limitations include the need for fixed-width fonts to maintain table integrity.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openai/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`cb295c351a <https://github.com/apache/airflow/commit/cb295c351a016c0a10cab07f2a628b865cff3ca3>`__  2025-04-14   ``remove superfluous else block (#49199)``\n`4a8567b20b <https://github.com/apache/airflow/commit/4a8567b20bdd6555cbdc936d6674bf4fa390b0d5>`__  2025-04-10   ``Prepare docs for Apr 2nd wave of providers (#49051)``\n`7b2ec33c7a <https://github.com/apache/airflow/commit/7b2ec33c7ad4998d9c9735b79593fcdcd3b9dd1f>`__  2025-04-08   ``Remove unnecessary entries in get_provider_info and update the schema (#48849)``\n`139673d3ce <https://github.com/apache/airflow/commit/139673d3ce5552c2cf8bcb2d202e97342c4b237c>`__  2025-04-07   ``Remove fab from preinstalled providers (#48457)``\n`67858fd7e7 <https://github.com/apache/airflow/commit/67858fd7e7ac82788854844c1e6ef5a35f1d0d23>`__  2025-04-06   ``Improve documentation building iteration (#48760)``\n`adbb062b50 <https://github.com/apache/airflow/commit/adbb062b50e2e128fe475a76b7ce10ec93c39ee2>`__  2025-04-06   ``Prepare docs for Apr 1st wave of providers (#48828)``\n`d4473555c0 <https://github.com/apache/airflow/commit/d4473555c0e7022e073489b7163d49102881a1a6>`__  2025-04-02   ``Simplify tooling by switching completely to uv (#48223)``\n==================================================================================================  ===========  ==================================================================================\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Documentation\nDESCRIPTION: Basic command to build Airflow documentation using Breeze\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs\n```\n\n----------------------------------------\n\nTITLE: Fix and Reapply Provider Documentation Templates (Excluded from v1.1.0 Changelog)\nDESCRIPTION: Addresses fixes and reapplies templates used for generating provider documentation, referenced by pull request #35686. This change was intentionally excluded from the main changelog notes for version 1.1.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_30\n\nLANGUAGE: plaintext\nCODE:\n```\nFix and reapply templates for provider documentation (#35686)\n```\n\n----------------------------------------\n\nTITLE: Including External Security Documentation in Sphinx (reStructuredText)\nDESCRIPTION: This reStructuredText directive (`.. include::`) is used within the Sphinx documentation generator to embed the content of another file (`security.rst`) located in a specific relative path. This is common practice for modularizing documentation and reusing content like license or security information across different parts of a project's documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/compat/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Beam Provider Package in Airflow via pip\nDESCRIPTION: Command to install the Apache Beam provider package on an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-beam\n```\n\n----------------------------------------\n\nTITLE: Adding Deferrable Support to DatabricksNotebookOperator\nDESCRIPTION: New feature adding deferrable support to the DatabricksNotebookOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"add deferrable support to 'DatabricksNotebookOperator' (#39295)\"\n```\n\n----------------------------------------\n\nTITLE: Stopping Breeze and Airflow in Bash\nDESCRIPTION: Commands to stop Breeze and Airflow, depending on how they were started.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nroot@f3619b74c59a:/opt/airflow# stop_airflow\nbreeze down\n```\n\nLANGUAGE: bash\nCODE:\n```\nroot@f3619b74c59a:/opt/airflow# exit\nbreeze down\n```\n\n----------------------------------------\n\nTITLE: Installing Trino Extras for Apache Airflow\nDESCRIPTION: Command to install all Trino related operators and hooks for Apache Airflow. This enables integration with Trino distributed SQL query engine.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_53\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[trino]'\n```\n\n----------------------------------------\n\nTITLE: Running Specific Pre-commit Hooks\nDESCRIPTION: Commands to run individual pre-commit hooks (black and ruff) on specific files\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run black --files airflow/decorators.py tests/utils/test_task_group.py\npre-commit run ruff --files airflow/decorators.py tests/utils/test_task_group.py\n```\n\n----------------------------------------\n\nTITLE: Defining Airflow Connections via Environment Variables\nDESCRIPTION: Creates a new connection with the specified connection ID using a URI value. Useful for setting up connections without using the Airflow UI.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/cli-and-env-variables-ref.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_CONN_{CONN_ID}\n```\n\n----------------------------------------\n\nTITLE: Exception Suppression for Unsupported JDBC Driver Methods\nDESCRIPTION: Bug fix that suppresses jaydebeapi.Error when setAutoCommit or getAutoCommit is unsupported by JDBC driver. Improves compatibility with various JDBC drivers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/changelog.rst#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nSuppress jaydebeapi.Error when setAutoCommit or getAutoCommit is unsupported by JDBC driver (#38707)\n```\n\n----------------------------------------\n\nTITLE: Defining Integration Service with Docker Compose YAML\nDESCRIPTION: This YAML snippet demonstrates how to define an integration service (Apache Drill) and its associated Airflow service in Docker Compose for CI testing. It specifies image source, labels, volume mounts for persistent and configuration data, restart policy, port mapping (with parameterized host port), and service dependencies (Airflow depends on Drill). Requires Docker Compose (v3.8+), mounted config files, and a volume for persistent data. Inputs include environment variables for host ports; outputs an up-and-running isolated service environment for integration testing.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/integration_tests.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nversion: \"3.8\"\nservices:\n  drill:\n    container_name: drill\n    image: \"apache/drill:1.21.1-openjdk-17\"\n    labels:\n      breeze.description: \"Integration required for drill operator and hook.\"\n    volumes:\n      - drill-db-volume:/data\n      - ./drill/drill-override.conf:/opt/drill/conf/drill-override.conf\n    restart: \"on-failure\"\n    ports:\n      - \"${DRILL_HOST_PORT}:8047\"\n    stdin_open: true\n  airflow:\n    depends_on:\n      - drill\n    environment:\n      - INTEGRATION_DRILL=true\nvolumes:\n  drill-db-volume:\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Parameterized Timetable\nDESCRIPTION: Example of a parameterized timetable implementation with serialization support for custom scheduling times.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/timetable.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass SometimeAfterWorkdayTimetable(Timetable):\n    def __init__(self, schedule_at: Time) -> None:\n        self._schedule_at = schedule_at\n\n    def next_dagrun_info(self, last_automated_dagrun, restriction):\n        ...\n        end = start + timedelta(days=1)\n        return DagRunInfo(\n            data_interval=DataInterval(start=start, end=end),\n            run_after=DateTime.combine(end.date(), self._schedule_at).replace(tzinfo=UTC),\n        )\n\n    def serialize(self) -> dict[str, Any]:\n        return {\"schedule_at\": self._schedule_at.isoformat()}\n\n    @classmethod\n    def deserialize(cls, value: dict[str, Any]) -> Timetable:\n        return cls(Time.fromisoformat(value[\"schedule_at\"]))\n```\n\n----------------------------------------\n\nTITLE: Using get_connection for Retrieving Single Connections in Airflow\nDESCRIPTION: Example demonstrating how to use the get_connection method from LocalFilesystemBackend to retrieve a single connection by ID. This replaces the deprecated get_connections function.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41533.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconnection_by_conn_id = LocalFilesystemBackend().get_connection(conn_id=\"conn_id\")\n```\n\n----------------------------------------\n\nTITLE: UnicodeDecodeError Example in Airflow\nDESCRIPTION: This snippet shows an example of the UnicodeDecodeError that may occur when Airflow encounters non-ASCII characters without proper encoding configuration. It highlights the specific error message thrown by WTForms templating and other Airflow modules.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/faq.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n'ascii' codec can't decode byte 0xae in position 506: ordinal not in range(128)\n```\n\n----------------------------------------\n\nTITLE: Deleting Inspection Template from Google Cloud DLP\nDESCRIPTION: Shows how to delete an inspection template using CloudDLPDeleteInspectTemplateOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/data_loss_prevention.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_dlp_delete_inspect_template]\n[END howto_operator_dlp_delete_inspect_template]\n```\n\n----------------------------------------\n\nTITLE: Defining Product Object for Google Cloud Vision in Python\nDESCRIPTION: Shows how to define a Product object for use with Google Cloud Vision product operations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nproduct = Product(\n    display_name=PRODUCT_DISPLAY_NAME,\n    product_category=PRODUCT_CATEGORY,\n    description=PRODUCT_DESCRIPTION,\n)\n```\n\n----------------------------------------\n\nTITLE: Upgrading Apache Airflow with Providers\nDESCRIPTION: This script upgrades Apache Airflow along with specified extras (postgres, google) to a specific version, using constraints from a dynamically generated URL based on the Airflow version and Python version.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-pypi.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_VERSION=|version|\nPYTHON_VERSION=\"$(python -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")')\"\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\npip install \"apache-airflow[postgres,google]==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n```\n\n----------------------------------------\n\nTITLE: Using KubernetesJobOperator in Deferrable Mode\nDESCRIPTION: Demonstrates how to use the KubernetesJobOperator in deferrable mode, which allows for asynchronous execution of Kubernetes Jobs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nKubernetesJobOperator(\n    task_id=\"kubernetes_job_task_deferrable\",\n    namespace=\"default\",\n    image=\"ubuntu\",\n    cmds=[\"bash\", \"-cx\"],\n    arguments=[\"echo\", \"10\"],\n    labels={\"foo\": \"bar\"},\n    name=\"airflow-job-test-deferrable\",\n    on_finish_action=\"delete_pod\",\n    deferrable=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Getting a Hyperparameter Tuning Job using Vertex AI Operator - Python\nDESCRIPTION: Provides an example of retrieving details for a specific Vertex AI hyperparameter tuning job via GetHyperparameterTuningJobOperator. The operator requires job ID and project/context configuration, and outputs job details for use in downstream Airflow tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nget_hpt_job_task = GetHyperparameterTuningJobOperator(\n    task_id=\"get_hyperparameter_tuning_job_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    hyperparameter_tuning_job_id=HPTUNING_JOB_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Disabling Airflow UI Web Frame Embedding (INI)\nDESCRIPTION: This configuration snippet disables the ability to render the Airflow UI within an HTML frame (e.g., <iframe>) on another website. Setting `x_frame_enabled` to `False` in the `[webserver]` section of the Airflow configuration file helps prevent clickjacking attacks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[webserver]\nx_frame_enabled = False\n```\n\n----------------------------------------\n\nTITLE: Configuring Worker Security Contexts\nDESCRIPTION: Configuration for setting security contexts for worker pods and containers.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nworkers:\n  securityContexts:\n    pod:\n      runAsUser: 5000\n      fsGroup: 0\n    containers:\n      allowPrivilegeEscalation: false\n```\n\n----------------------------------------\n\nTITLE: Pushing ECS Task ARN to XCom in EcsRunTaskOperator\nDESCRIPTION: Always pushes the ECS task ARN to XCom when using the EcsRunTaskOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntask.xcom_push(key='task_arn', value=task_arn)\n```\n\n----------------------------------------\n\nTITLE: Recommended Python Version Check Using sys.version_info\nDESCRIPTION: This snippet shows the recommended way to check Python versions using sys.version_info. This approach should be used instead of the deprecated airflow constants.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43562.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport sys\n\nif sys.version_info >= (3, 6):\n    # perform some action\n    ...\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Query with SQLExecuteQueryOperator in Redshift\nDESCRIPTION: Example showing how to use SQLExecuteQueryOperator to execute SQL queries against an Amazon Redshift cluster using a Redshift connection. The snippet demonstrates task configuration with connection ID and SQL parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/redshift/redshift_sql.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith models.DAG(\n    \"example_sql_execute_query\",\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START howto_operator_sql_execute_query]\n    execute_sql_query = SQLExecuteQueryOperator(\n        task_id=\"execute_sql_query\",\n        conn_id=\"exasol_default\",\n        sql=\"select * from dummy\",\n        dag=dag,\n    )\n    # [END howto_operator_sql_execute_query]\n```\n\n----------------------------------------\n\nTITLE: Improving ECS Executor Failure Logging in Python\nDESCRIPTION: Adds logging of failure reasons for containers when a task fails in the ECS Executor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"Log failure reason for containers if a task fails for ECS Executor (#35496)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Telegram Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Telegram integration, enabling Telegram hooks and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_40\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[telegram]'\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Kylin Provider Package for Airflow\nDESCRIPTION: Command to install the Apache Kylin provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kylin/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-kylin\n```\n\n----------------------------------------\n\nTITLE: Initializing Dataform Workspace in Python\nDESCRIPTION: This snippet demonstrates how to initialize a workspace with the default project structure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_initialize_workspace]\n# [END howto_initialize_workspace]\n```\n\n----------------------------------------\n\nTITLE: Adding apt Packages to Airflow Docker Image\nDESCRIPTION: This Dockerfile example demonstrates how to add new apt packages (vim in this case) to the Airflow image. It switches to root user for installation and back to airflow user afterwards.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apache/airflow:2.7.1\nUSER root\nRUN apt-get update \\\n  && apt-get install -y --no-install-recommends \\\n         vim \\\n  && apt-get autoremove -yqq --purge \\\n  && apt-get clean \\\n  && rm -rf /var/lib/apt/lists/*\nUSER airflow\n```\n\n----------------------------------------\n\nTITLE: Create AWS Glue Data Quality Example\nDESCRIPTION: Example showing how to create or update AWS Glue Data Quality ruleset using the GlueDataQualityOperator to measure and monitor data quality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/glue.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_glue_data_quality_operator]\n[END howto_operator_glue_data_quality_operator]\n```\n\n----------------------------------------\n\nTITLE: Exporting Connections to STDOUT in Airflow CLI\nDESCRIPTION: This command exports Airflow connections to STDOUT using the Airflow CLI.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nairflow connections export -\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Home Directory in Docker\nDESCRIPTION: This snippet shows the default AIRFLOW_HOME directory setting in the Airflow Docker image. It specifies the location for DAGs and logs.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_HOME=/opt/airflow/\n```\n\n----------------------------------------\n\nTITLE: Running Python Commands in Airflow Docker Container\nDESCRIPTION: Shows how to execute Python commands within the Airflow Docker container using the default entrypoint.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it apache/airflow:3.1.0.dev0-python3.9 python -c \"print('test')\"\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Kafka Provider with Google Provider Dependency\nDESCRIPTION: This command installs the Apache Kafka provider package along with its Google provider dependency. This is necessary to use all features of the package that depend on Google provider functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-kafka[google]\n```\n\n----------------------------------------\n\nTITLE: Templating Fields for GCE Instance Group Manager Insertion in Python\nDESCRIPTION: Defines the template fields available for the ComputeEngineInsertInstanceGroupManagerOperator, which allow for runtime parameter substitution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"project_id\",\n    \"zone\",\n    \"body\",\n    \"request_id\",\n    \"gcp_conn_id\",\n    \"api_version\",\n    \"impersonation_chain\",\n)\n```\n\n----------------------------------------\n\nTITLE: Syncing a Vertex AI Feature View using Airflow Operator in Python\nDESCRIPTION: Illustrates how to use the `SyncFeatureViewOperator` from `airflow.providers.google.cloud.operators.vertex_ai.feature_store` to synchronize a feature view in Vertex AI Feature Store. The name of the sync job is returned via XCom under the 'return_value' key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_55\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/vertex_ai/example_vertex_ai_feature_store.py\n    :language: python\n    :dedent: 4\n    :start-after: [START how_to_cloud_vertex_ai_feature_store_sync_feature_view_operator]\n    :end-before: [END how_to_cloud_vertex_ai_feature_store_sync_feature_view_operator]\n```\n\n----------------------------------------\n\nTITLE: Verifying Apache Airflow Package SHA512 Checksums - Bash\nDESCRIPTION: Command to verify the SHA512 checksum of downloaded packages by comparing against the provided .sha512 file.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/installing-helm-chart-from-sources.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nshasum -a 512 airflow-********  | diff - airflow-********.sha512\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Extra Parameters for Looker Connection in Airflow\nDESCRIPTION: JSON configuration for the 'extras' field in the Looker connection. Includes parameters for SSL verification and HTTP request timeout settings.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp_looker.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"verify_ssl\": true,\n   \"timeout\": 120,\n}\n```\n\n----------------------------------------\n\nTITLE: Using FacebookAdsReportToGcsOperator in Apache Airflow\nDESCRIPTION: Example demonstrating how to use the FacebookAdsReportToGcsOperator to fetch Facebook ads reports and load them to Google Cloud Storage. The operator enables data transfer from Facebook's advertising platform to GCS.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/facebook_ads_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_facebook_ads_to_gcs]\n# This example code shows how to fetch a Facebook Ads report and load it to Google Cloud Storage\n```\n\n----------------------------------------\n\nTITLE: SQLExecuteQueryOperator with External SQL File for SQLite in Python\nDESCRIPTION: Example of using SQLExecuteQueryOperator with an external SQL file. The script file must be located in the same folder level as the DAG file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    # [START howto_operator_sqlite_external_file]\n\n    insert_sqlite_task = SQLExecuteQueryOperator(\n        task_id=\"insert_sqlite\",\n        conn_id=\"sqlite_default\",\n        sql=\"insert_data.sql\",\n    )\n\n    # [END howto_operator_sqlite_external_file]\n```\n\n----------------------------------------\n\nTITLE: Implementing Provider Info Function in Python\nDESCRIPTION: Example implementation of the get_provider_info function that returns required metadata for an Airflow provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/howto/create-custom-providers.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_provider_info():\n    return {\n        \"package-name\": \"my-package-name\",\n        \"name\": \"name\",\n        \"description\": \"a description\",\n        \"hook-class-names\": [\n            \"myproviderpackage.hooks.source.SourceHook\",\n        ],\n    }\n```\n\n----------------------------------------\n\nTITLE: Running Airflow CLI Commands in Cloud Composer\nDESCRIPTION: This snippet shows how to use the CloudComposerRunAirflowCLICommandOperator to run Airflow CLI commands in a Cloud Composer environment. It specifies the project ID, region, environment name, and command to execute.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_composer.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nrun_airflow_command = CloudComposerRunAirflowCLICommandOperator(\n    task_id=\"run-command\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    environment_name=ENVIRONMENT_NAME,\n    command=[\"dags\", \"list\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Presto to GCS with File Splitting in Python\nDESCRIPTION: Example showing how to split large query results into multiple files of specified size using the approx_max_file_size_bytes parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/presto_to_gcs.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntask_presto_to_gcs_many_chunks = PrestoToGCSOperator(\n    task_id=\"presto_to_gcs_many_chunks\",\n    sql=PRESTO_SQL_MULTIPLE_TYPES,\n    bucket=BUCKET,\n    filename=f\"{DATASET_PREFIX}/many_chunks/data*.json\",\n    approx_max_file_size_bytes=10000000,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding 'autocommit' Option to OracleHook in RST\nDESCRIPTION: This commit (0f712e307a, committed on 2021-12-07) adds an 'autocommit' parameter to the OracleHook to control transaction behavior. Refers to issue #20085.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_15\n\nLANGUAGE: rst\nCODE:\n```\nAdd ''autocommit'' to ''OracleHook''  (#20085)\n```\n\n----------------------------------------\n\nTITLE: Building Docs for Apache Beam Provider with Breeze\nDESCRIPTION: This command demonstrates building documentation specifically for the Apache Beam provider package using the Breeze development environment.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/docs/README.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs apache.beam\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Provider with Cross-Provider Dependencies\nDESCRIPTION: Command to install the Docker provider package along with its cross-provider dependencies, specifically the common.compat extra.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/docker/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-docker[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Airflow Connection from AWS Secrets Manager\nDESCRIPTION: AWS CLI command to retrieve a stored Airflow connection secret from AWS Secrets Manager. The command returns detailed information about the secret, including its ARN, name, and the actual secret string.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-secrets-manager.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naws secretsmanager get-secret-value --secret-id airflow/connections/smtp_default\n```\n\n----------------------------------------\n\nTITLE: Configuring Executor in Airflow INI File\nDESCRIPTION: Shows how to set the executor in the Airflow configuration file. The example demonstrates setting the KubernetesExecutor.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nexecutor = KubernetesExecutor\n```\n\n----------------------------------------\n\nTITLE: Creating Document with Text for Google Cloud Natural Language in Python\nDESCRIPTION: This snippet demonstrates how to create a Document object with text provided as a string for use with Google Cloud Natural Language API.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/natural_language.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndocument = language.Document(\n    content=\"Airflow is an open-source workflow management platform developed by Airbnb.\",\n    type_=language.Document.Type.PLAIN_TEXT,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Edge3 Provider Package with FAB Dependency\nDESCRIPTION: This command installs the Edge3 provider package along with its cross-provider dependency for Flask-AppBuilder (FAB). This is necessary if you want to use all features of the package that depend on FAB.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/edge3/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-edge3[fab]\n```\n\n----------------------------------------\n\nTITLE: Installing Elasticsearch Provider with Optional Common SQL Dependencies using pip (bash)\nDESCRIPTION: This code snippet provides the command to install the apache-airflow-providers-elasticsearch package with all optional common SQL dependencies via pip. The command uses pip's extra requirements syntax to include dependencies such as apache-airflow-providers-common-sql. Users must have an existing Python environment with pip available. Input is via command line, and upon successful execution it ensures all required provider integrations are installed. This command assumes Airflow 2.9.0+ and Python 3+; installation may fail if requirements are not satisfied or if run in an incompatible environment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-elasticsearch[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Running Go Pipeline with DirectRunner using GCS File in Apache Beam\nDESCRIPTION: This example shows how to use BeamRunGoPipelineOperator to execute a Go pipeline using DirectRunner with a file from GCS. The operator downloads the file from GCS and specifies pipeline options.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/operators.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nbeam_task_go_direct_runner_gcs_file = BeamRunGoPipelineOperator(\n    task_id=\"beam_task_go_direct_runner_gcs_file\",\n    go_file=\"{{ var.json.beam_variables.gcs_go_file_path }}\",\n    pipeline_options={\n        \"output\": \"/tmp/apache_beam/direct_runner_go_output_gcs\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Verifying Custom XCom Backend in Python\nDESCRIPTION: Shows how to verify the XCom backend class being used in a container environment. This is useful for debugging custom XCom backend implementations.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/xcoms.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models.xcom import XCom\n\nprint(XCom.__name__)\n```\n\n----------------------------------------\n\nTITLE: Configuring Extra Parameters for Apache Drill Connection in JSON\nDESCRIPTION: This JSON snippet shows the structure for specifying extra parameters in the Apache Drill connection configuration. It includes the dialect_driver and storage_plugin options.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/docs/connections/drill.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dialect_driver\": \"drill_sadrill\",\n    \"storage_plugin\": \"dfs\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running All Pre-commit Checks on All Files in Bash\nDESCRIPTION: This command runs all pre-commit checks on all files in the repository.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Installing Breeze Command for Release Management\nDESCRIPTION: These commands install the 'breeze' command, which is used for various release management tasks in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nuv tool install -e ./dev/breeze\n```\n\nLANGUAGE: shell\nCODE:\n```\npipx install -e ./dev/breeze\n```\n\n----------------------------------------\n\nTITLE: Running Core Integration Tests\nDESCRIPTION: Example of running Kerberos integration tests for core\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-integration-tests --integration kerberos\n```\n\n----------------------------------------\n\nTITLE: Migrating EmptyOperator Import Path in Python\nDESCRIPTION: Code change requirement for importing EmptyOperator from its new location in the standard provider package instead of Airflow core.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/46231.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old import path\nfrom airflow.operators.empty import EmptyOperator\n\n# New import path\nfrom airflow.providers.standard.operators.empty import EmptyOperator\n```\n\n----------------------------------------\n\nTITLE: Creating Connection Secret in Google Cloud\nDESCRIPTION: Command to create a new connection secret in Google Cloud Secret Manager using gcloud CLI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/secrets-backends/google-cloud-secret-manager-backend.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ echo \"mysql://example.org\" | gcloud beta secrets create \\\n    airflow-connections-first-connection \\\n    --data-file=- \\\n    --replication-policy=automatic\n```\n\n----------------------------------------\n\nTITLE: Start Airflow API Server\nDESCRIPTION: Starts the Airflow API server (formerly webserver) in version 3.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading_to_airflow3.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nairflow api-server\n```\n\n----------------------------------------\n\nTITLE: Configuring AssumeRoleWithWebIdentity (File Token) in Connection Extra (JSON)\nDESCRIPTION: This JSON snippet for the 'Extra' field configures Airflow to use AWS STS AssumeRoleWithWebIdentity for authentication, specifically using a file-based token. It requires the 'role_arn', sets the 'assume_role_method', specifies 'file' for federation, and provides the path to the access token file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"role_arn\": \"arn:aws:iam::112223334444:role/my_role\",\n  \"assume_role_method\": \"assume_role_with_web_identity\",\n  \"assume_role_with_web_identity_federation\": \"file\",\n  \"assume_role_with_web_identity_token_file\": \"/path/to/access_token\"\n}\n```\n\n----------------------------------------\n\nTITLE: Preventing File Reference Interpretation in Python\nDESCRIPTION: Demonstrates how to prevent Airflow from interpreting a string as a file reference by using the literal() function.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import literal\n\n\nfixed_print_script = BashOperator(\n    task_id=\"fixed_print_script\",\n    bash_command=literal(\"cat script.sh\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Microsoft Graph API Hooks and Operators in Python\nDESCRIPTION: This snippet shows the import statements for the Microsoft Graph API hook and operator classes used in Apache Airflow. These classes are essential for interacting with the Microsoft Graph API.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/msgraph.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.microsoft.azure.hooks.msgraph import KiotaRequestAdapterHook\nfrom airflow.providers.microsoft.azure.operators.msgraph import MSGraphAsyncOperator\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-openlineage with Optional Dependencies using Bash\nDESCRIPTION: This bash command demonstrates how to install the `apache-airflow-providers-openlineage` package using pip, including an optional extra dependency (`common.compat`). This method allows users to install necessary cross-provider features, specified within square brackets, alongside the main OpenLineage provider package. Requires pip and a compatible Python environment with an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-openlineage[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Services for Apprise Connection in JSON\nDESCRIPTION: This snippet demonstrates the JSON configuration format for specifying multiple services in the Apprise connection. Each service is defined with a 'path' for the URI and a 'tag' for identification.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/docs/connections.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"path\": \"URI for the service 1\",\n    \"tag\": \"tag name\"\n  },\n  {\n    \"path\": \"URI for the service 2\",\n    \"tag\": \"tag name\"\n  },\n]\n```\n\n----------------------------------------\n\nTITLE: ZSH Airflow Completion Setup\nDESCRIPTION: Configuration commands to enable Airflow CLI completion in ZSH shell\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nautoload bashcompinit\nbashcompinit\neval \"$(register-python-argcomplete airflow)\"\n```\n\n----------------------------------------\n\nTITLE: Implementing get_uri Method Override for JDBC Connection\nDESCRIPTION: Feature added in version 5.1.0 that overrides the 'get_uri' method for JDBC connections. This allows customized URI generation for JDBC connections.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nfeat: overwrite 'get_uri' for 'JDBC' (#48915)\n```\n\n----------------------------------------\n\nTITLE: Setting GCS Bucket Permissions for Cloud Memorystore\nDESCRIPTION: Example demonstrating how to set ACL permissions on a GCS bucket for Cloud Memorystore import/export operations\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nset_acl_permission = GCSBucketCreateAclEntryOperator(task_id=\"set_acl_permission\", bucket=BUCKET_NAME, entity=\"{{ task_instance.xcom_pull('get-instance')['persistenceIamIdentity'] }}\", role=\"OWNER\")\n```\n\n----------------------------------------\n\nTITLE: Deploying Vertex AI Model using Python\nDESCRIPTION: This snippet illustrates how to deploy a Vertex AI model to an endpoint using the DeployModelOperator in Airflow. It specifies the project ID, region, endpoint, deployed model display name, model, dedicated resources, and traffic split.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/automl.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nDeployModelOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    endpoint=ENDPOINT_ID,\n    deployed_model_display_name=DEPLOYED_MODEL_ID,\n    model=MODEL_ID,\n    dedicated_resources={\n        \"machine_spec\": {\n            \"machine_type\": \"n1-standard-2\",\n        },\n        \"min_replica_count\": 1,\n        \"max_replica_count\": 1,\n    },\n    traffic_split={\"0\": 100},\n    task_id=\"deploy_model\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Amazon Provider with Cross-Provider Dependencies (Hive Example)\nDESCRIPTION: This command installs the `apache-airflow-providers-amazon` package along with optional dependencies needed for specific cross-provider features. The example shows installing the `apache.hive` extra, which enables integrations requiring the `apache-airflow-providers-apache-hive` package. This pattern allows users to install only the necessary dependencies for their specific use cases.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-amazon[apache.hive]\n```\n\n----------------------------------------\n\nTITLE: Replacing 'external_trigger' Check with DagRunType\nDESCRIPTION: Commit message indicating the replacement of checks for 'external_trigger' with the more explicit 'DagRunType' enum or attribute. References pull request #45961.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_20\n\nLANGUAGE: plaintext\nCODE:\n```\nReplace 'external_trigger' check with DagRunType (#45961)\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Driver Parameters in Python\nDESCRIPTION: Shows different methods to configure JDBC driver parameters (driver path and driver class) when using the JDBC hook in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/changelog.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# 1. Supply as constructor arguments\nhook = JdbcHook(driver_path='/path/to/driver', driver_class='com.example.Driver')\n\n# 2. Set in hook_params dictionary\nSQLOperator(\n    ...,\n    hook_params={'driver_path': '/path/to/driver', 'driver_class': 'com.example.Driver'}\n)\n\n# 3. Set in connection extras (requires configuration)\n# In airflow.cfg:\n# [providers.jdbc]\n# allow_driver_path_in_extra = True\n# allow_driver_class_in_extra = True\n\n# 4. Patch default values in local_settings.py\nJdbcHook.default_driver_path = '/path/to/driver'\nJdbcHook.default_driver_class = 'com.example.Driver'\n```\n\n----------------------------------------\n\nTITLE: Installing Snowflake Provider Extras with pip - Bash\nDESCRIPTION: Demonstrates how to install the apache-airflow-providers-snowflake Python package with optional 'common.compat' extra dependencies using pip. Prerequisites include Python 3.9 or higher and an existing Airflow 2.9.0+ installation. The command argument inside square brackets allows inclusion of related provider packages for extended functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-snowflake[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Installing Postgres Provider with Cross-Provider Dependencies\nDESCRIPTION: Example command to install the Apache Airflow Postgres provider along with optional cross-provider dependencies using pip extras. This specific example installs dependencies needed for integration with Amazon services by specifying the `[amazon]` extra.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-postgres[amazon]\n```\n\n----------------------------------------\n\nTITLE: Starting Release Candidate Process\nDESCRIPTION: This command initiates the release candidate process for Apache Airflow, requiring a GitHub token for issue generation.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nexport GITHUB_TOKEN=\"my_token\"\n\ngit checkout main\ngit pull # Ensure that the script is up-to-date\nbreeze release-management start-rc-process --version ${VERSION} --previous-version <PREVIOUS_VERSION>\n```\n\n----------------------------------------\n\nTITLE: Fixing DayOfWeekSensor Logical Date Condition\nDESCRIPTION: Commit message detailing a fix for the condition related to 'use_task_logical_date' in the 'DayOfWeekSensor'. References pull request #47825.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nFix DayOfWeekSensor use_task_logical_date condition (#47825)\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Provider with Dependencies in Apache Airflow\nDESCRIPTION: Command to install the Docker provider package with common compatibility dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/docker/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-docker[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Adding Deferrable Functionality to Airbyte Operators/Sensors (Commit Message)\nDESCRIPTION: Commit message announcing the addition of deferrable (asynchronous) functionality to the AirbyteJobSensor and AirbyteTriggerSyncOperator within the Apache Airflow Airbyte provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_24\n\nLANGUAGE: text\nCODE:\n```\nAdd deferrable functionality to the AirbyteJobSensor and AirbyteTriggerSyncOperator (#36780)\n```\n\n----------------------------------------\n\nTITLE: Running Airflow Database Migration Command (Bash)\nDESCRIPTION: This command executes the necessary database schema migrations after an Airflow core upgrade. It must be run manually to complete the migration process when upgrading Airflow to version 2.1.0 or higher, especially if the core package was upgraded automatically due to provider dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nairflow upgrade db\n```\n\n----------------------------------------\n\nTITLE: Operator Deprecations in Google Provider\nDESCRIPTION: Lists removed operators and their recommended replacements, primarily focused on Google Analytics, Campaign Manager, Display Video 360, and other Google services.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nDataprocSubmitPySparkJobOperator -> DataprocSubmitJobOperator\nGoogleAnalyticsListAccountsOperator -> GoogleAnalyticsAdminListAccountsOperator\nGoogleAnalyticsGetAdsLinkOperator -> GoogleAnalyticsAdminGetGoogleAdsLinkOperator\n```\n\n----------------------------------------\n\nTITLE: Enabling Amazon Tests (Boolean String)\nDESCRIPTION: A boolean flag indicating whether tests related to Amazon Web Services (AWS) should be run. 'true' enables these tests.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_19\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Initializing Airflow Database Schema\nDESCRIPTION: This bash command initializes the Airflow database schema after configuring the database connection.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-up-database.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nairflow db migrate\n```\n\n----------------------------------------\n\nTITLE: Configuring Extra Connection Parameters for Alibaba Cloud in Airflow (JSON)\nDESCRIPTION: This snippet provides an example of the required JSON structure for the \"Extra\" field when configuring an Alibaba Cloud connection in Apache Airflow. Dependencies include setting up a connection within Airflow using the \"oss_default\" or \"adb_spark_default\" IDs and providing valid Alibaba Cloud credentials. Parameters such as \"auth_type\", \"access_key_id\", and \"access_key_secret\" need to be filled with user-specific values. Inputs are JSON key-value pairs, and this configuration is consumed by Airflow hooks to authenticate against Alibaba Cloud.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/connections/alibaba.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"auth_type\": \"AK\",\n  \"access_key_id\": \"\",\n  \"access_key_secret\": \"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Pre-commit Git Hooks\nDESCRIPTION: Command to install and enable pre-commit hooks for git commit operations.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Installing Asana Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Asana integration, enabling Asana hooks and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[asana]'\n```\n\n----------------------------------------\n\nTITLE: Running Test Class in Breeze Shell\nDESCRIPTION: Example of running an entire test class using pytest in Breeze shell\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/core/test_core.py::TestCore\n```\n\n----------------------------------------\n\nTITLE: Setting Slack Connection via Environment Variable\nDESCRIPTION: Bash command showing how to set up the Slack Webhook connection using an environment variable. The URI includes the webhook token and timeout parameter with proper URL encoding.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/connections/slack-incoming-webhook.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SLACK_DEFAULT='slackwebhook://:T00000000%2FB00000000%2FXXXXXXXXXXXXXXXXXXXXXXXX@/?timeout=42'\n```\n\n----------------------------------------\n\nTITLE: Vision Product Update Template Fields\nDESCRIPTION: Template fields configuration for Product Update operator\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n[START vision_product_update_template_fields]\\n[END vision_product_update_template_fields]\n```\n\n----------------------------------------\n\nTITLE: Including Vertica Operator Python Example (reStructuredText)\nDESCRIPTION: This reStructuredText directive (`exampleinclude`) is used to embed a Python code example from an external file (`/../../vertica/tests/system/vertica/example_vertica.py`). The included code demonstrates the usage of the `SQLExecuteQueryOperator` for Vertica, specifically extracting the portion between the `[START howto_operator_vertica]` and `[END howto_operator_vertica]` markers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/vertica/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../vertica/tests/system/vertica/example_vertica.py\n    :language: python\n    :start-after: [START howto_operator_vertica]\n    :end-before: [END howto_operator_vertica]\n```\n\n----------------------------------------\n\nTITLE: Building Airflow from Own Requirements\nDESCRIPTION: Shell command demonstrating how to build Airflow Docker image with custom requirements specified in requirements.txt file using docker-context-files.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . \\\n    --build-arg DOCKER_CONTEXT_FILES=docker-context-files \\\n    --tag my-own-requirements-airflow:latest\n```\n\n----------------------------------------\n\nTITLE: Installing Google Provider Package for Apache Airflow with pip\nDESCRIPTION: Command to install the Google provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-google\n```\n\n----------------------------------------\n\nTITLE: Initializing a Local Virtual Environment for Airflow Development\nDESCRIPTION: Command to initialize a local virtual environment for Airflow development that can be used with an IDE. This enables features like debugging and code completion while working with Airflow code.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/tools/initialize_virtualenv.py\n```\n\n----------------------------------------\n\nTITLE: Handling Import Failure for Suspended Amazon Provider in Python\nDESCRIPTION: This snippet demonstrates how to handle an import failure when the Amazon provider has been suspended. It shows the error traceback and the recommended fix using conditional imports and custom exception handling.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/MANAGING_PROVIDERS_LIFECYCLE.rst#2025-04-22_snippet_9\n\nLANGUAGE: txt\nCODE:\n```\nTraceback (most recent call last):\n  File \"/opt/airflow/scripts/in_container/verify_providers.py\", line 266, in import_all_classes\n    _module = importlib.import_module(modinfo.name)\n  File \"/usr/local/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name, package, level)\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.8/site-packages/airflow/providers/mysql/transfers/s3_to_mysql.py\", line 23, in <module>\n    from airflow.providers.amazon.aws.hooks.s3 import S3Hook\nModuleNotFoundError: No module named 'airflow.providers.amazon'\n```\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    from airflow.providers.amazon.aws.hooks.s3 import S3Hook\nexcept ImportError as e:\n    from airflow.exceptions import AirflowOptionalProviderFeatureException\n\n    raise AirflowOptionalProviderFeatureException(e)\n```\n\n----------------------------------------\n\nTITLE: Deleting AlloyDB Cluster with Airflow Operator\nDESCRIPTION: Uses AlloyDBDeleteClusterOperator to delete an AlloyDB cluster from Google Cloud.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/alloy_db.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndelete_cluster = AlloyDBDeleteClusterOperator(\n    task_id=\"delete_cluster\",\n    project_id=GCP_PROJECT_ID,\n    cluster_id=CLUSTER_ID,\n    region=GCP_ALLOYDB_REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating JSON Connection Configuration in Python\nDESCRIPTION: Python code demonstrating how to generate a JSON connection representation using the Connection class's as_json property. Shows creation of a MySQL connection with various parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/connection.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from airflow.models.connection import Connection\\n>>> c = Connection(\\n...     conn_id=\"some_conn\",\\n...     conn_type=\"mysql\",\\n...     description=\"connection description\",\\n...     host=\"myhost.com\",\\n...     login=\"myname\",\\n...     password=\"mypassword\",\\n...     extra={\"this_param\": \"some val\", \"that_param\": \"other val*\"},\\n... )\\n>>> print(f\"AIRFLOW_CONN_{c.conn_id.upper()}='{c.as_json()}'\")\n```\n\n----------------------------------------\n\nTITLE: Testing DAG Using Airflow CLI Command\nDESCRIPTION: Example of using the Airflow CLI command to test a DAG with specific parameters. Shows the syntax for testing a DAG with a given dag_id and execution_date.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/dag_testing.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# airflow dags test [dag_id] [execution_date]\nairflow dags test example_branch_operator 2018-01-01\n```\n\n----------------------------------------\n\nTITLE: Specifying SQL Dialect in Airflow Connection Extras using YAML\nDESCRIPTION: This configuration snippet demonstrates how to explicitly specify the SQL dialect (e.g., 'mssql') to be used for an Airflow connection by setting the `dialect_name` parameter within the connection's extra options. This is useful when the dialect cannot be automatically derived from the connection string or when overriding the default behavior.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/dialects.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndialect_name: 'mssql'\n```\n\n----------------------------------------\n\nTITLE: Updating a Tag with CloudDataCatalogUpdateTagOperator in Python\nDESCRIPTION: Example of updating a tag in Google Cloud DataCatalog using the CloudDataCatalogUpdateTagOperator. Jinja templating can be used to dynamically determine parameter values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_gcp_datacatalog_update_tag]\nupdate_tag = CloudDataCatalogUpdateTagOperator(\n    task_id=\"update_tag\",\n    tag={\n        \"name\": TAG,\n        \"fields\": {\n            \"key\": {\n                \"string_value\": \"new-value\",\n            }\n        },\n    },\n    update_mask={\"paths\": [\"fields.key.string_value\"]},\n)\n# [END howto_operator_gcp_datacatalog_update_tag]\n```\n\n----------------------------------------\n\nTITLE: Defining Apache Airflow Docker Image Environment Variables\nDESCRIPTION: This snippet defines the default environment variables and file paths used in the Apache Airflow Docker images, including AIRFLOW_HOME and database connection settings.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nThe [`AIRFLOW_HOME`](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html#envvar-AIRFLOW_HOME) is set by default to ``/opt/airflow/`` - this means that dags\nare in default in the ``/opt/airflow/dags`` folder and logs are in the ``/opt/airflow/logs``\n\nThe working directory is ``/opt/airflow`` by default.\n\nIf no `AIRFLOW__DATABASE__SQL_ALCHEMY_CONN` variable is set then SQLite database is created in\n``${AIRFLOW_HOME}/airflow.db``.\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Celery Backend\nDESCRIPTION: Configuration for using a custom Celery backend instead of the default Redis.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nredis:\n  enabled: false\ndata:\n  brokerUrl: redis://redis-user:password@redis-host:6379/0\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry 1.1.1\nDESCRIPTION: RST formatted changelog entry for version 1.1.1 noting the removal of Python 3.7 support\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n1.1.1\n.....\n\n.. note::\n  This release dropped support for Python 3.7\n\nMisc\n~~~~\n\n* ``Remove Python 3.7 support (#30963)``\n```\n\n----------------------------------------\n\nTITLE: Adding Kerberos Credential Cache Support for SparkSubmitHook\nDESCRIPTION: A feature added in version 4.3.0 that enables the use of Kerberos credential cache with the SparkSubmitHook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"Add 'use_krb5ccache' option to 'SparkSubmitHook' (#34386)\"\n```\n\n----------------------------------------\n\nTITLE: Example Vertica Connection Extras (Session Label, Timeout, Failover)\nDESCRIPTION: This JSON snippet provides another example configuration for the 'Extra' field in an Airflow Vertica connection. It sets a 'session_label' for identifying the session on the server, configures a 'connection_timeout' of 30 seconds for socket operations, and specifies a list of 'backup_server_node' addresses for connection failover. These parameters correspond to options available in the 'vertica-python' driver.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/vertica/docs/connections/vertica.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"session_label\": \"airflow-session\",\n   \"connection_timeout\": 30,\n   \"backup_server_node\": [\"bck_server_1\", \"bck_server_2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling EventBridge Rules using EnableRuleOperator\nDESCRIPTION: Shows how to enable an existing rule in Amazon EventBridge using EventBridgeEnableRuleOperator. This operator activates rules to allow event processing.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/eventbridge.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nenable_rule = EventBridgeEnableRuleOperator(\n    task_id=\"enable_rule\",\n    name=\"test-rule\",\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Yandex Lockbox Backend via Environment Variable (Bash)\nDESCRIPTION: Sets the `AIRFLOW__SECRETS__BACKEND` environment variable to enable the Yandex Lockbox secret backend. This provides an alternative to modifying the `airflow.cfg` file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__SECRETS__BACKEND=airflow.providers.yandex.secrets.lockbox.LockboxSecretBackend\n```\n\n----------------------------------------\n\nTITLE: Building Provider Documentation Locally - Bash - bash\nDESCRIPTION: Shows the two bash commands used to build documentation for a specific Airflow provider and the main project. These commands utilize the 'breeze' CLI tool, assuming it and necessary dependencies are installed. The first command builds documentation for the given provider, while the second ensures the main Airflow docs build with provider integrationexpected to be run from the project root.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/MANAGING_PROVIDERS_LIFECYCLE.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs <provider id>\nbreeze build-docs apache-airflow\n```\n\n----------------------------------------\n\nTITLE: Importing PGP Keys for Apache Airflow Verification\nDESCRIPTION: These commands import the PGP keys used to sign Apache Airflow releases. Users can choose between gpg, pgpk, or pgp commands depending on their system setup.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngpg -i KEYS\n```\n\nLANGUAGE: bash\nCODE:\n```\npgpk -a KEYS\n```\n\nLANGUAGE: bash\nCODE:\n```\npgp -ka KEYS\n```\n\n----------------------------------------\n\nTITLE: Configuring Yandex Lockbox with Service Account Key JSON in Airflow (INI)\nDESCRIPTION: Illustrates configuring `backend_kwargs` in `airflow.cfg` to authenticate using a Yandex Cloud service account key provided directly as a JSON string within the configuration. Requires `folder_id` and the `yc_sa_key_json` containing the key details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend_kwargs = {\"folder_id\": \"b1g66mft1vo1n4vbn57j\", \"yc_sa_key_json\": {\"id\": \"...\", \"service_account_id\": \"...\", \"private_key\": \"...\"}\"}\n```\n\n----------------------------------------\n\nTITLE: Running a Google Cloud Tasks Task in Python\nDESCRIPTION: This snippet demonstrates how to run a task in a Google Cloud Tasks queue using the CloudTasksTaskRunOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# [START run_task]\nCloudTasksTaskRunOperator(\n    task_id=\"run_task\",\n    location=LOCATION,\n    queue_name=QUEUE_ID,\n    task_name=TASK_NAME,\n).execute(context=context)\n# [END run_task]\n```\n\n----------------------------------------\n\nTITLE: Creating Writable Directory in Airflow Image (Dockerfile)\nDESCRIPTION: Dockerfile example demonstrating how to create a directory within the image that needs to be writable by the arbitrary user running the container. It uses `umask 0002` before creating the directory to ensure correct group write permissions (GID=0).\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_16\n\nLANGUAGE: Dockerfile\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/writable-directory/Dockerfile\n    :language: Dockerfile\n    :start-after: [START Dockerfile]\n    :end-before: [END Dockerfile]\n```\n\n----------------------------------------\n\nTITLE: Running Airflow CLI Commands in Docker\nDESCRIPTION: Example of running Airflow CLI commands within Docker containers using docker compose\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose run airflow-worker airflow info\n```\n\n----------------------------------------\n\nTITLE: GitHub Container Registry Naming Convention\nDESCRIPTION: Naming convention for Apache Airflow Docker images in GitHub Container Registry. This shows the pattern for both CI and production images, with branch and Python version placeholders.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nghcr.io/apache/airflow/<BRANCH>/ci/python<X.Y>         - for CI images\nghcr.io/apache/airflow/<BRANCH>/prod/python<X.Y>       - for production images\n```\n\n----------------------------------------\n\nTITLE: Deleting a Job on GKE using GKEDeleteJobOperator in Python\nDESCRIPTION: This snippet demonstrates using the `GKEDeleteJobOperator` to delete a specified Job from a GKE cluster. Similar to `GKEStartJobOperator`, it extends the base `KubernetesDeleteJobOperator` and handles Google Cloud authentication automatically, removing the need for manual kube_config management.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/kubernetes_engine/example_kubernetes_engine_job.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_gke_delete_job]\n    :end-before: [END howto_operator_gke_delete_job]\n```\n\n----------------------------------------\n\nTITLE: Monitoring SageMaker Transform Job State in Python\nDESCRIPTION: This code snippet shows how to use SageMakerTransformOperator to check the state of an Amazon SageMaker transform job until it reaches a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_sensor_sagemaker_transform]\n# Code snippet not provided in the original text\n# [END howto_sensor_sagemaker_transform]\n```\n\n----------------------------------------\n\nTITLE: Installing Google Provider Package in Python\nDESCRIPTION: Command to install the Google provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-google\n```\n\n----------------------------------------\n\nTITLE: Installing Cloudant Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Cloudant provider package for Apache Airflow using pip. This package allows integration with IBM Cloudant in Airflow workflows.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cloudant/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-cloudant\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Livy Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Apache Livy provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/livy/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-livy\n```\n\n----------------------------------------\n\nTITLE: Retrieving Listed Instances\nDESCRIPTION: Example showing how to retrieve the list of instances using XCom\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ninstances = \"{{ task_instance.xcom_pull('list-instances')['instances'] }}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Task Publish Max Retries - Python Configuration\nDESCRIPTION: Configuration settings for task publishing retries in case of Kube API quota errors. The default is 0 (no retries), -1 for unlimited retries, or a positive integer for fixed number of retries.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntask_publish_max_retries = 0  # default - no retries\ntask_publish_max_retries = -1  # unlimited retries\ntask_publish_max_retries = 5  # fixed number of retries\n```\n\n----------------------------------------\n\nTITLE: Sensing Dataproc Metastore Hive Partitions using Airflow Sensor in Python\nDESCRIPTION: Illustrates the usage of the `MetastoreHivePartitionSensor` in an Airflow DAG. This sensor waits until specified Hive partitions exist in a table within the target Dataproc Metastore service. Requires `service_id`, `location`, `project_id`, `table`, and a list of `partitions` to check.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncheck_metastore_partition = MetastoreHivePartitionSensor(\n    task_id=\"check_metastore_partition\",\n    service_id=SERVICE_ID,\n    location=REGION,\n    project_id=PROJECT_ID,\n    table=\"partitioned_table\",\n    partitions=[f\"ds={{{{ ds }}}} / hour={{{{ execution_date.hour }}}}\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Migrating from days_ago to Pendulum UTC Date Calculation\nDESCRIPTION: Example of replacing the deprecated days_ago method with the recommended Pendulum alternative for calculating past dates in UTC timezone.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41496.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npendulum.today(\"UTC\").add(days=-N, ...)\n```\n\n----------------------------------------\n\nTITLE: Checking Email Backend Configuration (Bash)\nDESCRIPTION: This command shows how to check the currently set email backend in Airflow using the command line interface.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ airflow config get-value email email_backend\nairflow.utils.email.send_email_smtp\n```\n\n----------------------------------------\n\nTITLE: Displaying Airflow Provider Commits (Dec 2023 - v1.10.0) using RST Table\nDESCRIPTION: This reStructuredText (RST) snippet formats a list of commits related to Apache Airflow provider version 1.10.0 (December 2023, 2nd wave) into a table. It includes the commit hash (linked to the specific GitHub commit), the commit date, and the commit subject, utilizing RST's table structure, hyperlink syntax (`<link>`__), and inline code formatting (``code``).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`b15d5578da <https://github.com/apache/airflow/commit/b15d5578dac73c4c6a3ca94d90ab0dc9e9e74c9c>`__  2023-12-23   ``Re-apply updated version numbers to 2nd wave of providers in December (#36380)``\n`f5883d6e7b <https://github.com/apache/airflow/commit/f5883d6e7be83f1ab9468e67164b7ac381fdb49f>`__  2023-12-23   ``Prepare 2nd wave of providers in December (#36373)``\n`5fe5d31a46 <https://github.com/apache/airflow/commit/5fe5d31a46885fbb2fb6ba9c0bd551a6b57d129a>`__  2023-12-22   ``Return common data structure in DBApi derived classes``\n`f84eb2ab6f <https://github.com/apache/airflow/commit/f84eb2ab6fe777938f85a5fbb2a0b8a6dc07b9bc>`__  2023-12-21   ``Make \"placeholder\" of ODBC configurable in UI (#36000)``\n`5c1d8f40a1 <https://github.com/apache/airflow/commit/5c1d8f40a10b3e0beb1cae70d301fe704e64ab0e>`__  2023-12-20   ``SQLCheckOperator fails if returns dict with any False values (#36273)``\n==================================================================================================  ===========  ==================================================================================\n```\n\n----------------------------------------\n\nTITLE: Configuring DAG with Default Schedule\nDESCRIPTION: When creating a DAG without specifying the 'schedule' parameter, it now defaults to None instead of a previous default value. The DAG remains triggerable manually or via TriggerDagRunOperator but won't run on any automatic schedule.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/24842.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDAG()\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Update Telegram Provider Compatibility\nDESCRIPTION: This commit message, linked to commit 644cea14ff dated 2023-02-23, describes an update to the Telegram Provider to ensure compatibility with versions 20.0.0 and higher of the underlying library.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_18\n\nLANGUAGE: text\nCODE:\n```\n``Updated Telegram Provider to ensure compatbility with >=20.0.0 (#28953)``\n```\n\n----------------------------------------\n\nTITLE: Enabling Black Python Auto Formatter in Airflow\nDESCRIPTION: This commit message indicates the enablement of the Black Python auto-formatter tool across the Airflow project codebase. It references pull request #9550.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_64\n\nLANGUAGE: text\nCODE:\n```\nEnable Black - Python Auto Formmatter (#9550)\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow from PyPI with Constraints\nDESCRIPTION: Command to install Apache Airflow from PyPI with specific version and constraints file, including Google and Amazon providers with async support.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/13_airflow_dependencies_and_extras.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"apache-airflow[google,amazon,async]==2.2.5\" \\\n  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.2.5/constraints-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Extending Airflow Docker Image with Custom Script\nDESCRIPTION: Dockerfile example showing how to extend the official Airflow image by adding a custom script that runs after the entrypoint completes its setup process.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_7\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM airflow:2.9.0.dev0\nCOPY my_after_entrypoint_script.sh /\n```\n\n----------------------------------------\n\nTITLE: Updating a GCP Transfer Job in Python\nDESCRIPTION: Example of updating a Google Cloud Platform transfer job using the CloudDataTransferServiceUpdateJobOperator in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbody = {\n    \"transferJob\": {\n        \"description\": \"A modified GCP transfer job description\"\n    },\n    \"updateTransferJobFieldMask\": \"description\"\n}\n```\n\n----------------------------------------\n\nTITLE: Migration Checklist for Airflow Plugin Changes\nDESCRIPTION: Markdown checklist detailing the types of changes and migration rules needed for removing executor plugin support. Includes Ruff rule AIR302 affecting executors, operators, and sensors extensions.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43289.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* Types of change\n\n  * [ ] Dag changes\n  * [ ] Config changes\n  * [ ] API changes\n  * [ ] CLI changes\n  * [ ] Behaviour changes\n  * [x] Plugin changes\n  * [ ] Dependency changes\n  * [ ] Code interface changes\n\n* Migration rules needed\n\n  * ruff\n\n    * AIR302\n\n      * [x] extension ``executors`` in ``airflow.plugins_manager.AirflowPlugin``\n      * [x] extension ``operators`` in ``airflow.plugins_manager.AirflowPlugin``\n      * [x] extension ``sensors`` in ``airflow.plugins_manager.AirflowPlugin``\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Spark Provider via pip\nDESCRIPTION: Command to install the Apache Spark provider package on top of an existing Airflow 2 installation\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-spark\n```\n\n----------------------------------------\n\nTITLE: Installing Common-SQL Extras for Apache Airflow\nDESCRIPTION: Command to install Core SQL Operators for Apache Airflow. This provides basic SQL functionality and is preinstalled with Airflow by default.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_57\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[common-sql]'\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Airflow Database Configuration\nDESCRIPTION: This bash snippet demonstrates how to set environment variables for Airflow database configuration, specifically for using a PostgreSQL database with a custom schema.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-up-database.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=\"postgresql://postgres@localhost:5432/my_database?options=-csearch_path%3Dairflow\"\nexport AIRFLOW__DATABASE__SQL_ALCHEMY_SCHEMA=\"airflow\"\n```\n\n----------------------------------------\n\nTITLE: Building CI Docker Image with GCP Extras - Docker/Bash\nDESCRIPTION: This bash snippet builds a CI Docker image for Apache Airflow 3.9 using the 'gcp' extra only (Google Cloud Platform integrations). The AIRFLOW_EXTRAS build argument restricts installed extras to 'gcp', reducing the image size and dependency set. Dependencies include Docker, Dockerfile.ci, and Airflow source files.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build . -f Dockerfile.ci \\\n  --pull \\\n  --build-arg PYTHON_BASE_IMAGE=\"python:3.9-slim-bookworm\" \\\n  --build-arg AIRFLOW_EXTRAS=gcp --tag my-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Verifying Provider Distributions\nDESCRIPTION: Commands to verify if all providers are properly named and importable, with optional Airflow version specification for compatibility testing.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/PROVIDER_DISTRIBUTIONS_DETAILS.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management verify-provider-distributions\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management verify-provider-distributions --use-airflow-version 2.1.0\n```\n\n----------------------------------------\n\nTITLE: Running All Complete Kubernetes Tests Using Breeze - Bash\nDESCRIPTION: This bash command triggers the complete test workflow via the `breeze k8s run-complete-tests` command, automating cluster creation, Airflow deployment, test execution, and cluster cleanup. Primarily used in CI to guarantee a clean lifecycle for full integration runs. Requires properly set up environment and Breeze.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s run-complete-tests\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Pull Request Analysis in Python\nDESCRIPTION: This snippet imports necessary libraries for processing pull request data. It adds the current directory to the system path and imports pickle for loading data, pandas for data manipulation, and a custom PrStat class.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/stats/explore_pr_candidates.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsys.path.append(\".\")\n\nimport pickle\nimport pandas as pd\nfrom get_important_pr_candidates import PrStat\n```\n\n----------------------------------------\n\nTITLE: Setting OpenLineage Execution Timeout Using Environment Variable\nDESCRIPTION: Example of setting the OpenLineage execution timeout using an environment variable instead of airflow.cfg configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_10\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__EXECUTION_TIMEOUT=60\n```\n\n----------------------------------------\n\nTITLE: Running a Specific Test with Additional Pytest Flags Using Breeze - Bash\nDESCRIPTION: This command runs the `test_kubernetes_executor.py` test while suppressing Pytest's output capture (`-s` flag), using the Breeze interface. The command demonstrates passing parameters after `--` to allow for custom Pytest or shell flags during test invocation.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s tests -- test_kubernetes_executor.py -s\n```\n\n----------------------------------------\n\nTITLE: Running a Specific Complete Test with Additional Pytest Flags Using Breeze - Bash\nDESCRIPTION: This command selectively runs the `test_kubernetes_executor.py` test as part of the complete test workflow and passes the `-s` flag to Pytest for output display. Demonstrates advanced parameter passing with Breeze for customized test execution.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s run-complete-tests -- test_kubernetes_executor.py -s\n```\n\n----------------------------------------\n\nTITLE: Implementing CLI Commands for Custom Executor in Python\nDESCRIPTION: Example of how to implement the get_cli_commands method to vend CLI commands for a custom Airflow executor. This method returns a list of GroupCommand objects containing subcommands for the executor.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/index.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\ndef get_cli_commands() -> list[GroupCommand]:\n    sub_commands = [\n        ActionCommand(\n            name=\"command_name\",\n            help=\"Description of what this specific command does\",\n            func=lazy_load_command(\"path.to.python.function.for.command\"),\n            args=(),\n        ),\n    ]\n\n    return [\n        GroupCommand(\n            name=\"my_cool_executor\",\n            help=\"Description of what this group of commands do\",\n            subcommands=sub_commands,\n        ),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Generating Airflow Provider Constraints for Compatibility Tests (Bash)\nDESCRIPTION: This bash command uses the Breeze release-management tool to generate provider constraints needed for compatibility testing, enforcing dependency management concordant with the selected source/provider releases. The command includes flags for specifying constraints mode and automatically answering prompts, and is typically run ahead of test/install commands.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management generate-constraints --airflow-constraints-mode constraints-source-providers --answer yes\n```\n\n----------------------------------------\n\nTITLE: Displaying Airflow Help in Docker Container\nDESCRIPTION: Shows how to view the Airflow help menu within the Docker container using the default entrypoint.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it apache/airflow:3.1.0.dev0-python3.9 help\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Connection String Format for AWS Batch\nDESCRIPTION: Demonstrates the format for PostgreSQL connection string used to connect Airflow tasks to the RDS database. The string includes username, password, endpoint, and database name parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/executors/batch-executor.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npostgresql+psycopg2://<username>:<password>@<endpoint>/<database_name>\n```\n\n----------------------------------------\n\nTITLE: Configuring In-Memory SQLite Connection in Airflow URI Format\nDESCRIPTION: This snippet illustrates how to set up an in-memory SQLite database connection using the Airflow URI format. The 'mode' parameter is set to read-only.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/connections/sqlite.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SQLITE_DEFAULT='sqlite://?mode=ro'\n```\n\n----------------------------------------\n\nTITLE: Semicolon Stripping Support for DbApiHook\nDESCRIPTION: Adds support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook. This enhances SQL query compatibility and processing across different database systems.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nAdd support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)\n```\n\n----------------------------------------\n\nTITLE: Configuring ECS Operator for Realtime Logging in Python\nDESCRIPTION: Implementation of realtime logging for the ECS Operator, allowing for more immediate feedback during task execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nECSOperator realtime logging (#17626)\n```\n\n----------------------------------------\n\nTITLE: Creating ObjectStoragePath in Airflow\nDESCRIPTION: Demonstrates how to create an ObjectStoragePath object for interacting with cloud storage. Shows URL syntax and connection handling.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/objectstorage.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nObjectStoragePath(\"s3://airflow-tutorial-data/\", conn_id=\"aws_default\")\n```\n\n----------------------------------------\n\nTITLE: Installing the Core Amazon Provider Package via pip\nDESCRIPTION: This command installs the `apache-airflow-providers-amazon` package using pip. It requires an existing Airflow 2 installation (version 2.9.0 or higher). This installs the base functionality for interacting with various AWS services within Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-amazon\n```\n\n----------------------------------------\n\nTITLE: Installing the OpenFaaS Provider via pip (Bash)\nDESCRIPTION: This command demonstrates how to install the `apache-airflow-providers-openfaas` package using pip. This command should be executed in a shell environment where pip is configured for the target Airflow installation. It installs the provider package on top of an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openfaas/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-openfaas\n```\n\n----------------------------------------\n\nTITLE: Installing Vertica Provider with Extras via pip - Bash\nDESCRIPTION: This snippet demonstrates the installation of the Apache Airflow Vertica provider package along with the optional 'common.sql' extra dependency using the pip package manager. Requires Python and pip to be installed. The command expects Airflow 2 to be already present, and adds any needed extras for enhanced SQL capabilities. The expected input is a shell command; successful output installs the package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/vertica/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-vertica[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Remote Logging Settings in airflow.cfg\nDESCRIPTION: Configuration settings required in airflow.cfg to enable remote logging to HDFS. Includes settings for enabling remote logging, specifying the HDFS path, and setting up the connection ID for WebHDFS access.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/logging/hdfs-task-handler.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\n# Airflow can store logs remotely in HDFS. Users must supply a remote\n# location URL (starting with either 'hdfs://...') and an Airflow connection\n# id that provides access to the storage location.\nremote_logging = True\nremote_base_log_folder = hdfs://some/path/to/logs\nremote_log_conn_id = webhdfs_default\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Connection Parameters in Apache Airflow\nDESCRIPTION: This snippet lists the required and optional parameters for configuring a JDBC connection in Apache Airflow. It includes details on host, schema, login, password, port, and extra parameters such as driver_class and driver_path.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/connections/jdbc.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nHost (required)\n    The host to connect to.\n\nSchema (required)\n    Specify the database name to be used in.\n\nLogin (required)\n    Specify the user name to connect to.\n\nPassword (required)\n    Specify the password to connect to.\n\nPort (optional)\n    Port of host to connect to. Not user in ``JdbcOperator``.\n\nExtra (optional)\n    Specify the extra parameters (as json dictionary) that can be used in JDBC connection. The following parameters out of the standard python parameters are supported:\n\n    - ``driver_class``\n        * Full qualified Java class name of the JDBC driver. For ``JdbcOperator``.\n          Note that this is only considered if ``allow_driver_class_in_extra`` is set to True in airflow config section\n          ``providers.jdbc`` (by default it is not considered).  Note: if setting this config from env vars, use\n          ``AIRFLOW__PROVIDERS_JDBC__ALLOW_DRIVER_CLASS_IN_EXTRA=true``.\n\n    - ``driver_path``\n        * Jar filename or sequence of filenames for the JDBC driver libs. For ``JdbcOperator``.\n          Note that this is only considered if ``allow_driver_path_in_extra`` is set to True in airflow config section\n          ``providers.jdbc`` (by default it is not considered).  Note: if setting this config from env vars, use\n          ``AIRFLOW__PROVIDERS_JDBC__ALLOW_DRIVER_PATH_IN_EXTRA=true``.\n```\n\n----------------------------------------\n\nTITLE: Documenting Version 2.0.2 Changes in RST\nDESCRIPTION: A reStructuredText section outlining the changes in version 2.0.2 of the DingTalk provider. It notes the addition of support for Python 3.10.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n2.0.2\n.....\n\nMisc\n~~~~\n\n* ``Support for Python 3.10``\n```\n\n----------------------------------------\n\nTITLE: Installing ODBC Provider with SQL Dependencies\nDESCRIPTION: Command to install the ODBC provider package with its common SQL dependencies via pip\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-odbc[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Python Code Property Update\nDESCRIPTION: Configuration property update removing the deprecated oauth_whitelists in favor of oauth_allow_list.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\noauth_whitelists -> oauth_allow_list\n```\n\n----------------------------------------\n\nTITLE: Setting Flower URL Prefix via Command Line\nDESCRIPTION: Shows how to configure a URL prefix for Flower deployment using command line arguments.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/flower.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairflow celery flower --url-prefix=flower\n```\n\n----------------------------------------\n\nTITLE: Removing 'args' Parameter from Airflow Provider Operator Constructors\nDESCRIPTION: This commit message specifically targets provider operators, removing the 'args' parameter from their constructors, consistent with the broader change mentioned in #10163. It references pull request #10097.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_81\n\nLANGUAGE: text\nCODE:\n```\nRemove 'args' parameter from provider operator constructors (#10097)\n```\n\n----------------------------------------\n\nTITLE: Generating Social Media Announcement\nDESCRIPTION: Command to generate announcement message for social media platforms.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\nWe've just released Apache Airflow Helm chart ${VERSION} \n\n ArtifactHub: https://artifacthub.io/packages/helm/apache-airflow/airflow\n Docs: https://airflow.apache.org/docs/helm-chart/${VERSION}/\n Release Notes: https://airflow.apache.org/docs/helm-chart/${VERSION}/release_notes.html\n\nThanks to all the contributors who made this possible.\nEOF\n```\n\n----------------------------------------\n\nTITLE: Building Production Image with Cache Disabled\nDESCRIPTION: Command for building a production image with caching disabled, which forces rebuilding the image from scratch. This is similar to the approach used in scheduled CI builds.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nbreeze prod-image build --python 3.9 --docker-cache disabled\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with Helm\nDESCRIPTION: Commands to add the Apache Airflow Helm repository and install Airflow on a Kubernetes cluster with default configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add apache-airflow https://airflow.apache.org\nhelm upgrade --install airflow apache-airflow/airflow --namespace airflow --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-pgvector with Extra Dependencies using pip - Bash\nDESCRIPTION: This Bash snippet shows how to install the 'apache-airflow-providers-pgvector' package along with its 'common.sql' extra dependencies using pip. The installation command assumes you have an existing Airflow environment set up. The key parameter is '[common.sql]', which ensures that extra provider integrations required for SQL operations are installed. No output is displayed unless pip encounters errors. Make sure you have pip and Python environment configured beforehand.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-pgvector[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Installing HTTP Extras for Apache Airflow\nDESCRIPTION: Command to install HTTP hooks, operators and sensors for Apache Airflow. This enables HTTP-based integrations and is preinstalled with Airflow by default.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_60\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[http]'\n```\n\n----------------------------------------\n\nTITLE: Running Kubernetes Tests with uv\nDESCRIPTION: Command for running Kubernetes tests using the uv runner with environment variables from .env file.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nuv run --env .env pytest tests/kubernetes_tests/test_kubernetes_executor.py::TestKubernetesExecutor::test_integration_run_dag_with_scheduler_failure -s\n```\n\n----------------------------------------\n\nTITLE: Storing Variables in .env Format\nDESCRIPTION: This snippet shows how to store variables in a .env file. Each line contains a variable name and its corresponding value.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/secrets-backend/local-filesystem-secrets-backend.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nVAR_A=some_value\nvar_B=different_value\n```\n\n----------------------------------------\n\nTITLE: Refactoring Provider Security Documentation\nDESCRIPTION: Modifies the provider security documentation (`security.rst`) to utilize includes, likely for better modularity and consistency, detailed in issue #35435.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nChange security.rst to use includes in providers (#35435)\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Image with Additional Extras and Dependencies\nDESCRIPTION: Builds a production Airflow image using Python 3.9, Airflow version 2.3.0 from PyPI with additional extras (mssql, hdfs) and an extra dependency (oauth2client).\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . -f Dockerfile --pull --build-arg PYTHON_BASE_IMAGE=\"python:3.9-slim-bullseye\" --build-arg AIRFLOW_VERSION=\"2.3.0\" --build-arg ADDITIONAL_AIRFLOW_EXTRAS=\"mssql,hdfs\" --build-arg ADDITIONAL_PYTHON_DEPS=\"oauth2client\" --tag \"my-company/airflow:2.3.0-python3.9-mssql-hdfs\"\n```\n\n----------------------------------------\n\nTITLE: Building and Preparing Airflow RC Packages - Shell Script\nDESCRIPTION: This shell script snippet sets up environment variables, checks out the specified Airflow release branch, and builds all distribution packages using Airflow's 'breeze' CLI commands. It ensures reproducible builds for binary and source packages. The script should be run from the Airflow git repository root with 'breeze' and optionally 'hatch' installed and available in the environment. Inputs include VERSION; outputs are build artifacts in the dist/ folder.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nVERSION=X.Y.Zrc1\ngit checkout ${VERSION}\nexport AIRFLOW_REPO_ROOT=$(pwd)\nrm -rf dist/*\nbreeze release-management prepare-airflow-distributions --distribution-format both\nbreeze release-management prepare-airflow-tarball --version ${VERSION}\n```\n\n----------------------------------------\n\nTITLE: Example requirements.txt for Airflow Image\nDESCRIPTION: An example `requirements.txt` file intended to be used with the corresponding Dockerfile example for installing multiple PyPI packages into the Airflow image.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/add-requirement-packages/requirements.txt\n    :language: text\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Image with mpi4py Package\nDESCRIPTION: Builds a production Airflow image that includes the mpi4py package, which requires both build-essential and mpi compiler.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . --pull --build-arg ADDITIONAL_DEV_APT_DEPS=\"mpich\" --build-arg ADDITIONAL_RUNTIME_APT_DEPS=\"mpich\" --build-arg ADDITIONAL_PYTHON_DEPS=\"mpi4py==3.1.4\" -t my-image:my-image-tag\n```\n\n----------------------------------------\n\nTITLE: Installing the Qdrant Provider Package via pip\nDESCRIPTION: This shell command installs the `apache-airflow-providers-qdrant` package using the Python package installer, pip. This command should be run in an environment where Apache Airflow 2 (version 2.9.0 or higher) is already installed. It adds the necessary classes and functionalities for interacting with Qdrant within Airflow DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/qdrant/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-qdrant\n```\n\n----------------------------------------\n\nTITLE: Using AwaitMessageSensor in Apache Airflow with Kafka\nDESCRIPTION: This snippet demonstrates how to use the AwaitMessageSensor in an Apache Airflow DAG. It sets up a sensor that waits for a specific message in a Kafka topic before proceeding.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/sensors.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nAwaitMessageSensor(\n    task_id=\"awaiting_message\",\n    kafka_config_id=\"kafka_default\",\n    topic=\"sensor-topic\",\n    apply_function=lambda x: x if x == b\"test message\" else None,\n    poke_interval=0,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Drill Provider Package in Python\nDESCRIPTION: Command to install the Apache Drill provider package for Apache Airflow using pip. This package can be installed on top of an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-drill\n```\n\n----------------------------------------\n\nTITLE: Running Pytest Provider Tests (Bash)\nDESCRIPTION: This bash command executes pytest on a specified provider test file or directory within the Airflow testing environment. It assumes the Breeze shell is already running and the necessary providers/tests are available from sources. Required arguments should specify the path to the desired test file (e.g., providers/<provider>/tests/.../test.py). Output includes standard Pytest pass/fail results.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\npytest providers/<provider>/tests/.../test.py\n```\n\n----------------------------------------\n\nTITLE: Updating Translation Glossary with TranslateUpdateGlossaryOperator in Python\nDESCRIPTION: This snippet demonstrates the basic usage of TranslateUpdateGlossaryOperator to update a translation glossary using the Cloud Translate API. It allows updating the display_name and input_config fields, effectively updating the glossary dictionary.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nupdate_op = TranslateUpdateGlossaryOperator(\n    task_id=\"update_glossary\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    glossary_id=GLOSSARY_ID,\n    display_name=DISPLAY_NAME,\n    input_config={\n        \"gcs_source\": {\"input_uri\": \"gs://cloud-samples-data/translation/glossary_ja.csv\"}\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Configuration Using Environment Variables\nDESCRIPTION: Environment variables can be used to override Airflow configuration settings. The naming convention follows AIRFLOW__{SECTION}__{KEY} pattern to set options in the Airflow configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/cli-and-env-variables-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW__CORE__DAGS_FOLDER\n```\n\n----------------------------------------\n\nTITLE: Running Helm Repository Update Before Chart Upgrade\nDESCRIPTION: Command to update the Helm repository before upgrading the chart to the latest version.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/RELEASE_NOTES.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Improved Test Collection with Delayed Object Creation in Python\nDESCRIPTION: This code snippet shows an improved way of writing tests with database dependencies. The TaskInstance creation is moved inside the test function to avoid collection failures.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npytestmark = pytest.mark.db_test\n\n\nclass TestCallbackRequest:\n    @pytest.mark.parametrize(\n        \"input,request_class\",\n        [\n            (CallbackRequest(full_filepath=\"filepath\", msg=\"task_failure\"), CallbackRequest),\n            (\n                None,  # to be generated when test is run\n                TaskCallbackRequest,\n            ),\n            (\n                DagCallbackRequest(\n                    full_filepath=\"filepath\",\n                    dag_id=\"fake_dag\",\n                    run_id=\"fake_run\",\n                    is_failure_callback=False,\n                ),\n                DagCallbackRequest,\n            ),\n            (\n                SlaCallbackRequest(\n                    full_filepath=\"filepath\",\n                    dag_id=\"fake_dag\",\n                ),\n                SlaCallbackRequest,\n            ),\n        ],\n    )\n    def test_from_json(self, input, request_class):\n        if input is None:\n            ti = TaskInstance(\n                task=BashOperator(\n                    task_id=\"test\", bash_command=\"true\", dag=DAG(dag_id=\"id\"), start_date=datetime.now()\n                ),\n                run_id=\"fake_run\",\n                state=State.RUNNING,\n            )\n\n            input = TaskCallbackRequest(\n                full_filepath=\"filepath\",\n                simple_task_instance=SimpleTaskInstance.from_ti(ti=ti),\n                is_failure_callback=True,\n            )\n```\n\n----------------------------------------\n\nTITLE: Implementing AssetOrTimeSchedule in Airflow DAG\nDESCRIPTION: Example showing how to use AssetOrTimeSchedule to create a DAG that runs based on both time-based schedules and asset events. The schedule combines a CronTriggerTimetable with asset dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timetable.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.timetables.assets import AssetOrTimeSchedule\nfrom airflow.timetables.trigger import CronTriggerTimetable\n\n\n@dag(\n    schedule=AssetOrTimeSchedule(\n        timetable=CronTriggerTimetable(\"0 1 * * 3\", timezone=\"UTC\"), assets=(dag1_asset & dag2_asset)\n    )\n    # Additional arguments here, replace this comment with actual arguments\n)\ndef example_dag():\n    # DAG tasks go here\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring Worker-Specific Secrets Backend\nDESCRIPTION: INI configuration for setting up a separate secrets backend specifically for Airflow workers.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/secrets-backend/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[workers]\nsecrets_backend =\nsecrets_backend_kwargs =\n```\n\n----------------------------------------\n\nTITLE: Installing Amazon Provider with Cross-Provider Extras via pip (Bash)\nDESCRIPTION: This command demonstrates installing the `apache-airflow-providers-amazon` package along with optional 'extra' dependencies required for specific cross-provider interactions. The example specifically shows how to include dependencies for Apache Hive integration by specifying `[apache.hive]`. Running this installs the base Amazon provider plus any packages needed for the Hive-related features within the Amazon provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-amazon[apache.hive]\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Cloud Provider Package via pip\nDESCRIPTION: Command to install the dbt Cloud provider package on top of an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-dbt-cloud\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure File Share Connection Using Environment Variable with SAS Token\nDESCRIPTION: Example of setting up an Azure File Share connection using an environment variable with URI syntax. This specific example demonstrates connecting with a SAS token authentication method, where the components of the URI must be URL-encoded.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/azure_fileshare.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_WASP_DEFAULT='azure_fileshare://blob%20username@myblob.com?sas_token=token'\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Custom Airflow Operators Package with pip\nDESCRIPTION: Provides the command to uninstall a custom Airflow operators package using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/modules_management.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall airflow_operators\n```\n\n----------------------------------------\n\nTITLE: Creating YDB Table Structure\nDESCRIPTION: SQL schema definition for creating a pet table with columns for pet_id, name, pet_type, birth_date, and owner.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/docs/operators/ydb_operator_howto_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE pet (\n  pet_id INT,\n  name TEXT NOT NULL,\n  pet_type TEXT NOT NULL,\n  birth_date TEXT NOT NULL,\n  owner TEXT NOT NULL,\n  PRIMARY KEY (pet_id)\n);\n```\n\n----------------------------------------\n\nTITLE: Running Apache RAT License Check on Airflow Sources - Shell Script\nDESCRIPTION: This shell script command runs the Apache RAT license compliance check on the unpacked source distribution. Requires Java and the 'apache-rat-0.13.jar' tool downloaded from the official site, and the presence of a .rat-excludes file in the source tree. Executes the license analysis in the provided directory, outputting compliance results to the console.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\njava -jar ../../apache-rat-0.13/apache-rat-0.13.jar -E .rat-excludes -d .\n```\n\n----------------------------------------\n\nTITLE: Migrating Kerberos Auth Backend Imports in Apache Airflow\nDESCRIPTION: This snippet provides migration rules for updating deprecated Kerberos authentication backend imports to the new path. It uses the AIR303 rule identifier for the migration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41693.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n\"airflow.api.auth.backend.kerberos_auth\"  \"airflow.providers.fab.auth_manager.api.auth.backend.kerberos_auth\"\n\"airflow.auth.managers.fab.api.auth.backend.kerberos_auth\"  \"airflow.providers.fab.auth_manager.api.auth.backend.kerberos_auth\"\n```\n\n----------------------------------------\n\nTITLE: Installing Google Provider Package with Amazon Extra\nDESCRIPTION: Command to install the Google provider package along with the Amazon provider dependency using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-google[amazon]\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Impala Provider Package for Airflow\nDESCRIPTION: This command installs the Apache Impala provider package on top of an existing Airflow 2 installation using pip. It requires Airflow version 2.9.0 or higher and supports Python versions 3.9, 3.10, 3.11, and 3.12.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/impala/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-impala\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Apache Airflow Breeze with pipx - Bash\nDESCRIPTION: This snippet demonstrates removing the 'apache-airflow-breeze' tool using the 'pipx' Python package manager. 'pipx' is typically used for installing and running Python CLI applications in isolated environments. For this command to work, Breeze must have been installed using pipx. The command requires no arguments and removes the Breeze binary from the environment.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/01_installation.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npipx uninstall apache-airflow-breeze\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Queries on Apache Pinot using SQLExecuteQueryOperator in Python\nDESCRIPTION: Example demonstrating how to use SQLExecuteQueryOperator to connect to and query an Apache Pinot instance. The connection requires specifying the Pinot broker host, port (default 8000), and optional endpoint configuration in the connection metadata.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndefault_args = {\n    \"start_date\": datetime(2021, 1, 1),\n}\n\nwith DAG(\"pinot_example\", default_args=default_args, schedule=\"@daily\") as dag:\n    # [START howto_operator_pinot]\n    execute_pinot_query = SQLExecuteQueryOperator(\n        task_id=\"execute_pinot_query\",\n        sql=\"select * from my_table\",\n        conn_id=\"pinot_connection\",  # connection must be configured with broker host/port\n    )\n    # [END howto_operator_pinot]\n```\n\n----------------------------------------\n\nTITLE: Setting Basic AWS Connection via Environment Variable (JSON Format) in Bash\nDESCRIPTION: This snippet demonstrates how to configure a basic AWS connection in Airflow by setting the `AIRFLOW_CONN_AWS_DEFAULT` environment variable. It uses a JSON string to specify the connection type as 'aws'. This method is suitable for simple configurations where credentials might be implicitly picked up (e.g., from EC2 instance profile).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AWS_DEFAULT='{\"conn_type\": \"aws\"}'\n```\n\n----------------------------------------\n\nTITLE: Replacing Newlines in SSHOperator Command for Cross-Platform Compatibility\nDESCRIPTION: Command for replacing newlines with '\\n' literals in SSHOperator to make it compatible across MacOS and Linux platforms.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"Make command for replacing newlines with literals '\\\\n' in 'SSHOperator'connection MacOs/Linux compatible (#47491)\"\n```\n\n----------------------------------------\n\nTITLE: Annotating Images with Google Cloud Vision Operator in Airflow\nDESCRIPTION: Demonstrates how to use CloudVisionImageAnnotateOperator to perform image annotation with specified features and settings.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nannotate_image = CloudVisionImageAnnotateOperator(\n    request={\n        'image': {'source': {'image_uri': IMAGE_SOURCE}},\n        'features': [\n            {'type': enums.Feature.Type.FACE_DETECTION},\n            {'type': enums.Feature.Type.LANDMARK_DETECTION},\n            {'type': enums.Feature.Type.LOGO_DETECTION},\n            {'type': enums.Feature.Type.LABEL_DETECTION},\n            {'type': enums.Feature.Type.TEXT_DETECTION},\n            {'type': enums.Feature.Type.DOCUMENT_TEXT_DETECTION},\n            {'type': enums.Feature.Type.SAFE_SEARCH_DETECTION},\n            {'type': enums.Feature.Type.IMAGE_PROPERTIES},\n            {'type': enums.Feature.Type.CROP_HINTS},\n            {'type': enums.Feature.Type.WEB_DETECTION},\n        ],\n    },\n    retry=Retry(maximum=10.0),\n    timeout=5,\n    task_id=\"annotate_image\",\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting GCE Instance Template Without Project ID in Python\nDESCRIPTION: Creates a ComputeEngineDeleteInstanceTemplateOperator without specifying a project ID, which will be automatically retrieved from the Google Cloud connection used.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndelete_template = ComputeEngineDeleteInstanceTemplateOperator(\n    task_id=\"delete_template\",\n    resource_id=OLD_TEMPLATE_NAME,\n    region=GCE_ZONE[:-2],\n)\n```\n\n----------------------------------------\n\nTITLE: Adding PyPI Packages to Airflow Image\nDESCRIPTION: Dockerfile and commands to create a custom Airflow image with additional Python packages\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/quick-start.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmkdir my-airflow-project && cd my-airflow-project\ncat <<EOM > Dockerfile\nFROM apache/airflow\nRUN pip install --no-cache-dir lxml\nEOM\n\ndocker build --pull --tag my-image:0.0.1 .\n\nkind load docker-image my-image:0.0.1\n\nhelm upgrade $RELEASE_NAME apache-airflow/airflow --namespace $NAMESPACE \\\n    --set images.airflow.repository=my-image \\\n    --set images.airflow.tag=0.0.1\n```\n\n----------------------------------------\n\nTITLE: Running Pytest with Test Selection - Bash\nDESCRIPTION: These bash code snippets illustrate using pytest to selectively run test files, classes, or individual tests within Airflow. The -k flag provides keyword-based filtering, while specifying full paths enables targeting of granular test entities. No dependencies are needed beyond pytest and Airflow's own test suite. Expected input is a test path or keyword, and the output is detailed test execution and results as per pytest's output. The snippets assume an environment where pytest and Airflow are properly installed and configured.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/core -k \"TestCore and not check\"\n```\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/core/test_core.py -k \"TestCore and not bash\"\n```\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/core/test_core.py -k \"test_check_operators\"\n```\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/core/test_core.py::TestCore::test_dag_params_and_task_params\n```\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/core/test_core.py::TestCore\n```\n\nLANGUAGE: bash\nCODE:\n```\npytest --log-cli-level=DEBUG tests/core/test_core.py::TestCore\n```\n\n----------------------------------------\n\nTITLE: Listing Memcached Instances using Airflow Operator\nDESCRIPTION: Example of using CloudMemorystoreMemcachedListInstancesOperator to list all Memcached instances.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore_memcached.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlist_instances = CloudMemorystoreMemcachedListInstancesOperator(\n    task_id=\"list-instances\",\n    location=LOCATION,\n    project_id=PROJECT_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Hyperparameter Tuning Job using Vertex AI Operator - Python\nDESCRIPTION: Shows how to delete an existing Vertex AI hyperparameter tuning job using the DeleteHyperparameterTuningJobOperator. Requires the Airflow provider for GCP and job identification parameters. Deletion is permanent and should be handled with caution in production workflows.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndelete_hpt_job_task = DeleteHyperparameterTuningJobOperator(\n    task_id=\"delete_hptuning_job_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    hyperparameter_tuning_job_id=HPTUNING_JOB_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating MSSQL Table from SQL File using Airflow SQLExecuteQueryOperator (Reference)\nDESCRIPTION: References a Python snippet showing how to use `SQLExecuteQueryOperator` to execute SQL commands stored in an external `.sql` file (e.g., `dags/create_table.sql`). The `sql` parameter should reference the file path relative to the DAG file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../tests/system/microsoft/mssql/example_mssql.py\n    :language: python\n    :start-after: [START mssql_operator_howto_guide_create_table_mssql_from_external_file]\n    :end-before: [END mssql_operator_howto_guide_create_table_mssql_from_external_file]\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare docs 1st wave (RC1) April 2024\nDESCRIPTION: This text is a commit message summary for preparing documentation related to the first wave (Release Candidate 1) of Apache Airflow provider releases in April 2024, linked to pull request #38863.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 1st wave (RC1) April 2024 (#38863)\n```\n\n----------------------------------------\n\nTITLE: Documentation Path Mapping for Apache Airflow\nDESCRIPTION: Comprehensive mapping of documentation file paths showing source and destination locations for Apache Airflow's documentation restructuring. Includes redirects for security documentation, operator guides, web UI content, logging & monitoring, and provider-specific documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/redirects.txt#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nadministration-and-deployment/security/index.rst security/index.rst\nadministration-and-deployment/security/kerberos.rst security/kerberos.rst\nadministration-and-deployment/security/access-control.rst ../apache-airflow-providers-fab/stable/auth-manager/access-control.rst\nadministration-and-deployment/security/access-control/index.rst ../apache-airflow-providers-fab/stable/auth-manager/access-control.rst\nadministration-and-deployment/security/api.rst security/api.rst\nadministration-and-deployment/security/audit_logs.rst security/audit_logs.rst\nadministration-and-deployment/security/flower.rst security/flower.rst\nadministration-and-deployment/security/webserver.rst ../apache-airflow-providers-fab/stable/auth-manager/security.rst\nadministration-and-deployment/security/workload.rst security/workload.rst\nadministration-and-deployment/security/secrets/secrets-backends/index.rst security/secrets/secrets-backends/index.rst\nadministration-and-deployment/security/secrets/secrets-backends/local-filesystem-secrets-backend.rst security/secrets/secrets-backends/local-filesystem-secrets-backend.rst\nadministration-and-deployment/security/secrets/fernet.rst security/secrets/fernet.rst\nadministration-and-deployment/security/secrets/index.rst security/secrets/index.rst\nadministration-and-deployment/security/secrets/mask-sensitive-values.rst security/secrets/mask-sensitive-values.rst\n```\n\n----------------------------------------\n\nTITLE: Installing Base Postgres Provider Package via pip\nDESCRIPTION: Command to install the base Apache Airflow Postgres provider package using pip. This assumes an existing Airflow 2 installation. The minimum required Airflow version is specified in the requirements section (>=2.9.0).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-postgres\n```\n\n----------------------------------------\n\nTITLE: Starting and Stopping Breeze Environment\nDESCRIPTION: Commands to start Airflow services in Breeze and shut down the environment\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nbreeze start-airflow\nbreeze down\n```\n\n----------------------------------------\n\nTITLE: Publishing Airflow Python Client Release to Apache SVN (Shell Script)\nDESCRIPTION: A sequence of shell commands designed to publish the approved release artifacts to the Apache Software Foundation's Subversion (SVN) distribution repository. It involves setting repository paths, checking out/updating the SVN directories, creating a new version directory, copying the approved release candidate artifacts from the dev SVN path to the release SVN path, removing the previous version's directory from the release area according to policy, and committing the changes with an appropriate message. Requires environment variables like `AIRFLOW_REPO_ROOT`, `CLIENT_REPO_ROOT`, `VERSION`, `VERSION_SUFFIX`, and `PREVIOUS_VERSION` to be correctly set.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_25\n\nLANGUAGE: shell script\nCODE:\n```\n# Go to Airflow sources first\ncd <YOUR_AIRFLOW_REPO_ROOT>\nexport AIRFLOW_REPO_ROOT=\"$(pwd)\"\n# Go to Airflow python client sources first\ncd <YOUR_AIRFLOW_CLIENT_REPO_ROOT>\nexport CLIENT_REPO_ROOT=\"$(pwd)\"\ncd ..\n# Clone the AS\n[ -d asf-dist ] || svn checkout --depth=immediates https://dist.apache.org/repos/dist asf-dist\nsvn update --set-depth=infinity asf-dist/{release,dev}/airflow\nCLIENT_DEV_SVN=\"${PWD}/asf-dist/dev/airflow/clients/python\"\nCLIENT_RELEASE_SVN=\"${PWD}/asf-dist/release/airflow/clients/python\"\ncd \"${CLIENT_RELEASE_SVN}\"\n\nexport VERSION=\"2.8.1\"\n# Update the approved RC version here\nexport VERSION_SUFFIX=\"rc1\"\n# Update the previous version that have been released here\n# There should be only one version in https://downloads.apache.org/airflow/clients/python/\n# Policy here: http://www.apache.org/legal/release-policy.html#when-to-archive\nexport PREVIOUS_VERSION=2.8.0\n\n# Create new folder for the release\nsvn mkdir ${VERSION}\ncd ${VERSION}\n\n# Move the artifacts to svn folder & commit\nfor f in ${CLIENT_DEV_SVN}/${VERSION}${VERSION_SUFFIX}/*; do\n  svn cp $f . ;\ndone\n# Remove old release\ncd ..\nsvn rm ${PREVIOUS_VERSION}\nsvn commit -m \"Release Apache Airflow Python Client ${VERSION} from ${VERSION}${VERSION_SUFFIX}\"\n```\n\n----------------------------------------\n\nTITLE: Describing a Job on GKE using GKEDescribeJobOperator in Python\nDESCRIPTION: This snippet shows how to use the `GKEDescribeJobOperator` to retrieve detailed information about an existing Job on a GKE cluster. It requires the job's name and namespace as parameters to fetch the description.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/kubernetes_engine/example_kubernetes_engine_job.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_gke_describe_job]\n    :end-before: [END howto_operator_gke_describe_job]\n```\n\n----------------------------------------\n\nTITLE: Defining Package Requirements for Apache HDFS Provider\nDESCRIPTION: Lists the required pip packages and their versions for the Apache HDFS provider. It includes apache-airflow, hdfs with specific features, and pandas, with version constraints based on the Python version.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n=================================  =====================================\nPIP package                        Version required\n=================================  =====================================\n\"apache-airflow\"                 \">=2.9.0\"\n\"hdfs[avro,dataframe,kerberos]\"  \">=2.5.4; python_version < \\\"3.12\\\"\"\n\"hdfs[avro,dataframe,kerberos]\"  \">=2.7.3; python_version >= \\\"3.12\\\"\"\n\"pandas\"                         \">=2.1.2,<2.2\"\n================================= =====================================\n```\n\n----------------------------------------\n\nTITLE: Installing the Segment Provider via pip (Shell)\nDESCRIPTION: This shell command installs the `apache-airflow-providers-segment` package using pip, the Python package installer. It's used to add Segment integration capabilities to an existing Apache Airflow installation (version 2.9.0 or higher is required).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/segment/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-segment\n```\n\n----------------------------------------\n\nTITLE: Check Airflow Configuration Compatibility\nDESCRIPTION: Runs a configuration check utility to verify compatibility with Airflow 3.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading_to_airflow3.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairflow config update\n```\n\n----------------------------------------\n\nTITLE: Exporting Connections to JSON File in Airflow CLI\nDESCRIPTION: This command exports Airflow connections to a JSON file named 'connections.json' using the Airflow CLI.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nairflow connections export connections.json\n```\n\n----------------------------------------\n\nTITLE: Retrieving Custom Column Details with GoogleSearchAdsGetCustomColumnOperator\nDESCRIPTION: This snippet demonstrates how to use the GoogleSearchAdsGetCustomColumnOperator to retrieve details of a specific custom column. The operator supports Jinja templating for dynamic parameter determination.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/search_ads.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n[START howto_search_ads_get_custom_column]\n[END howto_search_ads_get_custom_column]\n```\n\n----------------------------------------\n\nTITLE: Logging into GitHub Container Registry\nDESCRIPTION: Command to log into GitHub Container Registry using Docker CLI. This is required for committers who need to push images to the registry.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ndocker login ghcr.io\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Breeze CI Image (Bash)\nDESCRIPTION: This bash command builds the Breeze CI Docker image using Python version 3.9. It is a prerequisite for running both local and CI-based compatibility tests, ensuring all necessary dependencies and environment settings are prepared. The command should be executed before entering the testing environment or building provider distributions.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image build --python 3.9\n```\n\n----------------------------------------\n\nTITLE: Running Specific Test File Sequentially\nDESCRIPTION: Command to run tests from a specific file sequentially\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/helm_unit_tests.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npytest helm-tests/tests/other/test_airflow_common.py\n```\n\n----------------------------------------\n\nTITLE: Verifying Apache Airflow Package SHA512 Checksums\nDESCRIPTION: This command compares the computed SHA512 checksum of a downloaded Apache Airflow package with the provided checksum file to ensure file integrity.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-sources.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nshasum -a 512 apache-airflow--********  | diff - apache-airflow--********.sha512\n```\n\n----------------------------------------\n\nTITLE: Upgrading SQLite on AmazonLinux\nDESCRIPTION: This bash script provides steps to upgrade SQLite to a version compatible with Airflow on AmazonLinux AMI or Container Image.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-up-database.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyum -y install wget tar gzip gcc make expect\n\nwget https://www.sqlite.org/src/tarball/sqlite.tar.gz\ntar xzf sqlite.tar.gz\ncd sqlite/\nexport CFLAGS=\"-DSQLITE_ENABLE_FTS3 \\\n    -DSQLITE_ENABLE_FTS3_PARENTHESIS \\\n    -DSQLITE_ENABLE_FTS4 \\\n    -DSQLITE_ENABLE_FTS5 \\\n    -DSQLITE_ENABLE_JSON1 \\\n    -DSQLITE_ENABLE_LOAD_EXTENSION \\\n    -DSQLITE_ENABLE_RTREE \\\n    -DSQLITE_ENABLE_STAT4 \\\n    -DSQLITE_ENABLE_UPDATE_DELETE_LIMIT \\\n    -DSQLITE_SOUNDEX \\\n    -DSQLITE_TEMP_STORE=3 \\\n    -DSQLITE_USE_URI \\\n    -O2 \\\n    -fPIC\"\nexport PREFIX=\"/usr/local\"\nLIBS=\"-lm\" ./configure --disable-tcl --enable-shared --enable-tempstore=always --prefix=\"$PREFIX\"\nmake\nmake install\n\nexport LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Redeploying Airflow to Kubernetes Cluster\nDESCRIPTION: Command to redeploy Airflow to an existing Kubernetes cluster without recreating the entire cluster, useful when updating configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s deploy-airflow --upgrade\n```\n\n----------------------------------------\n\nTITLE: Setting Speech-to-Text Filename Argument\nDESCRIPTION: Example demonstrating how to specify the filename argument for speech recognition.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/speech_to_text.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_speech_to_text_api_arguments]\n[END howto_operator_speech_to_text_api_arguments]\n```\n\n----------------------------------------\n\nTITLE: Accessing Kubernetes Shell Environment\nDESCRIPTION: Commands to enter a shell environment prepared for Kubernetes testing with kubectl and kind CLI tools available.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s shell\n\nbreeze k8s shell --executor CeleryExecutor\n```\n\n----------------------------------------\n\nTITLE: Indicating Dependency Upgrade Attempt (Boolean/String)\nDESCRIPTION: Controls whether the image build process should attempt to upgrade all dependencies to their newer versions. Can be 'true', 'false', or a specific commit hash. 'false' means no upgrade attempt.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_36\n\nLANGUAGE: text\nCODE:\n```\nfalse\n```\n\n----------------------------------------\n\nTITLE: Preparing Vote Email for Airflow RC - Shell Script\nDESCRIPTION: This shell script uses a here document (cat <<EOF) to generate the subject and body of a release vote email for Apache Airflow. It replaces shell variables (e.g., ${VERSION}, ${VERSION_WITHOUT_RC}) to customize the email content. The script is intended to be executed in a shell environment where the appropriate variables are set prior to execution. The resulting output can be copied and sent to the mailing list. Requires a shell that supports here-documents.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\n[VOTE] Release Airflow ${VERSION_WITHOUT_RC} from ${VERSION}\nEOF\n```\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\nHey fellow Airflowers,\n\nI have cut Airflow ${VERSION}. This email is calling a vote on the release,\nwhich will last at least 72 hours, from Friday, October 8, 2021 at 4:00 pm UTC\nuntil Monday, October 11, 2021 at 4:00 pm UTC, and until 3 binding +1 votes have been received.\n\nhttps://www.timeanddate.com/worldclock/fixedtime.html?msg=8&iso=20211011T1600&p1=1440\n\nStatus of testing of the release is kept in TODO:URL_OF_THE_ISSUE_HERE\n\nConsider this my (binding) +1.\n\nAirflow ${VERSION} is available at:\nhttps://dist.apache.org/repos/dist/dev/airflow/$VERSION/\n\n*apache-airflow-${VERSION_WITHOUT_RC}-source.tar.gz* is a source release that comes with INSTALL instructions.\n*apache-airflow-${VERSION_WITHOUT_RC}.tar.gz* is the binary Python \"sdist\" release fore airflow meta distribution.\n*apache_airflow-${VERSION_WITHOUT_RC}-py3-none-any.whl* is the binary Python wheel \"binary\" release for airflow meta distribution.\n*apache-airflow_core-${VERSION_WITHOUT_RC}.tar.gz* is the binary Python \"sdist\" release for airflow core distribution.\n*apache_airflow_core-${VERSION_WITHOUT_RC}-py3-none-any.whl* is the binary Python wheel \"binary\" release for airflow core distribution.\n\nPublic keys are available at:\nhttps://dist.apache.org/repos/dist/release/airflow/KEYS\n\nPlease vote accordingly:\n\n[ ] +1 approve\n[ ] +0 no opinion\n[ ] -1 disapprove with the reason\n\nOnly votes from PMC members are binding, but all members of the community\nare encouraged to test the release and vote with \"(non-binding)\".\n\nThe test procedure for PMC members is described in:\nhttps://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#verify-the-release-candidate-by-pmc-members\n\nThe test procedure for contributors and members of the community who would like to test this RC is described in:\nhttps://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#verify-the-release-candidate-by-contributors\n\nPlease note that the version number excludes the 'rcX' string, so it's now\nsimply ${VERSION_WITHOUT_RC}. This will allow us to rename the artifact without modifying\nthe artifact checksums when we actually release.\n\nRelease Notes: https://github.com/apache/airflow/blob/${VERSION}/RELEASE_NOTES.rst\n\nFor information on what goes into a release please see: https://github.com/apache/airflow/blob/main/dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md\n\nChanges since PREVIOUS_VERSION_OR_RC:\n*Bugs*:\n[AIRFLOW-3732] Fix issue when trying to edit connection in RBAC UI\n[AIRFLOW-2866] Fix missing CSRF token head when using RBAC UI (#3804)\n...\n\n\n*Improvements*:\n[AIRFLOW-3302] Small CSS fixes (#4140)\n[Airflow-2766] Respect shared datetime across tabs\n...\n\n\n*New features*:\n[AIRFLOW-2874] Enables FAB's theme support (#3719)\n[AIRFLOW-3336] Add new TriggerRule for 0 upstream failures (#4182)\n...\n\n\n*Doc-only Change*:\n[AIRFLOW-XXX] Fix BashOperator Docstring (#4052)\n[AIRFLOW-3018] Fix Minor issues in Documentation\n...\n\nCheers,\n<your name>\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring Sentry before_send Function in Airflow\nDESCRIPTION: Configuration for setting a custom before_send function for Sentry in Airflow. This function allows modifying or dropping events before they are sent to Sentry by providing a dotted path to a function.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/errors.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[sentry]\nbefore_send = path.to.my.sentry.before_send\n```\n\n----------------------------------------\n\nTITLE: Installing JDBC Extras for Apache Airflow\nDESCRIPTION: Command to install JDBC hooks and operators for Apache Airflow. This enables integration with databases via JDBC protocol.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_62\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[jdbc]'\n```\n\n----------------------------------------\n\nTITLE: Breaking Changes in Oracle Provider\nDESCRIPTION: Documentation of breaking changes in version 4.0.0, including removal of deprecated classes, parameters, and features from the Oracle provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. warning::\n  All deprecated classes, parameters and features have been removed from the Oracle provider package.\n  The following breaking changes were introduced:\n\n  * Hooks\n     * Remove deprecated support setting the Oracle Service Name using ``conn.schema``. Please use ``conn.extra.service_name`` instead.\n  * Operators\n     * Remove ``airflow.providers.oracle.operators.oracle.OracleOperator``. Please use ``airflow.providers.common.sql.operators.sql.SQLExecuteQueryOperator`` instead.\n```\n\n----------------------------------------\n\nTITLE: Deleting a Dataplex Data Profile Scan with Airflow Operator\nDESCRIPTION: Uses the DataplexDeleteDataProfileScanOperator to delete a Dataplex Data Profile scan. This operator removes a specified data profile scan from Google Cloud Dataplex.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndelete_data_profile = DataplexDeleteDataProfileScanOperator(\n    task_id=\"delete_data_profile\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    data_scan_id=DATA_PROFILE_SCAN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: SQLAlchemy Engine Creation Enhancement\nDESCRIPTION: Feature that creates SQLAlchemy engine from connection in DB Hook and adds autocommit parameter to the insert_rows method. Improves flexibility and control over database operations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/changelog.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nCreate SQLAlchemy engine from connection in DB Hook and added autocommit param to insert_rows method (#40669)\n```\n\n----------------------------------------\n\nTITLE: Waiting for AWS CloudFormation Stack Deletion State with Airflow Sensor - Python\nDESCRIPTION: This Python snippet uses CloudFormationDeleteStackSensor to monitor the deletion of an AWS CloudFormation stack, blocking further DAG steps until the stack deletion reaches a terminal state. Key dependencies include Airflow's AWS provider and necessary credentials. Parameters allow specifying stack name and sensing interval. Outputs include a Boolean flag or exceptions, depending on the stack deletion progress.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/cloudformation.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwait_for_deletion = CloudFormationDeleteStackSensor(\n    task_id=\"wait_for_stack_deletion\",\n    stack_name=STACK_NAME,\n    aws_conn_id=\"aws_default\",\n    poke_interval=60,\n)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Update release notes for RC2 release of Providers for May 2022 (#24307)\nDESCRIPTION: This commit message, associated with version 3.0.0, updates the release notes for the second release candidate (RC2) of the May 2022 wave of Apache Airflow Providers. Commit hash: dcdcf3a2b8, Date: 2022-06-09.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_29\n\nLANGUAGE: text\nCODE:\n```\n``Update release notes for RC2 release of Providers for May 2022 (#24307)``\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow from Source with Development Dependencies\nDESCRIPTION: Command to install Airflow in editable mode from source with development dependencies using source provider constraints.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/13_airflow_dependencies_and_extras.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[devel]\" \\\n  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-main/constraints-source-providers-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Installing Discord Provider with Cross-Provider Dependencies\nDESCRIPTION: Command to install the Discord provider package along with its cross-provider dependencies, specifically the common.compat extra.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/discord/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-discord[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Creating an Entry with CloudDataCatalogCreateEntryOperator in Python\nDESCRIPTION: Uses CloudDataCatalogCreateEntryOperator to create a new entry in Google Cloud Data Catalog. The newly created entry ID is saved to XCom for use by other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datacatalog.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncreate_entry_gcs = CloudDataCatalogCreateEntryOperator(\n    task_id=\"create_entry_gcs\",\n    location=LOCATION,\n    entry_group=ENTRY_GROUP_ID,\n    entry_id=ENTRY_ID,\n    entry={\n        \"display_name\": \"SDK Sample GCS Entry\",\n        \"description\": \"This entry is created from SDK Sample GCS Entry\",\n        \"gcs_fileset_spec\": {\"file_patterns\": [\"gs://example-bucket/gcs_sample.txt\"]},\n        \"type\": \"FILESET\",\n    },\n    project_id=PROJECT_ID,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ncreate_entry_gcs_result = create_entry_gcs.output[\"entry_id\"]\n```\n\n----------------------------------------\n\nTITLE: Templating Fields for GCE Instance Group Manager Deletion in Python\nDESCRIPTION: Defines the template fields available for the ComputeEngineDeleteInstanceGroupManagerOperator, which allow for runtime parameter substitution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"project_id\",\n    \"zone\",\n    \"resource_id\",\n    \"request_id\",\n    \"gcp_conn_id\",\n    \"api_version\",\n    \"impersonation_chain\",\n)\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 3.0.0\nDESCRIPTION: Changelog entry documenting switch to smbprotocol library and connection importing optimization\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: restructuredtext\nCODE:\n```\n3.0.0\n.....\n\nBreaking changes\n~~~~~~~~~~~~~~~~\n\n* ``Switch to 'smbprotocol' library (#17273)``\n\n\nMisc\n~~~~\n\n* ``Optimise connection importing for Airflow 2.2.0``\n```\n\n----------------------------------------\n\nTITLE: Adding Support for Azure Managed Identities in Databricks Operator\nDESCRIPTION: Bug fix to add support for Azure managed identities in the Databricks operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"Bug/fix support azure managed identities in Databricks operator (#40332)\"\n```\n\n----------------------------------------\n\nTITLE: Feature: AWS S3Hook Download File Enhancement\nDESCRIPTION: Addition of 'preserve_file_name' parameter to S3Hook.download_file method to maintain original filenames when downloading.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# S3Hook with preserve_file_name parameter\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\n\nhook = S3Hook()\nhook.download_file(\n    key='my/s3/key',\n    bucket_name='my-bucket',\n    preserve_file_name=True\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Cross-provider Dependencies for Apache Druid\nDESCRIPTION: Command to install cross-provider dependencies for the Apache Druid provider package, specifically the Apache Hive dependency.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/druid/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-druid[apache.hive]\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Bump Minimum Airflow Version in Providers\nDESCRIPTION: This commit message, associated with commit a7eb32a5b2 dated 2023-04-30, notes an update that increases the minimum required Airflow version for the providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n``Bump minimum Airflow version in providers (#30917)``\n```\n\n----------------------------------------\n\nTITLE: Adding Provider Package Support for Airflow 2.0\nDESCRIPTION: This commit message signifies the addition of support for the provider packages system, a key feature introduced in Airflow 2.0. It references pull request #11487.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_69\n\nLANGUAGE: text\nCODE:\n```\nAdded support for provider packages for Airflow 2.0 (#11487)\n```\n\n----------------------------------------\n\nTITLE: Yarn Package Installation\nDESCRIPTION: Command to install third-party libraries defined in package.json using Yarn.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/15_node_environment_setup.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Updating DAG Parameter Name in Python\nDESCRIPTION: Migration of the DAG constructor parameter name from fail_stop to fail_fast to maintain consistency with Airflow 3.0 naming conventions. This change is enforced through the ruff linting rule AIR302.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/45327.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old usage\ndag = DAG(fail_stop=True)\n\n# New usage\ndag = DAG(fail_fast=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Cassandra Provider Package for Airflow\nDESCRIPTION: Command to install the Apache Cassandra provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/cassandra/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-cassandra\n```\n\n----------------------------------------\n\nTITLE: Configuring Breeze Environment Variables for Airflow API Testing (Shell)\nDESCRIPTION: Sets environment variables within the `files/airflow-breeze-config/init.sh` file for the Breeze development environment. `AIRFLOW__API__AUTH_BACKENDS` enables session and basic authentication for the Airflow API, and `AIRFLOW__WEBSERVER__EXPOSE_CONFIG` allows the Airflow configuration to be accessed via the API endpoint, facilitating client testing.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\nexport AIRFLOW__API__AUTH_BACKENDS=airflow.providers.fab.auth_manager.api.auth.backend.session,airflow.providers.fab.auth_manager.api.auth.backend.basic_auth\nexport AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True\n```\n\n----------------------------------------\n\nTITLE: Running Core System Tests via Pytest\nDESCRIPTION: Command to run Airflow core system tests using pytest testing framework.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/system_tests.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest --system airflow-core/tests/system/example_empty.py\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenLineage Custom Run Facets in Airflow\nDESCRIPTION: Configuration examples for registering custom run facet functions in Airflow using both the ini configuration format and environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/developer.rst#2025-04-22_snippet_10\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\ncustom_run_facets = full.path.to.get_my_custom_facet;full.path.to.another_custom_facet_function\n```\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__CUSTOM_RUN_FACETS='full.path.to.get_my_custom_facet;full.path.to.another_custom_facet_function'\n```\n\n----------------------------------------\n\nTITLE: Logging out of GitHub Container Registry\nDESCRIPTION: Command to log out of GitHub Container Registry. This is useful when your token expires and you need to refresh it for continued access.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ndocker logout ghcr.io\n```\n\n----------------------------------------\n\nTITLE: RDS Snapshot Status Sensor\nDESCRIPTION: Monitors Amazon RDS snapshot status using RdsSnapshotExistenceSensor. Default status is 'available'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nwait_for_snapshot = RdsSnapshotExistenceSensor(\n    task_id=\"wait_for_snapshot\",\n    db_type=\"instance\",\n    db_snapshot_identifier=DB_SNAPSHOT_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Representing Airflow Connection Details in JSON Format\nDESCRIPTION: Provides an example JSON structure for storing Airflow connection details. This includes standard fields like `conn_type`, `host`, `login`, `password`, and custom parameters within the `extra` object. This JSON can be stored as the value in a Yandex Lockbox secret.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"conn_type\": \"mysql\",\n  \"host\": \"host.com\",\n  \"login\": \"myname\",\n  \"password\": \"mypassword\",\n  \"extra\": {\n    \"this_param\": \"some val\",\n    \"that_param\": \"other val*\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Azure File Share Hook in Python\nDESCRIPTION: Adds managed identity support to the Azure File Share hook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfeat(provider/azure): add managed identity support to fileshare hook (#35330)\n```\n\n----------------------------------------\n\nTITLE: Deleting DataFusion Instance with CloudDataFusionDeleteInstanceOperator in Python\nDESCRIPTION: This snippet shows how to use the CloudDataFusionDeleteInstanceOperator to delete a Google Cloud DataFusion instance. It includes parameters for instance name, project ID, location, and retry settings.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datafusion.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCloudDataFusionDeleteInstanceOperator(\n    task_id=\"delete_instance\",\n    instance_name=INSTANCE_NAME,\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    retry=RETRY,\n    timeout=TIMEOUT,\n    metadata=METADATA,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding PyPI Package to Airflow Image (Dockerfile)\nDESCRIPTION: Dockerfile example demonstrating how to install a Python package (lxml) from PyPI using pip into the Airflow image's user environment. Requires switching to the airflow user.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_11\n\nLANGUAGE: Dockerfile\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/add-pypi-packages/Dockerfile\n    :language: Dockerfile\n    :start-after: [START Dockerfile]\n    :end-before: [END Dockerfile]\n```\n\n----------------------------------------\n\nTITLE: Installing the FAB Provider Package using pip\nDESCRIPTION: This shell command uses pip to install the core `apache-airflow-providers-fab` package. It should be executed in a terminal on top of an existing Airflow 2 installation (version 3.0.0 or newer is required). This command installs the necessary components for the FAB provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-fab\n```\n\n----------------------------------------\n\nTITLE: Configuring a Hive Job for Google Cloud Dataproc\nDESCRIPTION: This code defines a configuration for a Hive job to be submitted to a Dataproc cluster. It specifies the query file and query variables for the Hive execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nHIVE_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"hive_job\": {\n        \"query_file_uri\": f\"gs://{BUCKET_NAME}/{HIVE_SCRIPT}\",\n        \"script_variables\": {\n            \"input\": f\"gs://{BUCKET_NAME}/{HIVE_INPUT}\",\n            \"output\": f\"gs://{BUCKET_NAME}/{HIVE_OUTPUT_DIR}\",\n        },\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Database Migration Files for Airflow\nDESCRIPTION: Command to auto-generate database migration files after making changes to ORM models. This creates migration scripts in the airflow/migrations/versions directory.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nbreeze generate-migration-file -m \"Your migration message\"\n```\n\n----------------------------------------\n\nTITLE: Stopping Celery Worker for Apache Airflow\nDESCRIPTION: Command to stop a Celery worker running on a machine, using a graceful shutdown process with SIGTERM signal.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/celery_executor.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairflow celery stop\n```\n\n----------------------------------------\n\nTITLE: Executing Spark Application with AzureSynapseRunSparkBatchOperator in Python\nDESCRIPTION: This snippet demonstrates how to use the AzureSynapseRunSparkBatchOperator to execute a Spark application within Azure Synapse Analytics. The operator periodically checks the status of the executed Spark job until it terminates with a 'Succeeded' status.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/azure_synapse.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n  .. exampleinclude:: /../tests/system/microsoft/azure/example_azure_synapse.py\n      :language: python\n      :dedent: 4\n      :start-after: [START howto_operator_azure_synapse]\n      :end-before: [END howto_operator_azure_synapse]\n```\n\n----------------------------------------\n\nTITLE: Initializing AVP Policy Store using Airflow CLI (Bash)\nDESCRIPTION: This command initializes the required Amazon Verified Permissions (AVP) policy store for the Airflow AWS auth manager. It automates the creation process. Requires the AWS auth manager to be set as the auth manager in the Airflow configuration beforehand. If a policy store already exists, it will output an error and not make changes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/setup/amazon-verified-permissions.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairflow aws-auth-manager init-avp\n```\n\n----------------------------------------\n\nTITLE: Installing the Apache Airflow FTP Provider using pip (Bash)\nDESCRIPTION: This command installs the core `apache-airflow-providers-ftp` package using pip. It requires an existing Airflow 2 installation (version 2.9.0 or higher) and is compatible with Python versions 3.9, 3.10, 3.11, and 3.12.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ftp/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-ftp\n```\n\n----------------------------------------\n\nTITLE: Saving DAG Structure as Image\nDESCRIPTION: Command to save a DAG structure visualization as a PNG file\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairflow dags show example_complex --save example_complex.png\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents for Connection Types in Apache Airflow\nDESCRIPTION: This reStructuredText snippet creates a table of contents for all connection type documents in the current directory. It uses the toctree directive with maxdepth set to 1 and glob option enabled.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Setting Azure Container Registry Connection via Environment Variable in Bash\nDESCRIPTION: This Bash command demonstrates how to configure the default Azure Container Registry connection in Airflow using an environment variable. It sets the `AIRFLOW_CONN_AZURE_CONTAINER_REGISTRY_DEFAULT` variable with a URI containing connection details like username, password, host, tenant ID, and account name. All components within the URI must be URL-encoded.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/acr.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AZURE_CONTAINER_REGISTRY_DEFAULT='azure-container-registry://username:password@myregistry.com?tenant=tenant+id&account_name=store+name'\n```\n\n----------------------------------------\n\nTITLE: Setting dbt Cloud Connection Environment Variables\nDESCRIPTION: Examples of setting up dbt Cloud connections using environment variables with different configurations including account ID and tenant domain specifications.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/connections.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_DBT_CLOUD_DEFAULT='dbt-cloud://account_id:api_token@'\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_DBT_CLOUD_DEFAULT='dbt-cloud://:api_token@'\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_DBT_CLOUD_DEFAULT='dbt-cloud://:api_token@my-access-url'\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_DBT_CLOUD_DEFAULT='dbt-cloud://:api_token@ab123.us1.dbt.com'\n```\n\n----------------------------------------\n\nTITLE: Implementing XCom Value Access with Dynamic Task Mapping in Python\nDESCRIPTION: Code example showing how to properly access XCom values in providers while maintaining backwards compatibility with dynamic task mapping. Includes handling for both ti_key and legacy datetime-based retrieval.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/howto/create-custom-providers.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_link(\n    self,\n    operator: BaseOperator,\n    dttm: datetime | None = None,\n    ti_key: \"TaskInstanceKey\" | None = None,\n):\n    if ti_key is not None:\n        job_ids = XCom.get_value(key=\"job_id\", ti_key=ti_key)\n    else:\n        assert dttm is not None\n        job_ids = XCom.get_one(\n            key=\"job_id\",\n            dag_id=operator.dag.dag_id,\n            task_id=operator.task_id,\n            execution_date=dttm,\n        )\n    if not job_ids:\n        return None\n    if len(job_ids) < self.index:\n        return None\n    job_id = job_ids[self.index]\n    return BIGQUERY_JOB_DETAILS_LINK_FMT.format(job_id=job_id)\n```\n\n----------------------------------------\n\nTITLE: Running Ruff DAG Compatibility Check\nDESCRIPTION: Checks DAG files for compatibility issues with Airflow 3 using the Ruff linter with the AIR301 rule set.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading_to_airflow3.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nruff check dag/ --select AIR301\n```\n\n----------------------------------------\n\nTITLE: Adding Airflow Helm Repository\nDESCRIPTION: Commands to add and update the Apache Airflow Helm repository\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/quick-start.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add apache-airflow https://airflow.apache.org\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Implementing Registered Serializer for Decimal in Python\nDESCRIPTION: This snippet shows how to create a registered serializer and deserializer for the Decimal type in Airflow. It includes version checking, type validation, and handles different decimal representations.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/serializers.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom decimal import Decimal\nfrom typing import TYPE_CHECKING\n\nfrom airflow.utils.module_loading import qualname\n\nif TYPE_CHECKING:\n    from airflow.serialization.serde import U\n\n\nserializers = [\n    Decimal\n]  # this can be a type or a fully qualified str. Str can be used to prevent circular imports\ndeserializers = serializers  # in some cases you might not have a deserializer (e.g. k8s pod)\n\n__version__ = 1  # required\n\n\n# the serializer expects output, classname, version, is_serialized?\ndef serialize(o: object) -> tuple[U, str, int, bool]:\n    if isinstance(o, Decimal):\n        name = qualname(o)\n        _, _, exponent = o.as_tuple()\n        if exponent >= 0:  # No digits after the decimal point.\n            return int(o), name, __version__, True\n            # Technically lossy due to floating point errors, but the best we\n            # can do without implementing a custom encode function.\n        return float(o), name, __version__, True\n\n    return \"\", \"\", 0, False\n\n\n# the deserializer sanitizes the data for you, so you do not need to deserialize values yourself\ndef deserialize(classname: str, version: int, data: object) -> Decimal:\n    # always check version compatibility\n    if version > __version__:\n        raise TypeError(f\"serialized {version} of {classname} > {__version__}\")\n\n    if classname != qualname(Decimal):\n        raise TypeError(f\"{classname} != {qualname(Decimal)}\")\n\n    return Decimal(str(data))\n```\n\n----------------------------------------\n\nTITLE: Running an AppFlow Flow Using AppflowRunOperator - Apache Airflow - Python\nDESCRIPTION: Demonstrates how to schedule a standard Amazon AppFlow run within an Airflow DAG using the AppflowRunOperator. Assumes you have installed the Airflow Amazon provider and boto3. Requires defining AWS credentials and connection configuration in Airflow. Input parameters typically include flow name, run options, and target destination. The output is execution of the flow according to the DAG's schedule, with task success/failure depending on the underlying AppFlow job result.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/appflow.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith DAG(\n    dag_id=\"example_appflow_run\",\n    schedule_interval=None,\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    tags=[\"example\", \"appflow\"],\n) as dag:\n\n    run_flow = AppflowRunOperator(\n        task_id=\"appflow_run_task\",\n        flow_name=\"your_appflow_flow_name\",\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Timetable Description\nDESCRIPTION: Shows how to implement a dynamic description in the timetable's constructor that includes runtime parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/timetable.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self) -> None:\n    self.description = \"Schedule: after each workday, at f{self._schedule_at}\"\n```\n\n----------------------------------------\n\nTITLE: Moving a Single File from GCS to SFTP with Custom Destination in Python\nDESCRIPTION: This example shows how to move a file from Google Cloud Storage to SFTP using the move_object parameter. When set to True, the file is deleted from GCS after being copied to SFTP. The destination_path parameter defines the full path for the file on the SFTP server.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_sftp.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmove_file_from_gcs_to_sftp_destination = GCSToSFTPOperator(\n    task_id=\"move-file-from-gcs-to-sftp-destination\",\n    source_bucket=BUCKET_NAME,\n    source_object=GCS_SRC_FILE,\n    destination_path=SFTP_DST_PATH + \"/destination.txt\",\n    move_object=True,\n    sftp_conn_id=\"sftp_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Building Airflow CI Image with Breeze in Bash\nDESCRIPTION: This command builds the Airflow CI image using Breeze after updating dependencies. It should be run to ensure that the Docker image includes the latest changes.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/10_advanced_breeze_topics.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image build\n```\n\n----------------------------------------\n\nTITLE: Configuring Policy Entrypoints in pyproject.toml\nDESCRIPTION: Example of setting up the entrypoint configuration in pyproject.toml to register custom Airflow policy functions using setuptools.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/cluster-policies.rst#2025-04-22_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-airflow-plugin\"\nversion = \"0.0.1\"\n# ...\n\ndependencies = [\"apache-airflow>=2.6\"]\n[project.entry-points.'airflow.policy']\n_ = 'my_airflow_plugin.policies'\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow Dependencies in Local Environment\nDESCRIPTION: Commands to install necessary system packages and initialize a virtual environment for Airflow development.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install sqlite3 libsqlite3-dev default-libmysqlclient-dev postgresql\n./scripts/tools/initialize_virtualenv.py\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Hive Provider Package with Amazon Extra\nDESCRIPTION: Command to install the Apache Hive provider package with Amazon integration dependencies via pip\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-hive[amazon]\n```\n\n----------------------------------------\n\nTITLE: Configuring pipx PATH\nDESCRIPTION: Commands to ensure pipx is accessible in the system PATH and configure it using Python module if needed.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/01_installation.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npipx ensurepath\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m pipx ensurepath\n```\n\n----------------------------------------\n\nTITLE: Starting AWS Glue DataBrew Job using Airflow Operator\nDESCRIPTION: Example showing how to use the GlueDataBrewStartJobOperator to submit a new AWS Glue DataBrew job. The operator requires an AWS connection ID, job name, and allows specifying a wait for completion timeout.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/glue_databrew.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nstart_databrew_job = GlueDataBrewStartJobOperator(\n    task_id=\"start_job\",\n    wait_for_completion=False,\n    job_name=JOB_NAME,\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Backend-Specific Tests with Pytest - Bash\nDESCRIPTION: Shows a Bash command using pytest's --backend option to execute only tests flagged for a particular backend (e.g., postgres). Requires the pytest testing framework, Airflow test suite, and implementation of the backend marker. Input is the backend name; output is filtered test execution. Assumes prior configuration supporting backend-aware testing.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\npytest --backend postgres\n```\n\n----------------------------------------\n\nTITLE: Installing SFTP Extras for Apache Airflow\nDESCRIPTION: Command to install SFTP hooks, operators and sensors for Apache Airflow. This enables secure file transfer integration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_68\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[sftp]'\n```\n\n----------------------------------------\n\nTITLE: Setting Jupyter Kernel Connection as Environment Variable in Bash\nDESCRIPTION: This snippet demonstrates how to set a Jupyter Kernel connection as an environment variable using a JSON string in Bash. It includes the host and extra parameters for the connection.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/connections/jupyter_kernel.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_JUPYTER_KERNEL_DEFAULT='{\"host\": \"remote_host\", \"extra\": {\"session_key\": \"notebooks\"}}'\n```\n\n----------------------------------------\n\nTITLE: Monitoring Amazon Bedrock Data Ingestion Job with Python\nDESCRIPTION: This snippet shows how to use the BedrockIngestionJobSensor to wait for an Amazon Bedrock data ingestion job to reach a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/bedrock.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_sensor_bedrock_ingest_data]\n# Example code not provided in the original text\n# [END howto_sensor_bedrock_ingest_data]\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Remove Spurious Headers in Provider Changelogs\nDESCRIPTION: This commit message, associated with commit 3878fe6fab dated 2023-07-05, notes the removal of unnecessary or incorrect headers from the changelogs for Apache Airflow providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n``Remove spurious headers for provider changelogs (#32373)``\n```\n\n----------------------------------------\n\nTITLE: Importing Product Object for Google Cloud Vision in Python\nDESCRIPTION: Imports the Product class from Google Cloud Vision library used for product operations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud.vision_v1.types import Product\n```\n\n----------------------------------------\n\nTITLE: Manual Yarn Installation on macOS\nDESCRIPTION: Commands for installing Node.js and Yarn package manager on macOS, including PATH configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/15_node_environment_setup.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbrew install node\nbrew install yarn\nyarn config set prefix ~/.yarn\n```\n\n----------------------------------------\n\nTITLE: Git Identity Configuration\nDESCRIPTION: Commands to configure Git user name and email for commit identification.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/10_working_with_git.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit config user.name \"someone\"\ngit config user.email \"someone@someplace.com\"\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow with Example DAGs in Breeze (Shell)\nDESCRIPTION: Executes the `breeze start-airflow` command with the `--load-example-dags` flag. This command initializes and starts an Airflow instance within the Breeze container, including the serialization of example DAGs into the database, which is necessary for comprehensive client testing.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\nbreeze start-airflow --load-example-dags\n```\n\n----------------------------------------\n\nTITLE: Enabling All Testable Integrations\nDESCRIPTION: Command to start Breeze with all testable integrations enabled\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/integration_tests.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze --integration all-testable\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow Helm Chart\nDESCRIPTION: Commands for basic and example-enabled Airflow chart installation\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/quick-start.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport RELEASE_NAME=example-release\nhelm install $RELEASE_NAME apache-airflow/airflow --namespace $NAMESPACE\n\nexport NAMESPACE=example-namespace\nhelm install $RELEASE_NAME apache-airflow/airflow \\\n  --namespace $NAMESPACE \\\n  --set-string \"env[0].name=AIRFLOW__CORE__LOAD_EXAMPLES\" \\\n  --set-string \"env[0].value=True\"\n```\n\n----------------------------------------\n\nTITLE: Demonstrating CloudFormation Operator with Trigger Rule in Python\nDESCRIPTION: Example of how to keep code snippets clean in system tests by adding test-specific parameters outside of the snippet boundaries. This shows a CloudFormationDeleteStackOperator with the trigger_rule parameter added separately.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/tests/system/amazon/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_cloudformation_delete_stack]\ndelete_stack = CloudFormationDeleteStackOperator(\n    task_id='delete_stack',\n    stack_name=cloudformation_stack_name,\n)\n# [END howto_operator_cloudformation_delete_stack]\ndelete_stack.trigger_rule = TriggerRule.ALL_DONE\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Metastore Connection using Environment Variable in Bash\nDESCRIPTION: This snippet demonstrates how to set up a Hive Metastore connection using an environment variable in Bash. It specifies the connection URI with the host, port, and authentication mechanism.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/connections/hive_metastore.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_METASTORE_DEFAULT='hive-metastore://hive-metastore-node:80?auth_mechanism=NOSASL'\n```\n\n----------------------------------------\n\nTITLE: Installing Multi-Platform Emulation Support\nDESCRIPTION: Command to enable multi-architecture build support through QEMU emulation\nSOURCE: https://github.com/apache/airflow/blob/main/dev/MANUALLY_BUILDING_IMAGES.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --privileged --rm tonistiigi/binfmt --install all\n```\n\n----------------------------------------\n\nTITLE: Updating Provider Dependencies with Pre-commit\nDESCRIPTION: Command to update provider dependencies using pre-commit hook after modifying provider.yaml files\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/10_advanced_breeze_topics.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run update-providers-dependencies  --all-files\n```\n\n----------------------------------------\n\nTITLE: Disabling Built-in Scheduler in Airflow Helm Chart\nDESCRIPTION: Configuration to disable the built-in scheduler when using an external scheduler instance.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nscheduler:\n  enabled: false\n```\n\n----------------------------------------\n\nTITLE: Changing to Airflow Directory\nDESCRIPTION: Command to change working directory to the cloned Airflow repository\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/01_installation.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd airflow\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow Helm Chart with Custom Parameters\nDESCRIPTION: Example command showing how to install the Airflow Helm chart with custom parameter values using the helm install command.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/parameters-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm install my-release apache-airflow/airflow \\\n    --set executor=CeleryExecutor \\\n    --set enablePodLaunching=false .\n```\n\n----------------------------------------\n\nTITLE: Download Airflow Shell Script\nDESCRIPTION: Commands to download and make executable a wrapper script for easier Airflow command execution\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LfO '{{ doc_root_url }}airflow.sh'\nchmod +x airflow.sh\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow Services\nDESCRIPTION: Command to start all Airflow services using Docker Compose\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Data Transformation Task with String Issue\nDESCRIPTION: Example showing problematic data transformation where order_data is passed as a string instead of a dictionary, leading to failure.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef transform(order_data):\n    total_order_value = sum(order_data.values())  # Fails because order_data is a str :(\n    return {\"total_order_value\": total_order_value}\n\n\ntransform = PythonOperator(\n    task_id=\"transform\",\n    op_kwargs={\"order_data\": \"{{ ti.xcom_pull('extract') }}\"},\n    python_callable=transform,\n)\n\nextract() >> transform\n```\n\n----------------------------------------\n\nTITLE: Building Provider Release Packages\nDESCRIPTION: Commands for building release candidate and final release packages for PyPI distribution in both wheel and source distribution formats.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/PROVIDER_DISTRIBUTIONS_DETAILS.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-provider-distributions --distribution-format both --version-suffix-for-pypi=rc1 http ...\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-provider-distributions --distribution-format both http ...\n```\n\n----------------------------------------\n\nTITLE: Installing Hashicorp Services Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Hashicorp Services integration, specifically for Vault.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[hashicorp]'\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTPS for OpenTelemetry Collector\nDESCRIPTION: YAML configuration for enabling HTTPS in OpenTelemetry collector, including SSL certificate and key file settings.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/traces.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      http:\n        endpoint: 0.0.0.0:4318\n        tls:\n          cert_file: \"/path/to/cert/cert.crt\"\n          key_file: \"/path/to/key/key.pem\"\n```\n\n----------------------------------------\n\nTITLE: Installing Common Messaging Provider with Amazon Extras using pip in Bash\nDESCRIPTION: Shows how to install the `apache-airflow-providers-common-messaging` package along with its optional 'amazon' extras using pip. This is necessary to use features that depend on the `apache-airflow-providers-amazon` package. Requires pip and a compatible Airflow version.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/messaging/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-common-messaging[amazon]\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Deprecate private_key_pass and rename to private_key_passphrase\nDESCRIPTION: This commit message, for commit 9351e2a2d4, describes the deprecation of the `private_key_pass` connection extra field in SFTPHook and its renaming to `private_key_passphrase` for clarity. Tracked in issue #14028.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_20\n\nLANGUAGE: text\nCODE:\n```\n``Depreciate private_key_pass in SFTPHook conn extra and rename to private_key_passphrase (#14028)``\n```\n\n----------------------------------------\n\nTITLE: Creating Release Notes Fragment\nDESCRIPTION: Command to create a release notes fragment using towncrier tool\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/18_contribution_workflow.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuv tool run towncrier create --dir airflow-core --config newsfragments/config.toml --content \"`cat airflow-core/newsfragments/template.significant.rst`\"\n```\n\n----------------------------------------\n\nTITLE: Exception Suppression for Unsupported JDBC Driver Methods\nDESCRIPTION: Bug fix that suppresses JExceptions when getAutoCommit and setAutoCommit methods aren't supported by the JDBC driver. Improves compatibility with various JDBC drivers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nSuppress JException when get_autocommit and set_autocommit methods aren't supported on JDBC driver (#43786)\n```\n\n----------------------------------------\n\nTITLE: Running Kubernetes Tests for Airflow\nDESCRIPTION: Command to run all Kubernetes tests for Airflow. The tests execute in a production container and communicate with the Kubernetes-deployed Airflow instance via a local virtualenv that is automatically created.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s tests\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenLineage Debug Mode in Airflow via INI\nDESCRIPTION: Activates OpenLineage debug mode by setting `debug_mode` to `true` in the `[openlineage]` section of the Airflow configuration. This causes more detailed diagnostic information to be sent in events and logged, intended for temporary debugging purposes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_23\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\ndebug_mode = true\n```\n\n----------------------------------------\n\nTITLE: Installing Oracle Provider Package with pip in Bash\nDESCRIPTION: Command to install the Oracle provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-oracle\n```\n\n----------------------------------------\n\nTITLE: Defining a Query for Google Cloud Datastore in Python\nDESCRIPTION: This snippet shows the definition of a query required by the CloudDatastoreRunQueryOperator. It includes the kind, filter, and order for the query operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nQUERY = {\n    \"partitionId\": {\"projectId\": GCP_PROJECT_ID},\n    \"readOptions\": {\"readConsistency\": \"STRONG\"},\n    \"query\": {\n        \"kind\": [{\"name\": \"airflow-system-test-entities\"}],\n        \"filter\": {\n            \"propertyFilter\": {\n                \"property\": {\"name\": \"symbol\"},\n                \"op\": \"EQUAL\",\n                \"value\": {\"stringValue\": \"GOOG\"},\n            }\n        },\n        \"order\": [{\"property\": {\"name\": \"symbol\"}, \"direction\": \"DESCENDING\"}],\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Building Regular Production Images\nDESCRIPTION: Command to build and release regular production Airflow images with version specification\nSOURCE: https://github.com/apache/airflow/blob/main/dev/MANUALLY_BUILDING_IMAGES.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nbreeze release-management release-prod-images --airflow-version \"${VERSION}\"\n```\n\n----------------------------------------\n\nTITLE: Prepare Documentation for March 2024 RC1 Wave (Excluded from v1.2.0 Changelog)\nDESCRIPTION: Indicates preparatory work on documentation for the first release candidate (RC1) wave of providers in March 2024, referenced by pull request #37876. This change was intentionally excluded from the main changelog notes for version 1.2.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_23\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 1st wave (RC1) March 2024 (#37876)\n```\n\n----------------------------------------\n\nTITLE: GCS ACL Permission Field Update - After Example\nDESCRIPTION: Example showing the new snake_case field naming convention required after the breaking change when setting GCS bucket ACL permissions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nset_acl_permission = GCSBucketCreateAclEntryOperator(\n    task_id=\"gcs-set-acl-permission\",\n    bucket=BUCKET_NAME,\n    entity=\"user-{{ task_instance.xcom_pull('get-instance')['persistence_iam_identity']\".split(':', 2)[1] }}\",\n    role=\"OWNER\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Providers Microsoft Azure Package\nDESCRIPTION: Command to install the Microsoft Azure provider package for Apache Airflow using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-microsoft-azure\n```\n\n----------------------------------------\n\nTITLE: Git Commit History Entry Format\nDESCRIPTION: Standard format for git commit entries showing commit hash, date, and commit message. Each entry follows the pattern of commit hash link, date, and descriptive message.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n`fdd9b6f65b <https://github.com/apache/airflow/commit/fdd9b6f65b608c516b8a062b058972d9a45ec9e3>`__  2020-08-25   ``Enable Black on Providers Packages (#10543)``\n```\n\n----------------------------------------\n\nTITLE: Ensuring Airbyte Provider Compatibility with APIs (Commit Message)\nDESCRIPTION: Commit message stating efforts to ensure the Apache Airflow Airbyte provider is compatible with both Airbyte Cloud and Config APIs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_18\n\nLANGUAGE: text\nCODE:\n```\nEnsure Airbyte Provider is Compatible with Cloud and Config APIs (#37943)\n```\n\n----------------------------------------\n\nTITLE: Dumping Kubernetes Cluster Logs\nDESCRIPTION: Command to view logs from the Kubernetes cluster using the breeze utility.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s logs\n```\n\n----------------------------------------\n\nTITLE: Updating SecretsMasker Import Path in Apache Airflow\nDESCRIPTION: Migration of import statement for the SecretsMasker module from the utils.log package to the new sdk.execution_time package. This change affects how DAG authors and users access the secrets masking functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/46375.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.utils.log.secrets_masker import SecretsMasker  # Old import\nfrom airflow.sdk.execution_time.secrets_masker import SecretsMasker  # New import\n```\n\n----------------------------------------\n\nTITLE: Installing Atlassian Jira Provider with Dependencies\nDESCRIPTION: Command to install the Atlassian Jira provider package with common compatibility dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/atlassian/jira/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-atlassian-jira[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Running Ruff-format Check for Multiple Files using Breeze in Bash\nDESCRIPTION: This command uses Breeze to run the Ruff-format check for multiple specific files in the Airflow example DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nbreeze static-checks --type ruff-format --file airflow/example_dags/example_bash_operator.py \\\n    airflow/example_dags/example_python_operator.py\n```\n\n----------------------------------------\n\nTITLE: Retrieving Connection from Secret Backend\nDESCRIPTION: Command to verify and retrieve connection details from the configured secret backend.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/secrets-backends/google-cloud-secret-manager-backend.rst#2025-04-22_snippet_8\n\nLANGUAGE: console\nCODE:\n```\n$ airflow connections get first-connection\nId: null\nConnection Id: first-connection\nConnection Type: mysql\nHost: example.org\nSchema: ''\nLogin: null\nPassword: null\nPort: null\nIs Encrypted: null\nIs Extra Encrypted: null\nExtra: {}\nURI: mysql://example.org\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Google Cloud Secrets\nDESCRIPTION: Commands to delete connection and variable secrets from Google Cloud Secret Manager.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/secrets-backends/google-cloud-secret-manager-backend.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ngcloud beta secrets delete airflow-connections-first-connection\ngcloud beta secrets delete airflow-variables-first-variable\n```\n\n----------------------------------------\n\nTITLE: Preparing Documentation for Provider Release Wave\nDESCRIPTION: Commit message related to preparing documentation updates for the March 1st wave of provider releases. References pull request #47545.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs for Mar 1st wave of providers (#47545)\n```\n\n----------------------------------------\n\nTITLE: Installing the Apache Airflow gRPC Provider via pip\nDESCRIPTION: This command installs the `apache-airflow-providers-grpc` package using the pip package manager. It should be executed in a command-line environment on top of an existing Airflow 2 installation (version 2.9.0 or higher). This package enables interactions with gRPC services within Airflow DAGs. Ensure Python version 3.9, 3.10, 3.11, or 3.12 is used.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-grpc\n```\n\n----------------------------------------\n\nTITLE: Listing Testable Provider Integrations (Python List)\nDESCRIPTION: Defines the list of provider-specific integrations (e.g., external services like databases, message queues) that are considered testable in the current build environment. This example includes 'mongo' and 'kafka'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n\\['mongo', 'kafka'\\]\n```\n\n----------------------------------------\n\nTITLE: Importing Enums for Google Cloud Vision in Python\nDESCRIPTION: Imports the enums module from Google Cloud Vision library for constants like feature types and likelihood values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud.vision import enums\n```\n\n----------------------------------------\n\nTITLE: Installing Breeze with PIPX\nDESCRIPTION: Command to install Breeze development environment tool using PIPX package manager with editable flag\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/01_installation.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npipx install -e ./dev/breeze\n```\n\n----------------------------------------\n\nTITLE: Installing SFTP Provider with Common Compatibility Dependencies\nDESCRIPTION: Command to install the SFTP provider package with common compatibility dependencies using pip\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-sftp[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry in Airflow Configuration File\nDESCRIPTION: Basic configuration settings for enabling OpenTelemetry metrics in Airflow. These settings need to be added to the airflow.cfg file to enable metrics collection via OpenTelemetry.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/metrics.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[metrics]\notel_on = True\notel_host = localhost\notel_port = 8889\notel_prefix = airflow\notel_interval_milliseconds = 30000  # The interval between exports, defaults to 60000\notel_ssl_active = False\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Airflow Helm Chart Release\nDESCRIPTION: Sets up essential environment variables including version numbers and repository root path for the release process.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Set Version\nexport VERSION=1.1.0\nexport VERSION_SUFFIX=rc1\n\n# Set AIRFLOW_REPO_ROOT to the path of your git repo\nexport AIRFLOW_REPO_ROOT=$(pwd -P)\n\n# Example after cloning\ngit clone https://github.com/apache/airflow.git airflow\ncd airflow\nexport AIRFLOW_REPO_ROOT=$(pwd -P)\n```\n\n----------------------------------------\n\nTITLE: Verifying Airflow Deployment\nDESCRIPTION: Commands to verify the Airflow deployment and access the web UI\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/quick-start.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods --namespace $NAMESPACE\nhelm list --namespace $NAMESPACE\n\nkubectl port-forward svc/$RELEASE_NAME-webserver 8080:8080 --namespace $NAMESPACE\n```\n\n----------------------------------------\n\nTITLE: Implementing Database Hello Operator with MySQL Hook\nDESCRIPTION: Extended operator implementation that uses MySqlHook to fetch data from a MySQL database.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass HelloDBOperator(BaseOperator):\n    def __init__(self, name: str, mysql_conn_id: str, database: str, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.name = name\n        self.mysql_conn_id = mysql_conn_id\n        self.database = database\n\n    def execute(self, context):\n        hook = MySqlHook(mysql_conn_id=self.mysql_conn_id, schema=self.database)\n        sql = \"select name from user\"\n        result = hook.get_first(sql)\n        message = f\"Hello {result['name']}\"\n        print(message)\n        return message\n```\n\n----------------------------------------\n\nTITLE: Pytest Importorskip for Suspended Provider - Python - python\nDESCRIPTION: Demonstrates adding pytest.importorskip at the start of a test module to skip execution if a dependent provider is not installed. This is used to avoid test collection/import errors when another provider has been suspended or removed. Place this line at the top of the relevant test file; requires pytest as a dependency.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/MANAGING_PROVIDERS_LIFECYCLE.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npytest.importorskip(\"apache.airflow.providers.google\")\n```\n\n----------------------------------------\n\nTITLE: Installing Elasticsearch Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Elasticsearch provider package on top of an existing Apache Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-elasticsearch\n```\n\n----------------------------------------\n\nTITLE: Temporary Airflow CLI Completion\nDESCRIPTION: Command for one-time activation of Airflow command completion\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\neval \"$(register-python-argcomplete airflow)\"\n```\n\n----------------------------------------\n\nTITLE: Renaming SparkSubmitOperator Parameters\nDESCRIPTION: A change in version 4.1.1 that renames the 'spark_conn_id' parameter to 'conn_id' in the SparkSubmitOperator for better consistency.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"SparkSubmitOperator: rename spark_conn_id to conn_id (#31952)\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Apache Airflow Installation Information in Go Template\nDESCRIPTION: A Go template that generates post-installation notes including access URLs, login credentials, and various deprecation warnings for Apache Airflow Helm chart installation.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/templates/NOTES.txt#2025-04-22_snippet_0\n\nLANGUAGE: go-template\nCODE:\n```\n{{/*\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n*/}}\n\nThank you for installing Apache {{ title .Chart.Name }} {{ .Values.airflowVersion }}!\n\nYour release is named {{ .Release.Name }}.\n\n{{- if or .Values.ingress.web.enabled .Values.ingress.flower.enabled .Values.ingress.enabled }}\nYou can now access your service(s) by following defined Ingress urls:\n\n{{- if .Values.ingress.web.host }}\n\nDEPRECATION WARNING:\n   `ingress.web.host` has been renamed to `ingress.web.hosts` and is now an array.\n   Please change your values as support for the old name will be dropped in a future release.\n\n{{- end }}\n\n{{- if .Values.ingress.web.tls }}\n\nDEPRECATION WARNING:\n   `ingress.web.tls` has been renamed to `ingress.web.hosts[*].tls` and can be set per host.\n   Please change your values as support for the old name will be dropped in a future release.\n\n{{- end }}\n\n{{- if .Values.ingress.flower.host }}\n\nDEPRECATION WARNING:\n   `ingress.flower.host` has been renamed to `ingress.flower.hosts` and is now an array.\n   Please change your values as support for the old name will be dropped in a future release.\n\n{{- end }}\n\n\n{{- if .Values.ingress.flower.tls }}\n\nDEPRECATION WARNING:\n   `ingress.flower.tls` has been renamed to `ingress.flower.hosts[*].tls` and can be set per host.\n   Please change your values as support for the old name will be dropped in a future release.\n\n{{- end }}\n\n{{- if .Values.ingress.enabled }}\n\nDEPRECATION WARNING:\n   `ingress.enabled` has been deprecated. There are now separate flags to control the webserver and\n   flower individually, ``ingress.web.enabled`` and ``ingress.flower.enabled``.\n   Please change your values as support for the old name will be dropped in a future release.\n\n{{- end }}\n\n{{- if or .Values.ingress.web.enabled .Values.ingress.enabled }}\nAirflow Webserver:\n{{- range .Values.ingress.web.hosts | default (list .Values.ingress.web.host) }}\n      {{- $tlsEnabled := $.Values.ingress.web.tls.enabled -}}\n      {{- $hostname := $.Values.ingress.web.host -}}\n      {{- if . | kindIs \"string\" | not }}\n      {{- if .tls }}\n      {{- $tlsEnabled = .tls.enabled -}}\n      {{- $hostname = .name -}}\n      {{- end }}\n      {{- end }}\n      http{{ if $tlsEnabled }}s{{ end }}://{{ (tpl $hostname $) }}{{ $.Values.ingress.web.path }}/\n{{- end }}\n{{- end }}\n{{- if and (or .Values.ingress.flower.enabled .Values.ingress.enabled) (or (contains \"CeleryExecutor\" .Values.executor) (contains \"CeleryKubernetesExecutor\" .Values.executor)) }}\nFlower dashboard:\n{{- range .Values.ingress.flower.hosts | default (list .Values.ingress.flower.host) }}\n      {{- $tlsEnabled := $.Values.ingress.flower.tls.enabled -}}\n      {{- $hostname := $.Values.ingress.flower.host -}}\n      {{- if . | kindIs \"string\" | not }}\n      {{- if .tls }}\n      {{- $tlsEnabled = .tls.enabled -}}\n      {{- $hostname = .name -}}\n      {{- end }}\n      {{- end }}\n      http{{ if $tlsEnabled }}s{{ end }}://{{ (tpl $hostname $) }}{{ $.Values.ingress.flower.path }}/\n{{- end }}\n{{- end }}\n{{- else }}\nYou can now access your dashboard(s) by executing the following command(s) and visiting the corresponding port at localhost in your browser:\n\nAirflow Webserver:     kubectl port-forward svc/{{ include \"airflow.fullname\" . }}-webserver {{ .Values.ports.airflowUI }}:{{ .Values.ports.airflowUI }} --namespace {{ .Release.Namespace }}\n\n{{- if .Values.flower.enabled }}\n{{- if or (contains \"CeleryExecutor\" .Values.executor) (contains \"CeleryKubernetesExecutor\" .Values.executor)}}\nFlower dashboard:      kubectl port-forward svc/{{ include \"airflow.fullname\" . }}-flower {{ .Values.ports.flowerUI }}:{{ .Values.ports.flowerUI }} --namespace {{ .Release.Namespace }}\n\n{{- end }}\n{{- end }}\n{{- end }}\n\n\n{{- if .Values.webserver.defaultUser.enabled}}\nDefault Webserver (Airflow UI) Login credentials:\n    username: {{ .Values.webserver.defaultUser.username }}\n    password: {{ .Values.webserver.defaultUser.password }}\n{{- end }}\n\n{{- if .Values.postgresql.enabled }}\nDefault Postgres connection credentials:\n    username: {{ .Values.data.metadataConnection.user }}\n    password: {{ .Values.data.metadataConnection.pass }}\n    port: {{ .Values.data.metadataConnection.port }}\n\n{{- end }}\n\n{{- if not .Values.fernetKeySecretName }}\n\nYou can get Fernet Key value by running the following:\n\n    echo Fernet Key: $(kubectl get secret --namespace {{ .Release.Namespace }} {{ .Release.Name }}-fernet-key -o jsonpath=\"{.data.fernet-key}\" | base64 --decode)\n\n{{- end }}\n\n{{- if or (contains \"KubernetesExecutor\" .Values.executor) (contains \"CeleryKubernetesExecutor\" .Values.executor) }}\n{{- if and (not .Values.logs.persistence.enabled) (eq (lower (tpl .Values.config.logging.remote_logging .)) \"false\") }}\n\nWARNING:\n    Kubernetes workers task logs may not persist unless you configure log persistence or remote logging!\n    Logging options can be found at: https://airflow.apache.org/docs/helm-chart/stable/manage-logs.html\n    (This warning can be ignored if logging is configured with environment variables or secrets backend)\n\n{{- end }}\n{{- end }}\n\n{{- if and .Values.dags.gitSync.enabled .Values.dags.gitSync.sshKeySecret (not .Values.dags.gitSync.knownHosts)}}\n\n#####################################################\n#  WARNING: You should set dags.gitSync.knownHosts  #\n#####################################################\n\nYou are using ssh authentication for your gitsync repo, however you currently have SSH known_hosts verification disabled,\nmaking you susceptible to man-in-the-middle attacks!\n\nInformation on how to set knownHosts can be found here:\nhttps://airflow.apache.org/docs/helm-chart/stable/production-guide.html#knownhosts\n\n{{- end }}\n\n{{- if .Values.flower.extraNetworkPolicies }}\n\nDEPRECATION WARNING:\n   `flower.extraNetworkPolicies` has been renamed to `flower.networkPolicy.peers`.\n   Please change your values as support for the old name will be dropped in a future release.\n\n{{- end }}\n\n\n{{- if .Values.webserver.extraNetworkPolicies }}\n\nDEPRECATION WARNING:\n    `webserver.extraNetworkPolicies` has been renamed to `webserver.networkPolicy.peers`.\n    Please change your values as support for the old name will be dropped in a future release.\n\n{{- end }}\n\n{{- if not (or .Values.webserverSecretKey .Values.webserverSecretKeySecretName) }}\n\n{{- if .Values.securityContext }}\n\n DEPRECATION WARNING:\n    `securityContext` has been renamed to `securityContexts`, to be enabled on container and pod level.\n    Please change your values as support for the old name will be dropped in a future release.\n\n{{- end }}\n\n###########################################################\n#  WARNING: You should set a static webserver secret key  #\n###########################################################\n\nYou are using a dynamically generated webserver secret key, which can lead to\nunnecessary restarts of your Airflow components.\n\nInformation on how to set a static webserver secret key can be found here:\nhttps://airflow.apache.org/docs/helm-chart/stable/production-guide.html#webserver-secret-key\n\n{{- end }}\n\n{{- if or .Values.postgresql.postgresqlUsername .Values.postgresql.postgresqlPassword }}\n\n   {{ fail \"postgresql.postgresqlUsername and postgresql.postgresqlPassword are no longer supported. If you wish to use the 'postgres' user, set its password with postgresql.auth.postgresPassword. If you wish to create a different user, do so with postgresql.auth.username and postgresql.auth.password.\" }}\n\n{{- end }}\n\n{{- if ne .Values.executor (tpl .Values.config.core.executor $) }}\n   {{ fail \"Please configure the executor with `executor`, not `config.core.executor`.\" }}\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Using Externally Provisioned PVC for Logging\nDESCRIPTION: Helm command to configure Airflow to use an existing ReadWriteMany PVC for log storage by specifying the existing claim name.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-logs.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airflow apache-airflow/airflow \\\n  --set logs.persistence.enabled=true \\\n  --set logs.persistence.existingClaim=my-volume-claim\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Format for Exported Airflow Connections\nDESCRIPTION: This JSON snippet shows the structure of exported Airflow connections, including connection IDs, types, and configuration details.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"airflow_db\": {\n    \"conn_type\": \"mysql\",\n    \"host\": \"mysql\",\n    \"login\": \"root\",\n    \"password\": \"plainpassword\",\n    \"schema\": \"airflow\",\n    \"port\": null,\n    \"extra\": null\n  },\n  \"druid_broker_default\": {\n    \"conn_type\": \"druid\",\n    \"host\": \"druid-broker\",\n    \"login\": null,\n    \"password\": null,\n    \"schema\": null,\n    \"port\": 8082,\n    \"extra\": \"{\\\"endpoint\\\": \\\"druid/v2/sql\\\"}\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Beam Provider with Cross-provider Dependencies\nDESCRIPTION: Command to install the Apache Beam provider package with its cross-provider dependencies for additional functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-beam[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Exporting SFTP Connection with Host Key - Bash\nDESCRIPTION: This snippet provides a Bash example for setting the default Airflow SFTP connection using environment variable syntax, specifying both host_key (base64-encoded, URL-encoded) and disabling host key checks in the query parameters. It's intended for use on systems configuring Airflow via environment variables, with dependencies being correct user/key/host details and Airflow installed. Inputs: the connection string with credentials and extras; outputs: Airflow processes this as the SFTP default connection. The snippet requires that host_key and no_host_key_check values are correctly URL-encoded. Limitation: avoid leaking sensitive connection information in process environments.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/connections/sftp.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_CONN_SFTP_DEFAULT='sftp://user:pass@localhost:22?host_key=AAAHD...YDWwq%3D%3D&no_host_key_check=false'\n```\n\n----------------------------------------\n\nTITLE: Executing the Airflow Python Client Test Script (Shell Script)\nDESCRIPTION: Runs the Python client test script located at `/opt/airflow/clients/python/test_python_client.py` from within the Breeze container using the `python` interpreter. This script executes tests against the Airflow instance started in the previous step, typically connecting to `http://localhost:8080` with `admin`/`admin` credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_23\n\nLANGUAGE: shell script\nCODE:\n```\npython /opt/airflow/clients/python/test_python_client.py\n```\n\n----------------------------------------\n\nTITLE: Verifying Airflow and Python Versions in Docker Images (bash)\nDESCRIPTION: These commands should be run locally to ensure that the Airflow and Python versions inside the Docker container match the host where the Airflow scheduler runs. This is necessary for compatibility, especially when using the AWS Executor. Replace <image_name> with your built image tag.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/executors/general.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run <image_name> version\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run <image_name> python --version\n```\n\n----------------------------------------\n\nTITLE: Setting SSH Connection with Encoded Private Key\nDESCRIPTION: Example of setting an SSH connection environment variable with an encoded private key.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/docs/connections/ssh.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SSH_SERVER='ssh://127.0.0.1?private_key=-----BEGIN+RSA+PRIVATE+KEY-----%0D%0AMII.....jBV50%0D%0A-----END+RSA+PRIVATE+KEY-----'\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Standard Provider via pip\nDESCRIPTION: Command for installing the Standard Provider package on top of an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-standard\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-weaviate via pip\nDESCRIPTION: This shell command installs the `apache-airflow-providers-weaviate` package using pip, enabling Weaviate integration within an existing Apache Airflow 2 environment. It requires pip and a compatible Python environment with Apache Airflow 2 (version 2.9.0 or higher) already installed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/weaviate/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-weaviate\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with Constraints\nDESCRIPTION: This snippet demonstrates how to install Apache Airflow with a specific version and constraints file. It uses pip to install Airflow with the 'celery' extra and applies constraints from a local file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-pypi.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install \"apache-airflow[celery]==|version|\" --constraint \"my-constraints.txt\"\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Providers Microsoft Azure Package with Amazon Extra\nDESCRIPTION: Command to install the Microsoft Azure provider package for Apache Airflow with the Amazon extra dependency using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-microsoft-azure[amazon]\n```\n\n----------------------------------------\n\nTITLE: Deleting a Google Dataprep Flow in Python\nDESCRIPTION: Example usage of the DataprepDeleteFlowOperator to delete a flow in Google Dataprep.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataprep.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# [START how_to_dataprep_delete_flow_operator]\n# Example usage code would be here\n# [END how_to_dataprep_delete_flow_operator]\n```\n\n----------------------------------------\n\nTITLE: Running Tests Using Breeze CLI - Bash\nDESCRIPTION: These bash commands demonstrate invoking Breeze to run Airflow unit and provider tests from the host machine or inside the Breeze container. Options include specifying test modules, enabling database reset, adjusting providers tested, and controlling test groupings. Dependencies include Breeze, Docker, and Airflow with required test dependencies. Key parameters are test target paths, --db-reset, --test-type, and others. Inputs are the command-line switches and test targets; outputs are test suite results. Proper container setup is assumed.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-tests providers/http/tests/http/hooks/test_http.py tests/core/test_core.py --db-reset --log-cli-level=DEBUG\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze core-testing tests --db-reset\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze providers-testing tests --db-reset\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-tests --db-reset tests/core/test_core.py::TestCore\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-tests --test-type Other\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-tests --test-type Providers\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-tests --test-type \"Providers[airbyte,http]\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-tests --test-type \"Providers[-amazon,google]\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-tests --skip--docker-compose-down\n```\n\n----------------------------------------\n\nTITLE: Using TaskInstance log_url in PythonOperator for Failure Notification\nDESCRIPTION: This snippet demonstrates how to use the 'ti.log_url' property available in the TaskInstance context to include log links in task callbacks. It's shown as an example of replacing the removed 'conf' variable usage.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/44820.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nPythonOperator(\n    task_id=\"my_task\",\n    python_callable=my_task_callable,\n    on_failure_callback=SmtpNotifier(\n        from_email=\"example@example.com\",\n        to=\"example@example.com\",\n        subject=\"Task {{ ti.task_id }} failed\",\n        html_content=\"Task <b>{{ ti.task_id }}</b> failed. Log URL: {{ ti.log_url }}\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Stopping Neptune Database Cluster with Airflow Operator\nDESCRIPTION: Example showing how to use the StopNeptuneDbClusterOperator to stop a running Neptune database cluster. The operator can be run in deferrable mode by setting deferrable=True and requires aiobotocore module for deferrable execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/neptune.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstop_cluster = StopNeptuneDbClusterOperator(\n    task_id=\"stop_cluster\",\n    db_cluster_identifier=\"neptune-cluster-demo\",\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Issue Content for RC Testing Status using Breeze in Shell\nDESCRIPTION: Uses the 'breeze' tool to generate the main content body for the testing status issue. It requires specifying the previous release tag (`<PREVIOUS_RELEASE>`) and the current release candidate tag (`${VERSION}${VERSION_SUFFIX}`).\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nbreeze release-management generate-issue-content-helm-chart \\\n--previous-release helm-chart/<PREVIOUS_RELEASE> --current-release helm-chart/${VERSION}${VERSION_SUFFIX}\n```\n\n----------------------------------------\n\nTITLE: Automating Minimum Airflow Version Check for Providers\nDESCRIPTION: Excluded Change (Version > 3.1.1): Introduces full automation for determining the minimum required Airflow version for providers, referencing pull request #30994.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Add full automation for min Airflow version for providers (#30994)``\n```\n\n----------------------------------------\n\nTITLE: Apache Iceberg Provider Package Name Declaration\nDESCRIPTION: Package name declaration for the Apache Iceberg provider in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/iceberg/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\napache-airflow-providers-apache-iceberg\n```\n\n----------------------------------------\n\nTITLE: Preparing Provider Documentation with Breeze (Bash)\nDESCRIPTION: Uses the Breeze command `release-management prepare-provider-documentation` to generate documentation for Apache Airflow providers. The `--answer yes` flag can be added for non-interactive execution. This is part of the provider release procedure.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-provider-documentation\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenLineage Execution Timeout in airflow.cfg\nDESCRIPTION: Configuration for increasing the OpenLineage execution timeout in airflow.cfg, which defines how long OpenLineage methods can run in a separate process.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_9\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\nexecution_timeout = 60\n```\n\n----------------------------------------\n\nTITLE: Killing All Docker Containers - Bash\nDESCRIPTION: A Bash one-liner for force-killing all running Docker containers on the system using docker ps -q to retrieve container IDs and docker kill to stop them. Intended for post-test cleanup after interrupted or parallel test runs. Input is no arguments; output is termination of all running Docker containers. Use with caution, as it will kill all containers indiscriminately.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ndocker kill $(docker ps -q)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop for Kerberos Impersonation\nDESCRIPTION: XML configuration to enable Kerberos impersonation in Hadoop's core-site.xml file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/kerberos.rst#2025-04-22_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hadoop.proxyuser.airflow.groups</name>\n  <value>*</value>\n</property>\n\n<property>\n  <name>hadoop.proxyuser.airflow.users</name>\n  <value>*</value>\n</property>\n\n<property>\n  <name>hadoop.proxyuser.airflow.hosts</name>\n  <value>*</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Basic Airflow Webserver Docker Configuration\nDESCRIPTION: Initial Docker command to run Airflow webserver with admin password configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n--env \"_AIRFLOW_WWW_USER_PASSWORD_CMD=echo admin\" \\\n      apache/airflow:3.1.0.dev0-python3.9 webserver\n```\n\n----------------------------------------\n\nTITLE: Installing All Airflow Provider Dependencies\nDESCRIPTION: Command to install all system dependencies needed for full Airflow installation with all providers.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install apt-transport-https apt-utils build-essential ca-certificates dirmngr \\\nfreetds-bin freetds-dev git graphviz graphviz-dev krb5-user ldap-utils libffi-dev \\\nlibkrb5-dev libldap2-dev libpq-dev libsasl2-2 libsasl2-dev libsasl2-modules \\\nlibssl-dev locales lsb-release openssh-client sasl2-bin \\\nsoftware-properties-common sqlite3 sudo unixodbc unixodbc-dev\n```\n\n----------------------------------------\n\nTITLE: Creating Asset with Extra Information\nDESCRIPTION: Demonstrates creating an Asset with additional metadata in the extra dictionary.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nexample_asset = Asset(\n    \"s3://asset/example.csv\",\n    extra={\"team\": \"trainees\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Cosmos DB Connection Using Environment Variable in Bash\nDESCRIPTION: Example of how to set up an Azure Cosmos DB connection using an environment variable in Bash. This shows how to specify the connection string in URI syntax with URL-encoded components, including the endpoint URI, master key, database name, and collection name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/azure_cosmos.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AZURE_COSMOS_DEFAULT='azure-cosmos://https%3A%2F%2Fairflow.azure.com:master%20key@?database_name=mydatabase&collection_name=mycollection'\n```\n\n----------------------------------------\n\nTITLE: Docker Image Naming Patterns\nDESCRIPTION: Naming patterns for Python base image, CI image, and production image with version variables\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_27\n\nLANGUAGE: markdown\nCODE:\n```\npython:<X.Y>-slim-bookworm\nairflow/<BRANCH>/ci/python<X.Y>\nairflow/<BRANCH>/prod/python<X.Y>\n```\n\n----------------------------------------\n\nTITLE: Building Production Image with Python Version\nDESCRIPTION: Command to build a production image with specific Python version and APT dependencies\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/06_managing_docker_images.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbreeze prod-image build --python 3.9 --additional-dev-deps \"libasound2-dev\" --additional-runtime-apt-deps \"libasound2\"\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Hooks in Bash\nDESCRIPTION: Command to execute all pre-commit hooks for code quality checks and validation before committing changes to the repository.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/16_adding_api_endpoints.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Enabling Security Context Constraints for OpenShift\nDESCRIPTION: Configuration for enabling Security Context Constraints (SCC) role binding in OpenShift.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nrbac:\n  create: true\n  createSCCRoleBinding: true\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Interactive Shell for Kubernetes Testing\nDESCRIPTION: Example output showing the entry into an interactive shell for Kubernetes testing, which displays the active Kubernetes cluster and executor information for the testing environment.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_18\n\nLANGUAGE: text\nCODE:\n```\nEntering interactive k8s shell.\n\n(kind-airflow-python-3.9-v1.24.2:KubernetesExecutor)>\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Local DAGs for Airflow Executor (bash)\nDESCRIPTION: This snippet builds a Docker image with DAGs loaded from a host-local directory into the container. The build arguments specify the source folder (host_dag_path) and destination within the container (container_dag_path). You must update the Airflow config if using a non-default DAGs path.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/executors/general.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t my-airflow-image --build-arg host_dag_path=./dags_on_host --build-arg container_dag_path=/path/on/container .\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Apache Airflow Helm Chart\nDESCRIPTION: Command to remove the Airflow deployment and associated Kubernetes components.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete airflow --namespace airflow\n```\n\n----------------------------------------\n\nTITLE: Apache Kafka Hook Documentation Structure\nDESCRIPTION: ReStructuredText documentation detailing various Kafka hooks including base hook, admin client hook, consumer hook, producer hook and a custom authentication error class. Each section includes references to official Kafka documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/hooks.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _howto/hook:KafkaBaseHook:\n\nKafkaHook\n------------------------\n\nA base hook for interacting with Apache Kafka. Use this hook as a base class when creating your own Kafka hooks.\nFor parameter definitions take a look at :class:`~airflow.providers.apache.kafka.hooks.base.KafkaBaseHook`.\n\n\n.. _howto/hook:KafkaAdminClientHook:\n\nKafkaAdminClientHook\n------------------------\n\nA hook for interacting with an Apache Kafka cluster.\nFor parameter definitions take a look at :class:`~airflow.providers.apache.kafka.hooks.client.KafkaAdminClientHook`.\n\nReference\n\"\"\"\"\"\"\"\"\"\n\nFor further information, look at `Apache Kafka Admin config documentation <https://kafka.apache.org/documentation/#adminclientconfigs>`_.\n\n\n.. _howto/hook:KafkaAuthenticationError:\n\nKafkaAuthenticationError\n------------------------\n\nCustom exception for Kafka authentication failures.\n\nReference\n\"\"\"\"\"\"\"\"\"\n\nFor further information, look at\n`Confluent Kafka Documentation <https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#pythonclient-kafkaerror>`_.\n\n\n.. _howto/hook:KafkaConsumerHook:\n\nKafkaConsumerHook\n------------------------\n\nA hook for creating a Kafka Consumer. This hook is used by the ``ConsumeFromTopicOperator`` and the ``AwaitMessageTrigger``.\nFor parameter definitions take a look at :class:`~airflow.providers.apache.kafka.hooks.consume.KafkaConsumerHook`.\n\nReference\n\"\"\"\"\"\"\"\"\"\n\nFor further information, look at `Apache Kafka Consumer documentation <https://kafka.apache.org/documentation/#consumerconfigs>`_.\n\n\n.. _howto/hook:KafkaProducerHook:\n\nKafkaProducerHook\n------------------------\n\nA hook for creating a Kafka Producer. This hook is used by the ``ProduceToTopicOperator``.\nFor parameter definitions take a look at :class:`~airflow.providers.apache.kafka.hooks.produce.KafkaProducerHook`.\n\nReference\n\"\"\"\"\"\"\"\"\"\n\nFor further information, look at `Apache Kafka Producer documentation <https://kafka.apache.org/documentation/#producerconfigs>`_.\n```\n\n----------------------------------------\n\nTITLE: Manual Timer Usage with Duration Logging in Python\nDESCRIPTION: Shows how to use Stats.timer() without metric submission to manually track and log execution time duration.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.stats import Stats\n\n...\n\nwith Stats.timer() as timer:\n    ...\n\nlog.info(\"Code took %.3f seconds\", timer.duration)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Verifying Apache Airflow PyPI Packages\nDESCRIPTION: This bash script downloads the Apache Airflow wheel package from PyPI along with its signature and checksum files for local verification.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-sources.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nAIRFLOW_VERSION=\"{{ airflow_version }}\"\nairflow_download_dir=\"$(mktemp -d)\"\npip download --no-deps \"apache-airflow==${AIRFLOW_VERSION}\" --dest \"${airflow_download_dir}\"\ncurl \"https://downloads.apache.org/airflow/${AIRFLOW_VERSION}/apache_airflow-${AIRFLOW_VERSION}-py3-none-any.whl.asc\" \\\n    -L -o \"${airflow_download_dir}/apache_airflow-${AIRFLOW_VERSION}-py3-none-any.whl.asc\"\ncurl \"https://downloads.apache.org/airflow/${AIRFLOW_VERSION}/apache_airflow-${AIRFLOW_VERSION}-py3-none-any.whl.sha512\" \\\n    -L -o \"${airflow_download_dir}/apache_airflow-${AIRFLOW_VERSION}-py3-none-any.whl.sha512\"\necho\necho \"Please verify files downloaded to ${airflow_download_dir}\"\nls -la \"${airflow_download_dir}\"\necho\n```\n\n----------------------------------------\n\nTITLE: Installing Weaviate Integration for Apache Airflow\nDESCRIPTION: Command to install Weaviate integration package that provides hooks and operators for Apache Airflow\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_42\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[weaviate]'\n```\n\n----------------------------------------\n\nTITLE: Installing Common-Messaging Extras for Apache Airflow\nDESCRIPTION: Command to install Core Messaging Operators for Apache Airflow. This provides basic messaging functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_56\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[common-messaging]'\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Spark Transport Info Injection via Environment Variable\nDESCRIPTION: Sets the `AIRFLOW__OPENLINEAGE__SPARK_INJECT_TRANSPORT_INFO` environment variable to `true`. This enables the automatic injection of Airflow's OpenLineage transport configuration into Spark job properties for supported operators, simplifying configuration for Spark lineage reporting.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_32\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__SPARK_INJECT_TRANSPORT_INFO=true\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Google Cloud SDK for Airflow\nDESCRIPTION: This command builds a custom Airflow Docker image with Google Cloud SDK installed. The image is based on a specified Airflow version and tagged for later use.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/recipes.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . \\\n  --pull \\\n  --build-arg BASE_AIRFLOW_IMAGE=\"apache/airflow:2.0.2\" \\\n  --tag my-airflow-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Initializing Airflow Database in Breeze\nDESCRIPTION: Commands to initialize Airflow database and create admin user in Breeze environment\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nairflow db reset\nairflow users create \\\n        --username admin \\\n        --firstname FIRST_NAME \\\n        --lastname LAST_NAME \\\n        --role Admin \\\n        --email admin@example.org\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Pinot Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Pinot integration, enabling all Pinot related hooks.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[apache-pinot]'\n```\n\n----------------------------------------\n\nTITLE: Return Common Data Structure in DBApi Classes\nDESCRIPTION: Reference to a bug fix ensuring that DBApi derived classes return a common data structure for consistency across database providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: reStructuredText\nCODE:\n```\n* ``Return common data structure in DBApi derived classes``\n```\n\n----------------------------------------\n\nTITLE: Pausing a Google Cloud Tasks Queue in Python\nDESCRIPTION: This snippet shows how to pause a Google Cloud Tasks queue using the CloudTasksQueuePauseOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# [START pause_queue]\nCloudTasksQueuePauseOperator(\n    task_id=\"pause_queue\",\n    location=LOCATION,\n    queue_name=QUEUE_ID,\n).execute(context=context)\n# [END pause_queue]\n```\n\n----------------------------------------\n\nTITLE: Output of Breeze Kubernetes Image Build\nDESCRIPTION: This text block shows the output from the `breeze k8s build-k8s-image` command. It displays the Docker build process logs, including steps like loading the Dockerfile and context, using the specified base image, copying necessary files (example DAGs, pod templates), and finally exporting and tagging the resulting Kubernetes-ready Airflow image.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nBuilding the K8S image for Python 3.9 using Airflow base image: ghcr.io/apache/airflow/main/prod/python3.9:latest\n\n[+] Building 0.1s (8/8) FINISHED\n => [internal] load build definition from Dockerfile                                                                                                                                                                                                                                           0.0s\n => => transferring dockerfile: 301B                                                                                                                                                                                                                                                           0.0s\n => [internal] load .dockerignore                                                                                                                                                                                                                                                              0.0s\n => => transferring context: 35B                                                                                                                                                                                                                                                               0.0s\n => [internal] load metadata for ghcr.io/apache/airflow/main/prod/python3.9:latest                                                                                                                                                                                                             0.0s\n => [1/3] FROM ghcr.io/apache/airflow/main/prod/python3.9:latest                                                                                                                                                                                                                               0.0s\n => [internal] load build context                                                                                                                                                                                                                                                              0.0s\n => => transferring context: 3.00kB                                                                                                                                                                                                                                                            0.0s\n => CACHED [2/3] COPY airflow/example_dags/ /opt/airflow/dags/                                                                                                                                                                                                                                 0.0s\n => CACHED [3/3] COPY airflow/kubernetes_executor_templates/ /opt/airflow/pod_templates/                                                                                                                                                                                                       0.0s\n => exporting to image                                                                                                                                                                                                                                                                         0.0s\n => => exporting layers                                                                                                                                                                                                                                                                        0.0s\n => => writing image sha256:c0bdd363c549c3b0731b8e8ce34153d081f239ee2b582355b7b3ffd5394c40bb                                                                                                                                                                                                   0.0s\n => => naming to ghcr.io/apache/airflow/main/prod/python3.9-kubernetes:latest\n\nNEXT STEP: You might now upload your k8s image by:\n\nbreeze k8s upload-k8s-image\n```\n\n----------------------------------------\n\nTITLE: Updating Example DAGs to Use TaskFlow API\nDESCRIPTION: Excluded Change (Version 2.1.0): Updates miscellaneous provider example DAGs to utilize the TaskFlow API where appropriate, showcasing modern Airflow DAG authoring, referencing pull request #18278.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_32\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Updating miscellaneous provider DAGs to use TaskFlow API where applicable (#18278)``\n```\n\n----------------------------------------\n\nTITLE: Restricting Spark Binary Options\nDESCRIPTION: A bug fix in version 4.0.1 that restricts the spark binary passed via connection extra to valid values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"Only restrict spark binary passed via extra (#30213)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Pagerduty Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Pagerduty provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pagerduty/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-pagerduty\n```\n\n----------------------------------------\n\nTITLE: Transferring Google Sheets Data to Amazon S3 using GoogleApiToS3Operator in Python\nDESCRIPTION: This snippet demonstrates how to use the GoogleApiToS3Operator to load data from Google Sheets and save it to an Amazon S3 file. It specifies the Google Sheets API endpoint, authentication method, and S3 destination details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/google_api_to_s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSHEET_ID = \"<YOUR_SHEET_ID>\"\nRANGE = \"<YOUR_RANGE>\"\n\nload_sheet = GoogleApiToS3Operator(\n    task_id=\"load_sheet\",\n    google_api_service_name=\"sheets\",\n    google_api_service_version=\"v4\",\n    google_api_endpoint_path=f\"sheets.spreadsheets.values.get\",\n    google_api_endpoint_params={\n        \"spreadsheetId\": SHEET_ID,\n        \"range\": RANGE,\n    },\n    s3_destination_key=f\"s3://{{{{ var.value.s3_bucket }}}}/google_sheets_{{ ds_nodash }}.json\",\n    google_api_response_via_xcom=\"sheet_values\",\n    delegate_to=GOOGLE_DELEGATE_TO,\n    gcp_conn_id=GOOGLE_GCP_CONN_ID,\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Minor Version Branch and Checking Out Test Branch\nDESCRIPTION: These commands create a new minor version branch and check out the test branch for release preparation.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nbreeze release-management create-minor-branch --version-branch ${VERSION_BRANCH}\n```\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout v${VERSION_BRANCH}-test\ngit reset --hard origin/v${VERSION_BRANCH}-test\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare 2nd wave of providers in December\nDESCRIPTION: This text is a commit message summary indicating preparation work for the second wave of Apache Airflow provider releases in December, linked to pull request #36373.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_29\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare 2nd wave of providers in December (#36373)\n```\n\n----------------------------------------\n\nTITLE: Displaying Selective Check Outputs Table in Markdown\nDESCRIPTION: This markdown table lists all available selective check outputs for the Apache Airflow project. It includes the output name, its meaning, an example value, and whether it's a list-type output. The table provides a comprehensive overview of configuration options and test parameters used in the project's CI/CD pipeline.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Output                                         | Meaning of the output                                                                                  | Example value                           | List |\n|------------------------------------------------|--------------------------------------------------------------------------------------------------------|-----------------------------------------|------|\n| all-python-versions                            | List of all python versions there are available in the form of JSON array                              | ['3.9', '3.10']                       |      |\n| all-python-versions-list-as-string             | List of all python versions there are available in the form of space separated string                  | 3.9 3.10                                | *    |\n| all-versions                                   | If set to true, then all python, k8s, DB versions are used for tests.                                  | false                                   |      |\n| basic-checks-only                              | Whether to run all static checks (\"false\") or only basic set of static checks (\"true\")                 | false                                   |      |\n| build_system_changed_in_pyproject_toml         | When builds system dependencies changed in pyproject.toml changed in the PR.                           | false                                   |      |\n| chicken-egg-providers                          | List of providers that should be considered as \"chicken-egg\" - expecting development Airflow version   |                                         |      |\n| ci-image-build                                 | Whether CI image build is needed                                                                       | true                                    |      |\n| core-test-types-list-as-strings-in-json                 | Which test types should be run for unit tests for core                                                 | API Always Providers                    | *    |\n| debug-resources                                | Whether resources usage should be printed during parallel job execution (\"true\"/ \"false\")              | false                                   |      |\n| default-branch                                 | Which branch is default for the build (\"main\" for main branch, \"v2-4-test\" for 2.4 line etc.)          | main                                    |      |\n| default-constraints-branch                     | Which branch is default for the build (\"constraints-main\" for main branch, \"constraints-2-4\" etc.)     | constraints-main                        |      |\n| default-helm-version                           | Which Helm version to use as default                                                                   | v3.9.4                                  |      |\n| default-kind-version                           | Which Kind version to use as default                                                                   | v0.16.0                                 |      |\n| default-kubernetes-version                     | Which Kubernetes version to use as default                                                             | v1.25.2                                 |      |\n| default-mysql-version                          | Which MySQL version to use as default                                                                  | 5.7                                     |      |\n| default-postgres-version                       | Which Postgres version to use as default                                                               | 10                                      |      |\n| default-python-version                         | Which Python version to use as default                                                                 | 3.9                                     |      |\n| disable-airflow-repo-cache                     | Disables cache of the repo main cache in CI - aiflow will be installed without main installation cache | true                                    |      |\n| docker-cache                                   | Which cache should be used for images (\"registry\", \"local\" , \"disabled\")                               | registry                                |      |\n| docs-build                                     | Whether to build documentation (\"true\"/\"false\")                                                        | true                                    |      |\n| docs-list-as-string                            | What filter to apply to docs building - based on which documentation packages should be built          | apache-airflow helm-chart google        | *    |\n| excluded-providers-as-string   c               | List of providers that should be excluded from the build as space-separated string                     | amazon google                           | *    |\n| force-pip                                      | Whether pip should be forced in the image build instead of uv (\"true\"/\"false\")                         | false                                   |      |\n| full-tests-needed                              | Whether this build runs complete set of tests or only subset (for faster PR builds) [1]              | false                                   |      |\n| generated-dependencies-changed                 | Whether generated dependencies have changed (\"true\"/\"false\")                                           | false                                   |      |\n| has-migrations                                 | Whether the PR has migrations (\"true\"/\"false\")                                                         | false                                   |      |\n| hatch-build-changed                            | When hatch build.py changed in the PR.                                                                 | false                                   |      |\n| helm-test-packages-list-as-string              | List of helm packages to test as JSON array                                                            | [\"airflow_aux\", \"airflow_core\"]       | *    |\n| helm-version                                   | Which Helm version to use for tests                                                                    | v3.15.3                                 |      |\n| include-success-outputs                        | Whether to include outputs of successful parallel tests (\"true\"/\"false\")                               | false                                   |      |\n| individual-providers-test-types-list-as-strings-in-json | Which test types should be run for unit tests for providers (individually listed)                      | Providers[amazon] Providers[google] | *    |\n| is-airflow-runner                              | Whether runner used is an airflow or infrastructure runner (true if airflow/false if infrastructure)   | false                                   |      |\n| is-amd-runner                                  | Whether runner used is an AMD one                                                                      | true                                    |      |\n| is-arm-runner                                  | Whether runner used is an ARM one                                                                      | false                                   |      |\n| is-committer-build                             | Whether the build is triggered by a committer                                                          | false                                   |      |\n| is-k8s-runner                                  | Whether the build runs on our k8s infrastructure                                                       | false                                   |      |\n| is-legacy-ui-api-labeled                       | Whether the PR is labeled as legacy UI/API                                                             | false                                   |      |\n| is-self-hosted-runner                          | Whether the runner is self-hosted                                                                      | false                                   |      |\n| is-vm-runner                                   | Whether the runner uses VM to run                                                                      | true                                    |      |\n| kind-version                                   | Which Kind version to use for tests                                                                    | v0.24.0                                 |      |\n| kubernetes-combos-list-as-string               | All combinations of Python version and Kubernetes version to use for tests as space-separated string   | 3.9-v1.25.2 3.10-v1.28.13               | *    |\n| kubernetes-versions                            | All Kubernetes versions to use for tests as JSON array                                                 | ['v1.25.2']                           |      |\n| kubernetes-versions-list-as-string             | All Kubernetes versions to use for tests as space-separated string                                     | v1.25.2                                 | *    |\n| latest-versions-only                           | If set, the number of Python, Kubernetes, DB versions will be limited to the latest ones.              | false                                   |      |\n```\n\n----------------------------------------\n\nTITLE: Displaying TaskFlow Decorator Error Message - Text\nDESCRIPTION: This snippet illustrates the error message users encounter when attempting to use the TaskFlow docker decorator with an incompatible version of Airflow (pre-Airflow 2.2). No dependencies are required, as this is a static output intended to inform the user about unsupported functionality. The snippet helps clarify version compatibility for decorators in Airflow's Docker provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/docker/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nAttributeError: '_TaskDecorator' object has no attribute 'docker'\n```\n\n----------------------------------------\n\nTITLE: Installing SFTP Provider with Dependencies\nDESCRIPTION: Command to install the SFTP provider package with common compatibility dependencies via pip\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-sftp[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Commit History Entry Format\nDESCRIPTION: Example of the commit history entry format used throughout the changelog\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n`8b146152d6 <https://github.com/apache/airflow/commit/8b146152d62118defb3004c997c89c99348ef948>`__  2023-06-20   ``Add note about dropping Python 3.7 for providers (#32015)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Updated documentation for June 2021 provider release\nDESCRIPTION: Linked to commit 9c94b72d44, this message signifies updates made to the documentation specifically for the June 2021 release of Airflow providers, tracked via issue #16294.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_21\n\nLANGUAGE: text\nCODE:\n```\n``Updated documentation for June 2021 provider release (#16294)``\n```\n\n----------------------------------------\n\nTITLE: Interactive Test Shell with Breeze\nDESCRIPTION: Commands for entering interactive shell and running tests with pytest-xdist parallelization.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbreeze shell --backend none --python 3.9\n> pytest tests --skip-db-tests -n auto\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Cloud Provider with Common Compatibility Dependencies\nDESCRIPTION: Command to install the dbt Cloud provider package along with its common compatibility dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-dbt-cloud[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Extended Template Fields with Subclassing\nDESCRIPTION: Example showing how to extend template fields through subclassing.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass MyHelloOperator(HelloOperator):\n    template_fields: Sequence[str] = (*HelloOperator.template_fields, \"world\")\n```\n\n----------------------------------------\n\nTITLE: Generating Constraint Files in Airflow\nDESCRIPTION: Commands to build CI image and generate different types of constraint files for Apache Airflow. Sets up the environment and generates constraints for different modes including standard constraints, source providers, and no providers.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/MANUALLY_GENERATING_IMAGE_CACHE_AND_CONSTRAINTS.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image build --run-in-parallel --upgrade-to-newer-dependencies --answer yes\nbreeze release-management generate-constraints --airflow-constraints-mode constraints --run-in-parallel --answer yes\nbreeze release-management generate-constraints --airflow-constraints-mode constraints-source-providers --run-in-parallel --answer yes\nbreeze release-management generate-constraints --airflow-constraints-mode constraints-no-providers --run-in-parallel --answer yes\n\nAIRFLOW_SOURCES=$(pwd)\n```\n\n----------------------------------------\n\nTITLE: Creating Variable Secret in Google Cloud\nDESCRIPTION: Command to create a new variable secret in Google Cloud Secret Manager using gcloud CLI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/secrets-backends/google-cloud-secret-manager-backend.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ echo \"secret_content\" | gcloud beta secrets create \\\n    airflow-variables-first-variable \\\n    --data-file=-\\\n    --replication-policy=automatic\n```\n\n----------------------------------------\n\nTITLE: Debugging Airflow DAGs using Python Debugger (pdb) on Command Line\nDESCRIPTION: This snippet demonstrates how to use the Python debugger (pdb) to debug an Airflow DAG file on the command line. It shows the command to start the debugger and some basic debugging steps.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/debug.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nroot@ef2c84ad4856:/opt/airflow# python -m pdb airflow/example_dags/example_bash_operator.py\n> /opt/airflow/airflow/example_dags/example_bash_operator.py(18)<module>()\n-> \"\"\"Example DAG demonstrating the usage of the BashOperator.\"\"\"\n(Pdb) b 45\nBreakpoint 1 at /opt/airflow/airflow/example_dags/example_bash_operator.py:45\n(Pdb) c\n> /opt/airflow/airflow/example_dags/example_bash_operator.py(45)<module>()\n-> bash_command='echo 1',\n(Pdb) run_this_last\n<Task(EmptyOperator): run_this_last>\n```\n\n----------------------------------------\n\nTITLE: Installing Snowflake Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Snowflake integration, enabling Snowflake hooks and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_37\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[snowflake]'\n```\n\n----------------------------------------\n\nTITLE: Listing Google Ads Links with GoogleAnalyticsAdminListGoogleAdsLinksOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the GoogleAnalyticsAdminListGoogleAdsLinksOperator to list Google Ads links in Google Analytics. It shows the operator usage and notes that Jinja templating can be applied to the operator's template fields.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/analytics_admin.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nGoogleAnalyticsAdminListGoogleAdsLinksOperator(\n    task_id=\"list_google_ads_links\",\n    parent=\"properties/{{ task_instance.xcom_pull('create_property')['name'].split('/')[-1] }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing ArangoDB Provider for Apache Airflow\nDESCRIPTION: Command for installing the ArangoDB provider package on top of an existing Airflow 2 installation. This package enables integration between Apache Airflow and ArangoDB.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-arangodb\n```\n\n----------------------------------------\n\nTITLE: Defining Migration Rules for Apache Airflow using Ruff\nDESCRIPTION: Specifies the migration rules needed for updating import statements using the Ruff linter, focusing on the AIR302 rule for relocating ObjectStoragePath and attach function.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/45425.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n* Migration rules needed\n\n  * ruff\n\n    * AIR302\n\n      * [ ] ``airflow.io.path.ObjectStoragePath``  ``airflow.sdk.ObjectStoragePath``\n      * [ ] ``airflow.io.attach``  ``airflow.sdk.io.attach``\n```\n\n----------------------------------------\n\nTITLE: Specifying Postgres Test Versions (Python List)\nDESCRIPTION: Defines the specific versions of Postgres to be used during testing. This example specifies version '12'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\\['12'\\]\n```\n\n----------------------------------------\n\nTITLE: Preparing Task SDK Distributions\nDESCRIPTION: Command to prepare Airflow Task SDK wheel package in the dist folder.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-task-sdk-distributions\n```\n\n----------------------------------------\n\nTITLE: Installing Microsoft WinRM Extras for Apache Airflow\nDESCRIPTION: Command to install Windows Remote Management (WinRM) hooks and operators for Apache Airflow. This enables integration with Windows systems via WinRM.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_64\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[microsoft-winrm]'\n```\n\n----------------------------------------\n\nTITLE: Configuring Private Key in Airflow UI Extras\nDESCRIPTION: Example JSON for configuring a private key in the Airflow UI extras field, with newlines replaced by '\\n'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/docs/connections/ssh.rst#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\"private_key\": \"-----BEGIN RSA PRIVATE KEY-----\\nMII.....jBV50\\n-----END RSA PRIVATE KEY-----\"}\n```\n\n----------------------------------------\n\nTITLE: Auto-building Documentation with Live Preview\nDESCRIPTION: Commands to run documentation server with auto-refresh capability for iterative development.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/11_documentation_building.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd providers/fab\nuv run --group docs build-docs --autobuild\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Image from a Different GitHub Repository\nDESCRIPTION: Builds a production Airflow image using a fork of the Airflow repository (potiuk/airflow). Constraints are also downloaded from the same repository.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . -f Dockerfile --pull --build-arg AIRFLOW_INSTALLATION_METHOD=\"github\" --build-arg AIRFLOW_SOURCES_FROM=\"apache/airflow=potiuk/airflow@main\" --build-arg CONSTRAINTS_GITHUB_REPOSITORY=\"potiuk/airflow\" --tag \"my-company/airflow:potiuk-main\"\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-imap via pip\nDESCRIPTION: This shell command installs the `apache-airflow-providers-imap` package using the pip package manager. It is intended to be run in an environment where Apache Airflow 2 (version 2.9.0 or higher) is already installed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/imap/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-imap\n```\n\n----------------------------------------\n\nTITLE: Updating SQLite Connection URI in Python\nDESCRIPTION: This code snippet shows how to use a connection URI in the SQLiteHook instead of individual connection parameters. It improves consistency and flexibility in database connections.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"Use connection URI in SqliteHook (#28721)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Samba Extras for Apache Airflow\nDESCRIPTION: Command to install Samba hooks and operators for Apache Airflow. This enables integration with Samba file sharing services.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_50\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[samba]'\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Airflow Secrets\nDESCRIPTION: ReStructuredText documentation file that outlines the Apache License header and defines the structure for Airflow secrets documentation, including sections for variables, connections, and further reading topics.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\nSecrets\n========\n\n.. toctree::\n    :maxdepth: 1\n    :glob:\n    :caption: Further reading:\n\n    Encryption at rest <fernet>\n    Using external Secret stores <secrets-backend/index>\n    mask-sensitive-values\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: Sphinx documentation include directive that imports security documentation from the devel-common extensions folder.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/livy/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Adding PyPI Package with Constraints to Airflow Image (Dockerfile)\nDESCRIPTION: Dockerfile example showing how to install a Python package (lxml) from PyPI using pip while respecting the Airflow constraints file (`${HOME}/constraints.txt`). This ensures compatibility with the specific Airflow version used in the base image.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_12\n\nLANGUAGE: Dockerfile\nCODE:\n```\n.. exampleinclude:: docker-examples/extending/add-pypi-packages-constraints/Dockerfile\n    :language: Dockerfile\n    :start-after: [START Dockerfile]\n    :end-before: [END Dockerfile]\n```\n\n----------------------------------------\n\nTITLE: Setting YDB Connection Environment Variables in Bash\nDESCRIPTION: Examples of configuring YDB database connections using environment variables in two formats: URI string and JSON string. Shows how to specify host, credentials, port, and database parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/docs/connections/ydb.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_CONN_YDB_DEFAULT1='ydb://grpcs://my_name:my_password@example.com:2135/?database=%2Flocal'\nAIRFLOW_CONN_YDB_DEFAULT2='{\"conn_type\": \"ydb\", \"host\": \"grpcs://example.com\", \"login\": \"my_name\", \"password\": \"my_password\", \"port\": 2135, \"extra\": {\"database\": \"/local\"}}'\n```\n\n----------------------------------------\n\nTITLE: Installing Tabular Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Tabular integration, enabling Tabular hooks.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_39\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[tabular]'\n```\n\n----------------------------------------\n\nTITLE: Cloning and Updating Airflow SVN Release Repository - Shell Script\nDESCRIPTION: These two shell script snippets help users checkout the Airflow release directory from Apache SVN or keep an existing checkout up to date. The commands require 'svn' to be installed on the system and ensure the presence of local copies of current release artifacts needed for further comparison or validation steps. No inputs other than the URL; outputs are files and directories in the current working directory.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\nsvn co https://dist.apache.org/repos/dist/dev/airflow\n```\n\nLANGUAGE: shell\nCODE:\n```\nsvn update .\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Cluster for Airflow using Breeze\nDESCRIPTION: This command configures the previously created KinD cluster for Airflow deployment. It involves recreating necessary Kubernetes namespaces (`airflow`, `test-namespace`) and deploying test resources such as persistent volumes and services required by Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s configure-cluster\n```\n\n----------------------------------------\n\nTITLE: Running Complete Kubernetes Test Suite\nDESCRIPTION: Command to run the entire Kubernetes test suite, which creates a cluster, builds images, deploys Airflow, runs tests, and cleans up resources.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s run-complete-tests\n```\n\n----------------------------------------\n\nTITLE: Limiting Task Log Size in Airflow\nDESCRIPTION: This configuration demonstrates how to limit the log size of tasks by setting the max_bytes parameter in the task handler configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/advanced-logging-configuration.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom copy import deepcopy\nfrom pydantic.utils import deep_update\nfrom airflow.config_templates.airflow_local_settings import DEFAULT_LOGGING_CONFIG\n\nLOGGING_CONFIG = deep_update(\n    deepcopy(DEFAULT_LOGGING_CONFIG),\n    {\n        \"handlers\": {\n            \"task\": {\"max_bytes\": 104857600, \"backup_count\": 1}  # 100MB and keep 1 history rotate log.\n        }\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Client Packages with Local Hatch in Bash\nDESCRIPTION: This snippet demonstrates an alternative method to build Airflow Client packages using a local Hatch installation. It's faster and requires fewer resources compared to the Dockerized build.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-python-client --distribution-format both --use-local-hatch\n```\n\n----------------------------------------\n\nTITLE: Creating Directory in Dataform Workspace in Python\nDESCRIPTION: This snippet demonstrates how to create a directory with a given path in a specified workspace using the DataformMakeDirectoryOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_make_directory]\n# [END howto_operator_make_directory]\n```\n\n----------------------------------------\n\nTITLE: Connecting to Airflow Database\nDESCRIPTION: Command to open a database shell for direct database queries using Airflow CLI.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/07_local_virtualenv.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nairflow db shell\n```\n\n----------------------------------------\n\nTITLE: Consolidating Hook Management in AnalyticDBSparkSensor\nDESCRIPTION: Refactors the `AnalyticDBSparkSensor` (likely Alibaba provider) to consolidate how it manages database connections (hooks), improving code structure, referenced by #34435.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\nConsolidate hook management in AnalyticDBSparkSensor (#34435)\n```\n\n----------------------------------------\n\nTITLE: Defining Dataproc Metastore Backup Configuration in Python\nDESCRIPTION: Shows how to create a Python dictionary specifying the configuration for a Dataproc Metastore service backup. This includes parameters like the backup description. This dictionary is intended to be passed to the `DataprocMetastoreCreateBackupOperator`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nBACKUP = {\n    \"description\": \"Backup description\",\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Livy Provider Package with pip\nDESCRIPTION: Command to install the Apache Livy provider package on top of an existing Airflow installation. This installs the necessary components to use Apache Livy functionality in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/livy/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-livy\n```\n\n----------------------------------------\n\nTITLE: Installing Provider Dependencies with UV\nDESCRIPTION: Instructions for syncing provider dependencies using UV package manager within a specific provider directory.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/07_local_virtualenv.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd providers/PROVIDER\nuv sync\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Database Connection Command via Environment Variable in Bash\nDESCRIPTION: This command shows how to set a command for retrieving the database connection string using an environment variable in Bash.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-config.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__DATABASE__SQL_ALCHEMY_CONN_CMD=bash_command_to_run\n```\n\n----------------------------------------\n\nTITLE: Displaying DAG Run Task Instances Image in ReStructuredText\nDESCRIPTION: ReStructuredText directive to display the DAG Run Task Instances tab image in dark mode.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/ui.rst#2025-04-22_snippet_5\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. image:: img/ui-dark/dag_run_task_instances.png\n  :alt: DAG Run - Task Instances tab (dark mode)\n```\n\n----------------------------------------\n\nTITLE: Handling Deprecated Arguments in Python Tests\nDESCRIPTION: Example of proper warning capture using pytest.warns context manager for testing deprecated components.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef test_deprecated_argument():\n    with pytest.warns(AirflowProviderDeprecationWarning, match=\"expected warning pattern\"):\n        SomeDeprecatedClass(foo=\"bar\", spam=\"egg\")\n```\n\n----------------------------------------\n\nTITLE: Running DB Tests with Breeze Command\nDESCRIPTION: Examples of running database tests using the Breeze testing environment with different backends and configurations\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-tests --run-db-tests-only --backend postgres --run-in-parallel\n```\n\n----------------------------------------\n\nTITLE: Auto-applying apply_default Decorator (Breaking Change)\nDESCRIPTION: Breaking Change (Version 2.0.0): Automatically applies the `apply_default` decorator functionality, removing the need for explicit application. This change requires Airflow 2.1.0+, referencing pull request #15667.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_36\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Auto-apply apply_default decorator (#15667)``\n```\n\n----------------------------------------\n\nTITLE: Using get_parsing_context After Migration\nDESCRIPTION: Updated code example showing the new import path for get_parsing_context function from airflow.sdk module while maintaining the same functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/45694.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import get_parsing_context\n\ncurrent_dag_id = get_parsing_context().dag_id\n\n# The rest of the code remains the same\n```\n\n----------------------------------------\n\nTITLE: Bumping Minimum Airflow Version to 2.7.0 (Commit Message)\nDESCRIPTION: Commit message announcing that the minimum required Apache Airflow version for providers, including Airbyte, has been increased to 2.7.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\nBump minimum Airflow version in providers to Airflow 2.7.0 (#39240)\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 4.9.1\nDESCRIPTION: ReStructuredText formatted changelog entry detailing commits and changes for ODBC provider version 4.9.1.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n4.9.1\n.....\n\nLatest change: 2025-03-09\n\n==================================================================================================  ===========  =========================================================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =========================================================================================================================\n`492ecfe5c0 <https://github.com/apache/airflow/commit/492ecfe5c03102bfb710108038ebd5fc50cb55b5>`__  2025-03-09   ``Prepare docs for Mar 1st wave of providers (#47545)``\n```\n\n----------------------------------------\n\nTITLE: Resetting Airflow Database\nDESCRIPTION: Command to reset the Airflow database before initialization\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_gitpod.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairflow db reset\n```\n\n----------------------------------------\n\nTITLE: Activating Global Python Argcomplete\nDESCRIPTION: Command to enable global auto-completion for all argcomplete-enabled Python applications\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo activate-global-python-argcomplete\n```\n\n----------------------------------------\n\nTITLE: Running Core Tests with Minimum Dependencies in Breeze\nDESCRIPTION: Command to run Airflow core tests with minimum dependencies using Breeze. This ensures compatibility with the lowest supported versions of dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-tests --force-lowest-dependencies --test-type \"Core\"\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with Google Cloud support using pip\nDESCRIPTION: This command installs Apache Airflow with Google Cloud integration capabilities. It uses pip to install the 'apache-airflow[google]' package, which includes the necessary dependencies for working with Google Cloud services.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/_partials/prerequisite_tasks.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[google]'\n```\n\n----------------------------------------\n\nTITLE: Running Core Tests with Breeze\nDESCRIPTION: Command to run all core tests using Breeze testing command\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-tests\n```\n\n----------------------------------------\n\nTITLE: Attaching Metadata to Asset Events\nDESCRIPTION: Shows how to attach extra metadata to asset events using yield statements.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import Metadata, asset\n\n\n@asset(uri=\"s3://asset/example.csv\", schedule=None)\ndef example_s3(self):  # 'self' here refers to the current asset.\n    df = ...  # Get a Pandas DataFrame to write.\n    # Write df to asset...\n    yield Metadata(self, {\"row_count\": len(df)})\n```\n\n----------------------------------------\n\nTITLE: Template Fields for CloudVideoIntelligenceDetectVideoLabelsOperator (Python)\nDESCRIPTION: This snippet lists the template fields for CloudVideoIntelligenceDetectVideoLabelsOperator, enabling dynamic parameterization in Airflow DAGs. Template fields can include input_uri and result_path, which allows users to inject values at runtime using Airflow's templating engine (e.g., Jinja). Dependencies are Airflow and Google provider modules. This is essential for DAG configurability when automating workflows across environments.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/video_intelligence.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\"input_uri\", \"result_path\")\n```\n\n----------------------------------------\n\nTITLE: Entering Breeze Container without Mounting Local Sources\nDESCRIPTION: Command to enter a Breeze container without mounting local sources, allowing developers to interact with the exact CI environment.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/07_running_ci_locally.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze shell --mount-sources skip [OPTIONS]\n```\n\n----------------------------------------\n\nTITLE: Installing the Base Samba Provider via pip (Bash)\nDESCRIPTION: This command installs the core `apache-airflow-providers-samba` package using pip. It requires an existing Apache Airflow 2 installation (version 2.9.0 or higher).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-samba\n```\n\n----------------------------------------\n\nTITLE: Setting MySQL Connection URI in Environment Variable\nDESCRIPTION: Example of setting MySQL connection URI with SSL parameters as environment variable in Bash\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/connections/mysql.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_MYSQL_DEFAULT='mysql://mysql_user:XXXXXXXXXXXX@1.1.1.1:3306/mysqldb?ssl=%7B%22cert%22%3A+%22%2Ftmp%2Fclient-cert.pem%22%2C+%22ca%22%3A+%22%2Ftmp%2Fserver-ca.pem%22%2C+%22key%22%3A+%22%2Ftmp%2Fclient-key.pem%22%7D'\n```\n\n----------------------------------------\n\nTITLE: Listing Commits for Version 3.4.0 (rst)\nDESCRIPTION: This block lists commits associated with version 3.4.0 using reStructuredText table format. It includes commit hashes linked to GitHub, commit dates, and subject lines detailing changes like documentation updates for the November 2021 provider release and modifications to SalesforceHook extra requirements.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ======================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ======================================================================\n`853576d901 <https://github.com/apache/airflow/commit/853576d9019d2aca8de1d9c587c883dcbe95b46a>`__  2021-11-30   ``Update documentation for November 2021 provider's release (#19882)``\n`a24066bc4c <https://github.com/apache/airflow/commit/a24066bc4c3d8a218bd29f6c8fef80781488dd55>`__  2021-11-11   ``Do not require all extras for SalesforceHook (#19530)``\n==================================================================================================  ===========  ======================================================================\n```\n\n----------------------------------------\n\nTITLE: Building Docker Images for Airflow Executor with AWS Credentials (bash)\nDESCRIPTION: These code snippets show how to build Airflow Docker images for AWS Executor by supplying AWS credentials and configuration as build-time arguments. They support various authentication methods through environment variables or explicit build arguments, including region and session tokens. Users must replace placeholders with actual AWS values. Not recommended for production as credentials may persist in image history. Requires Docker installed, and Airflow should match host/container Python and Airflow versions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/executors/general.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t my-airflow-image \\\n --build-arg aws_default_region=YOUR_DEFAULT_REGION .\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker buildx build --platform=linux/amd64 -t my-airflow-image \\\n --build-arg aws_default_region=YOUR_DEFAULT_REGION .\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t my-airflow-image \\\n --build-arg aws_access_key_id=YOUR_ACCESS_KEY \\\n --build-arg aws_secret_access_key=YOUR_SECRET_KEY \\\n --build-arg aws_default_region=YOUR_DEFAULT_REGION \\\n --build-arg aws_session_token=YOUR_SESSION_TOKEN .\n```\n\n----------------------------------------\n\nTITLE: Importing PGP Keys for Apache Airflow Package Verification - Bash\nDESCRIPTION: Commands for importing PGP keys used to verify package signatures. Shows three alternative methods using gpg, pgpk, or pgp.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/installing-helm-chart-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngpg -i KEYS\n```\n\nLANGUAGE: bash\nCODE:\n```\npgpk -a KEYS\n```\n\nLANGUAGE: bash\nCODE:\n```\npgp -ka KEYS\n```\n\n----------------------------------------\n\nTITLE: Stopping Amazon Managed Service for Apache Flink Application - Python\nDESCRIPTION: Example code showing how to stop an Amazon Managed Service for Apache Flink application using the KinesisAnalyticsV2StopApplicationOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/kinesis_analytics.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstop_application = KinesisAnalyticsV2StopApplicationOperator(\n    task_id=\"stop_application\",\n    application_name=APPLICATION_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Building PROD Image from Sources with Breeze (Bash)\nDESCRIPTION: This command utilizes the Breeze tool to build the Airflow Production (PROD) Docker image from the current local source code. This image is optimized for size and suitable for production deployments.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbreeze prod-image build\n```\n\n----------------------------------------\n\nTITLE: Listing Pre-commit Check Skipping Rules in Airflow CI\nDESCRIPTION: This markdown snippet outlines the rules for skipping specific pre-commit checks in Airflow's CI process. It details conditions based on file changes and branch types that determine which checks are run or skipped to improve CI efficiency.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* The `identity` check is always skipped (saves space to display all changed files in CI)\n* The provider specific checks are skipped when builds are running in v2_* branches (we do not build\n  providers from those branches. Those are the checks skipped in this case:\n  * check-airflow-provider-compatibility\n  * check-extra-packages-references\n  * check-provider-yaml-valid\n  * lint-helm-chart\n  * mypy-providers\n* If \"full tests\" mode is detected, no more pre-commits are skipped - we run all of them\n* The following checks are skipped if those files are not changed:\n  * if no `All Providers Python files` changed - `mypy-providers` check is skipped\n  * if no `All Airflow Python files` changed - `mypy-airflow` check is skipped\n  * if no `All Dev Python files` changed - `mypy-dev` check is skipped\n  * if no `UI files` changed - `ts-compile-format-lint-ui` check is skipped\n  * if no `WWW files` changed - `ts-compile-format-lint-www` check is skipped\n  * if no `All Python files` changed - `flynt` check is skipped\n  * if no `Helm files` changed - `lint-helm-chart` check is skipped\n  * if no `All Providers Python files` and no `All Providers Yaml files` are changed -\n    `check-provider-yaml-valid` check is skipped\n```\n\n----------------------------------------\n\nTITLE: KEDA Scaling Query for Airflow Tasks\nDESCRIPTION: SQL query used by KEDA to determine the desired number of Celery workers based on running and queued tasks. Calculates ceiling of task count divided by worker concurrency.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/keda.rst#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n    ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }})\nFROM task_instance\nWHERE state='running' OR state='queued'\n```\n\n----------------------------------------\n\nTITLE: Installing Databricks Provider with SQL Support via pip\nDESCRIPTION: Command to install the Databricks provider package with common SQL dependencies via pip package manager.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-databricks[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Installing Microsoft Azure Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Microsoft Azure integration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[microsoft-azure]'\n```\n\n----------------------------------------\n\nTITLE: Fixing SparkSql Operator Infinite Log Loop\nDESCRIPTION: A bug fix in version 2.0.2 that resolves an issue where SparkSql Operator logs could go into an infinite loop.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"fix bug of SparkSql Operator log  going to infinite loop. (#19449)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Common SQL Provider with OpenLineage Extras using Pip (Bash)\nDESCRIPTION: This bash command demonstrates how to install the `apache-airflow-providers-common-sql` package using pip, along with optional dependencies specified by the 'openlineage' extra. This enables integration features with OpenLineage when using the common SQL provider. Requires pip and an existing Airflow environment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-common-sql[openlineage]\n```\n\n----------------------------------------\n\nTITLE: Getting Kind Help Information\nDESCRIPTION: Command to display the help information for Kind, showing available commands for cluster management.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_25\n\nLANGUAGE: text\nCODE:\n```\nkind --help\n```\n\n----------------------------------------\n\nTITLE: Listing Dataproc Batches with Airflow Operator in Python\nDESCRIPTION: This snippet shows how to use the `DataprocListBatchesOperator` to list existing Dataproc Batch jobs within a specified Google Cloud project and region. Optional filtering can be applied.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n# Code extracted from: /../../google/tests/system/google/cloud/dataproc/example_dataproc_batch.py\n# Between markers: [START how_to_cloud_dataproc_list_batches_operator] and [END how_to_cloud_dataproc_list_batches_operator]\n# \n# Example using DataprocListBatchesOperator(...)\n# ... (actual Python code would be here)\n\n```\n\n----------------------------------------\n\nTITLE: Installing IMAP Extras for Apache Airflow\nDESCRIPTION: Command to install IMAP hooks and sensors for Apache Airflow. This enables email integration via IMAP protocol and is preinstalled with Airflow by default.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_61\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[imap]'\n```\n\n----------------------------------------\n\nTITLE: Navigating Directory Structure in Object Storage\nDESCRIPTION: Shows how to navigate the directory structure in object storage by using the path division operator to create subdirectory paths.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/objectstorage.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbase = ObjectStoragePath(\"s3://my-bucket/\")\nsubdir = base / \"subdir\"\n\n# prints ObjectStoragePath(\"s3://my-bucket/subdir\")\nprint(subdir)\n```\n\n----------------------------------------\n\nTITLE: Query Parameter Format - New Implementation\nDESCRIPTION: Shows the new format for passing list parameters in API requests using repeated parameters, following FastAPI's default explode behavior.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43102.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: http\nCODE:\n```\nhttp://<URL>:<PORT>/<PATH>?param=item1&param=item2\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Speed up Breeze autocompletion\nDESCRIPTION: This commit message (hash 6937ae7647, dated 2023-12-30) describes an optimization to speed up the autocompletion feature in the Breeze development environment by simplifying the provider state representation (issue #36499).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n``Speed up autocompletion of Breeze by simplifying provider state (#36499)``\n```\n\n----------------------------------------\n\nTITLE: Checking Airflow Client Package Files with Twine (Shell Script)\nDESCRIPTION: Uses the `twine check` command to validate the distribution files (`.tar.gz` source distribution and `.whl` wheel file) for the Python client package. This step ensures the package metadata is correct and the files are well-formed before uploading them to the Python Package Index (PyPI). It should be run in the directory containing the built artifacts.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_26\n\nLANGUAGE: shell script\nCODE:\n```\ncd ${VERSION}\ntwine check *.tar.gz *.whl\n```\n\n----------------------------------------\n\nTITLE: Specifying Migration Rule for Ruff Linter in Airflow\nDESCRIPTION: This code snippet defines a migration rule for the Ruff linter, specifically for the AIR302 rule. It indicates that 'airflow.api_connexion.security.requires_access' should be replaced with 'airflow.api_connexion.security.requires_access_*'.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41910.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* Migration rules needed\n\n  * ruff\n\n    * AIR302\n\n      * [x] ``airflow.api_connexion.security.requires_access``  ``airflow.api_connexion.security.requires_access_*``\n```\n\n----------------------------------------\n\nTITLE: Datadog Package Requirements\nDESCRIPTION: Required package dependencies and their minimum versions for using the Datadog provider with Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/datadog/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\napache-airflow  >=2.9.0\ndatadog         >=0.14.0\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare docs for Oct 2nd wave of providers\nDESCRIPTION: This text is a commit message summary indicating work done to prepare documentation for the October 2nd wave of Apache Airflow provider releases, linked to pull request #43409.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs for Oct 2nd wave of providers (#43409)\n```\n\n----------------------------------------\n\nTITLE: Listing Changelog Entries in reStructuredText\nDESCRIPTION: Lists changelog entries for different versions of the Jenkins provider package, including commit hashes, dates, and descriptions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n4.0.4\n.....\n\nLatest change: 2025-04-14\n\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`cb295c351a <https://github.com/apache/airflow/commit/cb295c351a016c0a10cab07f2a628b865cff3ca3>`__  2025-04-14   ``remove superfluous else block (#49199)``\n`4a8567b20b <https://github.com/apache/airflow/commit/4a8567b20bdd6555cbdc936d6674bf4fa390b0d5>`__  2025-04-10   ``Prepare docs for Apr 2nd wave of providers (#49051)``\n`7b2ec33c7a <https://github.com/apache/airflow/commit/7b2ec33c7ad4998d9c9735b79593fcdcd3b9dd1f>`__  2025-04-08   ``Remove unnecessary entries in get_provider_info and update the schema (#48849)``\n`139673d3ce <https://github.com/apache/airflow/commit/139673d3ce5552c2cf8bcb2d202e97342c4b237c>`__  2025-04-07   ``Remove fab from preinstalled providers (#48457)``\n`67858fd7e7 <https://github.com/apache/airflow/commit/67858fd7e7ac82788854844c1e6ef5a35f1d0d23>`__  2025-04-06   ``Improve documentation building iteration (#48760)``\n`adbb062b50 <https://github.com/apache/airflow/commit/adbb062b50e2e128fe475a76b7ce10ec93c39ee2>`__  2025-04-06   ``Prepare docs for Apr 1st wave of providers (#48828)``\n`d4473555c0 <https://github.com/apache/airflow/commit/d4473555c0e7022e073489b7163d49102881a1a6>`__  2025-04-02   ``Simplify tooling by switching completely to uv (#48223)``\n`c762e17820 <https://github.com/apache/airflow/commit/c762e17820cae6b162caa3eec5123760e07d56cc>`__  2025-03-26   ``Prepare docs for Mar 2nd wave of providers (#48383)``\n`6adb2dbae4 <https://github.com/apache/airflow/commit/6adb2dbae47341eb61dbc62dbc56176d9aa83fd9>`__  2025-03-25   ``Upgrade providers flit build requirements to 3.12.0 (#48362)``\n`243fe86d4b <https://github.com/apache/airflow/commit/243fe86d4b3e59bb12977b3e36ca3f2ed27ca0f8>`__  2025-03-21   ``Move airflow sources to airflow-core package (#47798)``\n`935d2831fe <https://github.com/apache/airflow/commit/935d2831fe8fd509b618a738bf00e0c34e186e11>`__  2025-03-15   ``Remove links to x/twitter.com (#47801)``\n==================================================================================================  ===========  ==================================================================================\n```\n\n----------------------------------------\n\nTITLE: Airflow Configuration Migration Commands\nDESCRIPTION: Lists configuration mappings showing the migration path from deprecated core section to new database section settings. Used with 'airflow config lint' command to validate configuration changes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/42126.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\ncore.sql_alchemy_conn  database.sql_alchemy_conn\ncore.sql_engine_encoding  database.sql_engine_encoding\ncore.sql_engine_collation_for_ids  database.sql_engine_collation_for_ids\ncore.sql_alchemy_pool_enabled  database.sql_alchemy_pool_enabled\ncore.sql_alchemy_pool_size  database.sql_alchemy_pool_size\ncore.sql_alchemy_max_overflow  database.sql_alchemy_max_overflow\ncore.sql_alchemy_pool_recycle  database.sql_alchemy_pool_recycle\ncore.sql_alchemy_pool_pre_ping  database.sql_alchemy_pool_pre_ping\ncore.sql_alchemy_schema  database.sql_alchemy_schema\ncore.sql_alchemy_connect_args  database.sql_alchemy_connect_args\ncore.load_default_connections  database.load_default_connections\ncore.max_db_retries  database.max_db_retries\n```\n\n----------------------------------------\n\nTITLE: Customizing Jinja Environment in DAG Definition\nDESCRIPTION: Demonstrates how to pass custom options to the Jinja Environment when creating a DAG, such as keeping trailing newlines.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/operators.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmy_dag = DAG(\n    dag_id=\"my-dag\",\n    jinja_environment_kwargs={\n        \"keep_trailing_newline\": True,\n        # some other jinja2 Environment options here\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Azure Blob Storage Sensor in Airflow\nDESCRIPTION: This class extends BaseSensorOperator to create a sensor that checks for the existence of a blob in Azure Blob Storage. It uses WasbHook for authentication and connectivity, polls at a defined interval, and returns True when the blob is found.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/.latest-doc-only-change.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass WasbBlobSensor(BaseSensorOperator):\n    \"\"\"Waits for a blob to arrive on Azure Blob Storage.\n\n    :param container_name: name of the container in which the blob should appear.\n    :type container_name: str\n    :param blob_name: name of the blob to check existence for\n    :type blob_name: str\n    :param wasb_conn_id: the connection id for the wasb connection.\n    :type wasb_conn_id: str\n    :param check_options: optional keyword arguments that\n        `WasbHook.check_for_blob()` takes.\n    :type check_options: dict\n    \"\"\"\n\n    @apply_defaults\n    def __init__(\n        self,\n        container_name,\n        blob_name,\n        wasb_conn_id='wasb_default',\n        check_options=None,\n        *args,\n        **kwargs):\n        super(WasbBlobSensor, self).__init__(*args, **kwargs)\n        if check_options is None:\n            check_options = {}\n        self.wasb_conn_id = wasb_conn_id\n        self.container_name = container_name\n        self.blob_name = blob_name\n        self.check_options = check_options\n\n    def poke(self, context):\n        self.log.info('Poking for blob : %s\\n in wasb://%s',\n                      self.blob_name, self.container_name)\n        hook = WasbHook(wasb_conn_id=self.wasb_conn_id)\n        return hook.check_for_blob(self.container_name, self.blob_name, **self.check_options)\n```\n\n----------------------------------------\n\nTITLE: Generating Package Signatures\nDESCRIPTION: Commands to generate signatures and checksum files for the release packages.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncd ${AIRFLOW_REPO_ROOT}\npushd dist\n../dev/sign.sh *\npopd\n```\n\n----------------------------------------\n\nTITLE: Constraints File URL Template\nDESCRIPTION: Template for constructing the URL to access version-specific constraints files.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-pypi.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhttps://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\n```\n\n----------------------------------------\n\nTITLE: Installing PGVector Provider Package for Apache Airflow\nDESCRIPTION: This command installs the PGVector provider package on top of an existing Apache Airflow 2 installation. It requires Airflow version 2.9.0 or higher.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-pgvector\n```\n\n----------------------------------------\n\nTITLE: Installing GitHub Provider via pip\nDESCRIPTION: Command to install the GitHub provider package on an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/github/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-github\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Compose via Repository\nDESCRIPTION: Commands to install Docker Compose plugin through package repository on Ubuntu.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get install docker-compose-plugin\n```\n\n----------------------------------------\n\nTITLE: Providing Federation Credentials in Connection Extra for Custom Factory (JSON)\nDESCRIPTION: This JSON snippet for the 'Extra' field is used in conjunction with a custom AWS session factory. It provides custom credentials (e.g., 'username' and 'password') needed for a specific federation process implemented within the custom factory. The factory code would then access this 'federation' dictionary from the connection's extra config.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"federation\": {\n    \"username\": \"my_username\",\n    \"password\": \"my_password\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Package Requirements Table in RST\nDESCRIPTION: RST table showing the required PIP package (apache-airflow) and its minimum version requirement for the Iceberg provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/iceberg/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n\"apache-airflow\"  \">=2.9.0\"\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Deprecated Database Upgrade Command for Apache Airflow\nDESCRIPTION: This command was used to apply migrations prior to Airflow version 2.7.0. It has been deprecated in favor of 'airflow db migrate'.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/setting-up-the-database.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairflow db upgrade\n```\n\n----------------------------------------\n\nTITLE: MySQL Connection Extras with String-encoded SSL\nDESCRIPTION: Alternative JSON configuration example with SSL parameters encoded as a string\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/connections/mysql.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"charset\": \"utf8\",\n   \"cursor\": \"sscursor\",\n   \"unix_socket\": \"/var/socket\",\n   \"ssl\": \"{\\\"cert\\\": \\\"/tmp/client-cert.pem\\\", \\\"ca\\\": \\\"/tmp/server-ca.pem\\\", \\\"key\\\": \\\"/tmp/client-key.pem\\\"}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Dependency Lock File\nDESCRIPTION: Command to update and lock the dependencies using UV package manager.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nuv lock\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Image from GitHub v2-2-test Branch\nDESCRIPTION: Builds a production Airflow image from the latest v2-2-test branch on GitHub. Constraints are taken from the corresponding constraints-2-2 branch.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . -f Dockerfile --pull --build-arg AIRFLOW_INSTALLATION_METHOD=\"github\" --build-arg AIRFLOW_CONSTRAINTS_REFERENCE=\"constraints-2-2\" --build-arg AIRFLOW_SOURCES_FROM=\"v2-2-test\" --build-arg AIRFLOW_SOURCES_TO=\"v2-2-test\" --tag \"my-company/airflow:2.2.0-python3.9\"\n```\n\n----------------------------------------\n\nTITLE: Installing Hashicorp Provider with Google Extra using Pip (Bash)\nDESCRIPTION: This shell command demonstrates how to install the `apache-airflow-providers-hashicorp` package along with optional dependencies required for Google Cloud integration. It utilizes pip's 'extra' feature by specifying `[google]` after the package name. This ensures that dependencies needed for interacting with Google services via the Hashicorp provider are also installed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/hashicorp/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-hashicorp[google]\n```\n\n----------------------------------------\n\nTITLE: Installing MySQL Provider Package via pip\nDESCRIPTION: Command to install the MySQL provider package on top of an existing Airflow 2 installation\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-mysql\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with google-auth Extra\nDESCRIPTION: This command installs Apache Airflow with the google-auth extra, which enables the Google auth backend.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[google-auth]'\n```\n\n----------------------------------------\n\nTITLE: Registering Airflow Listener Plugin\nDESCRIPTION: Basic implementation of an Airflow plugin class that registers a listener for monitoring task states. The plugin class inherits from AirflowPlugin and specifies the listener implementation to be used.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/listener-plugin.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.plugins_manager import AirflowPlugin\n\n# This is the listener file created where custom code to monitor is added over hookimpl\nimport listener\n\n\nclass MetadataCollectionPlugin(AirflowPlugin):\n    name = \"MetadataCollectionPlugin\"\n    listeners = [listener]\n```\n\n----------------------------------------\n\nTITLE: Installing Discord Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Discord provider package on top of an existing Airflow 2 installation using pip. This package supports Python versions 3.9, 3.10, 3.11, and 3.12.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/discord/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-discord\n```\n\n----------------------------------------\n\nTITLE: Using Stats Timer for Performance Measurement in Python\nDESCRIPTION: Demonstrates how to use Stats.timer() to measure code execution time with metric tracking. The timer can be used either with a metric name for automatic timing and submission or without for manual timing retrieval.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.stats import Stats\n\n...\n\nwith Stats.timer(\"my_timer_metric\"):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Checking Current Secrets Backend Configuration\nDESCRIPTION: Bash command to check which secrets backend is currently configured in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/secrets-backend/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ airflow config get-value secrets backend\nairflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend\n```\n\n----------------------------------------\n\nTITLE: Setting Global AWS Retry Strategy via Environment Variables (Bash)\nDESCRIPTION: These bash commands set the AWS retry mode and maximum attempts globally using environment variables (`AWS_RETRY_MODE` and `AWS_MAX_ATTEMPTS`). This configuration applies to all AWS connections in the Airflow environment unless overridden by a specific connection's 'Extra' field configuration or an AWS profile setting.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nexport AWS_RETRY_MODE=standard\nexport AWS_MAX_ATTEMPTS=10\n```\n\n----------------------------------------\n\nTITLE: Kafka Provider Package Name\nDESCRIPTION: Python package name for the Apache Kafka provider in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"apache-airflow-providers-apache-kafka\"\n```\n\n----------------------------------------\n\nTITLE: Modifying PostgreSQL User Search Path\nDESCRIPTION: This SQL command alters the search path for the Airflow PostgreSQL user to include the public schema.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-up-database.rst#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nALTER USER airflow_user SET search_path = public;\n```\n\n----------------------------------------\n\nTITLE: Building Release Notes with Towncrier\nDESCRIPTION: Commands to preview and build release notes using towncrier, followed by git log command to gather additional entries.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ntowncrier build --draft --version=${VERSION} --date=2021-12-15 --dir chart --config chart/newsfragments/config.toml\n\ngit log --oneline helm-chart/1.1.0..main --pretty='format:- %s' -- chart/ docs/helm-chart/\n```\n\n----------------------------------------\n\nTITLE: Creating Dataform Compilation Result in Python\nDESCRIPTION: This snippet demonstrates how to create a Compilation Result using the DataformCreateCompilationResultOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_create_compilation_result]\n# [END howto_operator_create_compilation_result]\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Pre-commit Git Hook\nDESCRIPTION: Commands to disable pre-commit checks\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/Projects/airflow\npre-commit uninstall\n```\n\n----------------------------------------\n\nTITLE: Removing Password for LDAP or CUSTOM Mode in HiveServer2Hook\nDESCRIPTION: This code removes the password if in LDAP or CUSTOM mode for the HiveServer2Hook, enhancing security.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n\"Remove password if in LDAP or CUSTOM mode HiveServer2Hook (#11767)\"\n```\n\n----------------------------------------\n\nTITLE: Git Workflow Commands\nDESCRIPTION: Common Git commands demonstrating the workflow for updating and using Breeze across different branches.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0003-bootstrapping-virtual-environment.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout branch\\n./Breeze\\ngit rebase --onto apache/main\n```\n\n----------------------------------------\n\nTITLE: Setting Up Airflow Environment\nDESCRIPTION: Commands to create required directories and set up environment variables for Airflow\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p ./dags ./logs ./plugins ./config\necho -e \"AIRFLOW_UID=$(id -u)\" > .env\n```\n\n----------------------------------------\n\nTITLE: Converting SSH Private Key to Base64\nDESCRIPTION: Command to convert SSH private key to base64 format for use in Kubernetes secrets.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nbase64 <my-private-ssh-key> -w 0 > temp.txt\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Docker Image in Air-Gapped Environment\nDESCRIPTION: This bash command demonstrates how to build an Airflow Docker image in an air-gapped environment using pre-downloaded dependencies and custom build arguments.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\n.. exampleinclude:: docker-examples/restricted/restricted_environments.sh\n    :language: bash\n    :start-after: [START build]\n    :end-before: [END build]\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Client\nDESCRIPTION: Commands for installing the Apache Airflow client using pip or directly from GitHub.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-client\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/apache/airflow-client-python.git\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Beam Provider with Google Integration using PIP\nDESCRIPTION: This shell command installs the Apache Beam provider with Google integration using PIP. It's noted that this may lead to some BigQuery operators functionality being unavailable.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/changelog.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-apache-beam[google]\n```\n\n----------------------------------------\n\nTITLE: Validating Host and Schema for Spark JDBC Hook\nDESCRIPTION: A bug fix in version 4.0.1 that adds validation for host and schema parameters in the Spark JDBC Hook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"Validate host and schema for Spark JDBC Hook (#30223)\"\n```\n\n----------------------------------------\n\nTITLE: Listing All Custom Columns with GoogleSearchAdsListCustomColumnsOperator\nDESCRIPTION: This snippet demonstrates how to retrieve a list of all custom columns using the GoogleSearchAdsListCustomColumnsOperator. The operator supports Jinja templating for dynamic parameter determination.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/search_ads.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[START howto_search_ads_list_custom_columns]\n[END howto_search_ads_list_custom_columns]\n```\n\n----------------------------------------\n\nTITLE: Preparing Airflow Providers Release 0.0.2a1\nDESCRIPTION: This commit message signifies preparation steps taken for the alpha release version 0.0.2a1 of Airflow providers. It references pull request #11855.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_66\n\nLANGUAGE: text\nCODE:\n```\nPrepare providers release 0.0.2a1 (#11855)\n```\n\n----------------------------------------\n\nTITLE: Including Absolute Path Documentation\nDESCRIPTION: Example of including a file using absolute path reference in Sphinx documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/11_documentation_building.rst#2025-04-22_snippet_8\n\nLANGUAGE: rst\nCODE:\n```\n``.. include:: /../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst``\n```\n\n----------------------------------------\n\nTITLE: Building Slim Production Images\nDESCRIPTION: Command to build and release slim production Airflow images with version specification\nSOURCE: https://github.com/apache/airflow/blob/main/dev/MANUALLY_BUILDING_IMAGES.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nbreeze release-management release-prod-images --airflow-version \"${VERSION}\" --slim-images\n```\n\n----------------------------------------\n\nTITLE: Running Pytest with SQL Tracing in Airflow\nDESCRIPTION: Shows how to execute Airflow tests using pytest while tracing SQL statements. This command enables tracking of query numbers, SQL statements, and parameters for a specific test case.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_52\n\nLANGUAGE: bash\nCODE:\n```\npytest --trace-sql=num,sql,parameters --capture=no \\\n  tests/jobs/test_scheduler_job.py -k test_process_dags_queries_count_05\n```\n\n----------------------------------------\n\nTITLE: Creating Task with Asset Events\nDESCRIPTION: Shows how to create a DAG and task that emits events for an asset using PythonOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import DAG, Asset\nfrom airflow.providers.standard.operators.python import PythonOperator\n\nexample_asset = Asset(name=\"example_asset\", uri=\"s3://asset-bucket/example.csv\")\n\n\ndef _write_example_asset():\n    \"\"\"Write data to example_asset...\"\"\"\n\n\nwith DAG(dag_id=\"example_asset\", schedule=\"@daily\"):\n    PythonOperator(task_id=\"example_asset\", outlets=[example_asset], python_callable=_write_example_asset)\n```\n\n----------------------------------------\n\nTITLE: Building Production Image with Python Dependencies\nDESCRIPTION: Command to build a production image with additional Python dependencies using Breeze CLI\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/06_managing_docker_images.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbreeze prod-image build --additional-python-deps \"torchio==0.17.10\"\n```\n\n----------------------------------------\n\nTITLE: Template Fields for CloudVisionDeleteProductOperator\nDESCRIPTION: Lists the template fields available for the CloudVisionDeleteProductOperator for dynamic field resolution at runtime.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"location\",\n    \"project_id\",\n    \"product_id\",\n    \"gcp_conn_id\",\n    \"impersonation_chain\",\n)\n```\n\n----------------------------------------\n\nTITLE: Yielding TriggerEvent in Python for Deferrable Operator Completion\nDESCRIPTION: Shows how to yield a TriggerEvent to signal the completion of a deferrable operator's execution and pass information back to the operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/triggers/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nyield TriggerEvent({\"status\": \"success\", \"message\": \"Cluster Created\"})\n```\n\n----------------------------------------\n\nTITLE: Updating DayOfWeekSensor in Apache Airflow (Python)\nDESCRIPTION: This code snippet demonstrates how to update the DayOfWeekSensor class to use the use_task_logical_date parameter instead of the deprecated use_task_execution_day. It shows the correct way to initialize a DayOfWeekSensor object with the new parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41393.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsensor = DayOfWeekSensor(\n    task_id=\"example\",\n    week_day=\"Tuesday\",\n    use_task_logical_date=True,\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Building Production Image with Airflow Extras\nDESCRIPTION: Command to build a production image with additional Airflow extras using Breeze CLI\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/06_managing_docker_images.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze prod-image build --additional-airflow-extras \"jira\"\n```\n\n----------------------------------------\n\nTITLE: Configuring AVP Policy Store ID in Airflow Config (INI)\nDESCRIPTION: Sets the Amazon Verified Permissions policy store ID within the Airflow configuration file (`airflow.cfg`). This tells the AWS auth manager which AVP policy store to use for authorization checks. Replace `<avp_policy_store_id>` with the actual ID of the created policy store.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/setup/amazon-verified-permissions.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[aws_auth_manager]\navp_policy_store_id = <avp_policy_store_id>\n```\n\n----------------------------------------\n\nTITLE: Specifying Docs Build Runner Labels (JSON Array)\nDESCRIPTION: Defines the list of labels assigned to select runners specifically for documentation builds. This example specifies 'ubuntu-22.04'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n\\[\"ubuntu-22.04\"\\]\n```\n\n----------------------------------------\n\nTITLE: Setting Teradata Connection URI (Bash) in Airflow Environment\nDESCRIPTION: This Bash snippet shows how to configure a Teradata connection using an environment variable (`AIRFLOW_CONN_TERADATA_DEFAULT`). It follows the standard database connection URI format, embedding the username, password, host, and database. Extra parameters like 'tmode', 'sslmode', and 'sslca' are included as URL-encoded query parameters in the URI. Note the URL encoding for the file path in 'sslca' (%2F represents '/').\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/connections/teradata.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_TERADATA_DEFAULT='teradata://teradata_user:XXXXXXXXXXXX@1.1.1.1:/teradatadb?tmode=tera&sslmode=verify-ca&sslca=%2Ftmp%2Fserver-ca.pem'\n```\n\n----------------------------------------\n\nTITLE: Using Boto3 Client Linked to Resource Meta for Waiters\nDESCRIPTION: Uses the boto3 client linked to the resource meta instead of creating a new one for waiters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nwaiter = resource.meta.client.get_waiter('waiter_name')\n```\n\n----------------------------------------\n\nTITLE: Generating Python Client with Breeze\nDESCRIPTION: Command to generate Airflow Python client code using the dockerized breeze environment. Uses OpenAPI generator and Hatch for package generation.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-python-client --distribution-format both\n```\n\n----------------------------------------\n\nTITLE: Cloning Airflow Python Client Repository\nDESCRIPTION: Command to clone the Apache Airflow Python client repository from GitHub.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/python_client_tests.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/apache/airflow-client-python.git\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow with SSH Extra\nDESCRIPTION: Command to install Airflow with SSH provider support using source provider constraints.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/13_airflow_dependencies_and_extras.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install \".[ssh]\" \\\n  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-main/constraints-source-providers-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Updating TimeDeltaSensor Import Path in Python\nDESCRIPTION: Example showing how to update the import statement for TimeDeltaSensor from the deprecated path to the new provider-specific path.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41368.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old import (deprecated)\nfrom airflow.sensors import TimeDeltaSensor\n\n# New import\nfrom airflow.sensors.time_delta import TimeDeltaSensor\n```\n\n----------------------------------------\n\nTITLE: Specifying Mypy Check Folders (Python List)\nDESCRIPTION: Defines the list of folders to be included in mypy static type checking. This example specifies 'airflow_aux' and 'airflow_core'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\\[\"airflow_aux\", \"airflow_core\"\\]\n```\n\n----------------------------------------\n\nTITLE: Measuring DAG Loading Time with Linux Time Command\nDESCRIPTION: Shows how to measure DAG parsing time using the Linux 'time' command to evaluate performance and optimization efforts. The example demonstrates timing the execution of a sample DAG file and comparing it with Python interpreter startup time.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ntime python airflow/example_dags/example_python_operator.py\n```\n\n----------------------------------------\n\nTITLE: Enabling Webserver Config Exposure with Environment Variable - bash\nDESCRIPTION: This snippet sets the AIRFLOW__WEBSERVER__EXPOSE_CONFIG environment variable to expose the Airflow webserver's configuration through the UI/API, matching the airflow.cfg setting. It is intended for testing-only environments as exposing sensitive configuration data is a security risk.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True\n```\n\n----------------------------------------\n\nTITLE: Installing Dingding Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Dingding integration, enabling Dingding hooks and sensors.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[dingding]'\n```\n\n----------------------------------------\n\nTITLE: Building Production Image with APT Dependencies\nDESCRIPTION: Command to build a production image with additional APT dependencies for both build and runtime environments\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/06_managing_docker_images.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbreeze prod-image build --additional-dev-apt-deps \"libasound2-dev\" --additional-runtime-apt-deps \"libasound2\"\n```\n\n----------------------------------------\n\nTITLE: Installing Elasticsearch Provider with Common SQL Dependency\nDESCRIPTION: Command to install the Elasticsearch provider package along with its common SQL dependency using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-elasticsearch[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow with Breeze\nDESCRIPTION: Commands to start Airflow using Breeze with different configurations and executors.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\nbreeze start-airflow --use-airflow-version 2.7.0rc1 --python 3.9 --backend postgres\n```\n\nLANGUAGE: shell\nCODE:\n```\nbreeze start-airflow --use-airflow-version 2.7.0rc1 --python 3.9 --backend postgres \\\n  --executor CeleryExecutor --airflow-extras \"celery,google,amazon\"\n```\n\n----------------------------------------\n\nTITLE: Defining Apache Impala Provider Package Name in reStructuredText\nDESCRIPTION: Specifies the name of the Apache Impala provider package for Apache Airflow using reStructuredText syntax.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/impala/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n``apache-airflow-providers-apache-impala``\n```\n\n----------------------------------------\n\nTITLE: Updating Azure Synapse Hook in Python\nDESCRIPTION: Adds managed identity support to the Azure Synapse hook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfeat(provider/azure): add managed identity support to synapse hook (#35329)\n```\n\n----------------------------------------\n\nTITLE: Specifying MySQL Test Versions (Python List)\nDESCRIPTION: Defines the specific versions of MySQL to be used during testing. This example specifies version '8.0'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\\['8.0'\\]\n```\n\n----------------------------------------\n\nTITLE: Implementing Deprecation Warning in Python for Airflow Google Provider\nDESCRIPTION: Demonstrates how to mark a function as deprecated using a decorator that specifies the planned removal date and suggested alternative. Uses AirflowProviderDeprecationWarning to indicate deprecation in the Google provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/deprecation-policy.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.google.common.deprecated import deprecated\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\n\n\n@deprecated(\n    planned_removal_date=\"September 30, 2025\",\n    use_instead=\"airflow.providers.google.cloud.hooks.vertex_ai.auto_ml.AutoMLHook\",\n    category=AirflowProviderDeprecationWarning,\n)\ndef example() -> None: ...\n```\n\n----------------------------------------\n\nTITLE: Supporting Better Type-Hinting for Context in SDK (AIP-72)\nDESCRIPTION: Commit message describing improvements to support better type-hinting for the 'Context' dictionary within the Task SDK, as part of AIP-72. References pull request #45583.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_42\n\nLANGUAGE: plaintext\nCODE:\n```\nAIP-72: Support better type-hinting for Context dict in SDK  (#45583)\n```\n\n----------------------------------------\n\nTITLE: Adding Comment About Release Manager Version Updates (Commit Message)\nDESCRIPTION: Commit message indicating the addition of a comment explaining that certain version numbers are updated by the release manager during the Apache Airflow release process.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_20\n\nLANGUAGE: text\nCODE:\n```\nAdd comment about versions updated by release manager (#37488)\n```\n\n----------------------------------------\n\nTITLE: Exporting Connections to YAML File in Airflow CLI\nDESCRIPTION: This command exports Airflow connections to a YAML file in the /tmp directory using the Airflow CLI, specifying the file format explicitly.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nairflow connections export /tmp/connections --file-format yaml\n```\n\n----------------------------------------\n\nTITLE: Removing Old Helm Chart Version from ASF Dev SVN Repo in Shell\nDESCRIPTION: Navigates to the parent directory within the SVN checkout, removes the directory corresponding to a previous release candidate version (specified by `${PREVIOUS_VERSION_WITH_SUFFIX}`), and commits the removal to the SVN repository.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ncd ..\nexport PREVIOUS_VERSION_WITH_SUFFIX=1.0.0rc1\nsvn rm ${PREVIOUS_VERSION_WITH_SUFFIX}\nsvn commit -m \"Remove old Helm Chart release: ${PREVIOUS_VERSION_WITH_SUFFIX}\"\n```\n\n----------------------------------------\n\nTITLE: Update Mask Usage with Python Requests\nDESCRIPTION: Example showing how to use the update_mask parameter when updating resources using Python requests.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nresource = requests.get(\"/resource/my-id\").json()\nresource[\"my_field\"] = \"new-value\"\nrequests.patch(\"/resource/my-id?update_mask=my_field\", data=json.dumps(resource))\n```\n\n----------------------------------------\n\nTITLE: Generating Issue Content for Release Testing\nDESCRIPTION: This command generates the content for a GitHub issue to track the testing status of the new Apache Airflow release.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nbreeze release-management generate-issue-content-core --previous-release <PREVIOUS_VERSION>\n    --current-release ${VERSION}\n```\n\n----------------------------------------\n\nTITLE: Updating Airflow Provider READMEs for 1.0.0beta1 Releases\nDESCRIPTION: This commit message signifies updates to the README files for Airflow providers in preparation for the upcoming 1.0.0beta1 releases. It references pull request #12206.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_61\n\nLANGUAGE: text\nCODE:\n```\nUpdate provider READMEs for up-coming 1.0.0beta1 releases (#12206)\n```\n\n----------------------------------------\n\nTITLE: Generating Subject Line for Release Vote Email in Shell\nDESCRIPTION: Uses a 'here document' (`cat <<EOF ... EOF`) to generate the subject line for the email initiating the vote on the Helm chart release candidate. Requires `${VERSION}` and `${VERSION_SUFFIX}` variables.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\n[VOTE] Release Apache Airflow Helm Chart ${VERSION} based on ${VERSION}${VERSION_SUFFIX}\nEOF\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Implement fetchone() support for ODBCHook and Databricks SQL Hook\nDESCRIPTION: This commit message (hash 36010f6d0e, dated 2023-12-11) details the implementation of the `fetchone()` method, a standard DB-API 2.0 function, for the ODBCHook and the Databricks SQL Hook (issue #36161).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_20\n\nLANGUAGE: text\nCODE:\n```\n``Fix: Implement support for 'fetchone()' in the ODBCHook and the Databricks SQL Hook (#36161)``\n```\n\n----------------------------------------\n\nTITLE: Altering Table Constraints in SQL for Airflow Database\nDESCRIPTION: SQL command to add a constraint to a table in the Airflow database. This is used to prepare the database for migration to a new Airflow version.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading.rst#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n# Here you have to copy the statements from SHOW CREATE TABLE output\nALTER TABLE <TABLE> ADD CONSTRAINT `<CONSTRAINT_NAME>` <CONSTRAINT>\n```\n\n----------------------------------------\n\nTITLE: Removing Deprecated Parameter in Airflow SkipMixin - Python\nDESCRIPTION: Removal of execution_date parameter support from the skip() method in airflow.models.skipmixin.SkipMixin. This change affects DAG functionality and represents a code interface update to remove deprecated functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41780.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nairflow.models.skipmixin.SkipMixin.skip()\n```\n\n----------------------------------------\n\nTITLE: Importing GPG Keys for Signature Verification in Shell\nDESCRIPTION: This snippet demonstrates how to import GPG keys for verifying release signatures. It shows both importing from a KEYS file and from a keyserver.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\ngpg --import KEYS\n\n# Or import individually from a keyserver\ngpg --keyserver keys.openpgp.org --receive-keys CDE15C6E4D3A8EC4ECF4BA4B6674E08AD7DE406F\n\n# Alternative keyserver\ngpg --keyserver keys.gnupg.net --receive-keys CDE15C6E4D3A8EC4ECF4BA4B6674E08AD7DE406F\n```\n\n----------------------------------------\n\nTITLE: Creating Database Secret with kubectl\nDESCRIPTION: Bash command to create Kubernetes secret for database connection credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic mydatabase --from-literal=connection=postgresql://user:pass@host:5432/db\n```\n\n----------------------------------------\n\nTITLE: Checking Airflow Logging Configuration with CLI\nDESCRIPTION: Example of using the 'airflow info' command to check the current task handler and logging configuration in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/logging-tasks.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ airflow info\n\nApache Airflow\nversion                | 2.9.0.dev0\nexecutor               | LocalExecutor\ntask_logging_handler   | airflow.utils.log.file_task_handler.FileTaskHandler\nsql_alchemy_conn       | postgresql+psycopg2://postgres:airflow@postgres/airflow\ndags_folder            | /files/dags\nplugins_folder         | /root/airflow/plugins\nbase_log_folder        | /root/airflow/logs\nremote_base_log_folder |\n\n[skipping the remaining outputs for brevity]\n```\n\n----------------------------------------\n\nTITLE: Deleting AWS DMS Replication Task using DmsDeleteTaskOperator in Python\nDESCRIPTION: This snippet shows how to use the DmsDeleteTaskOperator to delete an AWS DMS replication task. It requires the ARN of the task to be deleted. Note that the task must be in a 'stopped' state before deletion. Depends on the airflow.providers.amazon.aws.operators.dms module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dms.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Example using DmsDeleteTaskOperator\n# from airflow.providers.amazon.aws.operators.dms import DmsDeleteTaskOperator\n\n# delete_task = DmsDeleteTaskOperator(\n#     task_id='delete_dms_task',\n#     replication_task_arn='arn:aws:dms:us-east-1:123456789012:task:ABCDEF123GHIJKL',\n#     # ... other parameters\n# )\n```\n\n----------------------------------------\n\nTITLE: Installing OpenSearch Provider Package with pip\nDESCRIPTION: Command to install the OpenSearch provider package on top of an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-opensearch\n```\n\n----------------------------------------\n\nTITLE: Displaying Help for Breeze Kubernetes Shell - Bash\nDESCRIPTION: This bash command reveals all available CLI options for the `breeze k8s shell` command by forwarding the `--help` argument to the shell. This is helpful for users exploring interactive shell capabilities and configuration flags.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s shell -- --help\n```\n\n----------------------------------------\n\nTITLE: Generating Webserver Secret Key\nDESCRIPTION: Commands to generate and configure a secure webserver secret key.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 -c 'import secrets; print(secrets.token_hex(16))'\nkubectl create secret generic my-webserver-secret --from-literal=\"webserver-secret-key=$(python3 -c 'import secrets; print(secrets.token_hex(16))')\"\n```\n\n----------------------------------------\n\nTITLE: Installing Databricks Provider with Dependencies\nDESCRIPTION: Command to install the Databricks provider package with additional common.sql dependencies via pip\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-databricks[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Colima Docker Socket Configuration\nDESCRIPTION: Commands to configure Colima as Docker runtime engine and set up socket paths.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo ln -sf $HOME/.colima/default/docker.sock /var/run/docker.sock\n\ndocker context use default\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow Environment Variables for ECS Executor (Bash)\nDESCRIPTION: This Bash script exports the necessary environment variables to configure Apache Airflow to use the AWS ECS Executor. It specifies the executor type, database connection string, AWS region, ECS cluster details (name, container, task definition), launch type (FARGATE), platform version, networking settings (public IP assignment, security groups, subnets). These variables must be set in the environment where the Airflow scheduler and webserver are running, replacing placeholders like `<postgres-connection-string>`, `<executor-region>`, etc., with actual values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/executors/ecs-executor.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__CORE__EXECUTOR='airflow.providers.amazon.aws.executors.ecs.ecs_executor.AwsEcsExecutor'\n\nexport AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=<postgres-connection-string>\n\nexport AIRFLOW__AWS_ECS_EXECUTOR__REGION_NAME=<executor-region>\n\nexport AIRFLOW__AWS_ECS_EXECUTOR__CLUSTER=<ecs-cluster-name>\n\nexport AIRFLOW__AWS_ECS_EXECUTOR__CONTAINER_NAME=<ecs-container-name>\n\nexport AIRFLOW__AWS_ECS_EXECUTOR__TASK_DEFINITION=<task-definition-name>\n\nexport AIRFLOW__AWS_ECS_EXECUTOR__LAUNCH_TYPE='FARGATE'\n\nexport AIRFLOW__AWS_ECS_EXECUTOR__PLATFORM_VERSION='LATEST'\n\nexport AIRFLOW__AWS_ECS_EXECUTOR__ASSIGN_PUBLIC_IP='True'\n\nexport AIRFLOW__AWS_ECS_EXECUTOR__SECURITY_GROUPS=<security-group-id-for-rds>\n\nexport AIRFLOW__AWS_ECS_EXECUTOR__SUBNETS=<subnet-id-for-rds>\n```\n\n----------------------------------------\n\nTITLE: Building Airflow with Custom Pip Configuration\nDESCRIPTION: Shell command showing how to build Airflow Docker image with custom pip configuration using docker-context-files.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . \\\n    --build-arg DOCKER_CONTEXT_FILES=docker-context-files \\\n    --tag my-custom-pip-airflow:latest\n```\n\n----------------------------------------\n\nTITLE: Installing Presto Provider with Optional Dependencies - Bash\nDESCRIPTION: This Bash code snippet demonstrates how to install the apache-airflow-providers-presto package along with its optional extra dependencies (in this example, 'common.sql') for expanded feature support. It assumes you have Python and pip already set up in your environment. The 'extra' flags, like [common.sql], enable related cross-provider integrations such as with Google or Common SQL providers. The command must be run in a compatible Python environment and requires internet connectivity for fetching packages from PyPI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/presto/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-presto[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Describing AWS DMS Serverless Replication Status using DmsDescribeReplicationsOperator\nDESCRIPTION: This snippet shows how to use the DmsDescribeReplicationsOperator to get the current status and details of AWS DMS serverless replications. Filters can be used to select specific replications based on ARN or other criteria. Depends on the airflow.providers.amazon.aws.operators.dms module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dms.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Example using DmsDescribeReplicationsOperator\n# from airflow.providers.amazon.aws.operators.dms import DmsDescribeReplicationsOperator\n\n# describe_replication = DmsDescribeReplicationsOperator(\n#     task_id=\"describe_dms_serverless_replication\",\n#     describe_replications_kwargs={\n#         'Filters': [\n#             {'Name': 'replication-arn', 'Values': ['arn:aws:dms:us-east-1:123456789012:replication:UVWDEF456GHI']}\n#         ]\n#     }\n#     # ... other parameters\n# )\n```\n\n----------------------------------------\n\nTITLE: Installing Slack Provider with Common Compat Dependencies\nDESCRIPTION: Command to install the Slack provider package along with common compatibility dependencies from PyPI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-slack[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Installing Segment Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Segment integration, enabling Segment hooks and sensors.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[segment]'\n```\n\n----------------------------------------\n\nTITLE: Generating Vote Email Body for Airflow Client Release in Shell\nDESCRIPTION: This snippet generates the body of the vote email using a here-document in shell script. It includes details about the release candidate, download links, test procedures, and a placeholder for the changelog.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\nHey fellow Airflowers,\n\nI have cut the first release candidate for the Apache Airflow Python Client ${VERSION}.\nThis email is calling for a vote on the release,\nwhich will last for 72 hours. Consider this my (binding) +1.\n\nAirflow Client ${VERSION}${VERSION_SUFFIX} is available at:\nhttps://dist.apache.org/repos/dist/dev/airflow/clients/python/${VERSION}${VERSION_SUFFIX}/\n\nThe apache_airflow_client-${VERSION}.tar.gz is an sdist release that contains INSTALL instructions, and also\nis the official source release.\n\nThe apache_airflow_client-${VERSION}-py3-none-any.whl is a binary wheel release that pip can install.\n\nThose packages do not contain .rc* version as, when approved, they will be released as the final version.\n\nThe rc packages are also available at PyPI (with rc suffix) and you can install it with pip as usual:\nhttps://pypi.org/project/apache-airflow-client/${VERSION}${VERSION_SUFFIX}/\n\nPublic keys are available at:\nhttps://dist.apache.org/repos/dist/release/airflow/KEYS\n\nOnly votes from PMC members are binding, but all members of the community\nare encouraged to test the release and vote with \"(non-binding)\".\n\nThe test procedure for PMC members is described in:\nhttps://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#verify-the-release-candidate-by-pmc-members\n\nThe test procedure for contributors and members of the community who would like to test this RC is described in:\nhttps://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#verify-the-release-candidate-by-contributors\n\n*Changelog:*\n\n*Major changes:*\n...\n\n*Major fixes:*\n...\n\n*New API supported:*\n...\n\nCheers,\n<your name>\nEOF\n```\n\n----------------------------------------\n\nTITLE: Specifying Requirements for Apache Kylin Provider Package\nDESCRIPTION: Table showing the required PIP packages and their versions for the Apache Kylin provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kylin/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n``apache-airflow``  ``>=2.9.0``\n``kylinpy``         ``>2.7.0``\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Problematic Test Collection with Database Dependencies in Python\nDESCRIPTION: This code snippet demonstrates a problematic way of writing tests that depend on database objects. The TaskInstance creation at the module level can cause test collection failures.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nTI = TaskInstance(\n    task=BashOperator(task_id=\"test\", bash_command=\"true\", dag=DAG(dag_id=\"id\"), start_date=datetime.now()),\n    run_id=\"fake_run\",\n    state=State.RUNNING,\n)\n\n\nclass TestCallbackRequest:\n    @pytest.mark.parametrize(\n        \"input,request_class\",\n        [\n            (CallbackRequest(full_filepath=\"filepath\", msg=\"task_failure\"), CallbackRequest),\n            (\n                TaskCallbackRequest(\n                    full_filepath=\"filepath\",\n                    simple_task_instance=SimpleTaskInstance.from_ti(ti=TI),\n                    is_failure_callback=True,\n                ),\n                TaskCallbackRequest,\n            ),\n            (\n                DagCallbackRequest(\n                    full_filepath=\"filepath\",\n                    dag_id=\"fake_dag\",\n                    run_id=\"fake_run\",\n                    is_failure_callback=False,\n                ),\n                DagCallbackRequest,\n            ),\n            (\n                SlaCallbackRequest(\n                    full_filepath=\"filepath\",\n                    dag_id=\"fake_dag\",\n                ),\n                SlaCallbackRequest,\n            ),\n        ],\n    )\n    def test_from_json(self, input, request_class): ...\n```\n\n----------------------------------------\n\nTITLE: Setting Neo4j Connection URL in Environment Variable\nDESCRIPTION: Example of setting up a Neo4j connection string as an environment variable using URI syntax with URL-encoded components.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/neo4j/docs/connections/neo4j.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_NEO4J_DEFAULT='neo4j://username:password@https%3A%2F%2Fneo4jhost/neo4j-schema?encrypted=true&neo4j_scheme=true&certs_self_signed=true&certs_trusted_ca=false'\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow with Custom Values\nDESCRIPTION: Helm command to install Airflow using custom values from an override file.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airflow apache-airflow/airflow -f override-values.yaml\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Airflow Deployment to Kubernetes\nDESCRIPTION: Example output of the Airflow deployment process, showing Helm chart deployment, release information, access instructions for the Airflow UI, and default credentials. The output also includes warnings about logging and webserver secret keys.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nDeploying Airflow for cluster airflow-python-3.9-v1.24.2\nDeploying kind-airflow-python-3.9-v1.24.2 with Airflow Helm Chart.\nCopied chart sources to /private/var/folders/v3/gvj4_mw152q556w2rrh7m46w0000gn/T/chart_edu__kir/chart\nDeploying Airflow from /private/var/folders/v3/gvj4_mw152q556w2rrh7m46w0000gn/T/chart_edu__kir/chart\nNAME: airflow\nLAST DEPLOYED: Tue Aug 30 22:57:54 2022\nNAMESPACE: airflow\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThank you for installing Apache Airflow 2.3.4!\n\nYour release is named airflow.\nYou can now access your dashboard(s) by executing the following command(s) and visiting the corresponding port at localhost in your browser:\n\nAirflow Webserver:     kubectl port-forward svc/airflow-webserver 8080:8080 --namespace airflow\nDefault Webserver (Airflow UI) Login credentials:\n    username: admin\n    password: admin\nDefault Postgres connection credentials:\n    username: postgres\n    password: postgres\n    port: 5432\n\nYou can get Fernet Key value by running the following:\n\n    echo Fernet Key: $(kubectl get secret --namespace airflow airflow-fernet-key -o jsonpath=\"{.data.fernet-key}\" | base64 --decode)\n\nWARNING:\n    Kubernetes workers task logs may not persist unless you configure log persistence or remote logging!\n    Logging options can be found at: https://airflow.apache.org/docs/helm-chart/stable/manage-logs.html\n    (This warning can be ignored if logging is configured with environment variables or secrets backend)\n\n###########################################################\n#  WARNING: You should set a static webserver secret key  #\n###########################################################\n\nYou are using a dynamically generated webserver secret key, which can lead to\nunnecessary restarts of your Airflow components.\n\nInformation on how to set a static webserver secret key can be found here:\nhttps://airflow.apache.org/docs/helm-chart/stable/production-guide.html#webserver-secret-key\nDeployed kind-airflow-python-3.9-v1.24.2 with Airflow Helm Chart.\n\nAirflow for Python 3.9 and K8S version v1.24.2 has been successfully deployed.\n\nThe KinD cluster name: airflow-python-3.9-v1.24.2\nThe kubectl cluster name: kind-airflow-python-3.9-v1.24.2.\n\n\nKinD Cluster API server URL: http://localhost:48366\nConnecting to localhost:18150. Num try: 1\nEstablished connection to webserver at http://localhost:18150/health and it is healthy.\nAirflow Web server URL: http://localhost:18150 (admin/admin)\n\nNEXT STEP: You might now run tests or interact with Airflow via shell (kubectl, pytest etc.) or k9s commands:\n\n\nbreeze k8s tests\n\nbreeze k8s shell\n\nbreeze k8s k9s\n```\n\n----------------------------------------\n\nTITLE: Enabling Unit Tests (Boolean String)\nDESCRIPTION: A boolean flag indicating whether general unit tests should be run. 'true' enables these tests.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_23\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Alternative Airflow Container Initialization Command\nDESCRIPTION: Simplified Docker run command that enables database migration and user creation without specifying a password, which would require interactive input or environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it -p 8080:8080 \\\n  --env \"_AIRFLOW_DB_MIGRATE=true\" \\\n  --env \"_AIRFLOW_WWW_USER_CREATE=true\" \\\n\n```\n\n----------------------------------------\n\nTITLE: Setting Task Instance Heartbeat Environment Variables in Bash\nDESCRIPTION: Configuration of environment variables to control task instance heartbeat timeout behavior for testing purposes. Sets the heartbeat interval, timeout duration, and detection interval.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/tasks.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__SCHEDULER__TASK_INSTANCE_HEARTBEAT_SEC=600\nexport AIRFLOW__SCHEDULER__TASK_INSTANCE_HEARTBEAT_TIMEOUT=2\nexport AIRFLOW__SCHEDULER__TASK_INSTANCE_HEARTBEAT_TIMEOUT_DETECTION_INTERVAL=5\n```\n\n----------------------------------------\n\nTITLE: Running Provider Tests with Minimum Dependencies\nDESCRIPTION: Command to run tests for a specific Airflow provider with minimum dependencies using Breeze. Replace PROVIDER_ID with the actual provider identifier.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_38\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-tests --force-lowest-dependencies --test-type \"Providers[PROVIDER_ID]\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Celery Executor SSL (INI)\nDESCRIPTION: Secures communication for the CeleryExecutor by enabling SSL. Set `ssl_active` to `True` and provide paths to the SSL private key (`ssl_key`), certificate (`ssl_cert`), and Certificate Authority certificate (`ssl_cacert`) within the `[celery]` section of the Airflow configuration file. Requires properly generated client and server certificates and keys.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/security.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[celery]\nssl_active = True\nssl_key = <path to key>\nssl_cert = <path to cert>\nssl_cacert = <path to cacert>\n```\n\n----------------------------------------\n\nTITLE: Verifying Airflow Secrets Backend Configuration using Console\nDESCRIPTION: Uses the `airflow config get-value` command to check the currently configured value for the secrets backend, verifying that it has been successfully set to `LockboxSecretBackend`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ airflow config get-value secrets backend\nairflow.providers.yandex.secrets.lockbox.LockboxSecretBackend\n```\n\n----------------------------------------\n\nTITLE: Complete Airflow DAG for MSSQL Operations using SQLExecuteQueryOperator (Reference)\nDESCRIPTION: References a full example Airflow DAG definition in Python. This DAG combines several steps: creating a table, inserting data, fetching data, and potentially using parameters with `SQLExecuteQueryOperator` for comprehensive MSSQL interaction, including task dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/operators.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../tests/system/microsoft/mssql/example_mssql.py\n    :language: python\n    :start-after: [START mssql_operator_howto_guide]\n    :end-before: [END mssql_operator_howto_guide]\n```\n\n----------------------------------------\n\nTITLE: Enabling ReadWriteMany Log Persistence\nDESCRIPTION: Helm command to enable log persistence with a ReadWriteMany PVC for all Airflow components to share a common logging volume.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-logs.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airflow apache-airflow/airflow \\\n  --set logs.persistence.enabled=true\n  # you can also override the other persistence\n  # by setting the logs.persistence.* values\n  # Please refer to values.yaml for details\n```\n\n----------------------------------------\n\nTITLE: Installing Yandex Provider Package via pip - Bash\nDESCRIPTION: This code snippet provides a bash command to install the 'apache-airflow-providers-yandex' package with the 'common.compat' extra requirements using pip. It requires Python 3.9, 3.10, 3.11, or 3.12 and assumes an existing Airflow 2 installation that meets the listed minimum version requirements. The command enables features that depend on the 'common.compat' provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-yandex[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Distribution\nDESCRIPTION: Command to build Apache Airflow distribution from sources using Breeze\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/testing_packages.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-airflow-distributions\n```\n\n----------------------------------------\n\nTITLE: Initializing Virtual Environment\nDESCRIPTION: Command to initialize the Python virtual environment with required dependencies\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/18_contribution_workflow.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/tools/initialize_virtualenv.py\n```\n\n----------------------------------------\n\nTITLE: Exporting Airflow SMTP Connection URI with Non-SSL Option - Bash\nDESCRIPTION: Illustrates exporting the AIRFLOW_CONN_SMTP_NOSSL environment variable for a non-SSL SMTP connection in Bash, appending the 'disable_ssl=true' parameter in the URI for disabling SSL. Proper for cases where SMTP servers require or allow non-secure communication; use with caution. All connection values must be URL-encoded as required by Airflow's connection parsing.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/smtp/docs/connections/smtp.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SMTP_NOSSL='smtp://username:password@smtp.sendgrid.net:587?disable_ssl=true'\n```\n\n----------------------------------------\n\nTITLE: Preparing Airflow CTL Wheel Distribution\nDESCRIPTION: Command to prepare only wheel distribution format for Airflow CTL.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-airflow-ctl-distributions --distribution-format=wheel\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Image with PyPI Selected Version\nDESCRIPTION: Builds a production Airflow image using Python 3.9 and Airflow version 2.3.0 from PyPI with default extras. The 2.3.0 constraints are used automatically.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . -f Dockerfile --pull --build-arg PYTHON_BASE_IMAGE=\"python:3.9-slim-bullseye\" --build-arg AIRFLOW_VERSION=\"2.3.0\" --tag \"my-company/airflow:2.3.0-python3.9\"\n```\n\n----------------------------------------\n\nTITLE: Starting Specific Airflow Version from PyPI with Breeze\nDESCRIPTION: This command starts a specific version of Airflow (2.7.0) from PyPI using Python 3.9 and MySQL backend in the Breeze environment.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nbreeze start-airflow --python 3.9 --backend mysql --use-airflow-version 2.7.0\n```\n\n----------------------------------------\n\nTITLE: Installing the Apache Airflow Alibaba Provider via pip\nDESCRIPTION: This shell command utilizes pip, the Python package installer, to install the `apache-airflow-providers-alibaba` package. This enables Apache Airflow integration with various Alibaba Cloud services. Installation requires an existing Airflow 2 environment (version 2.9.0 or newer) and Python/pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-alibaba\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 4.9.2\nDESCRIPTION: ReStructuredText formatted changelog entry detailing commits and changes for ODBC provider version 4.9.2.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n4.9.2\n.....\n\nLatest change: 2025-04-14\n\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`cb295c351a <https://github.com/apache/airflow/commit/cb295c351a016c0a10cab07f2a628b865cff3ca3>`__  2025-04-14   ``remove superfluous else block (#49199)``\n`4a8567b20b <https://github.com/apache/airflow/commit/4a8567b20bdd6555cbdc936d6674bf4fa390b0d5>`__  2025-04-10   ``Prepare docs for Apr 2nd wave of providers (#49051)``\n```\n\n----------------------------------------\n\nTITLE: Stopping AWS DMS Serverless Replication using DmsStopReplicationOperator\nDESCRIPTION: This snippet demonstrates using the DmsStopReplicationOperator to halt a running AWS DMS serverless replication. It requires the ARN of the active replication to stop. Depends on the airflow.providers.amazon.aws.operators.dms module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/dms.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Example using DmsStopReplicationOperator\n# from airflow.providers.amazon.aws.operators.dms import DmsStopReplicationOperator\n\n# stop_replication = DmsStopReplicationOperator(\n#     task_id=\"stop_dms_serverless_replication\",\n#     replication_arn=\"arn:aws:dms:us-east-1:123456789012:replication:UVWDEF456GHI\",\n#     # ... other parameters\n# )\n```\n\n----------------------------------------\n\nTITLE: Defining Provider Package Name\nDESCRIPTION: RST directive defining the Apprise provider package name\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n``apache-airflow-providers-apprise``\n```\n\n----------------------------------------\n\nTITLE: Defining SFTP Connection Extras With Host Key - JSON\nDESCRIPTION: This snippet provides a JSON example for setting the \"extras\" field of an Apache Airflow SFTP connection, demonstrating how to explicitly require a particular host key for connecting and configure host key change permissions. Required dependencies include a correctly configured SFTP connection in Airflow, and the ability to edit the connection extras as JSON. Key parameters specified are: no_host_key_check (disable automatic known_hosts enrollment), allow_host_key_change (disallow connecting on host key change), and host_key (required base64 encoded host public key). Inputs are the JSON object, typically pasted into the UI or connection file; output is Airflow's use of these extras when establishing SFTP connections. Designed for secure, key-matching SFTP integration without host key auto-enrollment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/connections/sftp.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"no_host_key_check\": \"false\",\n   \"allow_host_key_change\": \"false\",\n   \"host_key\": \"AAAHD...YDWwq==\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Cloud Build Config from Repository\nDESCRIPTION: Example configuration for creating a Cloud Build from Google Cloud Source Repository.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_build.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCREATE_BUILD_FROM_REPO_BODY = {\n    \"source\": {\"repoSource\": {\"projectId\": \"{{ var.value.gcp_project_id }}\", \"branchName\": \"master\"}},\n    \"steps\": [{\"name\": \"ubuntu\", \"args\": [\"echo\", \"Hello Cloud Build!\"]}],\n    \"status\": \"QUEUED\",\n}\n```\n\n----------------------------------------\n\nTITLE: Product Update with XCOM\nDESCRIPTION: Updates a Product using product_id extracted from XCOM\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_vision_product_update]\\n[END howto_operator_vision_product_update]\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit with UV Tool\nDESCRIPTION: Reinstalls pre-commit hooks using UV tool to resolve Python version compatibility issues.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/04_troubleshooting.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv tool uninstall pre-commit\nuv tool install pre-commit --python 3.9 --force --with pre-commit-uv\npre-commit clean\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Cloning Airflow SVN Repository in Shell\nDESCRIPTION: This snippet shows how to clone the Airflow SVN repository for release verification. It's part of the PMC member verification process.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nsvn co https://dist.apache.org/repos/dist/dev/airflow/clients/python\n```\n\n----------------------------------------\n\nTITLE: Applying Pre-upgrade Ruff Changes to Providers\nDESCRIPTION: Applies code style and linting changes suggested by `ruff` version 0.0.292 in preparation for a tool upgrade across providers, detailed in issue #35053.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nPre-upgrade 'ruff==0.0.292' changes in providers (#35053)\n```\n\n----------------------------------------\n\nTITLE: Package Requirements Table in RST\nDESCRIPTION: A reStructuredText table showing required pip packages and their version constraints for the Kubernetes provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n======================  =====================\nPIP package             Version required\n======================  =====================\n\"aiofiles\"            \">=23.2.0\"\n\"apache-airflow\"      \">=2.9.0\"\n\"asgiref\"             \">=3.5.2\"\n\"cryptography\"        \">=41.0.0\"\n\"kubernetes\"          \">=29.0.0,<=31.0.0\"\n\"kubernetes_asyncio\"  \">=29.0.0,<=31.0.0\"\n======================  =====================\n```\n\n----------------------------------------\n\nTITLE: Running Kubernetes Tests with Executor Configuration\nDESCRIPTION: Commands for running Kubernetes tests with optional executor specification. Shows how to run all tests or select specific tests to execute.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s tests\nbreeze k8s tests TEST TEST [TEST ...]\n\nbreeze k8s tests --executor CeleryExecutor\nbreeze k8s tests --executor CeleryExecutor TEST TEST [TEST ...]\n```\n\n----------------------------------------\n\nTITLE: Auth Decorator Migration in Python\nDESCRIPTION: Migration path for legacy @has_access decorator to new has_access_* decorators in airflow.www.auth\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41758.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@has_access  # Old way\n# Should be replaced with specific has_access_* decorator from airflow/www/auth.py\n```\n\n----------------------------------------\n\nTITLE: Custom Entrypoint Bash Script for Apache Airflow\nDESCRIPTION: Example bash script that modifies environment variables before executing the main Airflow entrypoint, demonstrating how to pass parameters and customize container initialization.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nexport CONNECTION_CHECK_MAX_COUNT=${1}\nshift\nexec /entrypoint \"${@}\"\n```\n\n----------------------------------------\n\nTITLE: Including Child Documents with Sphinx toctree (reStructuredText)\nDESCRIPTION: Uses the Sphinx \\\"toctree\\\" directive in reStructuredText format to automatically include all documentation sections or pages matching the pattern \\\"*\\\" at a single hierarchy level (maxdepth: 1). There are no programming dependencies outside Sphinx/reStructuredText itself. Inputs are the toctree configuration options and document path pattern. Output is an automatically collected group of references in the generated documentation. Only applicable within Sphinx documentation builds.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/connections/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\\n    :maxdepth: 1\\n    :glob:\\n\\n    *\n```\n\n----------------------------------------\n\nTITLE: Setting MSSQL Connection via JSON Environment Variable in Bash\nDESCRIPTION: Illustrates configuring an Apache Airflow MSSQL connection by serializing connection parameters into a JSON object and assigning it to an environment variable (`AIRFLOW_CONN_{CONN_ID}`). The example configures the `mssql_default` connection, specifying type, login, password, host, port, and schema.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/connections/mssql.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_MSSQL_DEFAULT='{\n    \"conn_type\": \"mssql\",\n    \"login\": \"username\",\n    \"password\": \"password\",\n    \"host\": \"server.com\",\n    \"port\": 1433,\n    \"schema\": \"database_name\"\n}'\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 2.0.0\nDESCRIPTION: Changelog entry documenting breaking changes in version 2.0.0 related to apply_default decorator removal\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sendgrid/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n2.0.0\n.....\n\nBreaking changes\n~~~~~~~~~~~~~~~~\n\n* ``Auto-apply apply_default decorator (#15667)``\n\n.. warning:: Due to apply_default decorator removal, this version of the provider requires Airflow 2.1.0+.\n   If your Airflow version is < 2.1.0, and you want to install this provider version, first upgrade\n   Airflow to at least version 2.1.0. Otherwise your Airflow package version will be upgraded\n   automatically and you will have to manually run ``airflow upgrade db`` to complete the migration.\n```\n\n----------------------------------------\n\nTITLE: Verifying Provider Distributions with Breeze (Bash)\nDESCRIPTION: Runs the `breeze release-management verify-provider-distributions` command to check provider packages in the `dist` folder. It verifies if provider classes are importable and adhere to established naming conventions, which is automatically done on CI.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management verify-provider-distributions\n```\n\n----------------------------------------\n\nTITLE: Updating Airflow SVN Repository in Shell\nDESCRIPTION: This snippet demonstrates how to update an existing Airflow SVN repository checkout. It's used in the PMC member verification process.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nsvn update .\n```\n\n----------------------------------------\n\nTITLE: Configuring a SparkSQL Job for Google Cloud Dataproc\nDESCRIPTION: This code defines a configuration for a SparkSQL job to be submitted to a Dataproc cluster. It specifies query file and properties for the Spark SQL execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nSPARK_SQL_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"spark_sql_job\": {\n        \"query_file_uri\": f\"gs://{BUCKET_NAME}/{SPARK_SQL_SCRIPT}\",\n        \"script_variables\": {\n            \"input\": f\"gs://{BUCKET_NAME}/{SPARK_SQL_INPUT}\",\n            \"output\": f\"gs://{BUCKET_NAME}/{SPARK_SQL_OUTPUT_DIR}\",\n        },\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Additional Environment Variables for Apache Airflow Testing\nDESCRIPTION: This YAML snippet defines additional environment variables for testing and development purposes when using Docker Compose with Apache Airflow. These variables allow for quick setup of admin accounts and installation of additional Python packages.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n+----------------------------------+-----------------------------------------------------+--------------------------+\n|   Variable                       | Description                                         | Default                  |\n+==================================+=====================================================+==========================+\n| ``_AIRFLOW_WWW_USER_USERNAME``   | Username for the administrator UI account.          | airflow                  |\n|                                  | If this value is specified, admin UI user gets      |                          |\n|                                  | created automatically. This is only useful when     |                          |\n|                                  | you want to run Airflow for a test-drive and        |                          |\n|                                  | want to start a container with embedded development |                          |\n|                                  | database.                                           |                          |\n+----------------------------------+-----------------------------------------------------+--------------------------+\n| ``_AIRFLOW_WWW_USER_PASSWORD``   | Password for the administrator UI account.          | airflow                  |\n|                                  | Only used when ``_AIRFLOW_WWW_USER_USERNAME`` set.  |                          |\n+----------------------------------+-----------------------------------------------------+--------------------------+\n| ``_PIP_ADDITIONAL_REQUIREMENTS`` | If not empty, airflow containers will attempt to    |                          |\n|                                  | install requirements specified in the variable.     |                          |\n|                                  | example: ``lxml==4.6.3 charset-normalizer==1.4.1``. |                          |\n|                                  | Available in Airflow image 2.1.1 and above.         |                          |\n+----------------------------------+-----------------------------------------------------+--------------------------+\n```\n\n----------------------------------------\n\nTITLE: Installing Snowflake Provider Package with Common Compat Extra\nDESCRIPTION: Command to install the Snowflake provider package with additional cross-provider dependencies for common.compat functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-snowflake[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Generating Alembic Migration in Airflow\nDESCRIPTION: Commands to create a new database migration file using Alembic in Airflow development environment. This process uses the breeze development environment with postgres backend.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/14_metadata_database_updates.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# starting at the root of the project\n$ breeze --backend postgres\n$ cd airflow\n$ alembic revision -m \"add new field to db\" --autogenerate\n```\n\n----------------------------------------\n\nTITLE: Preparing Task SDK Wheel Distribution\nDESCRIPTION: Command to prepare only wheel distribution format for Airflow Task SDK.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-task-sdk-distributions --distribution-format=wheel\n```\n\n----------------------------------------\n\nTITLE: Breaking Changes Warning Block\nDESCRIPTION: Warning block describing breaking changes in version 3.0.0 regarding deprecated classes and parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/atlassian/jira/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. warning::\n   All deprecated classes, parameters and features have been removed from the Atlassian Jira provider package.\n   The following breaking changes were introduced:\n\n   * Hooks\n\n      * Removed the use of the ``verify`` extra parameters as a ``str`` from ``JiraHook``. Use ``verify`` extra parameters as a ``bool`` instead.\n```\n\n----------------------------------------\n\nTITLE: Installing GRPC Extras for Apache Airflow\nDESCRIPTION: Command to install gRPC hooks and operators for Apache Airflow. This enables integration with gRPC-based services.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_59\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[grpc]'\n```\n\n----------------------------------------\n\nTITLE: Installing Jenkins Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Jenkins provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-jenkins\n```\n\n----------------------------------------\n\nTITLE: Installing Release Candidate in Virtual Environment\nDESCRIPTION: Commands to install Apache Airflow release candidates with optional constraints.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow==<VERSION>rc<X>\n```\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow==<VERSION>rc<X> \\\n  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-<VERSION>/constraints-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Building Docs for Apache Provider Packages with Breeze\nDESCRIPTION: This command uses a package filter to build documentation for all Apache provider packages in the Breeze development environment.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/docs/README.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs --package-filter \"apache-airflow-providers-apache-*\"\n```\n\n----------------------------------------\n\nTITLE: PgBouncer Config Files\nDESCRIPTION: Sample PgBouncer configuration files including database and authentication settings.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n[databases]\nairflow-metadata = host={external_database_host} dbname={external_database_dbname} port=5432 pool_size=10\n\n[pgbouncer]\npool_mode = transaction\nlisten_port = 6543\nlisten_addr = *\nauth_type = scram-sha-256\nauth_file = /etc/pgbouncer/users.txt\nstats_users = postgres\nignore_startup_parameters = extra_float_digits\nmax_client_conn = 100\nverbose = 0\nlog_disconnections = 0\nlog_connections = 0\n\nserver_tls_sslmode = prefer\nserver_tls_ciphers = normal\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Apache Beam Go Stack for Airflow\nDESCRIPTION: This command builds a custom Airflow Docker image with Go installed for running Apache Beam Go pipelines. The image is based on Airflow 2.2.5 and requires specific provider packages to be installed.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/recipes.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . \\\n  --pull \\\n  --build-arg BASE_AIRFLOW_IMAGE=\"apache/airflow:2.2.5\" \\\n  --tag my-airflow-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Emitting Error on Duplicated DAG ID\nDESCRIPTION: Feature (Version 2.0.0): Implements a check to emit an error if a duplicated DAG ID is detected, preventing potential conflicts, referencing pull request #15302.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_37\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Emit error on duplicated DAG ID (#15302)``\n```\n\n----------------------------------------\n\nTITLE: Git Commit History Format\nDESCRIPTION: Standard git commit log format showing commit hash, date and description. Each line represents a single commit with link to full commit details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/commits.rst#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n`6889a333cf <https://github.com/apache/airflow/commit/6889a333cff001727eb0a66e375544a28c9a5f03>`__  2020-11-15   ``Improvements for operators and hooks ref docs (#12366)``\n```\n\n----------------------------------------\n\nTITLE: Running Task SDK Tests\nDESCRIPTION: Command to run all Task SDK tests without database initialization\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing task-sdk-tests\n```\n\n----------------------------------------\n\nTITLE: Git Commit History Entry Format\nDESCRIPTION: Shows the standard format used for documenting individual git commits in the changelog, including commit hash, date and subject.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/hashicorp/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`bbc627a3da <https://github.com/apache/airflow/commit/bbc627a3dab17ba4cf920dd1a26dbed6f5cebfd1>`__  2021-06-18   ``Prepares documentation for rc2 release of Providers (#16501)``\n```\n\n----------------------------------------\n\nTITLE: Adding Kerberos Credential Cache Support for SparkSubmitOperator\nDESCRIPTION: A feature added in version 4.4.0 that allows using Kerberos credential cache (krb5ccache) for Spark job submission.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"Add use_krb5ccache option to SparkSubmitOperator (#35331)\"\n```\n\n----------------------------------------\n\nTITLE: Generating Summary of Cherry-Picked PRs in Shell\nDESCRIPTION: This command generates a summary of cherry-picked PRs, including changelog, doc-only, and excluded changes. It outputs files with lists of commits separated by type.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./dev/assign_cherry_picked_prs_with_milestone.py assign-prs --previous-release v2-2-stable \\\n  --current-release apache/v2-2-test --milestone-number 48 --skip-assigned --assume-yes --print-summary \\\n  --output-folder /tmp\n```\n\n----------------------------------------\n\nTITLE: Updating Airflow Provider Package Documentation\nDESCRIPTION: This commit message signifies updates made to the documentation for Airflow provider packages in preparation for an April release. It references pull request #15236.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_37\n\nLANGUAGE: text\nCODE:\n```\nUpdated documentation for provider packages before April release (#15236)\n```\n\n----------------------------------------\n\nTITLE: Reviewing Cherry-Picked PRs and Assigning Labels in Shell\nDESCRIPTION: This script reviews cherry-picked PRs, assigns milestones and labels, and generates a summary of changes. It uses a custom Python script to process PRs between specified release versions.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./dev/assign_cherry_picked_prs_with_milestone.py assign-prs --previous-release v2-2-stable --current-release apache/v2-2-test --milestone-number 48\n```\n\n----------------------------------------\n\nTITLE: Installing SMTP Extras for Apache Airflow\nDESCRIPTION: Command to install SMTP hooks and operators for Apache Airflow. This enables email sending capabilities.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_69\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[smtp]'\n```\n\n----------------------------------------\n\nTITLE: Updating Azure Credential Settings in Python\nDESCRIPTION: Sets use_async=True for the get_async_default_azure_credential method.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/changelog.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfix(providers/microsoft): setting use_async=True for get_async_default_azure_credential (#35432)\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Failed Tasks Errors in DatabricksCreateJobsOperator\nDESCRIPTION: Bug fix to retrieve all failed tasks errors when an exception is raised in DatabricksCreateJobsOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"get all failed tasks errors in when exception raised in DatabricksCreateJobsOperator (#39354)\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Providers with Specific Airflow Version Check (Bash)\nDESCRIPTION: Verifies provider distributions from the `dist` folder while checking compatibility against a specified older Airflow version (e.g., 2.4.0) using the `--use-airflow-version` flag with the `breeze release-management verify-provider-distributions` command.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management verify-provider-distributions --use-airflow-version 2.4.0\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit with PIP\nDESCRIPTION: Command to install pre-commit hooks using Python's pip package manager.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\n```\n\n----------------------------------------\n\nTITLE: Creating and Adding GPG Keys for Release Signing - Shell Script\nDESCRIPTION: This shell script demonstrates the creation of a GPG key for signing release artifacts, checking out the ASF distribution repository, appending a developer's GPG public key/signatures to the KEYS file (with the person\\'s name replaced accordingly), and committing the changes using SVN. Dependencies include GPG (GNU Privacy Guard) and Subversion (svn). Required parameters involve the committer's full name. Expected output is the KEYS file updated and committed. Ensure adequate permissions to access the repository. Limitations: ensure you replace \\\"Kaxil Naik\\\" with your key name.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell script\nCODE:\n```\n# Create PGP Key\ngpg --gen-key\n\n# Checkout ASF dist repo\nsvn checkout https://dist.apache.org/repos/dist/release/airflow\ncd airflow\n\n\n# Add your GPG pub key to KEYS file. Replace \\\"Kaxil Naik\\\" with your name\n(gpg --list-sigs \\\"Kaxil Naik\\\" && gpg --armor --export \\\"Kaxil Naik\\\" ) >> KEYS\n\n\n# Commit the changes\nsvn commit -m \\\"Add PGP keys of Airflow developers\\\"\n```\n\n----------------------------------------\n\nTITLE: Installing DingTalk Provider with HTTP Dependencies\nDESCRIPTION: Command to install the DingTalk provider package along with HTTP provider dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-dingding[http]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenSearch Extras for Apache Airflow\nDESCRIPTION: Command to install OpenSearch hooks and operators for Apache Airflow. This enables integration with OpenSearch services.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_66\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[opensearch]'\n```\n\n----------------------------------------\n\nTITLE: Output of Breeze Cluster Status Check\nDESCRIPTION: This text block shows the detailed status output from `breeze k8s status`. It includes cluster name, KUBECONFIG/KINDCONFIG paths, control plane URL, CoreDNS status, storage class details, a list of running system pods (like coredns, etcd, kindnet, kube-proxy), and the result of checking the Airflow webserver connection, concluding with a cluster health assessment.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n========================================================================================================================\nCluster: airflow-python-3.9-v1.24.2\n\n    * KUBECONFIG=/Users/jarek/IdeaProjects/airflow/.build/.k8s-clusters/airflow-python-3.9-v1.24.2/.kubeconfig\n    * KINDCONFIG=/Users/jarek/IdeaProjects/airflow/.build/.k8s-clusters/airflow-python-3.9-v1.24.2/.kindconfig.yaml\n\nCluster info: airflow-python-3.9-v1.24.2\n\nKubernetes control plane is running at https://127.0.0.1:48366\nCoreDNS is running at https://127.0.0.1:48366/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\nStorage class for airflow-python-3.9-v1.24.2\n\nNAME                 PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nstandard (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  83s\n\nRunning pods for airflow-python-3.9-v1.24.2\n\nNAME                                                               READY   STATUS    RESTARTS   AGE\ncoredns-6d4b75cb6d-rwp9d                                           1/1     Running   0          71s\ncoredns-6d4b75cb6d-vqnrc                                           1/1     Running   0          71s\netcd-airflow-python-3.9-v1.24.2-control-plane                      1/1     Running   0          84s\nkindnet-ckc8l                                                      1/1     Running   0          69s\nkindnet-qqt8k                                                      1/1     Running   0          71s\nkube-apiserver-airflow-python-3.9-v1.24.2-control-plane            1/1     Running   0          84s\nkube-controller-manager-airflow-python-3.9-v1.24.2-control-plane   1/1     Running   0          84s\nkube-proxy-6g7hn                                                   1/1     Running   0          69s\nkube-proxy-dwfvp                                                   1/1     Running   0          71s\nkube-scheduler-airflow-python-3.9-v1.24.2-control-plane            1/1     Running   0          84s\n\nKinD Cluster API server URL: http://localhost:48366\nConnecting to localhost:18150. Num try: 1\nError when connecting to localhost:18150 : ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n\nAirflow webserver is not available at port 18150. Run `breeze k8s deploy-airflow --python 3.9 --kubernetes-version v1.24.2` to (re)deploy airflow\n\n\nCluster healthy: airflow-python-3.9-v1.24.2\n```\n\n----------------------------------------\n\nTITLE: Generating CLI Documentation for Edge Executor in Python\nDESCRIPTION: This code snippet uses the argparse module to generate command-line interface documentation for the Edge Executor in Apache Airflow. It imports the parser from the edge_executor module in the edge3 provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/edge3/docs/cli-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. argparse::\n   :module: airflow.providers.edge3.executors.edge_executor\n   :func: _get_parser\n   :prog: airflow\n```\n\n----------------------------------------\n\nTITLE: Running Selected Kubernetes Tests with Breeze - Bash\nDESCRIPTION: This bash command runs a specific test file (`test_kubernetes_executor.py`) within the Kubernetes test environment using Breeze. It allows for granular testing, which is useful for targeted development or debugging. Breeze and relevant test files must be properly set up in the environment.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s tests test_kubernetes_executor.py\n```\n\n----------------------------------------\n\nTITLE: Running Specific Kubernetes Tests with pytest\nDESCRIPTION: Command for running a specific Kubernetes executor test with pytest. The -s flag prints output and logs to the terminal immediately.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/kubernetes_tests/test_kubernetes_executor.py::TestKubernetesExecutor::test_integration_run_dag_with_scheduler_failure -s\n```\n\n----------------------------------------\n\nTITLE: Implementing Short Circuit and Branch Operators (AIP-72)\nDESCRIPTION: Commit message describing the implementation of short circuit and branch operators within the Task SDK framework as part of AIP-72. References pull request #46584.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nAIP-72: Implement short circuit and branch operators (#46584)\n```\n\n----------------------------------------\n\nTITLE: Defining Hooks Table in reStructuredText\nDESCRIPTION: Creates a table listing Airflow hooks, including filesystem and subprocess hooks. The table has two columns: Hooks and Guides.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/operators-and-hooks-ref.rst#2025-04-22_snippet_3\n\nLANGUAGE: reStructuredText\nCODE:\n```\n**Hooks:**\n\n.. list-table::\n   :header-rows: 1\n\n   * - Hooks\n     - Guides\n\n   * - :mod:`airflow.providers.standard.hooks.filesystem`\n     -\n\n   * - :mod:`airflow.providers.standard.hooks.subprocess`\n     -\n```\n\n----------------------------------------\n\nTITLE: Using GCSObjectsWithPrefixExistenceSensor in Python\nDESCRIPTION: Demonstrates the use of GCSObjectsWithPrefixExistenceSensor to wait for the existence of a file with a specified prefix in Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsensor_prefix_task = GCSObjectsWithPrefixExistenceSensor(\n    task_id=\"sensor_prefix_task\",\n    bucket=BUCKET_NAME,\n    prefix=PREFIX,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing FAB Provider with Cross-Provider Dependencies using pip\nDESCRIPTION: This shell command demonstrates how to install the `apache-airflow-providers-fab` package along with optional cross-provider dependencies using pip extras. The `[common.compat]` syntax specifically requests the installation of dependencies needed for features reliant on the `apache-airflow-providers-common-compat` package. Execute this in a terminal environment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-fab[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Deprecated Configuration Parameter Removal - YAML Configuration\nDESCRIPTION: Configuration parameter 'webserver.allow_raw_html_descriptions' has been removed from the Airflow webserver configuration. This setting previously controlled HTML rendering in descriptions within UI Trigger forms.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/40029.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nwebserver.allow_raw_html_descriptions\n```\n\n----------------------------------------\n\nTITLE: PgBouncer Configuration with External Secrets\nDESCRIPTION: Commands to create required secrets for PgBouncer setup with external database.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic airflow-pgbouncer-stats --from-literal=connection=postgresql://user:pass@127.0.0.1:6543/pgbouncer?sslmode=disable\n```\n\n----------------------------------------\n\nTITLE: Preparing and Uploading PyPI Packages\nDESCRIPTION: Commands to prepare and upload release candidate packages to PyPI.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nrm dist/*\nbreeze release-management prepare-python-client --distribution-format both --version-suffix-for-pypi \"${VERSION_SUFFIX}\"\n\ntwine check dist/*\n\ntwine upload -r pypi dist/*\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for AwaitMessageTrigger\nDESCRIPTION: ReStructuredText documentation explaining the AwaitMessageTrigger, which consumes messages from Kafka topics and processes them using a provided callable function. The trigger raises a TriggerEvent when the callable returns data.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/triggers.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nApache Kafka Triggers\n=====================\n\n.. _howto/triggers:AwaitMessageTrigger:\n\nAwaitMessageTrigger\n------------------------\n\nThe ``AwaitMessageTrigger`` is a trigger that will consume messages polled from a Kafka topic and process them with a provided callable.\nIf the callable returns any data, a TriggerEvent is raised.\n\nFor parameter definitions take a look at :class:`~airflow.providers.apache.kafka.triggers.await_message.AwaitMessageTrigger`.\n```\n\n----------------------------------------\n\nTITLE: Verifying Installed Dependencies\nDESCRIPTION: Command to verify that installed Python packages are consistent and not conflicting.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-pypi.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n> pip check\nNo broken requirements found.\n```\n\n----------------------------------------\n\nTITLE: Generating Body for Release Vote Email in Shell\nDESCRIPTION: Uses a 'here document' (`cat <<EOF ... EOF`) to generate a template for the body of the release vote email. This template includes placeholders for the release version (`${VERSION}`), suffix (`${VERSION_SUFFIX}`), vote end time (`$VOTE_END_TIME`), countdown URL (`$TIME_DATE_URL`), artifact links, verification instructions (including `helm gpg verify` output example and license check steps), voting instructions, and a placeholder for the testing status issue link. Requires manual updates for GPG output and issue link.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\nHello Apache Airflow Community,\n\nThis is a call for the vote to release Helm Chart version ${VERSION}.\n\nThe release candidate is available at:\nhttps://dist.apache.org/repos/dist/dev/airflow/helm-chart/${VERSION}${VERSION_SUFFIX}/\n\nairflow-chart-${VERSION}-source.tar.gz - is the \"main source release\" that comes with INSTALL instructions.\nairflow-${VERSION}.tgz - is the binary Helm Chart release.\n\nPublic keys are available at: https://www.apache.org/dist/airflow/KEYS\n\nFor convenience \"index.yaml\" has been uploaded (though excluded from voting), so you can also run the below commands.\n\nhelm repo add apache-airflow-dev https://dist.apache.org/repos/dist/dev/airflow/helm-chart/${VERSION}${VERSION_SUFFIX}/\nhelm repo update\nhelm install airflow apache-airflow-dev/airflow\n\nairflow-${VERSION}.tgz.prov - is also uploaded for verifying Chart Integrity, though not strictly required for releasing the artifact based on ASF Guidelines.\n\n$ helm gpg verify airflow-${VERSION}.tgz\ngpg: Signature made Thu Jan  6 21:33:35 2022 MST\ngpg:                using RSA key E1A1E984F55B8F280BD9CBA20BB7163892A2E48E\ngpg: Good signature from \"Jed Cunningham <jedcunningham@apache.org>\" [ultimate]\nplugin: Chart SHA verified. sha256:b33eac716e0416a18af89fb4fa1043fcfcf24f9f903cda3912729815213525df\n\nThe vote will be open for at least 72 hours ($VOTE_END_TIME UTC) or until the necessary number of votes is reached.\n\nhttps://www.timeanddate.com/countdown/$TIME_DATE_URL\n\nPlease vote accordingly:\n\n[ ] +1 approve\n[ ] +0 no opinion\n[ ] -1 disapprove with the reason\n\nOnly votes from PMC members are binding, but members of the community are\nencouraged to test the release and vote with \"(non-binding)\".\n\nConsider this my (binding) +1.\n\nFor license checks, the .rat-excludes files is included, so you can run the following to verify licenses (just update your path to rat):\n\ntar -xvf airflow-chart-${VERSION}-source.tar.gz\ncd airflow-chart-${VERSION}\njava -jar apache-rat-0.13.jar chart -E .rat-excludes\n\nPlease note that the version number excludes the \\`rcX\\` string, so it's now\nsimply ${VERSION}. This will allow us to rename the artifact without modifying\nthe artifact checksums when we actually release it.\n\nThe status of testing the Helm Chart by the community is kept here:\n<TODO COPY LINK TO THE ISSUE CREATED>\n\nThanks,\n<your name>\nEOF\n```\n\n----------------------------------------\n\nTITLE: Protocol Transfers Reference Directive\nDESCRIPTION: RST directive to generate documentation for protocol-based transfer operators\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/operators-and-hooks-ref/protocol.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. transfers-ref::\n   :tags: protocol\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Refreshing Federated AWS Credentials in Airflow\nDESCRIPTION: Method to refresh federated AWS credentials. It logs the credential refresh process and returns a dictionary with access key, secret key, session token, and expiry time.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef _refresh_federated_credentials(self) -> dict[str, str]:\n    self.log.debug(\"Refreshing federated AWS credentials\")\n    credentials = get_federated_aws_credentials(**self.extra_config[\"federation\"])\n    access_key_id = credentials[\"AccessKeyId\"]\n    expiry_time = credentials[\"Expiration\"]\n    self.log.info(\n        f\"New federated AWS credentials received with aws_access_key_id={access_key_id} and \"\n        f\"expiry_time={expiry_time} for connection {self.conn.conn_id}\"\n    )\n    return {\n        \"access_key\": access_key_id,\n        \"secret_key\": credentials[\"SecretAccessKey\"],\n        \"token\": credentials[\"SessionToken\"],\n        \"expiry_time\": expiry_time,\n    }\n```\n\n----------------------------------------\n\nTITLE: Generating Core Coverage Report for Airflow\nDESCRIPTION: Demonstrates how to generate a code coverage report for Airflow's core components using a Python script within the Breeze environment.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_48\n\nLANGUAGE: python\nCODE:\n```\npython scripts/cov/core_coverage.py\n```\n\n----------------------------------------\n\nTITLE: Referencing AWS Profile with Retry Settings in Connection Extra (JSON)\nDESCRIPTION: This JSON snippet for the 'Extra' field instructs an Airflow AWS connection to use the settings defined in the 'awesome_aws_profile' profile from the AWS config files (~/.aws/config, ~/.aws/credentials). This is used in conjunction with the INI snippet that defines retry settings within that profile.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/aws.rst#2025-04-22_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"profile_name\": \"awesome_aws_profile\"\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Webserver SSL (INI)\nDESCRIPTION: Configures the Airflow webserver to use SSL/TLS encryption. Specify the file paths for the SSL certificate (`web_server_ssl_cert`) and private key (`web_server_ssl_key`) within the `[webserver]` section of the Airflow configuration file. Requires valid certificate and key files.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/security.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[webserver]\nweb_server_ssl_cert = <path to cert>\nweb_server_ssl_key = <path to key>\n```\n\n----------------------------------------\n\nTITLE: Running Non-DB Tests with Pytest\nDESCRIPTION: Command for running non-database tests in parallel using pytest-xdist.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest tests --skip-db-tests -n auto\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow Provider Package\nDESCRIPTION: Command to install a specific Airflow provider package using pip package manager.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/packages-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install '{{ package[\"package-name\"] }}'\n```\n\n----------------------------------------\n\nTITLE: Comparing Built and SVN Airflow Packages - Shell Script\nDESCRIPTION: This shell script automates cloning or updating the SVN repository for release artifacts, then iterates over locally built distribution files and compares them to SVN copies using 'diff'. Empty output means the files are identical, while details indicate mismatches. Requires 'svn' CLI installed. The input is the version string and valid Airflow distribution directories; outputs are comparison results for each file.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\n# First clone the repo if you do not have it\ncd ..\n[ -d asf-dist ] || svn checkout --depth=immediates https://dist.apache.org/repos/dist asf-dist\nsvn update --set-depth=infinity asf-dist/dev/airflow\n\n# Then compare the packages\ncd asf-dist/dev/airflow/${VERSION}\nfor i in ${AIRFLOW_REPO_ROOT}/dist/*\ndo\n  echo \"Checking if $(basename $i) is the same as $i\"\n  diff \"$(basename $i)\" \"$i\" && echo \"OK\"\ndone\n```\n\n----------------------------------------\n\nTITLE: Installing Google Provider with Apache Beam Integration using PIP\nDESCRIPTION: This shell command installs the Google provider with Apache Beam integration using PIP. It's noted that this may lead to some BigQuery operators functionality being unavailable.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-google[apache.beam]\n```\n\n----------------------------------------\n\nTITLE: Setting up Databricks connection for sensors\nDESCRIPTION: Example of configuring a Databricks connection to be used with sensors. This snippet shows how to set up the connection with the necessary parameters for authentication and connecting to Databricks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/sql.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconn = Connection(\n    conn_id=DATABRICKS_SYSTEM_TEST_CONN_ID,\n    conn_type=\"databricks\",\n    host=databricks_host,\n    extra={\n        \"token\": databricks_token,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Building Stable Airflow from PyPI\nDESCRIPTION: Basic shell command for building a production Airflow image from PyPI packages.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . \\\n    --tag my-stable-airflow:latest\n```\n\n----------------------------------------\n\nTITLE: Including External Security Documentation via Sphinx in reStructuredText\nDESCRIPTION: This snippet demonstrates the use of the Sphinx '.. include::' directive in a reStructuredText (.rst) file to pull in shared security documentation from a relative path. It requires the Sphinx documentation generator and proper project directory setup. The input is a path to another RST file; the output, at build time, is the inclusion of that file's content. This approach promotes content reuse and consistent documentation across modules.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/imap/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: DingDing Rich Text Message Sending\nDESCRIPTION: Shows how to send rich text messages including link, markdown, actionCard and feedCard types through DingTalk Custom Robot.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/operators.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsend_dingding_msg_markdown = DingdingOperator(\n    task_id=\"markdown_msg\",\n    message_type=\"markdown\",\n    message={\n        \"title\": \"Test markdown message\",\n        \"text\": \"# This is a markdown message\\n * list item 1\\n * list item 2\",\n    },\n    at_mobiles=[\"156XXXXXXXX\"],\n    at_all=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom EKS Waiter Config in JSON\nDESCRIPTION: Example JSON configuration for a custom EKS waiter that checks if all nodegroups in a cluster have been deleted. The waiter polls the ListNodegroups API endpoint and succeeds when no nodegroups remain.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/waiters/README.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"version\": 2,\n    \"waiters\": {\n        \"all_nodegroups_deleted\": {\n            \"operation\": \"ListNodegroups\",\n            \"delay\": 30,\n            \"maxAttempts\": 60,\n            \"acceptors\": [\n                {\n                    \"matcher\": \"path\",\n                    \"argument\": \"length(nodegroups[]) == `0`\",\n                    \"expected\": true,\n                    \"state\": \"success\"\n                },\n                {\n                    \"matcher\": \"path\",\n                    \"expected\": true,\n                    \"argument\": \"length(nodegroups[]) > `0`\",\n                    \"state\": \"retry\"\n                }\n            ]\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Command Definitions for Airflow Project\nDESCRIPTION: This snippet defines various commands used in the Airflow project, each associated with a specific hash. These commands cover a wide range of operations including CI/CD, Kubernetes management, and release processes.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/images/output-commands-hash.txt#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmain:ffb1a766b791beaf5f8a983587db870f\nbuild-docs:2e9882744f219e56726548ce2d13c3f5\nci:fix-ownership:3e5a73533cc96045e72cb258783cfc96\nci:free-space:49af17b032039c05c41a7a8283f365cc\nci:get-workflow-info:8246038093359b9c3c110043419473e2\nci:resource-check:bfcca92f18a403ca630955074eb5e9ad\nci:selective-check:6657ed5d42affb7264b5efcc86f17a2a\nci:5315c29bd9f68725ef92e4db8aff5cda\nci-image:build:dd891a7e3c99131f7166ebbccd4f670b\nci-image:pull:f9248c6026da61fe0acdb5d8f37b20da\nci-image:verify:c90dc7e20fce2351eb89d8d1ebbd35e7\nci-image:973b722fdd947e21ff59f3bf9cfc6264\ncleanup:8d92d453a6700f6d8cb11fb6a8b50461\ncompile-www-assets:0963f1409f0aa1e3b137cddd4cc52e87\ndown:4580f5b3b178ea00182694f134a751f3\nexec:9d0fb86607526afb6b161115ae7bf9cc\nk8s:build-k8s-image:b625255c3e8f3f794ee404f9a4476836\nk8s:configure-cluster:9958c5aac726565ec043e850d56ec8f8\nk8s:create-cluster:3e43f9da5e7c0bb67f3d868c9005515a\nk8s:delete-cluster:5f580bb09b6456610bf1044321717673\nk8s:deploy-airflow:f4b05b2101a4a029c9706ecd6fbf3c5c\nk8s:k9s:892a7931e981ba01a21c0da72fac39bc\nk8s:logs:f1a3fa2c5747d86ff712d1b0a06ff48b\nk8s:run-complete-tests:5018013f47f6c60aae07eb35256eb240\nk8s:setup-env:a34e94744ca4e0592371fe55478c3d54\nk8s:shell:b872c01cedfd50b865d98ed85933fed7\nk8s:status:6e711c24648c9bf42372e5b73cb2ac0f\nk8s:tests:4fea1fee4cfbf15f313ffd9026219401\nk8s:upload-k8s-image:46c5f1b042222047fda3f18f1ef75835\nk8s:6994fe347c18bcc01d95fb721a3757d5\nprod-image:build:20f84ddadc2fe4ae2723b7ccdde0197f\nprod-image:pull:3817ef211b023b76df84ee1110ef64dd\nprod-image:verify:bd2b78738a7c388dbad6076c41a9f906\nprod-image:e9ecd759e51ebd926df3170b29d1d2dc\nrelease-management:add-back-references:51960e2831d0e03a2b127d252929b843\nrelease-management:create-minor-branch:a3834afc4aa5d1e98002c9e9e7a9931d\nrelease-management:generate-constraints:01aef235b11e59ed7f10c970a5cdaba7\nrelease-management:generate-issue-content-providers:cda108e7f2506c2816af8f2a6c24070c\nrelease-management:generate-providers-metadata:d4e8e5cfaa024e3963af02d7a873048d\nrelease-management:install-provider-distributions:34c38aca17d23dbb454fe7a6bfd8e630\nrelease-management:prepare-airflow-distributions:85d01c57e5b5ee0fb9e5f9d9706ed3b5\nrelease-management:prepare-provider-documentation:eb861d68b8d72cd98dc8732fc5393796\nrelease-management:prepare-provider-distributions:908e2c826f7b4959dfd8bc693f3857a7\nrelease-management:publish-docs:51ee9bf1268529513996a14bd5350c19\nrelease-management:release-prod-images:cfbfe8b19fee91fd90718f98ef2fd078\nrelease-management:start-rc-process:b27bd524dd3c89f50a747b60a7e892c1\nrelease-management:start-release:419f48f6a4ff4457cb9de7ff496aebbe\nrelease-management:update-constraints:02ec4b119150e3fdbac52026e94820ef\nrelease-management:verify-provider-distributions:96dce5644aad6b37080acf77b3d8de3a\nrelease-management:59d956e45fccf55e47f16e33cfc5d04a\nsbom:build-all-airflow-images:32f8acade299c2b112e986bae99846db\nsbom:generate-providers-requirements:3926848718283cf2ef00310a0892e867\nsbom:update-sbom-information:653be48be70b4b7ff5172d491aadc694\nsbom:386048e0c00c0de30cf181eb9f3862ea\nsetup:autocomplete:fffcd49e102e09ccd69b3841a9e3ea8e\nsetup:check-all-params-in-groups:f9ca6bef11ed65e40f06d7cf261a4859\nsetup:config:53a0aeec6237da4d46bde68fafa29dc3\nsetup:regenerate-command-images:ea2fba3440bd4e84311a53abe6e8dc56\nsetup:self-upgrade:4af905a147fcd6670a0e33d3d369a94b\nsetup:version:be116d90a21c2afe01087f7609774e1e\nsetup:304a70e939d78427c749c24e8c0992df\nshell:aa92fe60473e4b5d0f41b5b182c02468\nstart-airflow:f09871892c61bc889e6b56791115c923\nstatic-checks:f39d698d5735f372c01d9f1d5719fd13\ntesting:db-tests:e08e3f30ddc34d95ae56de5222392b59\ntesting:docker-compose-tests:fd154a058082fcfda12eb877a9a89338\ntesting:helm-tests:98a9ba6631249762b1633b76a29f4461\ntesting:integration-tests:c7fde5144126a445201d7e353aa19ba5\ntesting:non-db-tests:ed916603036dd9979b1593c4d088eb40\ntesting:tests:4ad1723c7b2b6d2d7d249d42964ced92\ntesting:eae1e62ff40d5012388abd104461b88e\n```\n\n----------------------------------------\n\nTITLE: Example GPG Signature Verification Output\nDESCRIPTION: Example output showing a successful GPG signature verification for an Apache Airflow provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/installing-from-sources.rst#2025-04-22_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ gpg --verify apache-airflow-providers-airbyte-1.0.0-source.tar.gz.asc apache-airflow-providers-airbyte-1.0.0-source.tar.gz\n  gpg: Signature made Sat 11 Sep 12:49:54 2021 BST\n  gpg:                using RSA key CDE15C6E4D3A8EC4ECF4BA4B6674E08AD7DE406F\n  gpg:                issuer \"kaxilnaik@apache.org\"\n  gpg: Good signature from \"Kaxil Naik <kaxilnaik@apache.org>\" [unknown]\n  gpg:                 aka \"Kaxil Naik <kaxilnaik@gmail.com>\" [unknown]\n  gpg: WARNING: The key's User ID is not certified with a trusted signature!\n  gpg:          There is no indication that the signature belongs to the owner.\n  Primary key fingerprint: CDE1 5C6E 4D3A 8EC4 ECF4  BA4B 6674 E08A D7DE 406F\n```\n\n----------------------------------------\n\nTITLE: Improving Airflow Operators and Hooks Reference Documentation\nDESCRIPTION: This commit message highlights improvements made to the reference documentation (ref docs) for Airflow operators and hooks. It references pull request #12366.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_58\n\nLANGUAGE: text\nCODE:\n```\nImprovements for operators and hooks ref docs (#12366)\n```\n\n----------------------------------------\n\nTITLE: Showing MySQL Table Structure\nDESCRIPTION: SQL command to display the create table statement for checking table structures in MySQL.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading.rst#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSHOW CREATE TABLE task_reschedule;\nSHOW CREATE TABLE xcom;\nSHOW CREATE TABLE task_fail;\nSHOW CREATE TABLE rendered_task_instance_fields;\nSHOW CREATE TABLE task_instance;\n```\n\n----------------------------------------\n\nTITLE: Viewing Collapsed Log Groups in Airflow UI\nDESCRIPTION: Example of how collapsed log groups appear in the Airflow web UI before expanding, showing the condensed view of grouped log sections.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/logging-tasks.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n[2024-03-08, 23:30:18 CET] {logging_mixin.py:188} INFO - Here is some standard text.\n[2024-03-08, 23:30:18 CET] {logging_mixin.py:188}  Non important details\n[2024-03-08, 23:30:18 CET] {logging_mixin.py:188} INFO - Here is again some standard text.\n```\n\n----------------------------------------\n\nTITLE: Configuring SendGrid Email Backend (INI)\nDESCRIPTION: This snippet shows how to configure the SendGrid email backend in the Airflow configuration file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_9\n\nLANGUAGE: ini\nCODE:\n```\n[email]\nemail_backend = airflow.providers.sendgrid.utils.emailer.send_email\nemail_conn_id = sendgrid_default\nfrom_email = \"hello@eg.com\"\n```\n\n----------------------------------------\n\nTITLE: Updating Databricks Connector Version Restriction in Python\nDESCRIPTION: Excludes Databricks connector version 2.9.0 due to a bug. This prevents using a problematic version of the connector.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nEcldude databrick connector 2.9.0 due to a bug (#33311)\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Hooks on All Files\nDESCRIPTION: Command to execute all pre-commit hooks on all files in the repository, showing various checks like license verification, code formatting, and security checks\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Running Parallel Core Tests\nDESCRIPTION: Example of running API and WWW tests in parallel using Breeze\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-tests --parallel-test-types \"API WWW\" --run-in-parallel\n```\n\n----------------------------------------\n\nTITLE: Accessing Airflow Configuration Before Deprecation (Python)\nDESCRIPTION: This code snippet demonstrates how to access the 'get' function from the airflow.configuration module before the deprecation. It imports the function directly and uses it to retrieve a configuration value.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43530.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.configuration import get\n\nvalue = get(\"section\", \"key\")\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Image from GitHub Main Branch\nDESCRIPTION: Builds a production Airflow image using Python 3.9 from the latest main version on GitHub. Constraints are taken from the latest version of the constraints-main branch on GitHub.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . -f Dockerfile --pull --build-arg PYTHON_BASE_IMAGE=\"python:3.9-slim-bullseye\" --build-arg AIRFLOW_INSTALLATION_METHOD=\"github\" --build-arg AIRFLOW_CONSTRAINTS_REFERENCE=\"constraints-main\" --tag \"my-company/airflow:main-python3.9\"\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Secret and StatefulSet Deletion Commands\nDESCRIPTION: Commands to manually delete Kubernetes Secret and StatefulSet objects when upgrading PostgreSQL subchart. Required for upgrading an existing helm release with built-in postgres database.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/RELEASE_NOTES.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete secret {RELEASE_NAME}-postgresql\nkubectl delete statefulset {RELEASE_NAME}-postgresql\n```\n\n----------------------------------------\n\nTITLE: Including Security RST Extension\nDESCRIPTION: Sphinx configuration to include the security notice from the common development extensions directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Configuring SQLite Connection with Relative Path in Airflow URI Format\nDESCRIPTION: This snippet demonstrates how to set up a SQLite connection using a relative path in the Airflow URI format. It includes the 'mode' parameter set to read-only.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/connections/sqlite.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SQLITE_DEFAULT='sqlite://relative/path/to/db?mode=ro'\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 2.0.1\nDESCRIPTION: Changelog entry noting connection import optimization for Airflow 2.2.0 in version 2.0.1\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sendgrid/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n2.0.1\n.....\n\nMisc\n~~~~\n\n* ``Optimise connection importing for Airflow 2.2.0``\n```\n\n----------------------------------------\n\nTITLE: Emitting Asset Events using AssetAlias and Metadata in Python with Airflow\nDESCRIPTION: This code demonstrates how to emit asset events using AssetAlias by yielding Metadata. It creates an S3 asset and yields a Metadata object with the asset, extra information, and an alias.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk.definitions.asset.metadata import Metadata\n\n\n@task(outlets=[AssetAlias(\"my-task-outputs\")])\ndef my_task_with_metadata():\n    s3_asset = Asset(uri=\"s3://bucket/my-task\", name=\"example_s3\")\n    yield Metadata(s3_asset, extra={\"k\": \"v\"}, alias=\"my-task-outputs\")\n```\n\n----------------------------------------\n\nTITLE: Securing PyPI Configuration File Permissions - Shell Script\nDESCRIPTION: This shell command sets the file permissions of the user's .pypirc to 600, ensuring only the owner can read and write, protecting sensitive API tokens used for publishing packages. It must be run after creating .pypirc, and requires user-level access to the home directory. No inputs other than the default location (~/.pypirc). No outputs except modified file permissions.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell script\nCODE:\n```\nchmod 600 ~/.pypirc\n```\n\n----------------------------------------\n\nTITLE: Installing OpenLineage Support for Apache Airflow\nDESCRIPTION: Command to install OpenLineage integration for Apache Airflow. This enables sending OpenLineage events for data lineage tracking.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_65\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[openlineage]'\n```\n\n----------------------------------------\n\nTITLE: Installing Slack Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Slack integration, enabling Slack hooks and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[slack]'\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Tree Structure for AWS Operators\nDESCRIPTION: Defines the documentation structure for AWS operators using sphinx toctree directive. Includes sections for Athena, EMR, Redshift, S3, and other AWS services with maxdepth of 1 and glob pattern enabled.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    athena/index\n    emr/index\n    redshift/index\n    s3/index\n    *\n```\n\n----------------------------------------\n\nTITLE: Running All Provider Tests\nDESCRIPTION: Command to run all provider tests using Breeze\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-tests\n```\n\n----------------------------------------\n\nTITLE: Configuring DAG Main Block for Testing\nDESCRIPTION: Python code block to add a main entry point for testing DAGs using the backfill functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_vscode.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    dag.test()\n```\n\n----------------------------------------\n\nTITLE: Publishing Microsoft Provider Documentation\nDESCRIPTION: Example of using package filter to publish documentation for Microsoft providers using glob pattern.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management publish-docs \"apache-airflow-providers-microsoft*\"\n```\n\n----------------------------------------\n\nTITLE: Installing Basic System Dependencies\nDESCRIPTION: Command to install basic system-level dependencies required for Airflow on Debian/Ubuntu systems.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install openssl sqlite3 default-libmysqlclient-dev libmysqlclient-dev postgresql\n```\n\n----------------------------------------\n\nTITLE: Installing Papermill Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Papermill provider package on top of an existing Airflow 2 installation using pip. This package supports Python versions 3.9, 3.10, 3.11, and 3.12.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-papermill\n```\n\n----------------------------------------\n\nTITLE: Creating Document from Google Cloud Storage for Natural Language Processing in Python\nDESCRIPTION: This snippet shows how to create a Document object that refers to content stored in Google Cloud Storage for use with Google Cloud Natural Language API.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/natural_language.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndocument = language.Document(\n    gcs_content_uri=\"gs://cloud-samples-data/language/android.txt\",\n    type_=language.Document.Type.PLAIN_TEXT,\n)\n```\n\n----------------------------------------\n\nTITLE: Airflow Database Reset and Connections Commands\nDESCRIPTION: Shows the commands affected by the change in default connection handling. The db reset command no longer automatically creates default connections, requiring an explicit call to create-default-connections.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/47414.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairflow db reset\n```\n\nLANGUAGE: bash\nCODE:\n```\nairflow connections create-default-connections\n```\n\n----------------------------------------\n\nTITLE: Verifying Apache Airflow Provider Package Signatures\nDESCRIPTION: Commands to verify the signatures of Apache Airflow provider packages using gpg, pgpv, and pgp. These commands check the authenticity of the downloaded packages.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngpg --verify apache-airflow-providers-********.asc apache-airflow-*********\n```\n\nLANGUAGE: bash\nCODE:\n```\npgpv apache-airflow-providers-********.asc\n```\n\nLANGUAGE: bash\nCODE:\n```\npgp apache-airflow-providers-********.asc\n```\n\n----------------------------------------\n\nTITLE: Including External reStructuredText Content via Directive\nDESCRIPTION: This reStructuredText directive (`.. include::`) instructs the Sphinx documentation generator to insert the content of the specified file (`/../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst`) at this point in the document. This is used to include shared instructions, in this case, how to install Apache Airflow providers from source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/tableau/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: AQLOperator Example - Listing Documents in ArangoDB Collection\nDESCRIPTION: Example showing how to use AQLOperator to list all documents in a students collection. The operator executes an AQL query and can process results using a custom callable function.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nlist_students = AQLOperator(\n    task_id=\"list_students_in_collection\",\n    arangodb_conn_id=\"arangodb_default\",\n    query=\"FOR doc IN students RETURN doc\"\n```\n\n----------------------------------------\n\nTITLE: Loading Pickled Pull Request Data in Python\nDESCRIPTION: This code opens a pickled file named 'prlist' and loads its contents into the 'selected_prs' variable. The file is expected to contain serialized pull request data.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/stats/explore_pr_candidates.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfile = open(\"prlist\", \"rb\")  # open the pickled file\nselected_prs = pickle.load(file)\n```\n\n----------------------------------------\n\nTITLE: Commit Hash Reference - May 2023 Provider Updates\nDESCRIPTION: Git commit hash references for version 5.1.0 changes including SDK updates and circular import fixes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n45548b9451 2023-05-19 \"Prepare RC2 docs for May 2023 wave of Providers (#31416)\"\nabea189022 2023-05-18 \"Use '__version__' in providers not 'version' (#31393)\"\nf5aed58d9f 2023-05-18 \"Fixing circular import error in providers caused by airflow version check (#31379)\"\n```\n\n----------------------------------------\n\nTITLE: Running All Helm Tests with Breeze\nDESCRIPTION: Command to run all Helm tests using breeze testing environment with parallel execution\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/helm_unit_tests.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing helm-tests\n```\n\n----------------------------------------\n\nTITLE: Dual Apache-MIT License Header Text\nDESCRIPTION: Standard copyright and license header text establishing dual licensing under Apache 2.0 and MIT licenses, with attribution to The Servo Project and zerolib developers. The header provides links to the full license texts and states usage terms.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/3rd-party-licenses/LICENSE-reproducible.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n# Copyright 2013 The Servo Project Developers.\n# Copyright 2017 zerolib Developers.\n#\n# Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n# http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n# <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n# option. This file may not be copied, modified, or distributed\n# except according to those terms.\n```\n\n----------------------------------------\n\nTITLE: Replacing strftime with f-strings in Python\nDESCRIPTION: Replaces strftime calls with f-strings where it results in nicer code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntimestamp = f\"{dt:%Y-%m-%d %H:%M:%S}\"\n```\n\n----------------------------------------\n\nTITLE: Summarizing Apache Airflow Client Release Vote Results (Shell Script)\nDESCRIPTION: Provides a template text block, intended to be manually completed and used in communications (e.g., email), summarizing the voting outcome for an Apache Airflow Python Client release candidate (RC). It requires filling in the number and names of binding/non-binding votes, the link to the official vote thread on the Apache mailing list, and the sender's name.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_24\n\nLANGUAGE: shell script\nCODE:\n```\nHello,\n\nApache Airflow Python Client 2.5.0 (based on RC1) has been accepted.\n\n3 \"+1\" binding votes received:\n- Ephraim Anierobi\n- Jarek Potiuk\n- Jed Cunningham\n\n\n1 \"+1\" non-binding votes received:\n\n- Pierre Jeambrun\n\nVote thread:\nhttps://lists.apache.org/thread/1qcj0r67dff3zg0w2vyfhr30fx9xtp3y\n\nI'll continue with the release process, and the release announcement will follow shortly.\n\nCheers,\n<your name>\n```\n\n----------------------------------------\n\nTITLE: Fixing Release Number for Fresh Airflow Release\nDESCRIPTION: This commit message notes the correction of a release number associated with a new (fresh) Airflow release. It references pull request #9408.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_82\n\nLANGUAGE: text\nCODE:\n```\nFixed release number for fresh release (#9408)\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow Version in Helm Chart\nDESCRIPTION: Sets the default Airflow version to 2.2.3 in the Helm chart values.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/RELEASE_NOTES.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- Use \"2.2.3\" as default Airflow version (#20450)\n```\n\n----------------------------------------\n\nTITLE: Inspecting sys.path in Python for Apache Airflow\nDESCRIPTION: Shows how to print the sys.path variable in Python, which includes the custom directory added to PYTHONPATH for Airflow operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/modules_management.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import sys\n>>> from pprint import pprint\n>>> pprint(sys.path)\n['',\n '/home/arch/projects/airflow_operators'\n '/home/arch/.pyenv/versions/3.9.4/lib/python37.zip',\n '/home/arch/.pyenv/versions/3.9.4/lib/python3.9',\n '/home/arch/.pyenv/versions/3.9.4/lib/python3.9/lib-dynload',\n '/home/arch/venvs/airflow/lib/python3.9/site-packages']\n```\n\n----------------------------------------\n\nTITLE: Optional Inclusion of User Names in Databricks list_jobs\nDESCRIPTION: Bug fix to make inclusion of user names optional in the Databricks 'list_jobs' function.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"Databricks: optional include of user names in 'list_jobs' (#40178)\"\n```\n\n----------------------------------------\n\nTITLE: MySQL local_infile parameter configuration\nDESCRIPTION: Demonstrates adding 'local_infile' parameter to S3ToMySqlOperator\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nAdd 'local_infile' parameter to 'S3ToMySqlOperator' (#33459)\n```\n\n----------------------------------------\n\nTITLE: Installing Opsgenie Provider with Common Compatibility Dependencies\nDESCRIPTION: This command installs the Opsgenie provider package along with its common compatibility dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opsgenie/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-opsgenie[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Adding Notification Settings Parameters\nDESCRIPTION: New feature adding notification settings parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"Add notification settings parameters (#39175)\"\n```\n\n----------------------------------------\n\nTITLE: Werkzeug Algorithm Configuration\nDESCRIPTION: Configuration to use different default algorithms based on Werkzeug versions for compatibility.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n[providers-fab/v1-5] Use different default algorithms for different werkzeug versions\n```\n\n----------------------------------------\n\nTITLE: Defining Dataproc Metastore Service Update Configuration in Python\nDESCRIPTION: Shows how to prepare the configuration for updating a Dataproc Metastore service. This involves creating a `service` dictionary with the new values and an `update_mask` FieldMask specifying which fields (e.g., 'labels') should be updated. This configuration is used by the `DataprocMetastoreUpdateServiceOperator`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nSERVICE_TO_UPDATE = {\"labels\": {\"airflow-version\": \"v2-3-test\"}}\nUPDATE_MASK = {\"paths\": [\"labels\"]}\n```\n\n----------------------------------------\n\nTITLE: Start Airflow DAG Processor\nDESCRIPTION: Starts the DAG processor separately, which is now required in Airflow 3.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading_to_airflow3.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nairflow dag-processor\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow with Constraints using UV\nDESCRIPTION: Command to install Airflow with specific provider dependencies and version constraints for Python 3.9.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/07_local_virtualenv.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install -e \".[devel,google]\" \\\n  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-main/constraints-source-providers-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Building Provider Documentation with Breeze\nDESCRIPTION: Command example for building specific provider documentation using breeze with clean build option.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/11_documentation_building.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs --doc-only --clean fab\n```\n\n----------------------------------------\n\nTITLE: Including External Documentation in RST File\nDESCRIPTION: This RST directive includes external documentation from a common source about installing Airflow providers from sources. It references a shared documentation file from a development common directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Retrieving Airflow Connection from Yandex Lockbox Backend using Console\nDESCRIPTION: Demonstrates how to verify that an Airflow connection stored in Yandex Lockbox can be successfully retrieved. The `airflow connections get` command fetches the connection details, and the `-o json` flag formats the output as JSON.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_13\n\nLANGUAGE: console\nCODE:\n```\n$ airflow connections get mysqldb -o json\n```\n\n----------------------------------------\n\nTITLE: Disabling Database Cleanup in Airflow Pytest\nDESCRIPTION: Demonstrates how to run Airflow tests using pytest without clearing the database before each test module. This is useful for scenarios like running parallel tests against the same database.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_46\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/core/ --no-db-cleanup\n```\n\n----------------------------------------\n\nTITLE: Adding Run ID Generation Logic for Null Logical Date (AIP-83 Amendment)\nDESCRIPTION: Commit message detailing an amendment to AIP-83, adding logic to generate a 'run_id' when the logical date is None. References pull request #46616.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_29\n\nLANGUAGE: plaintext\nCODE:\n```\nAIP-83 amendment: Add logic for generating run_id when logical date is None. (#46616)\n```\n\n----------------------------------------\n\nTITLE: Referencing the Airflow gRPC Hook Class\nDESCRIPTION: Provides the reStructuredText reference to the `GrpcHook` class within the Airflow gRPC provider. This hook encapsulates the logic for establishing connections and interacting with gRPC services.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/connections/grpc.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n:class:`~airflow.providers.grpc.hooks.grpc.GrpcHook`\n```\n\n----------------------------------------\n\nTITLE: Starting Breeze for Airflow Development in Bash\nDESCRIPTION: Commands to start Breeze, the development environment for Airflow, using Python 3.9 and PostgreSQL backend.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nbreeze --python 3.9 --backend postgres\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-grpc using pip\nDESCRIPTION: This shell command installs the apache-airflow-providers-grpc package using pip. It should be run in an environment with an existing Airflow 2 installation (version 2.9.0 or higher is required as specified in the requirements section).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-grpc\n```\n\n----------------------------------------\n\nTITLE: Sanitizing Beeline Principal Parameter in Python\nDESCRIPTION: Implements sanitization for the beeline principal parameter to enhance security in the Hive provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n\"Sanitize beeline principal parameter (#31983)\"\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Simplify airflow_version imports\nDESCRIPTION: This text is a commit message summary indicating a simplification of how 'airflow_version' is imported, likely for maintainability or performance, linked to pull request #39497.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_15\n\nLANGUAGE: plaintext\nCODE:\n```\nSimplify 'airflow_version' imports (#39497)\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow with CeleryExecutor and Extras from PyPI\nDESCRIPTION: This command starts Airflow 2.7.0 from PyPI with CeleryExecutor and celery extra in the Breeze environment.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nbreeze start-airflow --use-airflow-version 2.7.0 --executor CeleryExecutor --airflow-extras celery\n```\n\n----------------------------------------\n\nTITLE: Running MyPy Check for Staged Changes in Airflow using Breeze in Bash\nDESCRIPTION: This command uses Breeze to run the MyPy check for currently staged changes in the Airflow directory, excluding providers.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nbreeze static-checks --type mypy-airflow\n```\n\n----------------------------------------\n\nTITLE: Configuring Oracle Connection with Host and Schema in Python\nDESCRIPTION: Example of configuring an Oracle connection using a host and service name as schema. This approach uses separate parameters rather than a full connection string.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/connections/oracle.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nHost = \"dbhost.example.com\"\nSchema = \"orclpdb1\"\n```\n\n----------------------------------------\n\nTITLE: Building All Docs and Running Spellcheck with Breeze\nDESCRIPTION: This command builds all the documentation and runs a spellcheck using the Breeze development environment.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/docs/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs\n```\n\n----------------------------------------\n\nTITLE: Including External Security Documentation in ReStructuredText\nDESCRIPTION: This directive includes security-related documentation from an external file within the Apache Airflow project structure. It's typically used to centralize and maintain consistent security information across project documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/docker/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Preparing Python Client for Testing\nDESCRIPTION: Command to package the Airflow Python client for testing\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-python-client --distribution-format both\n```\n\n----------------------------------------\n\nTITLE: Installing Tableau Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Tableau integration, enabling Tableau hooks and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_38\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[tableau]'\n```\n\n----------------------------------------\n\nTITLE: Creating Google Spreadsheet with Airflow Operator\nDESCRIPTION: Demonstrates how to create a new Google Spreadsheet using the GoogleSheetsCreateSpreadsheetOperator. The operator allows setting spreadsheet properties and returns the spreadsheet ID via XCom.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/suite/sheets.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntask_create_spreadsheet = GoogleSheetsCreateSpreadsheetOperator(\n    task_id=\"create_spreadsheet\",\n    spreadsheet={\n        \"properties\": {\"title\": \"Test1\"}\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Class Level DB Test Marking\nDESCRIPTION: Example of marking an entire test class as database tests\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n@pytest.mark.db_test\nclass TestDatabricksHookAsyncAadTokenSpOutside: ...\n```\n\n----------------------------------------\n\nTITLE: Adding Proxy User Parameter for Hive in Python\nDESCRIPTION: Adds a proxy_user parameter to the Hive provider to allow impersonation of other users when executing Hive queries.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n\"Add param proxy user for hive (#36221)\"\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub SSH Known Hosts in Airflow DAGs\nDESCRIPTION: Configuration for adding GitHub's SSH public key to the known hosts for git sync functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndags:\n  gitSync:\n    knownHosts: |\n      github.com ssh-rsa AAAA...1/wsjk=\n```\n\n----------------------------------------\n\nTITLE: Running Arbitrary Commands in Breeze Shell\nDESCRIPTION: Demonstrates how to execute arbitrary commands inside the Breeze container using the shell command. This allows developers to run custom commands in the Airflow development environment.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nbreeze shell \"ls -la\"\n```\n\n----------------------------------------\n\nTITLE: Getting a Google Cloud Tasks Task in Python\nDESCRIPTION: This snippet shows how to get information about a specific task in a Google Cloud Tasks queue using the CloudTasksTaskGetOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# [START tasks_get]\nCloudTasksTaskGetOperator(\n    task_id=\"tasks_get\",\n    location=LOCATION,\n    queue_name=QUEUE_ID,\n    task_name=TASK_NAME,\n).execute(context=context)\n# [END tasks_get]\n```\n\n----------------------------------------\n\nTITLE: Fixing InvalidURL Error in Airbyte Hook (Commit Message)\nDESCRIPTION: Commit message detailing a fix for an 'InvalidURL' error in the Apache Airflow Airbyte hook by adding schema and port information.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_13\n\nLANGUAGE: text\nCODE:\n```\nfix(airbyte/hooks): add schema and port to prevent InvalidURL error (#38860)\n```\n\n----------------------------------------\n\nTITLE: Fixing and Reapplying Provider Documentation Templates\nDESCRIPTION: Corrects issues with provider documentation templates and reapplies them to ensure consistency and accuracy across provider documentation, as per issue #35686.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nFix and reapply templates for provider documentation (#35686)\n```\n\n----------------------------------------\n\nTITLE: Installing Vertica Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Vertica integration, enabling Vertica hook support as an Airflow backend.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_41\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[vertica]'\n```\n\n----------------------------------------\n\nTITLE: Installing MySQL Dependencies for Airflow\nDESCRIPTION: Command to install PyMySQL package in the airflow virtual environment for MySQL database connection.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_vscode.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pyenv activate airflow-env\n$ pip install PyMySQL\n```\n\n----------------------------------------\n\nTITLE: Implementing Log Grouping in Airflow Tasks\nDESCRIPTION: Example showing how to use log grouping markers in Airflow to create collapsible sections in task logs, compatible with GitHub and Azure DevOps CI pipelines.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/logging-tasks.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Here is some standard text.\")\nprint(\"::group::Non important details\")\nprint(\"bla\")\nprint(\"debug messages...\")\nprint(\"::endgroup::\")\nprint(\"Here is again some standard text.\")\n```\n\n----------------------------------------\n\nTITLE: Inspecting Python Path in Apache Airflow\nDESCRIPTION: Shows an example output of the Python path configuration in Airflow, which includes various system and user-defined directories where Python looks for modules.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/modules_management.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nPython PATH: [/home/rootcss/venvs/airflow/bin:/usr/lib/python38.zip:/usr/lib/python3.9:/usr/lib/python3.9/lib-dynload:/home/rootcss/venvs/airflow/lib/python3.9/site-packages:/home/rootcss/airflow/dags:/home/rootcss/airflow/config:/home/rootcss/airflow/plugins]\n```\n\n----------------------------------------\n\nTITLE: Removing/Refactoring default_args Pattern in Example DAGs\nDESCRIPTION: Excluded Change (Version 2.0.1): Removes or refactors the common `default_args` pattern in miscellaneous provider example DAGs, likely promoting newer practices, referencing pull request #16872.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_35\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Remove/refactor default_args pattern for miscellaneous providers (#16872)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Improve provider documentation and README structure (#32125)\nDESCRIPTION: This commit message, associated with version 3.2.1, details improvements made to the documentation structure and README files for Apache Airflow Providers. Commit hash: 09d4718d3a, Date: 2023-06-27.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n``Improve provider documentation and README structure (#32125)``\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents for Databricks Operators in reStructuredText\nDESCRIPTION: This snippet uses a toctree directive to create a table of contents for Databricks operators documentation. It is set to display a maximum depth of 1 level and includes all files in the current directory using a glob pattern.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Installing Samba Provider with Google Extras using Pip (Bash)\nDESCRIPTION: This command demonstrates how to install the apache-airflow-providers-samba package using pip, including optional extras for Google provider integration. This ensures that necessary dependencies for interacting with Google services via the Samba provider are also installed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-samba[google]\n```\n\n----------------------------------------\n\nTITLE: Removing Context Sending Feature in Venv Operators\nDESCRIPTION: Commit message describing the removal of the feature that allowed sending context in venv operators via the 'use_airflow_context' parameter. References pull request #46306.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_37\n\nLANGUAGE: plaintext\nCODE:\n```\nRemoving feature: send context in venv operators (using 'use_airflow_context') (#46306)\n```\n\n----------------------------------------\n\nTITLE: Displaying Host and Git Variables for Apache Airflow CI\nDESCRIPTION: This markdown table shows the host and Git variables used in Apache Airflow's CI environment. It compares the values set in local development versus CI contexts and provides comments on each variable's purpose.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/07_running_ci_locally.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n| Variable      | Local dev | CI         | Comment                                |\n|---------------|-----------|------------|----------------------------------------|\n| HOST_USER_ID  | Host UID  |            | User id of the host user.              |\n| HOST_GROUP_ID | Host GID  |            | Group id of the host user.             |\n| HOST_OS       | <from os> | linux      | OS of the Host (darwin/linux/windows). |\n| COMMIT_SHA    |           | GITHUB_SHA | SHA of the commit of the build is run  |\n```\n\n----------------------------------------\n\nTITLE: Verifying Release Signatures with GPG\nDESCRIPTION: Script to verify GPG signatures for all .asc files in the current directory.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\nfor i in *.asc\ndo\n   echo -e \"Checking $i\\n\"; gpg --verify $i\ndone\n```\n\n----------------------------------------\n\nTITLE: Building Helm Dependencies Command\nDESCRIPTION: Command to download and build chart dependencies\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/extending-the-chart.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm dependency build\n```\n\n----------------------------------------\n\nTITLE: Creating Dependency Depth JSON from Dependency Tree in Airflow\nDESCRIPTION: Command pipeline used to generate the dependency_depth.json file for Airflow 2.9.3. It reads the dependency tree file and processes it through a Python script to analyze and calculate the depth of each dependency.\nSOURCE: https://github.com/apache/airflow/blob/main/generated/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncat /opt/airflow/generated/dep_tree.txt | python /opt/airflow/scripts/in_container/get_dependency_status.py >/opt/airflow/generated/dependency_depth.json\n```\n\n----------------------------------------\n\nTITLE: Sample Commit History Entry from Changelog\nDESCRIPTION: Example of how commit information is structured in the changelog, showing the commit hash, date and commit message.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n`c85dd5442d <https://github.com/apache/airflow/commit/c85dd5442de2d729abfb8e4c7117bca0cf0b9131>`__  2025-04-16   ``Fix xcom for system tests (#49337)``\n```\n\n----------------------------------------\n\nTITLE: Storing Variables in YAML Format\nDESCRIPTION: This snippet demonstrates how to store variables in a YAML file. It contains key-value pairs representing variable names and their values.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/secrets-backend/local-filesystem-secrets-backend.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nVAR_A: some_value\nVAR_B: different_value\n```\n\n----------------------------------------\n\nTITLE: Package Name Declaration in RST\nDESCRIPTION: Declares the package name for the Databricks provider in RST format\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n``apache-airflow-providers-databricks``\n```\n\n----------------------------------------\n\nTITLE: Consolidating Hook Management in AnalyticDBSparkBaseOperator\nDESCRIPTION: Refactors the `AnalyticDBSparkBaseOperator` (likely Alibaba provider) to centralize connection hook management, enhancing maintainability, tracked in issue #34434.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_17\n\nLANGUAGE: text\nCODE:\n```\nConsolidate hook management in AnalyticDBSparkBaseOperator (#34434)\n```\n\n----------------------------------------\n\nTITLE: Configuring Flower URL Prefix in airflow.cfg\nDESCRIPTION: Demonstrates how to set up a URL prefix for Flower in the Airflow configuration file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/flower.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[celery]\nflower_url_prefix = flower\n```\n\n----------------------------------------\n\nTITLE: Installing Apache HDFS Provider Package in Python\nDESCRIPTION: Command to install the apache-airflow-providers-apache-hdfs package using pip. This package provides integration with HDFS and WebHDFS for Apache Airflow 2.9.0 or later.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-hdfs\n```\n\n----------------------------------------\n\nTITLE: Launching K9s CLI Tool\nDESCRIPTION: Command to start the K9s CLI tool for debugging and monitoring Kubernetes clusters through Breeze.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s k9s\n```\n\n----------------------------------------\n\nTITLE: Downloading Airflow Dependencies for Air-Gapped Systems\nDESCRIPTION: This bash script demonstrates how to download Airflow dependencies and constraint files for use in air-gapped systems. It removes the MySQL dependency and prepares the docker-context-files folder with necessary packages.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\n.. exampleinclude:: docker-examples/restricted/restricted_environments.sh\n    :language: bash\n    :start-after: [START download]\n    :end-before: [END download]\n```\n\n----------------------------------------\n\nTITLE: Running Specific Core Test Type\nDESCRIPTION: Example of running only 'Other' type core tests using Breeze\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-tests --test-type \"Other\"\n```\n\n----------------------------------------\n\nTITLE: Specify Airflow Site Directory\nDESCRIPTION: Command to specify custom airflow-site directory path when publishing documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management publish-docs --airflow-site-directory\n```\n\n----------------------------------------\n\nTITLE: Including Sections and Options Reference - Sphinx reStructuredText\nDESCRIPTION: Similar to the previous snippet, this one uses Sphinx's .. include:: directive to embed a detailed breakdown of supported sections and options from a shared reference file into the documentation. The included file provides technical reference information, and this pattern ensures DRY (Don't Repeat Yourself) documentation. It requires Sphinx and the presence of the specified file. Output is expanded at build time, incorporating external content seamlessly.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/configurations-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst\n```\n\n----------------------------------------\n\nTITLE: Installing Yandex Cloud Integration for Apache Airflow\nDESCRIPTION: Command to install Yandex.cloud integration package that provides hooks and operators for Apache Airflow\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_43\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[yandex]'\n```\n\n----------------------------------------\n\nTITLE: Implementing Watcher for Proper Test Cleanup in Python\nDESCRIPTION: Example showing how to implement the watcher() method for system tests that use cleanup tasks with TriggerRule.ALL_DONE to ensure proper success/failure marking.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/tests/system/amazon/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nchain(\n    # TEST SETUP\n    task0,\n    # TEST BODY\n    task1,\n    # TEST TEARDOWN\n    task2, # task2 has trigger rule \"all done\" defined\n)\n\nfrom tests_common.test_utils.watcher import watcher\n\n# This test needs watcher in order to properly mark success/failure\n# when \"tearDown\" task with trigger rule is part of the DAG\nlist(dag.tasks) >> watcher\n```\n\n----------------------------------------\n\nTITLE: Modifying template_fields in DatabricksSqlOperator\nDESCRIPTION: Modifies the 'template_fields' attribute of the 'DatabricksSqlOperator' to support parent class fields.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"Modify 'template_fields' of 'DatabricksSqlOperator' to support parent class fields (#32253)\"\n```\n\n----------------------------------------\n\nTITLE: Adding CloudBatchHook and operators in Python\nDESCRIPTION: Adds CloudBatchHook and related operators to the Google provider for Apache Airflow. This enables working with Google Cloud Batch services.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"Add 'CloudBatchHook' and operators (#32606)\"\n```\n\n----------------------------------------\n\nTITLE: Installing pgvector Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with pgvector integration, enabling pgvector operators and hook.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[pgvector]'\n```\n\n----------------------------------------\n\nTITLE: Distributing Helm Chart Artifacts to ASF Dev SVN Repo in Shell\nDESCRIPTION: Checks out the ASF dev distribution SVN repository, creates a new directory for the release candidate version, moves the generated Helm chart artifacts (.tgz, .tgz.prov, source.tar.gz, and their corresponding .asc/.sha512 files) into it, generates/updates the `index.yaml` file for the Helm repository, and commits the changes to SVN. Requires SVN client and Helm installed.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n# First clone the repo\nsvn checkout https://dist.apache.org/repos/dist/dev/airflow airflow-dev\n\n# Create new folder for the release\ncd airflow-dev/helm-chart\nsvn mkdir ${VERSION}${VERSION_SUFFIX}\n\n# Move the artifacts to svn folder\nmv ${AIRFLOW_REPO_ROOT}/dist/airflow-${VERSION}.tgz* ${VERSION}${VERSION_SUFFIX}/\nmv ${AIRFLOW_REPO_ROOT}/dist/airflow-chart-${VERSION}-source.tar.gz* ${VERSION}${VERSION_SUFFIX}/\ncd ${VERSION}${VERSION_SUFFIX}\n\n###### Generate index.yaml file - Start\n# Download the latest index.yaml on Airflow Website\ncurl https://airflow.apache.org/index.yaml --output index.yaml\n\n# Replace the URLs from \"https://downloads.apache.org\" to \"https://archive.apache.org\"\n# as the downloads.apache.org only contains latest releases.\nsed -i 's|https://downloads.apache.org/airflow/helm-chart/|https://archive.apache.org/dist/airflow/helm-chart/|' index.yaml\n\n# Generate / Merge the new version with existing index.yaml\nhelm repo index --merge ./index.yaml . --url \"https://dist.apache.org/repos/dist/dev/airflow/helm-chart/${VERSION}${VERSION_SUFFIX}\"\n\n###### Generate index.yaml file - End\n\n# Commit the artifacts\nsvn add *\nsvn commit -m \"Add artifacts for Helm Chart ${VERSION}${VERSION_SUFFIX}\"\n```\n\n----------------------------------------\n\nTITLE: Compare Unmerged PRs\nDESCRIPTION: Shell command to view cherry-picking candidates (unmerged PRs with appropriate milestone).\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n./dev/airflow-github compare 2.1.2 --unmerged\n```\n\n----------------------------------------\n\nTITLE: Extra Secrets and ConfigMaps Configuration\nDESCRIPTION: YAML configuration example for adding extra secrets and configmaps without Helm hooks for CI/CD compatibility.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/index.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nextraSecrets:\n  '{{ .Release.Name }}-example':\n    useHelmHooks: false\n    data: |\n      AIRFLOW_VAR_HELLO_MESSAGE: \"Hi!\"\n\nextraConfigMaps:\n  '{{ .Release.Name }}-example':\n    useHelmHooks: false\n    data: |\n      AIRFLOW_VAR_HELLO_MESSAGE: \"Hi!\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Git Commit Information in Markdown\nDESCRIPTION: This code snippet shows how to format Git commit information in a Markdown table, including commit hash, date, and subject. It's used to document changes in the ODBC provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n`83316b8158 <https://github.com/apache/airflow/commit/83316b81584c9e516a8142778fc509f19d95cc3e>`__  2024-03-04   ``Prepare docs 1st wave (RC1) March 2024 (#37876)``\n`5a0be392e6 <https://github.com/apache/airflow/commit/5a0be392e66f8e5426ba3478621115e92fcf245b>`__  2024-02-16   ``Add comment about versions updated by release manager (#37488)``\n`08036e5df5 <https://github.com/apache/airflow/commit/08036e5df5ae3ec9f600219361f86a1a3e8e9d19>`__  2024-02-08   ``D401 Support in Providers (simple) (#37258)``\n==================================================================================================  ===========  ==================================================================\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Static start_date and default_args in Example DAGs\nDESCRIPTION: Excluded Change (Version 2.1.0): Refactors miscellaneous provider example DAGs to remove static `start_date` values and clean up the usage of `default_args`, referencing pull request #18597.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_31\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Static start_date and default arg cleanup for misc. provider example DAGs (#18597)``\n```\n\n----------------------------------------\n\nTITLE: Loading CI Image from PR using Breeze\nDESCRIPTION: Command to load a CI image from a specific pull request using Breeze. This allows developers to reproduce the CI environment locally for debugging purposes.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/07_running_ci_locally.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image load --from-pr 12345 --python 3.9 --github-token <your_github_token>\n```\n\n----------------------------------------\n\nTITLE: Preparing Provider Release After PIP 21 Compatibility\nDESCRIPTION: Excluded Change (Version 2.0.0): Involves preparation tasks for a provider release following updates for compatibility with PIP version 21, referencing pull request #15576.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_39\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Prepares provider release after PIP 21 compatibility (#15576)``\n```\n\n----------------------------------------\n\nTITLE: Installing Twine with UV Tool - Shell Script\nDESCRIPTION: This shell script installs the 'twine' package using the 'uv' tool, which is a modern Python packaging utility. 'twine' is necessary for securely uploading packages to PyPI. Prerequisites include having 'uv' installed on your system. The command has no other required parameters. Expected output is an updated environment with 'twine' available for use.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell script\nCODE:\n```\nuv tool install twine\n```\n\n----------------------------------------\n\nTITLE: Example Inclusion Directive for Scaling Memorystore Instance (Python)\nDESCRIPTION: This directive includes a Python code example from an external file (`example_cloud_memorystore_redis.py`) demonstrating the usage of `CloudMemorystoreScaleInstanceOperator`. The example shows how to scale a Google Cloud Memorystore Redis instance, likely requiring parameters like `project_id`, `location`, `instance_id`, and the target `node_count`. The `dedent`, `start-after`, and `end-before` options control which part of the external file is included.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_12\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/cloud_memorystore/example_cloud_memorystore_redis.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_scale_instance]\n    :end-before: [END howto_operator_scale_instance]\n```\n\n----------------------------------------\n\nTITLE: Prepare Documentation for February 1st Provider Wave (Excluded from v1.4.1 Changelog)\nDESCRIPTION: Indicates preparatory work done on documentation for the February 1st wave of provider releases, referenced by pull request #46893. This change was intentionally excluded from the main changelog notes for version 1.4.1.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs for Feb 1st wave of providers (#46893)\n```\n\n----------------------------------------\n\nTITLE: Extending FTP to S3 Operator for Multiple File Transfers in Python\nDESCRIPTION: Enhancement of the FTP to S3 Operator to support transferring multiple files in a single operation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nEnable FTPToS3Operator to transfer several files (#17937)\n```\n\n----------------------------------------\n\nTITLE: Including Sections and Options Documentation in RST\nDESCRIPTION: This reStructuredText directive includes the content of the specified file (`/../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst`) into the current document during processing by Sphinx. This allows for the reuse of standardized documentation concerning sections and options.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/configurations-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst\n```\n\n----------------------------------------\n\nTITLE: Apply Ruff Automatic Fixes\nDESCRIPTION: Automatically applies recommended fixes for DAG compatibility issues.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading_to_airflow3.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nruff check dag/ --select AIR301 --fix\n```\n\n----------------------------------------\n\nTITLE: Configuring Iceberg Connection in RST\nDESCRIPTION: ReStructuredText documentation showing the configuration parameters for Iceberg connections in Airflow, including client ID, client secret, and host URL settings.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/iceberg/docs/connections.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _howto/connection:iceberg:\\n\\nConnecting to Iceberg\\n=====================\\n\\nThe Iceberg connection type enables connecting to an Iceberg REST catalog to request a short-lived token to access the Apache Iceberg tables. This token can be injected as an environment variable, to be used with Trino, Spark, Flink, or your favorite query engine that supports Apache Iceberg.\\n\\nAfter installing the Iceberg provider in your Airflow environment, the corresponding connection type of ``iceberg`` will be made available.\\n\\nDefault Connection IDs\\n----------------------\\n\\nIceberg Hook uses the parameter ``iceberg_conn_id`` for Connection IDs and the value of the parameter as ``iceberg_default`` by default. You can create multiple connections in case you want to switch between environments.\\n\\nConfiguring the Connection\\n--------------------------\\n\\nClient ID\\n    The OAuth2 Client ID\\n\\nClient Secret\\n    The OAuth2 Client Secret\\n\\nHost\\n    Sets the URL to the Tabular environment. By default `https://api.tabulardata.io/ws/v1`\n```\n\n----------------------------------------\n\nTITLE: Building Airflow CI Image with Custom Constraints\nDESCRIPTION: Command to build Airflow CI image using a custom constraints file. This allows developers to use their own version of dependencies during development.\nSOURCE: https://github.com/apache/airflow/blob/main/constraints/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image build --python 3.9 --airflow-constraints-location constraints/constraints-3.9.txt\n```\n\n----------------------------------------\n\nTITLE: Generating Configuration Documentation with Jinja2 for Apache Airflow\nDESCRIPTION: This Jinja2 template iterates through configuration sections and options to generate detailed documentation for Apache Airflow settings. It includes information such as descriptions, types, default values, environment variables, and examples for each configuration option.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/sphinx_exts/includes/sections-and-options.rst#2025-04-22_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{% for section_name, section in configs.items() %}\n\n.. _config:{{ section_name }}:\n\n[{{ section_name }}]\n{{ \"=\" * (section_name|length + 2) }}\n\n{% if 'renamed' in section %}\n*Renamed in version {{ section['renamed']['version'] }}, previous name was {{ section['renamed']['previous_name'] }}*\n{% endif %}\n\n{% if section[\"description\"] %}\n{{ section[\"description\"] }}\n{% endif %}\n\n{% for option_name, option in section[\"options\"].items() %}\n.. _config:{{ section_name }}__{{ option_name }}:\n\n{{ option_name }}\n{{ \"-\" * option_name|length }}\n\n    {% if option[\"version_added\"] %}\n .. versionadded:: {{ option[\"version_added\"] }}\n    {% endif %}\n\n    {% if option[\"description\"] %}\n{{ option[\"description\"] }}\n    {% endif %}\n\n    {% if option.get(\"version_deprecated\") %}\n.. deprecated:: {{ option[\"version_deprecated\"] }}\n    {{ option[\"deprecation_reason\"] | indent(width=8) }}\n    {% endif %}\n\n    {% if option.get(\"see_also\") %}\n.. seealso:: {{ option[\"see_also\"] }}\n    {% endif %}\n\n:Type: {{ option[\"type\"] }}\n:Default:\n      {% set default = option[\"default\"] %}\n      {% if default and \"\\n\" in default %}\n  .. code-block::\n\n    {{ default }}\n      {% else %}\n    ``{{ \"''\" if default == \"\" else default  }}``\n      {% endif %}\n    {% if option.get(\"sensitive\") %}\n:Environment Variables:\n  ``AIRFLOW__{{ section_name | replace(\".\", \"_\") | upper }}__{{ option_name | upper }}``\n\n  ``AIRFLOW__{{ section_name | replace(\".\", \"_\") | upper }}__{{ option_name | upper }}_CMD``\n\n  ``AIRFLOW__{{ section_name | replace(\".\", \"_\") | upper }}__{{ option_name | upper }}_SECRET``\n    {% else %}\n:Environment Variable: ``AIRFLOW__{{ section_name | replace(\".\", \"_\") | upper }}__{{ option_name | upper }}``\n    {% endif %}\n    {% set example = option[\"example\"] %}\n    {% if example %}\n:Example:\n      {% if \"\\n\" in example %}\n  .. code-block::\n\n    {{ example }}\n      {% else %}\n    ``{{ example }}``\n      {% endif %}\n    {% endif %}\n{% endfor %}\n\n{% if section_name in deprecated_options %}\n\n{% for deprecated_option_name, (new_section_name, new_option_name, since_version) in deprecated_options[section_name].items() %}\n.. _config:{{ section_name }}__{{ deprecated_option_name }}:\n\n{{ deprecated_option_name }} (Deprecated)\n{{ \"-\" * (deprecated_option_name + \" (Deprecated)\")|length }}\n\n.. deprecated:: {{ since_version }}\n The option has been moved to :ref:`{{ new_section_name }}.{{ new_option_name }} <config:{{ new_section_name }}__{{ new_option_name }}>`\n{% endfor %}\n{% endif %}\n\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Simplifying Length Checks in Apache Providers\nDESCRIPTION: Refactors conditional checks involving `len()` within Apache providers, likely replacing explicit length checks with more Pythonic approaches (e.g., truthiness), detailed in issue #33564.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_23\n\nLANGUAGE: text\nCODE:\n```\nSimplify conditions on len() in providers/apache (#33564)\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry 1.0.0\nDESCRIPTION: RST formatted changelog entry noting the initial provider version\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n1.0.0\n.....\n\nInitial version of the provider.\n```\n\n----------------------------------------\n\nTITLE: Retrieving Airflow Database Connection String\nDESCRIPTION: This command retrieves the current value of the database connection string used by Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-up-database.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ airflow config get-value database sql_alchemy_conn\nsqlite:////tmp/airflow/airflow.db\n```\n\n----------------------------------------\n\nTITLE: Setting Docker Cache to Registry via Environment Variable\nDESCRIPTION: Setting an environment variable to use registry-based Docker caching for image builds. This affects all subsequent Breeze commands.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_CACHE=\"registry\"\n```\n\n----------------------------------------\n\nTITLE: TaskInstance Deprecation Replacements in Python\nDESCRIPTION: List of deprecated elements removed from TaskInstance and SimpleTaskInstance classes with their recommended replacements. This includes method arguments, properties, and functions that have been deprecated and their modern alternatives.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41784.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Instead of activate_dag_runs in clear_task_instances()\nTaskInstance.clear_task_instances(dag_run_state=...)\n\n# Instead of execution_date in __init__()\nTaskInstance(__init__(run_id=...))\n\n# Instead of ti._try_number\nti.try_number\n\n# Instead of ti.prev_attempted_tries\nti.try_number\n\n# Instead of ti.next_try_number\nti.try_number + 1\n\n# Instead of ti.previous_ti\nti.get_previous_ti()\n\n# Instead of ti.previous_ti_success\nti.get_previous_ti()\n\n# Instead of ti.previous_start_date_success\nti.get_previous_start_date()\n\n# Instead of SimpleTaskInstance.as_dict()\nBaseSerialization.serialize()\n\n# Instead of SimpleTaskInstance.from_dict()\nBaseSerialization.deserialize()\n```\n\n----------------------------------------\n\nTITLE: Installing Oracle Provider with Common SQL Dependency in Bash\nDESCRIPTION: Command to install the Oracle provider package along with its cross-provider dependency for common SQL functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-oracle[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Waiting for Bigtable Table Replication with BigtableTableReplicationCompletedSensor in Python\nDESCRIPTION: This snippet demonstrates how to use the BigtableTableReplicationCompletedSensor to wait for a table to fully replicate in Google Cloud Bigtable. It shows how to create the sensor with and without specifying the project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigtable.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwait_for_table_replication_task = BigtableTableReplicationCompletedSensor(\n    project_id=GCP_PROJECT_ID,\n    instance_id=INSTANCE_ID,\n    table_id=TABLE_ID,\n    task_id=\"wait_for_table_replication_task\",\n)\n\n# The same sensor can be created without project_id:\nwait_for_table_replication_task_no_project_id = BigtableTableReplicationCompletedSensor(\n    instance_id=INSTANCE_ID,\n    table_id=TABLE_ID,\n    task_id=\"wait_for_table_replication_task\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Papermill Provider with Cross-Provider Dependencies\nDESCRIPTION: Command to install the apache-airflow-providers-papermill package along with its cross-provider dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-papermill[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Importing Retry Object for Google Cloud Vision Operations in Python\nDESCRIPTION: Imports the Retry object from Google API core library for handling retries in Cloud Vision operations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom google.api_core.retry import Retry\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare docs for Sep 1st wave of providers\nDESCRIPTION: This text represents a commit message summary for preparing documentation related to the September 1st wave of Apache Airflow provider releases, associated with pull request #42387.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs for Sep 1st wave of providers (#42387)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Return common data structure in DBApi derived classes\nDESCRIPTION: This commit message (hash 5fe5d31a46, dated 2023-12-22) details a change to ensure that classes derived from DBApi return a common, standardized data structure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\n``Return common data structure in DBApi derived classes``\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Buildx Cache Builders\nDESCRIPTION: Commands to set up local and remote builders for optimized multi-platform builds\nSOURCE: https://github.com/apache/airflow/blob/main/dev/MANUALLY_BUILDING_IMAGES.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker buildx create --name airflow_cache\ndocker buildx create --name airflow_cache --append HOST:PORT\n```\n\n----------------------------------------\n\nTITLE: Inspecting Python sys.path in Interactive Shell\nDESCRIPTION: This code snippet demonstrates how to view the contents of sys.path in a Python interactive terminal. It imports the sys module and uses pprint for formatted output of the sys.path list.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/modules_management.rst#2025-04-22_snippet_0\n\nLANGUAGE: pycon\nCODE:\n```\n>>> import sys\n>>> from pprint import pprint\n>>> pprint(sys.path)\n['',\n '/home/arch/.pyenv/versions/3.9.4/lib/python37.zip',\n '/home/arch/.pyenv/versions/3.9.4/lib/python3.9',\n '/home/arch/.pyenv/versions/3.9.4/lib/python3.9/lib-dynload',\n '/home/arch/venvs/airflow/lib/python3.9/site-packages']\n```\n\n----------------------------------------\n\nTITLE: Installing Celery Provider with CNCF Kubernetes Dependency\nDESCRIPTION: Command to install the Celery provider package along with its CNCF Kubernetes cross-provider dependency using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-celery[cncf.kubernetes]\n```\n\n----------------------------------------\n\nTITLE: Migration File Naming Convention\nDESCRIPTION: Example of standardized migration file naming format after pre-commit hook processing. The format includes migration number and target Airflow version.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/14_metadata_database_updates.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n1234_A_B_C_add_new_field_to_db.py\n```\n\n----------------------------------------\n\nTITLE: Template Field Initialization in Airflow Operator\nDESCRIPTION: Demonstrates the proper way to initialize template fields in an Airflow operator's constructor, showing how to handle default values for mutable fields like lists.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, *, my_field: list[str] = None, **kwargs):\n    super().__init__(**kwargs)\n    my_field = my_field or []\n    self.my_field = my_field\n```\n\n----------------------------------------\n\nTITLE: Enabling Helm Tests Need (Boolean String)\nDESCRIPTION: A boolean flag indicating whether Helm tests are required for the build. 'true' means they are needed.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: MySQL Provider Breaking Change\nDESCRIPTION: Moving local_infile option from connection extra to hook parameter\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nMove local_infile option from extra to hook parameter (#28811)\n```\n\n----------------------------------------\n\nTITLE: Setting Looker Connection URI in Airflow\nDESCRIPTION: Example of a connection URI string for Looker in Airflow. Shows how to format the connection string with client ID, client secret, host URL, port, and additional parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp_looker.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nAIRFLOW_CONN_YOUR_CONN_ID='http://YourClientID:YourClientSecret@https%3A%2F%2Fyour.looker.com:19999?verify_ssl=true&timeout=120'\n```\n\n----------------------------------------\n\nTITLE: Installing Celery Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Celery provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-celery\n```\n\n----------------------------------------\n\nTITLE: Installing the Apache Airflow YDB Provider via Pip (Bash)\nDESCRIPTION: This command installs the core `apache-airflow-providers-ydb` package using pip. It should be run in an environment with an existing Airflow 2 installation (version 2.9.0 or higher).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-ydb\n```\n\n----------------------------------------\n\nTITLE: Documenting Provider Update August 2023 (2nd Wave)\nDESCRIPTION: This commit prepares the documentation for the second wave of Apache Airflow provider updates released in August 2023, referenced by issue #33291.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_25\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs for Aug 2023 2nd wave of Providers (#33291)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add code snippet formatting via Ruff\nDESCRIPTION: This commit message (hash e9ba37bb58, dated 2023-12-17) indicates the implementation of automated code snippet formatting within docstrings using the Ruff linter/formatter tool (issue #36262).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_18\n\nLANGUAGE: text\nCODE:\n```\n``Add code snippet formatting in docstrings via Ruff (#36262)``\n```\n\n----------------------------------------\n\nTITLE: Consolidating Hook Management in HiveOperator in Python\nDESCRIPTION: Refactors the HiveOperator to consolidate hook management, likely improving code organization and maintainability.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n\"Consolidate hook management in HiveOperator (#34430)\"\n```\n\n----------------------------------------\n\nTITLE: Running Single Pytest Test in Breeze Shell\nDESCRIPTION: Example of running a specific test function within the TestCore class using pytest in Breeze shell\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/core/test_core.py::TestCore::test_dag_params_and_task_params\n```\n\n----------------------------------------\n\nTITLE: Installing ArangoDB Integration for Apache Airflow\nDESCRIPTION: Command to install ArangoDB integration package that provides operators, sensors and hooks for Apache Airflow\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_46\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[arangodb]'\n```\n\n----------------------------------------\n\nTITLE: Setting Up Breeze Autocomplete\nDESCRIPTION: Command to configure shell autocomplete functionality for Breeze commands\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/01_installation.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbreeze setup autocomplete\n```\n\n----------------------------------------\n\nTITLE: Preparing Documentation for July Provider Release in RST\nDESCRIPTION: This commit (b916b75079, committed on 2021-07-15) prepares the documentation associated with the July release of Airflow providers. Refers to issue #17015.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_24\n\nLANGUAGE: rst\nCODE:\n```\nPrepare documentation for July release of providers. (#17015)\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Hive Table for Twitter Data\nDESCRIPTION: HQL query to create a partitioned Hive table for storing Twitter data. The table is configured with comma-delimited fields, partitioned by date, and includes header row skipping.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/tests/system/apache/hive/example_twitter_README.md#2025-04-22_snippet_0\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TABLE toTwitter_A(id BIGINT, id_str STRING\n                         created_at STRING, text STRING)\n                         PARTITIONED BY (dt STRING)\n                         ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n                         STORED AS TEXTFILE;\n                         alter table toTwitter_A SET serdeproperties ('skip.header.line.count' = '1');\n```\n\n----------------------------------------\n\nTITLE: Add Comment About Release Manager Version Updates (Excluded from v1.2.0 Changelog)\nDESCRIPTION: Adds a comment clarifying that version numbers are updated by the release manager, referenced by pull request #37488. This change was intentionally excluded from the main changelog notes for version 1.2.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_19\n\nLANGUAGE: plaintext\nCODE:\n```\nAdd comment about versions updated by release manager (#37488)\n```\n\n----------------------------------------\n\nTITLE: Including Provider Configurations Reference in RST\nDESCRIPTION: This reStructuredText directive includes the content of the specified file (`/../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst`) into the current document during processing by Sphinx. It's used to reuse common documentation content related to provider configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst\n```\n\n----------------------------------------\n\nTITLE: Configuring Single Service for Apprise Connection in JSON\nDESCRIPTION: This snippet shows the JSON configuration format for specifying a single service in the Apprise connection. It includes the 'path' for the service URI and a 'tag' for identification.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/docs/connections.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"path\": \"URI for the service\",\n  \"tag\": \"tag name\"\n}\n```\n\n----------------------------------------\n\nTITLE: Including Full Task Information in Airflow OpenLineage Events via INI\nDESCRIPTION: Configures the OpenLineage integration to include all serializable task parameters in the START event's AirflowRunFacet by setting `include_full_task_info` to `true` in the `[openlineage]` section of the Airflow configuration. Be cautious as this can significantly increase event size.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_17\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\ninclude_full_task_info = true\n```\n\n----------------------------------------\n\nTITLE: Removing AirflowContextDeprecationWarning\nDESCRIPTION: Commit message indicating the removal of 'AirflowContextDeprecationWarning' as context handling is expected to be clean for Airflow 3. References pull request #46601.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_31\n\nLANGUAGE: plaintext\nCODE:\n```\nRemove AirflowContextDeprecationWarning as all context should be clean for Airflow 3 (#46601)\n```\n\n----------------------------------------\n\nTITLE: Displaying Grid View Image in ReStructuredText\nDESCRIPTION: ReStructuredText directive to display the Grid View image in dark mode, showing DAG run status matrix with varied task states.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/ui.rst#2025-04-22_snippet_3\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. image:: img/ui-dark/dag_overview_grid.png\n   :alt: Grid View showing DAG run status matrix with varied task states (Dark Mode)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Providers Edge3 with FAB dependency\nDESCRIPTION: Command to install the apache-airflow-providers-edge3 package with the FAB (Flask App Builder) extra dependency using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/edge3/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-edge3[fab]\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Apache Airflow Breeze with uv - Bash\nDESCRIPTION: This snippet uninstalls the 'apache-airflow-breeze' tool using the 'uv tool' command-line utility. 'uv' is a Python package manager; this command removes the breeze tool and also cleans up the associated executable from the local binary directory. Prerequisites: 'uv' must be installed and managing Breeze. No arguments are required. Outputs removal progress and performs a clean uninstall.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/01_installation.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nuv tool uninstall apache-airflow-breeze\n```\n\n----------------------------------------\n\nTITLE: Running Specific Airflow Test Types with Breeze in Bash\nDESCRIPTION: Commands to run specific types of Airflow tests, including Core tests and integration tests.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\nbreeze --backend postgres --postgres-version 15 --python 3.9 --db-reset testing tests --test-type Core\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze --backend postgres --postgres-version 15 --python 3.9 --db-reset testing tests --test-type All --integration mongo\n```\n\n----------------------------------------\n\nTITLE: Replacing airflow webserver with airflow api-server CLI command\nDESCRIPTION: The 'airflow webserver' CLI command has been replaced with 'airflow api-server'. The new Airflow UI is now served as part of the 'airflow api-server' command.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/47083.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairflow api-server\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry v2.0.1\nDESCRIPTION: Changelog entry noting optimization of connection importing for Airflow 2.2.0\nSOURCE: https://github.com/apache/airflow/blob/main/providers/datadog/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n2.0.1\n.....\n\nMisc\n~~~~\n\n* ``Optimise connection importing for Airflow 2.2.0``\n```\n\n----------------------------------------\n\nTITLE: Enabling Mypy Checks (Boolean String)\nDESCRIPTION: A boolean flag indicating whether mypy static type checks are supposed to run in this build. 'true' enables the checks.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 3.0.4\nDESCRIPTION: Changelog entry documenting bug fix for incorrectly added install requirements\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n3.0.4\n.....\n\nBug Fixes\n~~~~~~~~~\n\n* ``Fix mistakenly added install_requires for all providers (#22382)``\n```\n\n----------------------------------------\n\nTITLE: Restoring High Availability Support for Hive Metastore\nDESCRIPTION: This code restores High Availability (HA) support for the Hive metastore in the Hive provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n\"hive provider: restore HA support for metastore (#19777)\"\n```\n\n----------------------------------------\n\nTITLE: Configuration Path Updates for DAG Processing in Airflow\nDESCRIPTION: Relocation of configuration options from core and scheduler sections to the dedicated dag_processor section for better organization of DAG processing related settings.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/aip-66.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[core] dag_file_processor_timeout  [dag_processor] dag_file_processor_timeout\n[scheduler] parsing_processes  [dag_processor] parsing_processes\n[scheduler] file_parsing_sort_mode  [dag_processor] file_parsing_sort_mode\n[scheduler] max_callbacks_per_loop  [dag_processor] max_callbacks_per_loop\n[scheduler] min_file_process_interval  [dag_processor] min_file_process_interval\n[scheduler] stale_dag_threshold  [dag_processor] stale_dag_threshold\n[scheduler] print_stats_interval  [dag_processor] print_stats_interval\n```\n\n----------------------------------------\n\nTITLE: Displaying Dependencies Table in Markdown\nDESCRIPTION: A markdown table structure for tracking package dependencies with columns for package name, version, file location and SHA256 hash values.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/_vendor/vendor.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Package   | Version | File                   | SHA256                                                           |\n|-----------|---------|------------------------|------------------------------------------------------------------|\n```\n\n----------------------------------------\n\nTITLE: Installing Breeze with UV\nDESCRIPTION: Command to install Breeze development environment tool using UV package manager with editable flag\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/01_installation.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv tool install -e ./dev/breeze\n```\n\n----------------------------------------\n\nTITLE: Apache License Header\nDESCRIPTION: Standard Apache 2.0 license header included at the top of the file\nSOURCE: https://github.com/apache/airflow/blob/main/dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Migrating Chain Function Usage in Python\nDESCRIPTION: Replace deprecated airflow.utils.helpers.chain with airflow.sdk.chain method for DAG task chaining\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41520.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old usage (deprecated)\nfrom airflow.utils.helpers import chain\n\n# New usage\nfrom airflow.sdk import chain\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Image with MySQL Client\nDESCRIPTION: Builds a production Airflow image with the MySQL client instead of the default MariaDB client. This applies to both X86_64 and ARM64 platforms.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . --pull --build-arg INSTALL_MYSQL_CLIENT=\"true\" -t my-image:my-tag\n```\n\n----------------------------------------\n\nTITLE: Missing Template Field Assignment\nDESCRIPTION: Example showing invalid implementation where a template field is not assigned in the constructor.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass HelloOperator(BaseOperator):\n    template_fields = (\"foo\", \"bar\")\n\n    def __init__(self, foo, bar) -> None:\n        self.bar = bar\n```\n\n----------------------------------------\n\nTITLE: Implementing Limited Parallelism in Local Executor\nDESCRIPTION: Strategy for spawning a fixed number of worker processes based on parallelism setting. Uses task queue for work distribution and QueuedLocalWorker class for processing.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/executor/local.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nself.parallelism > 0\n```\n\n----------------------------------------\n\nTITLE: Setting System Test Environment ID in Bash\nDESCRIPTION: Sets the SYSTEM_TESTS_ENV_ID environment variable to avoid conflicts between different test runs.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/system_tests.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nSYSTEM_TESTS_ENV_ID=SOME_RANDOM_VALUE\n```\n\n----------------------------------------\n\nTITLE: Running Selected Complete Tests with Breeze - Bash\nDESCRIPTION: This bash command executes a complete run, focusing on a specific test file (`test_kubernetes_executor.py`) using Breeze. It follows the same full-lifecycle approach (setup, deploy, test, cleanup), but limits test scope for more efficient targeted runs.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s run-complete-tests test_kubernetes_executor.py\n```\n\n----------------------------------------\n\nTITLE: BaseHook Connection Fields Method Signature Fix\nDESCRIPTION: Bug fix that ensures child classes follow the BaseHook connection fields method signature. Maintains consistency across hook implementations and prevents potential issues.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/changelog.rst#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nFollow BaseHook connection fields method signature in child classes (#36086)\n```\n\n----------------------------------------\n\nTITLE: Manual UI Development Setup Commands\nDESCRIPTION: Commands for manually installing dependencies and starting the development server using pnpm package manager.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/ui/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npnpm install && pnpm dev\n```\n\n----------------------------------------\n\nTITLE: Git Commit References in Markdown Format\nDESCRIPTION: A list of Git commit references with links, dates and descriptions showing changes made to the Apache Airflow codebase. Includes commits for feature additions, bug fixes, and dependency updates.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n`51584b8c37 <https://github.com/apache/airflow/commit/51584b8c371263bf34725afedd3c2f1b35468a8e>`__  2024-12-18   ``Introduce gcp translation(V3), translate document providers (#44971)``\n`16022e008c <https://github.com/apache/airflow/commit/16022e008cc2eef122c5cd254cdb52d2c1b7e026>`__  2024-12-16   ``Add Google Vertex AI Feature Store - Feature View Sync Operators, Sensor (#44891)``\n`4b38bed76c <https://github.com/apache/airflow/commit/4b38bed76c1ea5fe84a6bc678ce87e20d563adc0>`__  2024-12-16   ``Bump min version of Providers to 2.9 (#44956)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs 1st wave Providers (Nov 2023)\nDESCRIPTION: This commit message (hash 1b059c57d6, dated 2023-11-08) marks the preparation of documentation for the first wave of provider releases in November 2023 (issue #35537).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_27\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs 1st wave of Providers November 2023 (#35537)``\n```\n\n----------------------------------------\n\nTITLE: Building Docs for Multiple Provider Packages with Breeze\nDESCRIPTION: This command builds documentation for multiple provider packages by specifying their PACKAGE_IDs in the Breeze development environment.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/docs/README.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs PACKAGE1_ID PACKAGE2_ID\n```\n\n----------------------------------------\n\nTITLE: OpenFaaS Provider Package Requirements Table\nDESCRIPTION: Table showing the required dependencies for the OpenFaaS provider package, specifying the minimum Apache Airflow version needed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openfaas/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n\"apache-airflow\"  \">=2.9.0\"\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Incorrect Implementation of Custom Timetable\nDESCRIPTION: This snippet shows an incorrect way of implementing a custom timetable in Airflow, where Airflow Variables are accessed at the top level of the timetable code.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import Variable\nfrom airflow.timetables.interval import CronDataIntervalTimetable\n\n\nclass CustomTimetable(CronDataIntervalTimetable):\n    def __init__(self, *args, something=Variable.get(\"something\"), **kwargs):\n        self._something = something\n        super().__init__(*args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Enabling Task SDK Tests (Boolean String)\nDESCRIPTION: A boolean flag indicating whether tests related to the Task SDK should be run. 'true' enables these tests.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_22\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Reference Image Import\nDESCRIPTION: Import statements for ReferenceImage and Retry objects\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_vision_reference_image_import]\\n[END howto_operator_vision_reference_image_import]\n```\n\n----------------------------------------\n\nTITLE: Method Level DB Test Marking\nDESCRIPTION: Example of marking an individual test method as a database test using pytest.mark.db_test decorator\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n@pytest.mark.db_test\ndef test_add_tagging(self, sentry, task_instance): ...\n```\n\n----------------------------------------\n\nTITLE: Disabling Deployment Exposure Warning (INI)\nDESCRIPTION: This configuration snippet disables the warning displayed in the Airflow UI when recent requests to the `/robots.txt` endpoint are detected. Set `warn_deployment_exposure` to `False` within the `[webserver]` section of the Airflow configuration file to suppress this warning.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/security.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[webserver]\nwarn_deployment_exposure = False\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes in Airflow Core Settings\nDESCRIPTION: Required configuration changes from Dataset to Asset in core Airflow settings\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41348.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ncore.dataset_manager_class  core.asset_manager_class\ncore.dataset_manager_kwargs  core.asset_manager_kwargs\n```\n\n----------------------------------------\n\nTITLE: Displaying Airbyte Provider Version 3.4.0 Changelog\nDESCRIPTION: Lists the changes made in version 3.4.0 of the Airbyte provider, including commit hashes, dates, and descriptions of changes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_30\n\nLANGUAGE: markdown\nCODE:\n```\n3.4.0\n.....\n\nLatest change: 2023-10-13\n\n==================================================================================================  ===========  ===============================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ===============================================================\n`e9987d5059 <https://github.com/apache/airflow/commit/e9987d50598f70d84cbb2a5d964e21020e81c080>`__  2023-10-13   ``Prepare docs 1st wave of Providers in October 2023 (#34916)``\n`0c8e30e43b <https://github.com/apache/airflow/commit/0c8e30e43b70e9d033e1686b327eb00aab82479c>`__  2023-10-05   ``Bump min airflow version of providers (#34728)``\n==================================================================================================  ===========  ===============================================================\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Documentation in RST\nDESCRIPTION: RST include directive that references external documentation for installing Airflow providers from source code\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/livy/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Documenting Version 2.0.4 Changes in RST\nDESCRIPTION: A reStructuredText section detailing the changes in version 2.0.4 of the DingTalk provider. It includes a bug fix related to installation requirements.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n2.0.4\n.....\n\nBug Fixes\n~~~~~~~~~\n\n* ``Fix mistakenly added install_requires for all providers (#22382)``\n```\n\n----------------------------------------\n\nTITLE: Adding Interactivity to Provider Documentation Generation\nDESCRIPTION: Excluded Change (Version 2.0.0): Enhances the provider documentation generation process by adding interactivity, referencing pull request #15518.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_38\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Adds interactivity when generating provider documentation. (#15518)``\n```\n\n----------------------------------------\n\nTITLE: Using PythonOperator with Compact OpenLineage Parent ID\nDESCRIPTION: Example of using the PythonOperator with a compact parent ID format that combines namespace, job name, and run ID into a single string. The example shows how to pass this information and parse it in the Python callable function.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/macros.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef my_task_function(templates_dict, **kwargs):\n    parent_job_namespace, parent_job_name, parent_run_id = templates_dict[\"parentRun\"].split(\"/\")\n    ...\n\n\nPythonOperator(\n    task_id=\"render_template\",\n    python_callable=my_task_function,\n    templates_dict={\n        # joined components as one string `<namespace>/<name>/<run_id>`\n        \"parentRun\": \"{{ macros.OpenLineageProviderPlugin.lineage_parent_id(task_instance) }}\",\n    },\n    provide_context=False,\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Help for Breeze Complete Tests - Bash\nDESCRIPTION: This bash command shows all help and usage options for the `breeze k8s run-complete-tests` command by passing `--help` to the underlying shell. This facilitates discovery of available CLI flags and parameters for advanced usage.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s run-complete-tests -- --help\n```\n\n----------------------------------------\n\nTITLE: Formatting Basic Authentication HTTP Header (Text)\nDESCRIPTION: Illustrates the required format for the `Authorization` HTTP header when using Basic Authentication. It involves prepending 'Basic ' to the Base64 encoded string of 'username:password'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/api-authentication.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nAuthorization: Basic Base64(username:password)\n```\n\n----------------------------------------\n\nTITLE: Bumping Minimum Airflow Version for Providers (October 2023)\nDESCRIPTION: Increases the minimum required Airflow version for certain providers, as detailed in issue #34728. This typically aligns providers with core Airflow features or fixes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_13\n\nLANGUAGE: text\nCODE:\n```\nBump min airflow version of providers (#34728)\n```\n\n----------------------------------------\n\nTITLE: Update JDBC Operator and Hook\nDESCRIPTION: Move handler parameter from JdbcOperator to JdbcHook.run and replace usage of DummyOperator with EmptyOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"Handler parameter from 'JdbcOperator' to 'JdbcHook.run' (#23817)\"\n\"Replace usage of 'DummyOperator' with 'EmptyOperator' (#22974)\"\n```\n\n----------------------------------------\n\nTITLE: Building Multiple Distribution Documentation\nDESCRIPTION: Example of building documentation for multiple provider packages simultaneously to resolve cross-references.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/11_documentation_building.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd providers/amazon\nuv run --group docs build-docs amazon google\n```\n\n----------------------------------------\n\nTITLE: Migrating Apache Airflow Operator and Sensor Import Paths\nDESCRIPTION: This code snippet demonstrates the migration of import paths for various Apache Airflow operators and sensors. It shows the old import paths and their corresponding new paths, which typically involve moving components to provider-specific packages.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41368.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Old import path  New import path\n\"airflow.operators.hive_stats_operator.HiveStatsCollectionOperator\"  \"airflow.providers.apache.hive.operators.hive_stats.HiveStatsCollectionOperator\"\n\"airflow.operators.hive_to_druid.HiveToDruidOperator\"  \"airflow.providers.apache.druid.transfers.hive_to_druid.HiveToDruidOperator\"\n\"airflow.operators.hive_to_mysql.HiveToMySqlOperator\"  \"airflow.providers.apache.hive.transfers.hive_to_mysql.HiveToMySqlOperator\"\n\"airflow.operators.jdbc_operator.JdbcOperator\"  \"airflow.providers.jdbc.operators.jdbc.JdbcOperator\"\n\"airflow.operators.mssql_operator.MsSqlOperator\"  \"airflow.providers.microsoft.mssql.operators.mssql.MsSqlOperator\"\n\"airflow.operators.mysql_operator.MySqlOperator\"  \"airflow.providers.mysql.operators.mysql.MySqlOperator\"\n\"airflow.operators.oracle_operator.OracleOperator\"  \"airflow.providers.oracle.operators.oracle.OracleOperator\"\n\"airflow.operators.papermill_operator.PapermillOperator\"  \"airflow.providers.papermill.operators.papermill.PapermillOperator\"\n\"airflow.operators.pig_operator.PigOperator\"  \"airflow.providers.apache.pig.operators.pig.PigOperator\"\n\"airflow.operators.postgres_operator.PostgresOperator\"  \"airflow.providers.postgres.operators.postgres.PostgresOperator\"\n\"airflow.operators.python_operator.PythonOperator\"  \"airflow.operators.python.PythonOperator\"\n\"airflow.operators.s3_file_transform_operator.S3FileTransformOperator\"  \"airflow.providers.amazon.aws.operators.s3_file_transform.S3FileTransformOperator\"\n\"airflow.operators.slack_operator.SlackAPIOperator\"  \"airflow.providers.slack.operators.slack.SlackAPIOperator\"\n\"airflow.sensors.hive_partition_sensor.HivePartitionSensor\"  \"airflow.providers.apache.hive.sensors.hive_partition.HivePartitionSensor\"\n\"airflow.sensors.http_sensor.HttpSensor\"  \"airflow.providers.http.sensors.http.HttpSensor\"\n\"airflow.sensors.s3_key_sensor.S3KeySensor\"  \"airflow.providers.amazon.aws.sensors.s3.S3KeySensor\"\n\"airflow.sensors.web_hdfs_sensor.WebHdfsSensor\"  \"airflow.providers.apache.hdfs.sensors.web_hdfs.WebHdfsSensor\"\n```\n\n----------------------------------------\n\nTITLE: Categorizing Changes in Airflow DAG Model\nDESCRIPTION: This snippet outlines the types of changes made in this commit. It uses a checklist format to indicate that the changes are specifically related to DAG modifications, with no impact on other areas of the Airflow system.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41440.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n* Types of change\n\n  * [x] Dag changes\n  * [ ] Config changes\n  * [ ] API changes\n  * [ ] CLI changes\n  * [ ] Behaviour changes\n  * [ ] Plugin changes\n  * [ ] Dependency changes\n  * [ ] Code interface changes\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Livy Provider with Common Compatibility Dependencies\nDESCRIPTION: Command to install the Apache Livy provider package with cross-provider dependencies. This ensures all features including common compatibility components are available.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/livy/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-livy[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Using OracleToGCSOperator for Data Transfer\nDESCRIPTION: Example demonstrating how to use OracleToGCSOperator to transfer data from Oracle database to Google Cloud Storage. The operator supports optional gzip compression of the uploaded data.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/oracle_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nOracle To Google Cloud Storage Operator\n=======================================\nThe `Google Cloud Storage <https://cloud.google.com/storage/>`__ (GCS) service is\nused to store large data from various applications. This page shows how to copy\ndata from Oracle to GCS.\n\nPrerequisite Tasks\n^^^^^^^^^^^^^^^^^^\n\n.. include:: /operators/_partials/prerequisite_tasks.rst\n\n.. _howto/operator:OracleToGCSOperator:\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for RC Preparation\nDESCRIPTION: This script sets necessary environment variables for preparing the Apache Airflow package release candidate, including version information and repository root.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nexport GPG_TTY=$(tty)\n\nexport VERSION=2.1.2rc3\nexport VERSION_SUFFIX=rc3\nexport VERSION_BRANCH=2-1\nexport VERSION_WITHOUT_RC=${VERSION/rc?/}\n\nexport AIRFLOW_REPO_ROOT=$(pwd)\n```\n\n----------------------------------------\n\nTITLE: Building CI Image with Local Docker Cache\nDESCRIPTION: Command for building a CI image using local Docker cache instead of registry cache. This approach takes longer initially but is faster for subsequent builds with small changes.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image build --python 3.9 --docker-cache local\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare docs 1st wave (RC1) March 2024\nDESCRIPTION: This text is a commit message summary for preparing documentation related to the first wave (Release Candidate 1) of Apache Airflow provider releases in March 2024, linked to pull request #37876.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_19\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 1st wave (RC1) March 2024 (#37876)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Provide logger_name param in providers hooks\nDESCRIPTION: This commit message (hash 6bd450da1e, dated 2024-01-10) describes the addition of a `logger_name` parameter to provider hooks, allowing users to override the default logger name (issue #36675). Note: This change was later reverted (see commit 0b680c9492).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n``Provide the logger_name param in providers hooks in order to override the logger name (#36675)``\n```\n\n----------------------------------------\n\nTITLE: Building CI Image with Dependencies\nDESCRIPTION: Commands for rebuilding the CI image after updating dependencies, including an option to upgrade to newer dependencies when conflicts occur\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/10_advanced_breeze_topics.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image build\n\nbreeze ci-image build --upgrade-to-newer-dependencies\n```\n\n----------------------------------------\n\nTITLE: Setting Teradata QueryBand via URI (Bash) in Airflow Environment\nDESCRIPTION: This Bash snippet demonstrates setting the Teradata QueryBand as part of the connection URI specified in an environment variable. The 'query_band' parameter is added to the URI's query string, with its value URL-encoded (e.g., '=' becomes '%3D', ';' becomes '%3B'). This allows passing session-specific information to Teradata when configuring connections via environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/connections/teradata.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_TERADATA_DEFAULT='teradata://teradata_user:XXXXXXXXXXXX@1.1.1.1:/teradatadb?query_band=appname%3Dairflow%3Borg%3Dtest%3B'\n```\n\n----------------------------------------\n\nTITLE: Template Fields for CloudVisionImageAnnotateOperator\nDESCRIPTION: Lists the template fields available for the CloudVisionImageAnnotateOperator for dynamic field resolution at runtime.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"request\",\n    \"gcp_conn_id\",\n    \"impersonation_chain\",\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Commits for Version 3.3.0 (rst)\nDESCRIPTION: This block lists commits associated with version 3.3.0 using reStructuredText table format. It includes commit hashes linked to GitHub, commit dates, and subject lines detailing changes like documentation preparation for the October provider release, addition of SalesforceApexRestOperator, addition of a provider download page, and typo fixes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  =================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =================================================================\n`d9567eb106 <https://github.com/apache/airflow/commit/d9567eb106929b21329c01171fd398fbef2dc6c6>`__  2021-10-29   ``Prepare documentation for October Provider's release (#19321)``\n`e9a72a4e95 <https://github.com/apache/airflow/commit/e9a72a4e95e6d23bae010ad92499cd7b06d50037>`__  2021-10-08   ``Add SalesforceApexRestOperator (#18819)``\n`1cb456cba1 <https://github.com/apache/airflow/commit/1cb456cba1099198bbdba50c2d1ad79664be8ce6>`__  2021-09-12   ``Add official download page for providers (#18187)``\n`046f02e5a7 <https://github.com/apache/airflow/commit/046f02e5a7097a6e6c928c28196b38b37e776916>`__  2021-09-09   ``fix misspelling (#18121)``\n==================================================================================================  ===========  =================================================================\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for Amazon Transfer Operators in reStructuredText\nDESCRIPTION: This snippet sets up a table of contents (toctree) for Amazon Transfer Operators documentation. It configures the maximum depth to 1 and uses a glob pattern to include all relevant files.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Upgrading Cherry Picker Tool\nDESCRIPTION: Command to upgrade the cherry-picker tool to latest version using UV package manager.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_AIRFLOW3_DEV.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv tool upgrade cherry-picker\n```\n\n----------------------------------------\n\nTITLE: Setting Azure Batch Connection Environment Variable in Bash\nDESCRIPTION: This code snippet demonstrates how to set the Azure Batch connection as an environment variable using URI syntax in Bash. The connection string includes the batch account name, batch key, and account URL.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/connections/azure_batch.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_AZURE_BATCH_DEFAULT='azure-batch://batch%20acount:batch%20key@?account_url=mybatchaccount.com'\n```\n\n----------------------------------------\n\nTITLE: Running Shell without Mounting Sources\nDESCRIPTION: Command to enter Breeze container without mounting local sources to reproduce CI environment\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/06_managing_docker_images.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze shell --mount-sources skip [OTHER OPTIONS]\n```\n\n----------------------------------------\n\nTITLE: Configuring Speech-to-Text API Arguments\nDESCRIPTION: Example showing how to configure the config and audio arguments for the speech recognition API call.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/speech_to_text.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_text_to_speech_api_arguments]\n[END howto_operator_text_to_speech_api_arguments]\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameter Removal - Airflow Config\nDESCRIPTION: Migration requirement for removing the navbar_logo_text_color parameter from webserver configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/49161.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nwebserver.navbar_logo_text_color\n```\n\n----------------------------------------\n\nTITLE: Internal API Job Runner Sequence Flow in Apache Airflow\nDESCRIPTION: Sequence diagram demonstrating the interaction between CLI component, JobRunner, Internal API, and Database for remote job execution. Shows the complete workflow including session management, job creation, execution, and completion phases.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/jobs/JOB_LIFECYCLE.md#2025-04-22_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant CLI component\n    participant JobRunner\n    participant Internal API\n    participant DB\n\n    activate CLI component\n\n\n    CLI component->>Internal API: Create Job\n    Internal API-->>DB: Create Session\n    activate DB\n    Internal API ->> DB: Create Job\n    DB ->> Internal API: Job object\n    DB --> Internal API: Close Session\n    deactivate DB\n\n    Internal API->>CLI component: Job object\n\n    CLI component->>JobRunner: Create Job Runner\n    JobRunner ->> CLI component: JobRunner object\n\n    CLI component->>JobRunner: Run Job\n\n    activate JobRunner\n\n    JobRunner->>Internal API: prepare_for_execution [Job]\n\n    Internal API-->>DB: Create Session\n    activate DB\n    Internal API ->> DB: prepare_for_execution [Job]\n    DB->>Internal API: prepared\n    DB-->>Internal API: Close Session\n    deactivate DB\n    Internal API->>JobRunner: prepared\n\n    par\n        JobRunner->>JobRunner: execute_job\n    and\n        JobRunner ->> Internal API: access DB (Variables/Connections etc.)\n        Internal API-->>DB: Create Session\n        activate DB\n        Internal API ->> DB: access DB (Variables/Connections etc.)\n        DB ->> Internal API: returned data\n        DB-->>Internal API: Close Session\n        deactivate DB\n        Internal API ->> JobRunner: returned data\n    and\n        JobRunner->>Internal API: perform_heartbeat <br> [Job]\n        Internal API-->>DB: Create Session\n        activate DB\n        Internal API->>DB: perform_heartbeat [Job]\n        Internal API ->> Internal API: Heartbeat Callback [Job]\n        DB ->> Internal API: heartbeat response\n        Internal API ->> JobRunner: heartbeat response\n        DB-->>Internal API: Close Session\n        deactivate DB\n    end\n\n    JobRunner->>Internal API: complete_execution  <br> [Job]\n    Internal API-->>DB: Create Session\n    Internal API->>DB: complete_execution [Job]\n    activate DB\n    DB ->> Internal API: completed\n    DB-->>Internal API: Close Session\n    deactivate DB\n    Internal API->>JobRunner: completed\n    JobRunner ->> CLI component: completed\n\n    deactivate JobRunner\n\n    deactivate CLI component\n```\n\n----------------------------------------\n\nTITLE: Installing Datadog Provider Package\nDESCRIPTION: Command to install the Datadog provider package for Apache Airflow via pip. Requires Airflow version 2.9.0 or higher.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/datadog/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-datadog\n```\n\n----------------------------------------\n\nTITLE: Download DV360 Report - Python Example\nDESCRIPTION: Example showing how to download a report to GCS bucket using GoogleDisplayVideo360DownloadReportV2Operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/display_video.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[START howto_google_display_video_get_report_operator]\n[END howto_google_display_video_get_report_operator]\n```\n\n----------------------------------------\n\nTITLE: Removing Spurious Headers in Provider Changelogs\nDESCRIPTION: Cleans up generated provider changelog files by removing incorrect or unnecessary headers, improving formatting and clarity, referenced by issue #32373.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_29\n\nLANGUAGE: text\nCODE:\n```\nRemove spurious headers for provider changelogs (#32373)\n```\n\n----------------------------------------\n\nTITLE: Speed Up Breeze Autocompletion (Excluded from v1.2.0 Changelog)\nDESCRIPTION: Improves the performance of autocompletion in the Breeze development environment by simplifying provider state management, referenced by pull request #36499. This change was intentionally excluded from the main changelog notes for version 1.2.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_27\n\nLANGUAGE: plaintext\nCODE:\n```\nSpeed up autocompletion of Breeze by simplifying provider state (#36499)\n```\n\n----------------------------------------\n\nTITLE: Fixing Python 3.9 Support in Hive\nDESCRIPTION: This code fixes compatibility issues with Python 3.9 in the Hive provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n\"Fix Python 3.9 support in Hive (#21893)\"\n```\n\n----------------------------------------\n\nTITLE: Running Specific Test File in Parallel\nDESCRIPTION: Command to run tests from a specific file with parallel execution\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/helm_unit_tests.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npytest helm-tests/tests/other/test_airflow_common.py -n auto\n```\n\n----------------------------------------\n\nTITLE: Jinja Template for Provider Documentation\nDESCRIPTION: Template that generates documentation for each Airflow provider package, including package details, installation instructions, available versions, and references.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/packages-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{% for package in providers %}\n\n.. provider:: {{ package[\"package-name\"] }}\n\n``{{ package[\"package-name\"] }}``\n{{ \"=\" * (package[\"package-name\"]|length + 4) }}\n\n{% if package[\"description\"] %}\n{{ package[\"description\"] }}\n{% endif %}\n\n{% if package[\"versions\"] %}\nTo install, run:\n\n.. code-block:: bash\n\n    pip install '{{ package[\"package-name\"] }}'\n\n:Available versions: {% for version in package[\"versions\"] %}``{{ version }}``{% if not loop.last %}, {% else %}.{% endif %}{%- endfor %}\n\n:Reference: `PyPI Repository <https://pypi.org/project/{{ package[\"package-name\"] }}/>`__\n{% if package[\"python-module\"] %}\n:Python API Reference: :mod:`{{ package[\"python-module\"] }}`\n{% endif %}\n{% else %}\n\n.. warning::\n\n  This package has not yet been released.\n\n{% if package[\"python-module\"] %}\n:Python API Reference: :mod:`{{ package[\"python-module\"] }}`\n{% endif %}\n{% endif %}\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Deprecating get_hook in OSSKeySensor\nDESCRIPTION: Deprecates the `get_hook` method in the `OSSKeySensor` (Alibaba Cloud OSS) in favor of using the `hook` property directly, promoting a more consistent pattern, as per issue #34426.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_18\n\nLANGUAGE: text\nCODE:\n```\nDeprecate get_hook in OSSKeySensor and use hook instead (#34426)\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment Block\nDESCRIPTION: Standard Apache 2.0 license header comment block defining the terms under which the code is licensed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/src/airflow/providers/common/sql/doc/adr/0003-introduce-notion-of-dialects-in-dbapihook.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkKubernetesOperator Job Template in JSON (Partial)\nDESCRIPTION: A sample JSON template file (`spark_job_template.json`), providing an alternative format to YAML for configuring a Spark application run via Airflow's `SparkKubernetesOperator`. It defines Spark-specific settings like API version, kind, metadata, and spec. This is a partial example showing the beginning of the configuration structure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n.. code-block:: json\n\n    {\n      \"spark\": {\n        \"apiVersion\": \"sparkoperator.k8s.io/v1beta2\",\n        \"version\": \"v1beta2\",\n        \"kind\": \"SparkApplication\",\n        \"apiGroup\": \"sparkoperator.k8s.io\",\n        \"metadata\": {\n          \"namespace\": \"ds\"\n        },\n        \"spec\": {\n          \"type\": \"Python\",\n          \"pythonVersion\": \"3\",\n          \"mode\": \"cluster\",\n          \"sparkVersion\": \"3.0.0\",\n          \"successfulRunHistoryLimit\": 1,\n          \"restartPolicy\": {\n            \"type\": \"Never\"\n          },\n          \"imagePullPolicy\": \"Always\",\n          \"hadoopConf\": {},\n          \"imagePullSecrets\": [],\n          \"dynamicAllocation\": {\n            \"enabled\": false,\n            \"initialExecutors\": 1,\n            \"minExecutors\": 1,\n            \"maxExecutors\": 1\n          },\n          \"labels\": {},\n          \"driver\": {\n            \"serviceAccount\": \"default\",\n            \"container_resources\": {\n              \"gpu\": {\n                \"name\": null,\n                \"quantity\": 0\n              },\n```\n\n----------------------------------------\n\nTITLE: Formatting Commit Information in Markdown\nDESCRIPTION: This snippet shows how commit information is formatted in the changelog using Markdown. It includes the commit hash as a link, the commit date, and a brief description of the change.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n`99534e47f3 <https://github.com/apache/airflow/commit/99534e47f330ce0efb96402629dda5b2a4f16e8f>`__  2023-11-19   ``Use reproducible builds for provider packages (#35693)``\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow with All Extras Using UV\nDESCRIPTION: Command to install Apache Airflow with all available extras using the UV package installer, replacing the previously available production and development bundles.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/47441.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install --all-extras\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Remove 'hook-class-names' from provider.yaml (#24702)\nDESCRIPTION: This commit message, associated with version 3.1.0, removes the 'hook-class-names' field from the provider.yaml configuration files. Commit hash: 510a6bab45, Date: 2022-06-28.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_28\n\nLANGUAGE: text\nCODE:\n```\n``Remove 'hook-class-names' from provider.yaml (#24702)``\n```\n\n----------------------------------------\n\nTITLE: Implementing S3 List Prefixes Operator in Python\nDESCRIPTION: Addition of a new operator to list prefixes in an S3 bucket, expanding the S3 interaction capabilities.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nAdds an s3 list prefixes operator (#17145)\n```\n\n----------------------------------------\n\nTITLE: Including External Content for Installing Providers from Sources in reStructuredText\nDESCRIPTION: This snippet uses the reStructuredText 'include' directive to incorporate external content from a file that provides instructions on installing Apache Airflow provider packages from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/docker/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Resolving Migration Conflicts Commands\nDESCRIPTION: Commands to resolve migration conflicts during rebasing by updating migration references and ER diagram using pre-commit hooks.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/14_metadata_database_updates.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run update-migration-references --all\npre-commit run update-er-diagram --all\n```\n\n----------------------------------------\n\nTITLE: Deprecated Code Reference in Airflow\nDESCRIPTION: Reference to the removed trigger rule showing its full import path. This code pattern needs to be removed from any DAGs or configurations where it is being used.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43349.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nairflow.utils.trigger_rule.TriggerRule.DUMMY\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry v1.0.1\nDESCRIPTION: Changelog entry noting documentation updates\nSOURCE: https://github.com/apache/airflow/blob/main/providers/datadog/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n1.0.1\n.....\n\nUpdated documentation and readme files.\n```\n\n----------------------------------------\n\nTITLE: Apache Airflow GitHub Provider Package Name Definition\nDESCRIPTION: RST declaration of the GitHub provider package name\nSOURCE: https://github.com/apache/airflow/blob/main/providers/github/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n``apache-airflow-providers-github``\n```\n\n----------------------------------------\n\nTITLE: Fixing Grammar in Airflow PIP Warning Message\nDESCRIPTION: This commit message details a correction made to the grammar within a PIP warning message displayed by Airflow. It references pull request #13380.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_46\n\nLANGUAGE: text\nCODE:\n```\nFix Grammar in PIP warning (#13380)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Update docs for September Provider's release (#26731)\nDESCRIPTION: This commit message, associated with version 3.1.0, details the updating of documentation for the September release of Apache Airflow Providers. Commit hash: f8db64c35c, Date: 2022-09-28.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_23\n\nLANGUAGE: text\nCODE:\n```\n``Update docs for September Provider's release (#26731)``\n```\n\n----------------------------------------\n\nTITLE: Updating Ruff Linting Rule for Auth Backend Migration in Apache Airflow\nDESCRIPTION: Specifies the migration rule for the ruff linter to update references to the removed default auth backend. The rule AIR303 now suggests replacing 'airflow.api.auth.backend.default' with 'airflow.providers.fab.auth_manager.api.auth.backend.session'.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43096.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* ruff\n\n  * AIR303\n\n    * [x] ``airflow.api.auth.backend.default``  ``airflow.providers.fab.auth_manager.api.auth.backend.session``\n```\n\n----------------------------------------\n\nTITLE: Setting OpenLineage Namespace Using Environment Variable\nDESCRIPTION: Example of setting the OpenLineage namespace using an environment variable as an alternative to airflow.cfg configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_8\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__NAMESPACE='my-team-airflow-instance'\n```\n\n----------------------------------------\n\nTITLE: Specifying Self-Hosted Runner Labels (JSON Array)\nDESCRIPTION: Defines the list of labels assigned to select self-hosted runners for CI jobs. This example specifies 'self-hosted', 'Linux', and 'X64'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_28\n\nLANGUAGE: json\nCODE:\n```\n\\[\"self-hosted\", \"Linux\", \"X64\"\\]\n```\n\n----------------------------------------\n\nTITLE: Generating and Signing Helm Chart Binary Package using Breeze in Shell\nDESCRIPTION: Uses the 'breeze' development tool to prepare the binary Helm chart package. It also signs the package using the GPG key associated with the provided email address. Requires the `helm gpg` plugin.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nbreeze release-management prepare-helm-chart-package --sign-email jedcunningham@apache.org\n```\n\n----------------------------------------\n\nTITLE: Verifying Apache Airflow Package Signatures\nDESCRIPTION: These commands verify the PGP signatures of downloaded Apache Airflow packages. Users can choose between gpg, pgpv, or pgp commands based on their system configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-sources.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngpg --verify apache-airflow-********.asc apache-airflow-*********\n```\n\nLANGUAGE: bash\nCODE:\n```\npgpv apache-airflow-********.asc\n```\n\nLANGUAGE: bash\nCODE:\n```\npgp apache-airflow-********.asc\n```\n\n----------------------------------------\n\nTITLE: Adding Development Dependencies for Providers\nDESCRIPTION: Example of adding additional development dependencies in the pyproject.toml file for a provider. This ensures necessary packages are installed when running UV sync.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_41\n\nLANGUAGE: toml\nCODE:\n```\n[dependency-groups]\ndev = [\n    \"apache-airflow\",\n    \"apache-airflow-task-sdk\",\n    \"apache-airflow-devel-common\",\n    \"apache-airflow-providers-common-sql\",\n    \"apache-airflow-providers-fab\",\n    # Additional devel dependencies (do not remove this line and add extra development dependencies)\n    \"deltalake>=0.12.0\",\n    \"apache-airflow-providers-microsoft-azure\",\n]\n```\n\n----------------------------------------\n\nTITLE: Installing Provider Distributions with Breeze (Bash)\nDESCRIPTION: Executes the `breeze release-management install-provider-distributions` command to install provider packages located in the `dist` folder. This is used to test the installability of the generated providers with the current Airflow environment without full verification.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management install-provider-distributions\n```\n\n----------------------------------------\n\nTITLE: List Documentation Changes\nDESCRIPTION: Shell commands to identify documentation changes that haven't been merged into the release branch.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngit fetch apache\ngit log --oneline apache/v2-2-test | sed -n 's/.*\\((#[0-9]*)\\)$/\\1/p' > /tmp/merged\ngit log --oneline --decorate apache/v2-2-stable..apache/main -- docs/apache-airflow docs/docker-stack/ | grep -vf /tmp/merged\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow UI Development Server with Breeze\nDESCRIPTION: Command to start Airflow in development mode using the Breeze development environment tool.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/ui/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze start-airflow --dev-mode\n```\n\n----------------------------------------\n\nTITLE: Updating Inlined Dockerfile Scripts with Pre-commit in Bash\nDESCRIPTION: This command runs the pre-commit hook to update inlined Dockerfile scripts after modifying dependencies in the install scripts. It ensures that the changes are properly reflected in the Dockerfile.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/10_advanced_breeze_topics.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run update-inlined-dockerfile-scripts --all-files\n```\n\n----------------------------------------\n\nTITLE: Including Shared Security Documentation - Sphinx Directives - reStructuredText\nDESCRIPTION: This snippet uses the Sphinx-specific reStructuredText include directive to dynamically insert content from a shared security documentation file (security.rst) located in the provided relative path. It is typically processed by the Sphinx documentation engine, requiring Sphinx to be configured with the appropriate source directory structure. This helps maintain consistency and DRY principles in project documentation by centralizing commonly reused sections. Input: reST documentation files; Output: expanded documentation with included content.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Including Common Security Documentation with Sphinx Include Directive - reStructuredText\nDESCRIPTION: This reStructuredText (RST) code snippet demonstrates usage of the '.. include::' directive to incorporate the contents of a shared security documentation file (security.rst) into the current document. This relies on the Sphinx documentation tool, and requires the referenced file to exist relative to the documentation structure. This approach ensures that security notices and compliance requirements can be centrally maintained and easily reused. Dependencies include Sphinx and a compatible documentation pipeline.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Deleting the KinD Cluster\nDESCRIPTION: Command to delete the KinD cluster when testing is complete, cleaning up resources.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s delete-cluster\n```\n\n----------------------------------------\n\nTITLE: Removing Webserver Configuration Options in Apache Airflow\nDESCRIPTION: List of configuration options being removed from the [webserver] section of Airflow's configuration. These options include timeouts, worker settings, proxy configurations, analytics settings, and various UI-related parameters that are no longer used.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/48066.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[webserver] web_server_master_timeout\n[webserver] worker_refresh_batch_size\n[webserver] worker_refresh_interval\n[webserver] reload_on_plugin_change\n[webserver] worker_class\n[webserver] expose_stacktrace\n[webserver] log_fetch_delay_sec\n[webserver] log_auto_tailing_offset\n[webserver] log_animation_speed\n[webserver] default_dag_run_display_number\n[webserver] enable_proxy_fix\n[webserver] proxy_fix_x_for\n[webserver] proxy_fix_x_proto\n[webserver] proxy_fix_x_host\n[webserver] proxy_fix_x_port\n[webserver] proxy_fix_x_prefix\n[webserver] cookie_secure\n[webserver] analytics_tool\n[webserver] analytics_id\n[webserver] analytics_url\n[webserver] show_recent_stats_for_completed_runs\n[webserver] run_internal_api\n[webserver] caching_hash_method\n[webserver] show_trigger_form_if_no_params\n[webserver] num_recent_configurations_for_trigger\n[webserver] allowed_payload_size\n[webserver] max_form_memory_size\n[webserver] max_form_parts\n```\n\n----------------------------------------\n\nTITLE: Installing Cherry Picker Tool with UV\nDESCRIPTION: Commands to install and upgrade the cherry-picker tool using UV package manager for handling PR backports.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_AIRFLOW3_DEV.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv tool install cherry-picker\n```\n\n----------------------------------------\n\nTITLE: Updating Azure Batch Hook in Python\nDESCRIPTION: Adds managed identity support to the Azure Batch hook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfeat(provider/azure): add managed identity support to batch hook (#35327)\n```\n\n----------------------------------------\n\nTITLE: Building Kubernetes Airflow Image with Breeze\nDESCRIPTION: This command builds a Docker image suitable for running Airflow on Kubernetes. It uses a pre-existing production Airflow image as a base and adds Kubernetes-specific configurations, such as example DAGs and pod templates. The build process can optionally use `uv` for faster installation.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s build-k8s-image\n```\n\n----------------------------------------\n\nTITLE: Commit Hash Links and Change Descriptions\nDESCRIPTION: Structured list of commit hashes with links and descriptions of changes made to the FAB provider\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n`28f94f8891 <https://github.com/apache/airflow/commit/28f94f8891ccf0827bb6e9a1538f2ffd98a4ea08>`__  2024-02-10   ``Move 'IMPORT_ERROR' from DAG related permissions to view related permissions (#37292)``\n`00ed46769e <https://github.com/apache/airflow/commit/00ed46769eaea24251fc4726a46df1f54f27c4bd>`__  2024-02-09   ``D401 support in fab provider (#37283)``\n`e99cfbbd51 <https://github.com/apache/airflow/commit/e99cfbbd51515fa947c16912acebbaa7ed816e8a>`__  2024-02-07   ``Upgrade to FAB 4.3.11 (#37233)``\n```\n\n----------------------------------------\n\nTITLE: Introducing BaseEventTrigger (AIP-82)\nDESCRIPTION: Commit message describing the introduction of 'BaseEventTrigger' as a base class for triggers used with event-driven scheduling, as part of AIP-82. References pull request #46391.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_28\n\nLANGUAGE: plaintext\nCODE:\n```\nAIP-82 Introduce 'BaseEventTrigger' as base class for triggers used with event driven scheduling (#46391)\n```\n\n----------------------------------------\n\nTITLE: Displaying Git Commit Information in Markdown\nDESCRIPTION: This code snippet shows how commit information is formatted in the changelog using Markdown syntax. It includes the commit hash, date, and description for each change.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`ee87fa0cba <https://github.com/apache/airflow/commit/ee87fa0cba4d83084b4bc617d63d117101d9e069>`__  2024-09-20   ``Minor fixups to FAB DB command docs (#42377)``\n```\n\n----------------------------------------\n\nTITLE: Publishing Mixed Package Documentation\nDESCRIPTION: Command to publish documentation for multiple packages including core, helm-chart and providers.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management publish-docs --airflow-site-directory DIRECTORY apache.airflow apache.beam google\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Local Mounts Command\nDESCRIPTION: Command to synchronize mounted volumes between docker command configuration and local.yml file.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/07_breeze_maintenance_tasks.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsynchronize-local-mounts\n```\n\n----------------------------------------\n\nTITLE: Generating Body Content for Airflow Client Release Announcement (Shell Script)\nDESCRIPTION: Uses the `cat` command with a heredoc (EOF) to generate the body content for the release announcement email. This template includes information about the PyPI release, links to the PyPI project page, documentation (GitHub repository), the changelog, and placeholders for the `${VERSION}` variable and the sender's name (`<your name>`).\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_30\n\nLANGUAGE: shell script\nCODE:\n```\ncat <<EOF\nDear Airflow community,\n\nI'm happy to announce that Apache Airflow Python Client ${VERSION} was just released.\n\nWe made this version available on PyPI for convenience:\n\\`pip install apache-airflow-client\\`\nhttps://pypi.org/project/apache-airflow-client/${VERSION}/\n\nThe documentation is available at:\nhttps://github.com/apache/airflow-client-python/\n\nFind the changelog here for more details:\nhttps://github.com/apache/airflow-client-python/blob/main/CHANGELOG.md\n\nThanks,\n<your name>\nEOF\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add Connection Documentation to more Providers\nDESCRIPTION: Linked to commit 7a0d412245, this message signifies an effort to add documentation related to connection configurations for a broader range of Airflow providers, as tracked in issue #15408.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_24\n\nLANGUAGE: text\nCODE:\n```\n``Add Connection Documentation to more Providers (#15408)``\n```\n\n----------------------------------------\n\nTITLE: Commit Reference Format\nDESCRIPTION: Example of commit reference formatting in the changelog showing commit hash, link, date and subject\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n`dcdcf3a2b8 <https://github.com/apache/airflow/commit/dcdcf3a2b8054fa727efb4cd79d38d2c9c7e1bd5>`__  2022-06-09   ``Update release notes for RC2 release of Providers for May 2022 (#24307)``\n```\n\n----------------------------------------\n\nTITLE: Executing Bash Commands in Airflow Docker Container\nDESCRIPTION: Demonstrates how to run bash commands within the Airflow Docker container using the default entrypoint.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it apache/airflow:3.1.0.dev0-python3.9 bash -c \"ls -la\"\n```\n\n----------------------------------------\n\nTITLE: SparkSqlOperator Configuration\nDESCRIPTION: Example showing the conf property change from string to dictionary type in SparkSqlOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"conf\": { # Changed from str to dict type\n    \"key\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding DatabricksTaskOperator\nDESCRIPTION: New feature introducing the DatabricksTaskOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"Add DatabricksTaskOperator (#40013)\"\n```\n\n----------------------------------------\n\nTITLE: Regenerating Documentation SVG Screenshots Command\nDESCRIPTION: Command used to regenerate SVG screenshots of help commands and parameters. Can be run with --force to regenerate all images or --check-only to verify which images need regeneration.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/07_breeze_maintenance_tasks.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nregenerate-command-images [--force] [--check-only]\n```\n\n----------------------------------------\n\nTITLE: Fixing Runtime Context Start Date Key\nDESCRIPTION: Commit message describing a fix to ensure the runtime context does not incorrectly contain 'start_date' as a key. References pull request #46961.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_21\n\nLANGUAGE: plaintext\nCODE:\n```\nRuntime context shouldn't have start_date as a key (#46961)\n```\n\n----------------------------------------\n\nTITLE: Generating Release Announcement Body\nDESCRIPTION: Command to generate detailed email body for release announcement.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\nDear Airflow community,\n\nI am pleased to announce that we have released Apache Airflow Helm chart ${VERSION}  \n\nThe source release, as well as the \"binary\" Helm Chart release, are available:\n\n   Official Sources: https://airflow.apache.org/docs/helm-chart/${VERSION}/installing-helm-chart-from-sources.html\n   ArtifactHub: https://artifacthub.io/packages/helm/apache-airflow/airflow\n   Docs: https://airflow.apache.org/docs/helm-chart/${VERSION}/\n   Quick Start Installation Guide: https://airflow.apache.org/docs/helm-chart/${VERSION}/quick-start.html\n   Release Notes: https://airflow.apache.org/docs/helm-chart/${VERSION}/release_notes.html\n\nThanks to all the contributors who made this possible.\n\nCheers,\n<your name>\nEOF\n```\n\n----------------------------------------\n\nTITLE: Generating Subject Line for Airflow Client Release Announcement (Shell Script)\nDESCRIPTION: Uses the `cat` command with a heredoc (EOF) to generate the subject line for the release announcement email sent to the Apache Airflow mailing lists. The `${VERSION}` variable, containing the released client version number, must be set in the shell environment.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_29\n\nLANGUAGE: shell script\nCODE:\n```\ncat <<EOF\n[ANNOUNCE] Apache Airflow Python Client ${VERSION} Released\nEOF\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Split providers out of the main airflow/ tree\nDESCRIPTION: This text is a commit message summary detailing the refactoring effort to split providers out of the main 'airflow/' directory into a UV workspace project, linked to pull request #42505.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nSplit providers out of the main \"airflow/\" tree into a UV workspace project (#42505)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Correctly capitalize names and abbreviations in docs\nDESCRIPTION: This commit message, associated with commit 43de625d42, describes a documentation update where names and abbreviations were corrected for proper capitalization. This change is tracked under issue #19908.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n``Correctly capitalize names and abbreviations in docs (#19908)``\n```\n\n----------------------------------------\n\nTITLE: Adding Airflow to PATH\nDESCRIPTION: This command adds the ~/.local/bin directory to the PATH environment variable, which can resolve issues with the 'airflow' command not being recognized, especially on Windows when using WSL.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-pypi.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nPATH=$PATH:~/.local/bin\n```\n\n----------------------------------------\n\nTITLE: Add Sequence Insert Support Reference\nDESCRIPTION: Reference to a feature addition that provides sequence insert support to the OracleHook class.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n* ``Add sequence insert support to OracleHook (#42947)``\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Image Name for Manual Docker Compose Deployment\nDESCRIPTION: Command to set the AIRFLOW_IMAGE_NAME environment variable for running Docker Compose deployment manually with a custom-built production image.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/docker_compose_tests.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_IMAGE_NAME=ghcr.io/apache/airflow/main/prod/python3.9:latest\n```\n\n----------------------------------------\n\nTITLE: Including Licensing Notice and External Documentation - Sphinx - reStructuredText\nDESCRIPTION: Establishes licensing terms for the documentation using Apache License 2.0. Outlines copyright ownership and references additional information via a NOTICE file. The file also uses the Sphinx '.. include::' directive to incorporate external installation documentation, ensuring reuse and consistency across docs. There are no code dependencies, but it expects a Sphinx documentation build environment and access to the referenced files.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/weaviate/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Neo4j Provider Package Requirements Table\nDESCRIPTION: A table showing the required dependencies and their versions for the Neo4j provider package\nSOURCE: https://github.com/apache/airflow/blob/main/providers/neo4j/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n\"apache-airflow\"  \">=2.9.0\"\n\"neo4j\"           \">=4.2.1\"\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Setting Google Service Account Key Path in Airflow Configuration\nDESCRIPTION: This configuration specifies the path to a Google service account key file. If omitted, Airflow will use Application Default Credentials for authorization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/api-auth-backend/google-openid.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[cli]\ngoogle_key_path = <KEY_PATH>\n```\n\n----------------------------------------\n\nTITLE: Initializing Redshift Hook for Async Operations in Python\nDESCRIPTION: Creates a Redshift hook to access asynchronous connection methods for interacting with AWS Redshift.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/triggers/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nself.redshift_hook = RedshiftHook(aws_conn_id=self.aws_conn_id)\n```\n\n----------------------------------------\n\nTITLE: Updated Airflow Scheduler Configuration Parameters\nDESCRIPTION: Configuration parameters in scheduler section that have been changed to False by default, affecting how data intervals are calculated for cron and time delta schedules.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/47070.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nscheduler.create_cron_data_intervals = False\nscheduler.create_delta_data_intervals = False\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Standard Provider in Python\nDESCRIPTION: Command to install the apache-airflow-providers-standard package using pip. This package is designed to work with Airflow 2 installations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-standard\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Remove Empty Lines in Generated Changelog\nDESCRIPTION: This commit message, associated with commit 706878ec35 on 2023-11-04, indicates the removal of empty lines from the automatically generated changelog.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nRemove empty lines in generated changelog (#35436)\n```\n\n----------------------------------------\n\nTITLE: Improving Airflow Documentation Installation Process\nDESCRIPTION: This commit message details improvements made to the process of installing or setting up the Airflow documentation. It references pull request #12304.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_59\n\nLANGUAGE: text\nCODE:\n```\nDocs installation improvements (#12304)\n```\n\n----------------------------------------\n\nTITLE: Building CI Docker Image with JDBC Extra and Default JRE Headless - Docker/Bash\nDESCRIPTION: This command sets up an Apache Airflow CI Docker image including the 'jdbc' extra and presumed addition of 'default-jre-headless' as an apt dependency (though only AIRFLOW_EXTRAS=jdbc is shown). This configuration is suitable for JDBC integration testing. Requires Docker, Airflow sources, and Dockerfile.ci. Outputs a custom-tagged image.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build . -f Dockerfile.ci \\\n  --pull \\\n  --build-arg PYTHON_BASE_IMAGE=\"python:3.9-slim-bookworm\" \\\n  --build-arg AIRFLOW_EXTRAS=jdbc \\\n  --tag my-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Interactive Shell Testing\nDESCRIPTION: Commands for running tests interactively in a Breeze shell environment\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nbreeze shell --backend postgres --python 3.9\n> pytest tests --run-db-tests-only\n```\n\n----------------------------------------\n\nTITLE: Migrating Airflow Decorators Import in Python\nDESCRIPTION: Example showing how to update imports for Airflow decorators (dag, task, task_group, setup, teardown) which have been moved from airflow.decorators to airflow.sdk.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/aip-72.significant.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.decorators import dag, task, task_group, setup, teardown\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import dag, task, task_group, setup, teardown\n```\n\n----------------------------------------\n\nTITLE: Comparing Built Packages with SVN Packages in Bash\nDESCRIPTION: This snippet shows how to compare locally built Airflow Client packages with those in the SVN repository to ensure they are identical. It's part of the reproducible package check.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncd ..\n[ -d asf-dist ] || svn checkout --depth=immediates https://dist.apache.org/repos/dist asf-dist\nsvn update --set-depth=infinity asf-dist/dev/airflow/clients/python\n\ncd asf-dist/dev/airflow/clients/python/${VERSION}\nfor i in ${AIRFLOW_REPO_ROOT}/dist/*\ndo\n  echo \"Checking if $(basename $i) is the same as $i\"\n  diff \"$(basename $i)\" \"$i\" && echo \"OK\"\ndone\n```\n\n----------------------------------------\n\nTITLE: Creating Inspection Template in Google Cloud DLP\nDESCRIPTION: Example of creating an inspection template using CloudDLPCreateInspectTemplateOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/data_loss_prevention.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_dlp_create_inspect_template]\n[END howto_operator_dlp_create_inspect_template]\n```\n\n----------------------------------------\n\nTITLE: Enabling Notification Channels with StackdriverEnableNotificationChannelsOperator in Python\nDESCRIPTION: This snippet demonstrates the usage of StackdriverEnableNotificationChannelsOperator to enable Notification Channels identified by a given filter. The operator can be used with or without a project ID, which will be retrieved from the Google Cloud connection if not provided.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/stackdriver.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nStackdriverEnableNotificationChannelsOperator(\n    task_id='enable_notification_channels',\n    filter_='',\n    project_id=None,\n    gcp_conn_id='google_cloud_default'\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Jupyter Kernel Connection Using Python in Airflow\nDESCRIPTION: This Python code snippet shows how to create a Jupyter Kernel connection in Airflow using the Connection class. It sets up the connection with a host and extra parameters, then generates the corresponding environment variable name and URI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/connections/jupyter_kernel.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models.connection import Connection\n\nconn = Connection(\n    conn_id=\"jupyter_kernel_default\",\n    conn_type=\"jupyter_kernel\",\n    host=\"remote_host\",\n    extra={\n        # Specify extra parameters here\n        \"session_key\": \"notebooks\",\n    },\n)\n\n# Generate Environment Variable Name\nenv_key = f\"AIRFLOW_CONN_{conn.conn_id.upper()}\"\n\nprint(f\"{env_key}='{conn.get_uri()}'\")\n```\n\n----------------------------------------\n\nTITLE: Setting Snowflake Connection with JSON in Bash Environment Variable\nDESCRIPTION: This Bash command exports the Snowflake connection for Airflow using a JSON-serialized dictionary. This method explicitly lists all connection parameters, including login, password, schema, and extra configuration such as account and warehouse. The JSON should be inserted as a single-quoted string to preserve formatting when setting environment variables. Ensure all sensitive information is handled securely and that the environment parses the variable correctly.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/connections/snowflake.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_SNOWFLAKE_DEFAULT='{\n    \"conn_type\": \"snowflake\",\n    \"login\": \"user\",\n    \"password\": \"password\",\n    \"schema\": \"db-schema\",\n    \"extra\": {\n        \"account\": \"account\",\n        \"database\": \"database\",\n        \"region\": \"us-east\",\n        \"warehouse\": \"snow-warehouse\"\n    }\n}'\n```\n\n----------------------------------------\n\nTITLE: Setting Airflow Database Connection via Environment Variable in Bash\nDESCRIPTION: This command demonstrates how to set the metadata database connection string using an environment variable in Bash.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/set-config.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=my_conn_string\n```\n\n----------------------------------------\n\nTITLE: Setting SendGrid Email Backend Environment Variables (Shell)\nDESCRIPTION: This snippet demonstrates how to set SendGrid email backend configuration options using environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nAIRFLOW__EMAIL__EMAIL_BACKEND=airflow.providers.sendgrid.utils.emailer.send_email\nAIRFLOW__EMAIL__EMAIL_CONN_ID=sendgrid_default\nSENDGRID_MAIL_FROM=hello@thelearning.dev\n```\n\n----------------------------------------\n\nTITLE: Defining Operators Table in reStructuredText\nDESCRIPTION: Creates a table listing various Airflow operators, including branch, empty, generic_transfer, latest_only, and trigger_dagrun operators. The table has two columns: Operators and Guides.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/operators-and-hooks-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n**Operators:**\n\n.. list-table::\n   :header-rows: 1\n\n   * - Operators\n     - Guides\n\n   * - :mod:`airflow.operators.branch`\n     -\n\n   * - :mod:`airflow.providers.standard.operators.empty`\n     -\n\n   * - :mod:`airflow.operators.generic_transfer`\n     -\n\n   * - :mod:`airflow.providers.standard.operators.latest_only`\n     -\n\n   * - :mod:`airflow.providers.standard.operators.trigger_dagrun`\n     -\n```\n\n----------------------------------------\n\nTITLE: Airflow Development Environment Comparison Table\nDESCRIPTION: A comparison matrix showing the key differences between local virtualenv, Breeze environment, and remote environments for Airflow development, highlighting aspects like test coverage, setup complexity, and resource usage.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/06_development_environments.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n+--------------------------+----------------------------------+---------------------------------------+----------------------------------------+\n| **Property**             | **Local virtualenv**             | **Breeze environment**                | **Remote environments**                |\n+==========================+==================================+=======================================+========================================+\n| Dev machine needed       | - (-) You need a dev PC          | - (-) You need a dev PC               | (+) Works with remote setup            |\n+--------------------------+----------------------------------+---------------------------------------+----------------------------------------+\n| Test coverage            | - (-) unit tests only            | - (+) integration and unit tests      | (*/-) integration tests (extra config) |\n+--------------------------+----------------------------------+---------------------------------------+----------------------------------------+\n| Setup                    | - (+) automated with breeze cmd  | - (+) automated with breeze cmd       | (+) automated with CodeSpaces/GitPod   |\n+--------------------------+----------------------------------+---------------------------------------+----------------------------------------+\n| Installation difficulty  | - (-) depends on the OS setup    | - (+) works whenever Docker works     | (+) works in a modern browser/VSCode   |\n+--------------------------+----------------------------------+---------------------------------------+----------------------------------------+\n| Team synchronization     | - (-) difficult to achieve       | - (+) reproducible within team        | (+) reproducible within team           |\n+--------------------------+----------------------------------+---------------------------------------+----------------------------------------+\n| Reproducing CI failures  | - (-) not possible in many cases | - (+) fully reproducible              | (+) reproduce CI failures              |\n+--------------------------+----------------------------------+---------------------------------------+----------------------------------------+\n| Ability to update        | - (-) requires manual updates    | - (+) automated update via breeze cmd | (+/-) can be rebuild on demand         |\n+--------------------------+----------------------------------+---------------------------------------+----------------------------------------+\n| Disk space and CPU usage | - (+) relatively lightweight     | - (-) uses GBs of disk and many CPUs  | (-) integration tests (extra config)   |\n+--------------------------+----------------------------------+---------------------------------------+----------------------------------------+\n| IDE integration          | - (+) straightforward            | - (-) via remote debugging only       | (-) integration tests (extra config)   |\n+--------------------------+----------------------------------+---------------------------------------+----------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Changelog Table\nDESCRIPTION: RST formatted changelog table showing commit history, dates and descriptions for changes to the Apache Drill provider\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ================================================================================\n`f5b96315fe <https://github.com/apache/airflow/commit/f5b96315fe65b99c0e2542831ff73a3406c4232d>`__  2022-03-07   ``Add documentation for Feb Providers release (#22056)``\n`d94fa37830 <https://github.com/apache/airflow/commit/d94fa378305957358b910cfb1fe7cb14bc793804>`__  2022-02-08   ``Fixed changelog for January 2022 (delayed) provider's release (#21439)``\n```\n\n----------------------------------------\n\nTITLE: Importing Hooks in Apache Airflow - Before Change\nDESCRIPTION: Shows the deprecated method of importing hooks through Airflow's plugin mechanism using the airflow.hooks namespace.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43291.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.hooks.my_plugin import MyHook\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Spark Parent Job Info Injection via Environment Variable\nDESCRIPTION: Sets the `AIRFLOW__OPENLINEAGE__SPARK_INJECT_PARENT_JOB_INFO` environment variable to `true`. This enables the automatic injection of Airflow parent job details (namespace, job name, run ID) into Spark job properties for supported operators, helping establish parent-child relationships in lineage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_30\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__SPARK_INJECT_PARENT_JOB_INFO=true\n```\n\n----------------------------------------\n\nTITLE: Setting Vote Email Timing Variables in Shell\nDESCRIPTION: Exports two environment variables: `VOTE_END_TIME` calculates the date and time 72 hours and 10 minutes from now in UTC format (`YYYY-MM-DD HH:MM`), and `TIME_DATE_URL` generates a corresponding URL parameter for a timeanddate.com countdown link. Requires the `date` command.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nexport VOTE_END_TIME=$(date --utc -d \"now + 72 hours + 10 minutes\" +'%Y-%m-%d %H:%M')\nexport TIME_DATE_URL=\"to?iso=$(date --utc -d \"now + 72 hours + 10 minutes\" +'%Y%m%dT%H%M')&p0=136&font=cursive\"\n```\n\n----------------------------------------\n\nTITLE: AWS DynamoDB Value Sensor Enhancement\nDESCRIPTION: Support for list attributes in DynamoDB Value Sensor attribute_value parameter\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nattribute_value = [\"value1\", \"value2\"]\n```\n\n----------------------------------------\n\nTITLE: Previewing Documentation\nDESCRIPTION: Command to start documentation preview server.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\n./docs/start_doc_server.sh\n```\n\n----------------------------------------\n\nTITLE: Release Notes File Mapping Configuration\nDESCRIPTION: Maps changelog.rst and updating.rst files to release_notes.rst in the documentation system. This configuration appears to specify how different documentation files should be consolidated into the main release notes.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/redirects.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nchangelog.rst release_notes.rst\nupdating.rst release_notes.rst\n```\n\n----------------------------------------\n\nTITLE: Displaying DAG Structure\nDESCRIPTION: Command to print a DAG structure to terminal in DOT format\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nairflow dags show example_complex\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fixing circular import error in providers caused by airflow version check (#31379)\nDESCRIPTION: This commit message, associated with version 3.2.0, addresses a circular import error within providers that was triggered by the Airflow version check mechanism. Commit hash: f5aed58d9f, Date: 2023-05-18.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n``Fixing circular import error in providers caused by airflow version check (#31379)``\n```\n\n----------------------------------------\n\nTITLE: Reapply Templates for All Providers (Excluded from v1.2.1 Changelog)\nDESCRIPTION: Notes the reapplication of templates across all providers, referenced by pull request #39554. This change was intentionally excluded from the main changelog notes for version 1.2.1.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_17\n\nLANGUAGE: plaintext\nCODE:\n```\nReapply templates for all providers (#39554)\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 3.0.1\nDESCRIPTION: Changelog entry documenting bug fixes related to Samba path handling and binary file operations\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: restructuredtext\nCODE:\n```\n3.0.1\n.....\n\nBug Fixes\n~~~~~~~~~\n\n* ``Handle leading slash in samba path (#18847)``\n* ``Open src and dst in binary for samba copy (#18752)``\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum Airflow Version to 2.3.0 for All Providers\nDESCRIPTION: Miscellaneous Change (Version 3.1.0): Updates the minimum required Airflow version to 2.3.0 across all providers as part of a general policy update, referencing pull request #27196.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Move min airflow version to 2.3.0 for all providers (#27196)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Enable String Normalization in Formatting\nDESCRIPTION: This commit message, associated with commit 2a34dc9e84 dated 2022-10-23, describes the enabling of string normalization rules within the Python code formatting tools used for provider code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_25\n\nLANGUAGE: text\nCODE:\n```\n``Enable string normalization in python formatting - providers (#27205)``\n```\n\n----------------------------------------\n\nTITLE: Referencing DagRun Configuration in RST\nDESCRIPTION: Shows the RST syntax for referencing the DagRun.conf property that is affected by the migration change. The code uses inline literal notation to highlight technical terms.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/44533.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n``DagRun.conf``\n```\n\nLANGUAGE: rst\nCODE:\n```\n``conf``\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Docker Image Pre-2.0.2\nDESCRIPTION: Dockerfile commands for Airflow versions prior to 2.0.2, including additional steps for OpenShift compatibility by setting proper file ownership.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker build --pull --tag \"my-company/airflow:8a0da78\" . -f - <<EOF\nFROM apache/airflow:2.0.2\n\nUSER root\n\nCOPY --chown=airflow:root ./dags/ \\${AIRFLOW_HOME}/dags/\n\nUSER airflow\n\nEOF\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Documentation in RST\nDESCRIPTION: Sphinx directive to include external RST file containing instructions for installing providers from source code\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Displaying Airflow Home Page Image in ReStructuredText\nDESCRIPTION: ReStructuredText directive to display the Airflow Home Page image in dark mode, showing system health, DAG/task stats, and asset events.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/ui.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. image:: img/ui-dark/home_dark.png\n   :alt: Airflow Home Page showing system health, DAG/task stats, and asset events (Dark Mode)\n```\n\n----------------------------------------\n\nTITLE: Running Core System Tests via Breeze\nDESCRIPTION: Command to execute Airflow core system tests using the Breeze development environment.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/system_tests.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing system-tests airflow-core/tests/system/example_empty.py\n```\n\n----------------------------------------\n\nTITLE: Fixing Deprecation Warning for BranchMixIn\nDESCRIPTION: Commit message indicating a fix for a deprecation warning related to the 'BranchMixIn' class. References pull request #47856.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nFix deprecation warning for 'BranchMixIn' (#47856)\n```\n\n----------------------------------------\n\nTITLE: Setting up Airflow Repository for Client Generation\nDESCRIPTION: Initial setup steps to clone and configure the Apache Airflow repository for client generation, including setting environment variables and checking out the correct branch.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# If you have not done so yet\ngit clone git@github.com/apache/airflow\ncd airflow\n# Checkout the right branch\ngit checkout v2-8-test\nexport AIRFLOW_REPO_ROOT=$(pwd -P)\nexport TEST_BRANCH=v2-8-test\nexport STABLE_BRANCH=v2-8-stable\ncd ..\n```\n\n----------------------------------------\n\nTITLE: Setting Partition Filter Limit in HiveMetastoreHook\nDESCRIPTION: This code sets a larger limit for get_partitions_by_filter in HiveMetastoreHook to improve performance when dealing with large numbers of partitions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n\"Set larger limit get_partitions_by_filter in HiveMetastoreHook (#21504)\"\n```\n\n----------------------------------------\n\nTITLE: Downgrading Dependencies for Provider Tests\nDESCRIPTION: Commands to enter Breeze shell, navigate to a specific provider directory, and use UV sync to downgrade dependencies for provider testing.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_39\n\nLANGUAGE: bash\nCODE:\n```\nbreeze shell\ncd providers/PROVIDER_ID\nuv sync --resolution lowest-direct\n```\n\n----------------------------------------\n\nTITLE: Valid Template Field Inheritance\nDESCRIPTION: Example showing correct template field implementation with inheritance.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass HelloOperator(BaseOperator):\n    template_fields = \"foo\"\n\n    def __init__(self, foo) -> None:\n        self.foo = foo\n\n\nclass MyHelloOperator(HelloOperator):\n    template_fields = \"foo\"\n```\n\n----------------------------------------\n\nTITLE: Publishing Release to SVN\nDESCRIPTION: Commands to initiate the release process and publish to SVN.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\nexport RC=2.0.2rc5\nexport VERSION=${RC/rc?/}\nexport AIRFLOW_REPO_ROOT=$(pwd)\nbreeze release-management start-release --release-candidate ${RC} --previous-release <PREVIOUS RELEASE>\n```\n\n----------------------------------------\n\nTITLE: Setting OpenLineage Config Path Using Environment Variable\nDESCRIPTION: Example of setting the path to the OpenLineage configuration YAML file using an environment variable instead of airflow.cfg.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__CONFIG_PATH='/path/to/openlineage.yml'\n```\n\n----------------------------------------\n\nTITLE: Listing Commits for Version 3.4.1 (rst)\nDESCRIPTION: This block lists commits associated with version 3.4.1 using reStructuredText table format. It includes commit hashes linked to GitHub, commit dates, and subject lines detailing changes like documentation additions, changelog fixes, type hint support updates, and MyPy fixes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ==========================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==========================================================================\n`f5b96315fe <https://github.com/apache/airflow/commit/f5b96315fe65b99c0e2542831ff73a3406c4232d>`__  2022-03-07   ``Add documentation for Feb Providers release (#22056)``\n`d94fa37830 <https://github.com/apache/airflow/commit/d94fa378305957358b910cfb1fe7cb14bc793804>`__  2022-02-08   ``Fixed changelog for January 2022 (delayed) provider's release (#21439)``\n`6c3a67d4fc <https://github.com/apache/airflow/commit/6c3a67d4fccafe4ab6cd9ec8c7bacf2677f17038>`__  2022-02-05   ``Add documentation for January 2021 providers release (#21257)``\n`cb73053211 <https://github.com/apache/airflow/commit/cb73053211367e2c2dd76d5279cdc7dc7b190124>`__  2022-01-27   ``Add optional features in providers. (#21074)``\n`602abe8394 <https://github.com/apache/airflow/commit/602abe8394fafe7de54df7e73af56de848cdf617>`__  2022-01-20   ``Remove ':type' lines now sphinx-autoapi supports typehints (#20951)``\n`f77417eb0d <https://github.com/apache/airflow/commit/f77417eb0d3f12e4849d80645325c02a48829278>`__  2021-12-31   ``Fix K8S changelog to be PyPI-compatible (#20614)``\n`97496ba2b4 <https://github.com/apache/airflow/commit/97496ba2b41063fa24393c58c5c648a0cdb5a7f8>`__  2021-12-31   ``Update documentation for provider December 2021 release (#20523)``\n`a0821235fb <https://github.com/apache/airflow/commit/a0821235fb6877a471973295fe42283ef452abf6>`__  2021-12-30   ``Use typed Context EVERYWHERE (#20565)``\n`92bc8047cf <https://github.com/apache/airflow/commit/92bc8047cf822bcedfecb8cbdc82a83dc7ea43ad>`__  2021-12-16   ``Fix mypy in  providers/salesforce (#20325)``\n`2fb5e1d0ec <https://github.com/apache/airflow/commit/2fb5e1d0ec306839a3ff21d0bddbde1d022ee8c7>`__  2021-12-15   ``Fix cached_property MyPy declaration and related MyPy errors (#20226)``\n`43de625d42 <https://github.com/apache/airflow/commit/43de625d4246af7014f64941f8effb09997731cb>`__  2021-12-01   ``Correctly capitalize names and abbreviations in docs (#19908)``\n==================================================================================================  ===========  ==========================================================================\n```\n\n----------------------------------------\n\nTITLE: Including Security Information in reStructuredText\nDESCRIPTION: This snippet includes an external file containing security information for the Apache Airflow project. It uses the reStructuredText include directive to embed the content of the security.rst file located in a specific directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Displaying Automated Documentation Generation Notice Block (reStructuredText, comments only)\nDESCRIPTION: This snippet is a multi-line reStructuredText comment block at the top of the file, denoting ASF licensing, edit constraints, and template guidance. It should always be preserved to maintain compliance and communicate file usage rules. The snippet requires no dependencies other than reST rendering and is strictly informational, not affecting rendered output. It accepts no inputs and provides no outputs other than visible documentation comments; modifications must be made only via the corresponding Jinja2 template.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openai/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n\n.. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!\n\n.. IF YOU WANT TO MODIFY THIS FILE, YOU SHOULD MODIFY THE TEMPLATE\n   `PROVIDER_COMMITS_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY\n\n.. THE REMAINDER OF THE FILE IS AUTOMATICALLY GENERATED. IT WILL BE OVERWRITTEN!\n```\n\n----------------------------------------\n\nTITLE: Defining Package License Header in RestructuredText\nDESCRIPTION: License header text block defining Apache 2.0 license terms for the apache-airflow-providers-openai package\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openai/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n```\n\n----------------------------------------\n\nTITLE: Using run_id for ExternalDag and TriggerDagRun Links\nDESCRIPTION: Commit message stating that 'run_id' is now used for generating links related to 'ExternalDag' and 'TriggerDagRun'. References pull request #46546.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_32\n\nLANGUAGE: plaintext\nCODE:\n```\nUse run_id for ExternalDag and TriggerDagRun links (#46546)\n```\n\n----------------------------------------\n\nTITLE: Environment Setup Commands\nDESCRIPTION: Commands for copying the local environment configuration file for Airflow UI development.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/15_node_environment_setup.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env.local\n```\n\n----------------------------------------\n\nTITLE: Cleanup of getattr Connection in DbApiHook\nDESCRIPTION: Maintenance update that cleans up remaining getattr connection calls in DbApiHook. Improves code quality and reduces potential issues with attribute access.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/changelog.rst#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nClean up remaining getattr connection DbApiHook (#40665)\n```\n\n----------------------------------------\n\nTITLE: Uploading Airflow Client Packages to PyPI with Twine (Shell Script)\nDESCRIPTION: Uses the `twine upload` command to publish the Python client package distribution files (`.tar.gz`, `.whl`) to the official Python Package Index (PyPI) production environment (`-r pypi`). This makes the specified version of the `apache-airflow-client` package publicly available for installation via pip.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_27\n\nLANGUAGE: shell script\nCODE:\n```\ntwine upload -r pypi *.tar.gz *.whl\n```\n\n----------------------------------------\n\nTITLE: Changelog Version Header\nDESCRIPTION: Section header format used to denote different versions in the changelog\nSOURCE: https://github.com/apache/airflow/blob/main/providers/atlassian/jira/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n2.6.0\n.....\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry Format in Markdown\nDESCRIPTION: Standardized format for documenting version changes, including commit hashes, dates, and descriptions\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ====================================================================================\nCommit                                                                                              Committed    Subject \n==================================================================================================  ===========  ====================================================================================\n`21990ed894 <https://github.com/apache/airflow/commit/21990ed8943ee4dc6e060ee2f11648490c714a3b>`__  2023-09-08   ``Prepare docs for 09 2023 - 1st wave of Providers (#34201)``\n```\n\n----------------------------------------\n\nTITLE: Path Import Update\nDESCRIPTION: Update to correctly import isabs from os.path module.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nCorrectly import isabs from os.path\n```\n\n----------------------------------------\n\nTITLE: Preparing Airflow Source Tarball\nDESCRIPTION: Command to prepare Airflow source tarball with specified version. Requires version flag for pre-release versions.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-airflow-tarball --version 2.8.0rc1\n```\n\n----------------------------------------\n\nTITLE: Creating KinD Cluster Configuration\nDESCRIPTION: Example KinD cluster configuration showing network settings and node configuration with port mappings.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnetworking:\n  ipFamily: ipv4\n  apiServerAddress: \"127.0.0.1\"\n  apiServerPort: 48366\nnodes:\n  - role: control-plane\n  - role: worker\n    extraPortMappings:\n      - containerPort: 30007\n        hostPort: 18150\n```\n\n----------------------------------------\n\nTITLE: Configuring Flower Authentication in airflow.cfg\nDESCRIPTION: Shows how to configure basic authentication for Flower in the Airflow configuration file using the celery section.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/flower.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[celery]\nflower_basic_auth = user1:password1,user2:password2\n```\n\n----------------------------------------\n\nTITLE: Loading CI Image from Specific Run\nDESCRIPTION: Command to load a CI image from a specific GitHub Actions job run using Breeze CLI\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/06_managing_docker_images.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image load --from-run 12538475388 --python 3.9 --github-token <your_github_token>\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare documentation for October Provider's release\nDESCRIPTION: Associated with commit d9567eb106, this message marks the preparation of documentation for the October 2021 release of Airflow providers, as noted in issue #19321.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n``Prepare documentation for October Provider's release (#19321)``\n```\n\n----------------------------------------\n\nTITLE: Preparing Documentation for Provider Release Wave (Feb)\nDESCRIPTION: Commit message related to preparing documentation updates for the February 1st wave of provider releases. References pull request #46893.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_24\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs for Feb 1st wave of providers (#46893)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add logs for last modified in SFTP, FTP, Filesystem sensor\nDESCRIPTION: This commit message, for commit 44a6648fd7, describes the addition of log messages that show the last modified timestamp in the SFTP, FTP, and Filesystem sensors, aiding in debugging and monitoring. Tracked in issue #15134.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_26\n\nLANGUAGE: text\nCODE:\n```\n``Add logs to show last modified in SFTP, FTP and Filesystem sensor (#15134)``\n```\n\n----------------------------------------\n\nTITLE: Listing Directory Permissions with umask 0002\nDESCRIPTION: Example showing file permissions when checking out the repository with umask 0002, where group write permissions are set.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0008-fixing-group-permissions-before-build.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nls -la scripts/ci\ntotal 132\ndrwxrwxr-x 19 jarek jarek  4096 Feb  5 20:49 .\ndrwxrwxr-x  8 jarek jarek  4096 Feb  5 20:49 ..\ndrwxrwxr-x  2 jarek jarek  4096 Feb  5 20:49 build_airflow\n```\n\n----------------------------------------\n\nTITLE: Enabling API Tests Need (Boolean String)\nDESCRIPTION: A boolean flag indicating whether the 'api-tests' step is required for the build. 'true' means it is needed.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Configuring EksPodOperator Execution in Python\nDESCRIPTION: Updates the execution configuration for the EksPodOperator to not rely on log level.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"Make EksPodOperator exec config not rely on log level (#35771)\"\n```\n\n----------------------------------------\n\nTITLE: Renaming Kerberos Principal Function in Python\nDESCRIPTION: Documentation of function rename from get_kerberos_principle to get_kerberos_principal to fix a spelling error while maintaining the same functionality in the Kerberos authentication module.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43679.misc.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nairflow.security.kerberos.get_kerberos_principle -> airflow.security.kerberos.get_kerberos_principal\n```\n\n----------------------------------------\n\nTITLE: Displaying OpenSearch Package Information in ReStructuredText\nDESCRIPTION: This ReStructuredText snippet provides information about the OpenSearch provider package, including a link to the OpenSearch website and instructions for viewing the changelog.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\nPackage apache-airflow-providers-opensearch\n------------------------------------------------------\n\n`OpenSearch <https://opensearch.org/>`__\n\n\nThis is detailed commit list of changes for versions provider package: ``opensearch``.\nFor high-level changelog, see :doc:`package information including changelog <index>`.\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Chart Index\nDESCRIPTION: Commands to regenerate and update index.yaml for Helm repository configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\nbreeze release-management add-back-references helm-chart\ncd \"${AIRFLOW_SITE_DIRECTORY}\"\ncurl https://dist.apache.org/repos/dist/dev/airflow/helm-chart/${VERSION}${VERSION_SUFFIX}/index.yaml -o index.yaml\ncp ${AIRFLOW_SVN_RELEASE_HELM}/${VERSION}/airflow-${VERSION}.tgz .\nhelm repo index --merge ./index.yaml . --url \"https://downloads.apache.org/airflow/helm-chart/${VERSION}\"\nrm airflow-${VERSION}.tgz\nmv index.yaml landing-pages/site/static/index.yaml\n```\n\n----------------------------------------\n\nTITLE: Building Reproducible Airflow Client Packages in Bash\nDESCRIPTION: This snippet shows how to build reproducible Airflow Client packages for verification. It uses the Breeze tool to prepare the packages in both distribution formats.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nVERSION=X.Y.Zrc1\ngit checkout python-client-${VERSION}\nexport AIRFLOW_REPO_ROOT=$(pwd)\nrm -rf dist/*\nbreeze release-management prepare-python-client --distribution-format both\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs 2nd wave Providers (Nov 2023)\nDESCRIPTION: This commit message (hash 0b23d5601c, dated 2023-11-24) marks the preparation of documentation for the second wave of provider releases in November 2023 (issue #35836).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_23\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs 2nd wave of Providers November 2023 (#35836)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Update documentation for November 2021 provider's release\nDESCRIPTION: This commit message, linked to commit 853576d901, signifies updates made to the documentation in preparation for the November 2021 release of Airflow providers. This is referenced by issue #19882.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n``Update documentation for November 2021 provider's release (#19882)``\n```\n\n----------------------------------------\n\nTITLE: Installing Multi-Platform Build Support in Docker\nDESCRIPTION: Command to install QEMU binaries for multi-platform builds using Docker's binfmt support, enabling emulation for building images across different architectures.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/MANUALLY_GENERATING_IMAGE_CACHE_AND_CONSTRAINTS.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --privileged --rm tonistiigi/binfmt --install all\n```\n\n----------------------------------------\n\nTITLE: Storing Google Cloud Connection with Key Dictionary\nDESCRIPTION: Example of storing a Google Cloud connection in AWS Secrets Manager using a key dictionary. The connection details, including the service account JSON, are stored in the 'extra' field.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-secrets-manager.rst#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"extra\": {\"keyfile_dict\": \"<copy & paste the service account json here>\",\n\"scope\": \"https://www.googleapis.com/auth/devstorage.read_only\"}}\n```\n\n----------------------------------------\n\nTITLE: Moving BaseOperatorLink to Task SDK (AIP-72)\nDESCRIPTION: Commit message indicating the migration of the 'BaseOperatorLink' class into the Task SDK framework as part of AIP-72. References pull request #47008.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nAIP-72: Moving BaseOperatorLink to task sdk (#47008)\n```\n\n----------------------------------------\n\nTITLE: Enabling UI Tests (Boolean String)\nDESCRIPTION: A boolean flag indicating whether UI tests should be run. 'true' enables these tests.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_24\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in reStructuredText\nDESCRIPTION: This directive includes external security documentation from a specified path. It's used to incorporate common security information across multiple files in the Apache Airflow project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/flink/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Adding Dynamic Task Mapping to TaskSDK Runtime\nDESCRIPTION: Commit message indicating the addition of dynamic task mapping capabilities into the TaskSDK runtime environment. References pull request #46032.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_34\n\nLANGUAGE: plaintext\nCODE:\n```\nAdd dynamic task mapping into TaskSDK runtime (#46032)\n```\n\n----------------------------------------\n\nTITLE: Using Jinja Templates with SQLExecuteQueryOperator in Apache Airflow\nDESCRIPTION: Example of using Jinja templates to parameterize SQL queries in SQLExecuteQueryOperator with JDBC connection in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/operators.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_jdbc_template]\n# [END howto_operator_jdbc_template]\n```\n\n----------------------------------------\n\nTITLE: Azure Documentation Structure in RST\nDESCRIPTION: RestructuredText markup defining the structure and sections of Azure integration documentation for Apache Airflow, including service operations and data transfer capabilities.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/operators-and-hooks-ref/azure.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\nAzure: Microsoft Azure\n----------------------\n\nAirflow has limited support for `Microsoft Azure <https://azure.microsoft.com/>`__.\n\nSome hooks are based on :mod:`airflow.providers.microsoft.azure.hooks.base_azure`\nwhich authenticate Azure's Python SDK Clients.\n\nServices\n'''''''''\n\nThese integrations allow you to perform various operations within the Microsoft Azure.\n\n.. operators-hooks-ref::\n   :tags: azure\n   :header-separator: \"\n\nTransfers\n'''''''''\n\nThese integrations allow you to copy data from/to Microsoft Azure.\n\n.. transfers-ref::\n   :tags: azure\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Configuring PyPI Upload Credentials Securely - INI File\nDESCRIPTION: This INI configuration defines the .pypirc file required for securely uploading packages to PyPI and Test PyPI using API tokens instead of plain-text passwords. It provides separate credentials for production and test environments, enhancing security during uploads. Dependencies: ensure you have upload tokens from both Test PyPI and Prod PyPI. Keys include username and password (API token). The file must be placed at ~/.pypirc with restricted permissions. Do not share this file.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README.md#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[distutils]\nindex-servers =\n  pypi\n  pypitest\n\n[pypi]\nusername=__token__\npassword=<API Upload Token>\n\n[pypitest]\nrepository=https://test.pypi.org/legacy/\nusername=__token__\npassword=<API Upload Token>\n```\n\n----------------------------------------\n\nTITLE: Specifying Change Types in Apache Airflow Migration\nDESCRIPTION: A checklist indicating the types of changes made in the Apache Airflow project, including DAG changes and code interface changes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/45425.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* Types of change\n\n  * [x] Dag changes\n  * [ ] Config changes\n  * [ ] API changes\n  * [ ] CLI changes\n  * [ ] Behaviour changes\n  * [ ] Plugin changes\n  * [ ] Dependency changes\n  * [x] Code interface changes\n```\n\n----------------------------------------\n\nTITLE: Updating Provider Dependencies Command\nDESCRIPTION: Command to regenerate all necessary files after changing provider dependencies using Breeze static checks.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/12_provider_distributions.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze static-checks --type update-providers-dependencies --all-files\n```\n\n----------------------------------------\n\nTITLE: Running Breeze with Python and MySQL Backend\nDESCRIPTION: Example command showing how to start Breeze shell with Python 3.9 and MySQL 8.0 backend\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze --python 3.9 --backend mysql --mysql-version 8.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow Logging Environment Variables in Breeze\nDESCRIPTION: Example environment configuration file for Breeze that sets up remote logging settings for Airflow, including the remote logging flag and base log folder location.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/02_customizing.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Logging settings\nAIRFLOW__LOGGING__REMOTE_LOGGING=true\nAIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER=\"logs\"\n\n# Remote logging Conn ID\nAIRFLOW__LOGGING__REMOTE_LOG_CONN_ID=\"logs_default\"\n```\n\n----------------------------------------\n\nTITLE: Release Announcement Email Template\nDESCRIPTION: Email templates for announcing new Airflow release to the community mailing lists.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\n[ANNOUNCE] Apache Airflow ${VERSION} Released\nEOF\n```\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\nDear Airflow community,\n\nI'm happy to announce that Airflow ${VERSION} was just released.\n\nThe released sources and packages can be downloaded via https://airflow.apache.org/docs/apache-airflow/${VERSION}/installation/installing-from-sources.html\n\nOther installation methods are described in https://airflow.apache.org/docs/apache-airflow/stable/installation/\n\nWe also made this version available on PyPI for convenience:\n\\`pip install apache-airflow\\`\nhttps://pypi.org/project/apache-airflow/${VERSION}/\n\nThe documentation is available at:\nhttps://airflow.apache.org/docs/apache-airflow/${VERSION}/\n\nFind the release notes here for more details:\nhttps://airflow.apache.org/docs/apache-airflow/${VERSION}/release_notes.html\n\nContainer images are published at:\nhttps://hub.docker.com/r/apache/airflow/tags/?page=1&name=${VERSION}\n\nCheers,\n<your name>\nEOF\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Docs for August 2022 Providers\nDESCRIPTION: This commit message, linked to commit e5ac6c7cfb dated 2022-08-10, indicates the preparation of documentation for the August 2022 release of new Apache Airflow Providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_28\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs for new providers release (August 2022) (#25618)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add test_connection method for sftp hook\nDESCRIPTION: This commit message, for commit ccb809550d, indicates the addition of a `test_connection` method to the SFTP hook, allowing users to verify SFTP connections easily. This feature addition is tracked in issue #19609.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n``Add test_connection method for sftp hook (#19609)``\n```\n\n----------------------------------------\n\nTITLE: Checking Current FAB Authentication Backends via Airflow CLI (Console)\nDESCRIPTION: Demonstrates how to use the `airflow config get-value` command to retrieve the currently configured value for `auth_backends` in the `[fab]` section of the Airflow configuration. The example output shows `basic_auth` configured.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/api-authentication.rst#2025-04-22_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ airflow config get-value fab auth_backends\nairflow.providers.fab.auth_manager.api.auth.backend.basic_auth\n```\n\n----------------------------------------\n\nTITLE: Including Release Notes in RST Documentation\nDESCRIPTION: RST directive to include external release notes file content into the documentation\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/release_notes.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: ../../RELEASE_NOTES.rst\n```\n\n----------------------------------------\n\nTITLE: Installing Helm GPG Plugin\nDESCRIPTION: Installs the helm-gpg plugin for signing charts using a specific verified commit version.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nhelm plugin install https://github.com/technosophos/helm-gpg --version 6303407eb63deaeb1b2f24de611e3468a27ec05b\n```\n\n----------------------------------------\n\nTITLE: Refreshing Airflow Provider Release with Kubernetes Retries\nDESCRIPTION: This commit message indicates an update to an upcoming Airflow provider release, specifically incorporating changes related to Kubernetes task retries. It references pull request #15239.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_36\n\nLANGUAGE: text\nCODE:\n```\nRefreshed provider's upcoming release with k8s retries (#15239)\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 2.0.2\nDESCRIPTION: Changelog entry documenting support for Python 3.10 in version 2.0.2\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sendgrid/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n2.0.2\n.....\n\nMisc\n~~~~\n\n* ``Support for Python 3.10``\n```\n\n----------------------------------------\n\nTITLE: Check Uncategorized PRs for Release\nDESCRIPTION: Shell command to identify bug fixes that are not marked with the target release milestone and need categorization.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./dev/airflow-github needs-categorization 2.3.2 HEAD\n```\n\n----------------------------------------\n\nTITLE: Fixing Month in Airflow Backport Package Naming (October)\nDESCRIPTION: This commit message details the correction of the month name (to October) used in the naming or versioning of backport packages. It references pull request #11242.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_72\n\nLANGUAGE: text\nCODE:\n```\nFixed month in backport packages to October (#11242)\n```\n\n----------------------------------------\n\nTITLE: Moving SQL Classes to Common-SQL Provider in Python\nDESCRIPTION: This code snippet involves moving all SQL-related classes to a common-sql provider. It centralizes SQL functionality and improves code organization across different database providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"Move all SQL classes to common-sql provider (#24836)\"\n```\n\n----------------------------------------\n\nTITLE: Downgrading Dependencies for Core Tests\nDESCRIPTION: Commands to enter Breeze shell, navigate to airflow-core directory, and use UV sync to downgrade dependencies to their lowest compatible versions for testing.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\nbreeze shell   # enter the container\ncd airflow-core\nuv sync --resolution lowest-direct\n```\n\n----------------------------------------\n\nTITLE: Raising Exception with Main Notebook Error in DatabricksRunNowDeferrableOperator\nDESCRIPTION: New feature to raise an exception with the main notebook error in DatabricksRunNowDeferrableOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"[FEAT] raise exception with main notebook error in DatabricksRunNowDeferrableOperator (#39110)\"\n```\n\n----------------------------------------\n\nTITLE: Speeding Up Breeze Autocompletion via Provider State (Commit Message)\nDESCRIPTION: Commit message detailing an optimization to speed up autocompletion in the Breeze development environment by simplifying how provider state (including Airbyte's) is handled.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_26\n\nLANGUAGE: text\nCODE:\n```\nSpeed up autocompletion of Breeze by simplifying provider state (#36499)\n```\n\n----------------------------------------\n\nTITLE: Prepare Documentation for April 2024 RC1 Wave (Excluded from v1.2.0 Changelog)\nDESCRIPTION: Indicates preparatory work on documentation for the first release candidate (RC1) wave of providers in April 2024, referenced by pull request #38863. This change was intentionally excluded from the main changelog notes for version 1.2.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_22\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 1st wave (RC1) April 2024 (#38863)\n```\n\n----------------------------------------\n\nTITLE: Preparing Documentation for August 2022 Provider Release\nDESCRIPTION: Excluded Change (Version 3.1.0): Involves documentation preparation for the August 2022 provider releases, referencing pull request #25618.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Prepare docs for new providers release (August 2022) (#25618)``\n```\n\n----------------------------------------\n\nTITLE: Move provider_tests to Unit Folder (Excluded from v1.4.1 Changelog)\nDESCRIPTION: Describes the relocation of 'provider_tests' to the 'unit' folder within provider tests, referenced by pull request #46800. This change was intentionally excluded from the main changelog notes for version 1.4.1.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nMove provider_tests to unit folder in provider tests (#46800)\n```\n\n----------------------------------------\n\nTITLE: Formatting Commit Information in Markdown\nDESCRIPTION: This code snippet demonstrates how commit information is formatted in the changelog using Markdown syntax. It includes the commit hash as a link, the commit date, and a brief description of the change.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`2723508345 <https://github.com/apache/airflow/commit/2723508345d5cf074aeb673955ce72996785f2bc>`__  2024-12-20   ``Prepare docs for Nov 1st wave of providers Dec 2024 (#45042)``\n```\n\n----------------------------------------\n\nTITLE: Implementing Reproducible Builds for Provider Packages\nDESCRIPTION: Introduces changes to ensure that building provider packages yields the same result every time, enhancing build consistency and reliability, tracked in issue #35693.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nUse reproducible builds for provider packages (#35693)\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for Microsoft Operators in reStructuredText\nDESCRIPTION: This snippet sets up a table of contents (toctree) for Microsoft Operators documentation. It uses the 'maxdepth' option to limit the depth of the tree and the 'glob' option to include all files in the current directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Tagging Final Airflow Client Release in Git Repositories (Shell Script)\nDESCRIPTION: Performs Git operations to tag the final release version in both the main Airflow repository and the dedicated Python client repository. It involves checking out the commit corresponding to the released RC version, creating signed Git tags (e.g., `python-client-X.Y.Z` and `X.Y.Z`), and pushing these tags to the `apache` remote (for the main repo) and `origin` remote (for the client repo). Requires `AIRFLOW_REPO_ROOT`, `CLIENT_REPO_ROOT`, `VERSION`, and `VERSION_SUFFIX` environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_28\n\nLANGUAGE: shell script\nCODE:\n```\ncd ${AIRFLOW_REPO_ROOT}\ngit checkout python-client-${VERSION}${VERSION_SUFFIX}\ngit tag -s python-client-${VERSION} -m \"Airflow Python Client ${VERSION}\"\ngit push apache tag python-client-${VERSION}\ncd ${CLIENT_REPO_ROOT}\ngit checkout ${VERSION}${VERSION_SUFFIX}\ngit tag -s ${VERSION} -m ${VERSION}\ngit push origin tag ${VERSION}\n```\n\n----------------------------------------\n\nTITLE: Committing Generated Constraint Files\nDESCRIPTION: Commands to copy, commit, and push newly generated constraint files to the Apache Airflow repository. Requires committer access and proper authentication.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/MANUALLY_GENERATING_IMAGE_CACHE_AND_CONSTRAINTS.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd <AIRFLOW_WITH_CONSTRAINTS-MAIN_DIRECTORY>\ngit pull\ncp ${AIRFLOW_SOURCES}/files/constraints-*/constraints*.txt .\ngit diff\ngit add .\ngit commit -m \"Your commit message here\" --no-verify\ngit push\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Auto-apply apply_default decorator\nDESCRIPTION: Associated with commit 37681bca00, this message indicates an internal change where the `apply_default` decorator is now automatically applied, likely simplifying operator/hook definitions. Tracked in issue #15667.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_22\n\nLANGUAGE: text\nCODE:\n```\n``Auto-apply apply_default decorator (#15667)``\n```\n\n----------------------------------------\n\nTITLE: Committing Generated Code\nDESCRIPTION: Commands to commit the generated client code to the Python Client repository.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncd ${CLIENT_REPO_ROOT}\ngit diff HEAD\ngit checkout -b release-${VERSION}\ngit add .\ngit commit -m \"Update Python Client to ${VERSION}${VERSION_SUFFIX}\"\ngit push apache release-${VERSION}\n```\n\n----------------------------------------\n\nTITLE: Allowing Providers to Contribute Configuration\nDESCRIPTION: Enables providers to contribute their own configuration settings to the central Airflow configuration system, allowing for provider-specific customization, detailed in #32604.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_27\n\nLANGUAGE: text\nCODE:\n```\nAllow configuration to be contributed by providers (#32604)\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Hook for Diagrams\nDESCRIPTION: Command to generate all diagrams using the pre-commit hook mechanism.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/17_architecture_diagrams.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run generate-airflow-diagrams\n```\n\n----------------------------------------\n\nTITLE: Generating Webserver Coverage Report for Airflow\nDESCRIPTION: Demonstrates how to produce a code coverage report for Airflow's webserver components using a Python script within the Breeze environment.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_51\n\nLANGUAGE: python\nCODE:\n```\npython scripts/cov/www_coverage.py\n```\n\n----------------------------------------\n\nTITLE: Initializing Other Arguments for Video Intelligence Operators in Airflow (Python)\nDESCRIPTION: This snippet demonstrates the initialization of base arguments required for Google Cloud Video Intelligence operators in Airflow system tests. It sets up context such as the input URI for a video file in Google Cloud Storage and specifies the project. Dependencies include Airflow's Google provider and access to Google Cloud. The expected input is a GCS URI string, and the output is a dictionary of argument values for use with downstream tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/video_intelligence.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOTHER_ARGS = {\n    \"input_uri\": f\"gs://{GCS_BUCKET_NAME}/{VIDEO_INPUT_FILE_NAME}\",\n    \"project_id\": GCP_PROJECT_ID,\n}\n```\n\n----------------------------------------\n\nTITLE: HTML Table of Contents\nDESCRIPTION: Auto-generated table of contents using DocToc\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0007-using-database-volumes-for-backends.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*\n\n- [7. Using database volumes for backends](#7-using-database-volumes-for-backends)\n  - [Status](#status)\n  - [Context](#context)\n  - [Decision](#decision)\n  - [Consequences](#consequences)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n```\n\n----------------------------------------\n\nTITLE: Including External RST File in Sphinx\nDESCRIPTION: This snippet uses the reStructuredText `include` directive, specific to the Sphinx documentation generator. It directs Sphinx to insert the content of the specified file (`/../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst`) at this location during the documentation build process. This is commonly used to manage reusable content blocks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Measuring Python Interpreter Startup Time\nDESCRIPTION: Command to assess the baseline Python interpreter initialization time, which helps calculate the actual DAG parsing time by subtracting this value from the total execution time.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ntime python -c ''\n```\n\n----------------------------------------\n\nTITLE: Removing MySQL Import from Generic Transfer Test\nDESCRIPTION: Commit message indicating the removal of an import from the MySQL provider tests within a generic transfer test file, likely to reduce coupling. References pull request #46274.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_38\n\nLANGUAGE: plaintext\nCODE:\n```\nRemove import from MySQL provider tests in generic transfer test (#46274)\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit with UV\nDESCRIPTION: Command to install pre-commit hooks using the UV package manager.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv tool install pre-commit\n```\n\n----------------------------------------\n\nTITLE: Version 3.4.1 Bug Fix Entry\nDESCRIPTION: RST changelog entry documenting a bug fix for SQLAlchemy scheme character validation\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n* ``Check if sqlalchemy_scheme extra contains forbidden characters (#31984)``\n```\n\n----------------------------------------\n\nTITLE: Checking SHA512 Checksums\nDESCRIPTION: Script to verify SHA512 checksums for release artifacts.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\nfor i in *.sha512\ndo\n    echo \"Checking $i\"; shasum -a 512 `basename $i .sha512 ` | diff - $i\ndone\n```\n\n----------------------------------------\n\nTITLE: Moving EmptyOperator to Standard Provider\nDESCRIPTION: Commit message describing the relocation of the 'EmptyOperator' to a standard provider structure. References pull request #46231.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_35\n\nLANGUAGE: plaintext\nCODE:\n```\nMoving EmptyOperator to standard provider (#46231)\n```\n\n----------------------------------------\n\nTITLE: Formatting Version Headers in Markdown\nDESCRIPTION: This snippet shows how to format version headers in Markdown, including the version number and underline.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/docker/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n2.3.0\n.....\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry v2.0.2\nDESCRIPTION: Changelog entry documenting support for Python 3.10\nSOURCE: https://github.com/apache/airflow/blob/main/providers/datadog/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n2.0.2\n.....\n\nMisc\n~~~~\n\n* ``Support for Python 3.10``\n```\n\n----------------------------------------\n\nTITLE: Checking SHA512 Checksums in Shell\nDESCRIPTION: This snippet demonstrates how to verify the SHA512 checksums of the release files. It compares the computed checksum with the provided checksum file for each release artifact.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\nfor i in *.sha512\ndo\n    echo \"Checking $i\"; shasum -a 512 `basename $i .sha512 ` | diff - $i\ndone\n```\n\n----------------------------------------\n\nTITLE: Including Provider Configuration References in RestructuredText Documentation\nDESCRIPTION: Directive to include external documentation files related to provider configurations and sections/options in Airflow documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst\n.. include:: /../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst\n```\n\n----------------------------------------\n\nTITLE: Upgrading Airflow Core Dependencies\nDESCRIPTION: Command to upgrade Airflow core dependencies without affecting providers using no-providers constraints file.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/13_airflow_dependencies_and_extras.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install . --upgrade \\\n  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-main/constraints-no-providers-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fix MyPy Errors for SFTP provider\nDESCRIPTION: This commit message, associated with commit 756b1207a9, indicates a fix applied to resolve MyPy type checking errors specifically within the Apache Airflow SFTP provider code. The change is linked to issue #20242.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n``Fix MyPy Errors for SFTP provider (#20242)``\n```\n\n----------------------------------------\n\nTITLE: Maintaining Provider Changelog Tables - Markdown\nDESCRIPTION: This snippet presents tabular changelogs in Markdown format, summarizing changes for different provider releases in Apache Airflow. The tables list commit identifiers, timestamps, and brief commit messages, with hyperlinks to specific commits on GitHub. It relies on standard Markdown syntax (pipes for columns and backticks/underscores for links), and expects readers to interpret and follow GitHub links as needed. The input is tabular text with commit metadata, and the output is rendered HTML tables in documentation or project portals. The pattern is highly consistent and leverages Markdown for easy consumption and automation in documentation systems.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/tableau/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  =======================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =======================================================================\n`b75f9e8806 <https://github.com/apache/airflow/commit/b75f9e880614fa0427e7d24a1817955f5de658b3>`__  2023-10-18   ``Upgrade pre-commits (#35033)``\n```\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ===============================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ===============================================================\n`e9987d5059 <https://github.com/apache/airflow/commit/e9987d50598f70d84cbb2a5d964e21020e81c080>`__  2023-10-13   ``Prepare docs 1st wave of Providers in October 2023 (#34916)``\n`0c8e30e43b <https://github.com/apache/airflow/commit/0c8e30e43b70e9d033e1686b327eb00aab82479c>`__  2023-10-05   ``Bump min airflow version of providers (#34728)``\n```\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ===================================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ===================================================================================================\n`21990ed894 <https://github.com/apache/airflow/commit/21990ed8943ee4dc6e060ee2f11648490c714a3b>`__  2023-09-08   ``Prepare docs for 09 2023 - 1st wave of Providers (#34201)``\n`42b8595697 <https://github.com/apache/airflow/commit/42b859569715d37e0632164f04a1dd6e4e41e754>`__  2023-09-07   ``fix(providers/tableau): respect soft_fail argument when exception is raised (#34163)``\n`9d8c77e447 <https://github.com/apache/airflow/commit/9d8c77e447f5515b9a6aa85fa72511a86a128c28>`__  2023-08-27   ``Improve modules import in Airflow providers by some of them into a type-checking block (#33754)``\n`c077d19060 <https://github.com/apache/airflow/commit/c077d190609f931387c1fcd7b8cc34f12e2372b9>`__  2023-08-26   ``Prepare docs for Aug 2023 3rd wave of Providers (#33730)``\n`b5a4d36383 <https://github.com/apache/airflow/commit/b5a4d36383c4143f46e168b8b7a4ba2dc7c54076>`__  2023-08-11   ``Prepare docs for Aug 2023 2nd wave of Providers (#33291)``\n`73b90c48b1 <https://github.com/apache/airflow/commit/73b90c48b1933b49086d34176527947bd727ec85>`__  2023-07-21   ``Allow configuration to be contributed by providers (#32604)``\n`21e8f878a3 <https://github.com/apache/airflow/commit/21e8f878a3c91250d0d198c6c3675b4b350fcb61>`__  2023-07-06   ``D205 Support - Providers: Snowflake to Zendesk (inclusive) (#32359)``\n`225e3041d2 <https://github.com/apache/airflow/commit/225e3041d269698d0456e09586924c1898d09434>`__  2023-07-06   ``Prepare docs for July 2023 wave of Providers (RC2) (#32381)``\n`3878fe6fab <https://github.com/apache/airflow/commit/3878fe6fab3ccc1461932b456c48996f2763139f>`__  2023-07-05   ``Remove spurious headers for provider changelogs (#32373)``\n`cb4927a018 <https://github.com/apache/airflow/commit/cb4927a01887e2413c45d8d9cb63e74aa994ee74>`__  2023-07-05   ``Prepare docs for July 2023 wave of Providers (#32298)``\n`09d4718d3a <https://github.com/apache/airflow/commit/09d4718d3a46aecf3355d14d3d23022002f4a818>`__  2023-06-27   ``Improve provider documentation and README structure (#32125)``\n```\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  =============================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =============================================================\n`79bcc2e668 <https://github.com/apache/airflow/commit/79bcc2e668e648098aad6eaa87fe8823c76bc69a>`__  2023-06-20   ``Prepare RC1 docs for June 2023 wave of Providers (#32001)``\n`8b146152d6 <https://github.com/apache/airflow/commit/8b146152d62118defb3004c997c89c99348ef948>`__  2023-06-20   ``Add note about dropping Python 3.7 for providers (#32015)``\n`a59076eaee <https://github.com/apache/airflow/commit/a59076eaeed03dd46e749ad58160193b4ef3660c>`__  2023-06-02   ``Add D400 pydocstyle check - Providers (#31427)``\n```\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ======================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ======================================================================================\n`45548b9451 <https://github.com/apache/airflow/commit/45548b9451fba4e48c6f0c0ba6050482c2ea2956>`__  2023-05-19   ``Prepare RC2 docs for May 2023 wave of Providers (#31416)``\n`abea189022 <https://github.com/apache/airflow/commit/abea18902257c0250fedb764edda462f9e5abc84>`__  2023-05-18   ``Use '__version__' in providers not 'version' (#31393)``\n`f5aed58d9f <https://github.com/apache/airflow/commit/f5aed58d9fb2137fa5f0e3ce75b6709bf8393a94>`__  2023-05-18   ``Fixing circular import error in providers caused by airflow version check (#31379)``\n`7ebda3898d <https://github.com/apache/airflow/commit/7ebda3898db2eee72d043a9565a674dea72cd8fa>`__  2023-05-17   ``Fix missing line in index.rst for provider documentation (#31343)``\n`d9ff55cf6d <https://github.com/apache/airflow/commit/d9ff55cf6d95bb342fed7a87613db7b9e7c8dd0f>`__  2023-05-16   ``Prepare docs for May 2023 wave of Providers (#31252)``\n`0a30706aa7 <https://github.com/apache/airflow/commit/0a30706aa7c581905ca99a8b6e2f05960d480729>`__  2023-05-03   ``Use 'AirflowProviderDeprecationWarning' in providers (#30975)``\n`eef5bc7f16 <https://github.com/apache/airflow/commit/eef5bc7f166dc357fea0cc592d39714b1a5e3c14>`__  2023-05-03   ``Add full automation for min Airflow version for providers (#30994)``\n`a7eb32a5b2 <https://github.com/apache/airflow/commit/a7eb32a5b222e236454d3e474eec478ded7c368d>`__  2023-04-30   ``Bump minimum Airflow version in providers (#30917)``\n`d23a3bbed8 <https://github.com/apache/airflow/commit/d23a3bbed89ae04369983f21455bf85ccc1ae1cb>`__  2023-04-04   ``Add mechanism to suspend providers (#30422)``\n```\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  =====================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =====================================================================\n`ce6ae2457e <https://github.com/apache/airflow/commit/ce6ae2457ef3d9f44f0086b58026909170bbf22a>`__  2023-02-08   ``Prepare docs for Feb 2023 wave of Providers (#29379)``\n`67b92d7b53 <https://github.com/apache/airflow/commit/67b92d7b53b2c15b3c9656787f64ff71599d7cb2>`__  2023-02-03   ``Add TableauOperator.template_fields = find, match_with (#29360)``\n`2b92c3c74d <https://github.com/apache/airflow/commit/2b92c3c74d3259ebac714f157c525836f0af50f0>`__  2023-01-05   ``Fix providers documentation formatting (#28754)``\n`c8e348dcb0 <https://github.com/apache/airflow/commit/c8e348dcb0bae27e98d68545b59388c9f91fc382>`__  2022-12-05   ``Add automated version replacement in example dag indexes (#28090)``\n```\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ====================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ====================================================================================\n`12c3c39d1a <https://github.com/apache/airflow/commit/12c3c39d1a816c99c626fe4c650e88cf7b1cc1bc>`__  2022-11-15   ``pRepare docs for November 2022 wave of Providers (#27613)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepares docs for Rc2 release of July providers\nDESCRIPTION: Associated with commit 87f408b1e7, this message signifies documentation preparations for the second release candidate (Rc2) of the July 2021 Airflow providers release, tracked by issue #17116.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n``Prepares docs for Rc2 release of July providers (#17116)``\n```\n\n----------------------------------------\n\nTITLE: Preparing Airflow Distributions with Breeze\nDESCRIPTION: Command to prepare Airflow .whl package in the dist folder. Supports optional distribution format flag to build either sdist or wheel formats.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-airflow-distributions\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-airflow-distributions --distribution-format=wheel\n```\n\n----------------------------------------\n\nTITLE: Git Commit History Formatting\nDESCRIPTION: Git commit history entries showing hash, date and subject changes. Each line contains a commit hash link, commit date, and commit message.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: git\nCODE:\n```\n`52d2032887 <https://github.com/apache/airflow/commit/52d20328872b68f0f80986006726f16a6dc56c45>`__  2024-02-21   ``Fix typo on DataflowStartFlexTemplateOperator documentation (#37595)``\n```\n\n----------------------------------------\n\nTITLE: Displaying Airflow CI/CD Workflow Table in Markdown\nDESCRIPTION: A markdown table showing the various jobs in the Airflow CI/CD workflow, indicating which jobs run for PRs, main branch, and release testing.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/05_workflows.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Job                                | Description                                                | PR      | main    | v*-*-test |\n|------------------------------------|------------------------------------------------------------|---------|---------|-----------|\n```\n\n----------------------------------------\n\nTITLE: Template Fields for Vision Operators\nDESCRIPTION: Shows available template fields for various Vision operators\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\"location\", \"product_set_id\", \"project_id\", \"retry\", \"timeout\", \"metadata\")\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Update Docs for September Provider Release\nDESCRIPTION: This commit message, linked to commit f8db64c35c dated 2022-09-28, notes the update of documentation corresponding to the September release of Apache Airflow Providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_26\n\nLANGUAGE: text\nCODE:\n```\n``Update docs for September Provider's release (#26731)``\n```\n\n----------------------------------------\n\nTITLE: Committing to ASF SVN Repository\nDESCRIPTION: Commands to commit release artifacts to the Apache Software Foundation dev distribution SVN repository.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nsvn checkout https://dist.apache.org/repos/dist/dev/airflow airflow-dev\n\n# Create new folder for the release\ncd airflow-dev/clients/python\nsvn mkdir ${VERSION}${VERSION_SUFFIX}\n\n# Move the artifacts to svn folder & commit\nmv ${AIRFLOW_REPO_ROOT}/dist/apache_airflow_client-* ${VERSION}${VERSION_SUFFIX}/\ncd ${VERSION}${VERSION_SUFFIX}\nsvn add *\nsvn commit -m \"Add artifacts for Apache Airflow Python Client ${VERSION}${VERSION_SUFFIX}\"\n\n# Remove old version\ncd ..\nexport PREVIOUS_VERSION_WITH_SUFFIX=2.8.0rc1\nsvn rm ${PREVIOUS_VERSION_WITH_SUFFIX}\nsvn commit -m \"Remove old Apache Airflow Python Client ${PREVIOUS_VERSION_WITH_SUFFIX}\"\n```\n\n----------------------------------------\n\nTITLE: Running Specific Integration Tests\nDESCRIPTION: Commands to run integration tests for specific services using pytest\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/integration_tests.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npytest --integration mongo tests/integration\n```\n\nLANGUAGE: bash\nCODE:\n```\npytest --integration mongo --integration celery tests/integration\n```\n\nLANGUAGE: bash\nCODE:\n```\npytest --integration cassandra tests/integrations/providers/apache\n```\n\n----------------------------------------\n\nTITLE: Integrating TriggerDagRunOperator with Task SDK (AIP-72)\nDESCRIPTION: Commit message indicating work done as part of AIP-72 to make the 'TriggerDagRunOperator' compatible with the Task SDK. References pull request #47882.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nAIP-72: Get 'TriggerDagRunOperator' working with Task SDK (#47882)\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit Information in Markdown\nDESCRIPTION: This code snippet shows how commit information is formatted in the changelog using Markdown syntax. It includes the commit hash, date, and commit message.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/smtp/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`b15d5578da <https://github.com/apache/airflow/commit/b15d5578dac73c4c6a3ca94d90ab0dc9e9e74c9c>`__  2023-12-23   ``Re-apply updated version numbers to 2nd wave of providers in December (#36380)``\n```\n\n----------------------------------------\n\nTITLE: Defining Security Documentation Structure in reStructuredText\nDESCRIPTION: This snippet defines the structure for the security documentation using reStructuredText directives. It creates a table of contents that includes all files in the current directory and a 'secrets' subdirectory.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n    secrets/index\n```\n\n----------------------------------------\n\nTITLE: Fixing Codespell Issues\nDESCRIPTION: Commit message indicating fixes for spelling errors detected by a newer version of the 'codespell' tool. References pull request #47259.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_15\n\nLANGUAGE: plaintext\nCODE:\n```\nFix codespell issues detected by new codespell (#47259)\n```\n\n----------------------------------------\n\nTITLE: Reference Image Creation\nDESCRIPTION: Creates a ReferenceImage instance\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_vision_reference_image]\\n[END howto_operator_vision_reference_image]\n```\n\n----------------------------------------\n\nTITLE: HTML License Header Comment\nDESCRIPTION: Apache License 2.0 header comment block for the documentation file\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0010-use-pipx-to-install-breeze.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Dropping Archived XCom Tables in Bash\nDESCRIPTION: Command to safely drop the _xcom_archive table after XCom changes that moved pickled data to an archive table. This is used when you no longer need the archived pickled XCom data.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/aip-72.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairflow db drop-archived -t \"_xcom_archive\"\n```\n\n----------------------------------------\n\nTITLE: Defining Base Modules Table in reStructuredText\nDESCRIPTION: Creates a table listing the base modules for Airflow, including hooks, baseoperator, and sensors. The table has two columns: Module and Guides.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/operators-and-hooks-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n**Base:**\n\n.. list-table::\n   :header-rows: 1\n\n   * - Module\n     - Guides\n\n   * - :mod:`airflow.hooks.base`\n     -\n\n   * - :mod:`airflow.models.baseoperator`\n     -\n\n   * - :mod:`airflow.sensors.base`\n     -\n```\n\n----------------------------------------\n\nTITLE: Adding 'get_current_context' Support in Task SDK (AIP-72)\nDESCRIPTION: Commit message detailing the addition of support for the 'get_current_context' function within the Task SDK framework as part of AIP-72. References pull request #45486.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_45\n\nLANGUAGE: plaintext\nCODE:\n```\nAIP-72: Add support for 'get_current_context' in Task SDK (#45486)\n```\n\n----------------------------------------\n\nTITLE: Migrating Kubernetes Secret Classes in Python\nDESCRIPTION: Migration rules for Kubernetes Secret classes, moving from the deprecated airflow.kubernetes module to airflow.providers.cncf.kubernetes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41735.significant.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"airflow.kubernetes.secret.Secret\"  \"airflow.providers.cncf.kubernetes.secret.Secret\"\n\"airflow.kubernetes.secret.K8SModel\"  \"airflow.providers.cncf.kubernetes.k8s_model.K8SModel\"\n```\n\n----------------------------------------\n\nTITLE: Removing Hook Class Names from Provider YAML in Python\nDESCRIPTION: Code snippet demonstrating the removal of 'hook-class-names' from the provider.yaml file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opsgenie/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"Remove 'hook-class-names' from provider.yaml (#24702)\"\n```\n\n----------------------------------------\n\nTITLE: MySQL Provider Changelog RST Format\nDESCRIPTION: Changelog entry in RestructuredText format documenting version history, breaking changes, bug fixes and features for the MySQL provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: restructuredtext\nCODE:\n```\n   * ``Removes pylint from our toolchain (#16682)``\n   * ``Prepare documentation for July release of providers. (#17015)``\n   * ``Fixed wrongly escaped characters in amazon's changelog (#17020)``\n   * ``Remove/refactor default_args pattern for miscellaneous providers (#16872)``\n\n2.0.0\n.....\n\nBreaking changes\n~~~~~~~~~~~~~~~~\n\n* ``Auto-apply apply_default decorator (#15667)``\n\n.. warning:: Due to apply_default decorator removal, this version of the provider requires Airflow 2.1.0+.\n   If your Airflow version is < 2.1.0, and you want to install this provider version, first upgrade\n   Airflow to at least version 2.1.0. Otherwise your Airflow package version will be upgraded\n   automatically and you will have to manually run ``airflow upgrade db`` to complete the migration.\n\nBug Fixes\n~~~~~~~~~\n\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!)\n```\n\n----------------------------------------\n\nTITLE: Version Header Format in RST\nDESCRIPTION: ReStructuredText format for version headers in changelog entries.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n10.5.1\n......\n```\n\n----------------------------------------\n\nTITLE: Importing Hooks in Apache Airflow - After Change\nDESCRIPTION: Demonstrates the new recommended way to import hooks directly from their source module without using Airflow's plugin mechanism.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43291.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom my_plugin import MyHook\n```\n\n----------------------------------------\n\nTITLE: Setting PYTHONPATH for Apache Airflow CLI Command\nDESCRIPTION: Shows how to set the PYTHONPATH environment variable when running an Airflow CLI command to include custom operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/modules_management.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nPYTHONPATH=/home/arch/projects/airflow_operators airflow info\n```\n\n----------------------------------------\n\nTITLE: Committing Documentation Changes\nDESCRIPTION: Commands to commit and push documentation changes to repository.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\ngit add .\ngit commit -m \"Add documentation for Apache Airflow Helm Chart ${VERSION}\"\ngit push\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation using Sphinx in RST\nDESCRIPTION: A Sphinx directive to include external security documentation from a common development source. This directive pulls in security-related documentation from a shared location to ensure consistent security information across the project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Adding Custom DAGs to Airflow Image\nDESCRIPTION: Dockerfile and commands to create a custom Airflow image with DAGs\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/quick-start.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmkdir my-airflow-project && cd my-airflow-project\nmkdir dags  # put dags here\ncat <<EOM > Dockerfile\nFROM apache/airflow\nCOPY . .\nEOM\n\ndocker build --pull --tag my-dags:0.0.1 .\n\nkind load docker-image my-dags:0.0.1\n\nhelm upgrade $RELEASE_NAME apache-airflow/airflow --namespace $NAMESPACE \\\n    --set images.airflow.repository=my-dags \\\n    --set images.airflow.tag=0.0.1\n```\n\n----------------------------------------\n\nTITLE: Using get_parsing_context Before Migration\nDESCRIPTION: Example showing how to use get_parsing_context function from the original airflow.utils.dag_parsing_context module for dynamic DAG generation.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/45694.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models.dag import DAG\nfrom airflow.utils.dag_parsing_context import get_parsing_context\n\ncurrent_dag_id = get_parsing_context().dag_id\n\nfor thing in list_of_things:\n    dag_id = f\"generated_dag_{thing}\"\n    if current_dag_id is not None and current_dag_id != dag_id:\n        continue  # skip generation of non-selected DAG\n\n    with DAG(dag_id=dag_id, ...):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Git Commit History Log\nDESCRIPTION: A git commit log showing version control history including commit hashes, dates, and commit messages for tracking changes to the provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/segment/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nf5aed58d9f <https://github.com/apache/airflow/commit/f5aed58d9fb2137fa5f0e3ce75b6709bf8393a94>  2023-05-18   \"Fixing circular import error in providers caused by airflow version check (#31379)\"\n7ebda3898d <https://github.com/apache/airflow/commit/7ebda3898db2eee72d043a9565a674dea72cd8fa>  2023-05-17   \"Fix missing line in index.rst for provider documentation (#31343)\"\nd9ff55cf6d <https://github.com/apache/airflow/commit/d9ff55cf6d95bb342fed7a87613db7b9e7c8dd0f>  2023-05-16   \"Prepare docs for May 2023 wave of Providers (#31252)\"\n```\n\n----------------------------------------\n\nTITLE: Including Release Notes in RST Documentation\nDESCRIPTION: RST directive to include external release notes file content\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/release_notes.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: ../RELEASE_NOTES.rst\n```\n\n----------------------------------------\n\nTITLE: Enabling Selective OpenLineage Tracking Policy via INI\nDESCRIPTION: Enables the selective enablement policy for OpenLineage by setting `selective_enable` to `true` in the `[openlineage]` section of the Airflow configuration. This allows fine-grained control over lineage tracking at the DAG and task level using helper functions, though the global `disabled` setting takes precedence.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_25\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\nselective_enable = True\n```\n\n----------------------------------------\n\nTITLE: Using PyUpgrade for Python 3.6 Features in Airflow\nDESCRIPTION: This commit message details the use of the PyUpgrade tool to automatically update the Airflow codebase to leverage features available in Python 3.6. It references pull request #11447.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_65\n\nLANGUAGE: text\nCODE:\n```\nUse PyUpgrade to use Python 3.6 features (#11447)\n```\n\n----------------------------------------\n\nTITLE: Defining Toctree Navigation in Sphinx Documentation (reStructuredText)\nDESCRIPTION: This set of snippets illustrates the use of Sphinx's 'toctree' directive to organize documentation for the apache-airflow-providers-pinecone package. Toctree directives create hierarchical navigation menus for sectioning documentation into basics, guides, resources, system tests, and commits. Key directives include ':hidden:' to hide navigation trees from the rendered document and ':maxdepth:' to restrict link depth. Toctrees can reference internal documentation, markdown, or external web pages.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pinecone/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\\n    :hidden:\\n    :maxdepth: 1\\n    :caption: Basics\\n\\n    Home <self>\\n    Changelog <changelog>\\n    Security <security>\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\\n    :hidden:\\n    :maxdepth: 1\\n    :caption: Guides\\n\\n    Connection types <connections>\\n    Operators <operators/pinecone>\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\\n    :hidden:\\n    :maxdepth: 1\\n    :caption: Resources\\n\\n    Python API <_api/airflow/providers/pinecone/index>\\n    PyPI Repository <https://pypi.org/project/apache-airflow-providers-pinecone/>\\n    Installing from sources <installing-providers-from-sources>\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\\n    :hidden:\\n    :maxdepth: 1\\n    :caption: System tests\\n\\n    System Tests <_api/tests/system/pinecone/index>\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\\n    :hidden:\\n    :maxdepth: 1\\n    :caption: Commits\\n\\n    Detailed list of commits <commits>\\n\n```\n\n----------------------------------------\n\nTITLE: Generating CLI Coverage Report for Airflow\nDESCRIPTION: Illustrates the process of generating a code coverage report for Airflow's CLI components using a Python script in the Breeze environment.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_50\n\nLANGUAGE: python\nCODE:\n```\npython scripts/cov/cli_coverage.py\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Impala Provider with Common SQL Dependency\nDESCRIPTION: This command installs the Apache Impala provider package along with its cross-provider dependency for common SQL features. It demonstrates how to include optional dependencies when installing from PyPI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/impala/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-impala[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Installing Extra Dependencies via pip for Apache Airflow Providers (Bash)\nDESCRIPTION: This Bash snippet demonstrates the command to install the apache-airflow-providers-fab package along with optional extra dependencies (common.compat) using pip. This ensures all optional features and cross-provider functionalities are enabled. No parameters are required besides specifying the desired package and extra; the output will be the successful installation of needed distributions, but requires pip and Python to be properly set up.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-fab[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Applying D401 Docstring Formatting to Providers\nDESCRIPTION: Updates docstrings in providers from Airbyte to Atlassian to comply with the D401 standard (imperative mood for first line), improving documentation consistency, as per issue #33354.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_24\n\nLANGUAGE: text\nCODE:\n```\nD401 Support - Providers: Airbyte to Atlassian (Inclusive) (#33354)\n```\n\n----------------------------------------\n\nTITLE: Prepare Documentation for Feb 2024 1st Wave (Excluded from v1.2.0 Changelog)\nDESCRIPTION: Indicates preparatory work on documentation for the first wave of provider releases in February 2024, referenced by pull request #37326. This change was intentionally excluded from the main changelog notes for version 1.2.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_24\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 1st wave of Providers February 2024 (#37326)\n```\n\n----------------------------------------\n\nTITLE: Documenting Provider Update November 2023 (1st Wave)\nDESCRIPTION: This commit prepares the documentation for the first wave of Apache Airflow provider updates released in November 2023, referenced by issue #35537.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs 1st wave of Providers November 2023 (#35537)\n```\n\n----------------------------------------\n\nTITLE: Prepare Documentation for Jan 2024 2nd Wave (Excluded from v1.2.0 Changelog)\nDESCRIPTION: Indicates preparatory work on documentation for the second wave of provider releases in January 2024, referenced by pull request #36945. This change was intentionally excluded from the main changelog notes for version 1.2.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_25\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 2nd wave of Providers January 2024 (#36945)\n```\n\n----------------------------------------\n\nTITLE: Adding run_after Column to DagRun Model\nDESCRIPTION: Commit message indicating the addition of a 'run_after' column to the 'DagRun' database model. References pull request #45732.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_36\n\nLANGUAGE: plaintext\nCODE:\n```\nAdd run_after column to DagRun model (#45732)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add full automation for min Airflow version for providers (#30994)\nDESCRIPTION: This commit message, associated with version 3.2.0, introduces automation for setting and managing the minimum required Airflow version for providers. Commit hash: eef5bc7f16, Date: 2023-05-03.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n``Add full automation for min Airflow version for providers (#30994)``\n```\n\n----------------------------------------\n\nTITLE: Documenting Provider Update December 2023 (1st Wave)\nDESCRIPTION: This commit finalizes documentation changes for the initial set of Apache Airflow provider releases scheduled for December 2023, as tracked in issue #36112.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs 1st wave of Providers December 2023 (#36112)\n```\n\n----------------------------------------\n\nTITLE: Importing PGP Keys for Apache Airflow Provider Packages\nDESCRIPTION: Commands to import PGP keys for verifying Apache Airflow provider package signatures. Three alternative methods are provided using gpg, pgpk, and pgp.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngpg -i KEYS\n```\n\nLANGUAGE: bash\nCODE:\n```\npgpk -a KEYS\n```\n\nLANGUAGE: bash\nCODE:\n```\npgp -ka KEYS\n```\n\n----------------------------------------\n\nTITLE: Yarn Code Quality Commands\nDESCRIPTION: Commands for code formatting, linting, and testing using Yarn.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/15_node_environment_setup.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Format code in .js, .jsx, .ts, .tsx, .json, .css, .html files\nyarn format\n\n# Check JS/TS code in .js, .jsx, .ts, .tsx, .html files and report any errors/warnings\nyarn run lint\n\n# Check JS/TS code in .js, .jsx, .ts, .tsx, .html files and report any errors/warnings and fix them if possible\nyarn run lint:fix\n\n# Run tests for all .test.js, .test.jsx, .test.ts, test.tsx files\nyarn test\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Follow BaseHook Signature\nDESCRIPTION: This commit message, associated with commit cd476acd8f on 2023-12-11, indicates an update to ensure child classes adhere to the method signature defined in the BaseHook connection fields.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nFollow BaseHook connection fields method signature in child classes (#36086)\n```\n\n----------------------------------------\n\nTITLE: Tmux Configuration Settings for Breeze\nDESCRIPTION: Example tmux configuration settings that can be added to customize the tmux environment within Breeze, including vi mode settings and keyboard shortcuts for pane navigation.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/02_customizing.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# if you like vi mode instead of emacs\nset-window-option -g mode-keys vi\n\n# will not clear the selection immediately\nbind-key -T copy-mode-vi MouseDragEnd1Pane send-keys -X copy-selection-no-clear\n\n# make it so ctrl+shift+arrow moves the focused pane\nbind -T root C-S-Left select-pane -L\nbind -T root C-S-Right select-pane -R\nbind -T root C-S-Up select-pane -U\nbind -T root C-S-Down select-pane -D\n```\n\n----------------------------------------\n\nTITLE: Clean Up PVC Commands\nDESCRIPTION: Bash commands to delete persistent volume claims for logs and redis when migrating to standard naming convention.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/index.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete pvc -n airflow logs-gta-triggerer-0\nkubectl delete pvc -n airflow logs-gta-worker-0\nkubectl delete pvc -n airflow redis-db-gta-redis-0\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Toctree\nDESCRIPTION: Example of configuring toctree directive in Sphinx for organizing documentation structure with operators and connections.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/11_documentation_building.rst#2025-04-22_snippet_6\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :hidden:\n    :maxdepth: 1\n    :caption: Guides\n\n    Operators <operators/airbyte>\n    Connection types <connections>\n```\n\n----------------------------------------\n\nTITLE: GitCommit References for Version 2.5.0\nDESCRIPTION: References to Git commits for version 2.5.0 updates, including provider bugfix documentation and Databricks Repos operator addition.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_40\n\nLANGUAGE: text\nCODE:\n```\n`d7dbfb7e26 <https://github.com/apache/airflow/commit/d7dbfb7e26a50130d3550e781dc71a5fbcaeb3d2>`__\n`cc920963a6 <https://github.com/apache/airflow/commit/cc920963a69aca840394c3c9e60e0c53235a6fe6>`__\n```\n\n----------------------------------------\n\nTITLE: Displaying Database Migration Table in reStructuredText\nDESCRIPTION: A table listing all database migrations for Apache Airflow, including revision IDs, parent revisions, Airflow versions, and descriptions. This table is automatically updated by a pre-commit script.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/migrations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n .. This table is automatically updated by pre-commit by ``scripts/ci/pre_commit/migration_reference.py``\n .. All table elements are scraped from migration files\n .. Beginning of auto-generated table\n\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| Revision ID             | Revises ID       | Airflow Version   | Description                                                  |\n+=========================+==================+===================+==============================================================+\n| ``29ce7909c52b`` (head) | ``959e216a3abb`` | ``3.0.0``         | Change TI table to have unique UUID id/pk per attempt.       |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``959e216a3abb``        | ``0e9519b56710`` | ``3.0.0``         | Rename ``is_active`` to ``is_stale`` column in ``dag``       |\n|                         |                  |                   | table.                                                       |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``0e9519b56710``        | ``ec62e120484d`` | ``3.0.0``         | Rename run_type from 'dataset_triggered' to                  |\n|                         |                  |                   | 'asset_triggered' in dag_run table.                          |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``ec62e120484d``        | ``be2cc2f742cf`` | ``3.0.0``         | Add new otel span fields.                                    |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``be2cc2f742cf``        | ``d469d27e2a64`` | ``3.0.0``         | Support bundles in DagPriorityParsingRequest.                |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``d469d27e2a64``        | ``16f7f5ee874e`` | ``3.0.0``         | Use ti_id as FK to TaskReschedule.                           |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``16f7f5ee874e``        | ``cf87489a35df`` | ``3.0.0``         | Remove dag.default_view column.                              |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``cf87489a35df``        | ``7645189f3479`` | ``3.0.0``         | Use TI.id as primary key to TaskInstanceNote.                |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``7645189f3479``        | ``e00344393f31`` | ``3.0.0``         | Add try_id to TI and TIH.                                    |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``e00344393f31``        | ``6a9e7a527a88`` | ``3.0.0``         | remove external_trigger field.                               |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``6a9e7a527a88``        | ``33b04e4bfa19`` | ``3.0.0``         | Add DagRun run_after.                                        |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``33b04e4bfa19``        | ``8ea135928435`` | ``3.0.0``         | add new task_instance field scheduled_dttm.                  |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``8ea135928435``        | ``e39a26ac59f6`` | ``3.0.0``         | Add relative fileloc column.                                 |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``e39a26ac59f6``        | ``38770795785f`` | ``3.0.0``         | remove pickled data from dagrun table.                       |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``38770795785f``        | ``5c9c0231baa2`` | ``3.0.0``         | Add asset reference models.                                  |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``5c9c0231baa2``        | ``237cef8dfea1`` | ``3.0.0``         | Remove processor_subdir.                                     |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``237cef8dfea1``        | ``038dc8bc6284`` | ``3.0.0``         | Add deadline alerts table.                                   |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``038dc8bc6284``        | ``e229247a6cb1`` | ``3.0.0``         | update trigger_timeout column in task_instance table to UTC. |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``e229247a6cb1``        | ``eed27faa34e3`` | ``3.0.0``         | Add DagBundleModel.                                          |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``eed27faa34e3``        | ``9fc3fc5de720`` | ``3.0.0``         | Remove pickled data from xcom table.                         |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``9fc3fc5de720``        | ``2b47dc6bc8df`` | ``3.0.0``         | Add references between assets and triggers.                  |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``2b47dc6bc8df``        | ``d03e4a635aa3`` | ``3.0.0``         | add dag versioning.                                          |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``d03e4a635aa3``        | ``d8cd3297971e`` | ``3.0.0``         | Drop DAG pickling.                                           |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``d8cd3297971e``        | ``5f57a45b8433`` | ``3.0.0``         | Add last_heartbeat_at directly to TI.                        |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``5f57a45b8433``        | ``486ac7936b78`` | ``3.0.0``         | Drop task_fail table.                                        |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``486ac7936b78``        | ``d59cbbef95eb`` | ``3.0.0``         | remove scheduler_lock column.                                |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``d59cbbef95eb``        | ``05234396c6fc`` | ``3.0.0``         | Add UUID primary key to ``task_instance`` table.             |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``05234396c6fc``        | ``3a8972ecb8f9`` | ``3.0.0``         | Rename dataset as asset.                                     |\n+-------------------------+------------------+-------------------+--------------------------------------------------------------+\n| ``3a8972ecb8f9``        | ``fb2d4922cd79`` | ``3.0.0``         | Add exception_reason and logical_date to BackfillDagRun.     |\n```\n\n----------------------------------------\n\nTITLE: Linking to Breeze Documentation in reStructuredText\nDESCRIPTION: This snippet creates a hyperlink in reStructuredText format, directing users to the README file in the Breeze documentation internal folder.\nSOURCE: https://github.com/apache/airflow/blob/main/BREEZE.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n`Breeze docs internal folder <dev/breeze/doc/README.rst>`_\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment\nDESCRIPTION: Standard Apache License 2.0 header comment included at the top of the file.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0009-exclude-all-files-from-dockerignore-by-default.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Linking to Apache Software Foundation Privacy Policy in reStructuredText\nDESCRIPTION: This snippet creates a hyperlink to the Apache Software Foundation's privacy policy using reStructuredText syntax. It's used to provide users with access to the full privacy policy that applies to the Apache Airflow project.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/privacy_notice.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n`Privacy Policy of the Apache Software Foundation <https://privacy.apache.org/policies/privacy-policy-public.html>`_\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameter Renaming in Airflow Config\nDESCRIPTION: Lists the configuration parameters being renamed from webserver namespace to fab and api namespaces. These changes affect the config_file, session_backend, and base_url settings.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/49017.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nwebserver.config_file  fab.config_file\nwebserver.session_backend  fab.session_backend\nwebserver.base_url  api.base_url\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: remove soft_fail\nDESCRIPTION: This text is a commit message summary indicating the removal of the 'soft_fail' functionality or parameter, linked to pull request #41710.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nremove soft_fail (#41710)\n```\n\n----------------------------------------\n\nTITLE: Enabling All Integrations\nDESCRIPTION: Command to start Breeze with all integrations including non-testable ones\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/integration_tests.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze --integration all\n```\n\n----------------------------------------\n\nTITLE: Applying D401 Docstring Style to Airbyte Files (Commit Message)\nDESCRIPTION: Commit message stating that the D401 docstring formatting rule (imperative mood for first line) has been applied to files within the Apache Airflow Airbyte provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_21\n\nLANGUAGE: text\nCODE:\n```\nApplied D401 to airbyte files. (#37370)\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Buildx Builder for Cache Refresh\nDESCRIPTION: Commands to set up a Docker buildx builder configuration for local and remote builders to enable multi-platform image building.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/MANUALLY_GENERATING_IMAGE_CACHE_AND_CONSTRAINTS.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker buildx create --name airflow_cache\ndocker buildx create --name airflow_cache --append HOST:PORT\n```\n\n----------------------------------------\n\nTITLE: Git Commit History - Google Cloud Provider\nDESCRIPTION: Git commit log showing changes to the Google Cloud provider codebase including commit hashes, dates and commit messages.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: git\nCODE:\n```\n1ea623a834 2024-01-17 Fix deprecations into the GCP Dataproc links (#36834)\n437d4e4467 2024-01-17 Deprecate AutoMLTrainModelOperator for Vision and Video (#36473)\n8e6bfc2956 2024-01-13 fix assignment of templated field in constructor (#36603)\n4a5da8e05e 2024-01-10 Remove backward compatibility check for KubernetesPodOperator module (#36724)\nc439ab87c4 2024-01-10 Standardize airflow build process and switch to Hatchling build backend (#36537)\n```\n\n----------------------------------------\n\nTITLE: Restructured Text Changelog Entry\nDESCRIPTION: RestructuredText formatted changelog entry showing commit links, dates and descriptions for Redis provider updates\nSOURCE: https://github.com/apache/airflow/blob/main/providers/redis/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n`e9987d5059 <https://github.com/apache/airflow/commit/e9987d50598f70d84cbb2a5d964e21020e81c080>`__  2023-10-13   ``Prepare docs 1st wave of Providers in October 2023 (#34916)``\n`0c8e30e43b <https://github.com/apache/airflow/commit/0c8e30e43b70e9d033e1686b327eb00aab82479c>`__  2023-10-05   ``Bump min airflow version of providers (#34728)``\n`8ecd576de1 <https://github.com/apache/airflow/commit/8ecd576de1043dbea40e5e16b5dc34859cc41725>`__  2023-09-14   ``Refactor shorter defaults in providers (#34347)``\n```\n\n----------------------------------------\n\nTITLE: Defining Elasticsearch Provider Version 4.3.1 in YAML\nDESCRIPTION: YAML configuration specifying version 4.3.1 of the Elasticsearch provider, including commit history and release date.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n4.3.1:\n  Latest change: 2022-11-26\n\n  Commit                                                                                              Committed    Subject\n  ==================================================================================================  ===========  ================================================================\n  `25bdbc8e67 <https://github.com/apache/airflow/commit/25bdbc8e6768712bad6043618242eec9c6632618>`__  2022-11-26   \"Updated docs for RC3 wave of providers (#27937)\"\n  `2e20e9f7eb <https://github.com/apache/airflow/commit/2e20e9f7ebf5f43bf27069f4c0063cdd72e6b2e2>`__  2022-11-24   \"Prepare for follow-up relase for November providers (#27774)\"\n  `80c327bd3b <https://github.com/apache/airflow/commit/80c327bd3b45807ff2e38d532325bccd6fe0ede0>`__  2022-11-24   \"Bump common.sql provider to 1.3.1 (#27888)\"\n```\n\n----------------------------------------\n\nTITLE: Security Manager Class Migration in Python\nDESCRIPTION: Migration path for security management class inheritance in Airflow, moving from legacy module to new provider-based implementation\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41758.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.fab.auth_manager.security_manager.override import FabAirflowSecurityManagerOverride  # New way\n# Replace inheritance from airflow.www.security\n```\n\n----------------------------------------\n\nTITLE: Installing Breeze using PIPX\nDESCRIPTION: Alternative commands to install Breeze development environment tool using PIPX\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_gitpod.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pipx\npipx install -e ./dev/breeze\n```\n\n----------------------------------------\n\nTITLE: Setting Hive Server2 Connection URI in Environment Variable\nDESCRIPTION: Example showing how to configure a Hive Server2 connection using environment variables with URI syntax. The connection includes username, password, host, port, database, and LDAP authentication mechanism.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/connections/hiveserver2.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_HIVESERVER2_DEFAULT='hiveserver2://username:password@hiveserver2-node:80/database?auth_mechanism=LDAP'\n```\n\n----------------------------------------\n\nTITLE: Default DAG Configuration Without Access Control\nDESCRIPTION: Shows DAG configuration without specifying access control, which maintains any existing permissions in the database.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/access-control.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nDAG(\n    dag_id=\"example_indifferent_to_fine_grained_access\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Including Security Information in RST Documentation for Apache Airflow\nDESCRIPTION: This RST directive includes a separate file containing security-related information into the current document. It's used to incorporate standard security details across multiple documentation files.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Moving Provider Dependencies Inside Provider Folders\nDESCRIPTION: Excluded Change (Version 3.1.0): Refactors the project structure by moving provider-specific dependencies into their respective provider folders, referencing pull request #24672.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Move provider dependencies to inside provider folders (#24672)``\n```\n\n----------------------------------------\n\nTITLE: Updating Documentation for June Provider Release in RST\nDESCRIPTION: This commit (1fba5402bb, committed on 2021-06-15) provides further updates to the documentation for the June providers release. Refers to issue #16405.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_28\n\nLANGUAGE: rst\nCODE:\n```\nMore documentation update for June providers release (#16405)\n```\n\n----------------------------------------\n\nTITLE: Including Legal Notices and Shared Sections in Sphinx Documentation (reStructuredText)\nDESCRIPTION: This snippet presents reStructuredText directives for Sphinx documentation. It details the Apache License, copyright statements, and utilizes the \"include\" directive to incorporate a shared security notice. Prerequisites include Sphinx and having the referenced security file available at the path specified. This ensures legal notices are always present and up to date wherever the file is included. The main input is the file inclusion path; output is the merged text in the rendered documentation. This approach only applies to Sphinx documentation systems supporting the reStructuredText standard.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/io/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Run mypy checks for full packages in CI\nDESCRIPTION: This commit message (hash f7b663d9af, dated 2024-01-07) indicates an update to the Continuous Integration (CI) process to include running mypy type checks for full packages (issue #36638).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n``Run mypy checks for full packages in CI (#36638)``\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Remove unnecessary return value from jenkins_request_with_headers\nDESCRIPTION: This text is a commit message summary describing the removal of an unnecessary return value from the 'jenkins_request_with_headers' function, linked to pull request #43207.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nRemove unnecessary return value from 'jenkins_request_with_headers' (#43207)\n```\n\n----------------------------------------\n\nTITLE: Fixing Databricks SQL operator serialization\nDESCRIPTION: Fixes serialization for the Databricks SQL operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n\"Fix Databricks SQL operator serialization (#31780)\"\n```\n\n----------------------------------------\n\nTITLE: Oracle Bulk Insert Fix Reference\nDESCRIPTION: Reference to a bug fix addressing an issue with Oracle bulk insert operations when a leftover chunk is empty.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n* ``Fix oracle bulk insert issue when leftover chunk is empty (#43467)``\n```\n\n----------------------------------------\n\nTITLE: Entering Breeze Development Environment\nDESCRIPTION: Command to enter the Breeze development environment for Apache Airflow\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/18_contribution_workflow.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Versions as String\nDESCRIPTION: Provides the Python versions for the build as a space-separated string. This example specifies '3.9'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_18\n\nLANGUAGE: text\nCODE:\n```\n3.9\n```\n\n----------------------------------------\n\nTITLE: Handling Custom XCom Backend in Task SDK (AIP-72)\nDESCRIPTION: Commit message detailing the implementation for handling custom XCom backends within the Task SDK framework as part of AIP-72. References pull request #47339.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nAIP-72: Handle Custom XCom Backend on Task SDK (#47339)\n```\n\n----------------------------------------\n\nTITLE: Fixing Typo in the Word 'release'\nDESCRIPTION: This commit message notes a simple correction of a typographical error in the word 'release' found somewhere in the Airflow codebase or documentation. It references pull request #10528.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_76\n\nLANGUAGE: text\nCODE:\n```\nFix typo in the word \"release\" (#10528)\n```\n\n----------------------------------------\n\nTITLE: Installing JQ on macOS\nDESCRIPTION: Command for installing the jq JSON processor on macOS using Homebrew package manager\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/18_contribution_workflow.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbrew install jq\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Session Authentication Backend in Airflow FAB (INI)\nDESCRIPTION: Specifies the default authentication backend for the FAB API within the `airflow.cfg` file. This setting uses the user's web session for API authentication.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/api-authentication.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[fab]\nauth_backends = airflow.providers.fab.auth_manager.api.auth.backend.session\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Faster airflow_version imports\nDESCRIPTION: This text is a commit message summary describing an optimization to make imports related to 'airflow_version' faster, linked to pull request #39552.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nFaster 'airflow_version' imports (#39552)\n```\n\n----------------------------------------\n\nTITLE: Adding Documentation for January 2021 Provider Release\nDESCRIPTION: Excluded Change (Version 2.2.1): Includes documentation updates for the January 2021 provider release, referencing pull request #21257.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_22\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Add documentation for January 2021 providers release (#21257)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Remove empty lines in generated changelog\nDESCRIPTION: This commit message (hash 706878ec35, dated 2023-11-04) describes a minor fix to remove empty lines that were appearing in the automatically generated changelog (issue #35436).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_28\n\nLANGUAGE: text\nCODE:\n```\n``Remove empty lines in generated changelog (#35436)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add mechanism to suspend providers (#30422)\nDESCRIPTION: This commit message, associated with version 3.2.0, introduces a feature allowing specific Apache Airflow Providers to be suspended or disabled. Commit hash: d23a3bbed8, Date: 2023-04-04.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n``Add mechanism to suspend providers (#30422)``\n```\n\n----------------------------------------\n\nTITLE: Reading Previous Asset Events\nDESCRIPTION: Shows how to access and read information from previously emitted asset events.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/assets.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@asset(schedule=None)\ndef post_process_s3_file(context, write_to_s3):  # Declaring an inlet to write_to_s3.\n    events = context[\"inlet_events\"][write_to_s3]\n    last_row_count = events[-1].extra[\"row_count\"]\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in reStructuredText\nDESCRIPTION: This snippet uses a reStructuredText directive to include an external file containing security-related documentation. The included file is located in a common development directory for Sphinx extensions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Commit Message: SFTP hook preference for paramiko key over key file path\nDESCRIPTION: This commit message, for commit 83eb80d4ec, details a change in the SFTP hook's behavior to prioritize the SSH paramiko key specified in the connection over the key file path. This modification is linked to issue #18988.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n``SFTP hook to prefer the SSH paramiko key over the key file path (#18988)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs for July 2023 wave of Providers (RC2) (#32381)\nDESCRIPTION: This commit message, associated with version 3.2.1, indicates the preparation of documentation for the second release candidate (RC2) of the July 2023 wave of Apache Airflow Providers. Commit hash: 225e3041d2, Date: 2023-07-06.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs for July 2023 wave of Providers (RC2) (#32381)``\n```\n\n----------------------------------------\n\nTITLE: Generating Diagrams with Breeze\nDESCRIPTION: Command to generate all diagrams using the Breeze development environment tool.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/17_architecture_diagrams.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze static-checks --type generate-airflow-diagrams --all-files\n```\n\n----------------------------------------\n\nTITLE: Teardown with Empty Operator\nDESCRIPTION: Demonstrates using EmptyOperator to limit setup scope.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/setup-and-teardown.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncreate_cluster >> run_query >> other_task\nrun_query >> EmptyOperator(task_id=\"cluster_teardown\").as_teardown(setups=create_cluster)\n```\n\n----------------------------------------\n\nTITLE: Formatting Version Headers in Markdown\nDESCRIPTION: This snippet demonstrates how version headers are formatted in the changelog using Markdown. It includes the version number and a line of dots for visual separation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/smtp/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n1.5.0\n.....\n```\n\n----------------------------------------\n\nTITLE: Cherry Pick Command\nDESCRIPTION: Git command to cherry-pick a commit with reference to the original commit.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit cherry-pick <hash-commit> -x\n```\n\n----------------------------------------\n\nTITLE: Updating Import Statement for TriggerDagRunOperator in Python\nDESCRIPTION: This code snippet demonstrates the required change in the import statement for TriggerDagRunOperator. It shows the old import path and the new import path after moving to the standard provider.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/44053.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nairflow.operators.trigger_dagrun import TriggerDagRunOperator  airflow.providers.standard.operators.trigger_dagrun.TriggerDagRunOperator\n```\n\n----------------------------------------\n\nTITLE: Apache Airflow System Information Output\nDESCRIPTION: Displays the output of the 'airflow info' command, which provides detailed information about the Airflow installation, system configuration, and installed providers.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/modules_management.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nApache Airflow: 2.0.0b3\n\nSystem info\nOS              | Linux\narchitecture    | x86_64\nuname           | uname_result(system='Linux', node='85cd7ab7018e', release='4.19.76-linuxkit', version='#1 SMP Tue May 26 11:42:35 UTC 2020', machine='x86_64', processor='')\nlocale          | ('en_US', 'UTF-8')\npython_version  | 3.9.6 (default, Nov 25 2020, 02:47:44)  [GCC 8.3.0]\npython_location | /usr/local/bin/python\n\nTools info\ngit             | git version 2.20.1\nssh             | OpenSSH_7.9p1 Debian-10+deb10u2, OpenSSL 1.1.1d  10 Sep 2019\nkubectl         | NOT AVAILABLE\ngcloud          | NOT AVAILABLE\ncloud_sql_proxy | NOT AVAILABLE\nmysql           | mysql  Ver 8.0.22 for Linux on x86_64 (MySQL Community Server - GPL)\nsqlite3         | 3.27.2 2019-02-25 16:06:06 bd49a8271d650fa89e446b42e513b595a717b9212c91dd384aab871fc1d0alt1\npsql            | psql (PostgreSQL) 11.9 (Debian 11.9-0+deb10u1)\n\nPaths info\nairflow_home    | /root/airflow\nsystem_path     | /usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\npython_path     | /usr/local/bin:/opt/airflow:/files/plugins:/usr/local/lib/python38.zip:/usr/local/lib/python3.9:/usr/\n                | local/lib/python3.9/lib-dynload:/usr/local/lib/python3.9/site-packages:/files/dags:/root/airflow/conf\n                | ig:/root/airflow/plugins\nairflow_on_path | True\n\nConfig info\nexecutor             | LocalExecutor\ntask_logging_handler | airflow.utils.log.file_task_handler.FileTaskHandler\nsql_alchemy_conn     | postgresql+psycopg2://postgres:airflow@postgres/airflow\ndags_folder          | /files/dags\nplugins_folder       | /root/airflow/plugins\nbase_log_folder      | /root/airflow/logs\n\nProviders info\napache-airflow-providers-amazon           | 1.0.0b2\napache-airflow-providers-apache-cassandra | 1.0.0b2\napache-airflow-providers-apache-druid     | 1.0.0b2\napache-airflow-providers-apache-hdfs      | 1.0.0b2\napache-airflow-providers-apache-hive      | 1.0.0b2\n```\n\n----------------------------------------\n\nTITLE: Moving Standard, Alibaba, common.sql Providers to New Structure\nDESCRIPTION: Commit message describing the migration of the 'standard', 'alibaba', and 'common.sql' providers to a new organizational structure. References pull request #45964.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_39\n\nLANGUAGE: plaintext\nCODE:\n```\nmove standard, alibaba and common.sql provider to the new structure (#45964)\n```\n\n----------------------------------------\n\nTITLE: Introducing 'transfers' Packages in Airflow\nDESCRIPTION: This commit message marks the introduction of dedicated 'transfers' packages within the Airflow provider system, likely grouping operators designed for transferring data between systems. It references pull request #9320.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_85\n\nLANGUAGE: text\nCODE:\n```\nIntroduce 'transfers' packages (#9320)\n```\n\n----------------------------------------\n\nTITLE: Google Cloud Connection with URI Format\nDESCRIPTION: Example demonstrating how to configure a Google Cloud connection using URI format with various parameters including key path, scope, project, and retries.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT='google-cloud-platform://?key_path=%2Fkeys%2Fkey.json&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform&project=airflow&num_retries=5'\n```\n\n----------------------------------------\n\nTITLE: License Header RST Block\nDESCRIPTION: Apache License 2.0 header documentation formatted in RST markup language.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n```\n\n----------------------------------------\n\nTITLE: Displaying License Terms and Including Shared Installation Docs - reStructuredText\nDESCRIPTION: This snippet embeds the project's licensing text via indented comment blocks and includes an external file containing installation instructions, both using reStructuredText. There are no code dependencies, but the file relies on Sphinx's \".include::\" directive and the presence of the referenced file for correct rendering. Inputs are the static license text and the source path for included documentation; output is the generated documentation page displaying both sections.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \\\"License\\\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Auto-applying apply_default Decorator in RST\nDESCRIPTION: This commit (37681bca00, committed on 2021-05-07) implements automatic application of the 'apply_default' decorator, likely simplifying operator definitions. Refers to issue #15667.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_32\n\nLANGUAGE: rst\nCODE:\n```\nAuto-apply apply_default decorator (#15667)\n```\n\n----------------------------------------\n\nTITLE: Warning Message for Apply Default Decorator Removal in Markdown\nDESCRIPTION: A warning message indicating that the apply_default decorator has been removed, requiring Airflow 2.1.0+ for this provider version. It also provides instructions for users with older Airflow versions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n.. warning:: Due to apply_default decorator removal, this version of the provider requires Airflow 2.1.0+.\n   If your Airflow version is < 2.1.0, and you want to install this provider version, first upgrade\n   Airflow to at least version 2.1.0. Otherwise your Airflow package version will be upgraded\n   automatically and you will have to manually run ``airflow upgrade db`` to complete the migration.\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents with DocToc in Markdown\nDESCRIPTION: This snippet demonstrates the use of DocToc to automatically generate a table of contents for the markdown file. It includes start and end markers to allow for automatic updates.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/tests_common/test_utils/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*\n\n- [Utilities for use in tests.](#utilities-for-use-in-tests)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n```\n\n----------------------------------------\n\nTITLE: Listing Package Requirements for SSH Provider in Apache Airflow\nDESCRIPTION: Table showing the required PIP packages and their versions for the SSH provider in Apache Airflow. Includes apache-airflow, paramiko, and sshtunnel with their respective version requirements.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n``apache-airflow``  ``>=2.9.0``\n``paramiko``        ``>=2.9.0``\n``sshtunnel``       ``>=0.3.2``\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Fixing Documentation for Provider Update October 2023 (3rd Wave)\nDESCRIPTION: Addresses issues found in the documentation prepared for the third wave of provider updates in October 2023, referenced by issue #35233.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs 3rd wave of Providers October 2023 - FIX (#35233)\n```\n\n----------------------------------------\n\nTITLE: Commit Hash Reference in RST Format\nDESCRIPTION: RST formatted link to commit hash with associated commit date and message\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/druid/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n`0de31bd73a <https://github.com/apache/airflow/commit/0de31bd73a8f41dded2907f0dee59dfa6c1ed7a1>`__  2022-06-29   ``Move provider dependencies to inside provider folders (#24672)``\n```\n\n----------------------------------------\n\nTITLE: Refactoring String Conversion Usage in Providers\nDESCRIPTION: Refactors the usage of `str()` conversions within provider code, likely for simplification, consistency, or performance improvements, as per issue #34320.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_15\n\nLANGUAGE: text\nCODE:\n```\nRefactor usage of str() in providers (#34320)\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Entries\nDESCRIPTION: Git commit history entries formatted in Markdown showing changes to the Airbyte provider across multiple versions\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_34\n\nLANGUAGE: markdown\nCODE:\n```\n`97496ba2b4 <https://github.com/apache/airflow/commit/97496ba2b41063fa24393c58c5c648a0cdb5a7f8>`__  2021-12-31   ``Update documentation for provider December 2021 release (#20523)``\n`d56e7b56bb <https://github.com/apache/airflow/commit/d56e7b56bb9827daaf8890557147fd10bdf72a7e>`__  2021-12-30   ``Fix template_fields type to have MyPy friendly Sequence type (#20571)``\n```\n\n----------------------------------------\n\nTITLE: Defining Ruff Migration Rule for Airflow Hook Imports\nDESCRIPTION: Specifies the AIR303 ruff rule for updating import paths of filesystem, package_index, and subprocess hooks to use the standard provider.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/42794.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* ruff\n\n  * AIR303\n\n    * [x] ``airflow.hooks.filesystem.*``  ``airflow.providers.standard.hooks.filesystem.*``\n    * [x] ``airflow.hooks.package_index.*``  ``airflow.providers.standard.hooks.package_index.*``\n    * [x] ``airflow.hooks.subprocess.*``  ``airflow.providers.standard.hooks.subprocess.*``\n```\n\n----------------------------------------\n\nTITLE: Creating Minor Branch in Airflow\nDESCRIPTION: Command to automate maintenance tasks when creating a new minor branch of Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management create-minor-branch\n```\n\n----------------------------------------\n\nTITLE: Importing ProductSet Object for Google Cloud Vision in Python\nDESCRIPTION: Imports the ProductSet class from Google Cloud Vision library used for product set operations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud.vision_v1.types import ProductSet\n```\n\n----------------------------------------\n\nTITLE: Verifying Google Cloud Storage Log Retrieval\nDESCRIPTION: This code block demonstrates the expected output when verifying that logs are being correctly retrieved from Google Cloud Storage. It shows the log path and sample log entries.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/logging/gcs.rst#2025-04-22_snippet_1\n\nLANGUAGE: none\nCODE:\n```\n*** Reading remote log from gs://<bucket where logs should be persisted>/example_bash_operator/run_this_last/2017-10-03T00:00:00/16.log.\n[2017-10-03 21:57:50,056] {cli.py:377} INFO - Running on host chrisr-00532\n[2017-10-03 21:57:50,093] {base_task_runner.py:115} INFO - Running: ['bash', '-c', 'airflow tasks run example_bash_operator run_this_last 2017-10-03T00:00:00 --job-id 47 --raw -S DAGS_FOLDER/example_dags/example_bash_operator.py']\n[2017-10-03 21:57:51,264] {base_task_runner.py:98} INFO - Subtask: [2017-10-03 21:57:51,263] {__init__.py:45} INFO - Using executor SequentialExecutor\n[2017-10-03 21:57:51,306] {base_task_runner.py:98} INFO - Subtask: [2017-10-03 21:57:51,306] {models.py:186} INFO - Filling up the DagBag from /airflow/dags/example_dags/example_bash_operator.py\n```\n\n----------------------------------------\n\nTITLE: Installing Amazon Provider for Airflow (INI)\nDESCRIPTION: This command shows how to install the Amazon provider for Airflow using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_11\n\nLANGUAGE: ini\nCODE:\n```\npip install 'apache-airflow[amazon]'\n```\n\n----------------------------------------\n\nTITLE: Moving Airflow Connection Guides to Provider Documentation\nDESCRIPTION: This commit message signifies the relocation of connection setup guides from a central location to the documentation packages specific to each provider. It references pull request #12653.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_54\n\nLANGUAGE: text\nCODE:\n```\nMove connection guides to provider documentation packages (#12653)\n```\n\n----------------------------------------\n\nTITLE: Building Release Notes with Towncrier\nDESCRIPTION: These commands use Towncrier to build and preview release notes for the new Apache Airflow version.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ntowncrier build --draft --version=${VERSION_WITHOUT_RC} --date=2021-12-15 --dir . --config newsfragments/config.toml\n```\n\nLANGUAGE: shell\nCODE:\n```\n./dev/airflow-github changelog v2-3-stable v2-3-test\n```\n\n----------------------------------------\n\nTITLE: Removing Old Releases\nDESCRIPTION: Commands to remove old Helm Chart releases from SVN repository.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_31\n\nLANGUAGE: shell\nCODE:\n```\ncd airflow-release/helm-chart\nexport PREVIOUS_VERSION=1.0.0\nsvn rm ${PREVIOUS_VERSION}\nsvn commit -m \"Remove old Helm Chart release: ${PREVIOUS_VERSION}\"\n```\n\n----------------------------------------\n\nTITLE: Expanding Watchtower Version Support in Python\nDESCRIPTION: Allows a wider range of watchtower versions to be used with the provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"Allow a wider range of watchtower versions (#35713)\"\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare RC1 docs for June 2023 wave of Providers (#32001)\nDESCRIPTION: This commit message, associated with version 3.2.1 (Latest change: 2023-06-20 indicates context for this block), describes the preparation of documentation for the first release candidate (RC1) of the June 2023 wave of Apache Airflow Providers. Commit hash: 79bcc2e668, Date: 2023-06-20.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n``Prepare RC1 docs for June 2023 wave of Providers (#32001)``\n```\n\n----------------------------------------\n\nTITLE: Allowing Non-Prefixed Extra Fields in AsanaHook\nDESCRIPTION: Updates AsanaHook to allow and prefer non-prefixed extra fields. Users should update their connections to replace prefixed fields like 'extra__asana__workspace' with 'workspace'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"Allow and prefer non-prefixed extra fields for AsanaHook (#27043)\"\n```\n\n----------------------------------------\n\nTITLE: Including External Security Documentation in RST\nDESCRIPTION: A Sphinx directive that includes security-related documentation from an external file. This allows for reusing common security documentation across multiple locations in the Apache Airflow documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opsgenie/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Header in HTML Comments\nDESCRIPTION: This snippet contains the Apache License 2.0 header enclosed in HTML comments. It specifies the terms under which the software is distributed and the conditions for its use.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/README.md#2025-04-22_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: HTML TOC Container\nDESCRIPTION: HTML markup for table of contents section generated by DocToc tool\nSOURCE: https://github.com/apache/airflow/blob/main/dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*\n```\n\n----------------------------------------\n\nTITLE: Re-applying Updated Version Numbers for Dec 2nd Wave (Commit Message)\nDESCRIPTION: Commit message stating that updated version numbers have been re-applied to the second wave of Apache Airflow provider releases in December, including Airbyte.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_27\n\nLANGUAGE: text\nCODE:\n```\nRe-apply updated version numbers to 2nd wave of providers in December (#36380)\n```\n\n----------------------------------------\n\nTITLE: Updating Wrong Commit Hash in Airflow Backport Provider Changes\nDESCRIPTION: This commit message notes the correction of an incorrect commit hash mentioned in the change history related to backport providers. It references pull request #12390.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_57\n\nLANGUAGE: text\nCODE:\n```\nUpdate wrong commit hash in backport provider changes (#12390)\n```\n\n----------------------------------------\n\nTITLE: API Endpoints Documentation Table in Markdown\nDESCRIPTION: A markdown table listing all available API endpoints for Apache Airflow, including their HTTP methods, paths, and associated class methods.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\nClass | Method | HTTP request | Description\n------------ | ------------- | ------------- | -------------\n*ConfigApi* | [**get_config**](docs/ConfigApi.md#get_config) | **GET** /config | Get current configuration\n*ConnectionApi* | [**delete_connection**](docs/ConnectionApi.md#delete_connection) | **DELETE** /connections/{connection_id} | Delete a connection\n*ConnectionApi* | [**get_connection**](docs/ConnectionApi.md#get_connection) | **GET** /connections/{connection_id} | Get a connection\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Synchronizes updated changelog after bugfix release\nDESCRIPTION: Linked to commit cbf8001d76, this message describes the action of updating and synchronizing the changelog file following a bugfix release, ensuring it reflects the latest changes. Related to issue #16464.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_18\n\nLANGUAGE: text\nCODE:\n```\n``Synchronizes updated changelog after buggfix release (#16464)``\n```\n\n----------------------------------------\n\nTITLE: Databricks Provider Version 7.3.2 Changelog\nDESCRIPTION: Commit log showing changes made in version 7.3.2 of the provider package, including bug fixes for notebook operator dependency issues and code cleanup.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n7.3.2\n.....\n\nLatest change: 2025-04-14\n\n==================================================================================================  ===========  =============================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =============================================================================\n`cb295c351a <https://github.com/apache/airflow/commit/cb295c351a016c0a10cab07f2a628b865cff3ca3>`__  2025-04-14   ``remove superfluous else block (#49199)``\n`9dcce2f71b <https://github.com/apache/airflow/commit/9dcce2f71b80e829f4b216d2ec48a8f9fbe29b1f>`__  2025-04-13   ``Fixing DatabricksNotebookOperator invalid dependency graph issue (#48492)``\n==================================================================================================\n```\n\n----------------------------------------\n\nTITLE: Changelog Version Header in RST\nDESCRIPTION: RST formatted version number header with underscore decoration\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/druid/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n3.0.0\n.....\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Enable string normalization in python formatting - providers (#27205)\nDESCRIPTION: This commit message, associated with version 3.1.0, enables string normalization rules for Python code formatting specifically within the provider codebase. Commit hash: 2a34dc9e84, Date: 2022-10-23.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_21\n\nLANGUAGE: text\nCODE:\n```\n``Enable string normalization in python formatting - providers (#27205)``\n```\n\n----------------------------------------\n\nTITLE: HTML Table of Contents Comment\nDESCRIPTION: Auto-generated table of contents using DocToc, containing navigation links to different sections of the document.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0014-fix-root-ownership-after-exiting-docker-command.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*\n\n- [14. Fix root ownership after exiting docker command](#14-fix-root-ownership-after-exiting-docker-command)\n  - [Status](#status)\n  - [Context](#context)\n  - [Decision](#decision)\n  - [Consequences](#consequences)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n```\n\n----------------------------------------\n\nTITLE: Airflow BaseHook Migration Rule for ruff\nDESCRIPTION: Specifies the migration rule for the ruff code analysis tool to replace the deprecated get_connections() function with get_connection() in Apache Airflow's BaseHook.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41733.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* ``airflow.hooks.base.BaseHook.get_connections``  ``airflow.hooks.base.BaseHook.get_connection``\n```\n\n----------------------------------------\n\nTITLE: Updating Provider to Use Airbyte API Python SDK (Commit Message)\nDESCRIPTION: Commit message indicating an update to the Apache Airflow Airbyte provider to utilize the official Airbyte API Python SDK. This likely modernizes the integration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nUpdate provider to use Airbyte API Python SDK (#41122)\n```\n\n----------------------------------------\n\nTITLE: Pinning airbyte-api to 0.51.0 (Commit Message)\nDESCRIPTION: Commit message specifying that the airbyte-api dependency for the Apache Airflow Airbyte provider is pinned to version 0.51.0. This ensures compatibility and stability.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nPin airbyte-api to 0.51.0 (#42154) (#42155)\n```\n\n----------------------------------------\n\nTITLE: PagerDuty Provider Version 2.0.1 Release Notes\nDESCRIPTION: Version history tracking commit hashes, dates and subjects for PagerDuty provider changes\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pagerduty/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n2.0.1\n.....\n\nLatest change: 2021-08-30\n\n==================================================================================================  ===========  =================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =================================================================\n`0a68588479 <https://github.com/apache/airflow/commit/0a68588479e34cf175d744ea77b283d9d78ea71a>`__  2021-08-30   ``Add August 2021 Provider's documentation (#17890)``\n`87f408b1e7 <https://github.com/apache/airflow/commit/87f408b1e78968580c760acb275ae5bb042161db>`__  2021-07-26   ``Prepares docs for Rc2 release of July providers (#17116)``\n`b916b75079 <https://github.com/apache/airflow/commit/b916b7507921129dc48d6add1bdc4b923b60c9b9>`__  2021-07-15   ``Prepare documentation for July release of providers. (#17015)``\n`866a601b76 <https://github.com/apache/airflow/commit/866a601b76e219b3c043e1dbbc8fb22300866351>`__  2021-06-28   ``Removes pylint from our toolchain (#16682)``\n```\n\n----------------------------------------\n\nTITLE: Fixing Circular Import Error from Airflow Version Check\nDESCRIPTION: Excluded Change (Version > 3.1.1): Addresses a circular import error occurring in providers caused by the Airflow version check mechanism, referencing pull request #31379.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Fixing circular import error in providers caused by airflow version check (#31379)``\n```\n\n----------------------------------------\n\nTITLE: Adding Official Provider Download Page in RST\nDESCRIPTION: This commit (1cb456cba1, committed on 2021-09-12) introduces an official download page for Airflow providers. Refers to issue #18187.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_17\n\nLANGUAGE: rst\nCODE:\n```\nAdd official download page for providers (#18187)\n```\n\n----------------------------------------\n\nTITLE: Listing Selectively Affected Providers (String)\nDESCRIPTION: A space-separated string listing providers that are affected by changes in a PR, used when tests are run selectively. This example lists 'airbyte' and 'http'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_31\n\nLANGUAGE: text\nCODE:\n```\nairbyte http\n```\n\n----------------------------------------\n\nTITLE: Improving Provider Documentation and README Structure\nDESCRIPTION: Enhances the structure and content of provider documentation, including README files, to improve clarity and usability for users, tracked in issue #32125.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_32\n\nLANGUAGE: text\nCODE:\n```\nImprove provider documentation and README structure (#32125)\n```\n\n----------------------------------------\n\nTITLE: Preparing Docs for 2nd Wave Jan 2024 (Commit Message)\nDESCRIPTION: Commit message indicating the preparation of documentation for the second wave of Apache Airflow provider releases in January 2024, including the Airbyte provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_23\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs 2nd wave of Providers January 2024 (#36945)\n```\n\n----------------------------------------\n\nTITLE: Fetching AWS Task Logs in AwsTaskLogFetcher\nDESCRIPTION: Fix for missing logs in AwsTaskLogFetcher class.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"Fix 'AwsTaskLogFetcher' missing logs (#41515)\"\n```\n\n----------------------------------------\n\nTITLE: Commit Hash Reference - June 2023 Provider Updates\nDESCRIPTION: Git commit hash references for documentation and Python version updates in June 2023.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n79bcc2e668 2023-06-20 \"Prepare RC1 docs for June 2023 wave of Providers (#32001)\"\n8b146152d6 2023-06-20 \"Add note about dropping Python 3.7 for providers (#32015)\"\na473facf6c 2023-06-01 \"Add D400 pydocstyle check - Apache providers only (#31424)\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Breeze Build-Docs Help Information\nDESCRIPTION: This command displays the help information for the build-docs command in the Breeze development environment, showing all available arguments.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/docs/README.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs --help\n```\n\n----------------------------------------\n\nTITLE: Adding August 2021 Provider Documentation in RST\nDESCRIPTION: This commit (0a68588479, committed on 2021-08-30) incorporates documentation for the August 2021 release of Airflow providers. Refers to issue #17890.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_19\n\nLANGUAGE: rst\nCODE:\n```\nAdd August 2021 Provider's documentation (#17890)\n```\n\n----------------------------------------\n\nTITLE: Iceberg Provider Version History\nDESCRIPTION: ReStructuredText formatted changelog showing version history and commit details for the Apache Iceberg provider, including version numbers, dates, commit hashes, and descriptions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/iceberg/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nPackage apache-airflow-providers-apache-iceberg\n------------------------------------------------------\n\n`Iceberg <https://iceberg.apache.org/>`__\n\n\nThis is detailed commit list of changes for versions provider package: ``apache.iceberg``.\nFor high-level changelog, see :doc:`package information including changelog <index>`.\n```\n\n----------------------------------------\n\nTITLE: Including Relative Path Documentation\nDESCRIPTION: Example of including a file using relative path reference in Sphinx documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/11_documentation_building.rst#2025-04-22_snippet_7\n\nLANGUAGE: rst\nCODE:\n```\n``.. include:: ../_partials/prerequisite_tasks.rst``\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare docs 3rd wave May 2024\nDESCRIPTION: This text is a commit message summary for preparing documentation related to the third wave of Apache Airflow provider releases in May 2024, linked to pull request #39738.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 3rd wave May 2024 (#39738)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs for May 2022 provider's release (#24231)\nDESCRIPTION: This commit message, associated with version 3.0.0, signifies the preparation of documentation for the May 2022 release of Apache Airflow Providers. Commit hash: aeabe994b3, Date: 2022-06-07.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_31\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs for May 2022 provider's release (#24231)``\n```\n\n----------------------------------------\n\nTITLE: Fixing MyPy for Elasticsearch, Oracle, Yandex Providers in RST\nDESCRIPTION: This commit (6e51608f28, committed on 2021-12-16) addresses MyPy type checking errors for the Elasticsearch, Oracle, and Yandex providers. Refers to issue #20344.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_13\n\nLANGUAGE: rst\nCODE:\n```\nFix mypy for providers: elasticsearch, oracle, yandex (#20344)\n```\n\n----------------------------------------\n\nTITLE: Adding 'callproc' Method to Oracle Hook in RST\nDESCRIPTION: This commit (c7f36f25cb, committed on 2021-12-13) introduces a 'callproc' method to the Oracle Hook, enabling the execution of stored procedures. Refers to issue #20072.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_14\n\nLANGUAGE: rst\nCODE:\n```\nAdd method 'callproc' on Oracle hook (#20072)\n```\n\n----------------------------------------\n\nTITLE: Including External reStructuredText File using Sphinx Directive\nDESCRIPTION: Uses the Sphinx `include` directive to embed the content of the `security.rst` file. The path specified (`/../../../devel-common/src/sphinx_exts/includes/security.rst`) is relative to the source directory root as indicated by the leading `/`, pointing to a common development resource. This directive instructs the Sphinx build system to insert the content of the target file at this location.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/qdrant/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Look for 'extra__' instead of 'extra_' in 'get_field' (#27489)\nDESCRIPTION: This commit message, associated with version 3.1.0, modifies the 'get_field' function to look for connection extra fields prefixed with 'extra__' instead of 'extra_'. Commit hash: 680965b2ea, Date: 2022-11-03.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_19\n\nLANGUAGE: text\nCODE:\n```\n``Look for 'extra__' instead of 'extra_' in 'get_field' (#27489)``\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare docs 2nd wave of Providers January 2024\nDESCRIPTION: This text is a commit message summary for preparing documentation related to the second wave of Apache Airflow provider releases in January 2024, linked to pull request #36945.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_24\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 2nd wave of Providers January 2024 (#36945)\n```\n\n----------------------------------------\n\nTITLE: Fixing Typo in Airflow PIP Upgrade Command\nDESCRIPTION: This commit message notes a correction of a typographical error in the PIP upgrade command provided within Airflow's documentation or messages. It references pull request #13148.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_47\n\nLANGUAGE: text\nCODE:\n```\nFix typo in pip upgrade command :( (#13148)\n```\n\n----------------------------------------\n\nTITLE: Setting Machine Type for a Compute Engine Instance with ComputeEngineSetMachineTypeOperator in Python\nDESCRIPTION: Changes the machine type of a Google Compute Engine instance using the ComputeEngineSetMachineTypeOperator. This snippet shows how to use the operator with and without specifying a project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineSetMachineTypeOperator(\n    project_id=GCP_PROJECT_ID,\n    zone=GCE_ZONE,\n    resource_id=INSTANCE_NAME,\n    body=SET_MACHINE_TYPE_BODY,\n    task_id=\"gcp_compute_set_machine_type\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nComputeEngineSetMachineTypeOperator(\n    zone=GCE_ZONE,\n    resource_id=INSTANCE_NAME,\n    body=SET_MACHINE_TYPE_BODY,\n    task_id=\"gcp_compute_set_machine_type\",\n)\n```\n\n----------------------------------------\n\nTITLE: Method Renames in HookLineageCollector\nDESCRIPTION: Updates to method names in the HookLineageCollector class, changing Dataset references to Asset\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41348.significant.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nairflow.lineage.hook.HookLineageCollector.create_dataset  airflow.lineage.hook.HookLineageCollector.create_asset\nairflow.lineage.hook.HookLineageCollector.add_input_dataset  airflow.lineage.hook.HookLineageCollector.add_input_asset\nairflow.lineage.hook.HookLineageCollector.add_output_dataset  airflow.lineage.hook.HookLineageCollector.dd_output_asset\nairflow.lineage.hook.HookLineageCollector.collected_datasets  airflow.lineage.hook.HookLineageCollector.collected_assets\n```\n\n----------------------------------------\n\nTITLE: Package Requirements Block\nDESCRIPTION: Requirements table showing dependencies needed for the Apache Kylin provider package\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kylin/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n\"apache-airflow\"  \">=2.9.0\"\n\"kylinpy\"         \">2.7.0\"\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Identifying TaskMixin Removal and DependencyMixin Usage in Apache Airflow\nDESCRIPTION: This snippet highlights the removal of the TaskMixin class and its replacement with DependencyMixin. It provides the full import path for both classes to help developers locate and update their code.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41394.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nThe ``airflow.models.taskMixin.TaskMixin`` class has been removed. It was previously deprecated in favor of the ``airflow.models.taskMixin.DependencyMixin`` class.\n```\n\n----------------------------------------\n\nTITLE: Curl Example for DAG Pause Operation\nDESCRIPTION: Demonstrates how to pause a DAG using curl with basic authentication.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X PATCH 'https://example.com/api/v1/dags/{dag_id}?update_mask=is_paused' \\\n-H 'Content-Type: application/json' \\\n--user \"username:password\" \\\n-d '{\n    \"is_paused\": true\n}'\n```\n\n----------------------------------------\n\nTITLE: Generating Issue Title for RC Testing Status in Shell\nDESCRIPTION: Uses a 'here document' (`cat <<EOF ... EOF`) to generate the title string for a new issue that will track the testing status of the specific Helm chart release candidate. Requires `${VERSION}` and `${VERSION_SUFFIX}` variables.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\nStatus of testing of Apache Airflow Helm Chart ${VERSION}${VERSION_SUFFIX}\nEOF\n```\n\n----------------------------------------\n\nTITLE: Fixing Changelog for Jan 2022 Provider Release in RST\nDESCRIPTION: This commit (d94fa37830, committed on 2022-02-08) corrects the changelog information for the delayed January 2022 provider release. Refers to issue #21439.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\nFixed changelog for January 2022 (delayed) provider's release (#21439)\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Config Value from Lockbox in Airflow\nDESCRIPTION: This command shows how to use Airflow CLI to retrieve a configuration value that is stored in Yandex Cloud Lockbox Secret Backend.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_19\n\nLANGUAGE: console\nCODE:\n```\n$ airflow config get-value sentry sentry_dsn\nhttps://public@sentry.example.com/1\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow Logging for Google Stackdriver\nDESCRIPTION: Configuration snippet for airflow.cfg to enable remote logging to Google Stackdriver. Defines the remote logging settings including the base log folder path and enables remote logging functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/logging/stackdriver.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\n# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.\n# Users must supply an Airflow connection id that provides access to the storage\n# location. If remote_logging is set to true, see UPDATING.md for additional\n# configuration requirements.\nremote_logging = True\nremote_base_log_folder = stackdriver:///logs-name\n```\n\n----------------------------------------\n\nTITLE: Running Airflow DAG Python File\nDESCRIPTION: This bash snippet demonstrates how to run an Airflow DAG Python file and shows the expected output, which helps in understanding which parts of the code are executed during DAG parsing.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nroot@cf85ab34571e:/opt/airflow# python /files/test_python.py\nExecuting 1\n```\n\n----------------------------------------\n\nTITLE: Adding APT Packages to Airflow Image\nDESCRIPTION: Dockerfile and commands to create a custom Airflow image with additional APT packages\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/quick-start.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmkdir my-airflow-project && cd my-airflow-project\ncat <<EOM > Dockerfile\nFROM apache/airflow\nUSER root\nRUN apt-get update \\\n  && apt-get install -y --no-install-recommends \\\n         vim \\\n  && apt-get autoremove -yqq --purge \\\n  && apt-get clean \\\n  && rm -rf /var/lib/apt/lists/*\nUSER airflow\nEOM\n\ndocker build --pull --tag my-image:0.0.1 .\n\nkind load docker-image my-image:0.0.1\n\nhelm upgrade $RELEASE_NAME apache-airflow/airflow --namespace $NAMESPACE \\\n    --set images.airflow.repository=my-image \\\n    --set images.airflow.tag=0.0.1\n```\n\n----------------------------------------\n\nTITLE: Creating a Multiset Table from Azure Blob Storage in Teradata - SQL\nDESCRIPTION: This SQL snippet illustrates how Teradata's READ_NOS and CREATE TABLE AS features are used to import data from an Azure Blob Storage URI directly into a new multiset table. The LOCATION parameter specifies the Azure Blob Storage URI, while the AUTHORIZATION parameter references security credentials. Replace placeholders for LOCATION and AUTHORIZATION as needed. The query creates a permanent table populated with data loaded on execution. No external libraries are required but Teradata VantageTM with Native Object Store is mandatory. Inputs: Azure Blob Storage URI, optional authorization object; Output: Multiset Teradata table populated with object store data.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/operators/azure_blob_to_teradata.rst#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE MULTISET TABLE multiset_table_name AS (\n  SELECT *\n  FROM (\n    LOCATION='YOUR-OBJECT-STORE-URI'\n    AUTHORIZATION=authorization_object\n  ) AS d\n) WITH DATA;\n```\n\n----------------------------------------\n\nTITLE: License Header Block in RST\nDESCRIPTION: Apache License 2.0 header block formatted in RST markup\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/flink/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n```\n\n----------------------------------------\n\nTITLE: Including External Content in RST Documentation\nDESCRIPTION: This reStructuredText directive instructs the Sphinx documentation generator to include the content from the specified file (`/../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst`) at this location in the final documentation. This mechanism is used to modularize documentation and reuse common content, such as installation instructions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/facebook/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenLineage Namespace in airflow.cfg\nDESCRIPTION: Configuration for setting a custom namespace in airflow.cfg to logically separate events from different OpenLineage producers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_7\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\ntransport = {\"type\": \"http\", \"url\": \"http://example.com:5000\", \"endpoint\": \"api/v1/lineage\"}\nnamespace = 'my-team-airflow-instance'\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Docs for Nov 2022 Providers Wave\nDESCRIPTION: This commit message, linked to commit 12c3c39d1a dated 2022-11-15, indicates the preparation of documentation for the November 2022 release wave of Apache Airflow Providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_22\n\nLANGUAGE: text\nCODE:\n```\n``pRepare docs for November 2022 wave of Providers (#27613)``\n```\n\n----------------------------------------\n\nTITLE: Package Name Declaration in RST\nDESCRIPTION: Declares the name of the Apache Airflow Flink provider package using RST syntax\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/flink/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n``apache-airflow-providers-apache-flink``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add repair run functionality for Databricks\nDESCRIPTION: This commit message (hash 574102fd29, dated 2024-01-11) introduces a new feature allowing users to repair Databricks runs (issue #36601).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n``[FEAT] adds repair run functionality for databricks (#36601)``\n```\n\n----------------------------------------\n\nTITLE: Specifying Minimum Airflow Version Requirement\nDESCRIPTION: Defines the minimum Apache Airflow version required for the Apache Iceberg provider package. This requirement is specified in a table format using reStructuredText.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/iceberg/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n``apache-airflow``  ``>=2.9.0``\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow SSH Provider Package via pip\nDESCRIPTION: Command to install the SSH provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-ssh\n```\n\n----------------------------------------\n\nTITLE: Version 1.0.0 Change Log\nDESCRIPTION: Initial release changelog entries for version 1.0.0 of the FAB provider\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n1.0.0\n.....\n\nLatest change: 2023-12-23\n\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`b15d5578da <https://github.com/apache/airflow/commit/b15d5578dac73c4c6a3ca94d90ab0dc9e9e74c9c>`__  2023-12-23   ``Re-apply updated version numbers to 2nd wave of providers in December (#36380)``\n```\n\n----------------------------------------\n\nTITLE: Preparing Airflow Backport Release 2020.09.07\nDESCRIPTION: This commit message signifies preparation steps taken for the backport release designated as 2020.09.07. It references pull request #11238.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_73\n\nLANGUAGE: text\nCODE:\n```\nPrepare Backport release 2020.09.07 (#11238)\n```\n\n----------------------------------------\n\nTITLE: Refactoring and Simplifying Code in Alibaba Provider\nDESCRIPTION: Simplifies code within the Alibaba provider namespace, enhancing readability and maintainability, tracked in issue #33225.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_26\n\nLANGUAGE: text\nCODE:\n```\nRefactor: Simplify code in providers/alibaba (#33225)\n```\n\n----------------------------------------\n\nTITLE: Preparing Docs for 3rd Wave May 2024 (Commit Message)\nDESCRIPTION: Commit message indicating the preparation of documentation for the third wave of Apache Airflow provider releases in May 2024, relevant to the Airbyte provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs 3rd wave May 2024 (#39738)\n```\n\n----------------------------------------\n\nTITLE: Breaking Changes Note RST Block\nDESCRIPTION: RST formatted note block explaining version compatibility requirements for Airflow 2.2+\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ftp/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. note::\n  This release of provider is only available for Airflow 2.2+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n```\n\n----------------------------------------\n\nTITLE: Adding ArtifactHUB Annotations in Helm Chart\nDESCRIPTION: Adds ArtifactHUB annotations for documentation and screenshots in the Helm chart.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/RELEASE_NOTES.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- Add ArtifactHUB annotations for docs and screenshots (#20558)\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit with pipx\nDESCRIPTION: Commands to install pre-commit and optionally configure it with uv for virtualenv management\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npipx install pre-commit\npipx install inject pre-commit pre-commit-uv\n```\n\n----------------------------------------\n\nTITLE: Migrating DBAPI Hook Import Path in Python\nDESCRIPTION: Shows the migration path from the deprecated DBAPI hook module to the new common SQL hooks module in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41748.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nairflow.hooks.dbapi  airflow.providers.common.sql.hooks.sql\n```\n\n----------------------------------------\n\nTITLE: Enabling Python CodeQL Scans (Boolean String)\nDESCRIPTION: A boolean flag indicating whether Python CodeQL security scans should be executed. 'true' enables the scans.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Updating Airflow Provider Versions to 1.0.0\nDESCRIPTION: This commit message signifies an update where the versions of Airflow providers were set to 1.0.0. It references pull request #12955.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_50\n\nLANGUAGE: text\nCODE:\n```\nUpdates providers versions to 1.0.0 (#12955)\n```\n\n----------------------------------------\n\nTITLE: Override Versioned Documentation\nDESCRIPTION: Command to override versioned directories while publishing documentation using the override-versioned flag.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management publish-docs --override-versioned\n```\n\n----------------------------------------\n\nTITLE: Building PROD Image with Specific Python and Extras (Bash)\nDESCRIPTION: Builds the Airflow PROD image using Breeze, specifically targeting Python 3.9. The `--airflow-extras \"all\"` parameter replaces the default extras with the 'all' extras set during the build process.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze prod-image build --python 3.9 --airflow-extras \"all\"\n```\n\n----------------------------------------\n\nTITLE: Generating Slack Announcement\nDESCRIPTION: Command to generate announcement message for Slack channel.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\nWe've just released Apache Airflow Helm Chart ${VERSION} \n\n ArtifactHub: https://artifacthub.io/packages/helm/apache-airflow/airflow\n Docs: https://airflow.apache.org/docs/helm-chart/${VERSION}/\n Quick Start Installation Guide: https://airflow.apache.org/docs/helm-chart/${VERSION}/quick-start.html\n Release Notes: https://airflow.apache.org/docs/helm-chart/${VERSION}/release_notes.html\n\nThanks to all the contributors who made this possible.\nEOF\n```\n\n----------------------------------------\n\nTITLE: Bump Minimum Airflow Version to 2.9.0 (v1.4.0)\nDESCRIPTION: Indicates that the minimum required Apache Airflow version for this provider release (v1.4.0) was raised to 2.9.0, referenced by pull request #44956.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nBump minimum Airflow version in providers to Airflow 2.9.0 (#44956)\n```\n\n----------------------------------------\n\nTITLE: Removing Pylint from Toolchain in RST\nDESCRIPTION: This commit (866a601b76, committed on 2021-06-28) removes the Pylint static analysis tool from the project's development toolchain. Refers to issue #16682.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_25\n\nLANGUAGE: rst\nCODE:\n```\nRemoves pylint from our toolchain (#16682)\n```\n\n----------------------------------------\n\nTITLE: Listing Commits for Version 3.1.0 (rst)\nDESCRIPTION: This block lists commits associated with version 3.1.0 using reStructuredText table format. It includes commit hashes linked to GitHub, commit dates, and subject lines detailing changes like preparing the Salesforce provider release, documentation preparations for July providers (including Rc2), adding custom Salesforce connection types, updates to SalesforceToS3Operator, and removing pylint.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ======================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ======================================================================================\n`4dc622d8ca <https://github.com/apache/airflow/commit/4dc622d8ca4b14a2bb3b8c5e68bde04cc80c5379>`__  2021-07-27   ``Prepares release for Salesforce provider (#17272)``\n`87f408b1e7 <https://github.com/apache/airflow/commit/87f408b1e78968580c760acb275ae5bb042161db>`__  2021-07-26   ``Prepares docs for Rc2 release of July providers (#17116)``\n`763919d415 <https://github.com/apache/airflow/commit/763919d4152ffa13433e2489fec85ed286b7b196>`__  2021-07-25   ``Adding custom Salesforce connection type + SalesforceToS3Operator updates (#17162)``\n`b916b75079 <https://github.com/apache/airflow/commit/b916b7507921129dc48d6add1bdc4b923b60c9b9>`__  2021-07-15   ``Prepare documentation for July release of providers. (#17015)``\n`866a601b76 <https://github.com/apache/airflow/commit/866a601b76e219b3c043e1dbbc8fb22300866351>`__  2021-06-28   ``Removes pylint from our toolchain (#16682)``\n==================================================================================================  ===========  ======================================================================================\n```\n\n----------------------------------------\n\nTITLE: Including Sections and Options Reference in reStructuredText\nDESCRIPTION: This directive includes external documentation for sections and options in the current document.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/configurations-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst\n```\n\n----------------------------------------\n\nTITLE: Conditional Package Version Template (Python/Jinja)\nDESCRIPTION: Template-based conditional version selection for funcsigs package based on environment parameters. Includes version specifications for pyparsing, python-dateutil, and pytz packages.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/tests/unit/config_templates/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npyparsing==2.4.7\n\n{% if params and params.environ and params.environ == 'templated_unit_test' %}\n    funcsigs==1.0.2\n{% else %}\n    funcsigs==0.4\n{% endif %}\n\npython-dateutil==2.8.1\npytz==2020.1\n```\n\n----------------------------------------\n\nTITLE: Enabling JavaScript CodeQL Scans (Boolean String)\nDESCRIPTION: A boolean flag indicating whether JavaScript CodeQL security scans should be executed. 'true' enables the scans.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Formatting Commit Hash and Date in Markdown\nDESCRIPTION: This snippet shows how commit hashes and dates are formatted in the changelog using markdown syntax. The commit hash is linked to the full commit on GitHub.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n`be553780e5 <https://github.com/apache/airflow/commit/be553780e56cf8c34a65aecf2c52a33b82e0e039>`__  2024-10-22   ``feat: add OpenLineage support for RedshiftToS3Operator (#41632)``\n```\n\n----------------------------------------\n\nTITLE: Re-apply Updated Version Numbers (Excluded from v1.2.0 Changelog)\nDESCRIPTION: Describes the re-application of updated version numbers to the second wave of providers released in December, referenced by pull request #36380. This change was intentionally excluded from the main changelog notes for version 1.2.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_28\n\nLANGUAGE: plaintext\nCODE:\n```\nRe-apply updated version numbers to 2nd wave of providers in December (#36380)\n```\n\n----------------------------------------\n\nTITLE: Displaying Graph View Image in ReStructuredText\nDESCRIPTION: ReStructuredText directive to display the Graph View image in dark mode, showing DAG structure with no DAG run selected.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/ui.rst#2025-04-22_snippet_4\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. image:: img/ui-dark/dag_overview_graph.png\n   :alt: Graph View showing DAG structure with no DAG run selected (Dark Mode)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Apply Pre-Upgrade Ruff Changes\nDESCRIPTION: This commit message, associated with commit dd7ba3cae1 on 2023-10-19, indicates the application of changes required before upgrading the 'ruff' linter/formatter tool to version 0.0.292 within the providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nPre-upgrade 'ruff==0.0.292' changes in providers (#35053)\n```\n\n----------------------------------------\n\nTITLE: Merging Changes to Stable Branch\nDESCRIPTION: Commands to merge changes from test branch to stable branch for release preparation.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout ${STABLE_BRANCH}\n# make sure you are up to date\ngit fetch origin ${STABLE_BRANCH}\ngit reset --hard origin/${STABLE_BRANCH}\n# merge the changes from the test branch\ngit merge --ff-only ${TEST_BRANCH}\n# push the changes to the stable branch\ngit push origin ${STABLE_BRANCH}\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: Sphinx directive to include a security-related RST file from a relative path in the documentation build.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry v1.0.0\nDESCRIPTION: Changelog entry for initial provider version\nSOURCE: https://github.com/apache/airflow/blob/main/providers/datadog/docs/changelog.rst#2025-04-22_snippet_6\n\nLANGUAGE: rst\nCODE:\n```\n1.0.0\n.....\n\nInitial version of the provider.\n```\n\n----------------------------------------\n\nTITLE: HTML License Header Comment\nDESCRIPTION: Apache License 2.0 header comment block defining the licensing terms for the file.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0011-unified-communication-with-the-users.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Model for DAG Response in Python\nDESCRIPTION: Example of creating a Pydantic model for DAG response serialization, including field definitions with their types and validation rules.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/16_adding_api_endpoints.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass DAGModelResponse(BaseModel):\n    \"\"\"DAG serializer for responses.\"\"\"\n\n    dag_id: str\n    dag_display_name: str\n    is_paused: bool\n    is_active: bool\n    last_parsed_time: datetime | None\n```\n\n----------------------------------------\n\nTITLE: Installing pipx Package Manager\nDESCRIPTION: Commands to install pipx package manager and configure the user's PATH environment variable for global access.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/01_installation.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --user \"pipx>=1.4.1\"\n```\n\n----------------------------------------\n\nTITLE: Defining default arguments for Google Cloud Functions operators in Python\nDESCRIPTION: This snippet shows how to define default arguments for Google Cloud Functions operators, including project ID, location, and GCP connection ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/functions.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndefault_args = {\n    \"project_id\": PROJECT_ID,\n    \"location\": GCP_LOCATION,\n    \"gcp_conn_id\": GCP_CONN_ID,\n}\n```\n\n----------------------------------------\n\nTITLE: Fixing Typo in Airflow Provider Name (Oracle)\nDESCRIPTION: This commit message describes the correction of a typographical error specifically in the name of the Oracle provider within Airflow. It references pull request #13147.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_48\n\nLANGUAGE: text\nCODE:\n```\nFix typo in provider name - Oracle (#13147)\n```\n\n----------------------------------------\n\nTITLE: Commit Reference Links\nDESCRIPTION: Links to commit references for Apache Airflow Beam provider changes\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nd2459a241b <https://github.com/apache/airflow/commit/d2459a241b54d596ebdb9d81637400279fff4f2d>\n```\n\n----------------------------------------\n\nTITLE: Add Documentation for 3rd December Provider Wave (Excluded from v1.2.0 Changelog)\nDESCRIPTION: Notes the addition of documentation for the third wave of provider releases in December, referenced by pull request #36464. This change was intentionally excluded from the main changelog notes for version 1.2.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_21\n\nLANGUAGE: plaintext\nCODE:\n```\nAdd documentation for 3rd wave of providers in Deember (#36464)\n```\n\n----------------------------------------\n\nTITLE: Removing Links to x/twitter.com\nDESCRIPTION: Commit message stating the removal of links pointing to x.com or twitter.com from the codebase or documentation. References pull request #47801.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nRemove links to x/twitter.com (#47801)\n```\n\n----------------------------------------\n\nTITLE: Importing Hive Macros Module Reference\nDESCRIPTION: RST directive to include documentation for Hive macros module, making the macro functions available for use in Jinja2 templates within Airflow DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/macros.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: airflow.providers.apache.hive.macros.hive\n    :members:\n    :noindex:\n```\n\n----------------------------------------\n\nTITLE: Moving Airflow Provider Packages Scripts to Dev Directory\nDESCRIPTION: This commit message details the relocation of scripts related to provider packages into a 'dev' directory within the Airflow project structure. It references pull request #12082.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_62\n\nLANGUAGE: text\nCODE:\n```\nMoves provider packages scripts to dev (#12082)\n```\n\n----------------------------------------\n\nTITLE: Documenting Provider Update November 2023 (2nd Wave)\nDESCRIPTION: This commit prepares the documentation for the second wave of Apache Airflow provider updates released in November 2023, as detailed in issue #35836.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs 2nd wave of Providers November 2023 (#35836)\n```\n\n----------------------------------------\n\nTITLE: Including External reStructuredText File\nDESCRIPTION: This directive includes the content of another reStructuredText file located at the specified relative path. It's used here to incorporate common security documentation into the current document.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Documenting Excluded Changes in reStructuredText\nDESCRIPTION: This snippet shows how to document changes that are excluded from the changelog using reStructuredText format. It includes comments about provider releases and code changes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Add documentation for July 2022 Provider's release (#25030)``\n   * ``Enable string normalization in python formatting - providers (#27205)``\n   * ``Update docs for September Provider's release (#26731)``\n   * ``Apply PEP-563 (Postponed Evaluation of Annotations) to non-core airflow (#26289)``\n   * ``Prepare docs for new providers release (August 2022) (#25618)``\n   * ``Move provider dependencies to inside provider folders (#24672)``\n```\n\n----------------------------------------\n\nTITLE: Setting Asset Compilation Wait Multiplier\nDESCRIPTION: Increases the timeout for asset compilation when facing ETIMEDOUT errors.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/04_troubleshooting.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport ASSET_COMPILATION_WAIT_MULTIPLIER=3\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare docs 1st wave May 2024\nDESCRIPTION: This text is a commit message summary for preparing documentation related to the first wave of Apache Airflow provider releases in May 2024, linked to pull request #39328.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 1st wave May 2024 (#39328)\n```\n\n----------------------------------------\n\nTITLE: Using Typed Context Universally in RST\nDESCRIPTION: This commit (a0821235fb, committed on 2021-12-30) implements the use of typed 'Context' objects throughout the codebase. Refers to issue #20565.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_12\n\nLANGUAGE: rst\nCODE:\n```\nUse typed Context EVERYWHERE (#20565)\n```\n\n----------------------------------------\n\nTITLE: Move PGVECTOR Provider to New Structure (Excluded from v1.4.1 Changelog)\nDESCRIPTION: Details the migration of the PGVECTOR provider to a new organizational structure, referenced by pull request #46051. This change was intentionally excluded from the main changelog notes for version 1.4.1.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nMove PGVECTOR provider to new structure (#46051)\n```\n\n----------------------------------------\n\nTITLE: Inserting Apache License 2.0 Header in HTML Comments\nDESCRIPTION: This snippet shows the standard Apache License 2.0 header enclosed in HTML comments. It specifies the licensing terms for the Apache Airflow project, including distribution rights and limitations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/src/airflow/providers/asana/README.md#2025-04-22_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Setting Default DB Authentication in Airflow (INI)\nDESCRIPTION: Specifies the default authentication type `AUTH_DB` in `$AIRFLOW_HOME/webserver_config.py`. This configuration uses the Flask-AppBuilder's built-in database authentication mechanism, requiring users to log in with credentials stored in Airflow's metadata database.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/webserver-authentication.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\nAUTH_TYPE = AUTH_DB\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add Automated Version Replacement in Example DAGs\nDESCRIPTION: This commit message, associated with commit c8e348dcb0 dated 2022-12-05, describes the addition of automated replacement of version numbers within the index files for example DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_21\n\nLANGUAGE: text\nCODE:\n```\n``Add automated version replacement in example dag indexes (#28090)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Re-apply updated version numbers (Dec Providers)\nDESCRIPTION: This commit message (hash b15d5578da, dated 2023-12-23) indicates that updated version numbers were re-applied to the second wave of provider releases in December (issue #36380).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n``Re-apply updated version numbers to 2nd wave of providers in December (#36380)``\n```\n\n----------------------------------------\n\nTITLE: Adding D200 Pydocstyle Check to Airflow\nDESCRIPTION: This commit message details the addition of the D200 check from the pydocstyle tool to the Airflow project's code quality checks. It references pull request #11688.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_68\n\nLANGUAGE: text\nCODE:\n```\nAdd D200 pydocstyle check (#11688)\n```\n\n----------------------------------------\n\nTITLE: Removing Sphinx :type Lines Due to AutoAPI Support in RST\nDESCRIPTION: This commit (602abe8394, committed on 2022-01-20) removes ':type' lines from documentation comments, as the sphinx-autoapi tool now supports type hints directly. Refers to issue #20951.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: rst\nCODE:\n```\nRemove ':type' lines now sphinx-autoapi supports typehints (#20951)\n```\n\n----------------------------------------\n\nTITLE: Disabling Built-in Webserver in Airflow Helm Chart\nDESCRIPTION: Configuration to disable the built-in webserver when using an external webserver instance.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nwebserver:\n  enabled: false\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Documentation in RST\nDESCRIPTION: Sphinx directive to include external documentation about installing provider packages from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs for Jan 2023 wave of Providers (#28651)\nDESCRIPTION: This commit message, associated with version 3.2.0, indicates the preparation of documentation for the January 2023 release wave of Apache Airflow Providers. Commit hash: 5246c009c5, Date: 2023-01-02.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs for Jan 2023 wave of Providers (#28651)``\n```\n\n----------------------------------------\n\nTITLE: Fixing Airbyte Trigger Yield Statement (Commit Message)\nDESCRIPTION: Commit message detailing a fix in the Apache Airflow Airbyte triggers by adding a return statement to a yield within a while loop, likely related to deferrable operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\nfix(airbyte): add return statement to yield within a while loop in triggers (#38390)\n```\n\n----------------------------------------\n\nTITLE: Implementing Airflow Provider Versioning Tools\nDESCRIPTION: This commit message details the implementation of tools designed for managing provider versioning within the Airflow ecosystem. It references pull request #13767.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_42\n\nLANGUAGE: text\nCODE:\n```\nImplement provider versioning tools (#13767)\n```\n\n----------------------------------------\n\nTITLE: Copying a Google Dataprep Flow in Python\nDESCRIPTION: Example usage of the DataprepCopyFlowOperator to copy a flow in Google Dataprep.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataprep.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# [START how_to_dataprep_copy_flow_operator]\n# Example usage code would be here\n# [END how_to_dataprep_copy_flow_operator]\n```\n\n----------------------------------------\n\nTITLE: Removing Scheduler Dependency Detector Configuration\nDESCRIPTION: Configuration change that removes the deprecated 'dependency_detector' parameter from the Airflow scheduler. This change requires updating configuration files and running 'airflow config lint' to verify configuration validity.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41609.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nscheduler.dependency_detector\n```\n\n----------------------------------------\n\nTITLE: Apache HDFS Provider Package Header\nDESCRIPTION: ReStructuredText header defining the package name and HDFS technology references\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\nPackage apache-airflow-providers-apache-hdfs\n------------------------------------------------------\n\n`Hadoop Distributed File System (HDFS) <https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html>`__\nand `WebHDFS <https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/WebHDFS.html>`__.\n```\n\n----------------------------------------\n\nTITLE: Migration Rule Definition for FileTaskHandler Deprecation\nDESCRIPTION: Specification of the ruff linting rule AIR302 that enforces the removal of the deprecated filename_template argument from FileTaskHandler and its subclasses.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41552.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nAIR302 - argument 'filename_template' in airflow.utils.log.file_task_handler.FileTaskHandler and its subclassses\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Instructions in Sphinx Documentation\nDESCRIPTION: A Sphinx directive that includes external documentation content about installing Airflow provider packages from source. This uses the include directive to reference a reusable documentation component.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Listing Airflow Provider Commits (v3.2.1) using RST\nDESCRIPTION: This reStructuredText snippet presents a table of commits related to the Apache Airflow provider release wave around version 3.2.1 (June 2023). It lists commit hashes linked to GitHub, commit dates, and subjects, specifically mentioning documentation preparation and notes about dropping Python 3.7 support.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/winrm/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  =============================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =============================================================\n`79bcc2e668 <https://github.com/apache/airflow/commit/79bcc2e668e648098aad6eaa87fe8823c76bc69a>`__  2023-06-20   ``Prepare RC1 docs for June 2023 wave of Providers (#32001)``\n`8b146152d6 <https://github.com/apache/airflow/commit/8b146152d62118defb3004c997c89c99348ef948>`__  2023-06-20   ``Add note about dropping Python 3.7 for providers (#32015)``\n==================================================================================================  ===========  =============================================================\n```\n\n----------------------------------------\n\nTITLE: Upgrade flit Dependency (v1.4.1)\nDESCRIPTION: Documents the upgrade of the 'flit' build tool to version 3.11.0 as part of the changes in version 1.4.1. This change is referenced by pull request #46938.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nUpgrade flit to 3.11.0 (#46938)\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit History in Markdown Table\nDESCRIPTION: This snippet shows a markdown table listing commit history for the Microsoft Azure provider, including commit hashes, dates, and subjects.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ===================================================================================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ===================================================================================================================================================\n`0653ffe78e <https://github.com/apache/airflow/commit/0653ffe78e4a0acaf70801a5ceef8dbabdac8b15>`__  2025-02-21   ``Prepare docs for Feb 1st wave of providers (fixed) (#46962)``\n`5d87bddf0a <https://github.com/apache/airflow/commit/5d87bddf0aa5f485f3684c909fb95f461e5a2ab6>`__  2025-02-21   ``Prepare docs for Feb 1st wave of providers (#46893)``\n`4833b53705 <https://github.com/apache/airflow/commit/4833b53705acfc4bd0a26bf3e4dd4fc7a22b0bfa>`__  2025-02-19   ``Refactor result_processor and event_handler signatures in MSGraphAsyncOperator (#46637)``\n`4e17ecd3f8 <https://github.com/apache/airflow/commit/4e17ecd3f892497e910f4e7df7ecb007a7f3d039>`__  2025-02-16   ``Avoid imports from \"providers\" (#46801)``\n`4d5846f58f <https://github.com/apache/airflow/commit/4d5846f58fe0de9b43358c0be75dd72e968dacc4>`__  2025-02-16   ``Move provider_tests to unit folder in provider tests (#46800)``\n`e027457a24 <https://github.com/apache/airflow/commit/e027457a24d0c6235bfed9c2a8399f75342e82f1>`__  2025-02-15   ``Removed the unused provider's distribution (#46608)``\n`fe5a2ea7a0 <https://github.com/apache/airflow/commit/fe5a2ea7a0b57901bb6239d666b875f6c71cd7e8>`__  2025-02-13   ``AIP-72: Improving Operator Links Interface to Prevent User Code Execution in Webserver (#46613)``\n`035060d7f3 <https://github.com/apache/airflow/commit/035060d7f384a4989eddb6fb05f512f9c6a7e5bf>`__  2025-02-11   ``AIP-83 amendment: Add logic for generating run_id when logical date is None. (#46616)``\n`fdb934fa06 <https://github.com/apache/airflow/commit/fdb934fa06ab881d24924cfa1839f9f94f628658>`__  2025-02-09   ``Migrate Amazon provider package (#46590)``\n`fd606b1d6e <https://github.com/apache/airflow/commit/fd606b1d6e1469ebf977e327c947a1d288b5f0d2>`__  2025-02-08   ``Provider moving/microsoft azure (#46254)``\n`aaaea355d3 <https://github.com/apache/airflow/commit/aaaea355d3adf430204d01f8fdb3bfafbd7c2bd9>`__  2025-02-06   ``Add dynamic task mapping into TaskSDK runtime (#46032)``\n`40fd35ca44 <https://github.com/apache/airflow/commit/40fd35ca444740d6788e489ac7493b39937f8d23>`__  2025-02-03   ``fix: 'KiotaRequestAdapterHook' make sure proxy config parameter is parsed correctly, even if it's a string or json (#46145)``\n`6b87d1cef9 <https://github.com/apache/airflow/commit/6b87d1cef91150984648f25bb2163a13bc205d6c>`__  2025-01-26   ``Fixed retry of PowerBIDatasetRefreshOperator when dataset refresh wasn't directly available (#45513)``\n`ff1e3a6f61 <https://github.com/apache/airflow/commit/ff1e3a6f617bb14b6007daff6ed8296d4a78c20d>`__  2025-01-26   ``Added support for certificate authentication with MSGraphAsyncOperator (#45935)``\n`ee785a89ba <https://github.com/apache/airflow/commit/ee785a89ba27a59246cdfcc0d83129b691a39f3e>`__  2025-01-19   ``Fixes compat issue HTTPX proxy configuration in KiotaRequestAdapterHook and fixed retry in MSGraphSensor (#45746)``\n`78eadb114b <https://github.com/apache/airflow/commit/78eadb114bcba8872e99dbd0870f895649e546a1>`__  2025-01-17   ``refactor: Fixed assignment of proxies parameter in KiotaRequestAdapterHook (#45741)``\n`301017de2d <https://github.com/apache/airflow/commit/301017de2d76ac9aeb55f966ee580481dd88e525>`__  2025-01-08   ``refactor: Updated instantiated of httpx AsyncClient as the proxies parameter is deprecated and mounts parameter should be used instead (#45464)``\n`3787844693 <https://github.com/apache/airflow/commit/378784469398fa51681265c6661b6906352f219b>`__  2025-01-08   ``Fixing return type for azure hook 'run_query' (#45470)``\n`4b90edf580 <https://github.com/apache/airflow/commit/4b90edf580bb306f73bf8bf67631ca027a51b70d>`__  2025-01-07   ``Move update subscription from 'AzureServiceBusTopicCreateOperator' to 'AdminClientHook' (#45367)``\n`cda0e9e9be <https://github.com/apache/airflow/commit/cda0e9e9bee23a1d04cc18060bb496c7fffc43da>`__  2025-01-01   ``Move create topic from 'AzureServiceBusTopicCreateOperator' to 'AdminClientHook' (#45297)``\n`b29a16c524 <https://github.com/apache/airflow/commit/b29a16c524a703d61b7da95c75b055578dc9c620>`__  2024-12-21   ``Refactor Azure Service Bus: move create_subscription to hook (#45125)``\n==================================================================================================  ===========  ===================================================================================================================================================\n```\n\n----------------------------------------\n\nTITLE: Pre-commit Hook Configuration Table\nDESCRIPTION: Table displaying pre-commit hook IDs and their descriptions. Each row contains a hook identifier and its corresponding functionality description.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| check-taskinstance-tis-attrs                              | Check that TI and TIS have the same attributes         |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| check-template-context-variable-in-sync                   | Sync template context variable refs                    |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| check-template-fields-valid                               | Check templated fields mapped in operators/sensors     | *       |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| check-tests-in-the-right-folders                          | Check if tests are in the right folders                |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| check-tests-unittest-testcase                             | Unit tests do not inherit from unittest.TestCase       |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| check-urlparse-usage-in-code                              | Don't use urlparse in code                             |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| check-xml                                                 | Check XML files with xmllint                           |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| check-zip-file-is-not-committed                           | Check no zip files are committed                       |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| codespell                                                 | Run codespell                                          |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| compile-fab-assets                                        | Compile FAB provider assets                            |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| compile-ui-assets                                         | Compile ui assets (manual)                             |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| compile-ui-assets-dev                                     | Compile ui assets in dev mode (manual)                 |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| create-missing-init-py-files-tests                        | Create missing init.py files in tests                  |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| debug-statements                                          | Detect accidentally committed debug statements         |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| detect-private-key                                        | Detect if private key is added to the repository       |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| doctoc                                                    | Add TOC for Markdown and RST files                     |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| end-of-file-fixer                                         | Make sure that there is an empty line at the end       |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| fix-encoding-pragma                                       | Remove encoding header from Python files               |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| flynt                                                     | Run flynt string format converter for Python           |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| generate-airflow-diagrams                                 | Generate airflow diagrams                              |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| generate-airflowctl-datamodels                            | Generate Datamodels for AirflowCTL                     | *       |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| generate-openapi-spec                                     | Generate the FastAPI API spec                          | *       |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| generate-pypi-readme                                      | Generate PyPI README                                   |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| generate-tasksdk-datamodels                               | Generate Datamodels for TaskSDK client                 | *       |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| generate-volumes-for-sources                              | Generate volumes for docker compose                    |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| identity                                                  | Print checked files                                    |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| insert-license                                            | * Add license for all SQL files                        |         |\n|                                                           | * Add license for all RST files                        |         |\n|                                                           | * Add license for CSS/JS/JSX/PUML/TS/TSX               |         |\n|                                                           | * Add license for all Shell files                      |         |\n|                                                           | * Add license for all toml files                       |         |\n|                                                           | * Add license for all Python files                     |         |\n|                                                           | * Add license for all XML files                        |         |\n|                                                           | * Add license for all Helm template files              |         |\n|                                                           | * Add license for all YAML files except Helm templates |         |\n|                                                           | * Add license for all Markdown files                   |         |\n|                                                           | * Add license for all other files                      |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| kubeconform                                               | Kubeconform check on our helm chart                    |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| lint-chart-schema                                         | Lint chart/values.schema.json file                     |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| lint-dockerfile                                           | Lint Dockerfile                                        |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| lint-helm-chart                                           | Lint Helm Chart                                        |         |\n+-----------------------------------------------------------+--------------------------------------------------------+---------+\n| lint-json-schema                                          | * Lint JSON Schema files                               |         |\n|                                                           | * Lint NodePort Service                                |         |\n|                                                           | * Lint Docker compose files                            |         |\n|                                                           | * Lint chart/values.schema.json                        |         |\n```\n\n----------------------------------------\n\nTITLE: Provider Package Declaration in RST\nDESCRIPTION: ReStructuredText declaration of the OpenFaaS provider package name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openfaas/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n``apache-airflow-providers-openfaas``\n```\n\n----------------------------------------\n\nTITLE: Documenting Bug Fix for TriggerDagRunOperator in RST\nDESCRIPTION: Documents a bug fix for the TriggerDagRunOperator's extra_link when trigger_dag_id is templated.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n\"* ``Fix TriggerDagRunOperator extra_link when trigger_dag_id is templated (#42810)``\"\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add D400 Pydocstyle Check for Providers\nDESCRIPTION: This commit message, linked to commit a59076eaee dated 2023-06-02, indicates the implementation of the D400 pydocstyle check for Apache Airflow provider code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n``Add D400 pydocstyle check - Providers (#31427)``\n```\n\n----------------------------------------\n\nTITLE: Version Header Format\nDESCRIPTION: Demonstrates the format used for version headers in the changelog, showing version number and latest change date.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/hashicorp/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n2.0.0\n.....\n\nLatest change: 2021-06-18\n```\n\n----------------------------------------\n\nTITLE: Displaying Airbyte Provider Version 3.3.0 Changelog\nDESCRIPTION: Lists the changes made in version 3.3.0 of the Airbyte provider, including commit hashes, dates, and descriptions of changes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_33\n\nLANGUAGE: markdown\nCODE:\n```\n3.3.0\n.....\n\nLatest change: 2023-05-19\n\n==================================================================================================  ===========  ======================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ======================================================================================\n`45548b9451 <https://github.com/apache/airflow/commit/45548b9451fba4e48c6f0c0ba6050482c2ea2956>`__  2023-05-19   ``Prepare RC2 docs for May 2023 wave of Providers (#31416)``\n`abea189022 <https://github.com/apache/airflow/commit/abea18902257c0250fedb764edda462f9e5abc84>`__  2023-05-18   ``Use '__version__' in providers not 'version' (#31393)``\n`f5aed58d9f <https://github.com/apache/airflow/commit/f5aed58d9fb2137fa5f0e3ce75b6709bf8393a94>`__  2023-05-18   ``Fixing circular import error in providers caused by airflow version check (#31379)``\n`7ebda3898d <https://github.com/apache/airflow/commit/7ebda3898db2eee72d043a9565a674dea72cd8fa>`__  2023-05-17   ``Fix missing line in index.rst for provider documentation (#31343)``\n`d9ff55cf6d <https://github.com/apache/airflow/commit/d9ff55cf6d95bb342fed7a87613db7b9e7c8dd0f>`__  2023-05-16   ``Prepare docs for May 2023 wave of Providers (#31252)``\n`eef5bc7f16 <https://github.com/apache/airflow/commit/eef5bc7f166dc357fea0cc592d39714b1a5e3c14>`__  2023-05-03   ``Add full automation for min Airflow version for providers (#30994)``\n==================================================================================================  ===========  ======================================================================================\n```\n\n----------------------------------------\n\nTITLE: Upgrading Flit Build Tool\nDESCRIPTION: Commit message stating an upgrade of the 'flit' build tool to version 3.11.0. References pull request #46938.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_23\n\nLANGUAGE: plaintext\nCODE:\n```\nUpgrade flit to 3.11.0 (#46938)\n```\n\n----------------------------------------\n\nTITLE: Removing Backport Providers\nDESCRIPTION: Excluded Change (Version 2.0.0): Removes Backport Provider packages, as they are no longer necessary with newer Airflow versions, referencing pull request #14886.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_40\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Remove Backport Providers (#14886)``\n```\n\n----------------------------------------\n\nTITLE: Listing Commits for Airflow Provider Version 4.2.0\nDESCRIPTION: This section lists commits associated with the Apache Airflow provider version 4.2.0 release. Key changes include documentation preparation for the May 2023 wave, fixes for version handling and imports, automation for minimum Airflow version requirements, and adding a provider suspension mechanism. It includes commit hashes linked to GitHub, commit dates, and subject lines.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n==================================================================================================  ===========  ======================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ======================================================================================\n`45548b9451 <https://github.com/apache/airflow/commit/45548b9451fba4e48c6f0c0ba6050482c2ea2956>`__  2023-05-19   ``Prepare RC2 docs for May 2023 wave of Providers (#31416)``\n`abea189022 <https://github.com/apache/airflow/commit/abea18902257c0250fedb764edda462f9e5abc84>`__  2023-05-18   ``Use '__version__' in providers not 'version' (#31393)``\n`f5aed58d9f <https://github.com/apache/airflow/commit/f5aed58d9fb2137fa5f0e3ce75b6709bf8393a94>`__  2023-05-18   ``Fixing circular import error in providers caused by airflow version check (#31379)``\n`7ebda3898d <https://github.com/apache/airflow/commit/7ebda3898db2eee72d043a9565a674dea72cd8fa>`__  2023-05-17   ``Fix missing line in index.rst for provider documentation (#31343)``\n`d9ff55cf6d <https://github.com/apache/airflow/commit/d9ff55cf6d95bb342fed7a87613db7b9e7c8dd0f>`__  2023-05-16   ``Prepare docs for May 2023 wave of Providers (#31252)``\n`eef5bc7f16 <https://github.com/apache/airflow/commit/eef5bc7f166dc357fea0cc592d39714b1a5e3c14>`__  2023-05-03   ``Add full automation for min Airflow version for providers (#30994)``\n`a7eb32a5b2 <https://github.com/apache/airflow/commit/a7eb32a5b222e236454d3e474eec478ded7c368d>`__  2023-04-30   ``Bump minimum Airflow version in providers (#30917)``\n`d23a3bbed8 <https://github.com/apache/airflow/commit/d23a3bbed89ae04369983f21455bf85ccc1ae1cb>`__  2023-04-04   ``Add mechanism to suspend providers (#30422)``\n==================================================================================================  ===========  ======================================================================================\n```\n\n----------------------------------------\n\nTITLE: Listing Recent Airflow Provider Commits (Aug 2023+) using RST\nDESCRIPTION: This reStructuredText snippet displays a table logging recent commits for Apache Airflow providers from late July to August 2023. The table includes linked commit hashes, commit dates, and commit subjects, primarily focusing on documentation preparation for provider releases and code refactoring.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/winrm/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ================================================================\n`c077d19060 <https://github.com/apache/airflow/commit/c077d190609f931387c1fcd7b8cc34f12e2372b9>`__  2023-08-26   ``Prepare docs for Aug 2023 3rd wave of Providers (#33730)``\n`85acbb4ae9 <https://github.com/apache/airflow/commit/85acbb4ae9bc26248ca624fa4d289feccba00836>`__  2023-08-24   ``Refactor: Remove useless str() calls (#33629)``\n`b5a4d36383 <https://github.com/apache/airflow/commit/b5a4d36383c4143f46e168b8b7a4ba2dc7c54076>`__  2023-08-11   ``Prepare docs for Aug 2023 2nd wave of Providers (#33291)``\n`73b90c48b1 <https://github.com/apache/airflow/commit/73b90c48b1933b49086d34176527947bd727ec85>`__  2023-07-21   ``Allow configuration to be contributed by providers (#32604)``\n`225e3041d2 <https://github.com/apache/airflow/commit/225e3041d269698d0456e09586924c1898d09434>`__  2023-07-06   ``Prepare docs for July 2023 wave of Providers (RC2) (#32381)``\n`3878fe6fab <https://github.com/apache/airflow/commit/3878fe6fab3ccc1461932b456c48996f2763139f>`__  2023-07-05   ``Remove spurious headers for provider changelogs (#32373)``\n`cb4927a018 <https://github.com/apache/airflow/commit/cb4927a01887e2413c45d8d9cb63e74aa994ee74>`__  2023-07-05   ``Prepare docs for July 2023 wave of Providers (#32298)``\n`09d4718d3a <https://github.com/apache/airflow/commit/09d4718d3a46aecf3355d14d3d23022002f4a818>`__  2023-06-27   ``Improve provider documentation and README structure (#32125)``\n==================================================================================================  ===========  ================================================================\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: RST directive to include external security documentation from the devel-common repository.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sendgrid/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Displaying DAG List View Image in ReStructuredText\nDESCRIPTION: ReStructuredText directive to display the DAG List View image in dark mode, showing search, filters, and DAG-level controls.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/ui.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. image:: img/ui-dark/dag_list.png\n   :alt: DAG List View in dark mode showing search, filters, and DAG-level controls\n```\n\n----------------------------------------\n\nTITLE: Secure GitHub Actions Checkout Configuration\nDESCRIPTION: GitHub Actions checkout configuration that prevents unauthorized repository pushes by setting persist-credentials to false and limiting fetch depth.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0005-preventing-using-contributed-code-when-building-images.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ env.TARGET_COMMIT_SHA }}\n          persist-credentials: false\n          fetch-depth: 2\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents for S3 Operators\nDESCRIPTION: ReStructuredText directive defining a table of contents for S3 operator documentation with maximum depth of 1 level and globbing pattern to include all files in directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/s3/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with async Extra\nDESCRIPTION: This command installs Apache Airflow with the async extra, which provides async worker classes for Gunicorn.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[async]'\n```\n\n----------------------------------------\n\nTITLE: Generating Provider Documentation\nDESCRIPTION: Commands for preparing release notes and documentation for provider packages. The script can handle single or multiple packages simultaneously.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/PROVIDER_DISTRIBUTIONS_DETAILS.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-provider-documentation <PACKAGE_ID> ...\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-provider-documentation google\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure with License Header\nDESCRIPTION: ReStructuredText documentation file that contains Apache License 2.0 header and basic structure for Google Marketing Platform operators documentation, including a table of contents and note about example DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n\n\nGoogle Marketing Platform Operators\n===================================\n\n\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n\n.. note::\n    You can learn how to use Google Cloud integrations by analyzing the\n    `source code <https://github.com/apache/airflow/tree/providers-google/8.0.0/airflow/providers/google/marketing_platform/example_dags>`_ of the particular example DAGs.\n```\n\n----------------------------------------\n\nTITLE: Adding PyPI Repository Link to Airflow Provider Docs\nDESCRIPTION: This commit message indicates the addition of a link to the Python Package Index (PyPI) repository within the documentation for Airflow providers. It references pull request #13064.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_49\n\nLANGUAGE: text\nCODE:\n```\nAdd link to PyPI Repository to provider docs (#13064)\n```\n\n----------------------------------------\n\nTITLE: Including Apache License and Security Notes - reStructuredText\nDESCRIPTION: This snippet provides boilerplate legal text for Apache 2.0 licensing and copyright attribution using reStructuredText syntax. It uses comment blocks (\".. \") to present the text and an include directive to import standardized security information from a shared location. The snippet is expected to be included at the top of documentation files where license and notifications must be visible. Dependencies: Sphinx or compatible reStructuredText processor with support for includes. No parameters or inputs required, but the include path must be valid; failure to resolve may cause Sphinx build warnings.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/trino/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Updating Local ASF Dev SVN Checkout in Shell\nDESCRIPTION: Updates an existing local checkout of the Apache Airflow development distribution SVN repository to the latest revision. Used when the repository has already been checked out previously.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\nsvn update .\n```\n\n----------------------------------------\n\nTITLE: Add Legacy Namespace Packages (Excluded from v1.4.1 Changelog)\nDESCRIPTION: Notes the addition of legacy namespace packages to 'airflow.providers', referenced by pull request #47064. This change was intentionally excluded from the main changelog notes for version 1.4.1.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nAdd legacy namespace packages to airflow.providers (#47064)\n```\n\n----------------------------------------\n\nTITLE: Fixing Broken Markdown Links in Airflow Providers README TOC\nDESCRIPTION: This commit message notes the correction of broken Markdown links found within the Table of Contents (TOC) section of the providers' README file. It references pull request #11249.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_71\n\nLANGUAGE: text\nCODE:\n```\nFix Broken Markdown links in Providers README TOC (#11249)\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit History in Markdown Table\nDESCRIPTION: Shows a table of commits for the DBT Cloud provider, including commit hash, date, and subject. The table is formatted in Markdown.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ===================================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ===================================================================================================\n`e6ea6709bb <https://github.com/apache/airflow/commit/e6ea6709bbd8ba7c024c4f75136a0af0cf9987b0>`__  2025-02-04   ``Moving EmptyOperator to standard provider (#46231)``\n`ee6bd7ee16 <https://github.com/apache/airflow/commit/ee6bd7ee162ff295b86d86fdd1b356c51b9bba78>`__  2025-02-03   ``Fix doc issues found with recent moves (#46372)``\n`5917548723 <https://github.com/apache/airflow/commit/5917548723abd87cf09520b2cd86d00e05fcb5c8>`__  2025-01-30   ``refactor(providers/dbt.cloud): move dbt cloud provider to new structure (#46208)``\n`019ed2ba0e <https://github.com/apache/airflow/commit/019ed2ba0e7059f864b68a63acab40ab1f355b33>`__  2025-01-13   ``Add missing newline on conn string example (#45603)``\n`6307a123d2 <https://github.com/apache/airflow/commit/6307a123d2c1bec99d671914cb18bc93c4c8933b>`__  2025-01-12   ``Remove classes from 'typing_compat' that can be imported directly (#45589)``\n==================================================================================================  ===========  ===================================================================================================\n```\n\n----------------------------------------\n\nTITLE: Displaying DAG Details Page Image in ReStructuredText\nDESCRIPTION: ReStructuredText directive to display the DAG Details Page image in dark mode, showing overview dashboard and failure diagnostics.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/ui.rst#2025-04-22_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. image:: img/ui-dark/dag_overview_dashboard.png\n   :alt: DAG Details Page in dark mode showing overview dashboard and failure diagnostics\n```\n\n----------------------------------------\n\nTITLE: Git Commit Hash Reference\nDESCRIPTION: A Git commit hash used for version control reference. Contains a 40-character SHA-1 hash identifying a specific commit in the Apache Airflow repository.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openfaas/docs/.latest-doc-only-change.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n7b2ec33c7ad4998d9c9735b79593fcdcd3b9dd1f\n```\n\n----------------------------------------\n\nTITLE: Preparing Docs for 1st Wave Jan 2024 (Commit Message)\nDESCRIPTION: Commit message indicating the preparation of documentation for the first wave of Apache Airflow provider releases in January 2024, relevant to the Airbyte provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_25\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs 1st wave of Providers January 2024 (#36640)\n```\n\n----------------------------------------\n\nTITLE: Installing MongoDB Provider via Pip (Bash)\nDESCRIPTION: This command installs the `apache-airflow-providers-mongo` package using the pip package installer. It should be executed in a command-line environment on top of an existing Airflow 2 installation (version 2.9.0 or higher is required). This package allows Airflow DAGs to interact with MongoDB.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mongo/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-mongo\n```\n\n----------------------------------------\n\nTITLE: Listing Documentation Locations in Markdown\nDESCRIPTION: This snippet lists the locations of documentation for different Airflow components and sub-projects. It includes paths for core components, providers, Helm chart, and future components. It also mentions locations for general documentation not tied to specific distributions.\nSOURCE: https://github.com/apache/airflow/blob/main/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nDocumentation in separate distributions:\n\n* `airflow-core/docs` - documentation for Airflow Core\n* `providers/**/docs` - documentation for Providers\n* `chart/docs` - documentation for Helm Chart\n* `task-sdk/docs` - documentation for Task SDK (new format not yet published)\n* `airflow-ctl/docs` - documentation for Airflow CLI (future)\n\nDocumentation for general overview and summaries not connected with any specific distribution:\n\n* `docker-stack-docs` - documentation for Docker Stack'\n* `providers-summary-docs` - documentation for provider summary page\n```\n\n----------------------------------------\n\nTITLE: Including Provider Configurations Reference in ReStructuredText\nDESCRIPTION: This directive includes the provider configurations reference file in the current document. It's likely used to import common documentation across multiple files.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst\n```\n\n----------------------------------------\n\nTITLE: Adding Connection/Hook Discovery from Airflow Providers\nDESCRIPTION: This commit message details the addition of functionality enabling the discovery of Connections and Hooks directly from Airflow providers. It references pull request #12466.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_53\n\nLANGUAGE: text\nCODE:\n```\nAdds support for Connection/Hook discovery from providers (#12466)\n```\n\n----------------------------------------\n\nTITLE: Checking Version Differences\nDESCRIPTION: Command to get changelog information between versions using git log.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd ${AIRFLOW_REPO_ROOT}\ngit log 2.8.0..HEAD --pretty=oneline -- clients/python/openapi_v1.yaml\n```\n\n----------------------------------------\n\nTITLE: Generating Supported OpenLineage Classes in reStructuredText\nDESCRIPTION: This directive generates a list of Airflow Operators and Hooks that support OpenLineage extraction. It's likely part of an automated documentation process.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/supported_classes.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. airflow-providers-openlineage-supported-classes::\n```\n\n----------------------------------------\n\nTITLE: Git Commit SHA-1 Hash\nDESCRIPTION: A 40-character Git commit hash representing a specific commit in the Apache Airflow repository.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/.latest-doc-only-change.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nd4473555c0e7022e073489b7163d49102881a1a6\n```\n\n----------------------------------------\n\nTITLE: Apache License Header\nDESCRIPTION: Standard Apache 2.0 license header included at the top of the file\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/CHANGELOG.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Commit Hash Reference - RestructuredText Format\nDESCRIPTION: Git commit hash reference formatting in RST format showing version history with links to commits, dates and commit messages\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n`9276310a43 <https://github.com/apache/airflow/commit/9276310a43d17a9e9e38c2cb83686a15656896b2>`__  2023-06-05   ``Improve docstrings in providers (#31681)``\n```\n\n----------------------------------------\n\nTITLE: UV Provider Development Commands\nDESCRIPTION: Commands for working on individual Airflow providers, including dependency installation and test execution.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/07_local_virtualenv.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd providers/mongo\nuv sync\nuv run pytest\n```\n\n----------------------------------------\n\nTITLE: Simplifying String Expressions in Airflow Codebase\nDESCRIPTION: This commit message describes a code refactoring effort focused on simplifying string expressions throughout the Airflow codebase. It references pull request #12093.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_63\n\nLANGUAGE: text\nCODE:\n```\nSimplify string expressions (#12093)\n```\n\n----------------------------------------\n\nTITLE: Fixing Type Annotations in DB Operators in RST\nDESCRIPTION: This commit (bb52098cd6, committed on 2021-08-06) corrects type annotations within the OracleOperator, JdbcOperator, and SqliteOperator. Refers to issue #17406.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_22\n\nLANGUAGE: rst\nCODE:\n```\nFix type annotations in OracleOperator,  JdbcOperator, SqliteOperator (#17406)\n```\n\n----------------------------------------\n\nTITLE: Bump Minimum Airflow Version to 2.8.0 (v1.3.0)\nDESCRIPTION: Indicates that the minimum required Apache Airflow version for this provider release (v1.3.0) was raised to 2.8.0, referenced by pull request #41396.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nBump minimum Airflow version in providers to Airflow 2.8.0 (#41396)\n```\n\n----------------------------------------\n\nTITLE: Moving Non-User Facing Code to '_internal' (AIP-72)\nDESCRIPTION: Commit message describing the relocation of non-user-facing code into an '_internal' module or directory as part of the Task SDK work (AIP-72). References pull request #45515.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_44\n\nLANGUAGE: plaintext\nCODE:\n```\nAIP-72: Move non-user facing code to '_internal' (#45515)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Standardize OpenSearch Naming\nDESCRIPTION: This commit message, associated with commit eb691fc013 on 2023-11-23, documents the standardization of the term 'OpenSearch', correcting previous variations like 'Open Search' and 'Opensearch'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nUse 'OpenSearch' instead of 'Open Search' and 'Opensearch' (#35821)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Docs for Jan 2023 Providers Wave\nDESCRIPTION: This commit message, associated with commit 5246c009c5 dated 2023-01-02, indicates the preparation of documentation for the January 2023 release wave of Apache Airflow Providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_19\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs for Jan 2023 wave of Providers (#28651)``\n```\n\n----------------------------------------\n\nTITLE: Listing Commits for Version 3.2.0 (rst)\nDESCRIPTION: This block lists commits associated with version 3.2.0 using reStructuredText table format. It includes commit hashes linked to GitHub, commit dates, and subject lines detailing changes like documentation for the August 2021 provider release, updates to provider metadata descriptions, lazy loading of Hooks, and enhancements to SalesforceHook (JWT, IP filtering, session login).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ==========================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==========================================================================================\n`0a68588479 <https://github.com/apache/airflow/commit/0a68588479e34cf175d744ea77b283d9d78ea71a>`__  2021-08-30   ``Add August 2021 Provider's documentation (#17890)``\n`be75dcd39c <https://github.com/apache/airflow/commit/be75dcd39cd10264048c86e74110365bd5daf8b7>`__  2021-08-23   ``Update description about the new ''connection-types'' provider meta-data``\n`76ed2a49c6 <https://github.com/apache/airflow/commit/76ed2a49c6cd285bf59706cf04f39a7444c382c9>`__  2021-08-19   ``Import Hooks lazily individually in providers manager (#17682)``\n`5c0e98cc77 <https://github.com/apache/airflow/commit/5c0e98cc770b4f055dbd1c0b60ccbd69f3166da7>`__  2021-08-06   ``Adding JWT, IP filtering, and direct session login support for SalesforceHook (#17399)``\n==================================================================================================  ===========  ==========================================================================================\n```\n\n----------------------------------------\n\nTITLE: Displaying Telegram Provider Package Information in RST\nDESCRIPTION: This RST snippet introduces the Telegram provider package for Apache Airflow, including a link to the Telegram website and a reference to the detailed changelog.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nPackage apache-airflow-providers-telegram\n------------------------------------------------------\n\n`Telegram <https://telegram.org/>`__\n\n\nThis is detailed commit list of changes for versions provider package: ``telegram``.\nFor high-level changelog, see :doc:`package information including changelog <index>`.\n```\n\n----------------------------------------\n\nTITLE: Defining Deferrable Operators Section in RST Documentation\nDESCRIPTION: RST documentation section that sets up the deferrable operators listing using a custom Sphinx directive. The directive airflow-deferrable-operators is configured to display operators with no specific tags.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/core-extensions/deferrable-operator-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. airflow-deferrable-operators::\n   :tags: None\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Running Package-Specific Helm Tests\nDESCRIPTION: Command to run tests from a specific package using breeze testing environment\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/helm_unit_tests.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing helm-tests --test-type basic\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fix Parameter Syntax in OpenSearch Docstrings\nDESCRIPTION: This commit message, associated with commit 9e2f1ce4c3 on 2023-11-01, documents the correction of parameter syntax within OpenSearch related docstrings.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nFix parameter syntax in OpenSearch docstrings (#35345)\n```\n\n----------------------------------------\n\nTITLE: Removing 'args' Argument from Airflow Operator Constructors\nDESCRIPTION: This commit message signifies a change across all operator constructors to remove the generic 'args' parameter, likely favouring explicit keyword arguments. It references pull request #10163.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_80\n\nLANGUAGE: text\nCODE:\n```\nChanges to all the constructors to remove the args argument (#10163)\n```\n\n----------------------------------------\n\nTITLE: Apache Airflow Neo4j Provider Package Name\nDESCRIPTION: Package name declaration for the Neo4j provider package in Apache Airflow\nSOURCE: https://github.com/apache/airflow/blob/main/providers/neo4j/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n``apache-airflow-providers-neo4j``\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Waiter Configuration for Redshift Cluster Pause in JSON\nDESCRIPTION: JSON configuration for a custom waiter that checks for a paused Redshift cluster state, including operation, delay, max attempts, and acceptance criteria.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/triggers/README.md#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"version\": 2,\n    \"waiters\": {\n        \"cluster_paused\": {\n            \"operation\": \"DescribeClusters\",\n            \"delay\": 30,\n            \"maxAttempts\": 60,\n            \"acceptors\": [\n                {\n                    \"matcher\": \"pathAll\",\n                    \"argument\": \"Clusters[].ClusterStatus\",\n                    \"expected\": \"paused\",\n                    \"state\": \"success\"\n                },\n                {\n                    \"expected\": \"ClusterNotFound\",\n                    \"matcher\": \"error\",\n                    \"state\": \"retry\"\n                },\n                {\n                    \"expected\": \"deleting\",\n                    \"matcher\": \"pathAny\",\n                    \"state\": \"failure\",\n                    \"argument\": \"Clusters[].ClusterStatus\"\n                }\n            ]\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add Automation for Min Airflow Version\nDESCRIPTION: This commit message, linked to commit eef5bc7f16 dated 2023-05-03, describes the addition of full automation for managing the minimum required Airflow version for providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n``Add full automation for min Airflow version for providers (#30994)``\n```\n\n----------------------------------------\n\nTITLE: Documenting Version 2.0.3 Changes in RST\nDESCRIPTION: A reStructuredText section describing the changes in version 2.0.3 of the DingTalk provider. It mentions the addition of Trove classifiers in PyPI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n2.0.3\n.....\n\nMisc\n~~~~~\n\n* ``Add Trove classifiers in PyPI (Framework :: Apache Airflow :: Provider)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fix and Reapply Provider Documentation Templates\nDESCRIPTION: This commit message, associated with commit 99df205f42 on 2023-11-16, describes fixing and reapplying templates used for generating provider documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nFix and reapply templates for provider documentation (#35686)\n```\n\n----------------------------------------\n\nTITLE: Documenting Jan 2021 Provider Release in RST\nDESCRIPTION: This commit (6c3a67d4fc, committed on 2022-02-05) adds documentation related to the January 2021 release of Airflow providers. Refers to issue #21257.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\nAdd documentation for January 2021 providers release (#21257)\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit Information for HTTP Provider Updates\nDESCRIPTION: This code snippet shows the format used to display commit information for updates to the Apache Airflow HTTP provider. It includes the commit hash, date, and a brief description of the change.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n`fe4605a10e <https://github.com/apache/airflow/commit/fe4605a10e26f1b8a180979ba5765d1cb7fb0111>`__  2024-05-01   ``Prepare docs 1st wave May 2024 (#39328)``\n`ead9b00f7c <https://github.com/apache/airflow/commit/ead9b00f7cd5acecf9d575c459bb62633088436a>`__  2024-04-25   ``Bump minimum Airflow version in providers to Airflow 2.7.0 (#39240)``\n`9619536e6f <https://github.com/apache/airflow/commit/9619536e6f1f5737d56d2ef761c2e4467f17cd4e>`__  2024-04-24   ``Allow trust env parameter to be defined in extra options of HTTP Connection (#39161)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add D400 pydocstyle check - Providers (#31427)\nDESCRIPTION: This commit message, associated with version 3.2.1 (Latest change: 2023-06-20 indicates context for this block), implements the D400 pydocstyle check (imperative mood for first line) specifically for Apache Airflow Provider code. Commit hash: a59076eaee, Date: 2023-06-02.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n``Add D400 pydocstyle check - Providers (#31427)``\n```\n\n----------------------------------------\n\nTITLE: Removing ':type' Lines Due to Sphinx AutoAPI Support\nDESCRIPTION: Excluded Change (Version 2.2.1): Removes redundant ':type' annotations from docstrings, as sphinx-autoapi now supports type hints directly, referencing pull request #20951.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_24\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Remove ':type' lines now sphinx-autoapi supports typehints (#20951)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Docs - Providers December 2023 (1st Wave)\nDESCRIPTION: This commit message, associated with commit 999b70178a on 2023-12-08, describes the preparation of documentation for the first wave of provider releases in December 2023.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 1st wave of Providers December 2023 (#36112)\n```\n\n----------------------------------------\n\nTITLE: RST Directive for Airflow Logging Documentation\nDESCRIPTION: ReStructuredText directive that generates documentation for Airflow logging implementations. The directive specifies no tags and uses a custom header separator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/core-extensions/logging.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. airflow-logging::\n   :tags: None\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: HTML Table of Contents Comment Block\nDESCRIPTION: Auto-generated table of contents section managed by doctoc tool with instructions not to manually edit.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/PROJECT_GUIDELINES.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n- [Adding a Committer or PMC Member](#adding-a-committer-or-pmc-member)\n- [Airflow Improvement Proposals (AIPs)](#airflow-improvement-proposals-aips)\n- [Support for Airflow 1.10.x releases](#support-for-airflow-110x-releases)\n- [Support for Backport Providers](#support-for-backport-providers)\n- [Release Guidelines](#release-guidelines)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n```\n\n----------------------------------------\n\nTITLE: Deprecated Module Import Path\nDESCRIPTION: Example of the deprecated module import path that has been removed from Apache Airflow. This pattern needs to be updated in any code still using contrib modules.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41366.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nairflow.contrib.*\n```\n\n----------------------------------------\n\nTITLE: Rendering Deprecated Objects Table with airflow-deprecations Directive - reStructuredText\nDESCRIPTION: This snippet uses the custom reStructuredText directive ``.. airflow-deprecations::`` to automatically display and summarize deprecated entities within the Apache Airflow Providers packages. It supports optional configuration like tag filtering (set to None here) and table formatting via header separator. The directive requires an environment where Sphinx and airflow-provided extensions are available, and currently does not support argument-level deprecations. Outputs a table of deprecated items when built with Sphinx.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/core-extensions/deprecations.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. airflow-deprecations::\\n   :tags: None\\n   :header-separator: \\\"\n```\n\n----------------------------------------\n\nTITLE: Updating Documentation for December 2021 Provider Release\nDESCRIPTION: Excluded Change (Version 2.2.0): Contains documentation updates related to the December 2021 provider release, referencing pull request #20523.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_28\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Update documentation for provider December 2021 release (#20523)``\n```\n\n----------------------------------------\n\nTITLE: Specify Package Name\nDESCRIPTION: Declares the name of the Apache Airflow provider package being documented, which is 'apache-airflow-providers-pgvector'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\napache-airflow-providers-pgvector\n```\n\n----------------------------------------\n\nTITLE: Using KubernetesPatchJobOperator in Python\nDESCRIPTION: Demonstrates the use of KubernetesPatchJobOperator to update existing Jobs on a Kubernetes cluster. It shows how to specify the patch for updating job properties.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/operators.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nKubernetesPatchJobOperator(\n    task_id=\"update_job\",\n    namespace=\"default\",\n    job_name=\"airflow-job-test\",\n    patch={\"spec\": {\"parallelism\": 2}},\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Yandex Provider for Airflow using Bash\nDESCRIPTION: Installs the necessary Apache Airflow provider package for Yandex Cloud integration using pip. This is a prerequisite for utilizing the Yandex Lockbox secret backend.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-yandex\n```\n\n----------------------------------------\n\nTITLE: Deploying from Private Registry\nDESCRIPTION: Helm command to deploy Airflow using an image from a private registry with credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airflow apache-airflow/airflow \\\n  --set images.airflow.repository=my-company/airflow \\\n  --set images.airflow.tag=8a0da78 \\\n  --set images.airflow.pullPolicy=Always \\\n  --set registry.secretName=gitlab-registry-credentials\n```\n\n----------------------------------------\n\nTITLE: Building Airflow CI Image with Upgraded Dependencies in Bash\nDESCRIPTION: This command builds the Airflow CI image using Breeze, upgrading to newer dependencies. It's useful when there are conflicting changes in dependencies between old constraints and new specifications.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/10_advanced_breeze_topics.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image build --upgrade-to-newer-dependencies\n```\n\n----------------------------------------\n\nTITLE: Encoding Private Key for Snowflake Connection in Python\nDESCRIPTION: This Python snippet is provided to help users base64-encode the contents of a Snowflake private key file before using it in the 'private_key_content' parameter. It requires the built-in 'base64' module and reads a private key from the file system, encodes it, and prints the encoded string, which is intended for use in Airflow Snowflake connection configurations. The script expects the path to the PEM file to be provided, reads the file in binary mode, encodes it, decodes the result to a UTF-8 string, and prints it; users should ensure the file path is correct and that the private key is protected appropriately.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport base64\\n\\nwith open(\"path/to/your/private_key.pem\", \"rb\") as key_file:\\n    encoded_key = base64.b64encode(key_file.read()).decode(\"utf-8\")\\n    print(encoded_key)\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Documentation in RST\nDESCRIPTION: ReStructuredText directive to include external documentation about installing providers from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openai/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 2.0.0\nDESCRIPTION: ReStructuredText formatted changelog entry documenting breaking changes for version 2.0.0 of the Asana provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n2.0.0\n.....\n\nBreaking changes\n~~~~~~~~~~~~~~~~\n\n.. note::\n  This release of provider is only available for Airflow 2.2+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n```\n\n----------------------------------------\n\nTITLE: Disabling Source Code Collection Using Environment Variable\nDESCRIPTION: Example of disabling OpenLineage source code collection using an environment variable as an alternative to airflow.cfg configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_14\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__DISABLE_SOURCE_CODE=true\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit Information in Markdown Table\nDESCRIPTION: This code snippet shows how commit information is formatted in a markdown table, including the commit hash, date, and subject.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ftp/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ====================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ====================================================================\n`16adc035b1 <https://github.com/apache/airflow/commit/16adc035b1ecdf533f44fbb3e32bea972127bb71>`__  2022-03-14   ``Add documentation for Classifier release for March 2022 (#22226)``\n==================================================================================================\n```\n\n----------------------------------------\n\nTITLE: Viewing Git Commit History for Apache Airflow\nDESCRIPTION: A formatted log of Git commits showing the commit hash, date, and commit message for changes to the Apache Airflow project. Documents version control history in chronological order.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n88bdcfa0df <https://github.com/apache/airflow/commit/88bdcfa0df5bcb4c489486e05826544b428c8f43>  2021-02-04   \"Prepare to release a new wave of providers. (#14013)\"\nac2f72c98d <https://github.com/apache/airflow/commit/ac2f72c98dc0821b33721054588adbf2bb53bb0b>  2021-02-01   \"Implement provider versioning tools (#13767)\"\n3fd5ef3555 <https://github.com/apache/airflow/commit/3fd5ef355556cf0ad7896bb570bbe4b2eabbf46e>  2021-01-21   \"Add missing logos for integrations (#13717)\"\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs 1st wave Providers (Dec 2023)\nDESCRIPTION: This commit message (hash 999b70178a, dated 2023-12-08) marks the preparation of documentation for the first wave of provider releases in December 2023 (issue #36112).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_21\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs 1st wave of Providers December 2023 (#36112)``\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit History for Apache Cassandra Provider\nDESCRIPTION: This code snippet shows a table of commit history for the Apache Cassandra provider, including commit hashes, dates, and commit messages. It covers changes made in recent versions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/cassandra/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ================================================\n`34500f3a2f <https://github.com/apache/airflow/commit/34500f3a2fa4652272bc831e3c18fd2a6a2da5ef>`__  2024-05-26   ``Prepare docs 3rd wave May 2024 (#39738)``\n`2b1a2f8d56 <https://github.com/apache/airflow/commit/2b1a2f8d561e569df194c4ee0d3a18930738886e>`__  2024-05-11   ``Reapply templates for all providers (#39554)``\n`2c05187b07 <https://github.com/apache/airflow/commit/2c05187b07baf7c41a32b18fabdbb3833acc08eb>`__  2024-05-10   ``Faster 'airflow_version' imports (#39552)``\n`73918925ed <https://github.com/apache/airflow/commit/73918925edaf1c94790a6ad8bec01dec60accfa1>`__  2024-05-08   ``Simplify 'airflow_version' imports (#39497)``\n==================================================================================================  ===========  ================================================\n```\n\n----------------------------------------\n\nTITLE: Installing Slack Provider via pip\nDESCRIPTION: Command to install the Slack provider package on top of an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-slack\n```\n\n----------------------------------------\n\nTITLE: Package Version Declaration\nDESCRIPTION: Package name declaration for the Apache Spark provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n\"apache-airflow-providers-apache-spark\"\n```\n\n----------------------------------------\n\nTITLE: Logging Configuration Updates in Airflow\nDESCRIPTION: Movement of child process log directory configuration from scheduler to logging section, with updated default path from AIRFLOW_HOME/logs/scheduler to AIRFLOW_HOME/logs/dag-processor.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/aip-66.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[scheduler] child_process_log_directory  [logging] dag_processor_child_process_log_directory\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Apply PEP-563 to Non-Core Airflow\nDESCRIPTION: This commit message, associated with commit 06acf40a43 dated 2022-09-13, describes the application of PEP-563 (Postponed Evaluation of Annotations) to non-core Airflow components, likely including providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_27\n\nLANGUAGE: text\nCODE:\n```\n``Apply PEP-563 (Postponed Evaluation of Annotations) to non-core airflow (#26289)``\n```\n\n----------------------------------------\n\nTITLE: Including External RST File in Sphinx Documentation\nDESCRIPTION: This reStructuredText directive instructs the Sphinx documentation generator to include the content of the specified file (`installing-providers-from-sources.rst`) at this location during the build process. It's used to incorporate common or lengthy content blocks, promoting reusability and maintainability in documentation. The path is relative to the source directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Image Upload to KinD Cluster\nDESCRIPTION: Example output showing the upload process of an Airflow image to a KinD cluster, including verification of required tools, image identification, and loading status on cluster nodes.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\nK8S Virtualenv is initialized in /Users/jarek/IdeaProjects/airflow/kubernetes-tests/.venv\nGood version of kind installed: 0.14.0 in /Users/jarek/IdeaProjects/airflow/kubernetes-tests/.venv/bin\nGood version of kubectl installed: 1.25.0 in /Users/jarek/IdeaProjects/airflow/kubernetes-tests/.venv/bin\nGood version of helm installed: 3.9.2 in /Users/jarek/IdeaProjects/airflow/kubernetes-tests/.venv/bin\nStable repo is already added\nUploading Airflow image ghcr.io/apache/airflow/main/prod/python3.9-kubernetes to cluster airflow-python-3.9-v1.24.2\nImage: \"ghcr.io/apache/airflow/main/prod/python3.9-kubernetes\" with ID \"sha256:fb6195f7c2c2ad97788a563a3fe9420bf3576c85575378d642cd7985aff97412\" not yet present on node \"airflow-python-3.9-v1.24.2-worker\", loading...\nImage: \"ghcr.io/apache/airflow/main/prod/python3.9-kubernetes\" with ID \"sha256:fb6195f7c2c2ad97788a563a3fe9420bf3576c85575378d642cd7985aff97412\" not yet present on node \"airflow-python-3.9-v1.24.2-control-plane\", loading...\n\nNEXT STEP: You might now deploy Airflow by:\n\nbreeze k8s deploy-airflow\n```\n\n----------------------------------------\n\nTITLE: Faster airflow_version Imports (v1.2.1)\nDESCRIPTION: Implements changes to speed up the import process for 'airflow_version', referenced by pull request #39552.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_15\n\nLANGUAGE: plaintext\nCODE:\n```\nFaster 'airflow_version' imports (#39552)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with github-enterprise Extra\nDESCRIPTION: This command installs Apache Airflow with the github-enterprise extra, which enables the GitHub Enterprise auth backend.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[github-enterprise]'\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Bump minimum Airflow version in providers (#30917)\nDESCRIPTION: This commit message, associated with version 3.2.0, increases the minimum required Airflow version dependency for Apache Airflow Providers. Commit hash: a7eb32a5b2, Date: 2023-04-30.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n``Bump minimum Airflow version in providers (#30917)``\n```\n\n----------------------------------------\n\nTITLE: Updating Airflow Oracle Provider Documentation\nDESCRIPTION: This commit message indicates updates made to the Oracle provider documentation (Oracle.rst) within Airflow. It references pull request #13871.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_43\n\nLANGUAGE: text\nCODE:\n```\nUpdates Oracle.rst documentation (#13871)\n```\n\n----------------------------------------\n\nTITLE: Including Configuration Sections and Options in RST\nDESCRIPTION: This RST directive includes a file that likely contains detailed sections and options for Airflow configuration. It's used to dynamically insert configuration documentation into the main document.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../devel-common/src/sphinx_exts/includes/sections-and-options.rst\n```\n\n----------------------------------------\n\nTITLE: Cleaning Git Checkout in Shell\nDESCRIPTION: Forcefully removes all untracked files and directories, including ignored files, from the Git working directory. It also removes any contents from the 'dist' directory to ensure a clean build environment. Caution: This can remove IDE settings or other local configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ngit clean -fxd\nrm -rf dist/*\n```\n\n----------------------------------------\n\nTITLE: Enabling String Normalization in Python Formatting\nDESCRIPTION: Excluded Change (Version 3.1.0): Enables string normalization within Python code formatting rules for providers, referencing pull request #27205.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Enable string normalization in python formatting - providers (#27205)``\n```\n\n----------------------------------------\n\nTITLE: Preparing Documentation for July Provider RC2 Release\nDESCRIPTION: Excluded Change (Version 2.0.1): Includes documentation preparation tasks for the second release candidate (Rc2) of the July providers, referencing pull request #17116.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_34\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Prepares docs for Rc2 release of July providers (#17116)``\n```\n\n----------------------------------------\n\nTITLE: GitHub Container Registry URL Pattern\nDESCRIPTION: URL pattern for accessing container images in GitHub Container Registry UI\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_26\n\nLANGUAGE: markdown\nCODE:\n```\nhttps://github.com/apache/airflow/pkgs/container/<CONTAINER_NAME>\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Airflow DAG in Python\nDESCRIPTION: This snippet imports necessary modules and classes from Airflow and other libraries to create a DAG and its tasks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/.latest-doc-only-change.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import PythonOperator\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.utils.task_group import TaskGroup\n```\n\n----------------------------------------\n\nTITLE: Importing Apache Airflow Client\nDESCRIPTION: Python code showing how to import the Airflow client library.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport airflow_client.client\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Update package description to remove double min-airflow specification (#24292)\nDESCRIPTION: This commit message, associated with version 3.0.0, corrects the package description to eliminate redundant specification of the minimum Airflow version requirement. Commit hash: 717a7588bc, Date: 2022-06-07.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_30\n\nLANGUAGE: text\nCODE:\n```\n``Update package description to remove double min-airflow specification (#24292)``\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte Provider using pip (Shell Command)\nDESCRIPTION: This shell command utilizes `pip`, the Python package installer, to install the `apache-airflow-providers-airbyte` package. Successful execution requires Python, pip, and a compatible Apache Airflow version (>=2.9.0) to be installed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-airbyte\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Airflow Tutorials in reStructuredText\nDESCRIPTION: This snippet defines a table of contents for Airflow tutorials using reStructuredText syntax. It includes links to various tutorial topics such as fundamentals, taskflow, pipeline, and object storage.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n\n    fundamentals\n    taskflow\n    pipeline\n    objectstorage\n```\n\n----------------------------------------\n\nTITLE: Installing JDBC Provider with Common SQL Dependencies\nDESCRIPTION: Command to install the JDBC provider package with its cross-provider dependencies, specifically the common.sql extra.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-jdbc[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Formatting Changelog Version Headers\nDESCRIPTION: Markdown formatting showing version numbers and commit details in a table format with alignment and borders\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/cassandra/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`dcdcf3a2b8 <https://github.com/apache/airflow/commit/dcdcf3a2b8054fa727efb4cd79d38d2c9c7e1bd5>`__  2022-06-09   ``Update release notes for RC2 release of Providers for May 2022 (#24307)``\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry v2.0.0\nDESCRIPTION: Changelog entry documenting breaking changes related to apply_default decorator removal\nSOURCE: https://github.com/apache/airflow/blob/main/providers/datadog/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n2.0.0\n.....\n\nBreaking changes\n~~~~~~~~~~~~~~~~\n\n* ``Auto-apply apply_default decorator (#15667)``\n```\n\n----------------------------------------\n\nTITLE: Auto-Generating CLI Argument Documentation with Sphinx Argparse - ReStructuredText\nDESCRIPTION: This snippet uses the Sphinx .. argparse:: directive to include help text for a command-line interface defined in Python within documentation. It points to the airflow.providers.amazon.aws.auth_manager.aws_auth_manager module, using the get_parser function to retrieve an argparse parser for the 'airflow' CLI. Sphinx automatically calls the parser and embeds parameter help, requirements, and descriptions into the document; no dependencies beyond Sphinx with the sphinx-argparse extension are required. All CLI argument parsing context is sourced live from the referenced Python code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/cli-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: ReStructuredText\nCODE:\n```\n.. argparse::\n   :module: airflow.providers.amazon.aws.auth_manager.aws_auth_manager\n   :func: get_parser\n   :prog: airflow\n```\n\n----------------------------------------\n\nTITLE: Output of Breeze Cluster Configuration\nDESCRIPTION: This text block shows the expected output after running `breeze k8s configure-cluster`. It logs the process of deleting (if existing) and creating namespaces, and deploying test resources like persistent volumes and services, confirming the cluster is ready for Airflow deployment.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nConfiguring airflow-python-3.9-v1.24.2 to be ready for Airflow deployment\nDeleting K8S namespaces for kind-airflow-python-3.9-v1.24.2\nError from server (NotFound): namespaces \"airflow\" not found\nError from server (NotFound): namespaces \"test-namespace\" not found\nCreating namespaces\nnamespace/airflow created\nnamespace/test-namespace created\nCreated K8S namespaces for cluster kind-airflow-python-3.9-v1.24.2\n\nDeploying test resources for cluster kind-airflow-python-3.9-v1.24.2\npersistentvolume/test-volume created\npersistentvolumeclaim/test-volume created\nservice/airflow-webserver-node-port created\nDeployed test resources for cluster kind-airflow-python-3.9-v1.24.2\n\n\nNEXT STEP: You might now build your k8s image by:\n\nbreeze k8s build-k8s-image\n```\n\n----------------------------------------\n\nTITLE: Refactoring remove_task Decorator using CST\nDESCRIPTION: Commit message describing a refactoring of the 'remove_task' decorator located in 'utils/decorators' using Concrete Syntax Trees (CST). References pull request #43383.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_33\n\nLANGUAGE: plaintext\nCODE:\n```\nrefactor(utils/decorators): rewrite remove task decorator to use cst (#43383)\n```\n\n----------------------------------------\n\nTITLE: Listing Commit for Airflow Provider Version 3.0.4\nDESCRIPTION: This section lists a single commit related to the Apache Airflow provider version 3.0.4 bugfix release. The commit focuses on adding documentation for this specific release. It includes the commit hash linked to GitHub, commit date, and subject line.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n==================================================================================================  ===========  ==============================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==============================================================\n`d7dbfb7e26 <https://github.com/apache/airflow/commit/d7dbfb7e26a50130d3550e781dc71a5fbcaeb3d2>`__  2022-03-22   ``Add documentation for bugfix release of Providers (#22383)``\n==================================================================================================  ===========  ==============================================================\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow with CeleryExecutor using Breeze\nDESCRIPTION: This command starts Airflow using the CeleryExecutor in the Breeze development environment.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nbreeze start-airflow --executor CeleryExecutor\n```\n\n----------------------------------------\n\nTITLE: Registering Custom OpenLineage Run Facets via Environment Variable\nDESCRIPTION: Sets the `AIRFLOW__OPENLINEAGE__CUSTOM_RUN_FACETS` environment variable with a semicolon-separated list of custom run facet function import paths. This enables the inclusion of custom facets in OpenLineage events as an alternative to INI configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_22\n\nLANGUAGE: ini\nCODE:\n```\nAIRFLOW__OPENLINEAGE__CUSTOM_RUN_FACETS='full.path.to.get_my_custom_facet;full.path.to.another_custom_facet_function'\n```\n\n----------------------------------------\n\nTITLE: Preparing Documentation for RC2 Provider Release in RST\nDESCRIPTION: This commit (bbc627a3da, committed on 2021-06-18) prepares the documentation for the Release Candidate 2 (RC2) of certain providers. Refers to issue #16501.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_26\n\nLANGUAGE: rst\nCODE:\n```\nPrepares documentation for rc2 release of Providers (#16501)\n```\n\n----------------------------------------\n\nTITLE: Installing YDB Provider Package via pip\nDESCRIPTION: Command to install the YDB provider package on top of an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-ydb\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry Version 1.1.4\nDESCRIPTION: Documentation for version 1.1.4 of the Microsoft Provider, noting bug fixes related to PowerShell Remoting.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/psrp/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n1.1.4\n.....\n\nBug Fixes\n~~~~~~~~~\n\n* ``PowerShell Remoting fail on non-zero exitcode (#22503)``\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry Format in Markdown\nDESCRIPTION: Structured format for displaying version history with commit hashes, dates and descriptions in a tabular markdown format\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ===================================================================================================\n`21990ed894 <https://github.com/apache/airflow/commit/21990ed8943ee4dc6e060ee2f11648490c714a3b>`__  2023-09-08   ``Prepare docs for 09 2023 - 1st wave of Providers (#34201)``\n`9d8c77e447 <https://github.com/apache/airflow/commit/9d8c77e447f5515b9a6aa85fa72511a86a128c28>`__  2023-08-27   ``Improve modules import in Airflow providers by some of them into a type-checking block (#33754)``\n==================================================================================================  ===========  ===================================================================================================\n```\n\n----------------------------------------\n\nTITLE: Version Header Format\nDESCRIPTION: ReStructuredText formatting for version number headers in changelog\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_8\n\nLANGUAGE: rst\nCODE:\n```\n8.17.0\n......\n```\n\n----------------------------------------\n\nTITLE: Specifying Requirements for Apache Cassandra Provider Package\nDESCRIPTION: Table showing the required PIP packages and their versions for the Apache Cassandra provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/cassandra/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n====================  ==================\nPIP package           Version required\n====================  ==================\n\"apache-airflow\"    \">=2.9.0\"\n\"cassandra-driver\"  \">=3.29.1\"\n====================  ==================\n```\n\n----------------------------------------\n\nTITLE: Updating Description for 'connection-types' Metadata in RST\nDESCRIPTION: This commit (be75dcd39c, committed on 2021-08-23) updates the description related to the new 'connection-types' provider metadata.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_20\n\nLANGUAGE: rst\nCODE:\n```\nUpdate description about the new ''connection-types'' provider meta-data\n```\n\n----------------------------------------\n\nTITLE: Fixing K8S Changelog PyPI Compatibility in RST\nDESCRIPTION: This commit (f77417eb0d, committed on 2021-12-31) fixes the Kubernetes (K8S) provider changelog to ensure it is compatible with PyPI rendering standards. Refers to issue #20614.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_8\n\nLANGUAGE: rst\nCODE:\n```\nFix K8S changelog to be PyPI-compatible (#20614)\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Full Text\nDESCRIPTION: The complete text of Apache License Version 2.0, defining terms, conditions, and limitations for software usage, reproduction, and distribution. Includes definitions of key terms, copyright and patent licenses, redistribution requirements, and liability limitations.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/license.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n                                 Apache License\n                            Version 2.0, January 2004\n                         http://www.apache.org/licenses/\n\n    TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n    1. Definitions.\n\n       \"License\" shall mean the terms and conditions for use, reproduction,\n       and distribution as defined by Sections 1 through 9 of this document.\n\n       \"Licensor\" shall mean the copyright owner or entity authorized by\n       the copyright owner that is granting the License.\n\n       \"Legal Entity\" shall mean the union of the acting entity and all\n       other entities that control, are controlled by, or are under common\n       control with that entity. For the purposes of this definition,\n       \"control\" means (i) the power, direct or indirect, to cause the\n       direction or management of such entity, whether by contract or\n       otherwise, or (ii) ownership of fifty percent (50%) or more of the\n       outstanding shares, or (iii) beneficial ownership of such entity.\n\n       \"You\" (or \"Your\") shall mean an individual or Legal Entity\n       exercising permissions granted by this License.\n\n       \"Source\" form shall mean the preferred form for making modifications,\n       including but not limited to software source code, documentation\n       source, and configuration files.\n\n       \"Object\" form shall mean any form resulting from mechanical\n       transformation or translation of a Source form, including but\n       not limited to compiled object code, generated documentation,\n       and conversions to other media types.\n\n       \"Work\" shall mean the work of authorship, whether in Source or\n       Object form, made available under the License, as indicated by a\n       copyright notice that is included in or attached to the work\n       (an example is provided in the Appendix below).\n\n       \"Derivative Works\" shall mean any work, whether in Source or Object\n       form, that is based on (or derived from) the Work and for which the\n       editorial revisions, annotations, elaborations, or other modifications\n       represent, as a whole, an original work of authorship. For the purposes\n       of this License, Derivative Works shall not include works that remain\n       separable from, or merely link (or bind by name) to the interfaces of,\n       the Work and Derivative Works thereof.\n\n       \"Contribution\" shall mean any work of authorship, including\n       the original version of the Work and any modifications or additions\n       to that Work or Derivative Works thereof, that is intentionally\n       submitted to Licensor for inclusion in the Work by the copyright owner\n       or by an individual or Legal Entity authorized to submit on behalf of\n       the copyright owner. For the purposes of this definition, \"submitted\"\n       means any form of electronic, verbal, or written communication sent\n       to the Licensor or its representatives, including but not limited to\n       communication on electronic mailing lists, source code control systems,\n       and issue tracking systems that are managed by, or on behalf of, the\n       Licensor for the purpose of discussing and improving the Work, but\n       excluding communication that is conspicuously marked or otherwise\n       designated in writing by the copyright owner as \"Not a Contribution.\"\n\n       \"Contributor\" shall mean Licensor and any individual or Legal Entity\n       on behalf of whom a Contribution has been received by Licensor and\n       subsequently incorporated within the Work.\n\n    2. Grant of Copyright License. Subject to the terms and conditions of\n       this License, each Contributor hereby grants to You a perpetual,\n       worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n       copyright license to reproduce, prepare Derivative Works of,\n       publicly display, publicly perform, sublicense, and distribute the\n       Work and such Derivative Works in Source or Object form.\n\n    3. Grant of Patent License. Subject to the terms and conditions of\n       this License, each Contributor hereby grants to You a perpetual,\n       worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n       (except as stated in this section) patent license to make, have made,\n       use, offer to sell, sell, import, and otherwise transfer the Work,\n       where such license applies only to those patent claims licensable\n       by such Contributor that are necessarily infringed by their\n       Contribution(s) alone or by combination of their Contribution(s)\n       with the Work to which such Contribution(s) was submitted. If You\n       institute patent litigation against any entity (including a\n       cross-claim or counterclaim in a lawsuit) alleging that the Work\n       or a Contribution incorporated within the Work constitutes direct\n       or contributory patent infringement, then any patent licenses\n       granted to You under this License for that Work shall terminate\n       as of the date such litigation is filed.\n\n    4. Redistribution. You may reproduce and distribute copies of the\n       Work or Derivative Works thereof in any medium, with or without\n       modifications, and in Source or Object form, provided that You\n       meet the following conditions:\n\n       (a) You must give any other recipients of the Work or\n           Derivative Works a copy of this License; and\n\n       (b) You must cause any modified files to carry prominent notices\n           stating that You changed the files; and\n\n       (c) You must retain, in the Source form of any Derivative Works\n           that You distribute, all copyright, patent, trademark, and\n           attribution notices from the Source form of the Work,\n           excluding those notices that do not pertain to any part of\n           the Derivative Works; and\n\n       (d) If the Work includes a \"NOTICE\" text file as part of its\n           distribution, then any Derivative Works that You distribute must\n           include a readable copy of the attribution notices contained\n           within such NOTICE file, excluding those notices that do not\n           pertain to any part of the Derivative Works, in at least one\n           of the following places: within a NOTICE text file distributed\n           as part of the Derivative Works; within the Source form or\n           documentation, if provided along with the Derivative Works; or,\n           within a display generated by the Derivative Works, if and\n           wherever such third-party notices normally appear. The contents\n           of the NOTICE file are for informational purposes only and\n           do not modify the License. You may add Your own attribution\n           notices within Derivative Works that You distribute, alongside\n           or as an addendum to the NOTICE text from the Work, provided\n           that such additional attribution notices cannot be construed\n           as modifying the License.\n\n       You may add Your own copyright statement to Your modifications and\n       may provide additional or different license terms and conditions\n       for use, reproduction, or distribution of Your modifications, or\n       for any such Derivative Works as a whole, provided Your use,\n       reproduction, and distribution of the Work otherwise complies with\n       the conditions stated in this License.\n\n    5. Submission of Contributions. Unless You explicitly state otherwise,\n       any Contribution intentionally submitted for inclusion in the Work\n       by You to the Licensor shall be under the terms and conditions of\n       this License, without any additional terms or conditions.\n       Notwithstanding the above, nothing herein shall supersede or modify\n       the terms of any separate license agreement you may have executed\n       with Licensor regarding such Contributions.\n\n    6. Trademarks. This License does not grant permission to use the trade\n       names, trademarks, service marks, or product names of the Licensor,\n       except as required for reasonable and customary use in describing the\n       origin of the Work and reproducing the content of the NOTICE file.\n\n    7. Disclaimer of Warranty. Unless required by applicable law or\n       agreed to in writing, Licensor provides the Work (and each\n       Contributor provides its Contributions) on an \"AS IS\" BASIS,\n       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n       implied, including, without limitation, any warranties or conditions\n       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n       PARTICULAR PURPOSE. You are solely responsible for determining the\n       appropriateness of using or redistributing the Work and assume any\n       risks associated with Your exercise of permissions under this License.\n\n    8. Limitation of Liability. In no event and under no legal theory,\n       whether in tort (including negligence), contract, or otherwise,\n       unless required by applicable law (such as deliberate and grossly\n       negligent acts) or agreed to in writing, shall any Contributor be\n```\n\n----------------------------------------\n\nTITLE: Updating Minimum Airflow Version for Providers to 2.6.0\nDESCRIPTION: Updates the minimum required Apache Airflow version to 2.6.0 across various providers, ensuring compatibility and leveraging newer core features, referenced by issue #36017.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nBump minimum Airflow version in providers to Airflow 2.6.0 (#36017)\n```\n\n----------------------------------------\n\nTITLE: Listing Airflow Provider Commits (v3.2.0) using RST\nDESCRIPTION: This reStructuredText snippet shows a table of commits for the Apache Airflow provider release wave around version 3.2.0 (April-May 2023). It includes linked commit hashes, dates, and subjects covering documentation preparation, fixes for version handling and imports, index file corrections, automation for minimum Airflow version checks, and the addition of a mechanism to suspend providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/winrm/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ======================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ======================================================================================\n`45548b9451 <https://github.com/apache/airflow/commit/45548b9451fba4e48c6f0c0ba6050482c2ea2956>`__  2023-05-19   ``Prepare RC2 docs for May 2023 wave of Providers (#31416)``\n`abea189022 <https://github.com/apache/airflow/commit/abea18902257c0250fedb764edda462f9e5abc84>`__  2023-05-18   ``Use '__version__' in providers not 'version' (#31393)``\n`f5aed58d9f <https://github.com/apache/airflow/commit/f5aed58d9fb2137fa5f0e3ce75b6709bf8393a94>`__  2023-05-18   ``Fixing circular import error in providers caused by airflow version check (#31379)``\n`7ebda3898d <https://github.com/apache/airflow/commit/7ebda3898db2eee72d043a9565a674dea72cd8fa>`__  2023-05-17   ``Fix missing line in index.rst for provider documentation (#31343)``\n`d9ff55cf6d <https://github.com/apache/airflow/commit/d9ff55cf6d95bb342fed7a87613db7b9e7c8dd0f>`__  2023-05-16   ``Prepare docs for May 2023 wave of Providers (#31252)``\n`eef5bc7f16 <https://github.com/apache/airflow/commit/eef5bc7f166dc357fea0cc592d39714b1a5e3c14>`__  2023-05-03   ``Add full automation for min Airflow version for providers (#30994)``\n`a7eb32a5b2 <https://github.com/apache/airflow/commit/a7eb32a5b222e236454d3e474eec478ded7c368d>`__  2023-04-30   ``Bump minimum Airflow version in providers (#30917)``\n`d23a3bbed8 <https://github.com/apache/airflow/commit/d23a3bbed89ae04369983f21455bf85ccc1ae1cb>`__  2023-04-04   ``Add mechanism to suspend providers (#30422)``\n==================================================================================================  ===========  ======================================================================================\n```\n\n----------------------------------------\n\nTITLE: Preparing Docs for 1st Wave (RC1) April 2024 (Commit Message)\nDESCRIPTION: Commit message indicating the preparation of documentation for the first wave (Release Candidate 1) of Apache Airflow provider releases in April 2024, relevant to the Airbyte provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs 1st wave (RC1) April 2024 (#38863)\n```\n\n----------------------------------------\n\nTITLE: YDB Provider Package Name Declaration\nDESCRIPTION: Package name declaration for the YDB provider in restructuredtext format\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n``apache-airflow-providers-ydb``\n```\n\n----------------------------------------\n\nTITLE: Including Security Information in ReStructuredText for Apache Airflow\nDESCRIPTION: This snippet includes an external file containing security information for the Apache Airflow project using ReStructuredText directive. It references a file located in the project's development common source directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/druid/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos Authentication Backend in Airflow FAB (INI)\nDESCRIPTION: Configures the FAB API to use Kerberos authentication. This requires setting the `auth_backends` key in the `[fab]` section to the Kerberos module and specifying the Kerberos keytab file path in the `[kerberos]` section. Requires a properly configured Kerberos environment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/api-authentication.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[fab]\nauth_backends = airflow.providers.fab.auth_manager.api.auth.backend.kerberos_auth\n\n[kerberos]\nkeytab = <KEYTAB>\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Docs - Providers October 2023 (2nd Wave)\nDESCRIPTION: This commit message, associated with commit 39e611b43b on 2023-10-18, describes the preparation of documentation for the second wave of provider releases in October 2023.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_15\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 2nd wave of Providers in October 2023 (#35020)\n```\n\n----------------------------------------\n\nTITLE: Preparing Docs for 1st Wave (RC1) March 2024 (Commit Message)\nDESCRIPTION: Commit message indicating the preparation of documentation for the first wave (Release Candidate 1) of Apache Airflow provider releases in March 2024, including Airbyte.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_19\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs 1st wave (RC1) March 2024 (#37876)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add Note About Dropping Python 3.7 Support\nDESCRIPTION: This commit message, associated with commit 8b146152d6 dated 2023-06-20, notes the addition of a notification regarding the discontinuation of Python 3.7 support for Apache Airflow providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n``Add note about dropping Python 3.7 for providers (#32015)``\n```\n\n----------------------------------------\n\nTITLE: Configuring SFTP Provider Package\nDESCRIPTION: Package name definition for the Apache Airflow SFTP provider package used for installation and dependency management.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"apache-airflow-providers-sftp\"\n```\n\n----------------------------------------\n\nTITLE: Defining Image Optimization Options for Apache Airflow Docker Image\nDESCRIPTION: This snippet outlines the image optimization options available for building Apache Airflow Docker images. It includes an argument for upgrading dependencies to newer versions, which can be used to produce more optimized images.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build-arg-ref.rst#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n+-------------------------------------+------------------------------------------+------------------------------------------+\n| Build argument                      | Default value                            | Description                              |\n+=====================================+==========================================+==========================================+\n| ``UPGRADE_RANDOM_INDICATOR_STRING`` |                                          | If set to a random, non-empty value      |\n|                                     |                                          | the dependencies are upgraded to newer   |\n|                                     |                                          | versions. In CI it is set to build id    |\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Use '__version__' in Providers\nDESCRIPTION: This commit message, linked to commit abea189022 dated 2023-05-18, notes a change to use the '__version__' attribute instead of 'version' within Apache Airflow providers for consistency.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n``Use '__version__' in providers not 'version' (#31393)``\n```\n\n----------------------------------------\n\nTITLE: Git Commit References with Change Descriptions\nDESCRIPTION: A series of git commit references with timestamps and change descriptions, showing updates to AWS provider components like EmrServerless, ECS, S3, and other AWS service integrations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\need6427b66 <https://github.com/apache/airflow/commit/eed6427b66e5af51d7e6ff4afb5f7115b2754cf3>  2023-12-01   \"Avoid creating the hook in the EmrServerlessCancelJobsTrigger init (#35992)\"\n8346fd58e8 <https://github.com/apache/airflow/commit/8346fd58e8290ba9c002a31659443601e941228e>  2023-12-01   \"Fix a bug with accessing hooks in EKS trigger (#35989)\"\n[...]\n```\n\n----------------------------------------\n\nTITLE: Split Providers into UV Workspace Project (Excluded from v1.4.0 Changelog)\nDESCRIPTION: Describes the structural change of splitting providers out of the main 'airflow/' source tree into a UV workspace project, referenced by pull request #42505. This change was intentionally excluded from the main changelog notes for version 1.4.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nSplit providers out of the main \"airflow/\" tree into a UV workspace project (#42505)\n```\n\n----------------------------------------\n\nTITLE: Including License and Documentation Sections in reStructuredText with Sphinx\nDESCRIPTION: This snippet shows how to include standard ASF license text and external documentation files in a reStructuredText (reST) document using the .. include:: Sphinx directive. It depends on the Sphinx documentation builder, and assumes the referenced files exist at the specified relative paths. This method enables reuse of central documentation content, such as shared configuration references or section templates. Inputs are the existing include files; output is an assembled documentation page. This approach is for documentation, not code execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n.. include:: /../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst\n.. include:: /../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst\n```\n\n----------------------------------------\n\nTITLE: Configuring Provider Entry Point in TOML\nDESCRIPTION: Example pyproject.toml configuration showing how to define the apache_airflow_provider entry point for a custom provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/howto/create-custom-providers.rst#2025-04-22_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[project.entry-points.\"apache_airflow_provider\"]\nprovider_info = \"airflow.providers.myproviderpackage.get_provider_info:get_provider_info\"\n```\n\n----------------------------------------\n\nTITLE: Updating AlloyDB Backup with Airflow Operator\nDESCRIPTION: Uses AlloyDBUpdateBackupOperator to update an existing backup of an AlloyDB instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/alloy_db.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nupdate_backup = AlloyDBUpdateBackupOperator(\n    task_id=\"update_backup\",\n    project_id=GCP_PROJECT_ID,\n    cluster_id=CLUSTER_ID,\n    backup_id=BACKUP_ID,\n    body=ALLOYDB_BACKUP,\n    region=GCP_ALLOYDB_REGION,\n)\n```\n\n----------------------------------------\n\nTITLE: Airflow Notifications Directive\nDESCRIPTION: RST directive for displaying Apache Airflow notifications documentation with specified configuration for tags and header separator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/core-extensions/notifications.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. airflow-notifications::\n   :tags: None\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: S3 File Copying with Conditional Logic\nDESCRIPTION: Example of using mapped tasks for conditional S3 file copying based on file extensions.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlist_filenames = S3ListOperator(...)  # Same as the above example.\n\n\n@task\ndef create_copy_kwargs(filename):\n    if filename.rsplit(\".\", 1)[-1] not in (\"json\", \"yml\"):\n        dest_bucket_name = \"my_text_bucket\"\n    else:\n        dest_bucket_name = \"my_other_bucket\"\n    return {\n        \"source_bucket_key\": filename,\n        \"dest_bucket_key\": filename,\n        \"dest_bucket_name\": dest_bucket_name,\n    }\n\n\ncopy_kwargs = create_copy_kwargs.expand(filename=list_filenames.output)\n\n# Copy files to another bucket, based on the file's extension.\ncopy_filenames = S3CopyObjectOperator.partial(\n    task_id=\"copy_files\", source_bucket_name=list_filenames.bucket\n).expand_kwargs(copy_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Updating Module Docstrings with Missing Params in RST\nDESCRIPTION: This commit (71c673e427, committed on 2021-04-22) updates the docstrings of various modules to include descriptions for parameters that were previously missing. Refers to issue #15391.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_35\n\nLANGUAGE: rst\nCODE:\n```\nUpdate Docstrings of Modules with Missing Params (#15391)\n```\n\n----------------------------------------\n\nTITLE: Listing Miscellaneous Changes in RST\nDESCRIPTION: Lists miscellaneous changes including moving TriggerDagRunOperator and filesystem sensor to standard provider, and renaming execution_date to logical_date.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n\"* ``Move 'TriggerDagRunOperator' to standard provider (#44053)``\\n* ``Move filesystem sensor to standard provider (#43890)``\\n* ``Rename execution_date to logical_date across codebase (#43902)``\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit Hash and Description in Markdown\nDESCRIPTION: This code snippet shows the format used to display commit hashes and descriptions in the change log. It uses backticks to format the commit hash as inline code and links it to the full commit on GitHub.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_11\n\nLANGUAGE: markdown\nCODE:\n```\n`1b18a501fe <https://github.com/apache/airflow/commit/1b18a501fe818079e535838fa4f232b03365fc75>`__  2023-02-03   ``Enable individual trigger logging (#27758)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add OpenSearch Provider\nDESCRIPTION: This commit message, associated with commit 94f144196f on 2023-10-15, signifies the initial addition of the OpenSearch provider to the project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\nAdd Open Search Provider (#34705)\n```\n\n----------------------------------------\n\nTITLE: Installing Opsgenie Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Opsgenie provider package for Apache Airflow using pip. This package can be installed on top of an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opsgenie/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-opsgenie\n```\n\n----------------------------------------\n\nTITLE: Building Documentation for Single Distribution\nDESCRIPTION: Commands to build documentation for specific Airflow distributions using uv run with docs group in local environment.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/11_documentation_building.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd providers/fab\nuv run --group docs build-docs\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd airflow-core\nuv run --group docs build-docs\n```\n\n----------------------------------------\n\nTITLE: Configuring Salesforce Connections in reStructuredText\nDESCRIPTION: This snippet demonstrates how to define and document a Salesforce connection within Apache Airflow through reStructuredText markup. It details the connection URI format required by Airflow (using 'salesforce' as the scheme), required fields such as username, password, and security_token, and describes parameter usage. No external code dependencies exist beyond Apache Airflow. Inputs include connection details, and outputs are correctly configured Airflow connections documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/redirects.txt#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\nSalesforce Connection\n=====================\n\nTo connect to Salesforce, set up an Airflow connection with the following parameters:\n\n.. code-block:: none\n\n    Conn Id: salesforce_default\n    Conn Type: Salesforce\n    Host: https://login.salesforce.com\n    Login: <your_username>\n    Password: <your_password>\n    Extra: {\"security_token\": \"<your_token>\"}\n\nThe ``extra`` field is used to pass in the required Salesforce security token.\n```\n\n----------------------------------------\n\nTITLE: RST Breaking Changes Notice\nDESCRIPTION: ReStructuredText formatted note about version compatibility requirements for Airflow 2.2+\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pig/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. note::\n  This release of provider is only available for Airflow 2.2+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n```\n\n----------------------------------------\n\nTITLE: Checking out Main Branch in Shell\nDESCRIPTION: Switches the current Git working directory to the 'main' branch from the 'apache' remote. The Helm chart is typically released from this branch.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout apache/main\n```\n\n----------------------------------------\n\nTITLE: Listing Directory Permissions with umask 0022\nDESCRIPTION: Example showing file permissions when checking out the repository with umask 0022, where group write permissions are not set.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0008-fixing-group-permissions-before-build.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nls -la scripts/ci\ntotal 132\ndrwxr-xr-x 19 jarek jarek  4096 Feb  5 20:49 .\ndrwxr-xr-x  8 jarek jarek  4096 Feb  5 20:49 ..\ndrwxr-xr-x  2 jarek jarek  4096 Feb  5 20:49 build_airflow\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs 3rd wave Providers (Oct 2023) - FIX\nDESCRIPTION: This commit message (hash d1c58d86de, dated 2023-10-28) represents a fix applied during the documentation preparation for the third wave of provider releases in October 2023 (issue #35233).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_32\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs 3rd wave of Providers October 2023 - FIX (#35233)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs 1st wave Providers (Dec 2023 RC2)\nDESCRIPTION: This commit message (hash 64931b1a65, dated 2023-12-12) marks the preparation of documentation for the second release candidate (RC2) of the first wave of provider updates in December 2023 (issue #36190).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_19\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs 1st wave of Providers December 2023 RC2 (#36190)``\n```\n\n----------------------------------------\n\nTITLE: Installing the Microsoft WinRM Provider Package via pip\nDESCRIPTION: This shell command uses pip, the Python package installer, to install the `apache-airflow-providers-microsoft-winrm` package. This installation is necessary to enable Microsoft WinRM functionalities within an Apache Airflow environment. It requires pip and a pre-existing Airflow 2 installation (version >= 2.9.0).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/winrm/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-microsoft-winrm\n```\n\n----------------------------------------\n\nTITLE: HTML Apache License Header Comment\nDESCRIPTION: Standard HTML comment block containing the Apache 2.0 license header that specifies copyright, ownership, and usage terms for Apache Airflow project files\nSOURCE: https://github.com/apache/airflow/blob/main/INTHEWILD.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Setting up Table of Contents for Databricks Plugins in reStructuredText\nDESCRIPTION: This snippet creates a table of contents directive in reStructuredText format that will include all files in the current directory. The maxdepth parameter limits the TOC to one level, and the glob parameter allows matching all files with a wildcard.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/plugins/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Guide in RST Documentation\nDESCRIPTION: RST include directive that references external documentation for installing Airflow providers from source code. The included file is located in the devel-common repository's sphinx extensions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding Kerberos Integration for Trino in Docker Compose (Python)\nDESCRIPTION: This Python snippet demonstrates how Airflow's ShellParams class dynamically appends the Kerberos integration YAML to the compose file list if Trino integration is present but Kerberos is not. It checks the integrations set and prints a warning via get_console if Kerberos is automatically enabled. Requires Airflow's internal compose configuration modules, and presumes the relevant paths and integration variables are set. Inputs: integrations list/set; output: compose_file_list is populated with kerberos YML file as needed; constraints: only applies to Trino integration tests.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/integration_tests.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nif \"trino\" in integrations and \"kerberos\" not in integrations:\n    get_console().print(\n        \"[warning]Adding `kerberos` integration as it is implicitly needed by trino\",\n    )\n    compose_file_list.append(DOCKER_COMPOSE_DIR / \"integration-kerberos.yml\")\n\n```\n\n----------------------------------------\n\nTITLE: Remove Whitespace in Provider Readme Template (Excluded from v1.4.1 Changelog)\nDESCRIPTION: Details the removal of extra whitespace from the provider readme template, referenced by pull request #46975. This change was intentionally excluded from the main changelog notes for version 1.4.1.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nRemove extra whitespace in provider readme template (#46975)\n```\n\n----------------------------------------\n\nTITLE: Running Specific System Tests in Bash\nDESCRIPTION: Example command to run a specific system test using Breeze. This demonstrates how to execute individual system tests, such as the example_external_task_child_deferrable test.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing system-tests airflow-core/tests/system/example_empty.py\n```\n\n----------------------------------------\n\nTITLE: Executing AIP-47 Issue Status Update Script in Bash\nDESCRIPTION: This command runs the Python script that updates the status of AIP-47 issues. It requires a dev environment and a GitHub token with write access to the repository. The script automatically updates issue statuses based on file presence in the repo.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/system_tests/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython dev/system_tests/update_issue_status.py\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in reStructuredText\nDESCRIPTION: This snippet includes an external file containing security-related documentation for the Apache Airflow project. It uses the reStructuredText include directive to embed content from a separate file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Prepare Documentation for Jan 2024 1st Wave (Excluded from v1.2.0 Changelog)\nDESCRIPTION: Indicates preparatory work on documentation for the first wave of provider releases in January 2024, referenced by pull request #36640. This change was intentionally excluded from the main changelog notes for version 1.2.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_26\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 1st wave of Providers January 2024 (#36640)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Field Metadata with GoogleSearchAdsGetFieldOperator\nDESCRIPTION: This snippet demonstrates how to use the GoogleSearchAdsGetFieldOperator to retrieve metadata for a specific field. This operator supports Jinja templating for dynamic parameter determination.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/search_ads.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[START howto_search_ads_get_field]\n[END howto_search_ads_get_field]\n```\n\n----------------------------------------\n\nTITLE: Refreshing Airflow Image Cache\nDESCRIPTION: Commands to refresh Docker image cache for Apache Airflow, including options for platform-specific builds (AMD64 or ARM64).\nSOURCE: https://github.com/apache/airflow/blob/main/dev/MANUALLY_GENERATING_IMAGE_CACHE_AND_CONSTRAINTS.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./dev/refresh_images.sh\n\nexport PLATFORM=linux/amd64\n./dev/refresh_images.sh\n\nexport PLATFORM=linux/arm64\n./dev/refresh_images.sh\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Standardize build process and switch to Hatchling\nDESCRIPTION: This commit message (hash c439ab87c4, dated 2024-01-10) details the standardization of the Airflow build process and the switch to using Hatchling as the build backend (issue #36537).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n``Standardize airflow build process and switch to Hatchling build backend (#36537)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Provider Docs for July 2023 Wave\nDESCRIPTION: This commit message, linked to commit cb4927a018 dated 2023-07-05, describes the preparation of documentation for the July 2023 release wave of Apache Airflow Providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs for July 2023 wave of Providers (#32298)``\n```\n\n----------------------------------------\n\nTITLE: Preparing Provider Release After PIP 21 Compatibility Fixes in RST\nDESCRIPTION: This commit (807ad32ce5, committed on 2021-05-01) prepares for a provider release following fixes made to ensure compatibility with PIP version 21. Refers to issue #15576.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_34\n\nLANGUAGE: rst\nCODE:\n```\nPrepares provider release after PIP 21 compatibility (#15576)\n```\n\n----------------------------------------\n\nTITLE: Fixing Changelog for Delayed January 2022 Provider Release\nDESCRIPTION: Excluded Change (Version 2.2.1): Corrects the changelog entries for the delayed January 2022 provider release, referencing pull request #21439.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_23\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Fixed changelog for January 2022 (delayed) provider's release (#21439)``\n```\n\n----------------------------------------\n\nTITLE: Specifying Provider Test Types (String)\nDESCRIPTION: A string defining which test types should be run for unit tests for providers. This example specifies running tests for all providers except 'google'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_15\n\nLANGUAGE: text\nCODE:\n```\nProviders Providers\\[-google\\]\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Follow BaseHook connection fields method signature\nDESCRIPTION: This text is a commit message summary describing a code change ensuring that child classes adhere to the method signature of 'connection_fields' defined in BaseHook, linked to pull request #36086.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_30\n\nLANGUAGE: plaintext\nCODE:\n```\nFollow BaseHook connection fields method signature in child classes (#36086)\n```\n\n----------------------------------------\n\nTITLE: Including jQuery UI Widgets and Effects - JavaScript\nDESCRIPTION: This snippet includes the minified bundle (with file headers) of jQuery UI v1.14.1, providing a comprehensive suite of interface components (widgets, effects, and utilities) for building robust, interactive UIs in web browsers. No explicit dependencies are visible in the snippet, but jQuery (typically v1.7+) is a required prerequisite. The file organizes and exposes functionality via module inclusions, as listed in the header comments. Inputs and outputs are defined at the API level within each contained script; usage involves importing this single file for access to all listed features.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/src/airflow/providers/fab/www/static/dist/jquery-ui.min.js.LICENSE.txt#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*! jQuery UI - v1.14.1 - 2024-10-30\\n* https://jqueryui.com\\n* Includes: widget.js, position.js, data.js, disable-selection.js, effect.js, effects/effect-blind.js, effects/effect-bounce.js, effects/effect-clip.js, effects/effect-drop.js, effects/effect-explode.js, effects/effect-fade.js, effects/effect-fold.js, effects/effect-highlight.js, effects/effect-puff.js, effects/effect-pulsate.js, effects/effect-scale.js, effects/effect-shake.js, effects/effect-size.js, effects/effect-slide.js, effects/effect-transfer.js, focusable.js, form-reset-mixin.js, jquery-patch.js, keycode.js, labels.js, scroll-parent.js, tabbable.js, unique-id.js, widgets/accordion.js, widgets/autocomplete.js, widgets/button.js, widgets/checkboxradio.js, widgets/controlgroup.js, widgets/datepicker.js, widgets/dialog.js, widgets/draggable.js, widgets/droppable.js, widgets/menu.js, widgets/mouse.js, widgets/progressbar.js, widgets/resizable.js, widgets/selectable.js, widgets/selectmenu.js, widgets/slider.js, widgets/sortable.js, widgets/spinner.js, widgets/tabs.js, widgets/tooltip.js\\n* Copyright OpenJS Foundation and other contributors; Licensed MIT */\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation with reStructuredText\nDESCRIPTION: This snippet uses the reStructuredText '.. include::' directive to incorporate shared security documentation from a common location. Dependencies include the referenced file path ('/../../../devel-common/src/sphinx_exts/includes/security.rst') and a documentation tool that supports includes (such as Sphinx). No parameters are required; the output will inline the content of the specified file at render time, promoting DRY principles in documentation. Usage assumes relative paths are correctly resolved and that permissions allow access to the included file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Installing Opsgenie Provider with Cross-Provider Dependencies\nDESCRIPTION: Command to install the Opsgenie provider package along with its cross-provider dependencies, specifically the common.compat extra.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opsgenie/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-opsgenie[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit History for Edge3 Provider in reStructuredText\nDESCRIPTION: This snippet shows the commit history for the Edge3 provider package, including version numbers, commit hashes, dates, and descriptions of changes. It is formatted in reStructuredText for documentation purposes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/edge3/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n1.0.0\n.....\n\nLatest change: 2025-04-19\n\n==================================================================================================  ===========  ==========================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==========================================================================\n`223a741869 <https://github.com/apache/airflow/commit/223a741869505ad31c38310f307bf2f0f0f193fb>`__  2025-04-19   ``capitalize the term airflow (#49450)``\n`2ae12c815b <https://github.com/apache/airflow/commit/2ae12c815bc704eff6890df56f7387da513d14f2>`__  2025-04-16   ``Update documentation for edge3 and git provider (#49365)``\n`99666f9789 <https://github.com/apache/airflow/commit/99666f97893dff28297555778620a9c114779667>`__  2025-04-16   ``Rename 'edge' provider fom 'edgeexecutor' to 'edge3' provider (#49358)``\n==================================================================================================  ===========  ==========================================================================\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add docs for RC2 wave of providers (Jan 2024)\nDESCRIPTION: This commit message (hash cead3da4a6, dated 2024-01-26) signifies the addition of documentation for the second release candidate wave of providers in the second round of January 2024 updates (associated with issue #37019).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n``Add docs for RC2 wave of providers for 2nd round of Jan 2024 (#37019)``\n```\n\n----------------------------------------\n\nTITLE: DocToc Generated Table of Contents in Markdown\nDESCRIPTION: Auto-generated table of contents using DocToc tool, showing the structure of the ADR document.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0009-exclude-all-files-from-dockerignore-by-default.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*\n\n- [9. Exclude all files from dockerignore by default](#9-exclude-all-files-from-dockerignore-by-default)\n  - [Status](#status)\n  - [Context](#context)\n  - [Decision](#decision)\n  - [Consequences](#consequences)\n```\n\n----------------------------------------\n\nTITLE: Exit Trap for Ownership Fix in Linux Containers\nDESCRIPTION: Command that sets up a trap to fix file ownership when exiting the container. The trap ensures files created as root are properly owned by the host user upon container exit.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0006-using-root-user-and-fixing-ownership-for-ci-container.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nadd_trap in_container_fix_ownership EXIT HUP INT TERM\n```\n\n----------------------------------------\n\nTITLE: Switching Airflow Codebase to f-strings using flynt\nDESCRIPTION: This commit message describes a codebase refactoring effort, switching string formatting to use f-strings with the help of the 'flynt' tool. It references pull request #13732.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_44\n\nLANGUAGE: text\nCODE:\n```\nSwitch to f-strings using flynt. (#13732)\n```\n\n----------------------------------------\n\nTITLE: Listing Commits for Airflow Provider Version 4.0.0\nDESCRIPTION: This section documents commits for the Apache Airflow provider version 4.0.0 release. It covers updates to release notes and documentation for the May 2022 providers release, fixes to package descriptions, and additions to contributor guidelines regarding changelogs. It includes commit hashes linked to GitHub, commit dates, and subject lines.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`dcdcf3a2b8 <https://github.com/apache/airflow/commit/dcdcf3a2b8054fa727efb4cd79d38d2c9c7e1bd5>`__  2022-06-09   ``Update release notes for RC2 release of Providers for May 2022 (#24307)``\n`717a7588bc <https://github.com/apache/airflow/commit/717a7588bc8170363fea5cb75f17efcf68689619>`__  2022-06-07   ``Update package description to remove double min-airflow specification (#24292)``\n`aeabe994b3 <https://github.com/apache/airflow/commit/aeabe994b3381d082f75678a159ddbb3cbf6f4d3>`__  2022-06-07   ``Prepare docs for May 2022 provider's release (#24231)``\n`027b707d21 <https://github.com/apache/airflow/commit/027b707d215a9ff1151717439790effd44bab508>`__  2022-06-05   ``Add explanatory note for contributors about updating Changelog (#24229)``\n==================================================================================================  ===========  ==================================================================================\n```\n\n----------------------------------------\n\nTITLE: Running MyPy Checks on All Files in Bash\nDESCRIPTION: This command runs MyPy checks on all files in the repository, not just staged ones.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run mypy-airflow --all-files\n```\n\n----------------------------------------\n\nTITLE: Version Header in RST\nDESCRIPTION: Version header and section formatting using RST syntax\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/flink/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n1.6.2\n.....\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Generation Notice\nDESCRIPTION: Warning notice in RST format indicating that the file is automatically generated from a template and should not be modified directly\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/impala/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!\n\n.. IF YOU WANT TO MODIFY THIS FILE, YOU SHOULD MODIFY THE TEMPLATE\n   `PROVIDER_COMMITS_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY\n\n.. THE REMAINDER OF THE FILE IS AUTOMATICALLY GENERATED. IT WILL BE OVERWRITTEN!\n```\n\n----------------------------------------\n\nTITLE: Yielding event in loop\nDESCRIPTION: Adds a return statement when an event is yielded in a loop to stop execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n\"add a return when the event is yielded in a loop to stop the execution (#31985)\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit History in Markdown Format\nDESCRIPTION: A formatted table showing commit history with commit hashes, dates and subjects. The entries are separated by version numbers and include links to the actual commits.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_34\n\nLANGUAGE: markdown\nCODE:\n```\n`88bdcfa0df <https://github.com/apache/airflow/commit/88bdcfa0df5bcb4c489486e05826544b428c8f43>`__  2021-02-04   ``Prepare to release a new wave of providers. (#14013)``\n`ac2f72c98d <https://github.com/apache/airflow/commit/ac2f72c98dc0821b33721054588adbf2bb53bb0b>`__  2021-02-01   ``Implement provider versioning tools (#13767)``\n`295d66f914 <https://github.com/apache/airflow/commit/295d66f91446a69610576d040ba687b38f1c5d0a>`__  2020-12-30   ``Fix Grammar in PIP warning (#13380)``\n```\n\n----------------------------------------\n\nTITLE: Adding Optional Result Handler to Database Hooks in RST\nDESCRIPTION: This commit (abcd487313, committed on 2021-05-17) adds an optional result handler function parameter to database hooks, allowing custom processing of query results. Refers to issue #15581.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_30\n\nLANGUAGE: rst\nCODE:\n```\nAdd optional result handler to database hooks (#15581)\n```\n\n----------------------------------------\n\nTITLE: Codespace Badge Definition in ReStructuredText\nDESCRIPTION: Defines a reference to the GitHub Codespaces badge image with a link to create a new codespace.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_codespaces.rst#2025-04-22_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. |codespace| image:: https://github.com/codespaces/badge.svg\n       :target: https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=33884891\n       :alt: Open in GitHub Codespaces\n```\n\n----------------------------------------\n\nTITLE: Displaying Help for Breeze Kubernetes Tests - Bash\nDESCRIPTION: This bash command displays the help information for the `breeze k8s tests` command by passing `--help` through the shell. The use of `--` ensures that the parameter is directed to the underlying shell rather than Breeze's own parser. Useful for discovering all available shell flags/options.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s tests -- --help\n```\n\n----------------------------------------\n\nTITLE: Removing Extra Whitespace in Provider Readme Template\nDESCRIPTION: Commit message indicating the removal of unnecessary whitespace from the provider README template file. References pull request #46975.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_22\n\nLANGUAGE: plaintext\nCODE:\n```\nRemove extra whitespace in provider readme template (#46975)\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit Information in Markdown\nDESCRIPTION: This snippet shows how commit information is formatted in the changelog using Markdown syntax. It includes the commit hash (as a link), commit date, and commit message.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n`2e20e9f7eb <https://github.com/apache/airflow/commit/2e20e9f7ebf5f43bf27069f4c0063cdd72e6b2e2>`__  2022-11-24   ``Prepare for follow-up relase for November providers (#27774)``\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 2.0.1\nDESCRIPTION: ReStructuredText formatted changelog entry documenting bug fixes for version 2.0.1 of the Asana provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n2.0.1\n.....\n\nBug Fixes\n~~~~~~~~~\n\n* ``Update providers to use functools compat for ''cached_property'' (#24582)``\n```\n\n----------------------------------------\n\nTITLE: Version History Changelog\nDESCRIPTION: Markdown formatted changelog listing version history, commit hashes, dates and descriptions for Apache Airflow provider releases.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n`b916b75079 <https://github.com/apache/airflow/commit/b916b7507921129dc48d6add1bdc4b923b60c9b9>`__  2021-07-15   ``Prepare documentation for July release of providers. (#17015)``\n`866a601b76 <https://github.com/apache/airflow/commit/866a601b76e219b3c043e1dbbc8fb22300866351>`__  2021-06-28   ``Removes pylint from our toolchain (#16682)``\n```\n\n----------------------------------------\n\nTITLE: Referencing GitHub Commit in Markdown\nDESCRIPTION: This snippet shows how to reference a GitHub commit hash and link in a Markdown changelog entry. It includes the commit hash, link to the commit on GitHub, date, and description of the change.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/commits.rst#2025-04-22_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n`c7399c7190 <https://github.com/apache/airflow/commit/c7399c7190750ba705b8255b7a92de2554e6eef3>`__  2022-04-21   ``KubernetesHook should try incluster first when not otherwise configured (#23126)``\n```\n\n----------------------------------------\n\nTITLE: Airflow Executors Documentation Directive\nDESCRIPTION: RestructuredText directive for automatically generating documentation about available Airflow executors from community providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/core-extensions/executors.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. airflow-executors::\n   :tags: None\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Defining Telegram Provider Package Name in reStructuredText\nDESCRIPTION: This code snippet defines the package name for the Telegram provider in reStructuredText format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n``apache-airflow-providers-telegram``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add official download page for providers\nDESCRIPTION: This commit message, for commit 1cb456cba1, signifies the addition of an official download page for Airflow providers to the documentation, referenced by issue #18187.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n``Add official download page for providers (#18187)``\n```\n\n----------------------------------------\n\nTITLE: Commit Reference Format\nDESCRIPTION: Reference format for Git commit hashes with links to GitHub and associated commit messages.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/commits.rst#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n`e5ac6c7cfb <https://github.com/apache/airflow/commit/e5ac6c7cfb189c33e3b247f7d5aec59fe5e89a00>`__  2022-08-10   ``Prepare docs for new providers release (August 2022) (#25618)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Docs for March 2023 Providers Wave\nDESCRIPTION: This commit message, associated with commit fcd3c0149f dated 2023-03-03, indicates the preparation of documentation for the March 2023 release wave of Apache Airflow Providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_17\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs for 03/2023 wave of Providers (#29878)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: More documentation update for June providers release\nDESCRIPTION: Associated with commit 1fba5402bb, this message indicates further updates were made to the documentation in preparation for the June 2021 providers release, tracked under issue #16405.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_19\n\nLANGUAGE: text\nCODE:\n```\n``More documentation update for June providers release (#16405)``\n```\n\n----------------------------------------\n\nTITLE: Structuring and Documenting Provider Commits in reStructuredText - reStructuredText\nDESCRIPTION: This snippet demonstrates the use of reStructuredText markup to organize and present detailed commit history for a Python provider package in Apache Airflow. It uses tables, headings, and hyperlinks to format versioned commit logs, ensuring clarity. Prerequisites: reStructuredText-compatible documentation system (such as Sphinx). Key features include commit tables with SHA links, version headings, and licensing info. Inputs are auto-generated from the project commit history; output is intended for end-user documentation display. Do not manually edit as changes will be overwritten; instead, modify the Jinja2 template as indicated.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cohere/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n .. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!\n\n .. IF YOU WANT TO MODIFY THIS FILE, YOU SHOULD MODIFY THE TEMPLATE\n    `PROVIDER_COMMITS_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY\n\n .. THE REMAINDER OF THE FILE IS AUTOMATICALLY GENERATED. IT WILL BE OVERWRITTEN!\n\nPackage apache-airflow-providers-cohere\n------------------------------------------------------\n\n`Cohere <https://docs.cohere.com/docs>`__\n\n\nThis is detailed commit list of changes for versions provider package: ``cohere``.\nFor high-level changelog, see :doc:`package information including changelog <index>`.\n\n\n\n1.4.3\n.....\n\nLatest change: 2025-03-05\n\n==================================================================================================  ===========  =====================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =====================================================================\n`e4002c3305 <https://github.com/apache/airflow/commit/e4002c3305a757f5926f96c996e701e8f998a042>`__  2025-03-05   ``Move tests_common package to devel-common project (#47281)``\n`1addb55154 <https://github.com/apache/airflow/commit/1addb55154fbef31bfa021537cfbd4395696381c>`__  2025-02-28   ``Improve documentation for updating provider dependencies (#47203)``\n`c6c4f95ed9 <https://github.com/apache/airflow/commit/c6c4f95ed9e3220133815b9126c135e805637022>`__  2025-02-25   ``Add legacy namespace packages to airflow.providers (#47064)``\n`dbf8bb4092 <https://github.com/apache/airflow/commit/dbf8bb409223687c7d2ad10649a92d02c24bb3b4>`__  2025-02-24   ``Remove extra whitespace in provider readme template (#46975)``\n`b28c336e8b <https://github.com/apache/airflow/commit/b28c336e8b7aa1d69c0f9520b182b1b661377337>`__  2025-02-21   ``Upgrade flit to 3.11.0 (#46938)``\n==================================================================================================  ===========  =====================================================================\n\n1.4.2\n.....\n\nLatest change: 2025-02-21\n\n==================================================================================================  ===========  =========================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =========================================================================\n`0653ffe78e <https://github.com/apache/airflow/commit/0653ffe78e4a0acaf70801a5ceef8dbabdac8b15>`__  2025-02-21   ``Prepare docs for Feb 1st wave of providers (fixed) (#46962)``\n`5d87bddf0a <https://github.com/apache/airflow/commit/5d87bddf0aa5f485f3684c909fb95f461e5a2ab6>`__  2025-02-21   ``Prepare docs for Feb 1st wave of providers (#46893)``\n`4d5846f58f <https://github.com/apache/airflow/commit/4d5846f58fe0de9b43358c0be75dd72e968dacc4>`__  2025-02-16   ``Move provider_tests to unit folder in provider tests (#46800)``\n`e027457a24 <https://github.com/apache/airflow/commit/e027457a24d0c6235bfed9c2a8399f75342e82f1>`__  2025-02-15   ``Removed the unused provider's distribution (#46608)``\n`3b533621db <https://github.com/apache/airflow/commit/3b533621dbb46e9bdf0a6e20b3536394902a5893>`__  2025-01-26   ``move cohere provider (#46050)``\n`f616c62209 <https://github.com/apache/airflow/commit/f616c62209d6b51d293ecf6f5c900f89a7fdc3a3>`__  2025-01-15   ``AIP-72: Support better type-hinting for Context dict in SDK  (#45583)``\n`2a78648848 <https://github.com/apache/airflow/commit/2a78648848d6f8edeed0e5df8c7120902965d1ce>`__  2024-12-29   ``Update Cohere to 5.13.4 v2 API (#45267)``\n==================================================================================================  ===========  =========================================================================\n\n1.4.0\n.....\n\nLatest change: 2024-12-20\n\n==================================================================================================  ===========  ========================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ========================================================================================\n`2723508345 <https://github.com/apache/airflow/commit/2723508345d5cf074aeb673955ce72996785f2bc>`__  2024-12-20   ``Prepare docs for Nov 1st wave of providers Dec 2024 (#45042)``\n`4b38bed76c <https://github.com/apache/airflow/commit/4b38bed76c1ea5fe84a6bc678ce87e20d563adc0>`__  2024-12-16   ``Bump min version of Providers to 2.9 (#44956)``\n`1275fec92f <https://github.com/apache/airflow/commit/1275fec92fd7cd7135b100d66d41bdcb79ade29d>`__  2024-11-24   ``Use Python 3.9 as target version for Ruff & Black rules (#44298)``\n`a53d9f6d25 <https://github.com/apache/airflow/commit/a53d9f6d257f193ea5026ba4cd007d5ddeab968f>`__  2024-11-14   ``Prepare docs for Nov 1st wave of providers (#44011)``\n`857ca4c06c <https://github.com/apache/airflow/commit/857ca4c06c9008593674cabdd28d3c30e3e7f97b>`__  2024-10-09   ``Split providers out of the main \"airflow/\" tree into a UV workspace project (#42505)``\n==================================================================================================  ===========  ========================================================================================\n\n1.3.0\n.....\n\nLatest change: 2024-08-19\n\n==================================================================================================  ===========  =======================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =======================================================================\n`75fb7acbac <https://github.com/apache/airflow/commit/75fb7acbaca09a040067f0a5a37637ff44eb9e14>`__  2024-08-19   ``Prepare docs for Aug 2nd wave of providers (#41559)``\n`fcbff15bda <https://github.com/apache/airflow/commit/fcbff15bda151f70db0ca13fdde015bace5527c4>`__  2024-08-12   ``Bump minimum Airflow version in providers to Airflow 2.8.0 (#41396)``\n`d23881c648 <https://github.com/apache/airflow/commit/d23881c6489916113921dcedf85077441b44aaf3>`__  2024-08-03   ``Prepare docs for Aug 1st wave of providers (#41230)``\n`09a7bd1d58 <https://github.com/apache/airflow/commit/09a7bd1d585d2d306dd30435689f22b614fe0abf>`__  2024-07-09   ``Prepare docs 1st wave July 2024 (#40644)``\n`a62bd83188 <https://github.com/apache/airflow/commit/a62bd831885957c55b073bf309bc59a1d505e8fb>`__  2024-06-27   ``Enable enforcing pydocstyle rule D213 in ruff. (#40448)``\n==================================================================================================  ===========  =======================================================================\n\n1.2.1\n.....\n\nLatest change: 2024-05-26\n\n==================================================================================================  ===========  ================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ================================================\n`34500f3a2f <https://github.com/apache/airflow/commit/34500f3a2fa4652272bc831e3c18fd2a6a2da5ef>`__  2024-05-26   ``Prepare docs 3rd wave May 2024 (#39738)``\n`2b1a2f8d56 <https://github.com/apache/airflow/commit/2b1a2f8d561e569df194c4ee0d3a18930738886e>`__  2024-05-11   ``Reapply templates for all providers (#39554)``\n`2c05187b07 <https://github.com/apache/airflow/commit/2c05187b07baf7c41a32b18fabdbb3833acc08eb>`__  2024-05-10   ``Faster 'airflow_version' imports (#39552)``\n`73918925ed <https://github.com/apache/airflow/commit/73918925edaf1c94790a6ad8bec01dec60accfa1>`__  2024-05-08   ``Simplify 'airflow_version' imports (#39497)``\n==================================================================================================  ===========  ================================================\n\n1.2.0\n.....\n\nLatest change: 2024-05-01\n\n==================================================================================================  ===========  =======================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =======================================================================\n\n```\n\n----------------------------------------\n\nTITLE: Enhancing AWS Glue Job Operator with Wait Functionality in Python\nDESCRIPTION: Addition of a wait_for_completion parameter to the AWS Glue Job Operator, allowing for synchronous job execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nAwsGlueJobOperator: add wait_for_completion to Glue job run (#18814)\n```\n\n----------------------------------------\n\nTITLE: AWS S3 Storage Options Configuration\nDESCRIPTION: Code allowing storage options to be passed to AWS S3 operations\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nstorage_options = {\n    \"key\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Column and Table Quality Checks with Airflow BigQueryColumnCheckOperator and BigQueryTableCheckOperator - Python\nDESCRIPTION: This example shows how to use BigQueryColumnCheckOperator and BigQueryTableCheckOperator to perform data quality checks at the column and table level. These operators allow user-defined tests or constraints to be validated on specific columns or across the table. Dependencies include airflow.providers.google.cloud.operators.bigquery. Parameters typically are table name, column definitions with associated tests, and check expressions for table-level tests. Successful execution confirms that data quality criteria are met. Limitations depend on test complexity and BigQuery SQL expressiveness.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/bigquery.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nbq_column_check = BigQueryColumnCheckOperator(\n    task_id=\"bq_column_check\",\n    table=\"my_table\",\n    column_mapping={\n        \"age\": {\"nullable\": False, \"min\": 0, \"max\": 120},\n        \"salary\": {\"nullable\": True, \"min\": 30000},\n    },\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\nbq_table_check = BigQueryTableCheckOperator(\n    task_id=\"bq_table_check\",\n    table=\"my_table\",\n    checks={\n        \"check_non_empty\": {\n            \"check_sql\": \"SELECT COUNT(*) > 0 FROM my_table\",\n        },\n        \"check_avg_salary\": {\n            \"check_sql\": \"SELECT AVG(salary) > 35000 FROM my_table\",\n        },\n    },\n)\n\n```\n\n----------------------------------------\n\nTITLE: Including RST File for Installing Airflow Providers from Sources in Sphinx\nDESCRIPTION: This RST directive includes an external file containing instructions for installing Apache Airflow providers from source code. The file is located in a development-common directory structure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/github/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Including External Documentation in reStructuredText\nDESCRIPTION: This snippet uses the reStructuredText 'include' directive to insert content from an external file into the current document. The included file contains instructions for installing Apache Airflow providers from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Including External Documentation in reStructuredText\nDESCRIPTION: This snippet uses the reStructuredText `.. include::` directive to insert the content of the specified file ('/../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst') into the current document. This is commonly used in Sphinx documentation to modularize content.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/messaging/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Referencing Changelog Link in reStructuredText\nDESCRIPTION: This snippet creates a hyperlink in reStructuredText format, pointing to the changelog for version 15.1.0 of the Google Cloud Provider for Apache Airflow. It uses the inline link syntax with the visible text 'changelog' and the full URL to the documentation page.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/README.rst#2025-04-22_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n`changelog <https://airflow.apache.org/docs/apache-airflow-providers-google/15.1.0/changelog.html>`_\n```\n\n----------------------------------------\n\nTITLE: Setting Version Information\nDESCRIPTION: Commands to set the version information in the Airflow repository and environment variables for the release process.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ${AIRFLOW_REPO_ROOT}\nexport VERSION=\"2.8.0\"\nexport VERSION_SUFFIX=\"rc1\"\necho \"${VERSION}\" > clients/python/version.txt\n```\n\n----------------------------------------\n\nTITLE: Including External RST Content\nDESCRIPTION: This directive in reStructuredText (RST) includes the content of the specified file (`/../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst`) at this location in the document. This is used to modularize documentation and reuse common sections, in this case, the instructions for installing Airflow providers from source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/singularity/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Preparing Python Client\nDESCRIPTION: Command to generate Python client source code and build distribution. Requires path to Python client repository.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-python-client --python-client-repo ~/code/airflow-client-python\n```\n\n----------------------------------------\n\nTITLE: Configuring TokenAwarePolicy for Cassandra Load Balancing in Airflow\nDESCRIPTION: Example JSON configuration for the Extra field to specify TokenAwarePolicy as the load balancing policy. This policy uses token awareness to route queries to replica nodes and requires a child policy to handle further routing decisions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/cassandra/docs/connections/cassandra.rst#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"load_balancing_policy\": \"TokenAwarePolicy\",\n  \"load_balancing_policy_args\": {\n    \"child_load_balancing_policy\": \"CHILD_POLICY_NAME\",\n    \"child_load_balancing_policy_args\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Exasol Airflow Provider with Optional Dependencies via pip (Bash)\nDESCRIPTION: Demonstrates installing the Exasol Airflow Provider package along with optional components for cross-provider dependencies by using pip. The extra 'common.sql' is specified within brackets to include features that may require additional packages. Users should have an existing compatible Airflow installation and supported Python version before running this command. The input (dependency group in brackets) determines which extras are installed, and output consists of installed packages in the environment. Requires internet access for package downloads.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/exasol/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-exasol[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL for OpenTelemetry Collector in YAML\nDESCRIPTION: YAML configuration for enabling HTTPS connection to the OpenTelemetry collector by configuring SSL certificate and key in the collector's config.yml file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/metrics.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      http:\n        endpoint: 0.0.0.0:4318\n        tls:\n          cert_file: \"/path/to/cert/cert.crt\"\n          key_file: \"/path/to/key/key.pem\"\n```\n\n----------------------------------------\n\nTITLE: Storing Connections in .env Format\nDESCRIPTION: This snippet demonstrates how to store connections in a .env file. Each line contains a connection ID and its corresponding URI definition.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/secrets-backend/local-filesystem-secrets-backend.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nmysql_conn_id=mysql://log:password@13.1.21.1:3306/mysqldbrd\ngoogle_custom_key=google-cloud-platform://?key_path=%2Fkeys%2Fkey.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for Atlassian Jira Notifications in reStructuredText\nDESCRIPTION: This snippet sets up a table of contents (toctree) for Atlassian Jira Notifications documentation. It configures the toctree to have a maximum depth of 1 and includes all files in the current directory using a glob pattern.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/atlassian/jira/docs/notifications/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit Information in Markdown Table\nDESCRIPTION: This code snippet shows how commit information is formatted in a markdown table, including the commit hash, date, and subject.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`56ab82ed7a <https://github.com/apache/airflow/commit/56ab82ed7a5c179d024722ccc697b740b2b93b6a>`__  2022-04-07   ``Prepare mid-April provider documentation. (#22819)``\n`9c28e766b3 <https://github.com/apache/airflow/commit/9c28e766b3a7bf93b4c8ec5422a5a25f10117fcc>`__  2022-04-07   ``Make ElasticSearch Provider compatible for Airflow<2.3 (#22814)``\n`c063fc688c <https://github.com/apache/airflow/commit/c063fc688cf20c37ed830de5e3dac4a664fd8241>`__  2022-03-25   ``Update black precommit (#22521)``\n==================================================================================================\n```\n\n----------------------------------------\n\nTITLE: VS Code Debug Configuration for Airflow\nDESCRIPTION: JSON configuration for setting up VS Code debugging with Airflow, including environment variables and Python interpreter path.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_vscode.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"configurations\": [\n        \"program\": \"${workspaceFolder}/files/dags/example_bash_operator.py\",\n        \"env\": {\n            \"PYTHONUNBUFFERED\": \"1\",\n            \"AIRFLOW__CORE__EXECUTOR\": \"LocalExecutor\"\n         },\n         \"python\": \"${env:HOME}/.pyenv/versions/airflow/bin/python\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Breeze to PATH in Bash\nDESCRIPTION: Command to add Breeze to the system PATH for easy access from anywhere.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nexport PATH=${PATH}:\"/home/${USER}/Projects/airflow\"\nsource ~/.bashrc\n```\n\n----------------------------------------\n\nTITLE: Documentation Generation Notice\nDESCRIPTION: Notice indicating that the file is automatically generated from a template and should not be modified directly\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!\n\n.. IF YOU WANT TO MODIFY THIS FILE, YOU SHOULD MODIFY THE TEMPLATE\n   `PROVIDER_COMMITS_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY\n\n.. THE REMAINDER OF THE FILE IS AUTOMATICALLY GENERATED. IT WILL BE OVERWRITTEN!\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs 3rd wave Providers (Oct 2023)\nDESCRIPTION: This commit message (hash 3592ff4046, dated 2023-10-28) marks the preparation of documentation for the third wave of provider releases in October 2023 (issue #35187).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_33\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs 3rd wave of Providers October 2023 (#35187)``\n```\n\n----------------------------------------\n\nTITLE: Separating Documentation Building per Airflow Provider\nDESCRIPTION: This commit message describes changes made to separate the documentation building process, allowing documentation to be built individually for each provider package. It references pull request #12444.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_55\n\nLANGUAGE: text\nCODE:\n```\nSeparate out documentation building per provider  (#12444)\n```\n\n----------------------------------------\n\nTITLE: Installing SendGrid Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with SendGrid integration, enabling email sending using SendGrid.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[sendgrid]'\n```\n\n----------------------------------------\n\nTITLE: Installing Pagerduty Provider with Cross-Provider Dependencies\nDESCRIPTION: Command to install the Pagerduty provider package along with its cross-provider dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pagerduty/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-pagerduty[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment Block in HTML\nDESCRIPTION: Standard Apache 2.0 license header formatted as an HTML comment block, specifying the terms under which the software is distributed.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/AIRFLOW_ERROR_GUIDE.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Including Security RST Extension\nDESCRIPTION: Sphinx documentation include directive that imports a security-related RST file from the devel-common source directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Displaying Apache License Notice in Sphinx Documentation - reStructuredText\nDESCRIPTION: This snippet demonstrates the use of reStructuredText (reST) for presenting the mandatory Apache License notice commonly included in documentation files for open source projects. It warns users about licensing terms and constraints associated with utilizing the file and references where users can read the full license. No code execution is involved; this is strictly for Sphinx-generated documentation and requires Sphinx as a prerequisite. The input is plain text and reST directives, and the output is rendered documentation sections.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Unit Test for Custom Airflow Operator\nDESCRIPTION: A complete test setup for a custom operator that creates a test DAG and DAG run, executes a task instance, and verifies the task completes successfully. Includes fixtures for setting up the test environment with appropriate date intervals.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nimport pendulum\nimport pytest\n\nfrom airflow.sdk import DAG\nfrom airflow.utils.state import DagRunState, TaskInstanceState\nfrom airflow.utils.types import DagRunTriggeredByType, DagRunType\n\nDATA_INTERVAL_START = pendulum.datetime(2021, 9, 13, tz=\"UTC\")\nDATA_INTERVAL_END = DATA_INTERVAL_START + datetime.timedelta(days=1)\n\nTEST_DAG_ID = \"my_custom_operator_dag\"\nTEST_TASK_ID = \"my_custom_operator_task\"\nTEST_RUN_ID = \"my_custom_operator_dag_run\"\n\n\n@pytest.fixture()\ndef dag():\n    with DAG(\n        dag_id=TEST_DAG_ID,\n        schedule=\"@daily\",\n        start_date=DATA_INTERVAL_START,\n    ) as dag:\n        MyCustomOperator(\n            task_id=TEST_TASK_ID,\n            prefix=\"s3://bucket/some/prefix\",\n        )\n    return dag\n\n\ndef test_my_custom_operator_execute_no_trigger(dag):\n    dagrun = dag.create_dagrun(\n        run_id=TEST_RUN_ID,\n        logical_date=DATA_INTERVAL_START,\n        data_interval=(DATA_INTERVAL_START, DATA_INTERVAL_END),\n        run_type=DagRunType.MANUAL,\n        triggered_by=DagRunTriggeredByType.TIMETABLE,\n        state=DagRunState.RUNNING,\n        start_date=DATA_INTERVAL_END,\n    )\n    ti = dagrun.get_task_instance(task_id=TEST_TASK_ID)\n    ti.task = dag.get_task(task_id=TEST_TASK_ID)\n    ti.run(ignore_ti_state=True)\n    assert ti.state == TaskInstanceState.SUCCESS\n    # Assert something related to tasks results.\n```\n\n----------------------------------------\n\nTITLE: Markdown Error Code Table\nDESCRIPTION: Detailed table of Airflow error codes containing error codes, exception types, error messages, descriptions, troubleshooting steps, and documentation links\nSOURCE: https://github.com/apache/airflow/blob/main/dev/AIRFLOW_ERROR_GUIDE.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|Error Code|       Exception Type       |                         User-facing Error Message                         |                                                 Description                                                 |                                                                                      First Steps                                                                                      |                                                  Documentation                                                 |\n|----------|----------------------------|---------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n|  AERR001 |      AirflowException      |                    Dynamic task mapping exceeded limit                    |              Happens when dynamically mapped tasks exceed the maximum number of tasks allowed.              |                                 Check the task count limit in the configuration. Consider increasing the task limit or optimizing task mapping logic.                                 |            https://airflow.apache.org/docs/apache-airflow/stable/concepts/dynamic-task-mapping.html            |\n```\n\n----------------------------------------\n\nTITLE: Provider Package Name\nDESCRIPTION: Name of the Apache Airflow provider package for ArangoDB\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n``apache-airflow-providers-arangodb``\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Integrations in Breeze\nDESCRIPTION: Command to start Breeze with both MongoDB and Cassandra integrations enabled\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/integration_tests.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbreeze --integration mongo --integration cassandra\n```\n\n----------------------------------------\n\nTITLE: Updating Documentation for September Provider Release\nDESCRIPTION: Excluded Change (Version 3.1.0): Contains documentation updates related to the September 2022 provider release, referencing pull request #26731.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Update docs for September Provider's release (#26731)``\n```\n\n----------------------------------------\n\nTITLE: Version Note RST Block\nDESCRIPTION: RST formatted note block explaining version compatibility requirements for Airflow 2.3+\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ftp/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. note::\n  This release of provider is only available for Airflow 2.3+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n```\n\n----------------------------------------\n\nTITLE: Deleting Dataplex Entry Type using Airflow Python\nDESCRIPTION: This snippet demonstrates how to delete an existing Entry Type from a specific location in Google Cloud Dataplex Catalog using the `DataplexCatalogDeleteEntryTypeOperator` within an Airflow DAG. It references an external example file for the specific implementation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_delete_entry_type]\n#     :end-before: [END howto_operator_dataplex_catalog_delete_entry_type]\n\n# This example uses DataplexCatalogDeleteEntryTypeOperator.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Configuring Enchant Library Path\nDESCRIPTION: Example of setting the PYENCHANT_LIBRARY_PATH environment variable in shell configuration to enable proper enchant library detection for documentation spelling checks.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/11_documentation_building.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport PYENCHANT_LIBRARY_PATH=/opt/homebrew/Cellar/enchant/2.8.2/lib/libenchant-2.dylib\n```\n\n----------------------------------------\n\nTITLE: Referencing Apache Airflow Providers Support Policy in RST\nDESCRIPTION: RST syntax for linking to the Apache Airflow providers support policy document, which explains minimum supported Airflow versions for community-managed providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n`Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_\n```\n\n----------------------------------------\n\nTITLE: Including External Documentation File (RST)\nDESCRIPTION: This reStructuredText directive includes the content of the specified file ('/../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst') into the current document. This is used to incorporate shared documentation, specifically instructions on installing Airflow providers from their source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cohere/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Version Support Note\nDESCRIPTION: RST note block indicating version compatibility requirements\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. note::\n  This release of provider is only available for Airflow 2.9+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry v2.0.4\nDESCRIPTION: Changelog entry documenting bug fix for incorrect install_requires specification\nSOURCE: https://github.com/apache/airflow/blob/main/providers/datadog/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n2.0.4\n.....\n\nBug Fixes\n~~~~~~~~~\n\n* ``Fix mistakenly added install_requires for all providers (#22382)``\n```\n\n----------------------------------------\n\nTITLE: Correcting Capitalization in Documentation in RST\nDESCRIPTION: This commit (43de625d42, committed on 2021-12-01) fixes the capitalization of names and abbreviations within the project documentation. Refers to issue #19908.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_16\n\nLANGUAGE: rst\nCODE:\n```\nCorrectly capitalize names and abbreviations in docs (#19908)\n```\n\n----------------------------------------\n\nTITLE: Importing Apache Kafka Provider Package in Python\nDESCRIPTION: This code snippet shows how to import the Apache Kafka provider package in a Python script for use with Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.apache.kafka import *\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fix typos in DatabricksSubmitRunOperator\nDESCRIPTION: This commit message (hash 322aa649ed, dated 2023-12-21) corrects typographical errors within the DatabricksSubmitRunOperator code or documentation (issue #36248).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_17\n\nLANGUAGE: text\nCODE:\n```\n``fix typos in DatabricksSubmitRunOperator (#36248)``\n```\n\n----------------------------------------\n\nTITLE: Enabling Kubernetes Tests (Boolean String)\nDESCRIPTION: A boolean flag indicating whether tests related to Kubernetes should be run. 'true' enables these tests.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_20\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Displaying Git Commit Information in Markdown\nDESCRIPTION: This code snippet shows how to format Git commit information in a Markdown table, including commit hash, date, and subject.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ===================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ===================================================================\n`0c8e30e43b <https://github.com/apache/airflow/commit/0c8e30e43b70e9d033e1686b327eb00aab82479c>`__  2023-10-05   ``Bump min airflow version of providers (#34728)``\n`659d94f0ae <https://github.com/apache/airflow/commit/659d94f0ae89f47a7d4b95d6c19ab7f87bd3a60f>`__  2023-09-21   ``Use 'airflow.exceptions.AirflowException' in providers (#34511)``\n==================================================================================================  ===========  ===================================================================\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Yandex Provider using pip - Bash\nDESCRIPTION: This snippet demonstrates how to install the Yandex provider package for Apache Airflow using pip, including specifying an extra for cross-provider compatibility (common.compat). Users should ensure that pip is available in their environment and that an existing compatible Airflow installation exists. The command expects the user to have network access and appropriate permissions to install Python packages. It accepts extras in square brackets to install additional dependencies required for certain features.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-yandex[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: Sphinx directive to include external security documentation file from the devel-common directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/discord/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Change security.rst to use includes in providers\nDESCRIPTION: This commit message (hash 052e26ad47, dated 2023-11-04) details an update to the `security.rst` documentation file within providers, changing it to use include directives for better modularity or consistency (issue #35435).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_29\n\nLANGUAGE: text\nCODE:\n```\n``Change security.rst to use includes in providers (#35435)``\n```\n\n----------------------------------------\n\nTITLE: Enabling Legacy WWW Tests (Boolean String)\nDESCRIPTION: A boolean flag indicating whether tests for the legacy WWW interface should be run. 'true' enables these tests.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_25\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: Sphinx directive to include security documentation from a shared location in the development common files.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Removing Deprecated Context Keys Code\nDESCRIPTION: Commit message indicating the removal of code related to the deprecation of certain keys within the 'Context' object. References pull request #45585.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_43\n\nLANGUAGE: plaintext\nCODE:\n```\nRemove code for deprecation of Context keys (#45585)\n```\n\n----------------------------------------\n\nTITLE: Moving tests_common Package to devel-common Project\nDESCRIPTION: Commit message describing the relocation of the 'tests_common' package to a separate 'devel-common' project structure. References pull request #47281.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nMove tests_common package to devel-common project (#47281)\n```\n\n----------------------------------------\n\nTITLE: Defining RST Table of Contents for Airflow Administration and Deployment\nDESCRIPTION: This RST code snippet defines a table of contents for the Administration and Deployment section of Apache Airflow documentation. It lists various topics related to deploying and managing Airflow in production environments.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 2\n\n    production-deployment\n    logging-monitoring/index\n    kubernetes\n    lineage\n    listeners\n    dag-bundles\n    dag-serialization\n    modules_management\n    scheduler\n    dagfile-processing\n    pools\n    cluster-policies\n    priority-weight\n    web-stack\n    plugins\n```\n\n----------------------------------------\n\nTITLE: Running Pytest with Environment Variables in Airflow\nDESCRIPTION: Demonstrates how to run Airflow tests using pytest while preserving environment variables. This is useful for testing against specific configurations set through environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_44\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/core/ --keep-env-variables\n```\n\n----------------------------------------\n\nTITLE: Commit Hash Reference in RST Format\nDESCRIPTION: RST-formatted link to a specific commit in the Apache Airflow repository, used in the changelog.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n`999b70178a <https://github.com/apache/airflow/commit/999b70178a1f5d891fd2c88af4831a4ba4c2cbc9>`__\n```\n\n----------------------------------------\n\nTITLE: Updating Documentation for Dec 2021 Provider Release in RST\nDESCRIPTION: This commit (97496ba2b4, committed on 2021-12-31) updates the documentation for the December 2021 release of Airflow providers. Refers to issue #20523.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_9\n\nLANGUAGE: rst\nCODE:\n```\nUpdate documentation for provider December 2021 release (#20523)\n```\n\n----------------------------------------\n\nTITLE: Configuring RST Table of Contents for Slack Documentation\nDESCRIPTION: RST directive for creating a table of contents that includes all files in the current directory with depth of 1 level\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/connections/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Using Typed Context Consistently\nDESCRIPTION: Excluded Change (Version 2.2.0): Refactors code to use typed Context objects throughout, improving type safety and code clarity, referencing pull request #20565.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_26\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Use typed Context EVERYWHERE (#20565)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Provider Docs for July 2023 (RC2)\nDESCRIPTION: This commit message, linked to commit 225e3041d2 dated 2023-07-06, describes the preparation of documentation for the second release candidate (RC2) of the July 2023 wave of Apache Airflow Providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs for July 2023 wave of Providers (RC2) (#32381)``\n```\n\n----------------------------------------\n\nTITLE: Running All Static Checks for Last Commit using Breeze in Bash\nDESCRIPTION: This command uses Breeze to run all static checks for the last commit in the repository.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nbreeze static-checks --last-commit\n```\n\n----------------------------------------\n\nTITLE: Displaying Help for Breeze k9s Tool - Bash\nDESCRIPTION: This bash command shows help documentation for the `breeze k8s k9s` command by forwarding `--help` to the k9s tool, enabling discovery of additional cluster UI interaction features. `k9s` is run inside a Docker image managed by Breeze.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s k9s -- --help\n```\n\n----------------------------------------\n\nTITLE: Commit Message: More f-strings\nDESCRIPTION: Linked to commit 86a2a19ad2, this message indicates code modernization efforts by converting more string formatting to use f-strings in the codebase, tracked under issue #18855.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n``More f-strings (#18855)``\n```\n\n----------------------------------------\n\nTITLE: Including External RST File\nDESCRIPTION: This reStructuredText directive includes the content of the specified file ('/../../../devel-common/src/sphinx_exts/includes/security.rst') into the current document during processing by Sphinx or another RST processor. This is used to incorporate common content, like security guidelines, across multiple documentation files.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/exasol/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Import Hooks lazily individually in providers manager\nDESCRIPTION: This commit message, for commit 76ed2a49c6, indicates a performance optimization where Hooks are now imported lazily and individually within the providers manager, as detailed in issue #17682.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n``Import Hooks lazily individually in providers manager (#17682)``\n```\n\n----------------------------------------\n\nTITLE: Breeze Kubernetes Shell Prompt Example - Bash\nDESCRIPTION: This is an example of the shell prompt shown when entering the Kubernetes shell using Breeze. The prompt displays the cluster and executor in use. Not an executable command, but illustrates what users should expect upon entering a configured shell session.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\n(kind-airflow-python-3.9-v1.24.0:KubernetesExecutor)>\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Docs - Providers November 2023 (1st Wave)\nDESCRIPTION: This commit message, associated with commit 1b059c57d6 on 2023-11-08, describes the preparation of documentation for the first wave of provider releases in November 2023.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 1st wave of Providers November 2023 (#35537)\n```\n\n----------------------------------------\n\nTITLE: Saving SVN Repository Root Path in Shell\nDESCRIPTION: Determines the absolute path of the current working directory (assumed to be the SVN repository root after checkout/update) and saves it to the environment variable `SVN_REPO_ROOT`.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\nSVN_REPO_ROOT=$(pwd -P)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare RC2 Docs for May 2023 Providers\nDESCRIPTION: This commit message, associated with commit 45548b9451 dated 2023-05-19, describes the preparation of documentation for the second release candidate (RC2) of the May 2023 wave of Apache Airflow Providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n``Prepare RC2 docs for May 2023 wave of Providers (#31416)``\n```\n\n----------------------------------------\n\nTITLE: Generating Airflow Constraints with Breeze (Bash)\nDESCRIPTION: Uses the `breeze release-management generate-constraints` command to create constraint files for Apache Airflow dependencies. This example specifies the 'constraints' mode using `--airflow-constraints-mode`. Generating constraints requires Docker images built with the `--upgrade-to-newer-dependencies` flag for all relevant Python versions. Constraints are based on `pyproject.toml` and generated for specific Python versions and constraint modes ('constraints', 'constraints-source-providers', 'constraints-no-providers').\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management generate-constraints --airflow-constraints-mode constraints\n```\n\n----------------------------------------\n\nTITLE: Package Version History in RST Format\nDESCRIPTION: ReStructuredText formatted changelog containing detailed commit history for the Apache Pig provider package. Shows version numbers, dates, commit hashes, and descriptions of changes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pig/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nPackage apache-airflow-providers-apache-pig\n------------------------------------------------------\n\n`Apache Pig <https://pig.apache.org/>`__\n\n\nThis is detailed commit list of changes for versions provider package: ``apache.pig``.\nFor high-level changelog, see :doc:`package information including changelog <index>`.\n```\n\n----------------------------------------\n\nTITLE: Displaying Airflow Provider Commits (Dec 2023 - v1.9.0) using RST Table\nDESCRIPTION: This reStructuredText (RST) snippet formats a list of commits related to Apache Airflow provider version 1.9.0 (December 2023, 1st wave) into a table. It includes the commit hash (linked to the specific GitHub commit), the commit date, and the commit subject, utilizing RST's table structure, hyperlink syntax (`<link>`__), and inline code formatting (``code``).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  =========================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =========================================================================\n`999b70178a <https://github.com/apache/airflow/commit/999b70178a1f5d891fd2c88af4831a4ba4c2cbc9>`__  2023-12-08   ``Prepare docs 1st wave of Providers December 2023 (#36112)``\n`d0918d77ee <https://github.com/apache/airflow/commit/d0918d77ee05ab08c83af6956e38584a48574590>`__  2023-12-07   ``Bump minimum Airflow version in providers to Airflow 2.6.0 (#36017)``\n`3bb5978e63 <https://github.com/apache/airflow/commit/3bb5978e63f3be21a5bb7ae89e7e3ce9d06a4ab8>`__  2023-12-06   ``Add Architecture Decision Record for common.sql introduction (#36015)``\n==================================================================================================  ===========  =========================================================================\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Docs for May 2023 Providers Wave\nDESCRIPTION: This commit message, associated with commit d9ff55cf6d dated 2023-05-16, indicates the preparation of documentation for the May 2023 release wave of Apache Airflow Providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs for May 2023 wave of Providers (#31252)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Remove Backport Providers\nDESCRIPTION: Linked to commit 68e4c4dcb0, this message indicates the removal of backport provider packages from the Airflow project, likely as part of moving towards a newer provider model. Tracked in issue #14886.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_27\n\nLANGUAGE: text\nCODE:\n```\n``Remove Backport Providers (#14886)``\n```\n\n----------------------------------------\n\nTITLE: Importing BaseExecutor in Python for Custom Executor Implementation\nDESCRIPTION: Shows how to import the BaseExecutor class, which is the foundation for creating custom executors in Airflow. All executors must derive from this base class.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/public-airflow-interface.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.executors.base_executor import BaseExecutor\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry 1.1.0\nDESCRIPTION: RST formatted changelog entry for version 1.1.0 documenting Airflow 2.4+ requirement\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n1.1.0\n.....\n\n.. note::\n  This release of provider is only available for Airflow 2.4+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n\nMisc\n~~~~\n\n* ``Bump minimum Airflow version in providers (#30917)``\n```\n\n----------------------------------------\n\nTITLE: Fixing template_fields Type for MyPy Compatibility in RST\nDESCRIPTION: This commit (d56e7b56bb, committed on 2021-12-30) adjusts the type annotation for 'template_fields' to use the MyPy-friendly 'Sequence' type. Refers to issue #20571.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_11\n\nLANGUAGE: rst\nCODE:\n```\nFix template_fields type to have MyPy friendly Sequence type (#20571)\n```\n\n----------------------------------------\n\nTITLE: Package Requirements Table in RST\nDESCRIPTION: A reStructuredText table showing the required package dependencies and their minimum versions for using the Asana provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n\"apache-airflow\"  \">=2.9.0\"\n\"asana\"           \">=5.0.0\"\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Including External Documentation File in reStructuredText\nDESCRIPTION: This reStructuredText directive includes the content of another file (`installing-providers-from-sources.rst`) located at a relative path within the project structure. This is commonly used in Sphinx documentation to reuse content blocks, such as installation instructions or common notices.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/winrm/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Migrating Kubernetes Pod and Resource Classes in Python\nDESCRIPTION: Migration rules for Kubernetes Pod and Resource classes, moving from custom Airflow implementations to official Kubernetes client models.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41735.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"airflow.kubernetes.pod.Port\"  \"kubernetes.client.models.V1ContainerPort\"\n\"airflow.kubernetes.pod.Resources\"  \"kubernetes.client.models.V1ResourceRequirements\"\n\"airflow.kubernetes.pod_runtime_info_env.PodRuntimeInfoEnv\"  \"kubernetes.client.models.V1EnvVar\"\n\"airflow.kubernetes.volume.Volume\"  \"kubernetes.client.models.V1Volume\"\n\"airflow.kubernetes.volume_mount.VolumeMount\"  \"kubernetes.client.models.V1VolumeMount\"\n```\n\n----------------------------------------\n\nTITLE: Changelog Table Format in RST\nDESCRIPTION: ReStructuredText formatted tables showing commit history, dates and descriptions for each version of the Apprise provider\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  =======================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =======================================================================\n`e9987d5059 <https://github.com/apache/airflow/commit/e9987d50598f70d84cbb2a5d964e21020e81c080>`__  2023-10-13   ``Prepare docs 1st wave of Providers in October 2023 (#34916)``\n`0c8e30e43b <https://github.com/apache/airflow/commit/0c8e30e43b70e9d033e1686b327eb00aab82479c>`__  2023-10-05   ``Bump min airflow version of providers (#34728)``\n==================================================================================================\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for Apache Airflow Providers\nDESCRIPTION: ReStructuredText documentation explaining Apache Airflow providers, their purpose, types, installation methods and extension capabilities. The document includes sections on configuration, custom connections, extra links, logging, secret backends and notifications.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n```\n\n----------------------------------------\n\nTITLE: Including Sections and Options Documentation in RST\nDESCRIPTION: This reStructuredText directive includes the content of the specified external file '/../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst'. This allows for the reuse of common documentation describing sections and options across different parts of the documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/configurations-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst\n```\n\n----------------------------------------\n\nTITLE: Commit Log Entry Format\nDESCRIPTION: Example of a commit log entry showing the standard format used in the changelog, including commit hash, commit date, and subject.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/druid/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n`cead3da4a6 <https://github.com/apache/airflow/commit/cead3da4a6f483fa626b81efd27a24dcb5a36ab0>`__  2024-01-26   ``Add docs for RC2 wave of providers for 2nd round of Jan 2024 (#37019)``\n```\n\n----------------------------------------\n\nTITLE: Excluding Changes from Changelog in RST (Repeated)\nDESCRIPTION: Another reStructuredText comment block listing additional changes excluded from the changelog. These are typically minor updates or internal modifications not relevant to end-users.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Fixed changelog for January 2022 (delayed) provider's release (#21439)``\n   * ``Fix K8S changelog to be PyPI-compatible (#20614)``\n   * ``Fix template_fields type to have MyPy friendly Sequence type (#20571)``\n   * ``Add documentation for January 2021 providers release (#21257)``\n   * ``Remove ':type' lines now sphinx-autoapi supports typehints (#20951)``\n   * ``Update documentation for provider December 2021 release (#20523)``\n   * ``Use typed Context EVERYWHERE (#20565)``\n   * ``Update documentation for November 2021 provider's release (#19882)``\n   * ``Prepare documentation for October Provider's release (#19321)``\n   * ``More f-strings (#18855)``\n   * ``Update documentation for September providers release (#18613)``\n   * ``Static start_date and default arg cleanup for misc. provider example DAGs (#18597)``\n```\n\n----------------------------------------\n\nTITLE: Updating DatasetManager.create_datasets Method Signature in Python\nDESCRIPTION: Modified function signature now accepts Dataset objects instead of DatasetModel objects. The function internally creates and returns DatasetModel objects. The session parameter has been changed to be keyword-only.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/42343.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDatasetManager.create_datasets(*, session, datasets: List[Dataset]) -> List[DatasetModel]\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Block for Airflow Database ERD\nDESCRIPTION: ReStructuredText markup defining the database schema documentation section, including Apache license header, warnings about schema stability, and ERD image inclusion.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/database-erd-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\nERD Schema of the Database\n''''''''''''''''''''''''''\n\nHere is the current Database schema diagram.\n\n.. warning::\n\n   The ER diagram shows the snapshot of the database structure valid for Airflow version |version| and it\n   should be treated as an internal detail. It might be changed at any time and you should not directly\n   access the database to retrieve information from it or modify the data - you should use\n   :doc:`stable-rest-api-ref` to do that instead.\n   The main purpose of this diagram is to help with troubleshooting and understanding of the\n   internal Airflow DB architecture in case you have any problems with the database - for example\n   when dealing with problems with migrations. See also :doc:`migrations-ref` for\n   list of detailed database migrations that are applied when running migration script and\n   `db command <cli-and-env-variables-ref.html#db>`_ for the commands that you can use to manage\n   the migrations.\n\n.. This image is automatically generated by pre-commit via ``scripts/ci/pre_commit/update_er_diagram.py``\n\n.. image:: img/airflow_erd.svg\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: A reStructuredText include directive that imports security documentation from a common development source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit History in Markdown Table\nDESCRIPTION: This code snippet shows a markdown table format used to display the commit history for a specific version of the Apache Livy provider. It includes columns for commit hash, commit date, and commit subject.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/livy/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  =====================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =====================================================================\n`2723508345 <https://github.com/apache/airflow/commit/2723508345d5cf074aeb673955ce72996785f2bc>`__  2024-12-20   ``Prepare docs for Nov 1st wave of providers Dec 2024 (#45042)``\n`35b927fe17 <https://github.com/apache/airflow/commit/35b927fe177065dad0e00c49d72b494e58b27ca8>`__  2024-12-19   ``Update path of example dags in docs (#45069)``\n`4b38bed76c <https://github.com/apache/airflow/commit/4b38bed76c1ea5fe84a6bc678ce87e20d563adc0>`__  2024-12-16   ``Bump min version of Providers to 2.9 (#44956)``\n`6f1351122d <https://github.com/apache/airflow/commit/6f1351122dfcbefd15d6414aa3e4a8b8416aba6e>`__  2024-12-03   ``Remove Provider Deprecations in Apache Livy (#44631)``\n`1275fec92f <https://github.com/apache/airflow/commit/1275fec92fd7cd7135b100d66d41bdcb79ade29d>`__  2024-11-24   ``Use Python 3.9 as target version for Ruff & Black rules (#44298)``\n`4dfae23532 <https://github.com/apache/airflow/commit/4dfae23532d26ed838069c49d48f28c185e954c6>`__  2024-11-15   ``Update DAG example links in multiple providers documents (#44034)``\n==================================================================================================  ===========  =====================================================================\n```\n\n----------------------------------------\n\nTITLE: Indicating pyproject.toml Change (Boolean String)\nDESCRIPTION: A boolean flag indicating if the `pyproject.toml` file was changed in the Pull Request. 'false' means it was not changed.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\nfalse\n```\n\n----------------------------------------\n\nTITLE: Update JDBC Provider Dependencies\nDESCRIPTION: Move provider dependencies to inside provider folders and remove hook-class-names from provider.yaml.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n\"Move provider dependencies to inside provider folders (#24672)\"\n\"Remove 'hook-class-names' from provider.yaml (#24702)\"\n```\n\n----------------------------------------\n\nTITLE: Simplifying airflow_version Imports (Commit Message)\nDESCRIPTION: Commit message detailing a simplification of how 'airflow_version' is imported in the Apache Airflow codebase, impacting providers such as Airbyte.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nSimplify 'airflow_version' imports (#39497)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Move Min Airflow Version to 2.3.0\nDESCRIPTION: This commit message, linked to commit 78b8ea2f22 dated 2022-10-24, indicates that the minimum required Airflow version for all providers was raised to 2.3.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_24\n\nLANGUAGE: text\nCODE:\n```\n``Move min airflow version to 2.3.0 for all providers (#27196)``\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Flink Provider with CNCF Kubernetes Dependency\nDESCRIPTION: Command to install the Apache Flink provider package with the CNCF Kubernetes cross-provider dependency using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/flink/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-flink[cncf.kubernetes]\n```\n\n----------------------------------------\n\nTITLE: Navigating to Airflow Project Directory\nDESCRIPTION: Command to change directory to the Airflow project location\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/Projects/airflow\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Table\nDESCRIPTION: A formatted markdown table showing commit history with commit hashes, dates and commit messages for the AWS provider package\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_15\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ======================================================================================================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ======================================================================================================================================================================\n`32971a1a2d <https://github.com/apache/airflow/commit/32971a1a2de1db0b4f7442ed26facdf8d3b7a36f>`__  2020-12-09   ``Updates providers versions to 1.0.0 (#12955)``\n`d5589673a9 <https://github.com/apache/airflow/commit/d5589673a95aaced0b851ea0a4061a010a924a82>`__  2020-12-08   ``Move dummy_operator.py to dummy.py (#11178) (#11293)``\n```\n\n----------------------------------------\n\nTITLE: Installing JQ on Ubuntu\nDESCRIPTION: Command for installing the jq JSON processor on Ubuntu systems using apt package manager\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/18_contribution_workflow.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install jq\n```\n\n----------------------------------------\n\nTITLE: Building Customized CI Image with Breeze\nDESCRIPTION: Equivalent Breeze command to build a customized CI image with Python 3.9, additional Airflow extras, and Python dependencies. Breeze automatically sets DOCKER_BUILDKIT=1.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image build --python 3.9 --additional-airflow-extras=jdbc --additional-python-deps=\"pandas\" \\\n    --additional-dev-apt-deps=\"gcc g++\"\n```\n\n----------------------------------------\n\nTITLE: Running Provider System Tests via Pytest\nDESCRIPTION: Command to run provider-specific system tests using pytest, demonstrated with BigQuery provider test.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/system_tests.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest --system providers/google/tests/system/google/cloud/bigquery/example_bigquery_queries.py\n```\n\n----------------------------------------\n\nTITLE: Example Test Collection Failure Error - TXT - txt\nDESCRIPTION: Provides a sample error message from pytest indicating failure to import modules due to a missing provider dependency. Used in the documentation to illustrate why 'pytest.importorskip' is necessary. Not for code execution; serves as reference in troubleshooting guides.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/MANAGING_PROVIDERS_LIFECYCLE.rst#2025-04-22_snippet_8\n\nLANGUAGE: txt\nCODE:\n```\n_____ ERROR collecting providers/apache/beam/tests/apache/beam/operators/test_beam.py ______\nImportError while importing test module '/opt/airflow/providers/apache/beam/tests/apache/beam/operators/test_beam.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/usr/local/lib/python3.8/importlib/__init__.py:127: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nproviders/apache/beam/tests/apache/beam/operators/test_beam.py:25: in <module>\n    from airflow.providers.apache.beam.operators.beam import (\nairflow/providers/apache/beam/operators/beam.py:35: in <module>\n    from airflow.providers.google.cloud.hooks.dataflow import (\nairflow/providers/google/cloud/hooks/dataflow.py:32: in <module>\n    from google.cloud.dataflow_v1beta3 import GetJobRequest, Job, JobState, JobsV1Beta3AsyncClient, JobView\nE   ModuleNotFoundError: No module named 'google.cloud.dataflow_v1beta3'\n_ ERROR collecting providers/microsoft/azure/tests/microsoft/azure/transfers/test_azure_blob_to_gcs.py _\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Kubernetes Test Execution\nDESCRIPTION: Example output showing the execution of Kubernetes tests, including environment setup, Python version verification, virtualenv creation, and test execution with pytest. The output demonstrates successful test runs.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\nUsing CPython 3.12.7\nCreating virtual environment at: .venv\nActivate with: source .venv/bin/activate\nResolved 775 packages in 84ms\nAudited 190 packages in 1ms\nGood version of kind installed: 0.26.0 in /Users/jarek/IdeaProjects/airflow/kubernetes-tests/.venv/bin\nGood version of kubectl installed: 1.31.0 in /Users/jarek/IdeaProjects/airflow/kubernetes-tests/.venv/bin\nGood version of helm installed: 3.16.4 in /Users/jarek/IdeaProjects/airflow/kubernetes-tests/.venv/bin\nStable repo is already added\n\nRunning tests with kind-airflow-python-3.9-v1.29.12 cluster.\n Command to run: uv run pytest kubernetes-tests/tests/\nInstalled 74 packages in 179ms\n/Users/jarek/IdeaProjects/airflow/.venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:208: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n======================================================================================================================================= test session starts ========================================================================================================================================\nplatform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /Users/jarek/IdeaProjects/airflow/.venv/bin/python3\ncachedir: .pytest_cache\nrootdir: /Users/jarek/IdeaProjects/airflow/kubernetes-tests\nconfigfile: pyproject.toml\nplugins: asyncio-0.24.0, cov-6.0.0, instafail-0.5.0, timeouts-1.2.1, time-machine-2.16.0, custom-exit-code-0.3.0, icdiff-0.9, kgb-7.2, rerunfailures-15.0, mock-3.14.0, unordered-0.6.1, anyio-4.6.2.post1, xdist-3.6.1, requests-mock-1.12.1\nasyncio: mode=Mode.STRICT, default_loop_scope=None\nsetup timeout: 0.0s, execution timeout: 0.0s, teardown timeout: 0.0s\ncollected 52 items\n\nkubernetes-tests/tests/kubernetes_tests/test_kubernetes_executor.py::TestKubernetesExecutor::test_integration_run_dag PASSED\n...                                                                                                                                                     [  1%]\n```\n\n----------------------------------------\n\nTITLE: Preparing Docs for 1st Wave May 2024 (Commit Message)\nDESCRIPTION: Commit message indicating the preparation of documentation for the first wave of Apache Airflow provider releases in May 2024, including the Airbyte provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs 1st wave May 2024 (#39328)\n```\n\n----------------------------------------\n\nTITLE: Defining a Google Cloud Workflow Structure in Python\nDESCRIPTION: Illustrates the expected structure for defining a Google Cloud Workflow, likely using a Python dictionary or a similar data structure. This definition is typically passed as the `workflow` parameter to the `WorkflowsCreateWorkflowOperator`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/workflows.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/workflows/example_workflows.py\n      :language: python\n      :dedent: 0\n      :start-after: [START how_to_define_workflow]\n      :end-before: [END how_to_define_workflow]\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Secrets Manager with Lookup Pattern\nDESCRIPTION: Configuration for AWS Secrets Manager backend that uses a lookup pattern to retrieve only specific connections. It sets a regex pattern to match connection names starting with 'm'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/aws-secrets-manager.rst#2025-04-22_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n[secrets]\nbackend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend\nbackend_kwargs = {\n  \"connections_prefix\": \"airflow/connections\",\n  \"connections_lookup_pattern\": \"^m\",\n  \"profile_name\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing the Apache Airflow Tableau Provider\nDESCRIPTION: Shell command to install the standalone `apache-airflow-providers-tableau` package. This is the required method to get Tableau integration after it was separated from the Salesforce provider, as noted in the breaking changes for versions 2.0.0 and 5.0.0. Requires the pip package installer.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-tableau\n```\n\n----------------------------------------\n\nTITLE: Listing Google Cloud Run Jobs using CloudRunListJobsOperator in Airflow\nDESCRIPTION: Demonstrates how to use the CloudRunListJobsOperator to list Cloud Run jobs in an Airflow DAG. Optional parameters for limiting results and including deleted jobs are shown.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_run.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlist_jobs = CloudRunListJobsOperator(\n    task_id=\"list_jobs\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    limit=10,\n    show_deleted=True\n)\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 2.0.0\nDESCRIPTION: Changelog entry documenting breaking changes related to apply_default decorator and addition of extra parameters support\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: restructuredtext\nCODE:\n```\n2.0.0\n.....\n\nBreaking changes\n~~~~~~~~~~~~~~~~\n\n* ``Auto-apply apply_default decorator (#15667)``\n\n.. warning:: Due to apply_default decorator removal, this version of the provider requires Airflow 2.1.0+.\n   If your Airflow version is < 2.1.0, and you want to install this provider version, first upgrade\n   Airflow to at least version 2.1.0. Otherwise your Airflow package version will be upgraded\n   automatically and you will have to manually run ``airflow upgrade db`` to complete the migration.\n\nFeatures\n~~~~~~~~\n\n* ``Add support for extra parameters to samba client (#16115)``\n```\n\n----------------------------------------\n\nTITLE: Version Header\nDESCRIPTION: Version number formatting in RST\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n2.3.0\n.....\n```\n\n----------------------------------------\n\nTITLE: Git Commit Hash Reference\nDESCRIPTION: A Git commit hash used for version control tracking in the Apache Airflow repository. The 40-character SHA-1 hash uniquely identifies a specific commit in the project's history.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/druid/docs/.latest-doc-only-change.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n7b2ec33c7ad4998d9c9735b79593fcdcd3b9dd1f\n```\n\n----------------------------------------\n\nTITLE: Including External reStructuredText File for Airflow Provider Installation\nDESCRIPTION: This directive includes an external reStructuredText file containing instructions for installing Airflow providers from sources. The file is located in a common development directory structure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/edge3/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Docker COPY Command Example\nDESCRIPTION: Example of Docker COPY command that can be affected by inconsistent file permissions, leading to cache invalidation.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0008-fixing-group-permissions-before-build.md#2025-04-22_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nCOPY scripts/ci/build_airflow /opt/airflow/scripts/ci/build\n```\n\n----------------------------------------\n\nTITLE: Installing Salesforce Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Salesforce integration, enabling the Salesforce hook.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[salesforce]'\n```\n\n----------------------------------------\n\nTITLE: Including External RST File for Provider Installation Instructions\nDESCRIPTION: This directive includes an external RST file that contains instructions for installing Apache Airflow provider packages from source code. The included file is located in a common development directory used across the project documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Formatting Version Header in Markdown\nDESCRIPTION: This snippet demonstrates how version headers are formatted in the changelog using Markdown syntax. It includes the version number and a separator line.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_38\n\nLANGUAGE: markdown\nCODE:\n```\n4.3.1\n.....\n```\n\n----------------------------------------\n\nTITLE: Including Sections and Options Reference in reStructuredText\nDESCRIPTION: This directive includes an external file containing reference documentation for sections and options in Apache Airflow configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/edge3/docs/configurations-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst\n```\n\n----------------------------------------\n\nTITLE: Removing Unused Provider's Distribution\nDESCRIPTION: Commit message stating the removal of an unused provider's distribution package or files. References pull request #46608.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_27\n\nLANGUAGE: plaintext\nCODE:\n```\nRemoved the unused provider's distribution (#46608)\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Tree Configuration\nDESCRIPTION: RST directive that configures the documentation tree structure with maximum depth of 1 level and glob pattern to include all files in the directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/logging/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Add Documentation for Jan 2024 RC2 Provider Wave (Excluded from v1.2.0 Changelog)\nDESCRIPTION: Indicates the addition of documentation for the second round (RC2) of the January 2024 provider release wave, referenced by pull request #37019. This change was intentionally excluded from the main changelog notes for version 1.2.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_20\n\nLANGUAGE: plaintext\nCODE:\n```\nAdd docs for RC2 wave of providers for 2nd round of Jan 2024 (#37019)\n```\n\n----------------------------------------\n\nTITLE: Defining Apache Airflow DingDing Provider Package\nDESCRIPTION: Specifies the name of the Apache Airflow provider package for DingDing integration.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n``apache-airflow-providers-dingding``\n```\n\n----------------------------------------\n\nTITLE: Service Transfers Reference Block\nDESCRIPTION: ReStructuredText directive that generates reference documentation for service-related transfers with specific formatting.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/operators-and-hooks-ref/services.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. transfers-ref::\n   :tags: service\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Including External File in reStructuredText\nDESCRIPTION: This reStructuredText directive (`.. include::`) instructs the Sphinx documentation generator to insert the content of the specified file (`/../../../devel-common/src/sphinx_exts/includes/security.rst`) at this point in the document. This is commonly used for reusing content like disclaimers, licenses, or common sections across multiple documentation pages.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Warning About Unused replace Parameter\nDESCRIPTION: Reference to an improvement that warns users about an unused 'replace' parameter in the insert_rows method of OracleHook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: reStructuredText\nCODE:\n```\n* ``Warn user about unused replace parameter in insert_rows (OracleHook) (#39408)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: D205 Support for Snowflake to Zendesk Providers\nDESCRIPTION: This commit message, associated with commit 21e8f878a3 dated 2023-07-06, indicates the addition of D205 style support for Apache Airflow providers, specifically covering the range from Snowflake to Zendesk.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n``D205 Support - Providers: Snowflake to Zendesk (inclusive) (#32359)``\n```\n\n----------------------------------------\n\nTITLE: Viewing Docker Context Files\nDESCRIPTION: Command to list all files in the Docker build context using a minimal scratch image\nSOURCE: https://github.com/apache/airflow/blob/main/dev/MANUALLY_BUILDING_IMAGES.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nprintf 'FROM scratch\\nCOPY . /' | DOCKER_BUILDKIT=1 docker build -q -f- -o- . | tar t\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fix missing line in index.rst for provider documentation (#31343)\nDESCRIPTION: This commit message, associated with version 3.2.0, corrects a missing line in the index.rst file related to provider documentation generation. Commit hash: 7ebda3898d, Date: 2023-05-17.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n``Fix missing line in index.rst for provider documentation (#31343)``\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Revert logger_name param addition\nDESCRIPTION: This text is a commit message summary indicating the reversion of a previous change (PR #36675) that added a 'logger_name' parameter to provider hooks. The revert is linked to pull request #37015.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_23\n\nLANGUAGE: plaintext\nCODE:\n```\nRevert \"Provide the logger_name param in providers hooks in order to override the logger name (#36675)\" (#37015)\n```\n\n----------------------------------------\n\nTITLE: Importing GPG Keys for Apache Airflow Release Verification\nDESCRIPTION: Commands to import GPG keys for verifying release signatures, either by importing the entire KEYS file or individual keys from keyservers.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\ngpg --import KEYS\n```\n\nLANGUAGE: shell\nCODE:\n```\ngpg --keyserver keys.openpgp.org --receive-keys CDE15C6E4D3A8EC4ECF4BA4B6674E08AD7DE406F\n```\n\nLANGUAGE: shell\nCODE:\n```\ngpg --keyserver keys.gnupg.net --receive-keys CDE15C6E4D3A8EC4ECF4BA4B6674E08AD7DE406F\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fix Missing Line in Provider Docs Index\nDESCRIPTION: This commit message, linked to commit 7ebda3898d dated 2023-05-17, describes a fix for a missing line in the index.rst file used for generating Apache Airflow provider documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n``Fix missing line in index.rst for provider documentation (#31343)``\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: Sphinx directive to include security documentation from a common development source directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/iceberg/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Including Security Information in reStructuredText\nDESCRIPTION: This snippet uses a reStructuredText directive to include security-related information from an external file. It's likely used to maintain consistent security documentation across the project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/impala/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Copying a File from GCS to an Existing Google Drive Folder\nDESCRIPTION: This snippet shows how to copy a file from Google Cloud Storage to a specific folder in Google Drive by specifying the folder ID. This allows targeting an existing folder in Google Drive's hierarchy.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gdrive.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntransfer_gcs_to_gdrive_folder = GCSToGoogleDriveOperator(\n    task_id=\"gcs_to_drive_folder\",\n    source_bucket=BUCKET_NAME,\n    source_object=GCS_FILE_PATH,\n    destination_object=DRIVE_FILE_PATH,\n    folder_id=FOLDER_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Hooks Lazily in Providers Manager in RST\nDESCRIPTION: This commit (76ed2a49c6, committed on 2021-08-19) modifies the providers manager to import Hooks lazily on an individual basis, potentially improving startup time or reducing dependencies. Refers to issue #17682.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_21\n\nLANGUAGE: rst\nCODE:\n```\nImport Hooks lazily individually in providers manager (#17682)\n```\n\n----------------------------------------\n\nTITLE: Improve Provider Dependency Update Documentation (Excluded from v1.4.1 Changelog)\nDESCRIPTION: Mentions an improvement made to the documentation regarding the process of updating provider dependencies, referenced by pull request #47203. This change was intentionally excluded from the main changelog notes for version 1.4.1.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nImprove documentation for updating provider dependencies (#47203)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Update description for connection-types provider meta-data\nDESCRIPTION: Linked to commit be75dcd39c, this message describes an update to the documentation regarding the new 'connection-types' provider meta-data field, explaining its purpose or usage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n``Update description about the new ''connection-types'' provider meta-data``\n```\n\n----------------------------------------\n\nTITLE: Commit hash and message formatting\nDESCRIPTION: Example of standardized commit entry format showing hash, link, date and message\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/cassandra/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n`e9987d5059 <https://github.com/apache/airflow/commit/e9987d50598f70d84cbb2a5d964e21020e81c080>`__  2023-10-13   ``Prepare docs 1st wave of Providers in October 2023 (#34916)``\n```\n\n----------------------------------------\n\nTITLE: Listing Commit for Classifier Release Documentation (rst)\nDESCRIPTION: This snippet shows a commit entry in reStructuredText format, linking to a specific commit on GitHub and displaying its commit date and subject line. This commit specifically adds documentation for the Classifier release in March 2022.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n`16adc035b1 <https://github.com/apache/airflow/commit/16adc035b1ecdf533f44fbb3e32bea972127bb71>`__  2022-03-14   ``Add documentation for Classifier release for March 2022 (#22226)``\n```\n\n----------------------------------------\n\nTITLE: Auto-generation Notice RST Block\nDESCRIPTION: Warning notice about file being automatically generated from a template.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!\n\n.. IF YOU WANT TO MODIFY THIS FILE, YOU SHOULD MODIFY THE TEMPLATE\n   `PROVIDER_COMMITS_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY\n\n.. THE REMAINDER OF THE FILE IS AUTOMATICALLY GENERATED. IT WILL BE OVERWRITTEN!\n```\n\n----------------------------------------\n\nTITLE: CloudSQL Clone Operator Templated Fields (Python)\nDESCRIPTION: Defines which fields can be Jinja-templated in CloudSQLCloneInstanceOperator allowing dynamic clone configuration at DAG runtime in Airflow. Includes project_id, source instance, destination clone name, and body. Requires Airflow Google provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n\"template_fields = (\\\"project_id\\\", \\\"instance\\\", \\\"destination_clone_name\\\", \\\"body\\\")\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Airflow Site Repository\nDESCRIPTION: Commands to clone the airflow-site repository, create a branch for documentation, and set environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/apache/airflow-site.git airflow-site\ncd airflow-site\ngit checkout -b helm-${VERSION}-docs\nexport AIRFLOW_SITE_DIRECTORY=\"$(pwd -P)\"\n```\n\n----------------------------------------\n\nTITLE: Including Sections and Options Documentation in Sphinx\nDESCRIPTION: This snippet uses another Sphinx directive to include documentation about sections and options, likely related to Apache Airflow configuration. It references an external file that contains this information.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/configurations-ref.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst\n```\n\n----------------------------------------\n\nTITLE: Listing Airflow Provider Commits (v3.0.0) using RST\nDESCRIPTION: This reStructuredText snippet shows a table of commits related to the Apache Airflow provider release wave around version 3.0.0 (June 2022). It lists linked commit hashes, commit dates, and subjects, specifically mentioning updates to release notes for a release candidate and fixing package descriptions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/winrm/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`dcdcf3a2b8 <https://github.com/apache/airflow/commit/dcdcf3a2b8054fa727efb4cd79d38d2c9c7e1bd5>`__  2022-06-09   ``Update release notes for RC2 release of Providers for May 2022 (#24307)``\n`717a7588bc <https://github.com/apache/airflow/commit/717a7588bc8170363fea5cb75f17efcf68689619>`__  2022-06-07   ``Update package description to remove double min-airflow specification (#24292)``\n==================================================================================================  ===========  ==================================================================================\n```\n\n----------------------------------------\n\nTITLE: Auto-generation Notice Comment Block\nDESCRIPTION: Warning notice indicating that this file is automatically generated from a template and should not be modified directly.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n .. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!\n\n .. IF YOU WANT TO MODIFY THIS FILE, YOU SHOULD MODIFY THE TEMPLATE\n    `PROVIDER_COMMITS_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY\n\n .. THE REMAINDER OF THE FILE IS AUTOMATICALLY GENERATED. IT WILL BE OVERWRITTEN!\n```\n\n----------------------------------------\n\nTITLE: Installing Snowflake Provider Package for Airflow\nDESCRIPTION: Command to install the Snowflake provider package for Apache Airflow using pip. This is a prerequisite for using the @task.snowpark decorator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/decorators/snowpark.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow-providers-snowflake'\n```\n\n----------------------------------------\n\nTITLE: Generating SSH Keys for GitHub Authentication\nDESCRIPTION: Command to generate SSH keys for authenticating with private GitHub repositories.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Webserver Port and Base URL for SSL (INI)\nDESCRIPTION: Sets the webserver listening port and base URL, often used with SSL. This example shows configuring the standard SSL port 443. Setting `web_server_port` to 443 requires appropriate system privileges. The `base_url` should reflect the correct scheme (https) and port.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/security.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n# Optionally, set the server to listen on the standard SSL port.\nweb_server_port = 443\nbase_url = http://<hostname or IP>:443\n```\n\n----------------------------------------\n\nTITLE: Setting Up Init Containers in Airflow Kubernetes\nDESCRIPTION: Example configuration for adding init containers to Airflow pods. Demonstrates a simple init container setup that executes a hello command before the main container starts.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/using-additional-containers.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nscheduler:\n  extraInitContainers:\n    - name: hello\n      image: debian\n      args:\n        - echo\n        - hello\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Filesystem (file://) for Airflow XComs (INI)\nDESCRIPTION: This configuration snippet shows how to configure Airflow to use the local filesystem for storing XComs using the `file://` scheme. It sets the `xcom_backend` to `XComObjectStorageBackend` and specifies the local directory path in `xcom_objectstorage_path`. A `xcom_objectstorage_threshold` greater than -1 is also required for this backend to function, although not explicitly shown in this example.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/io/docs/xcom_backend.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nxcom_backend = airflow.providers.common.io.xcom.backend.XComObjectStorageBackend\n\n[common.io]\nxcom_objectstorage_path = file://airflow/xcoms\n```\n\n----------------------------------------\n\nTITLE: Improving Schema Filtering in Hive Hook in Python\nDESCRIPTION: Enhances the filtering mechanism for invalid schemas in the Hive hook to improve data integrity and error handling.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n\"Improve filtering for invalid schemas in Hive hook (#27808)\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Migration Rule for Removing TriggerRule.NONE_FAILED_OR_SKIPPED in Apache Airflow\nDESCRIPTION: This code snippet specifies the migration rule needed for removing the TriggerRule.NONE_FAILED_OR_SKIPPED option. It uses the ruff linter with the AIR302 rule to identify and address this change.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/44475.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* Migration rules needed\n\n  * ruff\n\n    * AIR302\n\n      * [x] ``TriggerRule.NONE_FAILED_OR_SKIPPED``\n```\n\n----------------------------------------\n\nTITLE: Including External Documentation with Licensing Notice - reStructuredText\nDESCRIPTION: This reStructuredText snippet begins with a licensing notice for the Apache Software Foundation, specifying usage terms under the Apache License, Version 2.0. It then uses the \".. include::\" directive to incorporate security documentation from an external file path, enabling shared documentation fragments across multiple files. No executable code is present; the snippet is intended for use with Sphinx doc toolchains and relies on correct path resolution and the presence of the included file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Documenting Provider Update October 2023 (1st Wave)\nDESCRIPTION: This commit prepares the documentation for the first wave of Apache Airflow provider updates released in October 2023, referenced by issue #34916.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs 1st wave of Providers in October 2023 (#34916)\n```\n\n----------------------------------------\n\nTITLE: Templating Image Label Detection Field - Google Cloud Vision Airflow Operator - Python\nDESCRIPTION: This defines the template fields for the CloudVisionDetectImageLabelsOperator, facilitating dynamic parameterization of GCP project ID, location, image, and retry configurations in Airflow. It supports usage in parameterized and data-driven Airflow DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"project_id\",\n    \"location\",\n    \"image\",\n    \"retry\",\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents for OpenSearch Operators in reStructuredText\nDESCRIPTION: This snippet uses the toctree directive to create a table of contents for OpenSearch operator documentation. It sets the maximum depth to 1 and uses a glob pattern to include all files in the current directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Apache Kylin Documentation Link\nDESCRIPTION: ReStructuredText link to the Apache Kylin project documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kylin/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n`Apache Kylin <https://kylin.apache.org/>`__\n```\n\n----------------------------------------\n\nTITLE: Changelog Version Header in RST\nDESCRIPTION: Version header formatting in RST markup showing version number and latest change date\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n1.0.2\n.....\n\nLatest change: 2022-03-22\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Latest Release\nDESCRIPTION: ReStructuredText formatted changelog entry documenting provider updates including documentation changes and string normalization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pig/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n* ``Add documentation for July 2022 Provider's release (#25030)``\n* ``Enable string normalization in python formatting - providers (#27205)``\n* ``Update docs for September Provider's release (#26731)``\n* ``Apply PEP-563 (Postponed Evaluation of Annotations) to non-core airflow (#26289)``\n* ``Prepare docs for new providers release (August 2022) (#25618)``\n* ``Move provider dependencies to inside provider folders (#24672)``\n* ``Remove 'hook-class-names' from provider.yaml (#24702)``\n```\n\n----------------------------------------\n\nTITLE: Setting Up IDE Configuration Script\nDESCRIPTION: Command to run the setup script that configures PyCharm/IntelliJ IDEA project settings by creating necessary .idea configuration files.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_pycharm.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ uv run setup_idea.py\n```\n\n----------------------------------------\n\nTITLE: Installing Telegram Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Telegram provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-telegram\n```\n\n----------------------------------------\n\nTITLE: Installing Oracle Provider with Common SQL Extra\nDESCRIPTION: Command to install the Oracle provider package with the common.sql extra dependency using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-oracle[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Building CI Docker Image with apache-beam Extra - Docker/Bash\nDESCRIPTION: This bash command builds the Apache Airflow CI Docker image with the 'apache-beam' extra package added via the ADDITIONAL_AIRFLOW_EXTRAS argument. This allows for custom extra dependencies beyond the defaults. Requires Docker, Dockerfile.ci, and present Airflow sources. Outputs a locally tagged image.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build . -f Dockerfile.ci \\\n  --pull \\\n  --build-arg PYTHON_BASE_IMAGE=\"python:3.9-slim-bookworm\" \\\n  --build-arg ADDITIONAL_AIRFLOW_EXTRAS=\"apache-beam\" --tag my-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry Version Link\nDESCRIPTION: Git commit link format used in the changelog to reference specific commits\nSOURCE: https://github.com/apache/airflow/blob/main/providers/atlassian/jira/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n`34500f3a2f <https://github.com/apache/airflow/commit/34500f3a2fa4652272bc831e3c18fd2a6a2da5ef>`__\n```\n\n----------------------------------------\n\nTITLE: Enabling API Codegen Need (Boolean String)\nDESCRIPTION: A boolean flag indicating whether the 'api-codegen' step is required for the build. 'true' means it is needed.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Referencing Git Commit in Markdown\nDESCRIPTION: Shows how to reference a Git commit hash with a link to the full commit on GitHub in a Markdown changelog entry.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n``3b35325840 <https://github.com/apache/airflow/commit/3b35325840e484f86df00e087410f5d5da4b9130>``__  2022-07-06   ``Add test_connection method to 'GoogleBaseHook' (#24682)``\n```\n\n----------------------------------------\n\nTITLE: DocToc Generated Table of Contents\nDESCRIPTION: Automatically generated table of contents section using DocToc, containing a link to the Airflow Error Guide section.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/AIRFLOW_ERROR_GUIDE.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*\n\n- [Airflow Error Guide](#airflow-error-guide)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n```\n\n----------------------------------------\n\nTITLE: Listing Commits for Version 3.0.0 (rst)\nDESCRIPTION: This block lists commits associated with version 3.0.0 using reStructuredText table format. It includes commit hashes linked to GitHub, commit dates, and subject lines detailing changes like documentation preparations for provider rc2 release, synchronizing changelogs after bugfix releases, and documentation updates for the June 2021 provider release.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ====================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ====================================================================\n`bbc627a3da <https://github.com/apache/airflow/commit/bbc627a3dab17ba4cf920dd1a26dbed6f5cebfd1>`__  2021-06-18   ``Prepares documentation for rc2 release of Providers (#16501)``\n`cbf8001d76 <https://github.com/apache/airflow/commit/cbf8001d7630530773f623a786f9eb319783b33c>`__  2021-06-16   ``Synchronizes updated changelog after buggfix release (#16464)``\n`1fba5402bb <https://github.com/apache/airflow/commit/1fba5402bb14b3ffa6429fdc683121935f88472f>`__  2021-06-15   ``More documentation update for June providers release (#16405)``\n`9c94b72d44 <https://github.com/apache/airflow/commit/9c94b72d440b18a9e42123d20d48b951712038f9>`__  2021-06-07   ``Updated documentation for June 2021 provider release (#16294)``\n==================================================================================================  ===========  ====================================================================\n```\n\n----------------------------------------\n\nTITLE: Defining Dataplex Aspect Type Configuration in Python\nDESCRIPTION: This snippet shows a sample Python dictionary configuration for defining a Dataplex Aspect Type. This configuration object is intended to be used with the `DataplexCatalogCreateAspectTypeOperator` or `DataplexCatalogUpdateAspectTypeOperator`. The structure aligns with the `AspectType` resource definition.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 0\n#     :start-after: [START howto_dataplex_aspect_type_configuration]\n#     :end-before: [END howto_dataplex_aspect_type_configuration]\n\n# Example Aspect Type configuration dictionary (structure shown in linked example)\n```\n\n----------------------------------------\n\nTITLE: Listing Commits for Airflow Providers (Aug 2023 Updates)\nDESCRIPTION: This section lists commits related to preparing documentation and enabling configuration contribution for Apache Airflow providers leading up to and including the August 2023 releases. It includes commit hashes linked to GitHub, commit dates, and brief descriptions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n==================================================================================================  ===========  ===================================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ===================================================================================================\n`b5a4d36383 <https://github.com/apache/airflow/commit/b5a4d36383c4143f46e168b8b7a4ba2dc7c54076>`__  2023-08-11   ``Prepare docs for Aug 2023 2nd wave of Providers (#33291)``\n`73b90c48b1 <https://github.com/apache/airflow/commit/73b90c48b1933b49086d34176527947bd727ec85>`__  2023-07-21   ``Allow configuration to be contributed by providers (#32604)``\n`225e3041d2 <https://github.com/apache/airflow/commit/225e3041d269698d0456e09586924c1898d09434>`__  2023-07-06   ``Prepare docs for July 2023 wave of Providers (RC2) (#32381)``\n`3878fe6fab <https://github.com/apache/airflow/commit/3878fe6fab3ccc1461932b456c48996f2763139f>`__  2023-07-05   ``Remove spurious headers for provider changelogs (#32373)``\n`cb4927a018 <https://github.com/apache/airflow/commit/cb4927a01887e2413c45d8d9cb63e74aa994ee74>`__  2023-07-05   ``Prepare docs for July 2023 wave of Providers (#32298)``\n`09d4718d3a <https://github.com/apache/airflow/commit/09d4718d3a46aecf3355d14d3d23022002f4a818>`__  2023-06-27   ``Improve provider documentation and README structure (#32125)``\n==================================================================================================  ===========  ===================================================================================================\n```\n\n----------------------------------------\n\nTITLE: Example Provider Operator Documentation - RST - rst\nDESCRIPTION: Provides an example RST documentation section for a custom Airflow provider Operator. It demonstrates proper use of headings, references, usage explanations, and code inclusion blocks, serving as a template for new provider documentation files. No code execution occurs; this is rendered as part of the documentation build process using Sphinx or similar tools.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/MANAGING_PROVIDERS_LIFECYCLE.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. _howto/operator:NewProviderOperator:\n\nNewProviderOperator\n===================\n\nUse the :class:`~airflow.providers.<PROVIDER>.operators.NewProviderOperator` to do something\namazing with Airflow!\n\nUsing the Operator\n^^^^^^^^^^^^^^^^^^\n\nThe NewProviderOperator requires a ``connection_id`` and this other awesome parameter.\nYou can see an example below:\n\n.. exampleinclude:: /../../<PROVIDER>/example_dags/example_<PROVIDER>.py\n    :language: python\n    :start-after: [START howto_operator_<PROVIDER>]\n    :end-before: [END howto_operator_<PROVIDER>]\n```\n\n----------------------------------------\n\nTITLE: Installing Facebook Social Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Facebook Social integration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[facebook]'\n```\n\n----------------------------------------\n\nTITLE: HTML Table of Contents\nDESCRIPTION: Auto-generated table of contents using DocToc\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0004-using-docker-images-as-test-environment.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*\n\n- [4. Using Docker images as test environment](#4-using-docker-images-as-test-environment)\n  - [Status](#status)\n  - [Context](#context)\n  - [Decision](#decision)\n  - [Consequences](#consequences)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n```\n\n----------------------------------------\n\nTITLE: Python Interpreter Startup Time Results\nDESCRIPTION: Output showing Python interpreter startup time metrics, demonstrating that initialization takes about 0.07s, which should be subtracted from the total DAG loading time for accurate parsing time calculation.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_18\n\nLANGUAGE: text\nCODE:\n```\nreal    0m0.073s\nuser    0m0.037s\nsys     0m0.039s\n```\n\n----------------------------------------\n\nTITLE: Linking to Apache Airflow Documentation in reStructuredText\nDESCRIPTION: This snippet demonstrates how to create hyperlinks in reStructuredText format to redirect users to the new Apache Airflow documentation locations. It includes links for both the latest and stable versions of the documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/docs/rtd-deprecation/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n`s.apache.org/airflow-docs <https://s.apache.org/airflow-docs>`__\n\nFor documentation for stable versions, see: `airflow.apache.org <https://airflow.apache.org/docs/>`__\n```\n\n----------------------------------------\n\nTITLE: Fixing Broken Markdown References in Airflow Providers README\nDESCRIPTION: This commit message details the correction of broken Markdown references within the main README file for Airflow providers. It references pull request #10483.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_77\n\nLANGUAGE: text\nCODE:\n```\nFix broken Markdown refernces in Providers README (#10483)\n```\n\n----------------------------------------\n\nTITLE: HTML Image Element in ReStructuredText\nDESCRIPTION: Embeds an image showing the Airflow fork process using raw HTML with centered alignment and padding.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_codespaces.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n\n     <div align=\"center\" style=\"padding-bottom:10px\">\n       <img src=\"images/airflow_fork.png\"\n            alt=\"Forking Apache Airflow project\">\n     </div>\n```\n\n----------------------------------------\n\nTITLE: HTML License Header Comment\nDESCRIPTION: Apache License 2.0 header comment block for the markdown file, specifying the terms of use and distribution.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0014-fix-root-ownership-after-exiting-docker-command.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Uploading Airflow Image to KinD Cluster\nDESCRIPTION: Command to upload the Airflow Docker image to a KinD (Kubernetes in Docker) cluster for testing purposes. This makes the image available for subsequent deployments within the cluster.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s upload-k8s-image\n```\n\n----------------------------------------\n\nTITLE: Ruff Linting Rule AIR302\nDESCRIPTION: Ruff linting rules to identify and flag deprecated SLA-related parameters in DAGs and operators\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/42285.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nsla in BaseOperator and its subclassses\nsla_miss_callback in DAG\n```\n\n----------------------------------------\n\nTITLE: Defining Airflow Connections Documentation in RST\nDESCRIPTION: RestructuredText directive for generating documentation about Apache Airflow connections. Uses the airflow-connections directive to list community-managed provider connections.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/core-extensions/connections.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. airflow-connections::\n   :tags: None\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Including Security Information in reStructuredText\nDESCRIPTION: This snippet includes an external file containing security information for the Apache Airflow project using reStructuredText syntax.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Limiting Asana Python Client Version in Provider\nDESCRIPTION: Limits the Asana Python client version until the provider is adapted to version 4.* and above.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"Limit Asana Python client until provider is adapted to 4.* version (#32995)\"\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Remove spurious headers for provider changelogs (#32373)\nDESCRIPTION: This commit message, associated with version 3.2.1, details the removal of unnecessary headers from the changelogs for Apache Airflow Providers. Commit hash: 3878fe6fab, Date: 2023-07-05.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n``Remove spurious headers for provider changelogs (#32373)``\n```\n\n----------------------------------------\n\nTITLE: Including External Security Documentation in RST\nDESCRIPTION: This RST directive includes an external file containing security-related documentation for Apache Airflow. The included file is likely to contain important security information and guidelines for the project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Software Integrations RST Documentation Directive\nDESCRIPTION: ReStructuredText directive that generates documentation for software-related operators and hooks in Airflow, using specific tags for categorization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/operators-and-hooks-ref/software.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. operators-hooks-ref::\n   :tags: software\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Version Timestamp in RST Format\nDESCRIPTION: RST-formatted timestamp line indicating the latest change date for a version.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\nLatest change: 2023-10-13\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with UV Package Manager\nDESCRIPTION: Script to install Apache Airflow using UV package manager with version-specific constraints based on Python version.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/start.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_VERSION=3.0.0\n\nPYTHON_VERSION=\"$(python -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")')\"\n\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n\nuv pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare docs 1st wave of Providers December 2023\nDESCRIPTION: This text is a commit message summary for preparing documentation related to the first wave of Apache Airflow provider releases in December 2023, linked to pull request #36112.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_31\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 1st wave of Providers December 2023 (#36112)\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Header\nDESCRIPTION: Apache License 2.0 header documentation in RST format detailing the licensing terms and conditions for the Apache Airflow project.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/04_how_to_contribute.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n```\n\n----------------------------------------\n\nTITLE: Running Provider Tests with UV\nDESCRIPTION: Command to execute pytest for provider testing using UV.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/07_local_virtualenv.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuv run pytest\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-common-messaging with amazon extra (Bash)\nDESCRIPTION: This command installs the `apache-airflow-providers-common-messaging` package along with its optional dependencies required for Amazon provider integration. This enables features that depend on `apache-airflow-providers-amazon`. Installing this extra requires the `apache-airflow-providers-amazon` package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/messaging/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-common-messaging[amazon]\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs for 2nd wave Providers (Jan 2024)\nDESCRIPTION: This commit message (hash 2b4da0101f, dated 2024-01-22) marks the preparation of documentation for the second wave of provider releases in January 2024 (issue #36945).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs 2nd wave of Providers January 2024 (#36945)``\n```\n\n----------------------------------------\n\nTITLE: Adding Templated Field Support to PapermillOperator\nDESCRIPTION: Feature (Version 2.1.0): Implements support for templated fields within the PapermillOperator, allowing dynamic parameterization using Jinja templating, referencing pull request #18357.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_29\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Add support for templated fields in PapermillOperator (#18357)``\n```\n\n----------------------------------------\n\nTITLE: Installing DingTalk Provider with HTTP Dependencies\nDESCRIPTION: Command to install the DingTalk provider package with HTTP provider dependencies via pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-dingding[http]\n```\n\n----------------------------------------\n\nTITLE: Setting up Python Client Repository\nDESCRIPTION: Steps to clone and configure the Apache Airflow Python client repository where the generated code will be placed.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# If you have not done so yet\ngit clone git@github.com:apache/airflow-client-python\ncd airflow-client-python\n# Checkout the right branch\ngit checkout main\nexport CLIENT_REPO_ROOT=$(pwd -P)\ncd ..\n```\n\n----------------------------------------\n\nTITLE: Deleting Dataflow Pipeline with Airflow\nDESCRIPTION: Demonstrates how to use the `DataflowDeletePipelineOperator` in Airflow to delete a Dataflow pipeline definition. This action targets the pipeline itself, not necessarily a running job instance.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_pipeline.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_delete_dataflow_pipeline]\n    :end-before: [END howto_operator_delete_dataflow_pipeline]\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation Extension in Sphinx for Apache Airflow\nDESCRIPTION: This directive includes a security-related documentation extension from a specified path for use in Sphinx documentation. It imports common security documentation elements from a shared location.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/github/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit Information in Markdown Table\nDESCRIPTION: This code snippet shows a markdown table format used to display commit information including the commit hash, date, and subject for version changes in the Apprise provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  =======================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =======================================================================\n`d23881c6489 <https://github.com/apache/airflow/commit/d23881c6489916113921dcedf85077441b44aaf3>`__  2024-08-03   ``Prepare docs for Aug 1st wave of providers (#41230)``\n`67f117060e <https://github.com/apache/airflow/commit/67f117060e3a62556b92f8fbc8e2b7837d0c231c>`__  2024-08-01   ``Fix default behaviour for init function in AppriseNotifier (#41054)``\n==================================================================================================  ===========  =======================================================================\n```\n\n----------------------------------------\n\nTITLE: Creating Kind Kubernetes Cluster\nDESCRIPTION: Commands to create and verify a Kind Kubernetes cluster for Airflow deployment\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/quick-start.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkind create cluster --image kindest/node:v1.21.1\n\nkubectl cluster-info --context kind-kind\n```\n\n----------------------------------------\n\nTITLE: Installing Breeze using UV Package Manager\nDESCRIPTION: Commands to install Breeze development environment tool using the UV package manager\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_gitpod.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install uv\nuv tool install -e ./dev/breeze\n```\n\n----------------------------------------\n\nTITLE: Property Rename in ProvidersManager\nDESCRIPTION: Renaming of Dataset-related properties to Asset in the ProvidersManager class\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41348.significant.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nairflow.providers_manager.ProvidersManager.dataset_factories  airflow.providers_manager.ProvidersManager.asset_factories\nairflow.providers_manager.ProvidersManager.dataset_uri_handlers  airflow.providers_manager.ProvidersManager.asset_uri_handlers\nairflow.providers_manager.ProvidersManager.dataset_to_openlineage_converters  airflow.providers_manager.ProvidersManager.asset_to_openlineage_converters\n```\n\n----------------------------------------\n\nTITLE: Correcting Docs and Tools After Provider RC Release\nDESCRIPTION: Bug Fix (Version 1.0.2): Addresses corrections in documentation and tooling identified after releasing provider release candidates (RCs), referencing pull request #14082.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_44\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Corrections in docs and tools after releasing provider RCs (#14082)``\n```\n\n----------------------------------------\n\nTITLE: Listing Airflow Provider Commits (v3.1.1) using RST\nDESCRIPTION: This reStructuredText snippet displays a table logging commits related to the Apache Airflow provider release wave around version 3.1.1 (December 2022 - January 2023). It lists linked commit hashes, commit dates, and subjects, focusing on documentation preparation, removal of outdated compatibility code, and adding automated version replacement in example DAG indexes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/winrm/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  =====================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =====================================================================\n`5246c009c5 <https://github.com/apache/airflow/commit/5246c009c557b4f6bdf1cd62bf9b89a2da63f630>`__  2023-01-02   ``Prepare docs for Jan 2023 wave of Providers (#28651)``\n`38e40c6dc4 <https://github.com/apache/airflow/commit/38e40c6dc45b92b274a06eafd8790140a0c3c7b8>`__  2022-12-21   ``Remove outdated compat imports/code from providers (#28507)``\n`c8e348dcb0 <https://github.com/apache/airflow/commit/c8e348dcb0bae27e98d68545b59388c9f91fc382>`__  2022-12-05   ``Add automated version replacement in example dag indexes (#28090)``\n==================================================================================================  ===========  =====================================================================\n```\n\n----------------------------------------\n\nTITLE: Failed Test Warning Example in Console\nDESCRIPTION: Example showing how a test fails when an unhandled deprecation warning occurs.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nroot@91e633d08aa8:/opt/airflow# pytest tests/models/test_dag.py::TestDag::test_clear_dag\n...\nFAILED tests/models/test_dag.py::TestDag::test_clear_dag[None-None] - airflow.exceptions.RemovedInAirflow3Warning: Calling `DAG.create_dagrun()` without an explicit data interval is deprecated\n```\n\n----------------------------------------\n\nTITLE: Installing Distributions from Docker Context\nDESCRIPTION: This flag indicates that distributions (wheel packages) should be installed from the Docker context, specifically from the docker-context-files folder.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-context-files/.README.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n`--install-distributions-from-context`\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add pagination to HttpOperator and modularize\nDESCRIPTION: This commit message (hash 70b3bd3fb9, dated 2023-11-03) describes the addition of pagination support to the `HttpOperator` and efforts to make its implementation more modular (issue #34669).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_30\n\nLANGUAGE: text\nCODE:\n```\n``Add pagination to 'HttpOperator' and make it more modular (#34669)``\n```\n\n----------------------------------------\n\nTITLE: Listing Removed Methods from Airflow DAG Model\nDESCRIPTION: This snippet lists the methods that were removed from the `airflow/models/dag.py` file. These methods were determined to be unused and their removal helps simplify the codebase.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41440.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* Methods removed\n\n  * ``date_range``\n  * ``is_fixed_time_schedule``\n  * ``next_dagrun_after_date``\n  * ``get_run_dates``\n  * ``normalize_schedule``\n  * ``full_filepath``\n  * ``concurrency``\n  * ``filepath``\n  * ``concurrency_reached``\n  * ``normalized_schedule_interval``\n  * ``latest_execution_date``\n  * ``set_dag_runs_state``\n  * ``bulk_sync_to_db``\n```\n\n----------------------------------------\n\nTITLE: Importing Custom Airflow Operators Package in Python\nDESCRIPTION: Demonstrates how to import a custom Airflow operators package after setting the PYTHONPATH.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/modules_management.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import airflow_operators\nHello from airflow_operators\n>>>\n```\n\n----------------------------------------\n\nTITLE: Adding Legacy Namespace Packages to airflow.providers\nDESCRIPTION: Commit message describing the addition of legacy namespace packages under the 'airflow.providers' structure, likely for backward compatibility. References pull request #47064.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_19\n\nLANGUAGE: plaintext\nCODE:\n```\nAdd legacy namespace packages to airflow.providers (#47064)\n```\n\n----------------------------------------\n\nTITLE: Defining Discord Provider Package Name in RST\nDESCRIPTION: Defines the name of the Apache Airflow Discord provider package using RST syntax.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/discord/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n``apache-airflow-providers-discord``\n```\n\n----------------------------------------\n\nTITLE: Package Name Definition in RST\nDESCRIPTION: Definition of the Slack provider package name in RST format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n``apache-airflow-providers-slack``\n```\n\n----------------------------------------\n\nTITLE: Including RST Documentation for Provider Installation\nDESCRIPTION: RST directive to include external documentation about installing Airflow providers from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Specifying Public Runner Labels (JSON Array)\nDESCRIPTION: Defines the list of labels assigned to select public runners (e.g., GitHub-hosted runners). This example specifies 'ubuntu-22.04'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_30\n\nLANGUAGE: json\nCODE:\n```\n\\[\"ubuntu-22.04\"\\]\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Block List in Airflow\nDESCRIPTION: Configuration for specifying a block list of metric prefixes in Airflow. This prevents metrics collection for those that start with the elements in the list.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/metrics.rst#2025-04-22_snippet_7\n\nLANGUAGE: ini\nCODE:\n```\n[metrics]\nmetrics_block_list = scheduler,executor,dagrun,pool,triggerer,celery\n```\n\n----------------------------------------\n\nTITLE: Setting Docker Compose Project Name for Manual Testing\nDESCRIPTION: Command to set the COMPOSE_PROJECT_NAME environment variable for manual Docker Compose testing to avoid conflicts with other deployments.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/docker_compose_tests.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport COMPOSE_PROJECT_NAME=quick-start\n```\n\n----------------------------------------\n\nTITLE: Specifying Migration Rules for Apache Airflow Configuration\nDESCRIPTION: Details the migration rules needed for the configuration changes, including the removal of a specific config parameter and updates to validator classes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41975.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n* Migration rules needed\n\n  * ``airflow config liint``\n\n    * [x] ``metrics.metrics_use_pattern_match``\n\n  * ruff\n\n    * AIR302\n\n      * [x] ``airflow.metrics.validators.AllowListValidator``  ``airflow.metrics.validators.PatternAllowListValidator``\n      * [x] ``airflow.metrics.validators.BlockListValidator``  ``airflow.metrics.validators.PatternBlockListValidator``\n```\n\n----------------------------------------\n\nTITLE: HTML Table of Contents Comment\nDESCRIPTION: Auto-generated table of contents markup using DocToc tool with instructions to not edit manually\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0001-record-architecture-decisions.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n```\n\n----------------------------------------\n\nTITLE: Argo CD Database Migration Configuration\nDESCRIPTION: YAML configuration for enabling automatic database migrations with Argo CD sync events.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/index.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmigrateDatabaseJob:\n    jobAnnotations:\n        \"argocd.argoproj.io/hook\": Sync\n```\n\n----------------------------------------\n\nTITLE: Formatting Version Headers in Markdown\nDESCRIPTION: This snippet demonstrates how version headers are formatted in the changelog, including the version number and a horizontal line separator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: Markdown\nCODE:\n```\n3.4.0\n.....\n\nLatest change: 2023-12-08\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 3.0.2\nDESCRIPTION: Changelog entry documenting added support for Python 3.10\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n3.0.2\n.....\n\nMisc\n~~~~\n\n* ``Support for Python 3.10``\n```\n\n----------------------------------------\n\nTITLE: Apache License Header\nDESCRIPTION: Standard Apache 2.0 license header text included at the top of the file\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n```\n\n----------------------------------------\n\nTITLE: Misc Section Header in RST\nDESCRIPTION: Section header for miscellaneous changes using RST syntax\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/flink/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\nMisc\n~~~~\n```\n\n----------------------------------------\n\nTITLE: Documenting Breaking Changes in reStructuredText\nDESCRIPTION: This snippet demonstrates how to document breaking changes for a specific version (2.0.0) using reStructuredText format. It includes a note about provider availability and Airflow version compatibility.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: reStructuredText\nCODE:\n```\n2.0.0\n.....\n\nBreaking changes\n~~~~~~~~~~~~~~~~\n\n.. note::\n  This release of provider is only available for Airflow 2.2+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n```\n\n----------------------------------------\n\nTITLE: Clearing NPM Proxy Settings\nDESCRIPTION: Removes proxy configurations from NPM to resolve connection timeout issues.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/04_troubleshooting.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm config delete http-proxy\nnpm config delete https-proxy\n\nnpm config rm proxy\nnpm config rm https-proxy\n\nset HTTP_PROXY=null\nset HTTPS_PROXY=null\n```\n\n----------------------------------------\n\nTITLE: Configuring No Log Persistence in Airflow\nDESCRIPTION: Helm command to configure Airflow with local pod logging without persistence. Logs are only available during pod lifetime.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-logs.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airflow apache-airflow/airflow \\\n  --set logs.persistence.enabled=false\n  # --set workers.persistence.enabled=false (also needed if using ``CeleryExecutor``)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Limit Telegram Provider Version\nDESCRIPTION: This commit message, linked to commit 60f24dab8a dated 2023-01-02, notes a change to limit the compatible version of the Telegram library to less than 20.0.0 for the Telegram Provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_20\n\nLANGUAGE: text\nCODE:\n```\n``Limit Telegram to < 20.0.0 (#28671)``\n```\n\n----------------------------------------\n\nTITLE: Defining Apache License 2.0 Header in HTML Comments\nDESCRIPTION: This snippet contains the standard Apache License 2.0 header enclosed in HTML comments. It specifies the terms under which the software is licensed, including distribution and modification rights.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/triggers/README.md#2025-04-22_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Running Breeze Tests with UV\nDESCRIPTION: Command to execute Breeze tests using pytest within the UV virtual environment.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nuv run pytest\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Documentation in RST\nDESCRIPTION: ReStructuredText directive to include external documentation about installing Airflow providers from source code. Points to a common development documentation file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ydb/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Prepare Documentation for Nov 2023 2nd Wave (Excluded from v1.1.0 Changelog)\nDESCRIPTION: Indicates preparatory work on documentation for the second wave of provider releases in November 2023, referenced by pull request #35836. This change was intentionally excluded from the main changelog notes for version 1.1.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_31\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 2nd wave of Providers November 2023 (#35836)\n```\n\n----------------------------------------\n\nTITLE: Configuring ReStructuredText Table of Contents\nDESCRIPTION: RST directive that defines the table of contents structure for the how-to guides documentation, listing all available guide topics with a maximum depth of 2 levels.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 2\n\n    Using the CLI <usage-cli>\n    add-dag-tags\n    notifications\n    set-config\n    set-up-database\n    operator/index\n    timetable\n    custom-view-plugin\n    listener-plugin\n    customize-ui\n    custom-operator\n    create-custom-decorator\n    export-more-env-vars\n    connection\n    variable\n    setup-and-teardown\n    run-behind-proxy\n    run-with-systemd\n    define-extra-link\n    email-config\n    dynamic-dag-generation\n    docker-compose/index\n```\n\n----------------------------------------\n\nTITLE: Rendering Commit Table Header in Markdown\nDESCRIPTION: This snippet shows how the commit table header is formatted in the changelog using Markdown syntax. It includes column names and separator lines.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_39\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  ===============================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ===============================================================================================\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Move Provider Dependencies\nDESCRIPTION: This commit message, linked to commit 0de31bd73a dated 2022-06-29, describes the relocation of provider-specific dependencies to reside within their respective provider folders.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_30\n\nLANGUAGE: text\nCODE:\n```\n``Move provider dependencies to inside provider folders (#24672)``\n```\n\n----------------------------------------\n\nTITLE: Setting Oracle Current Schema Automatically in Hook in RST\nDESCRIPTION: This commit (471e368eac, committed on 2022-02-07) modifies the Oracle Hook to automatically set the 'current_schema' when it is defined in the Airflow Connection details. Refers to issue #19084.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n[Oracle] Oracle Hook - automatically set current_schema when defined in Connection (#19084)\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Guide in RST Documentation\nDESCRIPTION: Sphinx documentation directive to include external RST file containing instructions for installing Airflow providers from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Managing Provider Tags\nDESCRIPTION: Command to manage git tags for providers during releases with options to clean local tags.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management tag-providers\n```\n\n----------------------------------------\n\nTITLE: Adding support for Google Cloud Data Pipelines Run Operator in Python\nDESCRIPTION: Implements support for Google Cloud's Data Pipelines Run Operator in the Google provider for Apache Airflow. This allows managing and executing data pipeline runs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"Adding Support for Google Cloud's Data Pipelines Run Operator (#32846)\"\n```\n\n----------------------------------------\n\nTITLE: Importing Zendesk Provider Package\nDESCRIPTION: Package identifier for the Apache Airflow Zendesk provider\nSOURCE: https://github.com/apache/airflow/blob/main/providers/zendesk/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"apache-airflow-providers-zendesk\"\n```\n\n----------------------------------------\n\nTITLE: GCS ACL Permission Field Update - Before Example\nDESCRIPTION: Example showing the old CamelCase field naming convention used before the breaking change when setting GCS bucket ACL permissions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nset_acl_permission = GCSBucketCreateAclEntryOperator(\n    task_id=\"gcs-set-acl-permission\",\n    bucket=BUCKET_NAME,\n    entity=\"user-{{ task_instance.xcom_pull('get-instance')['persistenceIamIdentity'].split(':', 2)[1] }}\",\n    role=\"OWNER\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Kafka Provider Package for Airflow\nDESCRIPTION: This command installs the Apache Kafka provider package on top of an existing Airflow 2 installation using pip. It requires a minimum Airflow version as specified in the requirements section.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-kafka\n```\n\n----------------------------------------\n\nTITLE: HTML Table of Contents Comment\nDESCRIPTION: Auto-generated table of contents using DocToc, including section links for the ADR document\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0010-use-pipx-to-install-breeze.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*\n\n- [10. Use pipx to install breeze](#10-use-pipx-to-install-breeze)\n  - [Status](#status)\n  - [Context](#context)\n  - [Decision](#decision)\n  - [Consequences](#consequences)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n```\n\n----------------------------------------\n\nTITLE: Apache License Header in HTML Comment\nDESCRIPTION: Standard Apache License 2.0 header embedded as an HTML comment. This header is required for all source files in Apache Software Foundation projects to establish copyright and licensing terms.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/neo4j/src/airflow/providers/neo4j/README.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: ReStructuredText directive to include security documentation from a common development source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/zendesk/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Additional Documentation Updates for June Provider Release\nDESCRIPTION: Excluded Change (Version 2.0.0): Contains further documentation updates for the June 2021 provider release, referencing pull request #16405.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_42\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``More documentation update for June providers release (#16405)``\n```\n\n----------------------------------------\n\nTITLE: Displaying Git Commit Information in Markdown\nDESCRIPTION: This code snippet shows how git commit hashes, dates, and descriptions are formatted in Markdown to display release information.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_12\n\nLANGUAGE: markdown\nCODE:\n```\n`9d9b15989a <https://github.com/apache/airflow/commit/9d9b15989a02042a9041ff86bc7e304bb06caa15>`__  2022-12-14   ``Create 'LambdaCreateFunctionOperator' and sensor (#28241)``\n```\n\n----------------------------------------\n\nTITLE: Adding CloudRunHook and operators in Python\nDESCRIPTION: Adds CloudRunHook and related operators to the Google provider for Apache Airflow. This allows interaction with Google Cloud Run services.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"Add CloudRunHook and operators (#33067)\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authentication Backend in Airflow FAB (INI)\nDESCRIPTION: Enables Basic Authentication (username/password) for the FAB API by setting the `auth_backends` key in the `[fab]` section of `airflow.cfg`. This allows users created via LDAP or the Airflow CLI to authenticate using their credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/api-authentication.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[fab]\nauth_backends = airflow.providers.fab.auth_manager.api.auth.backend.basic_auth\n```\n\n----------------------------------------\n\nTITLE: Defining Partial Keys for Google Cloud Datastore in Python\nDESCRIPTION: This snippet shows the definition of partial keys required by the CloudDatastoreAllocateIdsOperator. It creates a list of partial keys with specified path elements.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nKEYS = [\n    {\n        \"partitionId\": {\"projectId\": GCP_PROJECT_ID},\n        \"path\": [{\"kind\": \"airflow-system-test-entities\"}],\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Template Fields for CloudVisionCreateProductOperator\nDESCRIPTION: Lists the template fields available for the CloudVisionCreateProductOperator for dynamic field resolution at runtime.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"location\",\n    \"project_id\",\n    \"product_id\",\n    \"gcp_conn_id\",\n    \"impersonation_chain\",\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting an Operation in Google Cloud Datastore using Python\nDESCRIPTION: This example shows how to use CloudDatastoreDeleteOperationOperator to delete an operation in Google Cloud Datastore. It specifies the name of the operation to be deleted.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datastore.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndelete_operation = CloudDatastoreDeleteOperationOperator(\n    task_id=\"delete_operation\",\n    name=\"{{ task_instance.xcom_pull('export_task')['name'] }}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Fixing EmrServerlessStartJobOperator Configuration in Python\nDESCRIPTION: Fixes the 'configuration_overrides' parameter in the EmrServerlessStartJobOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"Fix 'configuration_overrides' parameter in 'EmrServerlessStartJobOperator' (#35787)\"\n```\n\n----------------------------------------\n\nTITLE: Modifying Table Character Sets\nDESCRIPTION: SQL commands to modify table field character sets to utf8mb3 with proper collation.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading.rst#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE task_instance MODIFY task_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\nALTER TABLE task_reschedule MODIFY task_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\n\nALTER TABLE rendered_task_instance_fields MODIFY task_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\nALTER TABLE rendered_task_instance_fields MODIFY dag_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\n\nALTER TABLE task_fail MODIFY task_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\nALTER TABLE task_fail MODIFY dag_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\n\nALTER TABLE sla_miss MODIFY task_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\nALTER TABLE sla_miss MODIFY dag_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\n\nALTER TABLE task_map MODIFY task_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\nALTER TABLE task_map MODIFY dag_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\nALTER TABLE task_map MODIFY run_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\n\nALTER TABLE xcom MODIFY task_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\nALTER TABLE xcom MODIFY dag_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\nALTER TABLE xcom MODIFY run_id VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\nALTER TABLE xcom MODIFY key VARCHAR(250) CHARACTER SET utf8mb3 COLLATE utf8mb3_bin;\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Guide in RST Documentation\nDESCRIPTION: Sphinx documentation directive to include external RST file containing instructions for installing providers from sources\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Running Provider Tests in Parallel\nDESCRIPTION: Command for running specific provider tests in parallel using Breeze with test type filtering.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-tests --run-in-parallel --skip-db-tests --backend none --parallel-test-types \"Providers[google] Providers[amazon]\"\n```\n\n----------------------------------------\n\nTITLE: Creating QuickSight SPICE Ingestion with Airflow\nDESCRIPTION: Example showing how to use QuickSightCreateIngestionOperator to create and start a new SPICE ingestion for a QuickSight dataset. The operator initiates the data refresh process for existing SPICE datasets.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/quicksight.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nquicksight_create_ingestion = QuickSightCreateIngestionOperator(\n    task_id=\"create_quicksight_ingestion\",\n    aws_conn_id=\"aws_default\",\n    data_set_id=\"{{ data.PlaceholderDatasetId }}\",\n    ingestion_id=\"{{ data.PlaceholderId }}\",\n    wait_for_completion=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare 2nd wave of providers (Dec)\nDESCRIPTION: This commit message (hash f5883d6e7b, dated 2023-12-23) signifies the preparation steps for releasing the second wave of providers in December (issue #36373).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n``Prepare 2nd wave of providers in December (#36373)``\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Documentation in RST\nDESCRIPTION: Sphinx documentation include directive that references external documentation about installing providers from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mongo/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Instructions in RST Documentation\nDESCRIPTION: RST include directive that references common provider installation documentation from a shared development source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sendgrid/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Testing with Local Providers and Downloaded Airflow\nDESCRIPTION: Example of testing with locally built providers and downloaded Airflow distribution\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/testing_packages.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nrm dist/*\nbreeze release-management prepare-provider-distributions celery cncf.kubernetes\npip download apache-airflow==2.9.0 --dest dist --no-deps\nbreeze start-airflow --mount-sources remove --use-distributions-from-dist --use-airflow-version sdist\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents for Asana Operators in ReStructuredText\nDESCRIPTION: This snippet uses a toctree directive to create a table of contents for Asana operators documentation. It's set to maximum depth of 1 and uses a glob pattern to include all files in the current directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Deleting Apache Kafka Cluster with ManagedKafkaDeleteClusterOperator in Python\nDESCRIPTION: This code shows how to delete an Apache Kafka cluster using the ManagedKafkaDeleteClusterOperator. It requires the project ID, region, and cluster name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndelete_cluster = ManagedKafkaDeleteClusterOperator(\n    task_id=\"delete_cluster\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n    retry=Retry(\n        maximum=10.0,\n        total=300.0,\n        initial=3.0,\n        multiplier=2.0,\n    ),\n    timeout=300,\n)\n```\n\n----------------------------------------\n\nTITLE: Including External RST Content\nDESCRIPTION: This reStructuredText directive includes the content of the specified file (`/../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst`) into the current document. This is commonly used in Sphinx documentation to reuse content, in this case, the instructions for installing Airflow providers from their source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Bump Minimum Airflow Version to 2.7.0 (v1.2.0)\nDESCRIPTION: Indicates that the minimum required Apache Airflow version for this provider release (v1.2.0) was raised to 2.7.0, referenced by pull request #39240.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\nBump minimum Airflow version in providers to Airflow 2.7.0 (#39240)\n```\n\n----------------------------------------\n\nTITLE: Defining Apache Airflow Code of Conduct in Markdown\nDESCRIPTION: This Markdown snippet defines the code of conduct for the Apache Airflow project. It references the Apache Software Foundation's code of conduct and provides links to the conduct policy and reporting guidelines.\nSOURCE: https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# Contributor Covenant Code of Conduct\n\nThe Apache Airflow project follows the [Apache Software Foundation code of conduct](https://www.apache.org/foundation/policies/conduct.html).\n\nIf you observe behavior that violates those rules please follow the [ASF reporting guidelines](https://www.apache.org/foundation/policies/conduct#reporting-guidelines).\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add explanatory note for contributors about updating Changelog (#24229)\nDESCRIPTION: This commit message, associated with version 3.0.0, adds a note to guide contributors on the correct procedure for updating the Changelog file. Commit hash: 027b707d21, Date: 2022-06-05.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_32\n\nLANGUAGE: text\nCODE:\n```\n``Add explanatory note for contributors about updating Changelog (#24229)``\n```\n\n----------------------------------------\n\nTITLE: Version Header - RestructuredText Format\nDESCRIPTION: Version heading formatting in RST showing version number with underline\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n7.3.0\n.....\n```\n\n----------------------------------------\n\nTITLE: Licensing and Autogenerated File Notice - reStructuredText\nDESCRIPTION: This snippet provides mandated license, copyright, and ownership information, referencing the Apache License, Version 2.0. It additionally warns users that the file is autogenerated from a Jinja2 template, directing edits to the template source for persistence. Meant as a preface to the documentation file, it sets legal context and modification constraints, and must be kept intact at the beginning of the file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/zendesk/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n\n.. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!\n\n.. IF YOU WANT TO MODIFY THIS FILE, YOU SHOULD MODIFY THE TEMPLATE\n   `PROVIDER_COMMITS_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY\n\n.. THE REMAINDER OF THE FILE IS AUTOMATICALLY GENERATED. IT WILL BE OVERWRITTEN!\n\n```\n\n----------------------------------------\n\nTITLE: Installing Teradata Extras for Apache Airflow\nDESCRIPTION: Command to install Teradata hooks and operators for Apache Airflow. This enables integration with Teradata database systems.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_52\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[teradata]'\n```\n\n----------------------------------------\n\nTITLE: Git Commit Hash Reference\nDESCRIPTION: A 40-character SHA-1 hash representing a specific Git commit in the Apache Airflow repository.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/discord/docs/.latest-doc-only-change.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nd4473555c0e7022e073489b7163d49102881a1a6\n```\n\n----------------------------------------\n\nTITLE: Templating Image Safe Search Detection Field - Google Cloud Vision Airflow Operator - Python\nDESCRIPTION: This snippet lists template fields for CloudVisionDetectImageSafeSearchOperator, enabling dynamic injection of project ID, location, image, and retry parameters in Airflow-driven GCP image analyses. It is essential for scaling the operator across workloads where contextual data changes between DAG executions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"project_id\",\n    \"location\",\n    \"image\",\n    \"retry\",\n)\n```\n\n----------------------------------------\n\nTITLE: Including External RST File in Sphinx Documentation\nDESCRIPTION: This reStructuredText directive includes the content of the specified file (`installing-providers-from-sources.rst`) into the current document during the Sphinx build process. It requires the target file to exist at the relative path specified. This mechanism is used to reuse common documentation content, specifically instructions for installing Airflow providers from source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pagerduty/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Updated Changelog After Bugfix Release\nDESCRIPTION: Excluded Change (Version 2.0.0): Synchronizes the changelog file with updates following a bugfix release, referencing pull request #16464.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_43\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Synchronizes updated changelog after buggfix release (#16464)``\n```\n\n----------------------------------------\n\nTITLE: Updating BigQuery CreateEmptyTableOperator in Python\nDESCRIPTION: Adds table_resource to template fields for BigQueryCreateEmptyTableOperator, allowing more flexibility in table creation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"Add table_resource to template fields for BigQueryCreateEmptyTableOperator (#28235)\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for Apache Spark Connections in reStructuredText\nDESCRIPTION: This snippet sets up a table of contents (toctree) for Apache Spark connection documentation. It specifies a maximum depth of 1 and uses a glob pattern to include all files in the current directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/connections/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents for Microsoft Azure Sensors in reStructuredText\nDESCRIPTION: This code snippet uses the toctree directive to create a table of contents for Microsoft Azure sensor documentation. It is set to a maximum depth of 1 and uses a glob pattern to include all relevant files.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/sensors/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Docker Cleanup Command\nDESCRIPTION: Command to stop and remove Docker containers, volumes, and images\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down --volumes --rmi all\n```\n\n----------------------------------------\n\nTITLE: Improved Test Collection with Conditional Mocking in Python\nDESCRIPTION: This code snippet demonstrates how to conditionally mock Airflow Variables to avoid database access during test collection when running without database tests.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models.variable import Variable\n\npytestmark = pytest.mark.db_test\n\n\nif os.environ.get(\"_AIRFLOW_SKIP_DB_TESTS\") == \"true\":\n    # Handle collection of the test by non-db case\n    Variable = mock.MagicMock()  # type: ignore[misc] # noqa: F811\nelse:\n    initial_db_init()\n\n\n@pytest.mark.parametrize(\n    \"env, expected\",\n    [\n        pytest.param(\n            {\"plain_key\": \"plain_value\"},\n            \"{'plain_key': 'plain_value'}\",\n            id=\"env-plain-key-val\",\n        ),\n        pytest.param(\n            {\"plain_key\": Variable.setdefault(\"plain_var\", \"banana\")},\n            \"{'plain_key': 'banana'}\",\n            id=\"env-plain-key-plain-var\",\n        ),\n        pytest.param(\n            {\"plain_key\": Variable.setdefault(\"secret_var\", \"monkey\")},\n            \"{'plain_key': '***'}\",\n            id=\"env-plain-key-sensitive-var\",\n        ),\n        pytest.param(\n            {\"plain_key\": \"{{ var.value.plain_var }}\"},\n            \"{'plain_key': '{{ var.value.plain_var }}'}\",\n            id=\"env-plain-key-plain-tpld-var\",\n        ),\n    ],\n)\ndef test_rendered_task_detail_env_secret(patch_app, admin_client, request, env, expected): ...\n```\n\n----------------------------------------\n\nTITLE: Creating Timezone Aware DAG\nDESCRIPTION: Example of creating a DAG with timezone awareness using pendulum for the start date specification.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timezone.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pendulum\n\ndag = DAG(\"my_tz_dag\", start_date=pendulum.datetime(2016, 1, 1, tz=\"Europe/Amsterdam\"))\nop = EmptyOperator(task_id=\"empty\", dag=dag)\nprint(dag.timezone)  # <Timezone [Europe/Amsterdam]>\n```\n\n----------------------------------------\n\nTITLE: Installing Teradata Provider with Optional Dependencies (pip, Bash)\nDESCRIPTION: This bash command demonstrates how to install the Apache Airflow Teradata provider package along with its optional 'amazon' extra dependencies via pip. It requires Python 3.9 or newer and a working pip installation. Replace 'amazon' with any valid extra (e.g., 'microsoft.azure') as needed. Expected input is the pip command in a shell; successful execution installs the core Teradata provider and any specified cross-provider dependencies. Ensure Airflow is already installed and meets the minimum version requirements.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-teradata[amazon]\n```\n\n----------------------------------------\n\nTITLE: HTML License Header Comment\nDESCRIPTION: Apache License 2.0 header comment for the document\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0007-using-database-volumes-for-backends.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Entering Breeze Shell with Distributions from Dist and Clean Installation (Bash)\nDESCRIPTION: These bash commands launch the Breeze testing shell using provider distributions from 'dist', specifying wheel format, targeted Airflow version, and full installation/skip constraint flags for compatibility tests. The variant with '--clean-airflow-installation' is used to emulate a CI canary run, which first removes all existing Python packages before installing the specified Airflow and providers. This assures a clean, reproducible test environment.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\nbreeze shell --use-distributions-from-dist --distribution-format wheel --use-airflow-version 2.9.1  \\\n --install-airflow-with-constraints --providers-skip-constraints --mount-sources tests\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze shell --use-distributions-from-dist --distribution-format wheel --use-airflow-version 2.9.1  \\\n --install-airflow-with-constraints --providers-skip-constraints --mount-sources tests --clean-airflow-installation\n```\n\n----------------------------------------\n\nTITLE: Using '__version__' instead of 'version' in Providers\nDESCRIPTION: Excluded Change (Version > 3.1.1): Standardizes the use of '__version__' attribute instead of 'version' within providers, referencing pull request #31393.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Use '__version__' in providers not 'version' (#31393)``\n```\n\n----------------------------------------\n\nTITLE: Deprecated Python Version Check in Airflow\nDESCRIPTION: This snippet demonstrates the old way of checking Python versions using deprecated constants from the airflow module. This approach is no longer supported.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43562.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import PY36\n\nif PY36:\n    # perform some action\n    ...\n```\n\n----------------------------------------\n\nTITLE: Displaying Version History in RST\nDESCRIPTION: RST code blocks showing the commit history for different versions of the package, including commit hashes, dates, and subjects.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n10.4.3\n......\n\nLatest change: 2025-04-16\n\n==================================================================================================  ===========  ===================================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ===================================================================================================\n`694bdc6c43 <https://github.com/apache/airflow/commit/694bdc6c43f926a20dd765e5a20a238884325c29>`__  2025-04-16   ``Remove 'subdir' arg from CLI commands (#49317)``\n`47bd8961a9 <https://github.com/apache/airflow/commit/47bd8961a9ce4e2cea0dbabd400d2508eb291948>`__  2025-04-15   ``Use contextlib.suppress(exception) instead of try-except-pass and add SIM105 ruff rule (#49251)``\n`cb295c351a <https://github.com/apache/airflow/commit/cb295c351a016c0a10cab07f2a628b865cff3ca3>`__  2025-04-14   ``remove superfluous else block (#49199)``\n`0ca0f17996 <https://github.com/apache/airflow/commit/0ca0f17996c86efb292cf5b10181944c67e3b862>`__  2025-04-13   ``Remove unused db method in k8s provider (#49186)``\n==================================================================================================  ===========  ===================================================================================================\n```\n\n----------------------------------------\n\nTITLE: Starting Porting Mapped Task to SDK\nDESCRIPTION: Commit message indicating the beginning of the effort to port mapped task functionality to the Task SDK. References pull request #45627.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_41\n\nLANGUAGE: plaintext\nCODE:\n```\nStart porting mapped task to SDK (#45627)\n```\n\n----------------------------------------\n\nTITLE: Utility Methods Migration in Python\nDESCRIPTION: Migration paths for sensitive variable handling utility methods to new secrets masker module\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41758.significant.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.utils.log.secrets_masker import get_sensitive_variables_fields, should_hide_value_for_key  # New way\n# Replace methods from airflow.www.utils\n```\n\n----------------------------------------\n\nTITLE: Alembic Logger Configuration\nDESCRIPTION: Python code snippet showing how to configure Alembic logging to prevent disabling existing loggers when using a config file.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/14_metadata_database_updates.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name, disable_existing_loggers=False)\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in Apache Airflow\nDESCRIPTION: This reStructuredText directive includes an external file containing security information for the Apache Airflow project. The included file is located in a development-common directory, suggesting it's shared across multiple parts of the project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Preparing 2nd Wave of Providers in December (Commit Message)\nDESCRIPTION: Commit message indicating the preparation steps for the second wave of Apache Airflow provider releases in December, relevant to the Airbyte provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_28\n\nLANGUAGE: text\nCODE:\n```\nPrepare 2nd wave of providers in December (#36373)\n```\n\n----------------------------------------\n\nTITLE: Fixing try002 Linting Error for Airbyte Provider (Commit Message)\nDESCRIPTION: Commit message addressing a 'try002' linting error specifically within the Apache Airflow Airbyte provider code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_15\n\nLANGUAGE: text\nCODE:\n```\nfix: try002 for provider airbyte (#38786)\n```\n\n----------------------------------------\n\nTITLE: Including External Documentation File in Sphinx (rST)\nDESCRIPTION: This reStructuredText directive is used within Sphinx documentation to embed the content of another file. It includes the `installing-providers-from-sources.rst` file, located at a relative path, into the current document during the build process. This is typically used for modularity and content reuse.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: HTML License Header Comment\nDESCRIPTION: Apache License 2.0 header comment that specifies the licensing terms for the file\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0001-record-architecture-decisions.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Apache License Header in RST\nDESCRIPTION: Standard Apache License 2.0 header in RST documentation format, specifying the terms under which the software is licensed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/core-extensions/notifications.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n```\n\n----------------------------------------\n\nTITLE: Importing PGP Keys for Apache Airflow Package Verification\nDESCRIPTION: Commands to import PGP keys for verifying Apache Airflow provider packages. Shows three alternative methods using gpg, pgpk, or pgp.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/installing-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngpg -i KEYS\n```\n\nLANGUAGE: bash\nCODE:\n```\npgpk -a KEYS\n```\n\nLANGUAGE: bash\nCODE:\n```\npgp -ka KEYS\n```\n\n----------------------------------------\n\nTITLE: Including Sphinx Documentation for Installing Airflow Providers from Sources\nDESCRIPTION: This RST directive includes external documentation that explains the process of installing Apache Airflow providers from source code. The included file is likely to contain detailed instructions and prerequisites for this task.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Migrating DEFAULT_CELERY_CONFIG Import Path\nDESCRIPTION: Migration path for the DEFAULT_CELERY_CONFIG import statement from the deprecated location in config templates to the new location in the Celery provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/45017.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nairflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG  airflow.providers.celery.executors.default_celery.DEFAULT_CELERY_CONFIG\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST File\nDESCRIPTION: This snippet includes a shared security documentation file from a common source directory structure using Sphinx's include directive.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add documentation for July 2022 Provider's release (#25030)\nDESCRIPTION: This commit message, associated with version 3.1.0, adds documentation corresponding to the July 2022 release of Apache Airflow Providers. Commit hash: d2459a241b, Date: 2022-07-13.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_26\n\nLANGUAGE: text\nCODE:\n```\n``Add documentation for July 2022 Provider's release (#25030)``\n```\n\n----------------------------------------\n\nTITLE: Excluding Database Versions (Empty JSON Array)\nDESCRIPTION: Specifies which versions of MySQL, Postgres, or SQLite should be excluded from testing. An empty JSON array indicates no versions are excluded by default.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n[]\n```\n\n----------------------------------------\n\nTITLE: Removing Python 3.7 support\nDESCRIPTION: Removes support for Python 3.7 in the Databricks provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n\"Remove Python 3.7 support (#30963)\"\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: RST directive to include external security documentation from a common development source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Defining RST Documentation Header with License\nDESCRIPTION: RST documentation header containing Apache 2.0 license information and description of extra links functionality in Airflow providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/core-extensions/extra-links.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\nExtra Links\n-----------\n\nThis is a summary of all Apache Airflow Community provided implementations of operator extra links\nexposed via community-managed providers.\n\nAirflow can be extended by providers with custom operator extra links. For each operator, you can define\nits own extra links that can redirect users to external systems. The extra link buttons\nwill be available on the task page.\n\nThe operator extra links are explained in\n:doc:`apache-airflow:howto/define-extra-link` and here you can also see the extra links\nprovided by the community-managed providers:\n\n.. airflow-extra-links::\n   :tags: None\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Package Requirements Table in RST\nDESCRIPTION: RST formatted table showing the required pip packages and their minimum versions for the Singularity provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/singularity/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n\"apache-airflow\"  \">=2.9.0\"\n\"spython\"         \">=0.0.56\"\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Code Statistics Analysis for Apache Airflow\nDESCRIPTION: Breakdown of lines of code by directory and programming language across the Apache Airflow codebase, showing Python dominance at 92.65% and shell scripts at 3.51%.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0002-implement-standalone-python-command.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nSLOC    Directory         SLOC-by-Language (Sorted)\n\n144905  tests             python=144761,xml=132,sh=12\n130115  airflow           python=127249,javascript=2827,sh=39\n12052   docs              javascript=8977,python=2931,sh=144\n9073    scripts           sh=7457,python=1616\n6314    chart             python=6218,sh=96\n3665    top_dir           sh=2896,python=769\n3102    dev               python=2938,sh=164\n1723    kubernetes-tests  python=1723\n280     docker-tests      python=280\n140     metastore_browser python=140\n109     clients           sh=109\n28      images              sh=28\n\nTotals grouped by language (dominant language first):\npython:      288625 (92.65%)\njavascript:     11804 (3.79%)\nsh:           10945 (3.51%)\nxml:            132 (0.04%)\n```\n\n----------------------------------------\n\nTITLE: Making Basic Authenticated API Request using cURL (Bash)\nDESCRIPTION: Provides a sample cURL command to make an authenticated request to an Airflow API endpoint (`/api/v1/pools`) using Basic Authentication. The `--user` flag simplifies providing the username and password, which cURL then encodes and includes in the `Authorization` header.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/api-authentication.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nENDPOINT_URL=\"http://localhost:8080/\"\ncurl -X GET  \\\n    --user \"username:password\" \\\n    \"${ENDPOINT_URL}/api/v1/pools\"\n```\n\n----------------------------------------\n\nTITLE: Pre-commit Hook Configuration Table\nDESCRIPTION: A formatted table showing all pre-commit hooks and their descriptions used in Apache Airflow's development workflow. The table includes hook names and their corresponding purposes.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n| check-daysago-import-from-utils                           | days_ago imported from airflow.utils.dates             |\n| check-decorated-operator-implements-custom-name           | Check @task decorator implements custom_operator_name  |\n| check-default-configuration                               | Check the default configuration                        |\n[...truncated for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Removing Previous Provider Distributions and Preparing Provider Wheels (Bash)\nDESCRIPTION: These bash commands remove all files from the dist directory and use Breeze release management tools to build provider packages as wheels with a dev0 version suffix for testing. The commands are typically used prior to CI or managed compatibility runs to ensure only the latest builds are present and being tested. 'prepare-provider-distributions' also allows flags for including not-ready providers and specifying the distribution format.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\nrm dist/*\nbreeze release-management prepare-provider-distributions --include-not-ready-providers \\\n   --version-suffix-for-pypi dev0 --distribution-format wheel\n```\n\n----------------------------------------\n\nTITLE: Visualizing Airflow Worker Pod Failure Detection via Kubernetes Watcher (PlantUML)\nDESCRIPTION: This PlantUML sequence diagram illustrates the process by which the Airflow Scheduler detects a worker pod failure when using the KubernetesExecutor. It shows the initial pod request, the worker pod failing before completion, Kubernetes marking the pod as 'Failed', and the scheduler subsequently detecting this state change via the Kubernetes watcher thread and updating the task status in the Airflow database.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/kubernetes_executor.rst#2025-04-22_snippet_3\n\nLANGUAGE: plantuml\nCODE:\n```\n.. @startuml\n..\n.. Airflow_Scheduler -> Kubernetes: Request a new pod with command \"airflow run...\"\n.. Kubernetes -> Airflow_Worker: Create Airflow worker with command \"airflow run...\"\n.. Airflow_Worker -> Airflow_Worker: Pod fails before task can complete\n.. Airflow_Worker -> Kubernetes: Pod completes with state \"Failed\" and k8s records in ETCD\n.. Kubernetes -> Airflow_Scheduler: Airflow scheduler reads \"Failed\" from k8s watcher thread\n.. Airflow_Scheduler -> Airflow_DB: Airflow scheduler records \"FAILED\" state to DB for task\n..\n.. @enduml\n```\n\n----------------------------------------\n\nTITLE: SQS Sensor JSON Path Configuration\nDESCRIPTION: Support for jsonpath_ng.ext.parse in SQS Sensor for message filtering\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"jsonpath_ng.ext.parse\" \n```\n\n----------------------------------------\n\nTITLE: Defining Dataproc Metastore Service Configuration in Python\nDESCRIPTION: Defines a Python dictionary representing the configuration for a new Google Cloud Dataproc Metastore service. This dictionary includes parameters like labels and tier, conforming to the Dataproc Metastore API requirements. It is intended to be passed to the `DataprocMetastoreCreateServiceOperator`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc_metastore.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSERVICE = {\n    \"labels\": {\"airflow-version\": \"v2-3\"},\n    \"tier\": \"DEVELOPER\",\n}\n```\n\n----------------------------------------\n\nTITLE: Airflow Configuration Lint Rule\nDESCRIPTION: Configuration parameter that needs to be removed from airflow.cfg as part of SLA removal\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/42285.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncore.check_slas\n```\n\n----------------------------------------\n\nTITLE: Adding Apache Git Remote in Shell\nDESCRIPTION: Sets up a Git remote named 'apache' pointing to the official Apache Airflow repository and fetches its contents. This is a prerequisite for checking out specific branches or tags from the Apache repository.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit remote add apache git@github.com:apache/airflow.git\ngit fetch apache\n```\n\n----------------------------------------\n\nTITLE: Running Individual Diagram Generation\nDESCRIPTION: Command to generate a single diagram by executing its Python source file.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/17_architecture_diagrams.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython <path-to-diagram-file>.py\n```\n\n----------------------------------------\n\nTITLE: Including Provider Configuration Reference - Sphinx reStructuredText\nDESCRIPTION: This snippet uses the Sphinx-specific .. include:: directive in reStructuredText to import external documentation about provider configurations into the current file. The relative path specifies where the referenced documentation lives in the project. This approach assumes Sphinx is installed and that the referenced files exist. Inputs are the file paths, and output is the rendered content merged during documentation build. Proper permissions and Sphinx setup are required for this to process and render correctly.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst\n```\n\n----------------------------------------\n\nTITLE: Setting Null Logical Date Default in TriggerDagRunOperator\nDESCRIPTION: Commit message describing a change where 'TriggerDagRunOperator' now defaults to setting the logical date to null when triggering runs. References pull request #46633.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_25\n\nLANGUAGE: plaintext\nCODE:\n```\nTriggerDagRunOperator by defaults set logical date as null (#46633)\n```\n\n----------------------------------------\n\nTITLE: Checklist for Types of Changes in Apache Airflow\nDESCRIPTION: A markdown checklist indicating the types of changes made in the Apache Airflow project, including DAG, API, and CLI changes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41390.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* Types of change\n\n  * [x] Dag changes\n  * [ ] Config changes\n  * [x] API changes\n  * [x] CLI changes\n  * [ ] Behaviour changes\n  * [ ] Plugin changes\n  * [ ] Dependency changes\n  * [ ] Code interface changes\n```\n\n----------------------------------------\n\nTITLE: Including External RST File for Security Information\nDESCRIPTION: This reStructuredText (RST) directive includes the content of the specified file (`/../../../devel-common/src/sphinx_exts/includes/security.rst`) into the current document during processing by tools like Sphinx. This is used here to embed common security documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/segment/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Specifying Skipped Pre-Commit Hooks (String)\nDESCRIPTION: A comma-separated string listing pre-commit hooks that should be skipped during the static-checks run. This example skips 'flynt' and 'identity'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_32\n\nLANGUAGE: text\nCODE:\n```\nflynt,identity\n```\n\n----------------------------------------\n\nTITLE: Embedding Airflow Breeze Logo HTML\nDESCRIPTION: HTML code for centering and displaying the Airflow Breeze logo image with appropriate alt text for accessibility.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div align=\"center\">\n      <img src=\"images/AirflowBreeze_logo.png\"\n           alt=\"Airflow Breeze - Development and Test Environment for Apache Airflow\">\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Relocating weekday.py Utility to Standard Provider\nDESCRIPTION: Commit message describing the relocation of the 'utils/weekday.py' file to a standard provider structure within Airflow. References pull request #47892.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nRelocate utils/weekday.py to standard provider (#47892)\n```\n\n----------------------------------------\n\nTITLE: Migrating Kubernetes Client and Model Functions in Python\nDESCRIPTION: Migration rules for Kubernetes client and model functions, moving from the deprecated airflow.kubernetes module to airflow.providers.cncf.kubernetes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41735.significant.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"airflow.kubernetes.pod_launcher_deprecated.get_kube_client\"  \"airflow.providers.cncf.kubernetes.kube_client.get_kube_client\"\n\"airflow.kubernetes.k8s_model.K8SModel\"  \"airflow.providers.cncf.kubernetes.k8s_model.K8SModel\"\n\"airflow.kubernetes.k8s_model.append_to_pod\"  \"airflow.providers.cncf.kubernetes.k8s_model.append_to_pod\"\n\"airflow.kubernetes.kube_client._disable_verify_ssl\"  \"airflow.kubernetes.airflow.providers.cncf.kubernetes.kube_client._disable_verify_ssl\"\n\"airflow.kubernetes.kube_client._enable_tcp_keepalive\"  \"airflow.kubernetes.airflow.providers.cncf.kubernetes.kube_client._enable_tcp_keepalive\"\n\"airflow.kubernetes.kube_client.get_kube_client\"  \"airflow.kubernetes.airflow.providers.cncf.kubernetes.kube_client.get_kube_client\"\n```\n\n----------------------------------------\n\nTITLE: OracleHook get_uri Method Fix Reference\nDESCRIPTION: Reference to a bug fix for overwriting the 'get_uri' method in the OracleHook class to properly handle connection URIs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n* ``fix: overwrite 'get_uri' for 'Oracle' (#48734)``\n```\n\n----------------------------------------\n\nTITLE: Command Configuration Change\nDESCRIPTION: Breaking change note indicating that pinot-admin.sh is now hardcoded as the admin command and must be available in the system\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npinot-admin.sh\n```\n\n----------------------------------------\n\nTITLE: Yarn Build Commands\nDESCRIPTION: Commands for generating bundled files for Airflow UI in both production and development modes.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/15_node_environment_setup.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Compiles the production / optimized js & css\nyarn run prod\n\n# Starts a web server that manages and updates your assets as you modify them\n# You'll need to run the webserver in debug mode too: ``airflow webserver -d``\nyarn run dev\n```\n\n----------------------------------------\n\nTITLE: HTML License Header Comment\nDESCRIPTION: Apache License 2.0 header comment block for the document\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0004-using-docker-images-as-test-environment.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataset URI Schemes Documentation in RST\nDESCRIPTION: RST directive for generating documentation about dataset URI schemes in Apache Airflow. The directive airflow-dataset-schemes is used without tags and with a specified header separator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/core-extensions/asset-schemes.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. airflow-dataset-schemes::\n   :tags: None\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Uploading Google Calendar Data to Google Cloud Storage in Python\nDESCRIPTION: Uses the GoogleCalendarToGCSOperator to extract data from Google Calendar and upload it to Google Cloud Storage. The operator supports Jinja templating for dynamic parameter generation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/calendar_to_gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nupload_calendar_data = GoogleCalendarToGCSOperator(\n    task_id=\"upload_calendar_data\",\n    destination_bucket=BUCKET,\n    destination_path=PATH,\n    calendar_id=\"primary\",\n    api_version=\"v3\",\n    gcp_conn_id=\"google_cloud_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding a Note in RST Documentation\nDESCRIPTION: RST syntax for adding a note to the documentation, used to highlight important information about provider compatibility.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. note::\n  This release of provider is only available for Airflow 2.5+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n```\n\n----------------------------------------\n\nTITLE: Publishing Helm Chart Documentation\nDESCRIPTION: Command to publish documentation for the Helm chart package.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management publish-docs --airflow-site-directory DIRECTORY helm-chart\n```\n\n----------------------------------------\n\nTITLE: Fixing Comment Typo in PythonOperator\nDESCRIPTION: Commit message describing a minor fix for a typographical error in a comment within the 'PythonOperator' code. References pull request #47558.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nFix comment typo in PythonOperator (#47558)\n```\n\n----------------------------------------\n\nTITLE: Installing Diagram Dependencies\nDESCRIPTION: Command to install required Python packages for generating diagrams, including the 'diagrams' and 'rich' packages.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/17_architecture_diagrams.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install diagrams rich\n```\n\n----------------------------------------\n\nTITLE: Adding Strict Type Coverage for Oracle and Yandex Airflow Providers\nDESCRIPTION: This commit message indicates the implementation of strict type checking coverage specifically for the Oracle and Yandex provider packages within Airflow. It references pull request #11198.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_74\n\nLANGUAGE: text\nCODE:\n```\nStrict type coverage for Oracle and Yandex provider  (#11198)\n```\n\n----------------------------------------\n\nTITLE: Showing Diff for Files Modified by Pre-commit in Bash\nDESCRIPTION: This command shows the differences in files that were automatically modified by pre-commit hooks.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --show-diff-on-failure\n```\n\n----------------------------------------\n\nTITLE: Searching Entries in Dataplex Catalog using Python\nDESCRIPTION: This snippet illustrates how to use the DataplexCatalogSearchEntriesOperator to search for Entries matching a given query and scope in the Dataplex Catalog. It requires the appropriate Airflow provider and Google Cloud credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_59\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_dataplex_catalog_search_entry]\nsearch_entries = DataplexCatalogSearchEntriesOperator(\n    task_id=\"search_entries\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    query=QUERY,\n    location=LOCATION,\n)\n# [END howto_operator_dataplex_catalog_search_entry]\n```\n\n----------------------------------------\n\nTITLE: Complete Rebase Workflow Commands\nDESCRIPTION: Summary of all commands needed for rebasing a feature branch onto the main Apache Airflow branch.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/10_working_with_git.rst#2025-04-22_snippet_2\n\nLANGUAGE: console\nCODE:\n```\ngit fetch --all\ngit add .\ngit commit\ngit merge-base my-branch apache/main\ngit checkout my-branch\ngit rebase HASH --onto apache/main\ngit push --force-with-lease\n```\n\n----------------------------------------\n\nTITLE: Creating SageMaker Model\nDESCRIPTION: Example showing how to use SageMakerModelOperator to create a model in Amazon SageMaker using trained model artifacts.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_task = SageMakerModelOperator(\n    task_id='model_task',\n    config={\n        \"ModelName\": \"demo-model\",\n        \"PrimaryContainer\": {},\n        \"ExecutionRoleArn\": \"test_role\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for Standard Operators in reStructuredText\nDESCRIPTION: This snippet sets up a table of contents (toctree) for the standard operators documentation in Apache Airflow. It uses the glob option to include all files in the current directory and sets the maximum depth to 1.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Running Breeze with Default Builder on Windows WSL2\nDESCRIPTION: Command to run Breeze on Windows WSL2 with explicit default builder specification\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbreeze --builder=default --python 3.9 --backend mysql --mysql-version 8.0\n```\n\n----------------------------------------\n\nTITLE: RestructuredText Changelog Entry for Jira Provider Updates\nDESCRIPTION: Documented changelog entries showing version history, breaking changes, and bug fixes for the Jira provider. Highlights the migration from Atlassian Jira SDK to atlassian-python-api SDK and changes to the JiraOperator functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/atlassian/jira/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\nBug Fixes\n~~~~~~~~~\n\n* ``Handle 'jira_method_args' in JiraOperator when not provided (#29741)``\n\n2.0.0\n.....\n\nBreaking changes\n~~~~~~~~~~~~~~~~\n\n* ``Changing atlassian JIRA SDK to official atlassian-python-api SDK (#27633)``\n\nMigrated ``Jira`` provider from Atlassian ``Jira`` SDK to ``atlassian-python-api`` SDK.\n``Jira`` provider doesn't support ``validate`` and ``get_server_info`` in connection extra dict.\nChanged the return type of ``JiraHook.get_conn`` to return an ``atlassian.Jira`` object instead of a ``jira.Jira`` object.\n\n.. warning:: Due to the underlying SDK change, the ``JiraOperator`` now requires ``jira_method`` and ``jira_method_args``\n             arguments as per ``atlassian-python-api``.\n\n             Please refer `Atlassian Python API Documentation <https://atlassian-python-api.readthedocs.io/jira.html>`__\n\n1.1.0\n.....\n\n.. note::\n  This release of provider is only available for Airflow 2.3+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n\nMisc\n~~~~\n\n* ``Move min airflow version to 2.3.0 for all providers (#27196)``\n\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!)::\n   * ``Enable string normalization in python formatting - providers (#27205)``\n\n1.0.0\n.....\n\nInitial version of the provider.\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 1.0.2\nDESCRIPTION: Changelog entry noting documentation corrections after provider RC releases\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sendgrid/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n1.0.2\n.....\n\nBug fixes\n~~~~~~~~~\n\n* ``Corrections in docs and tools after releasing provider RCs (#14082)``\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for Azure Transfer Operators in reStructuredText\nDESCRIPTION: This snippet sets up a table of contents (toctree) for Azure Transfer Operators documentation. It specifies a maximum depth of 1 and uses a glob pattern to include all files in the current directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/transfer/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Service Operators and Hooks Reference Block\nDESCRIPTION: ReStructuredText directive that generates reference documentation for service-related operators and hooks with specific formatting.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/operators-and-hooks-ref/services.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. operators-hooks-ref::\n   :tags: service\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Airflow AWS Documentation Structure\nDESCRIPTION: ReStructuredText markup defining the structure and content of AWS integration documentation for Apache Airflow, including license header, AWS service integrations section, and transfer operations section.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/operators-and-hooks-ref/aws.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n.. _AWS:\n\nAWS: Amazon Web Services\n------------------------\n\nAirflow has support for `Amazon Web Services <https://aws.amazon.com/>`__.\n\nAll hooks are based on :mod:`airflow.providers.amazon.aws.hooks.base_aws`.\n\nServices\n''''''''\n\nThese integrations allow you to perform various operations within the Amazon Web Services.\n\n.. operators-hooks-ref::\n   :tags: aws\n   :header-separator: \"\n\nTransfers\n'''''''''\n\nThese integrations allow you to copy data from/to Amazon Web Services.\n\n.. transfers-ref::\n   :tags: aws\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Adding Explanatory Note for Changelog Updates\nDESCRIPTION: Excluded Change (Version 3.0.0): Adds a note for contributors explaining the process for updating the Changelog file, referencing pull request #24229.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_15\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Add explanatory note for contributors about updating Changelog (#24229)``\n```\n\n----------------------------------------\n\nTITLE: Documenting Changelog Entries Using reStructuredText - reStructuredText\nDESCRIPTION: This snippet uses reStructuredText syntax to record breaking changes, release notes, and upgrade instructions for a provider package in Apache Airflow. It includes directive blocks for warnings, section dividers, and bullet lists to enumerate changes. This format requires Sphinx or compatible documentation tools to render properly, and expects the editor to follow markup syntax for headings and blocks. Expected inputs are changelog details; outputs are rendered documentation with clear versioning and warnings. Limitations include the need for correct indentation and markup for proper rendering.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n2.0.0\n.....\n\nBreaking changes\n~~~~~~~~~~~~~~~~\n\n* ``Auto-apply apply_default decorator (#15667)``\n\n.. warning:: Due to apply_default decorator removal, this version of the provider requires Airflow 2.1.0+.\n   If your Airflow version is < 2.1.0, and you want to install this provider version, first upgrade\n   Airflow to at least version 2.1.0. Otherwise your Airflow package version will be upgraded\n   automatically and you will have to manually run ``airflow upgrade db`` to complete the migration.\n\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Adds interactivity when generating provider documentation. (#15518)``\n   * ``Prepares provider release after PIP 21 compatibility (#15576)``\n   * ``Remove Backport Providers (#14886)``\n   * ``Update documentation for broken package releases (#14734)``\n   * ``Updated documentation for June 2021 provider release (#16294)``\n   * ``More documentation update for June providers release (#16405)``\n   * ``Synchronizes updated changelog after buggfix release (#16464)``\n\n1.0.1\n.....\n\nUpdated documentation and readme files.\n\n1.0.0\n.....\n\nInitial version of the provider.\n\n```\n\n----------------------------------------\n\nTITLE: Setting up RST Documentation Structure for Airflow Core Extensions\nDESCRIPTION: RST directive for setting up the documentation structure with a table of contents tree that includes all files in the current directory with maximum depth of 2 levels.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/core-extensions/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 2\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Checking Parameter Groups Command\nDESCRIPTION: Command to verify that all Breeze parameters are properly assigned to rich groups for help output formatting.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/07_breeze_maintenance_tasks.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncheck-all-params-in-groups\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow with MySQL Backend using Breeze\nDESCRIPTION: This command starts Airflow using Python 3.9 and MySQL backend in the Breeze development environment.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbreeze --python 3.9 --backend mysql start-airflow\n```\n\n----------------------------------------\n\nTITLE: Configuring Fernet Key via Environment Variable in Bash\nDESCRIPTION: This snippet demonstrates how to set the Fernet key for Airflow using an environment variable. This method overwrites the value from the airflow.cfg file.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/fernet.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Note the double underscores\nexport AIRFLOW__CORE__FERNET_KEY=your_fernet_key\n```\n\n----------------------------------------\n\nTITLE: Installing All Core Extras for Apache Airflow\nDESCRIPTION: Command to install all optional core dependencies for Apache Airflow. This includes core functionality without external providers.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_73\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow[all-core]\n```\n\n----------------------------------------\n\nTITLE: Implementing Deferrable Sensor in Python with Airflow\nDESCRIPTION: Example of a deferrable sensor implementation that waits for one hour, supporting both deferrable and non-deferrable modes based on configuration. The sensor can either defer execution using a TimeDeltaTrigger or use traditional sleep-based waiting.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/deferring.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom datetime import timedelta\nfrom typing import Any\n\nfrom airflow.configuration import conf\nfrom airflow.sdk import BaseSensorOperator\nfrom airflow.providers.standard.triggers.temporal import TimeDeltaTrigger\nfrom airflow.utils.context import Context\n\n\nclass WaitOneHourSensor(BaseSensorOperator):\n    def __init__(\n        self, deferrable: bool = conf.getboolean(\"operators\", \"default_deferrable\", fallback=False), **kwargs\n    ) -> None:\n        super().__init__(**kwargs)\n        self.deferrable = deferrable\n\n    def execute(self, context: Context) -> None:\n        if self.deferrable:\n            self.defer(\n                trigger=TimeDeltaTrigger(timedelta(hours=1)),\n                method_name=\"execute_complete\",\n            )\n        else:\n            time.sleep(3600)\n\n    def execute_complete(\n        self,\n        context: Context,\n        event: dict[str, Any] | None = None,\n    ) -> None:\n        # We have no more work to do here. Mark as complete.\n        return\n```\n\n----------------------------------------\n\nTITLE: Marking Airflow Tests for Specific Backends with Pytest - Python\nDESCRIPTION: These Python snippets demonstrate using pytest's custom @pytest.mark.backend marker to annotate Airflow test functions for execution only against specified database backends (e.g., postgres, mysql). Dependencies include pytest, Airflow's test environment, and the custom backend marker. Inputs are backend names as marker parameters. Outputs are tests being run/skipped based on the selected backend in the test environment, providing targeted coverage. Assumes user understands pytest marking and test selection.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.backend(\"postgres\")\ndef test_copy_expert(self): ...\n```\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.backend(\"postgres\", \"mysql\")\ndef test_celery_executor(self): ...\n```\n\n----------------------------------------\n\nTITLE: PR Template Structure in ReStructuredText\nDESCRIPTION: Template structure for Airflow pull requests that includes sections for change summary, context, type of changes, and migration rules. Uses ReStructuredText format with commented instructions.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/template.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. Write a short and imperative summary of this changes\n\n.. Provide additional contextual information\n\n.. Check the type of change that applies to this change\n.. Dag changes: requires users to change their dag code\n.. Config changes: requires users to change their Airflow config\n.. API changes: requires users to change their Airflow REST API calls\n.. CLI changes: requires users to change their Airflow CLI usage\n.. Behaviour changes: the existing code won't break, but the behavior is different\n.. Plugin changes: requires users to change their Airflow plugin implementation\n.. Dependency changes: requires users to change their dependencies (e.g., Postgres 12)\n.. Code interface changes: requires users to change other implementations (e.g., auth manager)\n\n* Types of change\n\n  * [ ] Dag changes\n  * [ ] Config changes\n  * [ ] API changes\n  * [ ] CLI changes\n  * [ ] Behaviour changes\n  * [ ] Plugin changes\n  * [ ] Dependency changes\n  * [ ] Code interface changes\n\n.. List the migration rules needed for this change (see https://github.com/apache/airflow/issues/41641)\n\n* Migration rules needed\n\n.. e.g.,\n.. * Remove context key ``execution_date``\n.. * context key ``triggering_dataset_events``  ``triggering_asset_events``\n.. * Remove method ``airflow.providers_manager.ProvidersManager.initialize_providers_dataset_uri_resources``  ``airflow.providers_manager.ProvidersManager.initialize_providers_asset_uri_resources``\n```\n\n----------------------------------------\n\nTITLE: Pointing Airflow Provider Cross-Dependencies to PyPI Pages\nDESCRIPTION: This commit message indicates a change where cross-dependencies between provider packages now point directly to their respective PyPI project pages. It references pull request #12212.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_60\n\nLANGUAGE: text\nCODE:\n```\nPoint at pypi project pages for cross-dependency of provider packages (#12212)\n```\n\n----------------------------------------\n\nTITLE: Installing Hashicorp Provider Package with Bash pip Command\nDESCRIPTION: This Bash code snippet provides a pip command to install the 'apache-airflow-providers-hashicorp' package with the optional 'google' extra requirements. It demonstrates how to include cross-provider dependencies during installation, which is necessary if your workflow needs integration with dependent providers. The command must be run within an environment where Python, pip, and Airflow are already installed. Inputs are the package name and optional extras in bracket notation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/hashicorp/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-hashicorp[google]\n```\n\n----------------------------------------\n\nTITLE: Viewing Expanded Log Groups in Airflow UI\nDESCRIPTION: Example of how expanded log groups appear in the Airflow web UI after clicking on the group label, showing all the detailed log lines within the group.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/logging-tasks.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n[2024-03-08, 23:30:18 CET] {logging_mixin.py:188} INFO - Here is some standard text.\n[2024-03-08, 23:30:18 CET] {logging_mixin.py:188}  Non important details\n[2024-03-08, 23:30:18 CET] {logging_mixin.py:188} INFO - bla\n[2024-03-08, 23:30:18 CET] {logging_mixin.py:188} INFO - debug messages...\n[2024-03-08, 23:30:18 CET] {logging_mixin.py:188}  Log group end\n[2024-03-08, 23:30:18 CET] {logging_mixin.py:188} INFO - Here is again some standard text.\n```\n\n----------------------------------------\n\nTITLE: Setting Up Trino Operator Documentation Table of Contents (reStructuredText)\nDESCRIPTION: This snippet configures a reStructuredText toctree directive to automatically include all files in the current directory for the Trino Operator documentation section of Apache Airflow. The ':maxdepth: 1' option restricts the table of contents depth to top-level sections, and ':glob:' expands the '*' wildcard to include all matching files. No additional dependencies are required beyond the Sphinx documentation generator and standard reStructuredText processing.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/trino/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\\n    :maxdepth: 1\\n    :glob:\\n\\n    *\n```\n\n----------------------------------------\n\nTITLE: Defining Sensors Table in reStructuredText\nDESCRIPTION: Creates a table listing Airflow sensors, specifically the base sensor. The table has two columns: Sensors and Guides.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/operators-and-hooks-ref.rst#2025-04-22_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n**Sensors:**\n\n.. list-table::\n   :header-rows: 1\n\n   * - Sensors\n     - Guides\n\n   * - :mod:`airflow.sensors.base`\n     -\n```\n\n----------------------------------------\n\nTITLE: Fixing SQLite Hook Insert and Replace Functions\nDESCRIPTION: This code snippet fixes issues with the insert and replace functions in the SQLite hook. It improves the reliability of data manipulation operations using the SQLite provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/changelog.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"Fix sqlite hook - insert and replace functions (#17695)\"\n```\n\n----------------------------------------\n\nTITLE: Setting Python Version for Documentation Build\nDESCRIPTION: Command to pin Python version to 3.11 using uv, which is required for building documentation due to compatibility issues with Python 3.12.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/11_documentation_building.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv python pin 3.11\n```\n\n----------------------------------------\n\nTITLE: Installing Datadog Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Datadog integration, enabling Datadog hooks and sensors.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[datadog]'\n```\n\n----------------------------------------\n\nTITLE: Documenting RC3 Provider Release (Jan 2022) in RST\nDESCRIPTION: This commit (28378d867a, committed on 2022-02-14) adds documentation for the Release Candidate 3 (RC3) of Apache Airflow providers scheduled for January 2022. Refers to issue #21553.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nAdd documentation for RC3 release of providers for Jan 2022 (#21553)\n```\n\n----------------------------------------\n\nTITLE: Updating Documentation for Broken Airflow Package Releases\nDESCRIPTION: This commit message describes updates to documentation specifically addressing issues related to broken package releases in Airflow. It references pull request #14734.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_40\n\nLANGUAGE: text\nCODE:\n```\nUpdate documentation for broken package releases (#14734)\n```\n\n----------------------------------------\n\nTITLE: Shell Commands for Breeze Installation\nDESCRIPTION: Example commands showing how to install Breeze using pipx in editable mode and force reinstallation when needed.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0003-bootstrapping-virtual-environment.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npipx install -e <BREEZE FOLDER>\\npipx install --force -e <BREEZE FOLDER>\n```\n\n----------------------------------------\n\nTITLE: Setting Required HTTP Headers\nDESCRIPTION: Example of required HTTP headers for API requests that handle JSON data.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nContent-type: application/json\nAccept: application/json\n```\n\n----------------------------------------\n\nTITLE: Git Commit History for Apache Kylin Provider\nDESCRIPTION: List of git commits showing historical changes to the Apache Kylin provider package, including version releases, documentation updates, and code improvements.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kylin/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nbbc627a3da  2021-06-18   Prepares documentation for rc2 release of Providers (#16501)\ncbf8001d76  2021-06-16   Synchronizes updated changelog after buggfix release (#16464)\n1fba5402bb  2021-06-15   More documentation update for June providers release (#16405)\n...\n```\n\n----------------------------------------\n\nTITLE: Loading CI Image from Specific Run using Breeze\nDESCRIPTION: Command to load a CI image from a specific GitHub Actions run using Breeze. This is useful for reproducing the exact environment of a particular CI run.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/07_running_ci_locally.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbreeze ci-image load --from-run 12538475388 --python 3.9 --github-token <your_github_token>\n```\n\n----------------------------------------\n\nTITLE: Waiting for Athena Query Completion using Airflow AthenaSensor in Python\nDESCRIPTION: This Python snippet shows how to use the `airflow.providers.amazon.aws.sensors.athena.AthenaSensor` to wait for a specific Amazon Athena query to complete successfully. It requires the `query_execution_id` of the query to monitor (often obtained from a previous `AthenaOperator` task using XComs) and an AWS connection ID (`aws_conn_id`). The sensor polls the query status until it reaches a terminal state (success, failure, or cancelled).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/athena/athena_boto.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwait_for_query_tocomplete = AthenaSensor(\n    task_id=\"wait_for_query_tocomplete\",\n    query_execution_id=\"{{ task_instance.xcom_pull(task_ids='run_query', key='return_value') }}\",\n    aws_conn_id=\"aws_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Templating Product Removal Field - Google Cloud Vision Airflow Operator - Python\nDESCRIPTION: This snippet defines the template fields for the Airflow operator responsible for removing a product from a product set in Google Cloud Vision. Template fields include identifiers required to uniquely locate the product set and product within a given project and location. This field list allows dynamic substitution (templating) in Airflow DAGs and requires the Airflow Google provider package. The definition ensures that these fields can be parameterized at runtime for flexible workflow management.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"project_id\",\n    \"location\",\n    \"product_set_id\",\n    \"product_id\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Correct Signatures to Airflow Operators and Sensors\nDESCRIPTION: This commit message indicates that the method signatures for all operators and sensors within Airflow have been updated or corrected to ensure accuracy. It references pull request #10205.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_79\n\nLANGUAGE: text\nCODE:\n```\nAdd correct signature to all operators and sensors (#10205)\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-sqlite via pip - Bash\nDESCRIPTION: Demonstrates the use of pip to install the apache-airflow-providers-sqlite package with the optional 'common.sql' extra dependency. This command assumes that pip is available and that an appropriate Python environment is active. Inputs include the package name and specified optional extras, and successful execution ensures all core and additional dependencies required for advanced SQL provider features are present. Limitations include compatibility with Airflow version 2.9.0 or later and with the specified provider versions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-sqlite[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Updating S3 Key Check Method Docstring in Python\nDESCRIPTION: Updates the docstring for 'check_key_async' to align with the description of '_check_key_async'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"Updated docstring: 'check_key_async' is now in line with description of '_check_key_async' (#35799)\"\n```\n\n----------------------------------------\n\nTITLE: Improving Documentation for Updating Provider Dependencies\nDESCRIPTION: Commit message stating improvements made to the documentation regarding the process of updating provider dependencies. References pull request #47203.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\nImprove documentation for updating provider dependencies (#47203)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fix Links to Sources for Examples\nDESCRIPTION: This commit message, associated with commit 08b675cf66 dated 2022-06-13, addresses issues with broken links pointing to the source code of example DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_31\n\nLANGUAGE: text\nCODE:\n```\n``Fix links to sources for examples (#24386)``\n```\n\n----------------------------------------\n\nTITLE: Version Header in ReStructuredText\nDESCRIPTION: Version number formatting in ReStructuredText showing version header with underline decoration\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n3.4.1\n.....\n```\n\n----------------------------------------\n\nTITLE: Installing Common-IO Extras for Apache Airflow\nDESCRIPTION: Command to install Core IO Operators for Apache Airflow. This provides fundamental input/output functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_55\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[common-io]'\n```\n\n----------------------------------------\n\nTITLE: Defining PostgreSQL Connection String Format for Airflow Database (Bash)\nDESCRIPTION: This snippet shows the required format for the PostgreSQL connection string used by Airflow to connect to its metadata database. It needs to be set as the `AIRFLOW__DATABASE__SQL_ALCHEMY_CONN` environment variable within the ECS Task Definition. The placeholders `<username>`, `<password>`, `<endpoint>`, and `<database_name>` should be replaced with the actual credentials and details of the RDS instance created previously.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/executors/ecs-executor.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npostgresql+psycopg2://<username>:<password>@<endpoint>/<database_name>\n```\n\n----------------------------------------\n\nTITLE: Retry Import for Vision API\nDESCRIPTION: Import statement for Retry functionality\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_vision_retry_import]\\n[END howto_operator_vision_retry_import]\n```\n\n----------------------------------------\n\nTITLE: Retrieving YouTube Video Details and Saving to Amazon S3 using GoogleApiToS3Operator in Python\nDESCRIPTION: This snippet demonstrates using GoogleApiToS3Operator to fetch detailed information about specific YouTube videos (based on IDs from a previous task) and save the data to Amazon S3. It showcases the use of xcom to retrieve data from a previous task.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/google_api_to_s3.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nget_youtube_video_data = GoogleApiToS3Operator(\n    task_id=\"get_youtube_video_data\",\n    google_api_service_name=\"youtube\",\n    google_api_service_version=\"v3\",\n    google_api_endpoint_path=\"videos\",\n    google_api_endpoint_params={\n        \"part\": YOUTUBE_VIDEO_FIELDS,\n        \"id\": ','.join(\n            [\n                video['id']['videoId']\n                for video in \"{{ task_instance.xcom_pull('search_youtube_videos')['items'] }}\"\n            ]\n        ),\n    },\n    s3_destination_key=f\"s3://{S3_BUCKET_NAME}/youtube_videos_data_{{ ds_nodash }}.json\",\n    delegate_to=GOOGLE_DELEGATE_TO,\n    gcp_conn_id=GOOGLE_GCP_CONN_ID,\n    aws_conn_id=AWS_CONN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Product Import for Google Cloud Vision\nDESCRIPTION: Import statement for Product object from Google Cloud Vision library\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n[START howto_operator_vision_product_import]\\n[END howto_operator_vision_product_import]\n```\n\n----------------------------------------\n\nTITLE: Removed DAG Scheduling Arguments Annotation\nDESCRIPTION: Migration rule identifier AIR302 specifying the removal of schedule_interval and timetable arguments from DAG constructor.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41453.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: ruff\nCODE:\n```\nAIR302\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for GCP Logging in Apache Airflow\nDESCRIPTION: This reStructuredText snippet sets up a table of contents for documentation related to writing logs to Google Cloud Platform in Apache Airflow. It uses the toctree directive to include all files in the current directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/logging/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Defining Elasticsearch Provider Version 4.3.0 in YAML\nDESCRIPTION: YAML configuration specifying version 4.3.0 of the Elasticsearch provider, including commit history and release date.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n4.3.0:\n  Latest change: 2022-11-15\n\n  Commit                                                                                              Committed    Subject\n  ==================================================================================================  ===========  =========================================================================\n  `12c3c39d1a <https://github.com/apache/airflow/commit/12c3c39d1a816c99c626fe4c650e88cf7b1cc1bc>`__  2022-11-15   \"pRepare docs for November 2022 wave of Providers (#27613)\"\n  `9ab1a6a3e7 <https://github.com/apache/airflow/commit/9ab1a6a3e70b32a3cddddf0adede5d2f3f7e29ea>`__  2022-10-27   \"Update old style typing (#26872)\"\n  `78b8ea2f22 <https://github.com/apache/airflow/commit/78b8ea2f22239db3ef9976301234a66e50b47a94>`__  2022-10-24   \"Move min airflow version to 2.3.0 for all providers (#27196)\"\n  `2a34dc9e84 <https://github.com/apache/airflow/commit/2a34dc9e8470285b0ed2db71109ef4265e29688b>`__  2022-10-23   \"Enable string normalization in python formatting - providers (#27205)\"\n```\n\n----------------------------------------\n\nTITLE: Installing StatsD Integration for Apache Airflow\nDESCRIPTION: Command for installing the StatsD integration package for Apache Airflow. This is required before configuring Airflow to send metrics to StatsD.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/metrics.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[statsd]'\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Pig Provider Package in Python\nDESCRIPTION: Command to install the Apache Pig provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pig/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-pig\n```\n\n----------------------------------------\n\nTITLE: Adding Airflow Chart Dependencies in YAML\nDESCRIPTION: Configuration to add Apache Airflow chart as a dependency in Chart.yaml\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/extending-the-chart.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndependencies:\n  - name: airflow\n    version: 1.11.0\n    repository: https://airflow.apache.org\n```\n\n----------------------------------------\n\nTITLE: Updating Multiple Airflow Constraints Example\nDESCRIPTION: Example command for updating multiple constraint files across different Airflow versions, specifically updating pymssql and Authlib package versions.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/MANUALLY_GENERATING_IMAGE_CACHE_AND_CONSTRAINTS.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management update-constraints --constraints-repo /home/user/airflow-constraints \\\n    --airflow-versions 2.5.0,2.5.1,2.5.2,2.5.3,2.6.0,2.6.1,2.6.2,2.6.3 \\\n    --updated-constraint pymssql==2.2.8 \\\n    --updated-constraint Authlib==1.3.0 \\\n    --commit-message \"Update pymssql constraint to 2.2.8 and Authlib to 1.3.0\" \\\n    --airflow-constraints-mode constraints\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow in Development Mode\nDESCRIPTION: Command to start Airflow in development mode using Breeze\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_gitpod.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbreeze start-airflow --dev-mode\n```\n\n----------------------------------------\n\nTITLE: Setting Apprise Connection Configuration via Environment Variable in Bash\nDESCRIPTION: This snippet shows how to set the Apprise connection configuration using an environment variable in Bash. It includes the connection ID and the JSON configuration for the service.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/docs/connections.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_CONN_APPRISE_DEFAULT='{\"extra\": {\"config\": {\"path\": \"https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX\", \"tags\": \"alert\"}}}'\n```\n\n----------------------------------------\n\nTITLE: OpenAI Provider Package Requirements Table\nDESCRIPTION: Table showing the required dependencies and their minimum versions for the OpenAI provider package\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openai/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n===================  ==================\nPIP package          Version required\n===================  ==================\n\"apache-airflow\"   \">=2.9.0\"\n\"openai[datalib]\"  \">=1.66.0\"\n===================  ==================\n```\n\n----------------------------------------\n\nTITLE: Referencing Git Commits in Markdown\nDESCRIPTION: This snippet demonstrates how to reference Git commits with links in Markdown format. It includes the commit hash, link to the commit on GitHub, date, and description.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/docker/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n`97496ba2b4 <https://github.com/apache/airflow/commit/97496ba2b41063fa24393c58c5c648a0cdb5a7f8>`__  2021-12-31   ``Update documentation for provider December 2021 release (#20523)``\n```\n\n----------------------------------------\n\nTITLE: Adding DatabricksWorkflowTaskGroup\nDESCRIPTION: New feature introducing the DatabricksWorkflowTaskGroup.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"Add DatabricksWorkflowTaskGroup (#39771)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Microsoft Azure Provider Package with pip\nDESCRIPTION: Command to install the Microsoft Azure provider package on top of an existing Airflow 2 installation. This enables integration with Azure services in Airflow workflows.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-microsoft-azure\n```\n\n----------------------------------------\n\nTITLE: Updating Documentation for June 2021 Provider Release in RST\nDESCRIPTION: This commit (9c94b72d44, committed on 2021-06-07) updates the documentation corresponding to the June 2021 release of Airflow providers. Refers to issue #16294.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_29\n\nLANGUAGE: rst\nCODE:\n```\nUpdated documentation for June 2021 provider release (#16294)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add note about dropping Python 3.7 for providers (#32015)\nDESCRIPTION: This commit message, associated with version 3.2.1 (Latest change: 2023-06-20 indicates context for this block), adds a notification regarding the discontinuation of Python 3.7 support for Apache Airflow Providers. Commit hash: 8b146152d6, Date: 2023-06-20.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n``Add note about dropping Python 3.7 for providers (#32015)``\n```\n\n----------------------------------------\n\nTITLE: Installing Vertica Provider with Cross-Provider Extras (Bash)\nDESCRIPTION: Installs the `apache-airflow-providers-vertica` package along with optional cross-provider dependencies, specifically enabling features requiring the `common.sql` provider package. This command uses pip's extra syntax (`[common.sql]`) to pull in the necessary `apache-airflow-providers-common-sql` dependency.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/vertica/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-vertica[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Setting SendGrid SMTP Environment Variables (Shell)\nDESCRIPTION: This snippet demonstrates how to set SendGrid SMTP configuration options using environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nAIRFLOW__SMTP__SMTP_HOST=smtp.sendgrid.net\nAIRFLOW__SMTP__SMTP_STARTTLS=False\nAIRFLOW__SMTP__SMTP_SSL=False\nAIRFLOW__SMTP__SMTP_PORT=587\nAIRFLOW__SMTP__SMTP_MAIL_FROM=<your-from-email>\n```\n\n----------------------------------------\n\nTITLE: Updating DAG.tags Property Type in Python for Apache Airflow\nDESCRIPTION: This code snippet demonstrates the change in the DAG class where the 'tags' property type is modified from a Python list to a MutableSet. This change allows for more efficient tag handling by preventing duplicates while still maintaining flexibility in input types.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41420.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nDAG.tags: MutableSet\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Integration for Apache Airflow\nDESCRIPTION: Command for installing the OpenTelemetry integration package for Apache Airflow. This is required before configuring Airflow to send metrics to OpenTelemetry.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/metrics.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[otel]'\n```\n\n----------------------------------------\n\nTITLE: Installing Papermill Provider Package via pip\nDESCRIPTION: Command to install the apache-airflow-providers-papermill package using pip on top of an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-papermill\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-common-messaging via pip (Bash)\nDESCRIPTION: This command installs the `apache-airflow-providers-common-messaging` package using pip. It requires an existing Airflow installation (specifically, Airflow >= 3.0.0 as per the requirements section).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/messaging/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-common-messaging\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit Information in Markdown\nDESCRIPTION: This code snippet shows how commit information is formatted in the changelog using Markdown syntax. It includes the commit hash as a link, the commit date, and the commit message.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n`faa50cbe2f <https://github.com/apache/airflow/commit/faa50cbe2f6dbf816e599bbbb933ac4976a55778>`__  2023-08-24   ``feat(providers/microsoft): add DefaultAzureCredential support to AzureContainerInstanceHook (#33467)``\n```\n\n----------------------------------------\n\nTITLE: Updating External Task Sensor Import Paths\nDESCRIPTION: Migration of import statements for ExternalTaskMarker and ExternalTaskSensor from core Airflow to the standard provider package. This change follows the AIR303 ruff rule for standardizing provider imports.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/44288.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old imports\nfrom airflow.sensors.external_task import ExternalTaskMarker\nfrom airflow.sensors.external_task import ExternalTaskSensor\n\n# New imports\nfrom airflow.providers.standard.sensors.external_task import ExternalTaskMarker\nfrom airflow.providers.standard.sensors.external_task import ExternalTaskSensor\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST Files\nDESCRIPTION: Directive to include external security documentation from a common source file. This ensures consistent security information across the Airflow documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Adding default_deferrable config\nDESCRIPTION: Adds a default_deferrable configuration option.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"Add default_deferrable config (#31712)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Providers Edge3 via pip\nDESCRIPTION: Command to install the apache-airflow-providers-edge3 package using pip. This should be done on top of an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/edge3/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-edge3\n```\n\n----------------------------------------\n\nTITLE: Documenting Provider Update July 2023\nDESCRIPTION: This commit prepares the documentation for the main wave of Apache Airflow provider updates released in July 2023, as tracked in issue #32298.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_30\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs for July 2023 wave of Providers (#32298)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Flink Provider via pip\nDESCRIPTION: Command to install the Apache Flink provider package on an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/flink/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-flink\n```\n\n----------------------------------------\n\nTITLE: Testing PostgreSQL RDS Connection using psql (Bash)\nDESCRIPTION: This command tests the network connectivity to the configured AWS RDS PostgreSQL database instance using the psql command-line tool. It requires the database endpoint, username, and database name as parameters. A successful connection will prompt for the user's password. Ensure the security group associated with the RDS instance allows inbound traffic from your IP on TCP port 5432 and that the DB status is 'Available'.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/executors/general.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npsql -h <endpoint> -p 5432 -U <username> <db_name>\n```\n\n----------------------------------------\n\nTITLE: Installing PagerDuty Provider Package via pip\nDESCRIPTION: Command to install the PagerDuty provider package on top of an existing Apache Airflow installation. This package allows integration with PagerDuty services from Airflow workflows.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pagerduty/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-pagerduty\n```\n\n----------------------------------------\n\nTITLE: Conditional Airflow Dependencies in YAML\nDESCRIPTION: Configuration to add conditional Apache Airflow chart dependency with enabled flag\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/extending-the-chart.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndependencies:\n  - name: airflow\n    version: 1.11.0\n    repository: https://airflow.apache.org\n    condition: airflow.enabled\n```\n\n----------------------------------------\n\nTITLE: Exporting Processed Pull Request Data to CSV in Python\nDESCRIPTION: This code exports the processed pull request data stored in the 'rows' DataFrame to a CSV file named 'prlist.csv'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/stats/explore_pr_candidates.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrows.to_csv(\"prlist.csv\")\n```\n\n----------------------------------------\n\nTITLE: Building CI Docker Image with Default Extras - Docker/Bash\nDESCRIPTION: This bash command builds an Apache Airflow CI Docker image based on Python 3.9 with all default extras enabled (\"all\"). Requires Docker, a checked-out Airflow repository, and the Dockerfile.ci file. The PYTHON_BASE_IMAGE argument controls the Python version, and the resulting image is tagged as my-image:0.0.1. Default extras mean all optional packages are installed.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build . -f Dockerfile.ci \\\n   --pull \\\n   --build-arg PYTHON_BASE_IMAGE=\"python:3.9-slim-bookworm\" --tag my-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Launching Airflow Breeze Shell with Sources Mounted (Bash)\nDESCRIPTION: This bash command starts a Breeze shell configured to use a specific Airflow version (here 2.9.1) and mounts both providers and tests from source directories using the providers-and-tests mount setting. This allows for immediate reflection of source code changes in the container environment, expediting test development and debugging. The command is suited for local compatibility testing workflows.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nbreeze shell --use-airflow-version 2.9.1 --mount-sources providers-and-tests\n```\n\n----------------------------------------\n\nTITLE: Creating Model Version via Deferrable Custom Training Job (V2) in VertexAI (Python)\nDESCRIPTION: Shows how to create a new model version in VertexAI using CreateCustomTrainingJobOperator in deferrable mode, specifying the parent_model parameter. All standard and parent model parameters apply. The output is an updated VertexAI model, available after workflow completion.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n    create_training_job_v2_deferrable = CreateCustomTrainingJobOperator(\n        task_id=\"train_model_v2_deferrable\",\n        project_id=PROJECT_ID,\n        location=LOCATION,\n        display_name=DISPLAY_NAME,\n        script_path=SCRIPT_PATH,\n        dataset_id=DATASET_ID,\n        parent_model=PARENT_MODEL,\n        gcp_conn_id=GCP_CONN_ID,\n        deferrable=True,\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub Actions Workflow for Airflow CI/CD in YAML\nDESCRIPTION: This YAML configuration defines a GitHub Actions workflow for Airflow. It specifies the trigger events, job environment, and steps for setting up Python, installing dependencies, and running various checks and tests.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/atlassian/jira/docs/.latest-doc-only-change.txt#2025-04-22_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nname: Airflow CI/CD\n\non:\n  pull_request:\n  push:\n    branches: [main, master]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.x'\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n      - name: Run linter\n        run: pylint **/*.py\n      - name: Run tests\n        run: pytest\n      - name: Build documentation\n        run: |\n          cd docs\n          make html\n      - name: Deploy to staging\n        if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n        run: |\n          # Add deployment script here\n          echo \"Deploying to staging\"\n```\n\n----------------------------------------\n\nTITLE: Auto-generation Notice in RST\nDESCRIPTION: Warning notice about the file being automatically generated from a template, written in ReStructuredText format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!\n\n.. IF YOU WANT TO MODIFY THIS FILE, YOU SHOULD MODIFY THE TEMPLATE\n   `PROVIDER_COMMITS_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY\n\n.. THE REMAINDER OF THE FILE IS AUTOMATICALLY GENERATED. IT WILL BE OVERWRITTEN!\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents for Secret Backends in reStructuredText\nDESCRIPTION: This snippet creates a table of contents for secret backends documentation using reStructuredText directives. It sets the maximum depth to 1 and uses a glob pattern to include all files in the current directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/secrets-backends/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Installing JDBC Provider Package with pip\nDESCRIPTION: Command to install the JDBC provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-jdbc\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fixed wrongly escaped characters in amazon's changelog\nDESCRIPTION: This commit message, for commit d02ded65ea, indicates a fix applied to the changelog file for the Amazon provider, correcting characters that were improperly escaped. This is linked to issue #17020.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n``Fixed wrongly escaped characters in amazon's changelog (#17020)``\n```\n\n----------------------------------------\n\nTITLE: Dropping Incompatible Data Table in Python for Airflow\nDESCRIPTION: Python code to drop a table containing incompatible data after an Airflow database migration. This is executed within a Kubernetes pod to clean up moved data.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.settings import Session\n\nsession = Session()\nsession.execute(\"DROP TABLE _airflow_moved__2_2__task_instance\")\nsession.commit()\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Drill Provider with Common SQL Dependency\nDESCRIPTION: Command to install the Apache Drill provider package along with its common SQL dependency using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-drill[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Setting Email Configuration Environment Variables (Shell)\nDESCRIPTION: This snippet demonstrates how to set Airflow email configuration options using environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nAIRFLOW__EMAIL__EMAIL_BACKEND=airflow.utils.email.send_email_smtp\nAIRFLOW__EMAIL__SUBJECT_TEMPLATE=/path/to/my_subject_template_file\nAIRFLOW__EMAIL__HTML_CONTENT_TEMPLATE=/path/to/my_html_content_template_file\n```\n\n----------------------------------------\n\nTITLE: Migrating Apache Airflow Executor and Macro Import Paths\nDESCRIPTION: This code snippet shows the migration of import paths for Apache Airflow executors, executor-related constants, and macros. It includes changes for Kubernetes executor types, Celery executor, and Hive-related macros.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41368.significant.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Old import path  New import path\n\"airflow.executors.kubernetes_executor_types.ALL_NAMESPACES\"  \"airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types.ALL_NAMESPACES\"\n\"airflow.executors.kubernetes_executor_types.POD_EXECUTOR_DONE_KEY\"  \"airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types.POD_EXECUTOR_DONE_KEY\"\n\"airflow.hooks.hive_hooks.HIVE_QUEUE_PRIORITIES\"  \"airflow.providers.apache.hive.hooks.hive.HIVE_QUEUE_PRIORITIES\"\n\"airflow.executors.celery_executor.app\"  \"airflow.providers.celery.executors.celery_executor_utils.app\"\n\"airflow.macros.hive.closest_ds_partition\"  \"airflow.providers.apache.hive.macros.hive.closest_ds_partition\"\n\"airflow.macros.hive.max_partition\"  \"airflow.providers.apache.hive.macros.hive.max_partition\"\n```\n\n----------------------------------------\n\nTITLE: Handling Import Errors with Compatibility Context Manager (Python)\nDESCRIPTION: This Python snippet shows how to wrap a potentially incompatible provider import in the ignore_provider_compatibility_error context manager. It prevents pytest collection from failing when an import raises compatibility/runtime exceptions on certain Airflow versions, instead skipping the whole module. Parameters include the minimum required Airflow version and the current module filename.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nwith ignore_provider_compatibility_error(\"2.8.0\", __file__):\n    from airflow.providers.common.io.xcom.backend import XComObjectStorageBackend\n```\n\n----------------------------------------\n\nTITLE: Adding Trove Classifiers for PyPI\nDESCRIPTION: Miscellaneous Change (Version 2.2.2): Adds PyPI Trove classifiers (Framework :: Apache Airflow :: Provider) to package metadata, improving discoverability.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_20\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Add Trove classifiers in PyPI (Framework :: Apache Airflow :: Provider)``\n```\n\n----------------------------------------\n\nTITLE: Example Vertica Connection Extras (Load Balancing, Logging, SSL)\nDESCRIPTION: This JSON snippet demonstrates an example configuration for the 'Extra' field in an Airflow Vertica connection. It sets 'connection_load_balance' to true to enable client-side load balancing, sets 'log_level' to 'error' to configure logging level for the vertica-python client, and sets 'ssl' to true to enable SSL/TLS encryption for the connection. These parameters are passed to the underlying 'vertica-python' library.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/vertica/docs/connections/vertica.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"connection_load_balance\": true,\n   \"log_level\": \"error\",\n   \"ssl\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Waiting for AWS CloudFormation Stack Creation State with Airflow Sensor - Python\nDESCRIPTION: This snippet shows the use of CloudFormationCreateStackSensor in Python with Airflow to monitor the creation process of an AWS CloudFormation stack until it reaches a terminal state. It requires the stack's name, AWS connection credentials, and possibly a polling interval. The sensor waits for successful or failed creation, returning state or throwing errors on timeout or failure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/cloudformation.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwait_for_creation = CloudFormationCreateStackSensor(\n    task_id=\"wait_for_stack_creation\",\n    stack_name=STACK_NAME,\n    aws_conn_id=\"aws_default\",\n    poke_interval=60,\n)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Move min airflow version to 2.3.0 for all providers (#27196)\nDESCRIPTION: This commit message, associated with version 3.1.0, updates the minimum required Airflow version to 2.3.0 across all Apache Airflow Providers. Commit hash: 78b8ea2f22, Date: 2022-10-24.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_20\n\nLANGUAGE: text\nCODE:\n```\n``Move min airflow version to 2.3.0 for all providers (#27196)``\n```\n\n----------------------------------------\n\nTITLE: Migrating Label Class Import in Python\nDESCRIPTION: Example showing how to update imports for the Label class which has been moved from airflow.utils.edgemodifier to airflow.sdk.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/aip-72.significant.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.utils.edgemodifier import Label\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.sdk import Label\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Documentation in RST\nDESCRIPTION: ReStructuredText directive to include external documentation about installing Airflow providers from source code. The included file contains detailed instructions for building and installing Airflow providers from source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kylin/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Deprecated CLI Commands in Apache Airflow\nDESCRIPTION: List of removed database management commands and their replacements. The 'db init' and 'db upgrade' commands have been deprecated in favor of 'db migrate' for database initialization and migration. For creating default connections, users should use 'airflow connections create-default-connections'.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/44706.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\ndb init\ndb upgrade\ndb migrate\nairflow connections create-default-connections\n```\n\n----------------------------------------\n\nTITLE: Installing apache-airflow-providers-git via pip\nDESCRIPTION: Shell command to install the `apache-airflow-providers-git` package using the pip package manager. This command should be run in a terminal or command prompt where an existing Airflow 2 installation is present.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/git/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-git\n```\n\n----------------------------------------\n\nTITLE: Enabling Verbose Commands for Debugging\nDESCRIPTION: Enables verbose command output for better debugging of Breeze issues.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/04_troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport VERBOSE_COMMANDS=\"true\"\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Git Hook\nDESCRIPTION: Commands to enable pre-commit checks before git commits\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/Projects/airflow\npre-commit install\ngit commit -m \"Added xyz\"\n```\n\n----------------------------------------\n\nTITLE: Installing Apprise Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Apprise provider package on top of an existing Apache Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apprise\n```\n\n----------------------------------------\n\nTITLE: Enabling Production Image Build (Boolean String)\nDESCRIPTION: A boolean flag indicating whether the production Docker image build is needed. 'true' enables the build.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_13\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Problematic Test Collection with Variable Dependencies in Python\nDESCRIPTION: This code snippet shows a problematic way of using Airflow Variables in test parameterization, which can cause database access during test collection.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models.variable import Variable\n\npytestmark = pytest.mark.db_test\n\ninitial_db_init()\n\n\n@pytest.mark.parametrize(\n    \"env, expected\",\n    [\n        pytest.param(\n            {\"plain_key\": \"plain_value\"},\n            \"{'plain_key': 'plain_value'}\",\n            id=\"env-plain-key-val\",\n        ),\n        pytest.param(\n            {\"plain_key\": Variable.setdefault(\"plain_var\", \"banana\")},\n            \"{'plain_key': 'banana'}\",\n            id=\"env-plain-key-plain-var\",\n        ),\n        pytest.param(\n            {\"plain_key\": Variable.setdefault(\"secret_var\", \"monkey\")},\n            \"{'plain_key': '***'}\",\n            id=\"env-plain-key-sensitive-var\",\n        ),\n        pytest.param(\n            {\"plain_key\": \"{{ var.value.plain_var }}\"},\n            \"{'plain_key': '{{ var.value.plain_var }}'}\",\n            id=\"env-plain-key-plain-tpld-var\",\n        ),\n    ],\n)\ndef test_rendered_task_detail_env_secret(patch_app, admin_client, request, env, expected): ...\n```\n\n----------------------------------------\n\nTITLE: Installing Common Messaging Provider using pip in Bash\nDESCRIPTION: Demonstrates the basic pip command to install the `apache-airflow-providers-common-messaging` package. This installs the core provider package on top of an existing Airflow installation. Requires pip and a compatible Airflow version (>=3.0.0).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/messaging/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-common-messaging\n```\n\n----------------------------------------\n\nTITLE: Fixing Wrong Casing in Airbyte Hook (Commit Message)\nDESCRIPTION: Commit message detailing a fix for incorrect casing within the Airbyte hook in Apache Airflow. This addresses a potential bug or inconsistency in the Airbyte provider code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nFix wrong casing in airbyte hook. (#42170)\n```\n\n----------------------------------------\n\nTITLE: Listing Available Apache Airflow Docker Images\nDESCRIPTION: This snippet lists the different types of Docker images available for Apache Airflow, including latest and versioned images for both regular and slim variants.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* `apache/airflow:latest` - the latest released Airflow image with default Python version (3.12 currently)\n* `apache/airflow:latest-pythonX.Y` - the latest released Airflow image with specific Python version\n* `apache/airflow:3.1.0.dev0` - the versioned Airflow image with default Python version (3.12 currently)\n* `apache/airflow:3.1.0.dev0-pythonX.Y` - the versioned Airflow image with specific Python version\n\n* `apache/airflow:slim-latest`              - the latest released Airflow image with default Python version (3.12 currently)\n* `apache/airflow:slim-latest-pythonX.Y`    - the latest released Airflow image with specific Python version\n* `apache/airflow:slim-3.1.0.dev0`           - the versioned Airflow image with default Python version (3.12 currently)\n* `apache/airflow:slim-3.1.0.dev0-pythonX.Y` - the versioned Airflow image with specific Python version\n```\n\n----------------------------------------\n\nTITLE: Renaming Airflow Modules to Match AIP-21\nDESCRIPTION: This commit message details the renaming of remaining modules within Airflow to align with the specifications outlined in Airflow Improvement Proposal (AIP) 21. It references pull request #12917.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_51\n\nLANGUAGE: text\nCODE:\n```\nRename remaing modules to match AIP-21 (#12917)\n```\n\n----------------------------------------\n\nTITLE: reStructuredText Table of Contents for Amazon Executors\nDESCRIPTION: This reStructuredText snippet defines a table of contents using the `toctree` directive. It links to two sub-pages: 'ECS Executor <ecs-executor>' and 'Batch Executor (experimental) <batch-executor>'. The `:maxdepth: 1` option ensures that only the top-level headings from the linked pages are included in the table of contents generated on this page.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/executors/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n\n    ECS Executor <ecs-executor>\n    Batch Executor (experimental) <batch-executor>\n```\n\n----------------------------------------\n\nTITLE: Handling Import Failure for Azure Blob to GCS Transfer in Python\nDESCRIPTION: This snippet shows an error message when trying to import a module that doesn't exist due to a missing dependency. It illustrates the need for conditional imports when dealing with optional provider features.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/MANAGING_PROVIDERS_LIFECYCLE.rst#2025-04-22_snippet_10\n\nLANGUAGE: txt\nCODE:\n```\nError: The ``airflow.providers.microsoft.azure.transfers.azure_blob_to_gcs`` object in transfers list in\nairflow/providers/microsoft/azure/provider.yaml does not exist or is not a module:\nNo module named 'gcloud.aio.storage'\n```\n\n----------------------------------------\n\nTITLE: Setting Kerberos Environment Variables in Dockerfile\nDESCRIPTION: Dockerfile instructions to set Kerberos-related environment variables for Airflow in a containerized environment.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/kerberos.rst#2025-04-22_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nENV AIRFLOW__CORE__SECURITY kerberos\nENV AIRFLOW__KERBEROS__KEYTAB /etc/airflow/airflow.keytab\nENV AIRFLOW__KERBEROS__INCLUDE_IP False\n```\n\n----------------------------------------\n\nTITLE: Setting PYTHONPATH Environment Variable for Custom Logging in Airflow\nDESCRIPTION: This snippet shows how to set the PYTHONPATH environment variable to make custom logging configuration accessible to Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/administration-and-deployment/logging-monitoring/advanced-logging-configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport PYTHONPATH=~/airflow/\n```\n\n----------------------------------------\n\nTITLE: HTML Content Template Example (HTML)\nDESCRIPTION: This snippet provides an example of an HTML content template for email notifications in Airflow, using Jinja templating.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_5\n\nLANGUAGE: html\nCODE:\n```\nTry {{try_number}} out of {{max_tries + 1}}<br>\nException:<br>{{exception_html}}<br>\nLog: <a href=\"{{ti.log_url}}\">Link</a><br>\nHost: {{ti.hostname}}<br>\nMark success: <a href=\"{{ti.mark_success_url}}\">Link</a><br>\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Add comment about versions updated by release manager\nDESCRIPTION: This text is a commit message summary indicating that a comment was added to clarify that certain version numbers are updated by the release manager, linked to pull request #37488.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_20\n\nLANGUAGE: plaintext\nCODE:\n```\nAdd comment about versions updated by release manager (#37488)\n```\n\n----------------------------------------\n\nTITLE: Package Requirements Table (PlainText)\nDESCRIPTION: This table lists the required Python packages and their version constraints for `apache-airflow-providers-http` version 5.2.2. It specifies the minimum `apache-airflow` version and the required versions or ranges for other libraries like `requests`, `requests-toolbelt`, `aiohttp`, and `asgiref`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n=====================  ====================\nPIP package            Version required\n=====================  ====================\n``apache-airflow``     ``>=2.9.0``\n``requests``           ``>=2.31.0,<3``\n``requests-toolbelt``  ``>=1.0.0``\n``aiohttp``            ``!=3.11.0,>=3.9.2``\n``asgiref``            ``>=2.3.0``\n=====================  ====================\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response Format\nDESCRIPTION: Demonstrates the snake_case field naming convention used in API responses.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"string\",\n    \"slots\": 0,\n    \"occupied_slots\": 0,\n    \"used_slots\": 0,\n    \"queued_slots\": 0,\n    \"open_slots\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenLineage with External YAML File in airflow.cfg\nDESCRIPTION: Configuration in airflow.cfg that points to an external YAML file containing the OpenLineage configuration details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openlineage/docs/guides/user.rst#2025-04-22_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[openlineage]\nconfig_path = '/path/to/openlineage.yml'\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit with Pipx\nDESCRIPTION: Reinstalls pre-commit hooks using Pipx to resolve Python version compatibility issues.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/04_troubleshooting.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npipx uninstall pre-commit\npipx install pre-commit --python $(which python3.9) --force\npipx inject pre-commit pre-commit-uv\npre-commit clean\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Displaying Parameter Validation Error for EMR Connection - Documentation - text\nDESCRIPTION: Demonstrates the specific error message users receive when an unsupported parameter (such as 'region_name') is passed to the RunJobFlow API through the EMR connection in Airflow. This snippet provides guidance on acceptable parameters and reinforces correct usage by showing the exact wording of the validation failure. No dependencies are required, and it highlights the limitation that only listed API parameters are allowed; non-standard entries will result in failure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/connections/emr.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nParameter validation failed: Unknown parameter in input: \"region_name\", must be one of:\n```\n\n----------------------------------------\n\nTITLE: Setting Up Permanent Airflow CLI Completion\nDESCRIPTION: Command to enable permanent Airflow-specific command completion in bash\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/usage-cli.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nregister-python-argcomplete airflow >> ~/.bashrc\n```\n\n----------------------------------------\n\nTITLE: Defining Providers Compatibility Test Matrix (JSON Array)\nDESCRIPTION: Specifies the matrix for providers compatibility tests, likely combinations of Python version, Airflow version, and removed providers. This example shows an empty configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n\\[{}\\]\n```\n\n----------------------------------------\n\nTITLE: Publishing Amazon Provider Documentation\nDESCRIPTION: Example of publishing documentation specifically for the Amazon provider.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management publish-docs amazon\n```\n\n----------------------------------------\n\nTITLE: Batch Translating Documents with TranslateDocumentBatchOperator in Python\nDESCRIPTION: Shows how to use `TranslateDocumentBatchOperator` in an Airflow DAG for batch document translation using the Cloud Translate V3 API. Requires `project_id`, `location`, `target_language_codes`, `source_language_code`, `input_configs` (list of input document sources), and `output_config` (specifying batch output destination).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntranslate_document_batch = TranslateDocumentBatchOperator(\n    task_id=\"translate_document_batch\",\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n    source_language_code=SOURCE_LANG,\n    target_language_codes=[TARGET_LANG],\n    input_configs=INPUT_CONFIGS,\n    output_config=OUTPUT_CONFIG,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining GitHub Actions Workflow for Test Coverage in YAML\nDESCRIPTION: This YAML configuration defines a GitHub Actions workflow named 'Test Coverage'. It specifies when the workflow should run, sets up the environment, and executes test coverage using pytest.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/impala/docs/.latest-doc-only-change.txt#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: Test Coverage\n\non:\n  pull_request:\n    branches: [ main ]\n  push:\n    branches: [ main ]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: '3.x'\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n    - name: Run coverage\n      run: |\n        pytest --cov=./ --cov-report=xml\n    - name: Upload coverage to Codecov\n      uses: codecov/codecov-action@v1\n      with:\n        file: ./coverage.xml\n        flags: unittests\n        name: codecov-umbrella\n        fail_ci_if_error: true\n```\n\n----------------------------------------\n\nTITLE: Including External File using reStructuredText Directive\nDESCRIPTION: This reStructuredText snippet uses the 'include' directive to incorporate the content of the file located at '/../../../devel-common/src/sphinx_exts/includes/security.rst'. This is a standard mechanism in Sphinx/reStructuredText documentation to manage and reuse content modularly, specifically for security information in this context. The successful inclusion depends on the Sphinx build environment and the existence of the target file at the specified relative path.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/elasticsearch/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Running Provider Unit Tests with Pytest in Breeze (Bash)\nDESCRIPTION: Demonstrates how to run specific unit tests for a provider's hook component from within the Breeze development environment. It uses `pytest` to execute tests located in the provider's test directory. Assumes the Breeze environment is active and the provider structure exists. `<PROVIDER>` needs replacement.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/MANAGING_PROVIDERS_LIFECYCLE.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nroot@fafd8d630e46:/opt/airflow# python -m pytest providers/<PROVIDER>/tests/<PROVIDER>/hook/test_*.py\n```\n\n----------------------------------------\n\nTITLE: Defining a Teradata Table Creation SQL Script\nDESCRIPTION: This SQL script defines the structure for a `Users` table in Teradata, including columns `username` (varchar 50) and `description` (varchar 256). It uses the `FALLBACK` option for data protection. This script is intended to be saved in an external file (e.g., `create_table.sql`) and executed via the `TeradataOperator` in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/teradata/docs/operators/teradata.rst#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- create Users table\nCREATE TABLE Users, FALLBACK (\n  username   varchar(50),\n  description           varchar(256)\n);\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Updated Changelog After Bugfix Release in RST\nDESCRIPTION: This commit (cbf8001d76, committed on 2021-06-16) synchronizes the changelog file after a bugfix release has occurred. Refers to issue #16464.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_27\n\nLANGUAGE: rst\nCODE:\n```\nSynchronizes updated changelog after buggfix release (#16464)\n```\n\n----------------------------------------\n\nTITLE: Updating S3 List Operation in Python\nDESCRIPTION: Update to the s3_list.py file, likely enhancing or fixing the S3 listing functionality.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nUpdate s3_list.py (#18561)\n```\n\n----------------------------------------\n\nTITLE: Installing Snowflake Provider with pip - Bash\nDESCRIPTION: This code installs the Apache Airflow Snowflake provider package using pip. It is required to run Snowpark tasks with Airflow. Usage assumes pip is available in the environment, and the install command must be run in a shell or terminal before SnowparkOperator features can be used in DAGs. Input: None. Output: Installs Python package in the environment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/operators/snowpark.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow-providers-snowflake'\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Core without Providers\nDESCRIPTION: This script installs only the Apache Airflow core package without any providers, using a special 'no-providers' constraints file. It determines the appropriate constraints file based on the Airflow version and Python version.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/installing-from-pypi.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nAIRFLOW_VERSION=|version|\nPYTHON_VERSION=\"$(python -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")')\"\n# For example: 3.9\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-no-providers-${PYTHON_VERSION}.txt\"\n# For example: https://raw.githubusercontent.com/apache/airflow/constraints-|version|/constraints-no-providers-3.9.txt\npip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 1.0.1 and 1.0.0\nDESCRIPTION: Changelog entries for initial provider versions including deprecation notice and initial release\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sendgrid/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n1.0.1\n.....\n\nUpdated documentation and readme files.\n\n* ``Deprecate email credentials from environment variables. (#13601)``\n\n1.0.0\n.....\n\nInitial version of the provider.\n```\n\n----------------------------------------\n\nTITLE: Inserting Conversions in Google Campaign Manager via Airflow\nDESCRIPTION: Example of using GoogleCampaignManagerBatchInsertConversionsOperator to insert Campaign Manager conversions. The operator supports Jinja templating and saves results to XCom for use by other operators.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/marketing_platform/campaign_manager.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninsert_conversion = GoogleCampaignManagerBatchInsertConversionsOperator(\n    profile_id=PROFILE_ID,\n    conversions=[\n        {\n            \"conversion\": {\n                \"floodlightActivityId\": 1234,\n                \"floodlightConfigurationId\": 1234,\n                \"gclid\": \"KJH3S499-DSFKM-MV35-CSADG-5WAEG\",\n                \"ordinal\": \"0\",\n                \"quantity\": 1,\n                \"timestampMicros\": 1639410000000000,\n                \"value\": 5.0,\n            }\n        }\n    ],\n    encryption_entity_type=\"CONVERSION\",\n    encryption_entity_id=10,\n    encryption_source=\"ADWORDS\",\n    task_id=\"insert_conversion\",\n)\n```\n\n----------------------------------------\n\nTITLE: Transferring GCS Files to BigQuery Using Async Operator\nDESCRIPTION: Example demonstrating the deferrable mode of GCSToBigQueryOperator for asynchronous data loading from GCS to BigQuery, which helps with Airflow task scheduling efficiency.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_bigquery.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ngcs_to_bigquery_async = GCSToBigQueryOperator(\n    task_id=\"gcs_to_bigquery_async\",\n    bucket=\"cloud-samples-data\",\n    source_objects=[\"bigquery/us-states/us-states.csv\"],\n    destination_project_dataset_table=\"airflow_test.gcs_to_bq_table\",\n    schema_fields=[\n        {\"name\": \"name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n        {\"name\": \"post_abbr\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n    ],\n    write_disposition=\"WRITE_TRUNCATE\",\n    deferrable=True\n)\n```\n\n----------------------------------------\n\nTITLE: Making Authenticated API Request with Google Identity Token\nDESCRIPTION: This bash script demonstrates how to make an authenticated API request to Airflow using a Google identity token. It sets up the endpoint URL and audience, obtains an ID token, and uses curl to make a GET request to the Airflow API.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/api-auth-backend/google-openid.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nENDPOINT_URL=\"http://localhost:8080/\"\n\nAUDIENCE=\"project-id-random-value.apps.googleusercontent.com\"\nID_TOKEN=\"$(gcloud auth print-identity-token \"--audiences=${AUDIENCE}\")\"\n\ncurl -X GET  \\\n    \"${ENDPOINT_URL}/api/experimental/pools\" \\\n    -H 'Content-Type: application/json' \\\n    -H 'Cache-Control: no-cache' \\\n    -H \"Authorization: Bearer ${ID_TOKEN}\"\n```\n\n----------------------------------------\n\nTITLE: Generating Configuration Documentation with Jinja in Apache Airflow\nDESCRIPTION: This Jinja template generates documentation for all available configurations of a specific Airflow provider package. It includes notes about configuration usage in different Airflow versions and links to additional configuration documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n.. jinja:: config_ctx\n\n  This page contains the list of all available Airflow configurations for the\n  ``{{ package_name }}`` provider that can be set in the ``airflow.cfg`` file or using environment variables.\n\n  .. note::\n    The configuration embedded in providers started to be used as of Airflow 2.7.0. Previously the\n    configuration was described and configured in the Airflow core package - so if you are using Airflow\n    below 2.7.0, look at Airflow documentation for the list of available configuration options\n    that were available in Airflow core.\n\n  .. note::\n     For more information see :doc:`apache-airflow:howto/set-config`.\n```\n\n----------------------------------------\n\nTITLE: Terminating an EMR Job Flow using EmrTerminateJobFlowOperator in Python\nDESCRIPTION: Shows the usage of `EmrTerminateJobFlowOperator` to terminate a specified EMR cluster. It requires the `job_flow_id` of the cluster to be terminated and the `aws_conn_id`. The `task_id` identifies this termination task. Deferrable mode (`deferrable=True`) is available for this operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/emr.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nterminate_job_flow_task = EmrTerminateJobFlowOperator(\n    task_id=\"terminate_job_flow_task\",\n    job_flow_id=cluster_id,\n    aws_conn_id=\"aws_default\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Unit Test for DAG Structure Validation\nDESCRIPTION: A test function that verifies the structure of a code-generated DAG against a dictionary representing the expected task dependencies. It ensures tasks and their downstream relationships match the expected configuration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef assert_dag_dict_equal(source, dag):\n    assert dag.task_dict.keys() == source.keys()\n    for task_id, downstream_list in source.items():\n        assert dag.has_task(task_id)\n        task = dag.get_task(task_id)\n        assert task.downstream_task_ids == set(downstream_list)\n\n\ndef test_dag():\n    assert_dag_dict_equal(\n        {\n            \"DummyInstruction_0\": [\"DummyInstruction_1\"],\n            \"DummyInstruction_1\": [\"DummyInstruction_2\"],\n            \"DummyInstruction_2\": [\"DummyInstruction_3\"],\n            \"DummyInstruction_3\": [],\n        },\n        dag,\n    )\n```\n\n----------------------------------------\n\nTITLE: Specifying Default Runner Labels (JSON Array)\nDESCRIPTION: Defines the list of labels assigned to select runners for default CI jobs. This example specifies 'ubuntu-22.04'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n\\[\"ubuntu-22.04\"\\]\n```\n\n----------------------------------------\n\nTITLE: Listing Testable Core Integrations (Python List)\nDESCRIPTION: Defines the list of core Airflow integrations (e.g., executors, authentication backends) that are considered testable in the current build environment. This example includes 'celery' and 'kerberos'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n\\['celery', 'kerberos'\\]\n```\n\n----------------------------------------\n\nTITLE: Testing Task Relationships in Airflow DAGs with Python\nDESCRIPTION: This snippet defines a pytest fixture for creating a DAG and tests various task relationship scenarios, including direct dependencies, cross-DAG dependencies, and task group relationships.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/docs/.latest-doc-only-change.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport datetime\n\nimport pendulum\nimport pytest\nfrom freezegun import freeze_time\n\nfrom airflow.models.dag import DAG\nfrom airflow.models.dagrun import DagRun\nfrom airflow.models.taskinstance import TaskInstance as TI\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.session import create_session\nfrom airflow.utils.state import State\nfrom airflow.utils.types import DagRunType\n\nDAG_ID = 'test_dag'\nTASK_ID = 'base_task'\nDT = datetime.datetime(2016, 1, 1)\nDAGRUN_ID = 'test_dagrun'\n\n\n@pytest.fixture()\ndef dag():\n    with DAG(DAG_ID, start_date=DT) as dag:\n        task = EmptyOperator(task_id=TASK_ID)\n    return dag\n\n\ndef test_task_naive_datetime(dag):\n    \"\"\"Check that naive datetime is converted to tz-aware datetime\"\"\"\n    naive_datetime = datetime.datetime.utcnow()\n    di = TI(task=dag.get_task(TASK_ID), execution_date=naive_datetime)\n    assert di.execution_date == pendulum.instance(naive_datetime)\n\n\ndef test_task_with_start_end_dates(dag):\n    \"\"\"Check that tasks with non-None start_dates and end_dates are handled properly\"\"\"\n    task = dag.get_task(TASK_ID)\n    task.start_date = datetime.datetime(2016, 2, 1, 0, 0, 0)\n    task.end_date = datetime.datetime(2016, 3, 1, 0, 0, 0)\n\n    di = TI(task=task, execution_date=datetime.datetime(2016, 1, 1, 0, 0, 0))\n    assert di.execution_date == datetime.datetime(2016, 2, 1, 0, 0, 0)\n\n    di = TI(task=task, execution_date=datetime.datetime(2016, 3, 1, 0, 0, 1))\n    assert di.execution_date == datetime.datetime(2016, 3, 1, 0, 0, 0)\n\n\ndef test_task_with_dag_run(dag):\n    \"\"\"Check that task instances are properly associated with DAG runs\"\"\"\n    dag_run = DagRun(\n        dag_id=DAG_ID,\n        run_id=DAGRUN_ID,\n        run_type=DagRunType.SCHEDULED,\n        execution_date=DT,\n        start_date=DT,\n        state=State.RUNNING,\n    )\n    with create_session() as session:\n        session.add(dag_run)\n        session.commit()\n\n    task = dag.get_task(TASK_ID)\n    ti = TI(task=task, execution_date=DT)\n    ti.dag_run = dag_run\n\n    assert ti.get_dagrun() == dag_run\n\n\ndef test_set_dag_run(dag):\n    \"\"\"Test setting DAG run for a task instance\"\"\"\n    dr = DagRun(\n        dag_id=DAG_ID,\n        run_id=DAGRUN_ID,\n        run_type=DagRunType.SCHEDULED,\n        execution_date=DT,\n        state=State.RUNNING,\n    )\n    task = dag.get_task(TASK_ID)\n    ti = TI(task=task, execution_date=DT)\n    ti.dag_run = dr\n\n    assert ti._dag_run is dr\n    assert ti.get_dagrun() == dr\n\n\ndef test_set_dag_run_with_none(dag):\n    \"\"\"Test setting DAG run to None for a task instance\"\"\"\n    task = dag.get_task(TASK_ID)\n    ti = TI(task=task, execution_date=DT)\n    ti.dag_run = None\n\n    assert ti._dag_run is None\n    assert ti.get_dagrun() is None\n\n\ndef test_timezone(dag):\n    \"\"\"Test handling of timezones in task instances\"\"\"\n    assert dag.timezone.name == 'UTC'\n\n    with freeze_time('2020-01-01 00:00:00'):\n        di = TI(task=dag.get_task(TASK_ID), execution_date=DT)\n        assert di.execution_date == DT\n```\n\n----------------------------------------\n\nTITLE: Updating Azure Container Instance Operator in Python\nDESCRIPTION: Adds managed identity support to the Azure Container Instance hook.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfeat(provider/azure): add managed identity support to container_instance hook (#35319)\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Checks on Files Modified in Last Commit in Bash\nDESCRIPTION: This command runs all pre-commit checks only on files that were modified in the last locally available commit.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --source=HEAD^ --origin=HEAD\n```\n\n----------------------------------------\n\nTITLE: Declaring apache.beam Extra Dependency in Google Provider Setup (Python)\nDESCRIPTION: This Python code snippet shows the definition of an optional dependency extra named 'apache.beam' within the setup configuration (likely setup.py or setup.cfg) for the Apache Airflow Google provider. Installing this extra (`pip install apache-airflow-providers-google[apache.beam]`) pulls in the `apache-airflow-providers-apache-beam` package and the `apache-beam[gcp]` dependencies. This was introduced in version 3.0.0 to manage potential dependency conflicts between the Google and Apache Beam providers, specifically regarding Google Cloud client libraries.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nextras_require = (\n    {\n        # ...\n        \"apache.beam\": [\"apache-airflow-providers-apache-beam\", \"apache-beam[gcp]\"],\n        # ...\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Version 7.9.0 Changelog Entries\nDESCRIPTION: List of commits and changes for version 7.9.0 of the Kubernetes provider, including improvements to logging, pod management and configuration updates.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n7.9.0\n.....\n\nLatest change: 2023-11-08\n\nCommit                                                                                              Committed    Subject\n--------------------------------------------------------------------------------------------------  -----------  -------------------------------------------------------------------------------\n`1b059c57d6 <https://github.com/apache/airflow/commit/1b059c57d6d57d198463e5388138bee8a08591b1>`__  2023-11-08   \"Prepare docs 1st wave of Providers November 2023 (#35537)\"\n```\n\n----------------------------------------\n\nTITLE: Using CloudTranslateTextOperator for Basic Translation in Python\nDESCRIPTION: Demonstrates the basic usage of the `CloudTranslateTextOperator` in an Airflow DAG. This snippet shows how to translate a list of strings ('Hello World', 'Good day') from English ('en') to French ('fr') using the 'base' model via the Google Cloud Translate API V2.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntranslate_text_task = CloudTranslateTextOperator(\n    task_id=\"translate_text_task\",\n    values=[\"Hello World\", \"Good day\"],\n    target_language=\"fr\",\n    format_=\"text\",\n    source_language=\"en\",\n    model=\"base\",\n)\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Distributions with Local Hatch - Shell Script\nDESCRIPTION: This shell script builds Airflow distributions using a local 'hatch'-based Docker environment by appending the '--use-local-hatch' flag. It produces both binary and source distributions quickly, requiring less bandwidth than the standard Docker approach. This technique assumes 'breeze', Docker, and 'hatch' are installed. The output consists of built artifact files in the 'dist/' directory.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nbreeze release-management prepare-airflow-distributions --distribution-format both --use-local-hatch\nbreeze release-management prepare-airflow-tarball --version ${VERSION}\n```\n\n----------------------------------------\n\nTITLE: Moving a Single File in GCS Using GCSToGCSOperator\nDESCRIPTION: Example demonstrating how to move (copy and delete) a single file from one GCS bucket to another. The move parameter set to True causes the operator to delete the source object after copying is complete.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gcs.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmove_single_file = GCSToGCSOperator(\n    task_id=\"gcs_to_gcs_single_file_move\",\n    source_bucket=BUCKET_1_SRC,\n    source_object=OBJECT_1,\n    destination_bucket=BUCKET_1_DST,\n    move=True,\n    exact_match=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare docs 1st wave of Providers January 2024\nDESCRIPTION: This text is a commit message summary for preparing documentation related to the first wave of Apache Airflow provider releases in January 2024, linked to pull request #36640.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_26\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 1st wave of Providers January 2024 (#36640)\n```\n\n----------------------------------------\n\nTITLE: Storing Connection Information in JSON Format for Apache Airflow\nDESCRIPTION: This is a JSON representation of a MySQL connection in Apache Airflow. It contains connection details including host, credentials, and additional parameters stored in extra_dejson.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n[{\"id\": null, \"conn_id\": \"mysqldb\", \"conn_type\": \"mysql\", \"description\": null, \"host\": \"host.com\", \"schema\": \"\", \"login\": \"myname\", \"password\": \"mypassword\", \"port\": null, \"is_encrypted\": \"False\", \"is_extra_encrypted\": \"False\", \"extra_dejson\": {\"this_param\": \"some val\", \"that_param\": \"other val*\"}, \"get_uri\": \"mysql://myname:mypassword@myhost.com/?this_param=some+val&that_param=other+val%2A\"}]\n```\n\n----------------------------------------\n\nTITLE: Importing Data to AutoML Dataset with TranslateImportDataOperator in Python\nDESCRIPTION: Shows how to use the `TranslateImportDataOperator` in an Airflow DAG to import data into an existing AutoML translation dataset via the V3 API. It requires the `dataset_id`, `project_id`, `location`, and an `import_config` dictionary specifying the GCS source URI.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport_data = TranslateImportDataOperator(\n    task_id=\"import_data\",\n    dataset_id=DATASET_ID,\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n    import_config=IMPORT_CONFIG,\n)\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment\nDESCRIPTION: Standard Apache 2.0 License header used in Apache Airflow project files to specify licensing terms and conditions.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/dockerfiles/README.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n```\n\n----------------------------------------\n\nTITLE: Updating Cloud Memorystore Instance\nDESCRIPTION: Example showing how to update a Cloud Memorystore instance configuration\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nupdate_instance = CloudMemorystoreUpdateInstanceOperator(task_id=\"update-instance\", location=\"europe-north1\", instance=INSTANCE_NAME, project_id=PROJECT_ID, update_mask={\"paths\": [\"memory_size_gb\"]}, instance_update={\"memory_size_gb\": 6})\n```\n\n----------------------------------------\n\nTITLE: Executing Airflow Commands in Docker Container\nDESCRIPTION: Illustrates how to run Airflow-specific commands within the Docker container using the default entrypoint.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it apache/airflow:3.1.0.dev0-python3.9 airflow webserver\n```\n\n----------------------------------------\n\nTITLE: Waiting on AWS Batch Compute Environment Status using BatchComputeEnvironmentSensor in Python\nDESCRIPTION: Demonstrates the use of the Airflow BatchComputeEnvironmentSensor (:class:`~airflow.providers.amazon.aws.sensors.batch.BatchComputeEnvironmentSensor`) to wait until an AWS Batch compute environment reaches a specific terminal status (e.g., VALID, INVALID). This is useful for ensuring an environment is ready before submitting jobs or after modifications.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/batch.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../amazon/tests/system/amazon/aws/example_batch.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_sensor_batch_compute_environment]\n    :end-before: [END howto_sensor_batch_compute_environment]\n```\n\n----------------------------------------\n\nTITLE: Deleting a Specific Model Version using Vertex AI Model Service Operator - Python\nDESCRIPTION: Shows how to delete a specific version of a Vertex AI model using DeleteModelVersionOperator. This operator needs the project, region, model, and version IDs. Upon task completion, the designated model version is permanently deleted.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ndelete_version_task = DeleteModelVersionOperator(\n    task_id=\"delete_model_version_vertex_ai\",\n    project_id=GCP_PROJECT_ID,\n    region=REGION,\n    model_id=MODEL_ID,\n    version_id=VERSION_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Parameterized SQL Query with Jinja Templating (SQL)\nDESCRIPTION: SQL script, intended to be saved as `dags/sql/birth_date.sql`, demonstrating parameterization using Airflow's Jinja templating. It selects pets based on a birth date range specified by `params.begin_date` and `params.end_date` passed via the `params` attribute in the operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/postgres/docs/operators.rst#2025-04-22_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n-- dags/sql/birth_date.sql\nSELECT * FROM pet WHERE birth_date BETWEEN SYMMETRIC {{ params.begin_date }} AND {{ params.end_date }};\n```\n\n----------------------------------------\n\nTITLE: Deleting Spanner Instance in Python\nDESCRIPTION: Example of using SpannerDeleteInstanceOperator to delete a Cloud Spanner instance. The operator can be created with or without a project ID, which will be retrieved from the Google Cloud connection if not provided.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/spanner.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_spanner_delete]\n# Without project_id\ndelete_instance = SpannerDeleteInstanceOperator(\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    task_id=\"delete_instance\",\n)\n\n# With project_id\ndelete_instance = SpannerDeleteInstanceOperator(\n    project_id=\"{{ var.value.spanner_project }}\",\n    instance_id=\"{{ task.__class__.__name__}}_{{ ds_nodash }}\",\n    task_id=\"delete_instance\",\n)\n# [END howto_operator_spanner_delete]\n```\n\n----------------------------------------\n\nTITLE: Specifying ASF Self-Hosted Runner Labels (JSON Array)\nDESCRIPTION: Defines the list of labels assigned to select Apache Software Foundation (ASF) self-hosted runners. This example specifies 'self-hosted', 'Linux', and 'X64'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n\\[\"self-hosted\", \"Linux\", \"X64\"\\]\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit History in Markdown\nDESCRIPTION: This snippet shows the format used to display commit history in a Markdown table. It includes commit hash links, dates, and commit messages.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/imap/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n`4d03e33c11 <https://github.com/apache/airflow/commit/4d03e33c115018e30fa413c42b16212481ad25cc>`__  2020-02-22   ``[AIRFLOW-6817] remove imports from 'airflow/__init__.py', replaced implicit imports with explicit imports, added entry to 'UPDATING.MD' - squashed/rebased (#7456)``\n`97a429f9d0 <https://github.com/apache/airflow/commit/97a429f9d0cf740c5698060ad55f11e93cb57b55>`__  2020-02-02   ``[AIRFLOW-6714] Remove magic comments about UTF-8 (#7338)``\n`cf141506a2 <https://github.com/apache/airflow/commit/cf141506a25dbba279b85500d781f7e056540721>`__  2020-02-02   ``[AIRFLOW-6708] Set unique logger names (#7330)``\n`9a04013b0e <https://github.com/apache/airflow/commit/9a04013b0e40b0d744ff4ac9f008491806d60df2>`__  2020-01-27   ``[AIRFLOW-6646][AIP-21] Move protocols classes to providers package (#7268)``\n==================================================================================================\n```\n\n----------------------------------------\n\nTITLE: Waiting on AWS Batch Job Queue Status using BatchJobQueueSensor in Python\nDESCRIPTION: Shows how to use the Airflow BatchJobQueueSensor (:class:`~airflow.providers.amazon.aws.sensors.batch.BatchJobQueueSensor`) to pause execution until an AWS Batch job queue reaches a desired terminal status (e.g., VALID, INVALID). This can be used to verify queue readiness or status after updates.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/batch.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. exampleinclude:: /../../amazon/tests/system/amazon/aws/example_batch.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_sensor_batch_job_queue]\n    :end-before: [END howto_sensor_batch_job_queue]\n```\n\n----------------------------------------\n\nTITLE: Update JDBC Example DAGs\nDESCRIPTION: Migrate JDBC example DAGs to new design and fix links to sources for examples.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"Migrate JDBC example DAGs to new design #22450 (#24137)\"\n\"Fix links to sources for examples (#24386)\"\n```\n\n----------------------------------------\n\nTITLE: Updating Airflow Version in Dockerfile\nDESCRIPTION: This snippet updates the Airflow version in a Dockerfile. It changes the AIRFLOW_VERSION argument to 2.6.3, which will likely be used to install or configure Airflow in the Docker image.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/docs/.latest-doc-only-change.txt#2025-04-22_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nARG AIRFLOW_VERSION=\"2.6.3\"\n```\n\n----------------------------------------\n\nTITLE: Applying PEP-563 to Non-Core Airflow Code\nDESCRIPTION: Excluded Change (Version 3.1.0): Applies PEP-563 (Postponed Evaluation of Annotations) to non-core Airflow components, including providers, referencing pull request #26289. This impacts how type hints are evaluated.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Apply PEP-563 (Postponed Evaluation of Annotations) to non-core airflow (#26289)``\n```\n\n----------------------------------------\n\nTITLE: Indicating Only New UI Files Changed (Boolean String)\nDESCRIPTION: A boolean flag indicating if only new UI files (not existing ones) were modified in the Pull Request. 'false' suggests other files or existing UI files were changed.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nfalse\n```\n\n----------------------------------------\n\nTITLE: Sensor Deprecations in Google Provider\nDESCRIPTION: Details removed sensor classes and their replacements, particularly focusing on BigQuery, GCS, and Google services related sensors.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nBigQueryTableExistenceAsyncSensor -> BigQueryTableExistenceSensor (with deferrable=True)\nBigQueryTableExistencePartitionAsyncSensor -> BigQueryTablePartitionExistenceSensor (with deferrable=True)\nCloudComposerEnvironmentSensor -> CloudComposerCreateEnvironmentOperator\n```\n\n----------------------------------------\n\nTITLE: Deprecated Airflow Configuration Values\nDESCRIPTION: List of configuration values that were previously auto-updated in Airflow 2 but now need manual updates. These include settings for core hostname, email backend, logging filename template, and elasticsearch log ID template.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/47761.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nhostname = :\n\n[email]\nemail_backend = airflow.contrib.utils.sendgrid.send_email\n\n[logging]\nlog_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log\n\n[elasticsearch]\nlog_id_template = {dag_id}-{task_id}-{logical_date}-{try_number}\n```\n\n----------------------------------------\n\nTITLE: Copying Files Without Wildcards in GCSToGCSOperator\nDESCRIPTION: Example demonstrating how to copy all files in a subdirectory from one GCS bucket to another without using wildcards. This approach copies all files from the source directory to the destination directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_gcs.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncopy_files_without_wildcard = GCSToGCSOperator(\n    task_id=\"gcs_to_gcs_without_wildcard\",\n    source_bucket=BUCKET_1_SRC,\n    source_object=\"subdir/\",\n    destination_bucket=BUCKET_1_DST,\n    destination_object=\"backup/\",\n)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Allow and prefer non-prefixed extra fields for GrpcHook (#27045)\nDESCRIPTION: This commit message, associated with version 3.1.0, updates the GrpcHook to allow and prefer connection extra fields without the 'extra__' prefix. Commit hash: 4f9398fb3c, Date: 2022-10-22.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_22\n\nLANGUAGE: text\nCODE:\n```\n``Allow and prefer non-prefixed extra fields for GrpcHook (#27045)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Move provider dependencies to inside provider folders (#24672)\nDESCRIPTION: This commit message, associated with version 3.1.0, refactors the project structure by relocating provider-specific dependencies into their respective provider folders. Commit hash: 0de31bd73a, Date: 2022-06-29.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_27\n\nLANGUAGE: text\nCODE:\n```\n``Move provider dependencies to inside provider folders (#24672)``\n```\n\n----------------------------------------\n\nTITLE: Copying a Directory from GCS to SFTP using Wildcards in Python\nDESCRIPTION: This example demonstrates how to copy an entire directory from Google Cloud Storage to an SFTP server by using wildcards in the source_path parameter. This allows copying multiple files that match the wildcard pattern to the specified destination path.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_sftp.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncopy_directory_from_gcs_to_sftp = GCSToSFTPOperator(\n    task_id=\"copy-directory-from-gcs-to-sftp\",\n    source_bucket=BUCKET_NAME,\n    source_object=GCS_SRC_PATH + \"/*\",\n    destination_path=SFTP_DST_PATH,\n    move_object=False,\n    sftp_conn_id=\"sftp_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Slack Announcement Template\nDESCRIPTION: Template for announcing the new release in the Airflow community Slack channel.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_31\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\nWe've just released Apache Airflow $VERSION \n\n PyPI: https://pypi.org/project/apache-airflow/$VERSION/\n Docs: https://airflow.apache.org/docs/apache-airflow/$VERSION/\n Release Notes: https://airflow.apache.org/docs/apache-airflow/$VERSION/release_notes.html\n Docker Image: \"docker pull apache/airflow:$VERSION\"\n Constraints: https://github.com/apache/airflow/tree/constraints-$VERSION\n\nThanks to all the contributors who made this possible.\nEOF\n```\n\n----------------------------------------\n\nTITLE: Displaying Version History for Apache Cassandra Provider\nDESCRIPTION: This code snippet shows version history entries for the Apache Cassandra provider, including version numbers, release dates, and commit details for each version.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/cassandra/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n3.5.0\n.....\n\nLatest change: 2024-05-01\n\n==================================================================================================  ===========  =======================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =======================================================================\n`fe4605a10e <https://github.com/apache/airflow/commit/fe4605a10e26f1b8a180979ba5765d1cb7fb0111>`__  2024-05-01   ``Prepare docs 1st wave May 2024 (#39328)``\n`ead9b00f7c <https://github.com/apache/airflow/commit/ead9b00f7cd5acecf9d575c459bb62633088436a>`__  2024-04-25   ``Bump minimum Airflow version in providers to Airflow 2.7.0 (#39240)``\n==================================================================================================  ===========  =======================================================================\n```\n\n----------------------------------------\n\nTITLE: Pushing Helm Chart Release Candidate Tag in Shell\nDESCRIPTION: Navigates to the Airflow repository root and pushes the release candidate tag (e.g., `helm-chart/1.5.0rc1`) to the 'apache' remote repository. Assumes `${AIRFLOW_REPO_ROOT}`, `${VERSION}`, and `${VERSION_SUFFIX}` environment variables are set.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ncd ${AIRFLOW_REPO_ROOT}\ngit push apache tag helm-chart/${VERSION}${VERSION_SUFFIX}\n```\n\n----------------------------------------\n\nTITLE: Listing Documentation File Paths for Apache Airflow\nDESCRIPTION: This snippet lists file paths for Apache Airflow documentation. It includes paths for API authentication backends and secrets backends, with a focus on Google-related services.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/redirects.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\napi-auth-backend/index.rst api-auth-backend/google-openid.rst\nsecrets-backends/index.rst secrets-backends/google-cloud-secret-manager-backend.rst\n```\n\n----------------------------------------\n\nTITLE: Uploading SQL Data to Google Sheets using SQLToGoogleSheetsOperator in Python\nDESCRIPTION: This code snippet demonstrates how to use the SQLToGoogleSheetsOperator to transfer data from a SQL database to Google Sheets. It includes configuration for the SQL connection, Google Sheets destination, and various operator parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/sql_to_sheets.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSQLToGoogleSheetsOperator(\n    task_id=\"upload_sql_to_sheets\",\n    sql=SQL_QUERY,\n    sql_conn_id=SQL_CONNECTION_ID,\n    spreadsheet_id=SPREADSHEET_ID,\n    sheet_range=\"Sheet1!A1\",\n    spreadsheet_conn_id=GOOGLE_SHEETS_CONN_ID,\n    sheet_title=\"Sheet1\",\n    overwrite_sheet=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: pRepare docs for November 2022 wave of Providers (#27613)\nDESCRIPTION: This commit message, associated with version 3.1.0, signifies the preparation of documentation for the November 2022 release wave of Apache Airflow Providers. Commit hash: 12c3c39d1a, Date: 2022-11-15.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_18\n\nLANGUAGE: text\nCODE:\n```\n``pRepare docs for November 2022 wave of Providers (#27613)``\n```\n\n----------------------------------------\n\nTITLE: Example Yandex.Cloud Service Account JSON Configuration\nDESCRIPTION: Provides an example JSON string containing the service account ID and private key used for authentication with Yandex.Cloud services via the Airflow connection. This JSON is typically obtained when creating an authorized key for a Yandex.Cloud service account and is entered directly into the 'Service account auth JSON' field.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/connections/yandexcloud.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"id\": \"...\", \"service_account_id\": \"...\", \"private_key\": \"...\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Singularity Container Support for Apache Airflow\nDESCRIPTION: Command to install Singularity container operator for Apache Airflow. This enables working with Singularity containers within Airflow workflows.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_51\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[singularity]'\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow to Use Lockbox Secrets for Configuration\nDESCRIPTION: This INI configuration shows how to reference a secret value in the Airflow configuration by specifying the config value name with a '_secret' suffix.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_18\n\nLANGUAGE: ini\nCODE:\n```\n[sentry]\nsentry_dsn_secret = sentry_dsn_value\n```\n\n----------------------------------------\n\nTITLE: Building CI Docker Image with Additional Dev APT Dependencies (gcc, g++) - Docker/Bash\nDESCRIPTION: This bash command builds the CI Docker image, installing both 'gcc' and 'g++' as additional development APT dependencies by setting ADDITIONAL_DEV_APT_DEPS. This is useful for builds or tests requiring compilation or native extensions. Requires Docker and the Dockerfile.ci. Outputs a tagged local image.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build . -f Dockerfile.ci \\\n  --pull\n  --build-arg PYTHON_BASE_IMAGE=\"python:3.9-slim-bookworm\" \\\n  --build-arg ADDITIONAL_DEV_APT_DEPS=\"gcc g++\" --tag my-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Installing Databricks Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Databricks integration, enabling Databricks hooks and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[databricks]'\n```\n\n----------------------------------------\n\nTITLE: Importing Scrapbook in Python for Apache Airflow\nDESCRIPTION: This snippet imports the scrapbook library, which is used for saving and retrieving data in Jupyter notebooks. It's essential for the papermill integration in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/tests/system/papermill/input_notebook.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nimport scrapbook as sb\n```\n\n----------------------------------------\n\nTITLE: Initializing Airflow Development Environment using Breeze (Bash)\nDESCRIPTION: This command starts the Breeze environment, setting up Docker containers with local code mounted as volumes. This allows developers to run tests and apply changes within an environment similar to the CI workflow. Requires Breeze to be installed and configured in the Airflow source directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/MANAGING_PROVIDERS_LIFECYCLE.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./breeze\n```\n\n----------------------------------------\n\nTITLE: Creating Sensors with TaskFlow API\nDESCRIPTION: This snippet demonstrates how to create lightweight, reusable sensors using Python functions with the @task.sensor decorator. These sensors support both poke and reschedule modes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/taskflow.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pendulum import datetime\n\nfrom airflow.decorators import dag, task\n\n\n@dag(\n    schedule=None,\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef tutorial_taskflow_api_sensor():\n    @task.sensor(poke_interval=30, timeout=60 * 5, mode=\"poke\")\n    def wait_for_file():\n        from pathlib import Path\n\n        return Path(\"/tmp/wait_for_me.txt\").exists()\n\n    wait_for_file()\n\n\ntutorial_taskflow_api_sensor()\n```\n\n----------------------------------------\n\nTITLE: Formatting Commit History in Markdown\nDESCRIPTION: This snippet shows how commit history is formatted in a Markdown table, including commit hash, date, and subject. It uses fixed-width formatting for alignment.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: Markdown\nCODE:\n```\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`fe4605a10e <https://github.com/apache/airflow/commit/fe4605a10e26f1b8a180979ba5765d1cb7fb0111>`__  2024-05-01   ``Prepare docs 1st wave May 2024 (#39328)``\n`ead9b00f7c <https://github.com/apache/airflow/commit/ead9b00f7cd5acecf9d575c459bb62633088436a>`__  2024-04-25   ``Bump minimum Airflow version in providers to Airflow 2.7.0 (#39240)``\n`5fa80b6aea <https://github.com/apache/airflow/commit/5fa80b6aea60f93cdada66f160e2b54f723865ca>`__  2024-04-10   ``Prepare docs 1st wave (RC1) April 2024 (#38863)``\n```\n\n----------------------------------------\n\nTITLE: Listing Vertex AI Pipeline Jobs using Airflow Operator in Python\nDESCRIPTION: Demonstrates using the `ListPipelineJobOperator` from `airflow.providers.google.cloud.operators.vertex_ai.pipeline_job` to list pipeline jobs in Google Cloud Vertex AI. This operator would be used within an Airflow DAG context.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/vertex_ai/example_vertex_ai_pipeline_job.py\n    :language: python\n    :dedent: 4\n    :start-after: [START how_to_cloud_vertex_ai_list_pipeline_job_operator]\n    :end-before: [END how_to_cloud_vertex_ai_list_pipeline_job_operator]\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs for May 2023 wave of Providers (#31252)\nDESCRIPTION: This commit message, associated with version 3.2.0, signifies the preparation of documentation for the May 2023 release wave of Apache Airflow Providers. Commit hash: d9ff55cf6d, Date: 2023-05-16.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs for May 2023 wave of Providers (#31252)``\n```\n\n----------------------------------------\n\nTITLE: Extended Airflow Docker Configuration with Additional Requirements\nDESCRIPTION: Docker command demonstrating how to run Airflow with additional pip requirements, database migration, and user creation settings. Includes specific package versions for lxml and charset-normalizer.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it -p 8080:8080 \\\n    --env \"_PIP_ADDITIONAL_REQUIREMENTS=lxml==4.6.3 charset-normalizer==1.4.1\" \\\n    --env \"_AIRFLOW_DB_MIGRATE=true\" \\\n    --env \"_AIRFLOW_WWW_USER_CREATE=true\" \\\n    --env \"_AIRFLOW_WWW_USER_PASSWORD_CMD=echo admin\" \\\n      apache/airflow:3.1.0.dev0-python3.9 webserver\n```\n\n----------------------------------------\n\nTITLE: Git Commit Log Entries\nDESCRIPTION: A list of Git commit hashes, dates, and associated commit messages documenting changes to the Apache Airflow project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`71efa8e2c0 <https://github.com/apache/airflow/commit/71efa8e2c0d0ac20ac2a6102e8f8a1b374220c25>`__  2025-03-31   ``Fix signature of 'BatchWaitersHook.get_waiter' not matching parent class (#48581)``\n`ecc0e0e13c <https://github.com/apache/airflow/commit/ecc0e0e13c4a1031f228aeae30c2fbd04bed9b05>`__  2025-03-30   ``Fix failing eks tests with new moto 5.1.2 (#48556)``\n```\n\n----------------------------------------\n\nTITLE: Downloading File from GCS to Local Filesystem Using GCSToLocalFilesystemOperator in Python\nDESCRIPTION: This code snippet demonstrates how to create a task that downloads a file from Google Cloud Storage to a local filesystem using the GCSToLocalFilesystemOperator. The operator requires bucket and object names for the source, and a local file path for the destination.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/gcs_to_local.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndownload_file = GCSToLocalFilesystemOperator(\n    task_id=\"download_file\",\n    object_name=FILE_NAME,\n    bucket=BUCKET_NAME,\n    filename=LOCAL_PATH,\n)\n```\n\n----------------------------------------\n\nTITLE: Example Yandex.Cloud Service Account JSON File Path Configuration\nDESCRIPTION: Shows an example file path pointing to a JSON file containing Yandex.Cloud service account credentials (ID and private key). This path is entered into the 'Service account auth JSON file path' field as an alternative to providing the JSON content directly. Airflow requires read access to this file location.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/connections/yandexcloud.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n/home/airflow/authorized_key.json\n```\n\n----------------------------------------\n\nTITLE: Managing RC and Release Processes\nDESCRIPTION: Commands to automate steps for preparing release candidates and final releases.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management start-rc-process\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management start-release\n```\n\n----------------------------------------\n\nTITLE: Version History Table Formatting in RST\nDESCRIPTION: Restructured Text (RST) formatted table showing commit history with commit hashes, dates and descriptions for version 3.7.2 of the OpenFaaS provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/openfaas/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`cb295c351a <https://github.com/apache/airflow/commit/cb295c351a016c0a10cab07f2a628b865cff3ca3>`__  2025-04-14   ``remove superfluous else block (#49199)``\n`4a8567b20b <https://github.com/apache/airflow/commit/4a8567b20bdd6555cbdc936d6674bf4fa390b0d5>`__  2025-04-10   ``Prepare docs for Apr 2nd wave of providers (#49051)``\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Google Operators in reStructuredText\nDESCRIPTION: This snippet defines a table of contents in reStructuredText format for Google-related operators in Apache Airflow. It lists various Google services including Cloud, Firebase, Marketing Platform, G Suite, Ads, Transfer, and LevelDB.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n\n    cloud/index\n    firebase/firestore\n    marketing_platform/index\n    suite/sheets\n    ads\n    transfer/index\n    leveldb/leveldb\n```\n\n----------------------------------------\n\nTITLE: Listing Jobs on GKE using GKEListJobsOperator in Python\nDESCRIPTION: This snippet demonstrates using the `GKEListJobsOperator` to retrieve a list of existing Jobs on a GKE cluster. If a namespace is provided, it lists jobs within that namespace; otherwise, it lists jobs across all namespaces.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/kubernetes_engine.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/kubernetes_engine/example_kubernetes_engine_job.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_operator_gke_list_jobs]\n    :end-before: [END howto_operator_gke_list_jobs]\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Apply PEP-563 (Postponed Evaluation of Annotations) to non-core airflow (#26289)\nDESCRIPTION: This commit message, associated with version 3.1.0, applies PEP-563 ('from __future__ import annotations') to non-core Airflow components, including providers, to improve handling of type hints. Commit hash: 06acf40a43, Date: 2022-09-13.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_24\n\nLANGUAGE: text\nCODE:\n```\n``Apply PEP-563 (Postponed Evaluation of Annotations) to non-core airflow (#26289)``\n```\n\n----------------------------------------\n\nTITLE: Example of Provider Dependency Downgrade Output\nDESCRIPTION: Diff output showing example downgrades of dependencies for a provider (e.g., Google) when using UV sync. This includes both Airflow and provider-specific dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_40\n\nLANGUAGE: diff\nCODE:\n```\n- flask-login==0.6.3\n+ flask-login==0.6.2\n- flask-session==0.5.0\n+ flask-session==0.4.0\n- flask-wtf==1.2.1\n+ flask-wtf==1.1.0\n- fsspec==2023.12.2\n+ fsspec==2023.10.0\n- gcloud-aio-bigquery==7.1.0\n+ gcloud-aio-bigquery==6.1.2\n- gcloud-aio-storage==9.2.0\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare docs 1st wave July 2024\nDESCRIPTION: This text is a commit message summary indicating the preparation of documentation for the first wave of provider releases in July 2024, linked to pull request #40644.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 1st wave July 2024 (#40644)\n```\n\n----------------------------------------\n\nTITLE: Version 3.3.0 Feature Addition\nDESCRIPTION: RST changelog entry documenting the addition of SQLExecuteQueryOperator\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/mssql/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n* ``Add SQLExecuteQueryOperator (#25717)``\n```\n\n----------------------------------------\n\nTITLE: Building PROD Image from Local Distributions with Breeze (Bash)\nDESCRIPTION: Uses Breeze to build the Airflow PROD image for Python 3.9, adding the 'trino' extra. The `--install-distributions-from-context` flag instructs Breeze to install Airflow and its providers from local wheel files (.whl) located in the `docker-context-files` directory, instead of downloading them.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbreeze prod-image build --python 3.9 --additional-airflow-extras=trino --install-distributions-from-context\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare for follow-up relase for November providers (#27774)\nDESCRIPTION: This commit message, associated with version 3.2.0, describes preparations made for a follow-up release of the November Apache Airflow Providers. Commit hash: 2e20e9f7eb, Date: 2022-11-24.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_17\n\nLANGUAGE: text\nCODE:\n```\n``Prepare for follow-up relase for November providers (#27774)``\n```\n\n----------------------------------------\n\nTITLE: Translating a Single Document with TranslateDocumentOperator in Python\nDESCRIPTION: Illustrates using `TranslateDocumentOperator` in an Airflow DAG to translate a single document using the Cloud Translate V3 API. It requires `project_id`, `location`, `target_language_code`, `document_input_config` (specifying source document details), and `document_output_config` (specifying destination).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntranslate_document = TranslateDocumentOperator(\n    task_id=\"translate_document\",\n    project_id=GCP_PROJECT_ID,\n    location=GCP_LOCATION,\n    target_language_code=TARGET_LANG,\n    document_input_config=DOCUMENT_INPUT_CONFIG,\n    document_output_config=DOCUMENT_OUTPUT_CONFIG,\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Airflow Provider Commits (v3.1.0) using RST\nDESCRIPTION: This reStructuredText snippet presents a table logging commits for the Apache Airflow provider release wave around version 3.1.0 (June - November 2022). It includes linked commit hashes, dates, and subjects, highlighting documentation preparation for multiple monthly releases, bumping the minimum required Airflow version, enabling string normalization, applying PEP-563, moving provider dependencies, and fixing documentation links.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/winrm/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ====================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ====================================================================================\n`12c3c39d1a <https://github.com/apache/airflow/commit/12c3c39d1a816c99c626fe4c650e88cf7b1cc1bc>`__  2022-11-15   ``pRepare docs for November 2022 wave of Providers (#27613)``\n`78b8ea2f22 <https://github.com/apache/airflow/commit/78b8ea2f22239db3ef9976301234a66e50b47a94>`__  2022-10-24   ``Move min airflow version to 2.3.0 for all providers (#27196)``\n`2a34dc9e84 <https://github.com/apache/airflow/commit/2a34dc9e8470285b0ed2db71109ef4265e29688b>`__  2022-10-23   ``Enable string normalization in python formatting - providers (#27205)``\n`674f9ce6ea <https://github.com/apache/airflow/commit/674f9ce6eaae533cfe31bc92cc92fa75ed7223fc>`__  2022-10-01   ``A few docs fixups (#26788)``\n`f8db64c35c <https://github.com/apache/airflow/commit/f8db64c35c8589840591021a48901577cff39c07>`__  2022-09-28   ``Update docs for September Provider's release (#26731)``\n`06acf40a43 <https://github.com/apache/airflow/commit/06acf40a4337759797f666d5bb27a5a393b74fed>`__  2022-09-13   ``Apply PEP-563 (Postponed Evaluation of Annotations) to non-core airflow (#26289)``\n`e5ac6c7cfb <https://github.com/apache/airflow/commit/e5ac6c7cfb189c33e3b247f7d5aec59fe5e89a00>`__  2022-08-10   ``Prepare docs for new providers release (August 2022) (#25618)``\n`d2459a241b <https://github.com/apache/airflow/commit/d2459a241b54d596ebdb9d81637400279fff4f2d>`__  2022-07-13   ``Add documentation for July 2022 Provider's release (#25030)``\n`0de31bd73a <https://github.com/apache/airflow/commit/0de31bd73a8f41dded2907f0dee59dfa6c1ed7a1>`__  2022-06-29   ``Move provider dependencies to inside provider folders (#24672)``\n`08b675cf66 <https://github.com/apache/airflow/commit/08b675cf6642171cb1c5ddfb09607b541db70b29>`__  2022-06-13   ``Fix links to sources for examples (#24386)``\n==================================================================================================  ===========  ====================================================================================\n```\n\n----------------------------------------\n\nTITLE: Standard Directory Structure for Airflow Community Providers (Text)\nDESCRIPTION: Defines the recommended file and directory layout for a new community provider under `apache/airflow/providers/<PROVIDER>`. It includes locations for source code (`src`), tests (`tests` - unit, integration, system), configuration files (`pyproject.toml`, `provider.yaml`), and various component types like hooks, operators, sensors, etc. `<PROVIDER>` should be replaced with the actual provider name.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/MANAGING_PROVIDERS_LIFECYCLE.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nGIT apache/airflow/\n providers/\n              <PROVIDER>/\n                            pyproject.toml\n                            provider.yaml\n                            src/\n                               airflow/\n                                   providers/<PROVIDER>/\n                                                           __init__.py\n                                                           executors/\n                                                              __init__.py\n                                                              *.py\n                                                           hooks/\n                                                              __init__.py\n                                                              *.py\n                                                           notifications/\n                                                              __init__.py\n                                                              *.py\n                                                           operators/\n                                                              __init__.py\n                                                              *.py\n                                                           transfers/\n                                                              __init__.py\n                                                              *.py\n                                                           triggers/\n                                                               __init__.py\n                                                               *.py\n                            tests/\n                                     unit\\\n                                            <PROVIDER>/\n                                                          __init__.py\n                                                          executors/\n                                                             __init__.py\n                                                             test_*.py\n                                                          hooks/\n                                                             __init__.py\n                                                             test_*.py\n                                                          notifications/\n                                                             __init__.py\n                                                             test_*.py\n                                                          operators/\n                                                             __init__.py\n                                                             test_*.py\n                                                          transfers/\n                                                             __init__.py\n                                                             test_*.py\n                                                          triggers/\n                                                              __init__.py\n                                                              test_*.py\n                                     integration/<PROVIDER>/\n                                                              __init__.py\n                                                              test_integration_*.py\n                                     system/<PROVIDER>/\n                                                          __init__.py\n                                                          example_*.py\n```\n\n----------------------------------------\n\nTITLE: Example Yandex.Cloud OAuth Token Configuration\nDESCRIPTION: Displays an example OAuth token string used for authenticating as a Yandex.Cloud user account within an Airflow connection. This token is obtained through the Yandex.Cloud identity management system and entered into the 'OAuth Token' field.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/connections/yandexcloud.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ny3_Vd3eub7w9bIut67GHeL345gfb5GAnd3dZnf08FR1vjeUFve7Yi8hGvc\n```\n\n----------------------------------------\n\nTITLE: Proper Usage of Airflow Variables in DAG Definition\nDESCRIPTION: This snippet demonstrates the correct way to use Airflow Variables in DAG definitions, using Jinja templates to delay variable retrieval until task execution.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nbash_use_variable_good = BashOperator(\n    task_id=\"bash_use_variable_good\",\n    bash_command=\"echo variable foo=${foo_env}\",\n    env={\"foo_env\": \"{{ var.value.get('foo') }}\"},\n)\n\n@task\ndef my_task():\n    var = Variable.get(\"foo\")  # This is ok since my_task is called only during task run, not during DAG scan.\n    print(var)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous S3 to GCS Transfer Operation in Python\nDESCRIPTION: Example demonstrating asynchronous data transfer from S3 to GCS using the deferrable mode of S3ToGCSOperator. This implementation uses Google Cloud Storage Transfer Service and includes polling for transfer job status.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/s3_to_gcs.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntransfer_from_s3_to_gcs_async = S3ToGCSOperator(\n    task_id=\"transfer_from_s3_to_gcs_async\",\n    bucket=\"s3_bucket_name\",\n    prefix=\"s3_prefix\",\n    dest_gcs=\"gs://gcs_bucket_name\",\n    replace=False,\n    deferrable=True,\n    poll_interval=10,\n)\n```\n\n----------------------------------------\n\nTITLE: Using Deferrable DataflowJobStatusSensor in Airflow\nDESCRIPTION: Illustrates using the `DataflowJobStatusSensor` in deferrable mode (`deferrable=True`). The sensor yields control while waiting for the job status, making it more resource-efficient for long-running checks.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataflow.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n.. exampleinclude:: /../../google/tests/system/google/cloud/dataflow/example_dataflow_sensors_deferrable.py\n    :language: python\n    :dedent: 4\n    :start-after: [START howto_sensor_wait_for_job_status_deferrable]\n    :end-before: [END howto_sensor_wait_for_job_status_deferrable]\n```\n\n----------------------------------------\n\nTITLE: Installing Exasol Provider with common.sql Extra via pip (Bash)\nDESCRIPTION: This command installs the `apache-airflow-providers-exasol` package along with its optional cross-provider dependencies specified by the `common.sql` extra. This is necessary to use features that depend on the `apache-airflow-providers-common-sql` package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/exasol/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-exasol[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Provide logger_name param in providers hooks\nDESCRIPTION: This text is a commit message summary describing the addition of a 'logger_name' parameter to provider hooks, allowing the logger name to be overridden. This change is linked to pull request #36675 (later reverted by #37015).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_25\n\nLANGUAGE: plaintext\nCODE:\n```\nProvide the logger_name param in providers hooks in order to override the logger name (#36675)\n```\n\n----------------------------------------\n\nTITLE: Deleting Translation Glossary with TranslateDeleteGlossaryOperator in Python\nDESCRIPTION: This example demonstrates how to use TranslateDeleteGlossaryOperator to delete a translation glossary resource from a Google Cloud project. It shows the basic configuration for removing a specific glossary identified by its ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/translate.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndelete_glossary = TranslateDeleteGlossaryOperator(\n    task_id=\"delete_glossary\",\n    project_id=PROJECT_ID,\n    location=LOCATION,\n    glossary_id=GLOSSARY_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow with Multiple Provider Extras\nDESCRIPTION: This command installs Apache Airflow with Google, Amazon, and Apache Spark provider extras, using a specific version constraint file. It ensures consistent dependency versions across the installed packages.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow[google,amazon,apache-spark]==|version| \\\n  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-|version|/constraints-3.9.txt\"\n```\n\n----------------------------------------\n\nTITLE: Prepare Documentation for November 1st Provider Wave (Excluded from v1.4.0 Changelog)\nDESCRIPTION: Indicates preparatory work done on documentation for the November 1st wave of provider releases, referenced by pull request #44011. This change was intentionally excluded from the main changelog notes for version 1.4.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs for Nov 1st wave of providers (#44011)\n```\n\n----------------------------------------\n\nTITLE: Viewing Help for Provider Distribution Preparation (Bash)\nDESCRIPTION: Displays the help message for the `breeze release-management prepare-provider-distributions` command using the `--help` flag. This shows available options, flags, and lists all providers that can be built.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-provider-distributions --help\n```\n\n----------------------------------------\n\nTITLE: Displaying Pre-commit Install Help\nDESCRIPTION: Command to show help information for pre-commit installation options.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install --help\n```\n\n----------------------------------------\n\nTITLE: Listing Commits for Airflow Provider Version 4.1.0\nDESCRIPTION: This section outlines commits related to Apache Airflow provider version 4.1.0. Significant changes include documentation preparation for multiple provider releases (Nov, Sep, Aug, Jul 2022), moving the minimum Airflow version to 2.3.0, enabling string normalization, applying PEP-563, and refactoring provider dependencies and configuration. It includes commit hashes linked to GitHub, commit dates, and subject lines.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n==================================================================================================  ===========  ====================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ====================================================================================\n`12c3c39d1a <https://github.com/apache/airflow/commit/12c3c39d1a816c99c626fe4c650e88cf7b1cc1bc>`__  2022-11-15   ``pRepare docs for November 2022 wave of Providers (#27613)``\n`78b8ea2f22 <https://github.com/apache/airflow/commit/78b8ea2f22239db3ef9976301234a66e50b47a94>`__  2022-10-24   ``Move min airflow version to 2.3.0 for all providers (#27196)``\n`2a34dc9e84 <https://github.com/apache/airflow/commit/2a34dc9e8470285b0ed2db71109ef4265e29688b>`__  2022-10-23   ``Enable string normalization in python formatting - providers (#27205)``\n`f8db64c35c <https://github.com/apache/airflow/commit/f8db64c35c8589840591021a48901577cff39c07>`__  2022-09-28   ``Update docs for September Provider's release (#26731)``\n`06acf40a43 <https://github.com/apache/airflow/commit/06acf40a4337759797f666d5bb27a5a393b74fed>`__  2022-09-13   ``Apply PEP-563 (Postponed Evaluation of Annotations) to non-core airflow (#26289)``\n`e5ac6c7cfb <https://github.com/apache/airflow/commit/e5ac6c7cfb189c33e3b247f7d5aec59fe5e89a00>`__  2022-08-10   ``Prepare docs for new providers release (August 2022) (#25618)``\n`d2459a241b <https://github.com/apache/airflow/commit/d2459a241b54d596ebdb9d81637400279fff4f2d>`__  2022-07-13   ``Add documentation for July 2022 Provider's release (#25030)``\n`0de31bd73a <https://github.com/apache/airflow/commit/0de31bd73a8f41dded2907f0dee59dfa6c1ed7a1>`__  2022-06-29   ``Move provider dependencies to inside provider folders (#24672)``\n`510a6bab45 <https://github.com/apache/airflow/commit/510a6bab4595cce8bd5b1447db957309d70f35d9>`__  2022-06-28   ``Remove 'hook-class-names' from provider.yaml (#24702)``\n==================================================================================================  ===========  ====================================================================================\n```\n\n----------------------------------------\n\nTITLE: Deleting DataFusion Pipeline with CloudDataFusionDeletePipelineOperator in Python\nDESCRIPTION: This code demonstrates how to use CloudDataFusionDeletePipelineOperator to delete a pipeline from Google Cloud DataFusion. It includes parameters for pipeline name, instance name, location, and project ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/datafusion.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nCloudDataFusionDeletePipelineOperator(\n    task_id=\"delete_pipeline\",\n    pipeline_name=PIPELINE_NAME,\n    instance_name=INSTANCE_NAME,\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    namespace=\"default\",\n    retry=RETRY,\n    timeout=TIMEOUT,\n    metadata=METADATA,\n)\n```\n\n----------------------------------------\n\nTITLE: Basic DAG Access Control Configuration in Python\nDESCRIPTION: Demonstrates how to set basic DAG-level access control by assigning permissions to roles. This example gives Viewer role multiple permissions including edit, read, and delete capabilities.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/access-control.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nDAG(\n    dag_id=\"example_fine_grained_access\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    access_control={\n        \"Viewer\": {\"can_edit\", \"can_read\", \"can_delete\"},\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Bumping Minimum Airflow Version to 2.8.0 (Commit Message)\nDESCRIPTION: Commit message announcing that the minimum required Apache Airflow version for providers, including Airbyte, has been increased to 2.8.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nBump minimum Airflow version in providers to Airflow 2.8.0 (#41396)\n```\n\n----------------------------------------\n\nTITLE: Granting Log Access for Specific DAG\nDESCRIPTION: Policy that provides access to task logs of the 'test' DAG to a specific user group, with a context condition ensuring the access is specifically for task logs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/manage/index.rst#2025-04-22_snippet_7\n\nLANGUAGE: cedar\nCODE:\n```\npermit(\n  principal in Airflow::Group::\"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\",\n  action,\n  resource == Airflow::Dag::\"test\"\n) when {\n  context has dag_entity && context.dag_entity == \"TASK_LOGS\"\n};\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Revert logger_name param in providers hooks\nDESCRIPTION: This commit message (hash 0b680c9492, dated 2024-01-26) indicates a revert of a previous change (related to #36675) that provided a `logger_name` parameter in provider hooks to override the logger name. The revert is tracked under issue #37015.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n``Revert \"Provide the logger_name param in providers hooks in order to override the logger name (#36675)\" (#37015)``\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Re-apply updated version numbers\nDESCRIPTION: This text is a commit message summary describing the action of re-applying updated version numbers to the second wave of provider releases in December, linked to pull request #36380.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_28\n\nLANGUAGE: plaintext\nCODE:\n```\nRe-apply updated version numbers to 2nd wave of providers in December (#36380)\n```\n\n----------------------------------------\n\nTITLE: Setting GCP SSH Connection Environment Variable in Bash\nDESCRIPTION: Example of setting up a Google Cloud Platform SSH connection using environment variables in Airflow. Demonstrates configuration of instance name, zone, internal IP usage, IAP tunnel, OS login, and key expiration time.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/gcp_ssh.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_CONN_GOOGLE_CLOUD_SQL_DEFAULT=\"gcpssh://conn-user@conn-host?\\\ninstance_name=conn-instance-name&\\\nzone=zone&\\\nuse_internal_ip=True&\\\nuse_iap_tunnel=True&\\\nuse_oslogin=False&\\\nexpire_time=4242\"\n```\n\n----------------------------------------\n\nTITLE: Adding Mechanism to Suspend Providers\nDESCRIPTION: Excluded Change (Version > 3.1.1): Implements a mechanism to allow suspending providers, referencing pull request #30422.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Add mechanism to suspend providers (#30422)``\n```\n\n----------------------------------------\n\nTITLE: Creating New Version of AutoML Video Training Job in Vertex AI\nDESCRIPTION: Example demonstrating how to create a new version of an existing AutoML video training job by specifying the parent model.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vertex_ai.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nCreateAutoMLVideoTrainingJobOperator(\n    project_id=PROJECT_ID,\n    region=REGION,\n    display_name=TRAINING_JOB_NAME,\n    dataset_id=VIDEO_DATASET,\n    prediction_type=\"classification\",\n    training_budget_milli_node_hours=8000,\n    parent_model=PARENT_MODEL,\n    sync=True,\n    task_id=\"training_job\",)\n```\n\n----------------------------------------\n\nTITLE: Applying D205 Docstring Formatting to Airbyte/Alibaba Providers\nDESCRIPTION: Updates docstrings in Airbyte and Alibaba providers to comply with the D205 standard (1 blank line required between summary line and description), improving documentation consistency, as per issue #32214.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_31\n\nLANGUAGE: text\nCODE:\n```\nD205 Support - Providers: Airbyte and Alibaba (#32214)\n```\n\n----------------------------------------\n\nTITLE: Configuring a Pig Job for Google Cloud Dataproc\nDESCRIPTION: This code defines a configuration for a Pig job to be submitted to a Dataproc cluster. It specifies the query file and query variables for the Pig execution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataproc.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nPIG_JOB = {\n    \"reference\": {\"project_id\": PROJECT_ID},\n    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n    \"pig_job\": {\n        \"query_file_uri\": f\"gs://{BUCKET_NAME}/{PIG_SCRIPT}\",\n        \"script_variables\": {\n            \"input\": f\"gs://{BUCKET_NAME}/{PIG_INPUT}\",\n            \"output\": f\"gs://{BUCKET_NAME}/{PIG_OUTPUT_DIR}\",\n        },\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Using SparkJDBCOperator in Apache Airflow DAG\nDESCRIPTION: Example of using SparkJDBCOperator to transfer data between Spark and a JDBC database. The operator can be configured for spark_to_jdbc or jdbc_to_spark operations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nspark_jdbc_to_spark = SparkJDBCOperator(\n    task_id=\"spark_jdbc_to_spark\",\n    cmd_type=\"jdbc_to_spark\",\n    jdbc_table=\"jdbc_table\",\n    spark_jars_path=SPARK_JDBC_JAR,\n    jdbc_driver=\"org.postgresql.Driver\",\n    metastore_table=\"spark_table\",\n    save_mode=\"overwrite\",\n    save_format=\"hive\",\n    jdbc_conn_id=\"spark_default\",\n    spark_conn_id=\"spark_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Formatting GitHub Commit History in Markdown\nDESCRIPTION: Table showing GitHub commit history with links to commits, timestamps, and descriptions for version tracking.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/github/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n`08b675cf66 <https://github.com/apache/airflow/commit/08b675cf6642171cb1c5ddfb09607b541db70b29>`__  2022-06-13   ``Fix links to sources for examples (#24386)``\n```\n\n----------------------------------------\n\nTITLE: Refactoring and Simplifying Code in Apache/Alibaba Providers\nDESCRIPTION: Simplifies code within providers under the Apache and Alibaba namespaces, improving readability and maintainability, as tracked in issue #33227.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_22\n\nLANGUAGE: text\nCODE:\n```\nRefactor: Simplify code in Apache/Alibaba providers (#33227)\n```\n\n----------------------------------------\n\nTITLE: Neo4j Provider Package Notice Template\nDESCRIPTION: Template notice indicating the file is auto-generated and should not be modified directly\nSOURCE: https://github.com/apache/airflow/blob/main/providers/neo4j/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!\n\n.. IF YOU WANT TO MODIFY THIS FILE, YOU SHOULD MODIFY THE TEMPLATE\n   `PROVIDER_COMMITS_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY\n\n.. THE REMAINDER OF THE FILE IS AUTOMATICALLY GENERATED. IT WILL BE OVERWRITTEN!\n```\n\n----------------------------------------\n\nTITLE: Handling Task Execution Completion in Apache Airflow (Python)\nDESCRIPTION: This method is called when a task execution completes. It checks the status of the event and raises an AirflowException if the status is not 'success'. The method is likely part of a custom operator or sensor in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/triggers/README.md#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef execute_complete(self, context, event=None):\n    if event[\"status\"] != \"success\":\n        raise AirflowException(f\"Error creating cluster: {event}\")\n    return\n```\n\n----------------------------------------\n\nTITLE: Deleting a SageMaker Notebook Instance in Python\nDESCRIPTION: This code snippet shows how to use the SageMakerDeleteNotebookOperator to delete a SageMaker Notebook Instance. It terminates the instance and deletes the associated ML storage volume and network interface. The instance must be stopped before deletion.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_sagemaker_notebook_delete]\n# Code snippet not provided in the original text\n# [END howto_operator_sagemaker_notebook_delete]\n```\n\n----------------------------------------\n\nTITLE: Use Reproducible Builds for Providers (Excluded from v1.1.0 Changelog)\nDESCRIPTION: Implements the use of reproducible builds for provider packages, ensuring build consistency, referenced by pull request #35693. This change was intentionally excluded from the main changelog notes for version 1.1.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_32\n\nLANGUAGE: plaintext\nCODE:\n```\nUse reproducible builds for providers (#35693)\n```\n\n----------------------------------------\n\nTITLE: Installing the Exasol Provider via pip (Bash)\nDESCRIPTION: This command installs the `apache-airflow-providers-exasol` package using pip. It should be run in an environment with an existing Apache Airflow 2 installation (version 2.9.0 or higher is required as per the documentation).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/exasol/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-exasol\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameter Removal - metrics.timer_unit_consistency\nDESCRIPTION: Configuration parameter that was previously used to control timer unit consistency in metrics logging. This setting is being removed in Airflow 3.0 as milliseconds becomes the standard unit for all timer metrics.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43975.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmetrics.timer_unit_consistency\n```\n\n----------------------------------------\n\nTITLE: Building Airflow Image Using UV Package Installer\nDESCRIPTION: Builds a production Airflow image using the UV package installer instead of pip. This is an experimental feature as UV is a new tool in the Python ecosystem.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . --pull --build-arg INSTALL_PACKAGES_FROM_CONTEXT=\"false\" --build-arg INSTALL_FROM_PYPI=\"true\" --build-arg USE_PACKAGED_FROM_DOCKERHUB=\"false\" --build-arg AIRFLOW_INSTALLATION_METHOD=\"uv\" -t my-image:my-tag\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Pinot Provider with Common SQL Dependencies\nDESCRIPTION: This command installs the Apache Pinot provider package along with its common SQL dependencies. It's used when you need to use all features of the package, including those that depend on common SQL functionalities.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-pinot[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepares documentation for rc2 release of Providers\nDESCRIPTION: This commit message, for commit bbc627a3da, indicates that documentation was prepared for the second release candidate (rc2) of the June 2021 Airflow Providers release, tracked in issue #16501.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_17\n\nLANGUAGE: text\nCODE:\n```\n``Prepares documentation for rc2 release of Providers (#16501)``\n```\n\n----------------------------------------\n\nTITLE: Filtering Mapped Data in Python for Apache Airflow\nDESCRIPTION: Demonstrates how to filter mapped data by returning None for unwanted elements, specifically for copying only certain file types from an S3 bucket.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef create_copy_kwargs(filename):\n    # Skip files not ending with these suffixes.\n    if filename.rsplit(\".\", 1)[-1] not in (\"json\", \"yml\"):\n        return None\n    return {\n        \"source_bucket_key\": filename,\n        \"dest_bucket_key\": filename,\n        \"dest_bucket_name\": \"my_other_bucket\",\n    }\n\n\n# copy_kwargs and copy_files are implemented the same.\n```\n\n----------------------------------------\n\nTITLE: Generating CLI Documentation for Celery Executor in Apache Airflow\nDESCRIPTION: This code snippet uses the argparse directive to generate documentation for the Celery Executor's command-line interface in Apache Airflow. It specifies the module, function, and program name for parsing the CLI arguments.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/cli-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. argparse::\n   :module: airflow.providers.celery.executors.celery_executor\n   :func: _get_parser\n   :prog: airflow\n```\n\n----------------------------------------\n\nTITLE: Using Fixtures for Database-Dependent Objects in Python Tests\nDESCRIPTION: This code snippet shows how to use pytest fixtures to create database-dependent objects, avoiding issues during test collection.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.models import Connection\n\npytestmark = pytest.mark.db_test\n\n\n@pytest.fixture()\ndef get_connection1():\n    return Connection()\n\n\n@pytest.fixture()\ndef get_connection2():\n    return Connection(host=\"apache.org\", extra={})\n\n\n@pytest.mark.parametrize(\n    \"conn\",\n    [\n        \"get_connection1\",\n        \"get_connection2\",\n    ],\n)\ndef test_as_json_from_connection(self, conn: Connection):\n    conn = request.getfixturevalue(conn)\n    ...\n```\n\n----------------------------------------\n\nTITLE: Update pgvector Dependency Constraint (v1.2.2)\nDESCRIPTION: Updates the dependency constraint for the 'pgvector' library to exclude version 0.3.0 (using '!=0.3.0') due to issues identified in that specific release. Referenced by pull request #40683.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nupdating pgvector to !=0.3.0 to  avoid recent issue in latest releases(0.3.0) (#40683)\n```\n\n----------------------------------------\n\nTITLE: Virtualenv Task Configuration Example\nDESCRIPTION: Shows how to configure a task to run in an isolated Python virtual environment with its own dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/tutorial/taskflow.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@task.virtualenv(\n    task_id=\"virtualenv_python\",\n    requirements=[\"colorama==0.4.0\"],\n    system_site_packages=False\n)\ndef callable_virtualenv():\n    from time import sleep\n    from colorama import Back, Fore, Style\n    print(Fore.RED + 'some red text')\n    print(Back.GREEN + 'and with a green background')\n    print(Style.DIM + 'and in dim text')\n    print(Style.RESET_ALL)\n    for i in range(4):\n        print(f'Running {i}')\n        sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Displaying Airflow Provider Commits (Oct 2023 - v1.8.0) using RST Table\nDESCRIPTION: This reStructuredText (RST) snippet formats a list of commits related to Apache Airflow provider version 1.8.0 (September-October 2023) into a table. It includes the commit hash (linked to the specific GitHub commit), the commit date, and the commit subject, utilizing RST's table structure, hyperlink syntax (`<link>`__), and inline code formatting (``code``).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ====================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ====================================================================================\n`e9987d5059 <https://github.com/apache/airflow/commit/e9987d50598f70d84cbb2a5d964e21020e81c080>`__  2023-10-13   ``Prepare docs 1st wave of Providers in October 2023 (#34916)``\n`128f6b9e40 <https://github.com/apache/airflow/commit/128f6b9e40c4cf96f900629294175f9c5babd703>`__  2023-10-13   ``Add missing header into 'common.sql' changelog (#34910)``\n`0c8e30e43b <https://github.com/apache/airflow/commit/0c8e30e43b70e9d033e1686b327eb00aab82479c>`__  2023-10-05   ``Bump min airflow version of providers (#34728)``\n`7ebf4220c9 <https://github.com/apache/airflow/commit/7ebf4220c9abd001f1fa23c95f882efddd5afbac>`__  2023-09-28   ``Refactor usage of str() in providers (#34320)``\n`659d94f0ae <https://github.com/apache/airflow/commit/659d94f0ae89f47a7d4b95d6c19ab7f87bd3a60f>`__  2023-09-21   ``Use 'airflow.exceptions.AirflowException' in providers (#34511)``\n`f5c2748c33 <https://github.com/apache/airflow/commit/f5c2748c3346bdebf445afd615657af8849345dd>`__  2023-09-08   ``fix(providers/sql): respect soft_fail argument when exception is raised (#34199)``\n```\n\n----------------------------------------\n\nTITLE: Listing Commits for Airflow Provider Version 4.2.1\nDESCRIPTION: This section details commits related to the Apache Airflow provider version 4.2.1 release, primarily focusing on documentation preparation for the June 2023 wave, updates regarding Python 3.7 support, and pydocstyle checks. It includes commit hashes linked to GitHub, commit dates, and subject lines.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n==================================================================================================  ===========  =============================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =============================================================\n`79bcc2e668 <https://github.com/apache/airflow/commit/79bcc2e668e648098aad6eaa87fe8823c76bc69a>`__  2023-06-20   ``Prepare RC1 docs for June 2023 wave of Providers (#32001)``\n`8b146152d6 <https://github.com/apache/airflow/commit/8b146152d62118defb3004c997c89c99348ef948>`__  2023-06-20   ``Add note about dropping Python 3.7 for providers (#32015)``\n`a59076eaee <https://github.com/apache/airflow/commit/a59076eaeed03dd46e749ad58160193b4ef3660c>`__  2023-06-02   ``Add D400 pydocstyle check - Providers (#31427)``\n==================================================================================================  ===========  =============================================================\n```\n\n----------------------------------------\n\nTITLE: Asana Provider License Header\nDESCRIPTION: Apache License 2.0 header text for the Asana provider documentation file\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n```\n\n----------------------------------------\n\nTITLE: Airflow License Header in RestructuredText\nDESCRIPTION: Standard Apache License 2.0 header used in Airflow documentation files, written in RestructuredText format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/core-extensions/executors.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n```\n\n----------------------------------------\n\nTITLE: Reapplying Templates for All Providers (Commit Message)\nDESCRIPTION: Commit message stating that code generation templates have been reapplied across all Apache Airflow providers, including Airbyte.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nReapply templates for all providers (#39554)\n```\n\n----------------------------------------\n\nTITLE: Bumping Ruff Linter to 0.3.3 (Commit Message)\nDESCRIPTION: Commit message indicating an update of the Ruff linter tool to version 0.3.3 across the Apache Airflow project, affecting code quality checks for providers like Airbyte.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_17\n\nLANGUAGE: text\nCODE:\n```\nBump ruff to 0.3.3 (#38240)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Remove redundant logging in SFTP Hook\nDESCRIPTION: This commit message, for commit 81be82bfb7, indicates the removal of unnecessary or duplicative log messages from the SFTP Hook implementation, tracked under issue #16704.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n``Remove redundant logging in SFTP Hook (#16704)``\n```\n\n----------------------------------------\n\nTITLE: Excluding Changes from Changelog in RST\nDESCRIPTION: A reStructuredText comment block listing changes that are excluded from the changelog. These changes are typically minor updates or internal modifications not relevant to end-users.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Add explanatory note for contributors about updating Changelog (#24229)``\n   * ``Migrate DingTalk example DAGs to new design #22443 (#24133)``\n   * ``Prepare provider documentation 2022.05.11 (#23631)``\n   * ``Bump pre-commit hook versions (#22887)``\n   * ``Use new Breese for building, pulling and verifying the images. (#23104)``\n   * ``Update tree doc references to grid (#22966)``\n   * ``Prepare docs for May 2022 provider's release (#24231)``\n   * ``Update package description to remove double min-airflow specification (#24292)``\n```\n\n----------------------------------------\n\nTITLE: Removing return statement after yield in triggers\nDESCRIPTION: Removes unnecessary return statement after yield in triggers class.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n\"Remove return statement after yield from triggers class (#31703)\"\n```\n\n----------------------------------------\n\nTITLE: Legacy Airflow DAG Example for Linting\nDESCRIPTION: This Python snippet demonstrates a legacy Airflow DAG that would trigger linting warnings when checked with ruff, illustrating common issues to be addressed in Airflow 3.0 migration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/best-practices.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import dag\nfrom airflow.datasets import Dataset\nfrom airflow.sensors.filesystem import FileSensor\n\n\n@dag()\ndef legacy_dag():\n    FileSensor(task_id=\"wait_for_file\", filepath=\"/tmp/test_file\")\n```\n\n----------------------------------------\n\nTITLE: Adding Product to ProductSet with Google Cloud Vision Operator in Airflow\nDESCRIPTION: Demonstrates how to use CloudVisionAddProductToProductSetOperator to add a product to a product set by extracting IDs from XCOM.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/vision.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nadd_product_to_product_set = CloudVisionAddProductToProductSetOperator(\n    product_set_id=\"{{ task_instance.xcom_pull('create_product_set')['name'].split('/')[-1] }}\",\n    product_id=\"{{ task_instance.xcom_pull('create_product')['name'].split('/')[-1] }}\",\n    location=LOCATION,\n    project_id=PROJECT_ID,\n    retry=Retry(maximum=10.0),\n    timeout=5,\n    task_id=\"add_product_to_product_set\",\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Apache Kafka Cluster with ManagedKafkaUpdateClusterOperator in Python\nDESCRIPTION: This snippet demonstrates how to update an Apache Kafka cluster using the ManagedKafkaUpdateClusterOperator. It specifies the project ID, region, cluster name, and update mask.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/managed_kafka.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nupdate_cluster = ManagedKafkaUpdateClusterOperator(\n    task_id=\"update_cluster\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    cluster=CLUSTER,\n    update_mask={\"paths\": [\"display_name\"]},\n)\n```\n\n----------------------------------------\n\nTITLE: Git Commit History for Apache Airflow\nDESCRIPTION: List of git commit hashes, dates and commit messages showing the version history and changes made to the Apache Airflow project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nf6448b4e48 2020-12-15 Add link to PyPI Repository to provider docs (#13064)\n5090fb0c89 2020-12-15 Add script to generate integrations.json (#13073)\n32971a1a2d 2020-12-09 Updates providers versions to 1.0.0 (#12955)\nb40dffa085 2020-12-08 Rename remaing modules to match AIP-21 (#12917)\n[...]\n```\n\n----------------------------------------\n\nTITLE: Executing MySQL Query from External File\nDESCRIPTION: Example demonstrating how to execute MySQL queries stored in an external SQL file. The SQL file must be located in the same directory as the DAG file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/operators.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\nmysql_task_external_file = SQLExecuteQueryOperator(\n    task_id='mysql_task_external_file',\n    conn_id='mysql_default',\n    sql='script.sql',\n    database='mydb'\n)\n```\n\n----------------------------------------\n\nTITLE: Using GCSTimeSpanFileTransformOperator in Python\nDESCRIPTION: Demonstrates how to use the GCSTimeSpanFileTransformOperator to transform files modified within a specific time span in Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntransform_timespan_file = GCSTimeSpanFileTransformOperator(\n    task_id=\"transform_timespan_file\",\n    source_bucket=BUCKET_NAME,\n    source_prefix=f\"{BUCKET_FILE_PREFIX}-source\",\n    destination_bucket=BUCKET_NAME,\n    destination_prefix=f\"{BUCKET_FILE_PREFIX}-target\",\n    transform_script=[\"python\", \"-c\", \"import sys; print(sys.stdin.read().upper())\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Docs - Providers October 2023 (3rd Wave Fix)\nDESCRIPTION: This commit message, associated with commit d1c58d86de on 2023-10-28, describes a fix related to the preparation of documentation for the third wave of provider releases in October 2023.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 3rd wave of Providers October 2023 - FIX (#35233)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepares provider release after PIP 21 compatibility\nDESCRIPTION: This commit message, for commit 807ad32ce5, marks preparations for a provider release that includes changes to ensure compatibility with PIP version 21. Related to issue #15576.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_23\n\nLANGUAGE: text\nCODE:\n```\n``Prepares provider release after PIP 21 compatibility (#15576)``\n```\n\n----------------------------------------\n\nTITLE: Git Commit History Log Entry\nDESCRIPTION: Git commit log entry showing commit hash, author, date and message for various changes to the SQL provider codebase.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: git\nCODE:\n```\n25bdbc8e67 <https://github.com/apache/airflow/commit/25bdbc8e6768712bad6043618242eec9c6632618>  2022-11-26   Updated docs for RC3 wave of providers (#27937)\n```\n\n----------------------------------------\n\nTITLE: Including Security Information in reStructuredText\nDESCRIPTION: This snippet uses a reStructuredText directive to include external security information from a specified file path. This is likely used to incorporate standardized security documentation into the Airflow project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow to Load Example DAGs - ini\nDESCRIPTION: This snippet sets the 'load_examples' property in the [core] section of airflow.cfg to True, ensuring that example DAGs are loaded during Airflow initialization. Example DAGs are required for the smoke test script to function correctly as these DAGs must be available in the database for serialization and testing. This configuration can be set in the airflow.cfg file or via the equivalent environment variable.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_11\n\nLANGUAGE: ini\nCODE:\n```\n[core]\nload_examples = True\n\n```\n\n----------------------------------------\n\nTITLE: Running All Kubernetes Tests Using Breeze - Bash\nDESCRIPTION: This bash command initiates the execution of all Kubernetes-related tests using Breeze. It is typically used for validating the full test suite in development or CI/CD pipelines. Prerequisites include a proper Breeze installation and a configured Kubernetes testing environment. The command requires no parameters and executes the complete test workflow.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s tests\n```\n\n----------------------------------------\n\nTITLE: HTML Table of Contents Generator Comment\nDESCRIPTION: DocToc-generated table of contents section with instructions to maintain auto-updates.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/adr/0011-unified-communication-with-the-users.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*\n\n- [11. Unified communication with the users](#11-unified-communication-with-the-users)\n  - [Status](#status)\n  - [Context](#context)\n  - [Decision](#decision)\n  - [Consequences](#consequences)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n```\n\n----------------------------------------\n\nTITLE: Automatic Generation Warning for Command Images\nDESCRIPTION: This comment warns users not to manually resolve conflicts in this file, but instead to use a specific command to regenerate the help images.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/images/output-commands-hash.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# This file is automatically generated by pre-commit. If you have a conflict with this file\n# Please do not solve it but run `breeze setup regenerate-command-images`.\n# This command should fix the conflict and regenerate help images that you have conflict with.\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Bump minimum Airflow version in providers to 2.8.0\nDESCRIPTION: This text is a commit message summary indicating that the minimum required Airflow version for providers was updated to 2.8.0, linked to pull request #41396.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nBump minimum Airflow version in providers to Airflow 2.8.0 (#41396)\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Prepare docs for Aug 1st wave of providers\nDESCRIPTION: This text is a commit message summary for preparing documentation related to the August 1st wave of Apache Airflow provider releases, linked to pull request #41230.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs for Aug 1st wave of providers (#41230)\n```\n\n----------------------------------------\n\nTITLE: Mounting DAGs from External PVC using Helm\nDESCRIPTION: Helm command to configure Airflow to read DAGs from an external Persistent Volume Claim (PVC) with ReadOnlyMany or ReadWriteMany access mode.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airflow apache-airflow/airflow \\\n  --set dags.persistence.enabled=true \\\n  --set dags.persistence.existingClaim=my-volume-claim \\\n  --set dags.gitSync.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Implementing CronTriggerTimetable in Airflow DAG\nDESCRIPTION: Example of using CronTriggerTimetable with a cron expression to run a DAG at 01:00 every Wednesday. This timetable accepts a cron expression and triggers DAG runs according to it.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timetable.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.timetables.trigger import CronTriggerTimetable\n\n\n@dag(schedule=CronTriggerTimetable(\"0 1 * * 3\", timezone=\"UTC\"), ...)  # At 01:00 on Wednesday\ndef example_dag():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Formatting Version Header in Markdown\nDESCRIPTION: This snippet demonstrates how version headers are formatted in the changelog using Markdown. It includes the version number and the date of the latest change for that version.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/http/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n4.7.0\n.....\n\nLatest change: 2023-11-12\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL Settings for Cloud SQL Query Operator in Python\nDESCRIPTION: This example demonstrates how to specify SSL settings at the operator level for the Google Cloud SQL Query Operator. It shows how to override connection-level SSL settings by providing paths to certificate files.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_cloudsql_query_operators_ssl]\n# [END howto_operator_cloudsql_query_operators_ssl]\n```\n\n----------------------------------------\n\nTITLE: Getting a Dataplex Data Profile Scan with Airflow Operator\nDESCRIPTION: Uses the DataplexGetDataProfileScanOperator to retrieve a Dataplex Data Profile scan. This operator fetches information about a specific data profile scan in Google Cloud Dataplex.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nget_data_profile = DataplexGetDataProfileScanOperator(\n    task_id=\"get_data_profile\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    data_scan_id=DATA_PROFILE_SCAN_ID,\n)\n```\n\n----------------------------------------\n\nTITLE: Running a Dataplex Data Profile Scan with Airflow Operator\nDESCRIPTION: Uses the DataplexRunDataProfileScanOperator to execute a Dataplex Data Profile scan in asynchronous mode. This operator initiates a data profile scan job in Google Cloud Dataplex.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nrun_data_profile = DataplexRunDataProfileScanOperator(\n    task_id=\"run_data_profile\",\n    project_id=PROJECT_ID,\n    region=REGION,\n    data_scan_id=DATA_PROFILE_SCAN_ID,\n    asynchronous=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Extra Requirements for Google Provider Integration in Python\nDESCRIPTION: This code snippet shows the extra requirements definition for the Google provider to integrate with Apache Beam. It includes the Apache Beam provider and the GCP extras for Apache Beam.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nextras_require = (\n    {\n        # ...\n        \"apache.beam\": [\"apache-airflow-providers-apache-beam\", \"apache-beam[gcp]\"],\n        # ...\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Black Formatter on Airflow Provider Packages\nDESCRIPTION: This commit message signifies the application of the Black Python auto-formatter specifically to the codebase of the Airflow provider packages. It references pull request #10543.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_75\n\nLANGUAGE: text\nCODE:\n```\nEnable Black on Providers Packages (#10543)\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in reStructuredText\nDESCRIPTION: This snippet includes an external reStructuredText file containing security-related information for the Apache Airflow project. The included file is located in a common development directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Preparing Docs for Sep 1st Wave of Providers (Commit Message)\nDESCRIPTION: Commit message indicating the preparation of documentation for the September 1st wave of Apache Airflow provider releases. This relates to the Airbyte provider within the Airflow project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs for Sep 1st wave of providers (#42387)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare RC1 Docs for June 2023 Providers\nDESCRIPTION: This commit message, linked to commit 79bcc2e668 dated 2023-06-20, describes the preparation of documentation for the first release candidate (RC1) of the June 2023 wave of Apache Airflow Providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n``Prepare RC1 docs for June 2023 wave of Providers (#32001)``\n```\n\n----------------------------------------\n\nTITLE: Migrating Kubernetes Helper Functions in Python\nDESCRIPTION: Migration rules for Kubernetes helper functions, moving from the deprecated airflow.kubernetes module to airflow.providers.cncf.kubernetes.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41735.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"airflow.kubernetes.kubernetes_helper_functions.add_pod_suffix\"  \"airflow.providers.cncf.kubernetes.kubernetes_helper_functions.add_pod_suffix\"\n\"airflow.kubernetes.kubernetes_helper_functions.annotations_for_logging_task_metadata\"  \"airflow.providers.cncf.kubernetes.kubernetes_helper_functions.annotations_for_logging_task_metadata\"\n\"airflow.kubernetes.kubernetes_helper_functions.annotations_to_key\"  \"airflow.providers.cncf.kubernetes.kubernetes_helper_functions.annotations_to_key\"\n\"airflow.kubernetes.kubernetes_helper_functions.create_pod_id\"  \"airflow.providers.cncf.kubernetes.kubernetes_helper_functions.create_pod_id\"\n\"airflow.kubernetes.kubernetes_helper_functions.get_logs_task_metadata\"  \"airflow.providers.cncf.kubernetes.kubernetes_helper_functions.get_logs_task_metadata\"\n\"airflow.kubernetes.kubernetes_helper_functions.rand_str\"  \"airflow.providers.cncf.kubernetes.kubernetes_helper_functions.rand_str\"\n```\n\n----------------------------------------\n\nTITLE: Importing Opsgenie Provider Dependencies in Python\nDESCRIPTION: Code snippet showing the import of Opsgenie provider dependencies inside provider folders.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opsgenie/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"Move provider dependencies to inside provider folders (#24672)\"\n```\n\n----------------------------------------\n\nTITLE: Removing Old UI and Webserver\nDESCRIPTION: Commit message indicating the removal of the old user interface and webserver components from Airflow. References pull request #46942.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_17\n\nLANGUAGE: plaintext\nCODE:\n```\nRemove old UI and webserver (#46942)\n```\n\n----------------------------------------\n\nTITLE: Adding More Typing in Operators (template_fields/ext) in RST\nDESCRIPTION: This commit (83f8e178ba, committed on 2021-12-31) enhances type hinting within operators, specifically focusing on 'template_fields' and extensions. Refers to issue #20608.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_10\n\nLANGUAGE: rst\nCODE:\n```\nEven more typing in operators (template_fields/ext) (#20608)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Secret from Yandex Cloud Lockbox\nDESCRIPTION: This command deletes a secret from Yandex Cloud Lockbox using the 'yc' CLI tool. It demonstrates cleanup of a connection secret when it's no longer needed.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_20\n\nLANGUAGE: console\nCODE:\n```\n$ yc lockbox secret delete --name airflow/connections/mysqldb\nname: airflow/connections/mysqldb\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add August 2021 Provider's documentation\nDESCRIPTION: This commit message, for commit 0a68588479, marks the addition of documentation corresponding to the August 2021 release of Airflow providers, tracked under issue #17890.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n``Add August 2021 Provider's documentation (#17890)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Docs - Providers October 2023 (3rd Wave)\nDESCRIPTION: This commit message, associated with commit 3592ff4046 on 2023-10-28, describes the preparation of documentation for the third wave of provider releases in October 2023.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 3rd wave of Providers October 2023 (#35187)\n```\n\n----------------------------------------\n\nTITLE: Forcing Image Pull for Development\nDESCRIPTION: Helm command to ensure the image is pulled every time, useful for development with constant tags.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airflow apache-airflow/airflow \\\n  --set images.airflow.repository=my-company/airflow \\\n  --set images.airflow.tag=8a0da78 \\\n  --set images.airflow.pullPolicy=Always \\\n  --set airflowPodAnnotations.random=r$(uuidgen)\n```\n\n----------------------------------------\n\nTITLE: Bump Minimum Airflow Version to 2.6.0 (v1.1.0)\nDESCRIPTION: Indicates that the minimum required Apache Airflow version for this provider release (v1.1.0) was raised to 2.6.0, referenced by pull request #36017.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_29\n\nLANGUAGE: plaintext\nCODE:\n```\nBump minimum Airflow version in providers to Airflow 2.6.0 (#36017)\n```\n\n----------------------------------------\n\nTITLE: Disabling Database Cleanup in Airflow Breeze Tests\nDESCRIPTION: Shows how to use the Breeze testing command to run Airflow core tests without performing database cleanup. This is beneficial for specific testing scenarios that require preserving database state.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_47\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-tests --no-db-cleanup tests/core\n```\n\n----------------------------------------\n\nTITLE: Updating Package Description for Min Airflow Version\nDESCRIPTION: Excluded Change (Version 3.0.0): Corrects the package description to remove a redundant specification of the minimum Airflow version, referencing pull request #24292.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Update package description to remove double min-airflow specification (#24292)``\n```\n\n----------------------------------------\n\nTITLE: Documenting Provider Update October 2023 (3rd Wave)\nDESCRIPTION: This commit prepares the documentation for the third wave of Apache Airflow provider updates released in October 2023, as tracked in issue #35187.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs 3rd wave of Providers October 2023 (#35187)\n```\n\n----------------------------------------\n\nTITLE: Breaking Changes Warning Block\nDESCRIPTION: RST warning block describing breaking changes in version 2.0.0 including parameter changes for Hooks and Notifications\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. warning::\n  All deprecated classes, parameters and features have been removed from the {provider_name} provider package.\n  The following breaking changes were introduced:\n\n  * Hooks\n    * Parameter ``tag`` cannot be None. It is not set to MATCH_ALL_TAG as default.\n  * Notifications\n    * Parameter ``notify_type`` cannot be None. It is not set to NotifyType.INFO as default.\n    * Parameter ``body_format`` cannot be None. It is not set to NotifyFormat.TEXT as default.\n    * Parameter ``tag`` cannot be None. It is not set to MATCH_ALL_TAG as default.\n```\n\n----------------------------------------\n\nTITLE: Copying Directory from SFTP to GCS using Wildcard\nDESCRIPTION: Example showing how to copy an entire directory from SFTP to Google Cloud Storage using wildcards in the source path.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/transfer/sftp_to_gcs.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncopy_directory = SFTPToGCSOperator(\n    task_id=\"copy-directory\",\n    source_path=\"path/to/{{ds}}/*\",\n    destination_bucket=BUCKET_NAME,\n    destination_path=\"\",\n    sftp_conn_id=\"sftp_default\",\n    gcp_conn_id=\"google_cloud_default\",\n    dag=dag\n)\n```\n\n----------------------------------------\n\nTITLE: Preparing Docs for July Provider RC2 Release in RST\nDESCRIPTION: This commit (87f408b1e7, committed on 2021-07-26) prepares the documentation for the Release Candidate 2 (RC2) of the July providers release. Refers to issue #17116.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_23\n\nLANGUAGE: rst\nCODE:\n```\nPrepares docs for Rc2 release of July providers (#17116)\n```\n\n----------------------------------------\n\nTITLE: Using GCSUploadSessionCompleteSensor in Python\nDESCRIPTION: Demonstrates how to use the GCSUploadSessionCompleteSensor to check for changes in the number of files with a specified prefix in Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ngcs_upload_session_complete = GCSUploadSessionCompleteSensor(\n    task_id=\"gcs_upload_session_complete\",\n    bucket=BUCKET_NAME,\n    prefix=PREFIX,\n    inactivity_period=15,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating ACL Entry for GCS Bucket in Python\nDESCRIPTION: Shows how to use the GCSBucketCreateAclEntryOperator to create a new ACL entry on a specified Google Cloud Storage bucket.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/gcs.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncreate_bucket_acl_entry = GCSBucketCreateAclEntryOperator(\n    task_id=\"create_bucket_acl_entry\",\n    bucket=BUCKET_NAME,\n    entity=\"user-{{ USER_EMAIL }}\",\n    role=\"OWNER\",\n)\n```\n\n----------------------------------------\n\nTITLE: Opening K9s CLI for Kubernetes Cluster Monitoring\nDESCRIPTION: Command to open the K9s CLI (Kubernetes CLI) in a separate terminal for monitoring and interacting with the Kubernetes cluster where Airflow is deployed. K9s provides a terminal-based UI for cluster observation.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s k9s\n```\n\n----------------------------------------\n\nTITLE: Using SQLExecuteQueryOperator with JDBC in Apache Airflow\nDESCRIPTION: Example of using SQLExecuteQueryOperator to execute SQL queries via JDBC connection in Apache Airflow. This snippet shows how to create and use the operator with various parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jdbc/docs/operators.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_jdbc]\n# [END howto_operator_jdbc]\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Airflow Providers\nDESCRIPTION: ReStructuredText (RST) documentation structure defining the table of contents and organization for Airflow provider packages documentation. Uses toctree directive to include all documentation files in the current directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/operators-and-hooks-ref/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 3\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Bump Minimum Airflow Version\nDESCRIPTION: This commit message, associated with commit d0918d77ee on 2023-12-07, indicates that the minimum required Airflow version for providers was bumped to Airflow 2.6.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nBump minimum Airflow version in providers to Airflow 2.6.0 (#36017)\n```\n\n----------------------------------------\n\nTITLE: Moving Provider Tests to Unit Folder\nDESCRIPTION: Commit message indicating the relocation of 'provider_tests' directory into a 'unit' subfolder within the provider testing structure. References pull request #46800.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_26\n\nLANGUAGE: plaintext\nCODE:\n```\nMove provider_tests to unit folder in provider tests (#46800)\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: This RST directive includes an external file containing security-related documentation for the Apache Airflow project. It references a file named 'security.rst' located in a specific directory structure.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kylin/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Installing Asana Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Asana provider package for Apache Airflow using pip. This package is compatible with Airflow 2 and supports Python versions 3.9, 3.10, 3.11, and 3.12.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-asana\n```\n\n----------------------------------------\n\nTITLE: Array Parameter Configuration in Python\nDESCRIPTION: Examples of configuring array parameters with different validation rules and UI rendering options including examples and email format validation.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/params.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nParam([\"a\", \"b\", \"c\"], type=\"array\")\nParam([\"two\", \"three\"], type=\"array\", examples=[\"one\", \"two\", \"three\", \"four\", \"five\"])\nParam([\"one@example.com\", \"two@example.com\"], type=\"array\", items={\"type\": \"string\", \"format\": \"idn-email\"})\n```\n\n----------------------------------------\n\nTITLE: Commit Hash Link Format in Markdown\nDESCRIPTION: Format for linking commit hashes to GitHub repository with associated commit messages and dates.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n`93ad181684 <https://github.com/apache/airflow/commit/93ad181684c8314e199f2521f487cd4292e06b5c>`__  2024-10-26   ``Add context to Azure Service Bus Message callback (#43370)``\n```\n\n----------------------------------------\n\nTITLE: Including Security Information in ReStructuredText for Apache Airflow\nDESCRIPTION: This ReStructuredText directive includes external security information from a specified file path. It's used to incorporate standardized security documentation into the Apache Airflow project documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit History in Markdown\nDESCRIPTION: This code snippet shows the format used to display the commit history. Each line represents a commit, including the commit hash, date, and description.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/cassandra/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n`7d24b088cd <https://github.com/apache/airflow/commit/7d24b088cd736cfa18f9214e4c9d6ce2d5865f3d>`__  2020-07-25   ``Stop using start_date in default_args in example_dags (2) (#9985)``\n`33f0cd2657 <https://github.com/apache/airflow/commit/33f0cd2657b2e77ea3477e0c93f13f1474be628e>`__  2020-07-22   ``apply_default keeps the function signature for mypy (#9784)``\n`4d74ac2111 <https://github.com/apache/airflow/commit/4d74ac2111862186598daf92cbf2c525617061c2>`__  2020-07-19   ``Increase typing for Apache and http provider package (#9729)``\n`750555f261 <https://github.com/apache/airflow/commit/750555f261616d809d24b8550b9482a713ba3171>`__  2020-07-19   ``Add guide for Cassandra Operators (#9877)``\n```\n\n----------------------------------------\n\nTITLE: Adding Port to Oracle DSN Construction in RST\nDESCRIPTION: This commit (30eeac7b7e, committed on 2021-05-09) modifies the Oracle connection logic to include the port when constructing the Data Source Name (DSN). Refers to issue #15589.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_31\n\nLANGUAGE: rst\nCODE:\n```\n[Oracle] Add port to DSN (#15589)\n```\n\n----------------------------------------\n\nTITLE: Faster airflow_version Imports (Commit Message)\nDESCRIPTION: Commit message indicating an optimization to speed up the import of 'airflow_version' within the Apache Airflow codebase, affecting providers like Airbyte.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nFaster 'airflow_version' imports (#39552)\n```\n\n----------------------------------------\n\nTITLE: Pushing Multiple XComs in Python Task\nDESCRIPTION: Shows how to push multiple XCom values at once by returning a dictionary from a task. This is useful for tasks that produce multiple outputs.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/xcoms.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@task(do_xcom_push=True, multiple_outputs=True)\ndef push_multiple(**context):\n    return {\"key1\": \"value1\", \"key2\": \"value2\"}\n```\n\n----------------------------------------\n\nTITLE: Removing soft_fail Parameter (Commit Message)\nDESCRIPTION: Commit message stating the removal of the 'soft_fail' parameter, likely from an operator or sensor within the Apache Airflow Airbyte provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nremove soft_fail (#41710)\n```\n\n----------------------------------------\n\nTITLE: Running Python API Client Tests in Bash\nDESCRIPTION: Command to execute Python API client tests using Breeze. This allows testing of the Airflow Python API client.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing python-api-client-tests\n```\n\n----------------------------------------\n\nTITLE: Including Security Notice in reStructuredText\nDESCRIPTION: This directive includes an external security notice file into the current document. It is used to incorporate standard security information across multiple files in the Apache Airflow project documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kafka/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Entering Interactive Kubernetes Shell for Testing\nDESCRIPTION: Command to enter an interactive shell environment for running Kubernetes tests one by one. This creates and activates a virtualenv in the kubernetes-tests/.venv folder for test execution.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s shell\n```\n\n----------------------------------------\n\nTITLE: Breaking Change Note\nDESCRIPTION: Important note about provider compatibility\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. note::\n  This release of provider is only available for Airflow 2.3+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n```\n\n----------------------------------------\n\nTITLE: Triggering Parameterized DAG with Configuration via CLI in Airflow\nDESCRIPTION: This snippet shows how to trigger a parameterized DAG using the Airflow CLI, passing a JSON configuration blob as a parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/dag-run.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairflow dags trigger --conf '{\"conf1\": \"value1\"}' example_parameterized_dag\n```\n\n----------------------------------------\n\nTITLE: TaskFlow with Custom Object Serialization\nDESCRIPTION: Shows how to implement custom object serialization for TaskFlow tasks using class methods.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/taskflow.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import ClassVar\n\n\nclass MyCustom:\n    __version__: ClassVar[int] = 1\n\n    def __init__(self, x):\n        self.x = x\n\n    def serialize(self) -> dict:\n        return dict({\"x\": self.x})\n\n    @staticmethod\n    def deserialize(data: dict, version: int):\n        if version > 1:\n            raise TypeError(f\"version > {MyCustom.version}\")\n        return MyCustom(data[\"x\"])\n```\n\n----------------------------------------\n\nTITLE: Preparing Documentation for May 2022 Provider Release\nDESCRIPTION: Excluded Change (Version 3.0.0): Includes documentation preparation tasks for the May 2022 provider release cycle, referencing pull request #24231.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_17\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Prepare docs for May 2022 provider's release (#24231)``\n```\n\n----------------------------------------\n\nTITLE: Monitoring SageMaker AutoML Experiment State in Python\nDESCRIPTION: This code snippet shows how to use SageMakerAutoMLSensor to check the state of an Amazon SageMaker AutoML job until it reaches a terminal state.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/sagemaker.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_sagemaker_auto_ml]\n# Code snippet not provided in the original text\n# [END howto_operator_sagemaker_auto_ml]\n```\n\n----------------------------------------\n\nTITLE: Patching a Cloud SQL Database in Python\nDESCRIPTION: Example of using CloudSQLPatchInstanceDatabaseOperator to update a database inside a Cloud SQL instance using patch semantics.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_sql.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsql_patch_database = CloudSQLPatchInstanceDatabaseOperator(\n    instance=INSTANCE_NAME,\n    database=DB_NAME,\n    body=db_patch_body,\n    task_id=\"sql_patch_database\",\n)\n\nsql_patch_database_with_project = CloudSQLPatchInstanceDatabaseOperator(\n    project_id=GCP_PROJECT_ID,\n    instance=INSTANCE_NAME,\n    database=DB_NAME,\n    body=db_patch_body,\n    task_id=\"sql_patch_database_with_project\",\n)\n```\n\n----------------------------------------\n\nTITLE: Testing RC with Mixed Sources\nDESCRIPTION: Example of testing with locally built Airflow and downloaded provider distributions\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/testing_packages.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nrm dist/*\nbreeze release-management prepare-airflow-distributions\npip download apache-airflow-providers-celery==3.6.2rc1 --dest dist --no-deps\npip download apache-airflow-providers-cncf-kubernetes==8.1.0rc1 --dest dist --no-deps\nbreeze start-airflow --mount-sources remove --use-distributions-from-dist --use-airflow-version sdist --executor CeleryExecutor --backend postgres --load-default-connections --load-example-dags\n```\n\n----------------------------------------\n\nTITLE: Updating a Google Cloud Tasks Queue in Python\nDESCRIPTION: This snippet demonstrates how to update a Google Cloud Tasks queue using the CloudTasksQueueUpdateOperator in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/tasks.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# [START update_queue]\nCloudTasksQueueUpdateOperator(\n    task_id=\"update_queue\",\n    location=LOCATION,\n    queue_name=QUEUE_ID,\n    update_mask={\"paths\": [\"rate_limits\", \"retry_config\"]},\n    retry=Retry(maximum=10.0),\n    timeout=5,\n).execute(context=context)\n# [END update_queue]\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fix and reapply templates for provider documentation\nDESCRIPTION: This commit message (hash 99df205f42, dated 2023-11-16) indicates that templates used for generating provider documentation were fixed and reapplied (issue #35686).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_26\n\nLANGUAGE: text\nCODE:\n```\n``Fix and reapply templates for provider documentation (#35686)``\n```\n\n----------------------------------------\n\nTITLE: Publishing Documentation\nDESCRIPTION: Command to publish documentation to airflow-site repository.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\nbreeze release-management publish-docs helm-chart\n```\n\n----------------------------------------\n\nTITLE: Documenting Excluded Changes for Version 2.0.0 in reStructuredText\nDESCRIPTION: This snippet shows how to document changes that are excluded from the changelog for version 2.0.0 using reStructuredText format. It includes comments about documentation updates and package description changes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/changelog.rst#2025-04-22_snippet_4\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Clean up f-strings in logging calls (#23597)``\n   * ``Add explanatory note for contributors about updating Changelog (#24229)``\n   * ``Prepare docs for May 2022 provider's release (#24231)``\n   * ``Update package description to remove double min-airflow specification (#24292)``\n```\n\n----------------------------------------\n\nTITLE: HTML License Header Comment\nDESCRIPTION: Apache License 2.0 header comment block used at the beginning of the file to specify licensing terms.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/PROJECT_GUIDELINES.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Displaying Job Run and Repair Links in RST\nDESCRIPTION: RST code snippet showing how to include images of the DatabricksWorkflowPlugin UI enhancements in the documentation. It demonstrates the task-level and workflow-level links provided by the plugin.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/plugins/workflow.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. image:: ../img/workflow_plugin_single_task.png\n\n.. image:: ../img/workflow_plugin_launch_task.png\n```\n\n----------------------------------------\n\nTITLE: Updating Documentation for June 2021 Provider Release\nDESCRIPTION: Excluded Change (Version 2.0.0): Includes documentation updates for the June 2021 provider release cycle, referencing pull request #16294.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_41\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Updated documentation for June 2021 provider release (#16294)``\n```\n\n----------------------------------------\n\nTITLE: Defining Task Dependencies in Airflow using Bitshift Operators\nDESCRIPTION: Demonstrates how to define dependencies between Airflow tasks using the >> and << operators. This is the recommended way to specify task relationships due to its readability.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/core-concepts/tasks.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfirst_task >> second_task >> [third_task, fourth_task]\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow for Opensearch TLS Logging\nDESCRIPTION: Advanced configuration in airflow.cfg to enable TLS for Opensearch logging. Includes settings for SSL verification and custom CA certificates.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/logging/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[logging]\nremote_logging = True\n\n[opensearch_configs]\nuse_ssl = True\nverify_certs = True\nssl_assert_hostname = True\nca_certs=/path/to/CA_certs\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add Mechanism to Suspend Providers\nDESCRIPTION: This commit message, linked to commit d23a3bbed8 dated 2023-04-04, describes the introduction of a mechanism allowing for the suspension of specific Apache Airflow providers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\n``Add mechanism to suspend providers (#30422)``\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace\nDESCRIPTION: Commands to create a namespace for Airflow deployment\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/quick-start.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport NAMESPACE=example-namespace\nkubectl create namespace $NAMESPACE\n```\n\n----------------------------------------\n\nTITLE: Setting Instance Name Using Environment Variable\nDESCRIPTION: This snippet demonstrates how to set a custom title for the Airflow UI using an environment variable. This method provides an alternative to modifying the configuration file directly.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/customize-ui.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nAIRFLOW__WEBSERVER__INSTANCE_NAME = \"DevEnv\"\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Helm Chart with Bash\nDESCRIPTION: Command to create a new custom Helm chart directory structure\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/extending-the-chart.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm create my-custom-chart\n```\n\n----------------------------------------\n\nTITLE: Installing SendGrid Provider for Airflow (Bash)\nDESCRIPTION: These commands show how to install the SendGrid provider for Airflow using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[sendgrid]' --constraint ...\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow-providers-sendgrid' --constraint ...\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Libraries for Apache Airflow\nDESCRIPTION: This command installs the Azure integration package for Apache Airflow using pip. It allows users to use Azure-related operators and hooks in their Airflow DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/_partials/prerequisite_tasks.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[azure]'\n```\n\n----------------------------------------\n\nTITLE: Running MyPy Check for Branch Changes using Breeze in Bash\nDESCRIPTION: This command uses Breeze to run the MyPy check for all changes in the current branch since branching from main.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nbreeze static-checks --type mypy-airflow --only-my-changes\n```\n\n----------------------------------------\n\nTITLE: Using Built-in Waiter for Async Redshift Cluster Availability Check in Python\nDESCRIPTION: Demonstrates how to use a built-in waiter to asynchronously check for Redshift cluster availability using aiobotocore.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/triggers/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nasync with await self.redshift_hook.get_async_conn() as client:\n    await client.get_waiter(\"cluster_available\").wait(\n        ClusterIdentifier=self.cluster_identifier,\n        WaiterConfig={\n            \"Delay\": int(self.poll_interval),\n            \"MaxAttempts\": int(self.max_attempt),\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: HTML Link Definition in ReStructuredText\nDESCRIPTION: Defines a hyperlink reference to the Apache Airflow GitHub repository using ReStructuredText's raw HTML directive.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_codespaces.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. |airflow_github| raw:: html\n\n     <a href=\"https://github.com/apache/airflow/\" target=\"_blank\">https://github.com/apache/airflow/</a>\n```\n\n----------------------------------------\n\nTITLE: Verifying Apache Airflow Package Signatures\nDESCRIPTION: Commands to verify the authenticity of Apache Airflow provider packages using PGP signatures. Shows three alternative verification methods.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/installing-from-sources.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngpg --verify apache-airflow-providers-********.asc apache-airflow-providers-*********\n```\n\nLANGUAGE: bash\nCODE:\n```\npgpv apache-airflow-providers-********.asc\n```\n\nLANGUAGE: bash\nCODE:\n```\npgp apache-airflow-providers-********.asc\n```\n\n----------------------------------------\n\nTITLE: Provider Dependency Management in YAML\nDESCRIPTION: Provider dependencies are managed in the provider.yaml file which contains all provider dependencies, versions and constraints. The file is automatically updated through pre-commit hooks.\nSOURCE: https://github.com/apache/airflow/blob/main/PROVIDERS.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprovider.yaml\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Bump minimum Airflow version in providers to 2.7.0\nDESCRIPTION: This text is a commit message summary indicating that the minimum required Airflow version for providers was updated to 2.7.0, linked to pull request #39240.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_17\n\nLANGUAGE: plaintext\nCODE:\n```\nBump minimum Airflow version in providers to Airflow 2.7.0 (#39240)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fix Circular Import Error in Providers\nDESCRIPTION: This commit message, associated with commit f5aed58d9f dated 2023-05-18, addresses a circular import error within Apache Airflow providers that was triggered by an Airflow version check.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n``Fixing circular import error in providers caused by airflow version check (#31379)``\n```\n\n----------------------------------------\n\nTITLE: Removing Empty Lines in Generated Changelog\nDESCRIPTION: Cleans up the generated changelog files by removing unnecessary empty lines, improving readability, as tracked in issue #35436.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nRemove empty lines in generated changelog (#35436)\n```\n\n----------------------------------------\n\nTITLE: Fixing Mistakenly Added install_requires\nDESCRIPTION: Bug Fix (Version 2.2.3): Removes incorrect `install_requires` dependencies that were mistakenly added to all providers, referencing pull request #22382.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_19\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Fix mistakenly added install_requires for all providers (#22382)``\n```\n\n----------------------------------------\n\nTITLE: Adding More SQL Template Field Renderers in RST\nDESCRIPTION: This commit (39e395f981, committed on 2022-02-04) introduces additional renderers for SQL template fields, enhancing templating capabilities. Refers to issue #21237.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\nAdd more SQL template fields renderers (#21237)\n```\n\n----------------------------------------\n\nTITLE: Generated File Notice in RST\nDESCRIPTION: Warning notice indicating the file is auto-generated from a template\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/flink/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!\n\n.. IF YOU WANT TO MODIFY THIS FILE, YOU SHOULD MODIFY THE TEMPLATE\n   `PROVIDER_COMMITS_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY\n\n.. THE REMAINDER OF THE FILE IS AUTOMATICALLY GENERATED. IT WILL BE OVERWRITTEN!\n```\n\n----------------------------------------\n\nTITLE: Documenting Initial Version Release in reStructuredText\nDESCRIPTION: This snippet demonstrates how to document the initial version release (1.0.0) of the provider using reStructuredText format. It includes a brief note about the initial version.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: reStructuredText\nCODE:\n```\n1.0.0\n.....\n\nInitial version of the provider.\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Build Versions (Python List)\nDESCRIPTION: Defines the list of Python versions to use for the current build. This example specifies Python '3.9'.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n\\['3.9'\\]\n```\n\n----------------------------------------\n\nTITLE: Adding Google Cloud Data Pipelines Create Operator in Python\nDESCRIPTION: Implements a new operator for creating Google Cloud Data Pipelines in the Google provider for Apache Airflow. This allows programmatic creation of data pipelines.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"Add Google Cloud's Data Pipelines Create Operator (#32843)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow Provider Packages\nDESCRIPTION: Command to install Apache Airflow provider packages using pip, where PROVIDER is the provider ID and EXTRAS are optional extra packages to install.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/PROVIDER_DISTRIBUTIONS_DETAILS.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-<PROVIDER>[<EXTRAS>]\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Structure for EMR Operators\nDESCRIPTION: Sphinx toctree directive that organizes the EMR operators documentation with a maxdepth of 1 and glob pattern to include all related files.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/emr/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Running All Airflow Tests with Breeze in Bash\nDESCRIPTION: Command to run all Airflow tests using Breeze with specific Python version, backend, and database reset.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nbreeze --backend postgres --postgres-version 15 --python 3.9 --db-reset testing tests --test-type All\n```\n\n----------------------------------------\n\nTITLE: Reviewing Changed Files in Cherry-Picked Commits\nDESCRIPTION: These commands help review files changed by doc-only or excluded changes, ensuring correct classification of changes.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ngit show --format=tformat:\"\" --stat --name-only $(cat /tmp/doc-only-changes.txt) | sort | uniq\n```\n\nLANGUAGE: shell\nCODE:\n```\ngit log apache/v2-2-test --format=\"%H\" -- airflow/sensors/base.py | grep -f /tmp/doc-only-changes.txt | xargs git show\n```\n\nLANGUAGE: shell\nCODE:\n```\ngit log apache/v2-2-test --format=\"%H\" -- airflow/sensors/base.py | grep -f /tmp/doc-only-changes.txt | \\\n    xargs -n 1 git log --oneline --max-count=1 | \\\n    sed s'/.*(#\\([0-9]*\\))$/https:\\/\\/github.com\\/apache\\/airflow\\/pull\\/\\1/'\n```\n\n----------------------------------------\n\nTITLE: Requirements Table in RST\nDESCRIPTION: A reStructuredText table showing the minimum package requirements for the Sendgrid provider, including Apache Airflow and Sendgrid package versions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sendgrid/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n\"apache-airflow\"  \">=2.9.0\"\n\"sendgrid\"        \">=6.0.0\"\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Publishing Messages with PubSubPublishMessageOperator\nDESCRIPTION: Example showing how to publish messages to a PubSub topic using the PubSubPublishMessageOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/pubsub.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npublish_task = PubSubPublishMessageOperator(\n    task_id=\"publish_task\",\n    project_id=PROJECT_ID,\n    topic=TOPIC_ID,\n    messages=[{\"data\": b\"Test Message 1\"}, {\"data\": b\"Test Message 2\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Docker Memory Allocation\nDESCRIPTION: Command to verify available memory for Docker on Unix systems by checking physical pages and page size\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/docker-compose/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm \"debian:bookworm-slim\" bash -c 'numfmt --to iec $(echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE))))'\n```\n\n----------------------------------------\n\nTITLE: Implementing SFTP to S3 Stream File Option in Python\nDESCRIPTION: Addition of a stream file option for the SFTP to S3 operation, potentially improving efficiency for large file transfers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/changelog.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nsftp_to_s3 stream file option (#17609)\n```\n\n----------------------------------------\n\nTITLE: Starting Amazon Managed Service for Apache Flink Application - Python\nDESCRIPTION: Example code demonstrating how to start an Amazon Managed Service for Apache Flink application using the KinesisAnalyticsV2StartApplicationOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/kinesis_analytics.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstart_application = KinesisAnalyticsV2StartApplicationOperator(\n    task_id=\"start_application\",\n    application_name=APPLICATION_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Including Provider Configurations Reference in reStructuredText\nDESCRIPTION: This directive includes an external file containing reference documentation for Apache Airflow provider configurations.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/edge3/docs/configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst\n```\n\n----------------------------------------\n\nTITLE: Adding Missing Logos for Airflow Integrations\nDESCRIPTION: This commit message indicates the addition of missing logo images for various integrations within the Airflow project. It references pull request #13717.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_45\n\nLANGUAGE: text\nCODE:\n```\nAdd missing logos for integrations (#13717)\n```\n\n----------------------------------------\n\nTITLE: Updating Airflow Pods with New Image\nDESCRIPTION: Helm command to upgrade Airflow deployment with the custom image containing DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airflow apache-airflow/airflow \\\n  --set images.airflow.repository=my-company/airflow \\\n  --set images.airflow.tag=8a0da78\n```\n\n----------------------------------------\n\nTITLE: Running Static Checks on Specific Files with Breeze\nDESCRIPTION: This command runs the mypy static check on specific files in the Airflow codebase using Breeze.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbreeze static-checks --type mypy-airflow --file airflow/utils/code_utils.py --file airflow/utils/timeout.py\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow Components in Tmux Sessions\nDESCRIPTION: Series of commands to start various Airflow components (scheduler, API server, DAG processor, triggerer) in separate tmux panes.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nairflow scheduler\n```\n\nLANGUAGE: bash\nCODE:\n```\nairflow api-server\n```\n\nLANGUAGE: bash\nCODE:\n```\nairflow dag-processor\n```\n\nLANGUAGE: bash\nCODE:\n```\nairflow triggerer\n```\n\nLANGUAGE: bash\nCODE:\n```\n:select-layout tiled\n```\n\n----------------------------------------\n\nTITLE: Improving Backwards Compatibility in RST\nDESCRIPTION: This commit (d72098f6d8, committed on 2022-02-11) focuses on improving backwards compatibility within the Airflow providers. Refers to issue #21524.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\nimproved backwards compatibility (#21524)\n```\n\n----------------------------------------\n\nTITLE: Enhancing DynamoDBToS3Operator Functionality\nDESCRIPTION: Added incremental export and cross account export capabilities to the DynamoDBToS3Operator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"Add incremental export and cross account export functionality in 'DynamoDBToS3Operator' (#41304)\"\n```\n\n----------------------------------------\n\nTITLE: Fixing RedshiftDataOperator Deferred Mode\nDESCRIPTION: Corrected an issue where RedshiftDataOperator was not running in deferred mode when it should.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"Fix RedshiftDataOperator not running in deferred mode when it should (#41206)\"\n```\n\n----------------------------------------\n\nTITLE: Fixing Misspellings in RST\nDESCRIPTION: This commit (046f02e5a7, committed on 2021-09-09) corrects misspellings found in the codebase or documentation. Refers to issue #18121.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_18\n\nLANGUAGE: rst\nCODE:\n```\nfix misspelling (#18121)\n```\n\n----------------------------------------\n\nTITLE: Installing SSH Extras for Apache Airflow\nDESCRIPTION: Command to install SSH hooks and operators for Apache Airflow. This enables SSH-based integrations and remote command execution.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_71\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[ssh]'\n```\n\n----------------------------------------\n\nTITLE: Verifying SHA512 Checksums\nDESCRIPTION: Commands to verify the SHA512 checksum of Apache Airflow provider packages by comparing against the provided checksum file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/installing-from-sources.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nshasum -a 512 apache-airflow-providers-********  | diff - apache-airflow-providers-********.sha512\n```\n\nLANGUAGE: bash\nCODE:\n```\nshasum -a 512 apache-airflow-providers-airbyte-1.0.0-source.tar.gz  | diff - apache-airflow-providers-airbyte-1.0.0-source.tar.gz.sha512\n```\n\n----------------------------------------\n\nTITLE: Triggering DAG Programmatically\nDESCRIPTION: Example of how DAGs can still be triggered programmatically using the TriggerDagRunOperator even when they have no schedule.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/24842.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nTriggerDagRunOperator\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in reStructuredText\nDESCRIPTION: This directive includes the content of a security-related documentation file into the current document. It references an external file containing security information for the Apache Airflow project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/edge3/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Listing Commit Message: Reapply templates for all providers\nDESCRIPTION: This text is a commit message summary indicating that project templates were reapplied across all providers, likely for consistency or updates, linked to pull request #39554.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nReapply templates for all providers (#39554)\n```\n\n----------------------------------------\n\nTITLE: Configuring Pod Eviction Policies\nDESCRIPTION: YAML configuration for pod eviction settings when using Kubernetes Cluster Autoscaler.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/production-guide.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nworkers:\n  safeToEvict: true\nscheduler:\n  safeToEvict: true\nwebserver:\n  safeToEvict: true\n```\n\n----------------------------------------\n\nTITLE: Deleting RDS Database Instance\nDESCRIPTION: Deletes an AWS RDS database instance using RDSDeleteDbInstanceOperator. Can be run in deferrable mode.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/rds.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndelete_db = RDSDeleteDbInstanceOperator(\n    task_id=\"delete_db_instance\",\n    db_instance_identifier=DB_INSTANCE_NAME,\n    skip_final_snapshot=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Rewriting Asset Event Registration\nDESCRIPTION: Commit message indicating a rewrite or refactoring of the asset event registration mechanism in Airflow. References pull request #47677.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nRewrite asset event registration (#47677)\n```\n\n----------------------------------------\n\nTITLE: Replacing Assignment with Augmented Assignment in Airflow\nDESCRIPTION: This commit message describes a code refactoring effort where standard assignments (e.g., x = x + 1) were replaced with augmented assignments (e.g., x += 1) for conciseness and potentially minor performance benefits. It references pull request #10468.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_78\n\nLANGUAGE: text\nCODE:\n```\nReplace assigment with Augmented assignment (#10468)\n```\n\n----------------------------------------\n\nTITLE: Referencing Provider Package\nDESCRIPTION: Package name reference for the Apache Pinot provider\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n``apache-airflow-providers-apache-pinot``\n```\n\n----------------------------------------\n\nTITLE: Monitoring Flink Application Start Status - Python\nDESCRIPTION: Example code demonstrating how to monitor the start status of a Flink application using the KinesisAnalyticsV2StartApplicationCompletedSensor.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/kinesis_analytics.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwait_for_application_start = KinesisAnalyticsV2StartApplicationCompletedSensor(\n    task_id=\"wait_for_application_start\",\n    application_name=APPLICATION_NAME,\n)\n```\n\n----------------------------------------\n\nTITLE: Removing Redundant None Default in dict.get() in Airflow\nDESCRIPTION: This commit message describes a code cleanup where redundant 'None' values provided as the default argument to dictionary `get()` methods were removed. It references pull request #11448.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_70\n\nLANGUAGE: text\nCODE:\n```\nRemove redundant None provided as default to dict.get() (#11448)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Improve Provider Documentation Structure\nDESCRIPTION: This commit message, associated with commit 09d4718d3a dated 2023-06-27, indicates improvements made to the structure of Apache Airflow provider documentation and README files.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n``Improve provider documentation and README structure (#32125)``\n```\n\n----------------------------------------\n\nTITLE: Data Transfer RST Documentation Directive\nDESCRIPTION: ReStructuredText directive that generates documentation for data transfer-related functionalities in Airflow, using specific tags for categorization.\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/operators-and-hooks-ref/software.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. transfers-ref::\n   :tags: software\n   :header-separator: \"\n```\n\n----------------------------------------\n\nTITLE: Creating Asynchronous Dataform Workflow Invocation in Python\nDESCRIPTION: This snippet shows how to create an asynchronous Workflow Invocation and use a sensor to monitor its status.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_create_workflow_invocation_async]\n# [END howto_operator_create_workflow_invocation_async]\n```\n\n----------------------------------------\n\nTITLE: Celery Integration Test Example\nDESCRIPTION: Python code showing how to write an integration test using pytest markers for Celery integration\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/integration_tests.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.integration(\"celery\")\ndef test_real_ping(self):\n    hook = RedisHook(redis_conn_id=\"redis_default\")\n    redis = hook.get_conn()\n\n    assert redis.ping(), \"Connection to Redis with PING works.\"\n```\n\n----------------------------------------\n\nTITLE: Configuring RST Documentation Structure for Google Sensors\nDESCRIPTION: RST configuration for documenting Google sensors section, including a table of contents that links to Google Cloud Tasks documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/sensors/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n\n    google-cloud-tasks\n```\n\n----------------------------------------\n\nTITLE: Dropping Foreign Key Constraints\nDESCRIPTION: SQL commands to drop foreign key constraints before modifying table character sets.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading.rst#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE task_reschedule DROP FOREIGN KEY task_reschedule_ti_fkey;\nALTER TABLE xcom DROP FOREIGN KEY xcom_task_instance_fkey;\nALTER TABLE task_fail DROP FOREIGN KEY task_fail_ti_fkey;\nALTER TABLE rendered_task_instance_fields DROP FOREIGN KEY rtif_ti_fkey;\n```\n\n----------------------------------------\n\nTITLE: Installing Discord Provider with Common Compat Dependencies\nDESCRIPTION: Command to install the Discord provider package along with its common compatibility dependencies using pip package manager.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/discord/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-discord[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Creating Azure Service Bus Queue in Python\nDESCRIPTION: This snippet demonstrates how to use the AzureServiceBusCreateQueueOperator to create an Azure Service Bus queue with specific parameters.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/operators/asb.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_create_service_bus_queue]\n# [END howto_operator_create_service_bus_queue]\n```\n\n----------------------------------------\n\nTITLE: Adding Back References for All Providers\nDESCRIPTION: Command to add back references to documentation for all providers, requires airflow-site directory path.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management add-back-references --airflow-site-directory DIRECTORY all-providers\n```\n\n----------------------------------------\n\nTITLE: Running MyPy Check on Staged Files in Airflow (Excluding Providers) in Bash\nDESCRIPTION: This command runs the MyPy check specifically on staged files in the Airflow directory, excluding providers.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run mypy-airflow\n```\n\n----------------------------------------\n\nTITLE: Importing Data to Cloud Memorystore Instance\nDESCRIPTION: Example showing how to import data from Cloud Storage to a Cloud Memorystore instance\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport_instance = CloudMemorystoreImportOperator(task_id=\"import-instance\", location=\"europe-north1\", instance=INSTANCE_NAME, project_id=PROJECT_ID, input_config={\"gcs_source\": {\"uri\": \"gs://test-memorystore/my-export.rdb\"}})\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Use reproducible builds for provider packages\nDESCRIPTION: This commit message (hash 99534e47f3, dated 2023-11-19) details the implementation of reproducible builds for provider packages, ensuring build consistency (issue #35693).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_24\n\nLANGUAGE: text\nCODE:\n```\n``Use reproducible builds for provider packages (#35693)``\n```\n\n----------------------------------------\n\nTITLE: Rendering Commit Hash and Date in Markdown\nDESCRIPTION: This snippet shows how commit hashes and dates are formatted in the changelog using Markdown syntax. It includes a link to the commit on GitHub.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_37\n\nLANGUAGE: markdown\nCODE:\n```\n`60c49ab2df <https://github.com/apache/airflow/commit/60c49ab2dfabaf450b80a5c7569743dd383500a6>`__  2023-07-19   ``Add more accurate typing for DbApiHook.run method (#31846)``\n```\n\n----------------------------------------\n\nTITLE: Displaying License Information and Including Shared Sections - Sphinx - reStructuredText\nDESCRIPTION: This snippet utilizes reStructuredText markup to display copyright, licensing, and legal information required by the Apache Software Foundation at the start of a Sphinx documentation file. It further uses the \".include::\" directive to import shared documentation sections, ensuring consistency across multiple documentation builds. Dependencies include Sphinx for interpreting the includes and rendering the markup; there are no parameters, and the expected output is a rendered documentation page with the appropriate legal and shared content. There are limitations in that this only renders correctly with Sphinx or compatible tools.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/io/docs/configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst\n\n```\n\n----------------------------------------\n\nTITLE: Installing Provider Dependencies with UV\nDESCRIPTION: Command to synchronize all dependencies and extras needed for development and testing of an Airflow provider using UV's sync command.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/12_provider_distributions.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv sync --package apache-airflow-providers-amazon\n```\n\n----------------------------------------\n\nTITLE: Building Python Client Packages\nDESCRIPTION: Commands to build the Python client distribution packages and copy generated sources.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncd ${AIRFLOW_REPO_ROOT}\nrm dist/*\nbreeze release-management prepare-python-client --distribution-format both --python-client-repo \"${CLIENT_REPO_ROOT}\"\n```\n\n----------------------------------------\n\nTITLE: Fixing Exception Name and Dependency in AWS Provider\nDESCRIPTION: Corrected an exception name and unpinned a dependency in the AWS provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"Issue-41243 Fix the Exception name and unpin dependency (#41256)\"\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Cloud Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with dbt Cloud integration, enabling dbt Cloud hooks and operators.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[dbt-cloud]'\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Providers Apache Pinot Package with SQL Dependencies\nDESCRIPTION: Command to install the Apache Airflow providers package for Apache Pinot with additional SQL-related dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apache-pinot[common.sql]\n```\n\n----------------------------------------\n\nTITLE: Running Provider Integration Tests\nDESCRIPTION: Example of running Kerberos integration tests for providers\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-integration-tests --integration kerberos\n```\n\n----------------------------------------\n\nTITLE: Using CloudDataTransferServiceGetOperationOperator in Python\nDESCRIPTION: Example of using the CloudDataTransferServiceGetOperationOperator to get a transfer operation in Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_storage_transfer_service.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nget_operation = CloudDataTransferServiceGetOperationOperator(\n    task_id=\"get_operation\",\n    operation_name=\"{{ task_instance.xcom_pull('run_transfer', key='operation') }}\",\n    project_id=PROJECT_ID,\n    gcp_conn_id=GCP_CONN_ID\n)\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: Sphinx documentation include directive to incorporate security-related content from a common development extensions directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/arangodb/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Installing PagerDuty Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with PagerDuty integration, enabling the PagerDuty hook.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[pagerduty]'\n```\n\n----------------------------------------\n\nTITLE: Publishing Provider Documentation in Airflow\nDESCRIPTION: Command for publishing documentation for specific Airflow providers using the breeze tool. Supports provider ID as parameter.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management publish-docs <provider id>\n```\n\n----------------------------------------\n\nTITLE: Defining Memcached Instance Configuration in Python\nDESCRIPTION: Example of defining a Google Cloud Memorystore Memcached instance configuration using the Instance class from google.cloud.memcache_v1beta2.types.cloud_memcache.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/cloud_memorystore_memcached.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nMEMCACHED_INSTANCE = {\"name\": \"test-memcached-instance\", \"node_count\": 1}\n```\n\n----------------------------------------\n\nTITLE: Version Dependency Code Block\nDESCRIPTION: Code block showing the Google provider package name used for tracking versions and dependencies.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n``apache-airflow-providers-google``\n```\n\n----------------------------------------\n\nTITLE: Displaying Git Commit Hash\nDESCRIPTION: A 40-character Git commit hash identifier in hexadecimal format, likely referencing a specific commit in the Apache Airflow repository.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/.latest-doc-only-change.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nd4473555c0e7022e073489b7163d49102881a1a6\n```\n\n----------------------------------------\n\nTITLE: Including Security Notice using RST Directive\nDESCRIPTION: This reStructuredText directive includes the content of the 'security.rst' file located in a shared development directory ('devel-common'). It is processed by Sphinx during documentation generation to embed the contents of the specified file into the current document, ensuring common security information is consistently presented.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/facebook/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Removing Fixed Code Comments\nDESCRIPTION: Commit message indicating the removal of code comments that are no longer relevant or have been addressed ('fixed'). References pull request #47823.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nRemove fixed comments (#47823)\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry Format in Markdown\nDESCRIPTION: Structured changelog entry showing commit hash, date and description in a markdown table format\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n`027b707d21 <https://github.com/apache/airflow/commit/027b707d215a9ff1151717439790effd44bab508>`__  2022-06-05   ``Add explanatory note for contributors about updating Changelog (#24229)``\n```\n\n----------------------------------------\n\nTITLE: Setting Up Documentation Environment\nDESCRIPTION: Commands for cloning and setting up the airflow-site repository for documentation publishing.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/apache/airflow-site.git airflow-site\ncd airflow-site\ngit checkout -b ${VERSION}-docs\nexport AIRFLOW_SITE_DIRECTORY=\"$(pwd)\"\n```\n\n----------------------------------------\n\nTITLE: Installing GitHub Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with GitHub integration, enabling GitHub operators and hook.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[github]'\n```\n\n----------------------------------------\n\nTITLE: Setting Sender's Email Address Environment Variable (Shell)\nDESCRIPTION: This snippet demonstrates how to set the sender's email address using an environment variable.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/email-config.rst#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nAIRFLOW__EMAIL__FROM_EMAIL=\"John Doe <johndoe@example.com>\"\n```\n\n----------------------------------------\n\nTITLE: RST License Header Comment Block\nDESCRIPTION: Apache License 2.0 header comment block in RST format describing the licensing terms for the file\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/impala/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Package Filter\nDESCRIPTION: Command to build documentation for specific provider packages using glob pattern\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs --package-filter apache-airflow-providers-*\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Use '__version__' in providers not 'version' (#31393)\nDESCRIPTION: This commit message, associated with version 3.2.0, standardizes the use of the '__version__' attribute instead of 'version' within Apache Airflow Providers. Commit hash: abea189022, Date: 2023-05-18.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n``Use '__version__' in providers not 'version' (#31393)``\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry for Version 3.0.3\nDESCRIPTION: Changelog entry noting addition of Trove classifiers in PyPI\nSOURCE: https://github.com/apache/airflow/blob/main/providers/samba/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n3.0.3\n.....\n\nMisc\n~~~~~\n\n* ``Add Trove classifiers in PyPI (Framework :: Apache Airflow :: Provider)``\n```\n\n----------------------------------------\n\nTITLE: Installing Celery Provider Package for Apache Airflow\nDESCRIPTION: Command to install the Celery provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-celery\n```\n\n----------------------------------------\n\nTITLE: Version Header 7.3.2 in RST\nDESCRIPTION: RST format version header showing version 7.3.2 and subsections\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n7.3.2\n.....\n\nBug Fixes\n~~~~~~~~~\n\n* ``Fixing DatabricksNotebookOperator invalid dependency graph issue (#48492)``\n\nMisc\n~~~~\n\n* ``remove superfluous else block (#49199)``\n```\n\n----------------------------------------\n\nTITLE: Adding Code Snippet Formatting in Docstrings with Ruff in Python\nDESCRIPTION: Implements code snippet formatting in docstrings using the Ruff tool for the Hive provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n\"Add code snippet formatting in docstrings via Ruff (#36262)\"\n```\n\n----------------------------------------\n\nTITLE: Including Provider Configurations Reference in reStructuredText\nDESCRIPTION: This directive includes external documentation for provider configurations in the current document.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst\n```\n\n----------------------------------------\n\nTITLE: Getting Dataform Workflow Invocation in Python\nDESCRIPTION: This snippet shows how to get a Workflow Invocation using the DataformGetWorkflowInvocationOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_get_workflow_invocation]\n# [END howto_operator_get_workflow_invocation]\n```\n\n----------------------------------------\n\nTITLE: Setting Secret Backend via Environment Variable\nDESCRIPTION: Environment variable configuration for enabling Google Cloud Secret Manager backend.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/secrets-backends/google-cloud-secret-manager-backend.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__SECRETS__BACKEND=airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: Sphinx documentation directive to include external security documentation file from a common development directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: RST include directive that imports security-related documentation from a common development source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Cancelling Dataform Workflow Invocation in Python\nDESCRIPTION: This snippet shows how to cancel a Workflow Invocation using the DataformCancelWorkflowInvocationOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataform.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# [START howto_operator_cancel_workflow_invocation]\n# [END howto_operator_cancel_workflow_invocation]\n```\n\n----------------------------------------\n\nTITLE: Excluding Changes from Changelog in RST\nDESCRIPTION: RST syntax for excluding certain changes from the changelog, used to maintain a clean and relevant changelog.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Fix and reapply templates for provider documentation (#35686)``\n   * ``Prepare docs 3rd wave of Providers October 2023 - FIX (#35233)``\n   * ``Prepare docs 2nd wave of Providers November 2023 (#35836)``\n   * ``Use reproducible builds for providers (#35693)``\n   * ``Prepare docs 1st wave of Providers November 2023 (#35537)``\n   * ``Prepare docs 3rd wave of Providers October 2023 (#35187)``\n   * ``Pre-upgrade 'ruff==0.0.292' changes in providers (#35053)``\n```\n\n----------------------------------------\n\nTITLE: Migrating Papermill Example DAGs to New Design\nDESCRIPTION: Excluded Change (Version 3.0.0): Updates the Papermill example DAGs to conform to a new design pattern (related to issue #22456), referencing pull request #24146.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Migrate Papermill example DAGs to new design #22456 (#24146)``\n```\n\n----------------------------------------\n\nTITLE: Including External reStructuredText Content using Sphinx Directive\nDESCRIPTION: This reStructuredText directive utilizes Sphinx's 'include' feature to incorporate the content from the specified file path ('/../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst') into the current document at build time. This mechanism promotes content reuse for common sections like installation instructions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/vertica/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests from Host\nDESCRIPTION: Commands to run different types of integration tests from the host system using Breeze\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/integration_tests.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-integration-tests  --db-reset --integration all-testable\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-integration-tests  --db-reset --integration all-testable\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-integration-tests  --db-reset --integration mongo\n```\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing core-integration-tests --db-reset --integration kerberos\n```\n\n----------------------------------------\n\nTITLE: Setting GitHub Authentication Token\nDESCRIPTION: Command to set up GitHub authentication token for automated PR creation with cherry-picker.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_AIRFLOW3_DEV.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport GH_AUTH={token}\n```\n\n----------------------------------------\n\nTITLE: Table Header Format\nDESCRIPTION: ReStructuredText table header format used in changelog\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_9\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ===================================================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ===================================================================================================================\n```\n\n----------------------------------------\n\nTITLE: Indicating Skipped Provider Tests (Boolean String)\nDESCRIPTION: A boolean flag indicating if provider tests should be skipped, typically on non-main branches or when no provider changes are detected. 'true' means tests are skipped.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_33\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Converting Datetime in Templates\nDESCRIPTION: Example of converting UTC datetime to local timezone in Airflow templates using pendulum.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/timezone.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pendulum\n\nlocal_tz = pendulum.timezone(\"Europe/Amsterdam\")\nlocal_tz.convert(logical_date)\n```\n\n----------------------------------------\n\nTITLE: Downloading Provider Distribution from PyPI\nDESCRIPTION: Command to download a specific version of an Airflow provider distribution from PyPI\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/testing_packages.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip download apache-airflow-providers-<PROVIDER_NAME>==X.Y.Zrc1 --dest dist --no-deps\n```\n\n----------------------------------------\n\nTITLE: Version Reference - Apache Kylin Provider\nDESCRIPTION: Package identifier for the Apache Kylin provider showing the full package name used for distribution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/kylin/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nPackage apache-airflow-providers-apache-kylin\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment\nDESCRIPTION: Commands to create and activate a Python virtual environment for local development\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/18_contribution_workflow.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv venv && source venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Preview Ruff Recommended Fixes\nDESCRIPTION: Shows recommended fixes for DAG compatibility issues without applying them.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/installation/upgrading_to_airflow3.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nruff check dag/ --select AIR301 --show-fixes\n```\n\n----------------------------------------\n\nTITLE: Building Provider Distributions\nDESCRIPTION: Command to build specified Airflow provider distributions from sources using Breeze\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/testing_packages.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-provider-distributions <PROVIDER_1> <PROVIDER_2> ... <PROVIDER_N>\n```\n\n----------------------------------------\n\nTITLE: Installing Snowflake Provider Package via pip\nDESCRIPTION: Command to install the Apache Airflow Providers Snowflake package on an existing Airflow 2 installation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-snowflake\n```\n\n----------------------------------------\n\nTITLE: Setting Up Kubernetes Test Environment\nDESCRIPTION: Command to initialize the Kubernetes testing environment by setting up required tools and virtualenv.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbreeze k8s setup-env\n```\n\n----------------------------------------\n\nTITLE: Running Parallel Helm Tests in Breeze Container\nDESCRIPTION: Commands for running Helm tests in parallel within the breeze container environment\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/helm_unit_tests.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npytest helm-tests -n auto\n```\n\n----------------------------------------\n\nTITLE: Checking Worker Secrets Backend Configuration\nDESCRIPTION: Bash command to check which secrets backend is currently configured for workers.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/security/secrets/secrets-backend/index.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ airflow config get-value workers secrets_backend\nairflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend\n```\n\n----------------------------------------\n\nTITLE: Version Identifier RST Block\nDESCRIPTION: RST formatted version history entry showing changes excluded from the changelog.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ftp/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Add full automation for min Airflow version for providers (#30994)``\n   * ``Add mechanism to suspend providers (#30422)``\n   * ``Use '__version__' in providers not 'version' (#31393)``\n   * ``Fixing circular import error in providers caused by airflow version check (#31379)``\n   * ``Prepare docs for May 2023 wave of Providers (#31252)``\n```\n\n----------------------------------------\n\nTITLE: Remove Unused Provider Distribution (Excluded from v1.4.1 Changelog)\nDESCRIPTION: Notes the removal of an unused provider's distribution artifact, referenced by pull request #46608. This change was intentionally excluded from the main changelog notes for version 1.4.1.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nRemoved the unused provider's distribution (#46608)\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs for 1st wave Providers (Jan 2024)\nDESCRIPTION: This commit message (hash 19ebcac239, dated 2024-01-07) marks the preparation of documentation for the first wave of provider releases in January 2024 (issue #36640).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs 1st wave of Providers January 2024 (#36640)``\n```\n\n----------------------------------------\n\nTITLE: License Header Declaration\nDESCRIPTION: ASF License 2.0 declaration in ReStructuredText format\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n```\n\n----------------------------------------\n\nTITLE: Module Level DB Test Marking\nDESCRIPTION: Example of marking an entire module as database tests using pytestmark\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.models.dag import DAG\nfrom airflow.ti_deps.dep_context import DepContext\nfrom airflow.ti_deps.deps.task_concurrency_dep import TaskConcurrencyDep\n\npytestmark = pytest.mark.db_test\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare docs for new providers release (August 2022) (#25618)\nDESCRIPTION: This commit message, associated with version 3.1.0, signifies the preparation of documentation for the August 2022 release of Apache Airflow Providers. Commit hash: e5ac6c7cfb, Date: 2022-08-10.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_25\n\nLANGUAGE: text\nCODE:\n```\n``Prepare docs for new providers release (August 2022) (#25618)``\n```\n\n----------------------------------------\n\nTITLE: Getting Help with Breeze Commands in Bash\nDESCRIPTION: Command to display help information for Breeze.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nbreeze --help\n```\n\n----------------------------------------\n\nTITLE: Cloning Airflow Repository\nDESCRIPTION: Command to clone the Apache Airflow repository from GitHub\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/01_installation.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/apache/airflow.git\n```\n\n----------------------------------------\n\nTITLE: Displaying Docker Provider Package Information\nDESCRIPTION: This reStructuredText snippet shows the package name and a link to the Docker website. It introduces the detailed commit list for the Docker provider package.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/docker/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\nPackage apache-airflow-providers-docker\n------------------------------------------------------\n\n`Docker <https://www.docker.com/>`__\n\n\nThis is detailed commit list of changes for versions provider package: ``docker``.\nFor high-level changelog, see :doc:`package information including changelog <index>`.\n```\n\n----------------------------------------\n\nTITLE: Formatting Commit Table Header in Markdown\nDESCRIPTION: This snippet demonstrates how to create a table header for commit information in Markdown format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/docker/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  =================================================================\n```\n\n----------------------------------------\n\nTITLE: Defining Provider Package Name\nDESCRIPTION: Python module name for the DBT Cloud provider package\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dbt/cloud/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"apache-airflow-providers-dbt-cloud\"\n```\n\n----------------------------------------\n\nTITLE: Installing Google Provider Package with Cross-Provider Dependencies\nDESCRIPTION: Example command for installing the Google provider package with additional Amazon cross-provider dependencies using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-google[amazon]\n```\n\n----------------------------------------\n\nTITLE: Declaring Jira Provider Package Name\nDESCRIPTION: Package name declaration for the Jira provider using restructuredtext syntax.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/atlassian/jira/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n``apache-airflow-providers-jira``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Add documentation for bugfix release of Providers (#22383)\nDESCRIPTION: This commit message, associated with version 2.0.4, adds documentation related to a bugfix release for Apache Airflow Providers. Commit hash: d7dbfb7e26, Date: 2022-03-22.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/grpc/docs/commits.rst#2025-04-22_snippet_33\n\nLANGUAGE: text\nCODE:\n```\n``Add documentation for bugfix release of Providers (#22383)``\n```\n\n----------------------------------------\n\nTITLE: Adding Documentation for July 2022 Provider Release\nDESCRIPTION: Excluded Change (Version 3.1.0): Includes documentation updates for the July 2022 provider release cycle, referencing pull request #25030.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Add documentation for July 2022 Provider's release (#25030)``\n```\n\n----------------------------------------\n\nTITLE: Removing Custom Spark Home and Binaries\nDESCRIPTION: A breaking change in version 4.0.0 that removes support for custom Spark home directories and requires binaries to be available on the PATH.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/spark/docs/changelog.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"Remove custom spark home and custom binaries for spark (#27646)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Salesforce Provider with Tableau Extras (Obsolete)\nDESCRIPTION: Shell command demonstrating how to install the `apache-airflow-providers-salesforce` package with optional Tableau dependencies using the extras syntax. This method is noted as non-functional starting from version 5.0.0, as Tableau support was moved to a separate provider. Requires the pip package installer.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/salesforce/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-airflow-providers-salesforce[tableau]\n```\n\n----------------------------------------\n\nTITLE: Running Specific Provider Tests\nDESCRIPTION: Example of running only amazon and google provider tests\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-tests --test-type \"Providers[amazon,google]\"\n```\n\n----------------------------------------\n\nTITLE: Documenting Provider Update September 2023 (1st Wave)\nDESCRIPTION: This commit prepares the documentation for the first wave of Apache Airflow provider updates released in September 2023, tracked in issue #34201.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_19\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs for 09 2023 - 1st wave of Providers (#34201)\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry v2.0.3\nDESCRIPTION: Changelog entry noting addition of Trove classifiers in PyPI\nSOURCE: https://github.com/apache/airflow/blob/main/providers/datadog/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n2.0.3\n.....\n\nMisc\n~~~~~\n\n* ``Add Trove classifiers in PyPI (Framework :: Apache Airflow :: Provider)``\n```\n\n----------------------------------------\n\nTITLE: Package Name Definition in RST\nDESCRIPTION: Specifies the name of the Apache Pig provider package for Airflow\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pig/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n``apache-airflow-providers-apache-pig``\n```\n\n----------------------------------------\n\nTITLE: Version Header in RST Format\nDESCRIPTION: RST-formatted version header with underline decoration used in the changelog to denote version numbers.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n6.2.0\n.....\n```\n\n----------------------------------------\n\nTITLE: Example Code Comment\nDESCRIPTION: Code comment showing a feature addition reference\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/drill/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n* ``Add SQLExecuteQueryOperator (#25717)``\n```\n\n----------------------------------------\n\nTITLE: Migrating FileSensor Import in Apache Airflow (Markdown)\nDESCRIPTION: This snippet defines a migration rule for the FileSensor class, moving it from the airflow.sensors.filesystem module to the airflow.providers.standard.sensors.filesystem module. This change is identified by the ruff rule AIR303.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43890.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* ``airflow.sensors.filesystem.FileSensor``  ``airflow.providers.standard.sensors.filesystem.FileSensor``\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow SMTP Provider via Pip\nDESCRIPTION: Provides commands to install the `apache-airflow-providers-smtp` package using pip. The first command shows the standard installation. The second command demonstrates installing the package with optional extras, specifically `common.compat`, which might be needed for certain cross-provider features. Installation requires pip, Python versions 3.9-3.12, and Apache Airflow version 2.9.0 or higher.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/smtp/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-smtp\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-smtp[common.compat]\n```\n\n----------------------------------------\n\nTITLE: Installing FTP Extras for Apache Airflow\nDESCRIPTION: Command to install FTP hooks and operators for Apache Airflow. This enables integration with FTP services and is preinstalled with Airflow by default.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_58\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[ftp]'\n```\n\n----------------------------------------\n\nTITLE: Displaying Airbyte Provider Version 3.3.1 Changelog\nDESCRIPTION: Lists the changes made in version 3.3.1 of the Airbyte provider, including commit hashes, dates, and descriptions of changes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_32\n\nLANGUAGE: markdown\nCODE:\n```\n3.3.1\n.....\n\nLatest change: 2023-06-20\n\n==================================================================================================  ===========  =============================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =============================================================\n`79bcc2e668 <https://github.com/apache/airflow/commit/79bcc2e668e648098aad6eaa87fe8823c76bc69a>`__  2023-06-20   ``Prepare RC1 docs for June 2023 wave of Providers (#32001)``\n`8b146152d6 <https://github.com/apache/airflow/commit/8b146152d62118defb3004c997c89c99348ef948>`__  2023-06-20   ``Add note about dropping Python 3.7 for providers (#32015)``\n`a59076eaee <https://github.com/apache/airflow/commit/a59076eaeed03dd46e749ad58160193b4ef3660c>`__  2023-06-02   ``Add D400 pydocstyle check - Providers (#31427)``\n==================================================================================================  ===========  =============================================================\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Instructions in RST Documentation\nDESCRIPTION: ReStructuredText directive to include external documentation about installing Airflow providers from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Templating and Mapped Arguments Interaction in Python for Apache Airflow\nDESCRIPTION: Shows how templated fields and mapped arguments interact in Airflow, demonstrating that mapped templated fields are not templated and providing alternative approaches for interpolation.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/authoring-and-scheduling/dynamic-task-mapping.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef make_list():\n    return [\"{{ ds }}\"]\n\n\n@task\ndef printer(val):\n    print(val)\n\n\nprinter.expand(val=make_list())\n\n@task\ndef make_list(ds=None):\n    return [ds]\n\n\n@task\ndef make_list(**context):\n    return [context[\"task\"].render_template(\"{{ ds }}\", context)]\n```\n\n----------------------------------------\n\nTITLE: Getting Dataplex Entry Type using Airflow Python\nDESCRIPTION: This snippet demonstrates how to retrieve details of a specific Entry Type from a location in Google Cloud Dataplex Catalog using the `DataplexCatalogGetEntryTypeOperator` in an Airflow DAG. It references an external example file for the specific implementation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/dataplex.rst#2025-04-22_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n# .. exampleinclude:: /../../google/tests/system/google/cloud/dataplex/example_dataplex_catalog.py\n#     :language: python\n#     :dedent: 4\n#     :start-after: [START howto_operator_dataplex_catalog_get_entry_type]\n#     :end-before: [END howto_operator_dataplex_catalog_get_entry_type]\n\n# This example uses DataplexCatalogGetEntryTypeOperator.\n# Refer to the included example file for the full code.\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents with Sphinx Toctree - reStructuredText\nDESCRIPTION: These reStructuredText snippets use the '.. toctree::' directive to define navigation menus for Sphinx-based documentation. Each toctree creates navigable sections (Basics, References, System tests, Resources, Commits) with hidden entries and custom settings for max depth and captions. This structure enables organized, thematically grouped documentation links for users exploring the Tableau provider integration. Requires a Sphinx documentation build with the relevant files and structure present.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/tableau/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\\n    :hidden:\\n    :maxdepth: 1\\n    :caption: Basics\\n\\n    Home <self>\\n    Changelog <changelog>\\n    Security <security>\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\\n    :hidden:\\n    :maxdepth: 1\\n    :caption: References\\n\\n    Connection Types <connections/tableau>\\n    Operators <operators>\\n    Python API <_api/airflow/providers/tableau/index>\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\\n    :hidden:\\n    :maxdepth: 1\\n    :caption: System tests\\n\\n    System Tests <_api/tests/system/tableau/index>\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\\n    :hidden:\\n    :maxdepth: 1\\n    :caption: Resources\\n\\n    Example DAGs <https://github.com/apache/airflow/tree/providers-tableau/|version|/providers/tableau/tests/system/tableau>\\n    PyPI Repository <https://pypi.org/project/apache-airflow-providers-tableau/>\\n    Installing from sources <installing-providers-from-sources>\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\\n    :hidden:\\n    :maxdepth: 1\\n    :caption: Commits\\n\\n    Detailed list of commits <commits>\n```\n\n----------------------------------------\n\nTITLE: Including Apache License Header in RST Documentation\nDESCRIPTION: Standard Apache 2.0 license header text in RST format, followed by an include directive for security documentation from a common development source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/datadog/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: PATH Configuration for Yarn\nDESCRIPTION: Bash configuration to add Yarn's bin directory to the system PATH.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/15_node_environment_setup.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport PATH=\"$HOME/.yarn/bin:$PATH\"\n```\n\n----------------------------------------\n\nTITLE: License Header Comment Block\nDESCRIPTION: Standard Apache License 2.0 header text in RST format describing the licensing terms for this documentation file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pinot/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n .. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n```\n\n----------------------------------------\n\nTITLE: Installing Opsgenie Provider Package for Apache Airflow\nDESCRIPTION: This command installs the Opsgenie provider package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opsgenie/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-opsgenie\n```\n\n----------------------------------------\n\nTITLE: Adding Docs for RC2 Wave Jan 2024 (Commit Message)\nDESCRIPTION: Commit message indicating the addition of documentation for the Release Candidate 2 wave of Apache Airflow provider releases in the second round of January 2024, affecting the Airbyte provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_22\n\nLANGUAGE: text\nCODE:\n```\nAdd docs for RC2 wave of providers for 2nd round of Jan 2024 (#37019)\n```\n\n----------------------------------------\n\nTITLE: Adding Proxy User Template Check in Python\nDESCRIPTION: Implements a template check for the proxy_user parameter in the Hive provider to ensure correct usage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n\"Add proxy_user template check (#32334)\"\n```\n\n----------------------------------------\n\nTITLE: Defining Task Groups with Multiple Tasks in Airflow DAG using Python\nDESCRIPTION: This snippet shows how to create task groups containing multiple tasks with different operators and set up dependencies between them.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/telegram/docs/.latest-doc-only-change.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n    # section_1\n    with TaskGroup(\"section_1\", tooltip=\"Tasks for section_1\") as section_1:\n        task_1 = EmptyOperator(task_id=\"task_1\")\n        task_2 = EmptyOperator(task_id=\"task_2\")\n        task_3 = EmptyOperator(task_id=\"task_3\")\n\n        task_1 >> [task_2, task_3]\n\n    # section_2\n    with TaskGroup(\"section_2\", tooltip=\"Tasks for section_2\") as section_2:\n        task_1 = EmptyOperator(task_id=\"task_1\")\n\n        # inner_section_2\n        with TaskGroup(\"inner_section_2\", tooltip=\"Tasks for inner_section2\") as inner_section_2:\n            task_2 = BashOperator(\n                task_id=\"task_2\",\n                bash_command='echo \"TASK 2!!\";'\n            )\n            task_3 = EmptyOperator(task_id=\"task_3\")\n            task_4 = EmptyOperator(task_id=\"task_4\")\n\n            task_2 >> [task_3, task_4]\n\n        task_5 = EmptyOperator(task_id=\"task_5\")\n\n        task_1 >> inner_section_2 >> task_5\n\n    # section_3\n    with TaskGroup(\"section_3\", tooltip=\"Tasks for section_3\") as section_3:\n        def inner_func(multiple):\n            print(f\"multiple: {multiple}\")\n\n        with TaskGroup(\"inner_section_3\", tooltip=\"Tasks for inner_section3\") as inner_section_3:\n            task_1 = EmptyOperator(task_id=\"task_1\")\n            task_2 = PythonOperator(\n                task_id=\"task_2\",\n                python_callable=inner_func,\n                op_kwargs={'multiple': 100}\n            )\n            task_3 = EmptyOperator(task_id=\"task_3\")\n            task_4 = EmptyOperator(task_id=\"task_4\")\n\n            task_1 >> task_2 >> [task_3, task_4]\n```\n\n----------------------------------------\n\nTITLE: Example of Dependency Downgrade Output\nDESCRIPTION: Diff output showing example downgrades of dependencies when using UV sync with lowest-direct resolution. This illustrates how dependencies are adjusted for minimum version testing.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#2025-04-22_snippet_37\n\nLANGUAGE: diff\nCODE:\n```\n- aiohttp==3.9.5\n+ aiohttp==3.9.2\n- anyio==4.4.0\n+ anyio==3.7.1\n```\n\n----------------------------------------\n\nTITLE: Including Shared Security Documentation - Sphinx reStructuredText\nDESCRIPTION: This snippet uses the Sphinx \".. include::\" directive in reStructuredText to embed the contents of a shared security documentation file located at /../../../../devel-common/src/sphinx_exts/includes/security.rst. The include directive allows reusing documentation snippets across multiple files, ensuring consistency in security-related documentation throughout the project. The path provided is relative to the current file location, and the included file must be accessible during the Sphinx build.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment Block in RST\nDESCRIPTION: Standard Apache License 2.0 header comment block for the file, written in ReStructuredText format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/dingding/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n   software distributed under the License is distributed on an\n   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n   KIND, either express or implied.  See the License for the\n   specific language governing permissions and limitations\n   under the License.\n```\n\n----------------------------------------\n\nTITLE: Git Commit Hash Reference\nDESCRIPTION: Git commit hash identifier representing a specific version or change in the Apache Airflow repository.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/.latest-doc-only-change.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n7b2ec33c7ad4998d9c9735b79593fcdcd3b9dd1f\n```\n\n----------------------------------------\n\nTITLE: Adding init_command parameter example\nDESCRIPTION: Code showing how the init_command parameter can be configured in MySqlHook\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nAdd init_command parameter to MySqlHook (#33359)\n```\n\n----------------------------------------\n\nTITLE: Deleting Redshift Cluster Snapshot with RedshiftDeleteClusterSnapshotOperator\nDESCRIPTION: Example demonstrating how to delete a Redshift cluster snapshot using RedshiftDeleteClusterSnapshotOperator.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/operators/redshift/redshift_cluster.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndelete_cluster_snapshot = RedshiftDeleteClusterSnapshotOperator(\n    task_id='delete_cluster_snapshot',\n    snapshot_identifier=redshift_cluster_snapshot_identifier,\n    aws_conn_id='aws_default',\n)\n```\n\n----------------------------------------\n\nTITLE: Referencing Airflow Callback Configuration\nDESCRIPTION: Shows the configuration parameter name for on_success_callback that has modified behavior. This callback will no longer execute when tasks are skipped, representing a breaking change in Airflow's task execution flow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/40936.bugfix.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\non_success_callback\n```\n\n----------------------------------------\n\nTITLE: Enabling Git-Sync without Persistence\nDESCRIPTION: Helm command to enable git-sync without persistence, running as a sidecar on scheduler and worker pods.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/docs/manage-dag-files.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airflow apache-airflow/airflow \\\n  --set dags.persistence.enabled=false \\\n  --set dags.gitSync.enabled=true\n```\n\n----------------------------------------\n\nTITLE: Customizing Airflow Docker Image with Pre-Entrypoint Script\nDESCRIPTION: Demonstrates how to extend the Airflow Docker image to include a custom script that runs before the Airflow entrypoint.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/entrypoint.rst#2025-04-22_snippet_4\n\nLANGUAGE: docker\nCODE:\n```\nFROM airflow:2.9.0.dev0\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in ReStructuredText\nDESCRIPTION: A directive that includes an external security documentation file from a specified path. This directive imports content from the Airflow development common source directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/ssh/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Creating Roles Using Airflow CLI - Bash\nDESCRIPTION: This Bash code snippet demonstrates how to create new Airflow roles from the command line using the built-in CLI tool. Dependencies: Airflow must be installed and configured; the user executing the command must have rights to manage roles. Key parameter: a space-separated list of role names to be created. Input is the list of new role names, output is creation of those roles in the Airflow metadata database. Limitation: roles are created with no default permissions unless further configured.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/access-control.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nairflow roles create Role1 Role2\n```\n\n----------------------------------------\n\nTITLE: Specifying Apache Airflow Requirement for Apache Pig Provider\nDESCRIPTION: Requirement specification for the Apache Airflow version needed to use the Apache Pig provider package. The minimum required version of Apache Airflow is 2.9.0.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pig/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n``apache-airflow``  ``>=2.9.0``\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Airflow Providers Apprise Package via pip\nDESCRIPTION: Command to install the Apache Airflow Providers Apprise package on top of an existing Airflow 2 installation using pip.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apprise/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-airflow-providers-apprise\n```\n\n----------------------------------------\n\nTITLE: Configuring AVP Policy Store ID via Environment Variable (Bash)\nDESCRIPTION: Sets the Amazon Verified Permissions policy store ID using an environment variable. This is an alternative method to configuring it in `airflow.cfg`. Airflow reads this environment variable to identify the AVP policy store. Replace `<avp_policy_store_id>` with the actual ID.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/auth-manager/setup/amazon-verified-permissions.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW__AWS_AUTH_MANAGER__AVP_POLICY_STORE_ID='<avp_policy_store_id>'\n```\n\n----------------------------------------\n\nTITLE: Downloading Airflow Packages for Restricted Environments\nDESCRIPTION: Shell command for downloading Airflow wheel packages in restricted security environments.\nSOURCE: https://github.com/apache/airflow/blob/main/docker-stack-docs/build.rst#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\npip download apache-airflow==|airflow-version| \\\n    -d downloaded_airflow\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Tree Configuration\nDESCRIPTION: Configures the documentation tree structure with maxdepth of 1 and glob pattern to include all files in the directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/slack/docs/notifications/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Defining Admin Permissions - Flask AppBuilder (FAB) - Python\nDESCRIPTION: This Python snippet provides an example of how the Admin role in Airflow's FAB-based RBAC system is granted all available permissions. This includes every permission from Op and further access such as user-role assignments, complete resource access, and security administrative actions. Prerequisites include a properly configured FAB SecurityManager. This role is designed for full system management, with potential impact across all secured resources.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/access-control.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nadmin_perms = op_perms + [\n    (\"Users\", \"can_assign_roles\"),\n    (\"Roles\", \"can_edit\"),\n    (\"Resource\", \"can_read\"),\n    (\"Resource\", \"can_edit\"),\n    (\"Resource\", \"can_create\"),\n    (\"Resource\", \"can_delete\"),\n    (\"Security\", \"can_read\"),\n    (\"Security\", \"can_edit\"),\n    (\"Security\", \"can_create\"),\n    (\"Security\", \"can_delete\"),\n]\n\n```\n\n----------------------------------------\n\nTITLE: Including Security Information in reStructuredText\nDESCRIPTION: This snippet includes an external file containing security information for the Apache Airflow project documentation. It uses the reStructuredText include directive to reference the file.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Migration Rules for Ruff Linter in Apache Airflow\nDESCRIPTION: A markdown checklist specifying the migration rules needed for the Ruff linter, specifically mentioning the AIR302 rule for airflow.operators.subdag.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/41390.significant.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n* Migration rules needed\n\n  * ruff\n\n    * AIR302\n\n      * [x] ``airflow.operators.subdag.*``\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Google Cloud Storage to Amazon S3 using GCSToS3Operator in Python\nDESCRIPTION: This code snippet demonstrates how to use the GCSToS3Operator to copy data from a Google Cloud Storage bucket to an Amazon S3 bucket. It sets up the necessary task parameters including bucket names, object keys, and AWS credentials.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/transfer/gcs_to_s3.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nGCSToS3Operator(\n    task_id=\"gcs_to_s3_task\",\n    bucket=\"gcs-bucket\",\n    prefix=\"data\",\n    delimiter=\".csv\",\n    gcs_source_format=\"text/csv\",\n    aws_conn_id=\"aws_default\",\n    dest_s3_conn_id=\"amazon_s3_default\",\n    dest_s3_bucket=\"s3-bucket\",\n    dest_s3_key=\"s3_data\",\n    replace=False,\n    gzip=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Package Requirements Table in RST\nDESCRIPTION: A reStructuredText table showing the required pip packages and their minimum versions for the Cohere provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cohere/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n==================  ==================\nPIP package         Version required\n==================  ==================\n\"apache-airflow\"  \">=2.9.0\"\n\"cohere\"          \">=5.13.4\"\n==================  ==================\n```\n\n----------------------------------------\n\nTITLE: Creating Yandex Lockbox Secret for Airflow Connection (URI) using YC CLI\nDESCRIPTION: Demonstrates creating a secret in Yandex Lockbox using the `yc` command-line interface. The secret stores an Airflow connection named `mysqldb` using its URI representation as the payload's text value. The secret name follows the pattern `{connections_prefix}/{connection_name}`.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/yandex/docs/secrets-backends/yandex-cloud-lockbox-secret-backend.rst#2025-04-22_snippet_10\n\nLANGUAGE: console\nCODE:\n```\n$ yc lockbox secret create \\\n    --name airflow/connections/mysqldb \\\n    --payload '[{\"key\": \"value\", \"text_value\": \"mysql://myname:mypassword@myhost.com?this_param=some+val&that_param=other+val%2A\"}]'\ndone (1s)\nname: airflow/connections/mysqldb\n```\n\n----------------------------------------\n\nTITLE: Initializing Bootstrap 3 Typeahead Plugin in JavaScript\nDESCRIPTION: This code snippet represents the header of the Bootstrap 3 Typeahead plugin. It includes version information, project URL, and licensing details. The plugin is licensed under the Apache License, Version 2.0.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/3rd-party-licenses/LICENSE-bootstrap3-typeahead.txt#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* =============================================================\n * bootstrap3-typeahead.js v4.0.2\n * https://github.com/bassjobsen/Bootstrap-3-Typeahead\n * =============================================================\n * Original written by @mdo and @fat\n * =============================================================\n * Copyright 2014 Bass Jobsen @bassjobsen\n *\n * Licensed under the Apache License, Version 2.0 (the 'License');\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an 'AS IS' BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * ============================================================ */\n```\n\n----------------------------------------\n\nTITLE: Generating Custom Constraint File from Current Dependencies\nDESCRIPTION: Command to generate a new constraints file from currently installed packages. It filters out Airflow-specific packages and sorts the dependencies alphabetically.\nSOURCE: https://github.com/apache/airflow/blob/main/constraints/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip freeze | sort | \\\n    grep -v \"apache_airflow\" | \\\n    grep -v \"apache-airflow==\" | \\\n    grep -v \"@\" | \\\n    grep -v \"/opt/airflow\" > /opt/airflow/constraints/constraints-3.9.txt\n```\n\n----------------------------------------\n\nTITLE: Defining Viewer Permissions - Flask AppBuilder (FAB) - Python\nDESCRIPTION: This code snippet demonstrates how Viewer role permissions are defined in Airflow when using the FAB auth manager. It specifies a set of resource-action permission pairs that represent the minimum access for users assigned the Viewer role. Dependencies include Airflows FAB security model and the presence of well-defined resources. Inputs: Airflow SecurityManager configuration; Outputs: assignment of read-level permissions. Limitations: only descriptive of static role configuration, not a functional code sample.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/access-control.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nviewer_perms = [\n    (\"DAGs\", \"can_read\"),\n    (\"DAG Code\", \"can_read\"),\n    (\"DAG Runs\", \"can_read\"),\n    (\"Task Instances\", \"can_read\"),\n    (\"XCom Entries\", \"can_read\"),\n    (\"Audit Logs\", \"can_read\"),\n    (\"Import Errors\", \"can_read\"),\n    (\"Connections\", \"can_read\"),\n    (\"Event Logs\", \"can_read\"),\n    (\"Pools\", \"can_read\"),\n    (\"Variables\", \"can_read\"),\n    (\"SLA Misses\", \"can_read\"),\n    (\"Dag Warnings\", \"can_read\"),\n    (\"Trigger Requests\", \"can_read\"),\n    (\"Provider\", \"can_read\"),\n    (\"Roles\", \"can_read\"),\n    (\"Users\", \"can_read\"),\n    (\"Resource\", \"can_read\"),\n]\n\n```\n\n----------------------------------------\n\nTITLE: Docker Non-Root User Setup\nDESCRIPTION: Commands to enable Docker management as non-root user by adding user to docker group.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo groupadd docker\nsudo usermod -aG docker $USER\n```\n\n----------------------------------------\n\nTITLE: Verifying SHA512 Checksum for Apache Airflow Provider Packages\nDESCRIPTION: Command to verify the SHA512 checksum of an Apache Airflow provider package. This ensures the integrity of the downloaded file by comparing its checksum with the provided .sha512 file.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nshasum -a 512 apache-airflow-providers-********  | diff - apache-airflow-providers-********.sha512\n```\n\n----------------------------------------\n\nTITLE: MongoDB Breaking Changes Note\nDESCRIPTION: ReStructuredText note about breaking changes in version 3.0.0 related to Airflow compatibility\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mongo/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. note::\n  This release of provider is only available for Airflow 2.2+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n```\n\n----------------------------------------\n\nTITLE: Auto-generation Warning Notice\nDESCRIPTION: Notice indicating the file is automatically generated from a template\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hdfs/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!\n\n.. IF YOU WANT TO MODIFY THIS FILE, YOU SHOULD MODIFY THE TEMPLATE\n   `PROVIDER_COMMITS_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY\n```\n\n----------------------------------------\n\nTITLE: Templating Fields for GCE Instance Template Deletion in Python\nDESCRIPTION: Defines the template fields available for the ComputeEngineDeleteInstanceTemplateOperator, which allow for runtime parameter substitution.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/operators/cloud/compute.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntemplate_fields = (\n    \"project_id\",\n    \"resource_id\",\n    \"request_id\",\n    \"gcp_conn_id\",\n    \"api_version\",\n    \"impersonation_chain\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Apprise Notification Extra for Airflow\nDESCRIPTION: Command to install Apache Airflow with Apprise Notification integration.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[apprise]'\n```\n\n----------------------------------------\n\nTITLE: Running Ruff Check for Specific File using Breeze in Bash\nDESCRIPTION: This command uses Breeze to run the Ruff check for a specific file (tests/core.py) with verbose output.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nbreeze static-checks --type ruff --file tests/core.py --verbose\n```\n\n----------------------------------------\n\nTITLE: MongoDB Version Support Note\nDESCRIPTION: ReStructuredText note indicating version compatibility requirements for MongoDB provider\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mongo/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. note::\n  This release of provider is only available for Airflow 2.4+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Documentation in RST\nDESCRIPTION: Sphinx documentation directive to include external RST file containing instructions for installing Airflow providers from source\nSOURCE: https://github.com/apache/airflow/blob/main/providers/zendesk/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Increasing Python Recursion Limit for Large OpenAPI Imports - Python\nDESCRIPTION: This snippet sets a higher Python recursion limit to avoid RecursionError when importing large Airflow OpenAPI client modules. It should be run before importing airflow_client.client and the corresponding API and model modules. This pattern is critical for projects with large OpenAPI specifications that would otherwise exceed the default recursion stack during import-time. Dependencies: Python standard library sys module and the installed airflow_client package.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport sys\n\nsys.setrecursionlimit(1500)\nimport airflow_client.client\nfrom airflow_client.client.apis import *\nfrom airflow_client.client.models import *\n\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Engine\nDESCRIPTION: Commands to install Docker Engine and containerd on Ubuntu.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n```\n\n----------------------------------------\n\nTITLE: Defining Jenkins Provider Package in reStructuredText\nDESCRIPTION: Defines the Jenkins provider package and provides a link to the Jenkins website.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/jenkins/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\nPackage apache-airflow-providers-jenkins\n------------------------------------------------------\n\n`Jenkins <https://jenkins.io/>`__\n```\n\n----------------------------------------\n\nTITLE: Disabling Airflow Authentication via webserver_config.py (INI)\nDESCRIPTION: Shows how to configure `$AIRFLOW_HOME/webserver_config.py` to disable mandatory authentication and allow anonymous access. Setting `AUTH_ROLE_PUBLIC` defines the default role assigned to unauthenticated users (e.g., 'Admin' in this example).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/fab/docs/auth-manager/webserver-authentication.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\nAUTH_ROLE_PUBLIC = 'Admin'\n```\n\n----------------------------------------\n\nTITLE: Adding Dataplex Data Quality operators in Python\nDESCRIPTION: Adds new operators for working with Dataplex Data Quality in the Google provider for Apache Airflow. This enables data quality checks and management within Dataplex.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"Add Dataplex Data Quality operators. (#32256)\"\n```\n\n----------------------------------------\n\nTITLE: Invalid Template Field Parameter Naming\nDESCRIPTION: Example showing incorrect template field implementation where parameter name doesn't match the template field name.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass HelloOperator(BaseOperator):\n    template_fields = \"foo\"\n\n    def __init__(self, foo_id) -> None:  # should be def __init__(self, foo) -> None\n        self.foo = foo_id  # should be self.foo = foo\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Header\nDESCRIPTION: Standard Apache License 2.0 header that specifies the licensing terms for this file within the Apache Airflow project.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/tests/unit/amazon/aws/operators/test_notebook.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n```\n\n----------------------------------------\n\nTITLE: Invalid Template Field Value Modification\nDESCRIPTION: Example showing incorrect implementation where template field value is modified during assignment.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass HelloOperator(BaseOperator):\n    template_fields = \"foo\"\n\n    def __init__(self, foo) -> None:\n        self.foo = foo.lower()  # assignment should be only self.foo = foo\n```\n\n----------------------------------------\n\nTITLE: Installing Common-Compat Extras for Apache Airflow\nDESCRIPTION: Command to install compatibility code for old Airflow. This provides backward compatibility for deprecated Airflow features.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/extra-packages-ref.rst#2025-04-22_snippet_54\n\nLANGUAGE: bash\nCODE:\n```\npip install 'apache-airflow[common-compat]'\n```\n\n----------------------------------------\n\nTITLE: Adding sftp_prefetch parameter to SFTPToGCSOperator in Python\nDESCRIPTION: Adds a new parameter 'sftp_prefetch' to the SFTPToGCSOperator in the Google provider for Apache Airflow. This parameter likely controls prefetching behavior when transferring data from SFTP to Google Cloud Storage.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/changelog.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"Add parameter sftp_prefetch to SFTPToGCSOperator (#33274)\"\n```\n\n----------------------------------------\n\nTITLE: Fixing template_fields Type Hint for MyPy\nDESCRIPTION: Excluded Change (Version 2.2.0): Updates the type hint for `template_fields` to use `Sequence` for better compatibility with MyPy static type checking, referencing pull request #20571.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_27\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Fix template_fields type to have MyPy friendly Sequence type (#20571)``\n```\n\n----------------------------------------\n\nTITLE: Adding SQLExecuteQueryOperator in Python\nDESCRIPTION: This code snippet introduces a new SQLExecuteQueryOperator, which allows executing SQL queries as an Airflow operator. This enhances the capability to run SQL operations within Airflow DAGs.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"Add SQLExecuteQueryOperator (#25717)\"\n```\n\n----------------------------------------\n\nTITLE: Fixing SQLiteHook Compatibility with SQLAlchemy Engine\nDESCRIPTION: This code snippet addresses compatibility issues between SQLiteHook and SQLAlchemy engine. It ensures that SQLiteHook works correctly with the latest versions of SQLAlchemy.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sqlite/docs/changelog.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"Fix ''SqliteHook'' compatibility with SQLAlchemy engine (#23790)\"\n```\n\n----------------------------------------\n\nTITLE: Running Provider Tests with Exclusions\nDESCRIPTION: Example of running all provider tests except amazon and google\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/05_test_commands.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbreeze testing providers-tests --test-type \"Providers[-amazon,google]\"\n```\n\n----------------------------------------\n\nTITLE: Exposing Airflow Webserver Configuration - ini\nDESCRIPTION: This configuration snippet sets 'expose_config = True' in the [webserver] section of airflow.cfg, allowing the webserver to expose its configuration details. This is required only if you want the smoke test script to display configuration information; however, enabling this can have security implications and is not recommended for production deployments.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/README.md#2025-04-22_snippet_13\n\nLANGUAGE: ini\nCODE:\n```\n[webserver]\nexpose_config = True\n\n```\n\n----------------------------------------\n\nTITLE: Managing Dependencies with UV\nDESCRIPTION: Commands for adding and removing dependencies using UV package manager.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nuv add <package>\nuv remove <package>\n```\n\n----------------------------------------\n\nTITLE: Generating Python Client with Local Hatch\nDESCRIPTION: Command to generate Airflow Python client code using a local Hatch environment instead of dockerized one. Includes flag to specify local Hatch usage.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-python-client --distribution-format both --use-local-hatch\n```\n\n----------------------------------------\n\nTITLE: Fixing Null Logical Date in TimeDeltaSensorAsync\nDESCRIPTION: Commit message describing a fix for handling null logical dates within the 'TimeDeltaSensorAsync' component. References pull request #47652.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nHandle null logical date in TimeDeltaSensorAsync (#47652)\n```\n\n----------------------------------------\n\nTITLE: Enabling System Tests (Boolean String)\nDESCRIPTION: A boolean flag indicating whether system tests should be run. 'true' enables these tests.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md#2025-04-22_snippet_21\n\nLANGUAGE: text\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Setting up Kubernetes Environment for Helm Chart\nDESCRIPTION: Commands to set up the Kubernetes environment and enter the k8s shell for Helm operations.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nbreeze k8s setup-env\nbreeze k8s shell\n```\n\n----------------------------------------\n\nTITLE: Complex Customized CI Image Build with Microsoft SQL Dependencies\nDESCRIPTION: Advanced Docker build command that demonstrates complex customization including Microsoft SQL Server dependencies, custom apt commands, and environment variables.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build . -f Dockerfile.ci \\\n  --pull \\\n  --build-arg PYTHON_BASE_IMAGE=\"python:3.9-slim-bookworm\" \\\n  --build-arg AIRFLOW_INSTALLATION_METHOD=\"apache-airflow\" \\\n  --build-arg ADDITIONAL_AIRFLOW_EXTRAS=\"slack\" \\\n  --build-arg ADDITIONAL_PYTHON_DEPS=\"apache-airflow-providers-odbc \\\n      azure-storage-blob \\\n      sshtunnel \\\n      google-api-python-client \\\n      oauth2client \\\n      beautifulsoup4 \\\n      dateparser \\\n      rocketchat_API \\\n      typeform\" \\\n  --build-arg ADDITIONAL_DEV_APT_DEPS=\"msodbcsql17 unixodbc-dev g++\" \\\n  --build-arg ADDITIONAL_DEV_APT_COMMAND=\"curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add --no-tty - && curl https://packages.microsoft.com/config/debian/12/prod.list > /etc/apt/sources.list.d/mssql-release.list\" \\\n  --build-arg ADDITIONAL_DEV_ENV_VARS=\"ACCEPT_EULA=Y\"\n  --tag my-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Docker Installation Test\nDESCRIPTION: Command to verify Docker installation by running hello-world container.\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run hello-world\n```\n\n----------------------------------------\n\nTITLE: Generating Airflow Dependency Tree Using UV in Breeze Environment\nDESCRIPTION: Command used to generate the dep_tree.txt file containing the dependency tree of Airflow 2.9.3. This is executed within the Breeze development environment and uses the UV package manager to output the complete dependency hierarchy.\nSOURCE: https://github.com/apache/airflow/blob/main/generated/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv tree --no-dedupe  > /opt/airflow/generated/dep_tree.txt\n```\n\n----------------------------------------\n\nTITLE: Adding Kubernetes 1.21 Support in Helm Chart\nDESCRIPTION: Adds support for Kubernetes version 1.21 in the Airflow Helm chart.\nSOURCE: https://github.com/apache/airflow/blob/main/chart/RELEASE_NOTES.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n- Add kubernetes 1.21 support (#19557)\n```\n\n----------------------------------------\n\nTITLE: Verifying Production Images in Shell\nDESCRIPTION: Script to verify Apache Airflow production images for different Python versions using docker pull and breeze commands.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#2025-04-22_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\nfor PYTHON in 3.9 3.10 3.11 3.12\ndo\n    docker pull apache/airflow:${VERSION}-python${PYTHON}\n    breeze prod-image verify --image-name apache/airflow:${VERSION}-python${PYTHON}\ndone\ndocker pull apache/airflow:${VERSION}\nbreeze prod-image verify --image-name apache/airflow:${VERSION}\n```\n\n----------------------------------------\n\nTITLE: Specifying Migration Rules for Ruff Linter in Markdown\nDESCRIPTION: This snippet defines migration rules for the ruff linter, specifically the AIR303 rule. It outlines the changes in import paths for various trigger modules moving from airflow.triggers to airflow.providers.standard.triggers.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/newsfragments/43608.significant.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* ruff\n\n  * AIR303\n\n    * [x] ``airflow.triggers.external_task.*``  ``airflow.providers.standard.triggers.external_task.*``\n    * [x] ``airflow.triggers.file.*``  ``airflow.providers.standard.triggers.file.*``\n    * [x] ``airflow.triggers.temporal.*``  ``airflow.providers.standard.triggers.temporal.*``\n```\n\n----------------------------------------\n\nTITLE: Building CI Docker Image with Additional Python Dependency (mssql) - Docker/Bash\nDESCRIPTION: This command builds the Apache Airflow CI Docker image with an additional Python package ('mssql'). The ADDITIONAL_PYTHON_DEPS argument specifies extra pip-installable dependencies beyond those in AIRFLOW_EXTRAS. Requires Docker and the Airflow Dockerfile.ci.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build . -f Dockerfile.ci \\\n  --pull \\\n  --build-arg PYTHON_BASE_IMAGE=\"python:3.9-slim-bookworm\" \\\n  --build-arg ADDITIONAL_PYTHON_DEPS=\"mssql\" --tag my-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Bump minimum Airflow version for providers\nDESCRIPTION: This commit message (hash d0918d77ee, dated 2023-12-07) indicates that the minimum required Apache Airflow version for using the providers has been increased to 2.6.0 (issue #36017).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_22\n\nLANGUAGE: text\nCODE:\n```\n``Bump minimum Airflow version in providers to Airflow 2.6.0 (#36017)``\n```\n\n----------------------------------------\n\nTITLE: Building Customized CI Image with Docker Build\nDESCRIPTION: Advanced Docker build command to create a customized CI image with Python 3.9, additional Airflow extras, and Python dependencies. Requires DOCKER_BUILDKIT to be enabled.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build . -f Dockerfile.ci \\\n  --pull \\\n  --build-arg PYTHON_BASE_IMAGE=\"python:3.9-slim-bookworm\" \\\n  --build-arg ADDITIONAL_AIRFLOW_EXTRAS=\"jdbc\" \\\n  --build-arg ADDITIONAL_PYTHON_DEPS=\"pandas\" \\\n  --build-arg ADDITIONAL_DEV_APT_DEPS=\"gcc g++\" \\\n  --tag my-image:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Preparing Provider Distributions with Breeze (Bash)\nDESCRIPTION: Builds provider distribution packages using the Breeze command `release-management prepare-provider-distributions`. By default, it prepares both wheel and sdist formats for all providers and outputs them to the `dist` folder. Note that this command cleans the `dist` directory beforehand.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-provider-distributions\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fix databricks_sql hook query failing on empty result\nDESCRIPTION: This commit message (hash 13b0930bf4, dated 2024-01-17) addresses a bug in the `databricks_sql` hook where queries failed when encountering an empty result set, specifically with the `return_tuple` option (issue #36827).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n``Fix databricks_sql hook query failing on empty result for return_tuple (#36827)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Remove offset-based pagination from DatabricksHook list_jobs\nDESCRIPTION: This commit message (hash 10bac853d2, dated 2023-11-03) indicates the removal of offset-based pagination from the `list_jobs` function within the `DatabricksHook` (issue #34926), likely replaced by a different pagination method.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_31\n\nLANGUAGE: text\nCODE:\n```\n``Remove offset-based pagination from 'list_jobs' function in 'DatabricksHook' (#34926)``\n```\n\n----------------------------------------\n\nTITLE: Preparing Specific Provider Distributions with Breeze (Bash)\nDESCRIPTION: Demonstrates how to build distributions for specific providers (e.g., 'google', 'amazon') using the Breeze `release-management prepare-provider-distributions` command. Users can specify one or more provider names as arguments.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management prepare-provider-distributions google amazon\n```\n\n----------------------------------------\n\nTITLE: Updating Commit References in Python\nDESCRIPTION: This code snippet shows the format used to reference specific Git commits in the changelog. It includes the commit hash, link to GitHub, date, and a brief description of the change.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n`9053de7718 <https://github.com/apache/airflow/commit/9053de771873561aaf21a930056666e043b14b9c>`__  2021-11-10   ``Do not check for S3 key before attempting download (#19504)``\n```\n\n----------------------------------------\n\nTITLE: Documenting Breaking Changes in RST\nDESCRIPTION: Lists breaking changes for version 0.0.1, including parameter changes in BranchDayOfWeekOperator and PythonVirtualenvOperator modifications.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n\"* ``In BranchDayOfWeekOperator, DayOfWeekSensor, BranchDateTimeOperator parameter use_task_execution_date has been removed. Please use use_task_logical_date.``\\n* ``PythonVirtualenvOperator uses built-in venv instead of virtualenv package.``\\n* ``is_venv_installed method has been removed from PythonVirtualenvOperator as venv is built-in.``\\n\\n* ``Initial version of the provider. (#41564)``\"\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: Directive to include security documentation from an external source. This imports shared security content from the devel-common directory into the current document.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Building Documentation Package\nDESCRIPTION: Commands to checkout specific helm chart version and build documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\ncd \"${AIRFLOW_REPO_ROOT}\"\ngit checkout helm-chart/${VERSION}\nbreeze build-docs helm-chart --clean-build\n```\n\n----------------------------------------\n\nTITLE: Defining Connection Keyword Arguments in Airflow Provider - JSON\nDESCRIPTION: This snippet demonstrates the previously supported way of specifying connection keyword arguments (such as autocommit and ansi) as strings in the connect_kwargs dictionary of an Airflow connection. For example, the 'autocommit' key is set to the string 'false', and 'ansi' is set to 'true'. This method is deprecated; as of recent versions, booleans should be used instead of strings. The snippet expects a JSON object as input, mapping parameter names to string values.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/odbc/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\\n   \\\"connect_kwargs\\\": {\\n      \\\"autocommit\\\": \\\"false\\\",\\n      \\\"ansi\\\": \\\"true\\\"\\n   }\\n}\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Documentation\nDESCRIPTION: ReStructuredText format changelog documenting version history, bug fixes, features and breaking changes for the Neo4j provider package. Includes information about version compatibility, template fields, hooks and operator updates.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/neo4j/docs/changelog.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\nThis release of provider is only available for Airflow 2.2+ as explained in the\n`Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!)\n```\n\n----------------------------------------\n\nTITLE: Version Entry Table\nDESCRIPTION: Tabular format showing version updates and changes\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/beam/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\ndcdcf3a2b8                                                                                           2022-06-09   Update release notes for RC2 release of Providers for May 2022 (#24307)\n```\n\n----------------------------------------\n\nTITLE: Publishing Airflow Documentation with Breeze (Bash)\nDESCRIPTION: Executes the `breeze release-management publish-docs` command. This command is used to publish the Apache Airflow documentation, which should have been previously generated using the `build-docs` command within Breeze, to the official `airflow-site` repository.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/09_release_management_tasks.rst#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nbreeze release-management publish-docs\n```\n\n----------------------------------------\n\nTITLE: Git Commit Log Entry Format\nDESCRIPTION: Structured format showing git commit hashes, dates, and commit messages in a changelog style table\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_7\n\nLANGUAGE: rst\nCODE:\n```\n`988da34fab <https://github.com/apache/airflow/commit/988da34fab87d3425a5ad60a550d41dc79fc8b24>`__  2024-02-08   ``replace .rst with .html in links (#37265)``\n```\n\n----------------------------------------\n\nTITLE: Installing Breeze Development Environment\nDESCRIPTION: Commands to install and set up Breeze environment with autocomplete\nSOURCE: https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nbreeze setup autocomplete\nbreeze --python 3.9 --backend postgres\n```\n\n----------------------------------------\n\nTITLE: Installing Breeze with PIPX\nDESCRIPTION: Alternative command to install Breeze using PIPX package manager in editable mode with force flag for dependency updates.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npipx install -e ./dev/breeze --force\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Update version and add breaking change for Databricks\nDESCRIPTION: This commit message (hash 77b563bfc5, dated 2023-12-23) involves updating the version number and documenting a breaking change specifically for the Databricks provider (issue #36382).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n``Update version and added breaking change for databricks provider (#36382)``\n```\n\n----------------------------------------\n\nTITLE: Adding Support for Python 3.10\nDESCRIPTION: Miscellaneous Change (Version 2.2.1): Introduces compatibility with Python 3.10.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/papermill/docs/changelog.rst#2025-04-22_snippet_21\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``Support for Python 3.10``\n```\n\n----------------------------------------\n\nTITLE: Displaying Container Directory Structure for CI Environment\nDESCRIPTION: Shows the layout and purpose of key directories inside the CI container environment, including source code locations and dynamic file storage\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/10_advanced_breeze_topics.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n  /opt/airflow - Contains sources of Airflow mounted from the host (AIRFLOW_SOURCES).\n  /root/airflow - Contains all the \"dynamic\" Airflow files (AIRFLOW_HOME), such as:\n      airflow.db - sqlite database in case sqlite is used;\n      logs - logs from Airflow executions;\n      unittest.cfg - unit test configuration generated when entering the environment;\n      webserver_config.py - webserver configuration generated when running Airflow in the container.\n  /files - files mounted from \"files\" folder in your sources. You can edit them in the host as well\n      dags - this is the folder where Airflow DAGs are read from\n      airflow-breeze-config - this is where you can keep your own customization configuration of breeze\n```\n\n----------------------------------------\n\nTITLE: Adding EKS Overrides for AWS Batch Job Submission\nDESCRIPTION: Implemented EKS overrides functionality for submitting jobs in AWS Batch.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"EKS Overrides for AWS Batch submit_job (#40718)\"\n```\n\n----------------------------------------\n\nTITLE: Building Docs Without Spellcheck using Breeze\nDESCRIPTION: This command builds the documentation without running the spellcheck using the Breeze development environment.\nSOURCE: https://github.com/apache/airflow/blob/main/devel-common/src/docs/README.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs --docs-only\n```\n\n----------------------------------------\n\nTITLE: Adding Dynamic Connection Form Fields per Airflow Provider\nDESCRIPTION: This commit message indicates the implementation of support for dynamic connection form fields, allowing different fields based on the selected provider. It references pull request #12558.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_52\n\nLANGUAGE: text\nCODE:\n```\nAdd support for dynamic connection form fields per provider (#12558)\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in RST\nDESCRIPTION: RST directive to include external security documentation from the devel-common directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mongo/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Adding Oracle Connection Link in RST\nDESCRIPTION: This commit (3b4fdd0a7a, committed on 2021-05-06) adds a link related to Oracle connections, likely within the documentation. Refers to issue #15632.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_33\n\nLANGUAGE: rst\nCODE:\n```\nadd oracle  connection link (#15632)\n```\n\n----------------------------------------\n\nTITLE: Relocating SmoothOperator to Standard Provider with Tests\nDESCRIPTION: Commit message detailing the relocation of 'SmoothOperator' to a standard provider structure and the addition of associated tests. References pull request #47530.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nRelocate SmoothOperator to standard provider and add tests (#47530)\n```\n\n----------------------------------------\n\nTITLE: Move tests_common Package (Excluded from v1.4.1 Changelog)\nDESCRIPTION: Describes the relocation of the 'tests_common' package to the 'devel-common' project, referenced by pull request #47281. This change was intentionally excluded from the main changelog notes for version 1.4.1.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/pgvector/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nMove tests_common package to devel-common project (#47281)\n```\n\n----------------------------------------\n\nTITLE: Creating OpenSearch Documentation Table of Contents in reStructuredText\nDESCRIPTION: Sets up a table of contents directive that includes all files in the current directory using the glob pattern. The maxdepth parameter is set to 1 to limit the depth of the table of contents.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/connections/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Including RST Documentation for Provider Installation\nDESCRIPTION: RST directive to include external documentation about installing Airflow providers from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/druid/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Spellcheck Only\nDESCRIPTION: Command to run only the spellcheck phase of documentation building\nSOURCE: https://github.com/apache/airflow/blob/main/dev/breeze/doc/03_developer_tasks.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbreeze build-docs --spellcheck-only\n```\n\n----------------------------------------\n\nTITLE: Documenting Provider Changelog with reStructuredText - reStructuredText\nDESCRIPTION: This snippet defines the structure and formatting for changelog entries in the provider documentation using reStructuredText. It organizes changes into sections (Features, Bug Fixes, Misc, Breaking changes) with strict indentation and bullet formatting for clarity. Prerequisites: familiarity with reStructuredText formatting and Sphinx; inputs are text entries detailing each change; outputs are human-readable changelog documentation in documentation builds. Only text formatting rules apply, with no inherent constraints beyond honest, structured descriptions.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/changelog.rst#2025-04-22_snippet_19\n\nLANGUAGE: reStructuredText\nCODE:\n```\n2.6.0\n.....\n\nFeatures\n~~~~~~~~\n\n* ``More operators for Databricks Repos (#22422)``\n* ``Add a link to Databricks Job Run (#22541)``\n* ``Databricks SQL operators are now Python 3.10 compatible (#22886)``\n\nBug Fixes\n~~~~~~~~~\n\n* ``Databricks: Correctly handle HTTP exception (#22885)``\n\nMisc\n~~~~\n\n* ``Refactor 'DatabricksJobRunLink' to not create ad hoc TaskInstances (#22571)``\n\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Update black precommit (#22521)``\n   * ``Fix new MyPy errors in main (#22884)``\n   * ``Prepare mid-April provider documentation. (#22819)``\n\n   * ``Prepare for RC2 release of March Databricks provider (#22979)``\n\n2.5.0\n.....\n\nFeatures\n~~~~~~~~\n\n* ``Operator for updating Databricks Repos (#22278)``\n\nBug Fixes\n~~~~~~~~~\n\n* ``Fix mistakenly added install_requires for all providers (#22382)``\n\n2.4.0\n.....\n\nFeatures\n~~~~~~~~\n\n* ``Add new options to DatabricksCopyIntoOperator (#22076)``\n* ``Databricks hook - retry on HTTP Status 429 as well (#21852)``\n\nMisc\n~~~~\n\n* ``Skip some tests for Databricks from running on Python 3.10 (#22221)``\n\n2.3.0\n.....\n\nFeatures\n~~~~~~~~\n\n* ``Add-showing-runtime-error-feature-to-DatabricksSubmitRunOperator (#21709)``\n* ``Databricks: add support for triggering jobs by name (#21663)``\n* ``Added template_ext = ('.json') to databricks operators #18925 (#21530)``\n* ``Databricks SQL operators (#21363)``\n\nBug Fixes\n~~~~~~~~~\n\n* ``Fixed changelog for January 2022 (delayed) provider's release (#21439)``\n\nMisc\n~~~~\n\n* ``Support for Python 3.10``\n* ``Updated Databricks docs for correct jobs 2.1 API and links (#21494)``\n\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Add documentation for January 2021 providers release (#21257)``\n   * ``Remove ':type' lines now sphinx-autoapi supports typehints (#20951)``\n\n2.2.0\n.....\n\nFeatures\n~~~~~~~~\n\n* ``Add 'wait_for_termination' argument for Databricks Operators (#20536)``\n* ``Update connection object to ''cached_property'' in ''DatabricksHook'' (#20526)``\n* ``Remove 'host' as an instance attr in 'DatabricksHook' (#20540)``\n* ``Databricks: fix verification of Managed Identity (#20550)``\n\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Fix MyPy Errors for Databricks provider. (#20265)``\n   * ``Use typed Context EVERYWHERE (#20565)``\n   * ``Fix template_fields type to have MyPy friendly Sequence type (#20571)``\n   * ``Fix mypy databricks operator (#20598)``\n   * ``Update documentation for provider December 2021 release (#20523)``\n\n2.1.0\n.....\n\nFeatures\n~~~~~~~~\n\n* ``Databricks: add more methods to represent run state information (#19723)``\n* ``Databricks - allow Azure SP authentication on other Azure clouds (#19722)``\n* ``Databricks: allow to specify PAT in Password field (#19585)``\n* ``Databricks jobs 2.1 (#19544)``\n* ``Update Databricks API from 2.0 to 2.1 (#19412)``\n* ``Authentication with AAD tokens in Databricks provider (#19335)``\n* ``Update Databricks operators to match latest version of API 2.0 (#19443)``\n* ``Remove db call from DatabricksHook.__init__() (#20180)``\n\nBug Fixes\n~~~~~~~~~\n\n* ``Fixup string concatenations (#19099)``\n* ``Databricks hook: fix expiration time check (#20036)``\n\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Prepare documentation for October Provider's release (#19321)``\n   * ``Refactor DatabricksHook (#19835)``\n   * ``Update documentation for November 2021 provider's release (#19882)``\n   * ``Unhide changelog entry for databricks (#20128)``\n   * ``Update documentation for RC2 release of November Databricks Provider (#20086)``\n\n2.0.2\n.....\n\nBug Fixes\n~~~~~~~~~\n   * ``Move DB call out of DatabricksHook.__init__ (#18339)``\n\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Static start_date and default arg cleanup for misc. provider example DAGs (#18597)``\n\n2.0.1\n.....\n\nMisc\n~~~~\n\n* ``Optimise connection importing for Airflow 2.2.0``\n\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Update description about the new ''connection-types'' provider meta-data (#17767)``\n   * ``Import Hooks lazily individually in providers manager (#17682)``\n   * ``Prepares docs for Rc2 release of July providers (#17116)``\n   * ``Prepare documentation for July release of providers. (#17015)``\n   * ``Removes pylint from our toolchain (#16682)``\n\n2.0.0\n.....\n\nBreaking changes\n~~~~~~~~~~~~~~~~\n\n* ``Auto-apply apply_default decorator (#15667)``\n\n.. warning:: Due to apply_default decorator removal, this version of the provider requires Airflow 2.1.0+.\n   If your Airflow version is < 2.1.0, and you want to install this provider version, first upgrade\n   Airflow to at least version 2.1.0. Otherwise your Airflow package version will be upgraded\n   automatically and you will have to manually run ``airflow upgrade db`` to complete the migration.\n\n.. Below changes are excluded from the changelog. Move them to\n   appropriate section above if needed. Do not delete the lines(!):\n   * ``Prepares provider release after PIP 21 compatibility (#15576)``\n   * ``An initial rework of the 'Concepts' docs (#15444)``\n   * ``Remove Backport Providers (#14886)``\n   * ``Updated documentation for June 2021 provider release (#16294)``\n   * ``Add documentation for Databricks connection (#15410)``\n   * ``More documentation update for June providers release (#16405)``\n   * ``Synchronizes updated changelog after buggfix release (#16464)``\n\n1.0.1\n.....\n\nUpdated documentation and readme files.\n\n1.0.0\n.....\n\nInitial version of the provider.\n```\n\n----------------------------------------\n\nTITLE: Documenting Provider Changelog Using reStructuredText - reStructuredText\nDESCRIPTION: This snippet demonstrates the use of reStructuredText (reST) to clearly present change histories for Python packages. Dependencies include Sphinx or similar documentation toolchains that render reST. Key parameters are reST tables separating commit, date, and description columns; the file makes extensive use of directives, table syntax, and inline code-formatting. Inputs are the autogenerated commit data, outputs are tables suitable for HTML or PDF documentation; constraints include strict adherence to reST syntax (e.g., equal sign row separators) and the use of formatted hyperlinks to commits. Comments at the beginning warn about file-overwriting and point to the template source.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/snowflake/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n .. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!\n\n .. IF YOU WANT TO MODIFY THIS FILE, YOU SHOULD MODIFY THE TEMPLATE\n    `PROVIDER_COMMITS_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY\n\n .. THE REMAINDER OF THE FILE IS AUTOMATICALLY GENERATED. IT WILL BE OVERWRITTEN!\n\nPackage apache-airflow-providers-snowflake\n------------------------------------------------------\n\n`Snowflake <https://www.snowflake.com/>`__\n\n\nThis is detailed commit list of changes for versions provider package: ``snowflake``.\nFor high-level changelog, see :doc:`package information including changelog <index>`.\n\n\n\n6.2.2\n.....\n\nLatest change: 2025-04-14\n\n==================================================================================================  ===========  ==========================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==========================================\n`cb295c351a <https://github.com/apache/airflow/commit/cb295c351a016c0a10cab07f2a628b865cff3ca3>`__  2025-04-14   ``remove superfluous else block (#49199)``\n==================================================================================================  ===========  ==========================================\n\n6.2.1\n.....\n\nLatest change: 2025-04-10\n\n==================================================================================================  ===========  ==================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================\n`4a8567b20b <https://github.com/apache/airflow/commit/4a8567b20bdd6555cbdc936d6674bf4fa390b0d5>`__  2025-04-10   ``Prepare docs for Apr 2nd wave of providers (#49051)``\n`7b2ec33c7a <https://github.com/apache/airflow/commit/7b2ec33c7ad4998d9c9735b79593fcdcd3b9dd1f>`__  2025-04-08   ``Remove unnecessary entries in get_provider_info and update the schema (#48849)``\n`86d5b27f92 <https://github.com/apache/airflow/commit/86d5b27f92207571ebe0c29a42c42abbf6f8cb8c>`__  2025-04-08   ``Make '@task' import from airflow.sdk (#48896)``\n`139673d3ce <https://github.com/apache/airflow/commit/139673d3ce5552c2cf8bcb2d202e97342c4b237c>`__  2025-04-07   ``Remove fab from preinstalled providers (#48457)``\n`67858fd7e7 <https://github.com/apache/airflow/commit/67858fd7e7ac82788854844c1e6ef5a35f1d0d23>`__  2025-04-06   ``Improve documentation building iteration (#48760)``\n==================================================================================================  ===========  ==================================================================================\n\n6.2.0\n.....\n\nLatest change: 2025-04-06\n\n==================================================================================================  ===========  ==============================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==============================================================================\n`adbb062b50 <https://github.com/apache/airflow/commit/adbb062b50e2e128fe475a76b7ce10ec93c39ee2>`__  2025-04-06   ``Prepare docs for Apr 1st wave of providers (#48828)``\n`d4473555c0 <https://github.com/apache/airflow/commit/d4473555c0e7022e073489b7163d49102881a1a6>`__  2025-04-02   ``Simplify tooling by switching completely to uv (#48223)``\n`dce4ab474b <https://github.com/apache/airflow/commit/dce4ab474bf60882d75115433a42c6aa0a1858eb>`__  2025-04-01   ``feat: Send separate OpenLineage event for each Snowflake query_id (#47736)``\n`47002feacd <https://github.com/apache/airflow/commit/47002feacd8aaf794b47c2dd241aa25068354a2a>`__  2025-03-30   ``Upgrade ruff to latest version (#48553)``\n`c762e17820 <https://github.com/apache/airflow/commit/c762e17820cae6b162caa3eec5123760e07d56cc>`__  2025-03-26   ``Prepare docs for Mar 2nd wave of providers (#48383)``\n`6adb2dbae4 <https://github.com/apache/airflow/commit/6adb2dbae47341eb61dbc62dbc56176d9aa83fd9>`__  2025-03-25   ``Upgrade providers flit build requirements to 3.12.0 (#48362)``\n`243fe86d4b <https://github.com/apache/airflow/commit/243fe86d4b3e59bb12977b3e36ca3f2ed27ca0f8>`__  2025-03-21   ``Move airflow sources to airflow-core package (#47798)``\n`935d2831fe <https://github.com/apache/airflow/commit/935d2831fe8fd509b618a738bf00e0c34e186e11>`__  2025-03-15   ``Remove links to x/twitter.com (#47801)``\n==================================================================================================  ===========  ==============================================================================\n\n6.1.1\n.....\n\nLatest change: 2025-03-09\n\n==================================================================================================  ===========  ==================================================================================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ==================================================================================================================================================\n`492ecfe5c0 <https://github.com/apache/airflow/commit/492ecfe5c03102bfb710108038ebd5fc50cb55b5>`__  2025-03-09   ``Prepare docs for Mar 1st wave of providers (#47545)``\n`b275e79654 <https://github.com/apache/airflow/commit/b275e79654f54b6373c392d4750811d6373c40d9>`__  2025-03-06   ``SnowflakeSqlApiOperator(without deferable flag) marks Task as Success Before Snowflake Job Completes (#46648): implementation & tests (#46672)``\n`e4002c3305 <https://github.com/apache/airflow/commit/e4002c3305a757f5926f96c996e701e8f998a042>`__  2025-03-05   ``Move tests_common package to devel-common project (#47281)``\n`b599f6dddc <https://github.com/apache/airflow/commit/b599f6dddc8e198666a5df34311d0a4aac4b15ea>`__  2025-03-04   ``[OpenLineage] fixed inputs in OL implementation of CopyFromExternalStageToSnowflakeOperator (#47168)``\n`1addb55154 <https://github.com/apache/airflow/commit/1addb55154fbef31bfa021537cfbd4395696381c>`__  2025-02-28   ``Improve documentation for updating provider dependencies (#47203)``\n`28c93d93ca <https://github.com/apache/airflow/commit/28c93d93ca27a509182e7c6acfacc60ad45e739f>`__  2025-02-27   ``fIx deprecation warnings in common.sql (#47169)``\n`c6c4f95ed9 <https://github.com/apache/airflow/commit/c6c4f95ed9e3220133815b9126c135e805637022>`__  2025-02-25   ``Add legacy namespace packages to airflow.providers (#47064)``\n`dbf8bb4092 <https://github.com/apache/airflow/commit/dbf8bb409223687c7d2ad10649a92d02c24bb3b4>`__  2025-02-24   ``Remove extra whitespace in provider readme template (#46975)``\n`b28c336e8b <https://github.com/apache/airflow/commit/b28c336e8b7aa1d69c0f9520b182b1b661377337>`__  2025-02-21   ``Upgrade flit to 3.11.0 (#46938)``\n==================================================================================================  ===========  ==================================================================================================================================================\n\n6.1.0\n.....\n\nLatest change: 2025-02-21\n\n==================================================================================================  ===========  =========================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =========================================================================================\n`5d87bddf0a <https://github.com/apache/airflow/commit/5d87bddf0aa5f485f3684c909fb95f461e5a2ab6>`__  2025-02-21   ``Prepare docs for Feb 1st wave of providers (#46893)``\n`4d5846f58f <https://github.com/apache/airflow/commit/4d5846f58fe0de9b43358c0be75dd72e968dacc4>`__  2025-02-16   ``Move provider_tests to unit folder in provider tests (#46800)``\n`e027457a24 <https://github.com/apache/airflow/commit/e027457a24d0c6235bfed9c2a8399f75342e82f1>`__  2025-02-15   ``Removed the unused provider's distribution (#46608)``\n`035060d7f3 <https://github.com/apache/airflow/commit/035060d7f384a4989eddb6fb05f512f9c6a7e5bf>`__  2025-02-11   ``AIP-83 amendment: Add logic for generating run_id when logical date is None. (#46616)``\n\n```\n\n----------------------------------------\n\nTITLE: Referencing FlinkKubernetesOperator Class in reStructuredText\nDESCRIPTION: This snippet demonstrates how to reference the FlinkKubernetesOperator class in reStructuredText documentation. It provides a link to the class definition for parameter details.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/flink/docs/operators.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\nFor parameter definition take a look at :class:`~airflow.providers.apache.flink.operators.flink_kubernetes.FlinkKubernetesOperator`.\n```\n\n----------------------------------------\n\nTITLE: Verifying Release Signatures in Shell\nDESCRIPTION: This snippet shows how to verify the signatures of the release files using GPG. It loops through all .asc files and verifies their signatures.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\nfor i in *.asc\ndo\n   echo -e \"Checking $i\\n\"; gpg --verify $i\ndone\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Documentation in RST\nDESCRIPTION: Sphinx include directive to incorporate external documentation about installing Airflow providers from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/datadog/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Including Provider Configurations Documentation in RST\nDESCRIPTION: This reStructuredText directive includes the content of the specified external file '/../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst'. This is used to incorporate shared documentation about provider configurations into the current document.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst\n```\n\n----------------------------------------\n\nTITLE: Including Security Documentation in reStructuredText\nDESCRIPTION: This snippet includes an external security-related documentation file using the reStructuredText 'include' directive. The included file is located in a specific path relative to the current document.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/security.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/security.rst\n```\n\n----------------------------------------\n\nTITLE: Removing Backport Providers from Airflow\nDESCRIPTION: This commit message indicates the removal of backport providers from the Airflow system. It references pull request #14886.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_39\n\nLANGUAGE: text\nCODE:\n```\nRemove Backport Providers (#14886)\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry Version 2.0.0\nDESCRIPTION: Documentation for version 2.0.0 of the Microsoft Provider, highlighting breaking changes and compatibility requirements with Airflow 2.2+.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/psrp/docs/changelog.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n2.0.0\n.....\n\nBreaking changes\n~~~~~~~~~~~~~~~~\n\n.. note::\n  This release of provider is only available for Airflow 2.2+ as explained in the\n  `Apache Airflow providers support policy <https://github.com/apache/airflow/blob/main/PROVIDERS.rst#minimum-supported-version-of-airflow-for-community-managed-providers>`_.\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry Earlier Versions\nDESCRIPTION: Documentation for versions 1.1.2 through 1.0.0, covering various features, bug fixes, and the initial release.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/psrp/docs/changelog.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n1.1.2\n.....\n\nMisc\n~~~~~\n\n* ``Add Trove classifiers in PyPI (Framework :: Apache Airflow :: Provider)``\n\n1.1.1\n.....\n\nMisc\n~~~~\n\n* ``Support for Python 3.10``\n\n1.1.0\n.....\n\nFeatures\n~~~~~~~~\n\n* ``PSRP improvements (#19806)``\n\n1.0.1\n.....\n\nBug Fixes\n~~~~~~~~~\n\n* ``Fix unexpected bug in exiting hook context manager (#18014)``\n\n1.0.0\n.....\n\nInitial version of the provider.\n```\n\n----------------------------------------\n\nTITLE: Preparing Docs for Aug 2nd Wave of Providers (Commit Message)\nDESCRIPTION: Commit message indicating the preparation of documentation for the August 2nd wave of Apache Airflow provider releases, including the Airbyte provider.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/airbyte/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs for Aug 2nd wave of providers (#41559)\n```\n\n----------------------------------------\n\nTITLE: Documenting Provider Update July 2023 (RC2)\nDESCRIPTION: Prepares documentation updates for the second release candidate (RC2) of the July 2023 provider release wave, as tracked in issue #32381.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_28\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs for July 2023 wave of Providers (RC2) (#32381)\n```\n\n----------------------------------------\n\nTITLE: Generating Release Announcement Subject\nDESCRIPTION: Command to generate email subject for release announcement.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_HELM_CHART.md#2025-04-22_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\n[ANNOUNCE] Apache Airflow Helm Chart version ${VERSION} Released\nEOF\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Update Provider Security Docs to Use Includes\nDESCRIPTION: This commit message, associated with commit 052e26ad47 on 2023-11-04, describes changing the 'security.rst' file in providers to utilize includes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nChange security.rst to use includes in providers (#35435)\n```\n\n----------------------------------------\n\nTITLE: Markdown Heading for Version Release\nDESCRIPTION: Shows how to create a Markdown heading for a version release with date.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/commits.rst#2025-04-22_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n7.6.0\n.....\n```\n\n----------------------------------------\n\nTITLE: GitCommit References for Version 2.4.0\nDESCRIPTION: References to Git commits for version 2.4.0 updates, including classifier documentation, HTTP retry functionality, and DatabricksCopyIntoOperator enhancements.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/databricks/docs/commits.rst#2025-04-22_snippet_41\n\nLANGUAGE: text\nCODE:\n```\n`16adc035b1 <https://github.com/apache/airflow/commit/16adc035b1ecdf533f44fbb3e32bea972127bb71>`__\n`12e9e2c695 <https://github.com/apache/airflow/commit/12e9e2c695f9ebb9d3dde9c0f7dfaa112654f0d6>`__\n`af9d85ccd8 <https://github.com/apache/airflow/commit/af9d85ccd8abdc3c252c19764d3ea16970ae0f20>`__\n`4014194320 <https://github.com/apache/airflow/commit/401419432082d222b823e4f2a66f21e5cc3ab28d>`__\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents for Connection Types in reStructuredText\nDESCRIPTION: This snippet uses the toctree directive to create a table of contents for all connection type documentation files. It sets the maximum depth to 1 and uses a glob pattern to include all files in the current directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/hive/docs/connections/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: Displaying Package Information in RST\nDESCRIPTION: RST code block showing the package name and link to Kubernetes documentation.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nPackage apache-airflow-providers-cncf-kubernetes\n------------------------------------------------------\n\n`Kubernetes <https://kubernetes.io/>`__\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents Configuration\nDESCRIPTION: Sphinx documentation toctree directive that configures the display of connection type documentation pages with maxdepth of 1 and glob pattern matching.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/connections/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n    :glob:\n\n    *\n```\n\n----------------------------------------\n\nTITLE: RST Changelog Entry Version 1.1.3\nDESCRIPTION: Documentation for version 1.1.3 of the Microsoft Provider, addressing installation requirements fixes.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/psrp/docs/changelog.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n1.1.3\n.....\n\nBug Fixes\n~~~~~~~~~\n\n* ``Fix mistakenly added install_requires for all providers (#22382)``\n```\n\n----------------------------------------\n\nTITLE: Airflow Integration Documentation Structure\nDESCRIPTION: RestructuredText markup defining the integration documentation structure with links to various integration components and mechanisms supported by Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/airflow-core/docs/integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nIntegration\n===========\n\nAirflow has a mechanism that allows you to expand its functionality and integrate with other systems.\n\n* :doc:`API Authentication backends </security/api>`\n* :doc:`Email backends </howto/email-config>`\n* :doc:`Executor </core-concepts/executor/index>`\n* :doc:`Kerberos </security/kerberos>`\n* :doc:`Logging </administration-and-deployment/logging-monitoring/logging-tasks>`\n* :doc:`Metrics (statsd) </administration-and-deployment/logging-monitoring/metrics>`\n* :doc:`Operators and hooks </operators-and-hooks-ref>`\n* :doc:`Plugins </administration-and-deployment/plugins>`\n* :doc:`Listeners </administration-and-deployment/listeners>`\n* :doc:`Secrets backends </security/secrets/secrets-backend/index>`\n* :doc:`Web UI Authentication backends </security/api>`\n* :doc:`Serialization </authoring-and-scheduling/serializers>`\n```\n\n----------------------------------------\n\nTITLE: Commit Hash Links in ReStructuredText\nDESCRIPTION: Links to specific Git commits using ReStructuredText markup, showing commit hashes, dates and descriptions for version updates.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/celery/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n`706878ec35 <https://github.com/apache/airflow/commit/706878ec354cf867440c367a95c85753c19e54de>`__  2023-11-04   ``Remove empty lines in generated changelog (#35436)``\n```\n\n----------------------------------------\n\nTITLE: Displaying Airflow Provider Commits (Nov 2023 - v1.8.1) using RST Table\nDESCRIPTION: This reStructuredText (RST) snippet formats a list of commits related to Apache Airflow provider version 1.8.1 (spanning October-November 2023) into a table. It includes the commit hash (linked to the specific GitHub commit), the commit date, and the commit subject, utilizing RST's table structure, hyperlink syntax (`<link>`__), and inline code formatting (``code``).\nSOURCE: https://github.com/apache/airflow/blob/main/providers/common/sql/docs/commits.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n==================================================================================================  ===========  ====================================================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ====================================================================================================\n`0b23d5601c <https://github.com/apache/airflow/commit/0b23d5601c6f833392b0ea816e651dcb13a14685>`__  2023-11-24   ``Prepare docs 2nd wave of Providers November 2023 (#35836)``\n`2a469b3713 <https://github.com/apache/airflow/commit/2a469b3713d95ab15df8e9090abdb9d15e50cbb9>`__  2023-11-21   ``Remove backcompat inheritance for DbApiHook (#35754)``\n`99534e47f3 <https://github.com/apache/airflow/commit/99534e47f330ce0efb96402629dda5b2a4f16e8f>`__  2023-11-19   ``Use reproducible builds for provider packages (#35693)``\n`064fc2b775 <https://github.com/apache/airflow/commit/064fc2b7751a44e37ccce97609cff7c496098e56>`__  2023-11-17   ``Make pyodbc.Row and databricks.Row JSON-serializable via new 'make_serializable' method (#32319)``\n`99df205f42 <https://github.com/apache/airflow/commit/99df205f42a754aa67f80b5983e1d228ff23267f>`__  2023-11-16   ``Fix and reapply templates for provider documentation (#35686)``\n`1b059c57d6 <https://github.com/apache/airflow/commit/1b059c57d6d57d198463e5388138bee8a08591b1>`__  2023-11-08   ``Prepare docs 1st wave of Providers November 2023 (#35537)``\n`11bdfe4c12 <https://github.com/apache/airflow/commit/11bdfe4c12efa2f5d256cc49916a20beaa5487eb>`__  2023-11-07   ``Work around typing issue in examples and providers (#35494)``\n`706878ec35 <https://github.com/apache/airflow/commit/706878ec354cf867440c367a95c85753c19e54de>`__  2023-11-04   ``Remove empty lines in generated changelog (#35436)``\n`052e26ad47 <https://github.com/apache/airflow/commit/052e26ad473a9d50f0b96456ed094f2087ee4434>`__  2023-11-04   ``Change security.rst to use includes in providers (#35435)``\n`d1c58d86de <https://github.com/apache/airflow/commit/d1c58d86de1267d9268a1efe0a0c102633c051a1>`__  2023-10-28   ``Prepare docs 3rd wave of Providers October 2023 - FIX (#35233)``\n`3592ff4046 <https://github.com/apache/airflow/commit/3592ff40465032fa041600be740ee6bc25e7c242>`__  2023-10-28   ``Prepare docs 3rd wave of Providers October 2023 (#35187)``\n`dd7ba3cae1 <https://github.com/apache/airflow/commit/dd7ba3cae139cb10d71c5ebc25fc496c67ee784e>`__  2023-10-19   ``Pre-upgrade 'ruff==0.0.292' changes in providers (#35053)``\n`b75f9e8806 <https://github.com/apache/airflow/commit/b75f9e880614fa0427e7d24a1817955f5de658b3>`__  2023-10-18   ``Upgrade pre-commits (#35033)``\n`f23170c9dd <https://github.com/apache/airflow/commit/f23170c9dd23556a40bd07b5d24f06220eec15c4>`__  2023-10-16   ``D401 Support - A thru Common (Inclusive) (#34934)``\n==================================================================================================  ===========  ====================================================================================================\n```\n\n----------------------------------------\n\nTITLE: MySQL Provider Package Version Entry\nDESCRIPTION: Commit log entry showing changes made to the MySQL provider package, including version number, date, and specific commits with their subjects.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n==================================================================================================  ===========  ============================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  ============================================================\n`e6cee6c24f <https://github.com/apache/airflow/commit/e6cee6c24fb04bfe6394f44bdfdf81b4a56ac008>`__  2025-04-15   \"Remove limitation for mysql-connector-python (#49321)\"\n`d0d0e3c5db <https://github.com/apache/airflow/commit/d0d0e3c5db1f1e51d2a5ef80fcd3fc8cdeba341f>`__  2025-04-15   \"Exclude 9.3.0 release of mysql-connector-python (#49300)\"\n`cb295c351a <https://github.com/apache/airflow/commit/cb295c351a016c0a10cab07f2a628b865cff3ca3>`__  2025-04-14   \"remove superfluous else block (#49199)\"\n==================================================================================================\n```\n\n----------------------------------------\n\nTITLE: Fixing Main Branch Failures Related to DagRun Validation\nDESCRIPTION: Commit message indicating fixes implemented to address failures on the main branch caused by 'DagRun' validation issues. References pull request #45917.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/standard/docs/commits.rst#2025-04-22_snippet_40\n\nLANGUAGE: plaintext\nCODE:\n```\nFix failures on main related to DagRun validation (#45917)\n```\n\n----------------------------------------\n\nTITLE: Markdown Table for Release Notes\nDESCRIPTION: Demonstrates how to create a Markdown table listing commit details for a release.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n==================================================================================================  ===========  =========================================================================\nCommit                                                                                              Committed    Subject\n==================================================================================================  ===========  =========================================================================\n`7574e16e75 <https://github.com/apache/airflow/commit/7574e16e751e37cc012139da1a0e39874bab2918>`__  2023-09-14   ``Prepare docs for Sep 2023 2nd wave of Providers (#34360)``\n`de92a81f00 <https://github.com/apache/airflow/commit/de92a81f002e6c1b3e74ad9d074438b65acb87b6>`__  2023-09-13   ``Move definition of Pod*Exceptions to pod_generator (#34346)``\n`b435b8edef <https://github.com/apache/airflow/commit/b435b8edefd181fa85e6cc6b2b822d113f562e27>`__  2023-09-09   ``Push to xcom before 'KubernetesPodOperator' deferral (#34209)``\n`b5057e0e1f <https://github.com/apache/airflow/commit/b5057e0e1fc6b7a47e38037a97cac862706747f0>`__  2023-09-09   ``Add 'progress_callback' parameter to 'KubernetesPodOperator' (#34153)``\n`c5016f754d <https://github.com/apache/airflow/commit/c5016f754df1b62046b9c1fce09574a69d8edebc>`__  2023-09-08   ``Refactor: Consolidate import textwrap in providers (#34220)``\n```\n\n----------------------------------------\n\nTITLE: Documenting Provider Update August 2023 (3rd Wave)\nDESCRIPTION: This commit prepares the documentation for the third wave of Apache Airflow provider updates released in August 2023, referenced by issue #33730.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/alibaba/docs/commits.rst#2025-04-22_snippet_21\n\nLANGUAGE: text\nCODE:\n```\nPrepare docs for Aug 2023 3rd wave of Providers (#33730)\n```\n\n----------------------------------------\n\nTITLE: Generating Vote Email Subject for Airflow Client Release in Shell\nDESCRIPTION: This snippet creates the subject line for the vote email using a here-document in shell script. It includes the version and suffix of the release candidate.\nSOURCE: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PYTHON_CLIENT.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF\n[VOTE] Release Apache Airflow Python Client ${VERSION} from ${VERSION}${VERSION_SUFFIX}\nEOF\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Enable Reproducible Builds for Providers\nDESCRIPTION: This commit message, associated with commit 99534e47f3 on 2023-11-19, indicates the implementation of reproducible builds for provider packages.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nUse reproducible builds for provider packages (#35693)\n```\n\n----------------------------------------\n\nTITLE: Changelog Table Header\nDESCRIPTION: Header row format for the changelog tables showing commit, date and subject columns.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/commits.rst#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n==================================================================================================  ===========  =============================================================================\n```\n\n----------------------------------------\n\nTITLE: Referencing GitHub Issues in Python Comments\nDESCRIPTION: This code snippet demonstrates how GitHub issue numbers are referenced in commit messages within the changelog. The issue number is enclosed in parentheses and prefixed with a hash.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/amazon/docs/commits.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n``Doc: Fix typos in variable and comments (#19349)``\n```\n\n----------------------------------------\n\nTITLE: Specifying Apache Airflow Version\nDESCRIPTION: Version number declaration for Apache Airflow. Indicates release version 2.10.0.\nSOURCE: https://github.com/apache/airflow/blob/main/clients/python/version.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n2.10.0\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Fix misspelling\nDESCRIPTION: Associated with commit 046f02e5a7, this message indicates a minor fix correcting a misspelling within the codebase or documentation, linked to issue #18121.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/sftp/docs/commits.rst#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n``fix misspelling (#18121)``\n```\n\n----------------------------------------\n\nTITLE: Commit Message: Prepare Docs - Providers November 2023 (2nd Wave)\nDESCRIPTION: This commit message, associated with commit 0b23d5601c on 2023-11-24, describes the preparation of documentation for the second wave of provider releases in November 2023.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/commits.rst#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nPrepare docs 2nd wave of Providers November 2023 (#35836)\n```\n\n----------------------------------------\n\nTITLE: Version History Table Format\nDESCRIPTION: Commit history table showing version changes, commit hashes, dates and descriptions in a structured format.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/mysql/docs/commits.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n6e5ae26382  2024-06-22   \"Prepare docs 2nd wave June 2024 (#40273)\"\nc0f27094ab  2024-06-04   \"iMPlement per-provider tests with lowest-direct dependency resolution (#39946)\"\ne3c31752f1  2024-05-26   \"Resolving mysql deprecated operator warnings (#39725)\"\n```\n\n----------------------------------------\n\nTITLE: Preparing New Wave of Airflow Provider Releases\nDESCRIPTION: This commit message signifies preparation steps taken for releasing a new set of Airflow providers. It references pull request #14013.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_41\n\nLANGUAGE: text\nCODE:\n```\nPrepare to release a new wave of providers. (#14013)\n```\n\n----------------------------------------\n\nTITLE: Displaying Commit Information in Markdown\nDESCRIPTION: This code snippet shows how commit information is formatted in the changelog using Markdown syntax. It includes the commit hash as a link, the commit date, and a brief description of the change.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/asana/docs/commits.rst#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n`ead9b00f7c <https://github.com/apache/airflow/commit/ead9b00f7cd5acecf9d575c459bb62633088436a>`__  2024-04-25   ``Bump minimum Airflow version in providers to Airflow 2.7.0 (#39240)``\n```\n\n----------------------------------------\n\nTITLE: Including External reStructuredText File for Provider Installation\nDESCRIPTION: This directive includes an external reStructuredText file containing instructions for installing Apache Airflow provider packages from source code. The file is located in a common development directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/google/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Updating Airflow Provider READMEs for 1.0.0b2 Release\nDESCRIPTION: This commit message indicates updates made to the README files of Airflow providers in preparation for the 1.0.0b2 batch release. It references pull request #12449.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_56\n\nLANGUAGE: text\nCODE:\n```\nUpdate provider READMEs for 1.0.0b2 batch release (#12449)\n```\n\n----------------------------------------\n\nTITLE: Including Sphinx Extensions for Provider Configurations Reference in Apache Airflow\nDESCRIPTION: RST include directives that import Sphinx extension files for documenting provider configurations. The includes reference the providers-configurations-ref.rst and sections-and-options.rst files from the devel-common directory.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/microsoft/azure/docs/configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/sections-and-options.rst\n```\n\n----------------------------------------\n\nTITLE: Including Provider Installation Documentation in RST\nDESCRIPTION: Sphinx documentation directive to include external RST file containing instructions for installing providers from source code.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/cncf/kubernetes/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Neo4j Provider Package License Header\nDESCRIPTION: Apache License 2.0 header text for the Neo4j provider package documentation\nSOURCE: https://github.com/apache/airflow/blob/main/providers/neo4j/docs/commits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n```\n\n----------------------------------------\n\nTITLE: Documentation Path Mapping for Security Content\nDESCRIPTION: Specifies the remapping of documentation paths, moving content from howto/create-update-providers.rst to index.rst and from authoring-and-scheduling/datasets.rst to authoring-and-scheduling/assets.rst\nSOURCE: https://github.com/apache/airflow/blob/main/providers-summary-docs/redirects.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhowto/create-update-providers.rst index.rst\nauthoring-and-scheduling/datasets.rst authoring-and-scheduling/assets.rst\n```\n\n----------------------------------------\n\nTITLE: Generating Backport Providers READMEs/Setup for 2020.10.29\nDESCRIPTION: This commit message indicates the generation of README files and setup configurations for backport providers targeted for a specific date (2020.10.29). It references pull request #11826.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/oracle/docs/commits.rst#2025-04-22_snippet_67\n\nLANGUAGE: text\nCODE:\n```\nGenerated backport providers readmes/setup for 2020.10.29 (#11826)\n```\n\n----------------------------------------\n\nTITLE: Including RST Documentation for Installing Providers from Sources in Sphinx\nDESCRIPTION: A reStructuredText include directive that imports documentation about installing Apache Airflow providers from source code files, referencing content from a common development repository.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/apache/pig/docs/installing-providers-from-sources.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /../../../../devel-common/src/sphinx_exts/includes/installing-providers-from-sources.rst\n```\n\n----------------------------------------\n\nTITLE: Including Provider Configuration Reference in Sphinx Documentation\nDESCRIPTION: This snippet uses a Sphinx directive to include external documentation about provider configurations. It references a file that likely contains detailed information about configuring different providers in Apache Airflow.\nSOURCE: https://github.com/apache/airflow/blob/main/providers/opensearch/docs/configurations-ref.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. include:: /../../../devel-common/src/sphinx_exts/includes/providers-configurations-ref.rst\n```"
  }
]
[
  {
    "owner": "prometheus",
    "repo": "prometheus",
    "content": "TITLE: Finding Top 5 HTTP Request Counts with topk in PromQL\nDESCRIPTION: Example of using the 'topk' aggregation operator to get the 5 largest HTTP request counts across all instances.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_16\n\nLANGUAGE: promql\nCODE:\n```\ntopk(5, http_requests_total)\n```\n\n----------------------------------------\n\nTITLE: Many-to-one Vector Matching Syntax in PromQL\nDESCRIPTION: Syntax for many-to-one and one-to-many vector matching in PromQL using the 'group_left' and 'group_right' modifiers. These modifiers determine which vector has the higher cardinality.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_8\n\nLANGUAGE: promql\nCODE:\n```\n<vector expr> <bin-op> ignoring(<label list>) group_left(<label list>) <vector expr>\n<vector expr> <bin-op> ignoring(<label list>) group_right(<label list>) <vector expr>\n<vector expr> <bin-op> on(<label list>) group_left(<label list>) <vector expr>\n<vector expr> <bin-op> on(<label list>) group_right(<label list>) <vector expr>\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Range Query API\nDESCRIPTION: Example of querying the Prometheus range query API endpoint (/api/v1/query_range) with the 'up' expression over a 30-second range and 15-second step. Shows the request and JSON response format.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n$ curl 'http://localhost:9090/api/v1/query_range?query=up&start=2015-07-01T20:10:30.781Z&end=2015-07-01T20:11:00.781Z&step=15s'\n{\n   \"status\" : \"success\",\n   \"data\" : {\n      \"resultType\" : \"matrix\",\n      \"result\" : [\n         {\n            \"metric\" : {\n               \"__name__\" : \"up\",\n               \"job\" : \"prometheus\",\n               \"instance\" : \"localhost:9090\"\n            },\n            \"values\" : [\n               [ 1435781430.781, \"1\" ],\n               [ 1435781445.781, \"1\" ],\n               [ 1435781460.781, \"1\" ]\n            ]\n         },\n         {\n            \"metric\" : {\n               \"__name__\" : \"up\",\n               \"job\" : \"node\",\n               \"instance\" : \"localhost:9091\"\n            },\n            \"values\" : [\n               [ 1435781430.781, \"0\" ],\n               [ 1435781445.781, \"0\" ],\n               [ 1435781460.781, \"1\" ]\n            ]\n         }\n      ]\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Time Series with Label Matchers\nDESCRIPTION: Shows how to use label matchers in curly braces to filter time series by their label values, selecting only those series that match all specified criteria.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_7\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total{job=\"prometheus\",group=\"canary\"}\n```\n\n----------------------------------------\n\nTITLE: Using rate() Function in PromQL\nDESCRIPTION: Example showing how to calculate the per-second average rate of HTTP requests over a 5-minute window. This is commonly used for counter metrics and alerting.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_20\n\nLANGUAGE: promql\nCODE:\n```\nrate(http_requests_total{job=\"api-server\"}[5m])\n```\n\n----------------------------------------\n\nTITLE: Running Prometheus in Docker container\nDESCRIPTION: Command to launch a Prometheus container exposing it on localhost port 9090. This provides a quick way to try out Prometheus without installing it directly on the host system.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name prometheus -d -p 127.0.0.1:9090:9090 prom/prometheus\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Scrape Settings in YAML\nDESCRIPTION: Comprehensive YAML configuration for Prometheus scrape_config section. Includes job configuration, scraping intervals, protocols, authentication, service discovery configurations, and various limits and validation settings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n# The job name assigned to scraped metrics by default.\njob_name: <job_name>\n\n# How frequently to scrape targets from this job.\n[ scrape_interval: <duration> | default = <global_config.scrape_interval> ]\n\n# Per-scrape timeout when scraping this job.\n# It cannot be greater than the scrape interval.\n[ scrape_timeout: <duration> | default = <global_config.scrape_timeout> ]\n\n# The protocols to negotiate during a scrape with the client.\n# Supported values (case sensitive): PrometheusProto, OpenMetricsText0.0.1,\n# OpenMetricsText1.0.0, PrometheusText0.0.4, PrometheusText1.0.0.\n[ scrape_protocols: [<string>, ...] | default = <global_config.scrape_protocols> ]\n\n# Fallback protocol to use if a scrape returns blank, unparseable, or otherwise\n# invalid Content-Type.\n# Supported values (case sensitive): PrometheusProto, OpenMetricsText0.0.1,\n# OpenMetricsText1.0.0, PrometheusText0.0.4, PrometheusText1.0.0.\n[ fallback_scrape_protocol: <string> ]\n\n# Whether to scrape a classic histogram, even if it is also exposed as a native\n# histogram (has no effect without --enable-feature=native-histograms).\n[ always_scrape_classic_histograms: <boolean> | default = false ]\n\n# The HTTP resource path on which to fetch metrics from targets.\n[ metrics_path: <path> | default = /metrics ]\n\n# honor_labels controls how Prometheus handles conflicts between labels that are\n# already present in scraped data and labels that Prometheus would attach\n# server-side (\"job\" and \"instance\" labels, manually configured target\n# labels, and labels generated by service discovery implementations).\n#\n# If honor_labels is set to \"true\", label conflicts are resolved by keeping label\n# values from the scraped data and ignoring the conflicting server-side labels.\n#\n# If honor_labels is set to \"false\", label conflicts are resolved by renaming\n# conflicting labels in the scraped data to \"exported_<original-label>\" (for\n# example \"exported_instance\", \"exported_job\") and then attaching server-side\n# labels.\n#\n# Setting honor_labels to \"true\" is useful for use cases such as federation and\n# scraping the Pushgateway, where all labels specified in the target should be\n# preserved.\n#\n# Note that any globally configured \"external_labels\" are unaffected by this\n# setting. In communication with external systems, they are always applied only\n# when a time series does not have a given label yet and are ignored otherwise.\n[ honor_labels: <boolean> | default = false ]\n\n# honor_timestamps controls whether Prometheus respects the timestamps present\n# in scraped data.\n#\n# If honor_timestamps is set to \"true\", the timestamps of the metrics exposed\n# by the target will be used.\n#\n# If honor_timestamps is set to \"false\", the timestamps of the metrics exposed\n# by the target will be ignored.\n[ honor_timestamps: <boolean> | default = true ]\n\n# track_timestamps_staleness controls whether Prometheus tracks staleness of\n# the metrics that have an explicit timestamps present in scraped data.\n#\n# If track_timestamps_staleness is set to \"true\", a staleness marker will be\n# inserted in the TSDB when a metric is no longer present or the target\n# is down.\n[ track_timestamps_staleness: <boolean> | default = false ]\n\n# Configures the protocol scheme used for requests.\n[ scheme: <scheme> | default = http ]\n\n# Optional HTTP URL parameters.\nparams:\n  [ <string>: [<string>, ...] ]\n\n# If enable_compression is set to \"false\", Prometheus will request uncompressed\n# response from the scraped target.\n[ enable_compression: <boolean> | default = true ]\n\n# File to which scrape failures are logged.\n# Reloading the configuration will reopen the file.\n[ scrape_failure_log_file: <string> ]\n\n# HTTP client settings, including authentication methods (such as basic auth and\n# authorization), proxy configurations, TLS options, custom HTTP headers, etc.\n[ <http_config> ]\n\n# List of Azure service discovery configurations.\nazure_sd_configs:\n  [ - <azure_sd_config> ... ]\n\n# List of Consul service discovery configurations.\nconsul_sd_configs:\n  [ - <consul_sd_config> ... ]\n\n# List of DigitalOcean service discovery configurations.\ndigitalocean_sd_configs:\n  [ - <digitalocean_sd_config> ... ]\n\n# List of Docker service discovery configurations.\ndocker_sd_configs:\n  [ - <docker_sd_config> ... ]\n\n# List of Docker Swarm service discovery configurations.\ndockerswarm_sd_configs:\n  [ - <dockerswarm_sd_config> ... ]\n\n# List of DNS service discovery configurations.\ndns_sd_configs:\n  [ - <dns_sd_config> ... ]\n\n# List of EC2 service discovery configurations.\nec2_sd_configs:\n  [ - <ec2_sd_config> ... ]\n\n# List of Eureka service discovery configurations.\neureka_sd_configs:\n  [ - <eureka_sd_config> ... ]\n\n# List of file service discovery configurations.\nfile_sd_configs:\n  [ - <file_sd_config> ... ]\n\n# List of GCE service discovery configurations.\ngce_sd_configs:\n  [ - <gce_sd_config> ... ]\n\n# List of Hetzner service discovery configurations.\nhetzner_sd_configs:\n  [ - <hetzner_sd_config> ... ]\n\n# List of HTTP service discovery configurations.\nhttp_sd_configs:\n  [ - <http_sd_config> ... ]\n\n\n# List of IONOS service discovery configurations.\nionos_sd_configs:\n  [ - <ionos_sd_config> ... ]\n\n# List of Kubernetes service discovery configurations.\nkubernetes_sd_configs:\n  [ - <kubernetes_sd_config> ... ]\n\n# List of Kuma service discovery configurations.\nkuma_sd_configs:\n  [ - <kuma_sd_config> ... ]\n\n# List of Lightsail service discovery configurations.\nlightsail_sd_configs:\n  [ - <lightsail_sd_config> ... ]\n\n# List of Linode service discovery configurations.\nlinode_sd_configs:\n  [ - <linode_sd_config> ... ]\n\n# List of Marathon service discovery configurations.\nmarathon_sd_configs:\n  [ - <marathon_sd_config> ... ]\n\n# List of AirBnB's Nerve service discovery configurations.\nnerve_sd_configs:\n  [ - <nerve_sd_config> ... ]\n\n# List of Nomad service discovery configurations.\nnomad_sd_configs:\n  [ - <nomad_sd_config> ... ]\n\n# List of OpenStack service discovery configurations.\nopenstack_sd_configs:\n  [ - <openstack_sd_config> ... ]\n\n# List of OVHcloud service discovery configurations.\novhcloud_sd_configs:\n  [ - <ovhcloud_sd_config> ... ]\n\n# List of PuppetDB service discovery configurations.\npuppetdb_sd_configs:\n  [ - <puppetdb_sd_config> ... ]\n\n# List of Scaleway service discovery configurations.\nscaleway_sd_configs:\n  [ - <scaleway_sd_config> ... ]\n\n# List of Zookeeper Serverset service discovery configurations.\nserverset_sd_configs:\n  [ - <serverset_sd_config> ... ]\n\n# List of Triton service discovery configurations.\ntriton_sd_configs:\n  [ - <triton_sd_config> ... ]\n\n# List of Uyuni service discovery configurations.\nuyuni_sd_configs:\n  [ - <uyuni_sd_config> ... ]\n\n# List of labeled statically configured targets for this job.\nstatic_configs:\n  [ - <static_config> ... ]\n\n# List of target relabel configurations.\nrelabel_configs:\n  [ - <relabel_config> ... ]\n\n# List of metric relabel configurations.\nmetric_relabel_configs:\n  [ - <relabel_config> ... ]\n\n# An uncompressed response body larger than this many bytes will cause the\n# scrape to fail. 0 means no limit. Example: 100MB.\n# This is an experimental feature, this behaviour could\n# change or be removed in the future.\n[ body_size_limit: <size> | default = 0 ]\n\n# Per-scrape limit on the number of scraped samples that will be accepted.\n# If more than this number of samples are present after metric relabeling\n# the entire scrape will be treated as failed. 0 means no limit.\n[ sample_limit: <int> | default = 0 ]\n\n# Limit on the number of labels that will be accepted per sample. If more\n# than this number of labels are present on any sample post metric-relabeling,\n# the entire scrape will be treated as failed. 0 means no limit.\n[ label_limit: <int> | default = 0 ]\n\n# Limit on the length (in bytes) of each individual label name. If any label\n# name in a scrape is longer than this number post metric-relabeling, the\n# entire scrape will be treated as failed. Note that label names are UTF-8\n# encoded, and characters can take up to 4 bytes. 0 means no limit.\n[ label_name_length_limit: <int> | default = 0 ]\n\n# Limit on the length (in bytes) of each individual label value. If any label\n# value in a scrape is longer than this number post metric-relabeling, the\n# entire scrape will be treated as failed. Note that label values are UTF-8\n# encoded, and characters can take up to 4 bytes. 0 means no limit.\n[ label_value_length_limit: <int> | default = 0 ]\n\n# Limit per scrape config on number of unique targets that will be\n# accepted. If more than this number of targets are present after target\n# relabeling, Prometheus will mark the targets as failed without scraping them.\n# 0 means no limit. This is an experimental feature, this behaviour could\n# change in the future.\n[ target_limit: <int> | default = 0 ]\n\n# Limit per scrape config on the number of targets dropped by relabeling\n# that will be kept in memory. 0 means no limit.\n[ keep_dropped_targets: <int> | default = 0 ]\n\n# Specifies the validation scheme for metric and label names. Either blank or \n# \"utf8\" for full UTF-8 support, or \"legacy\" for letters, numbers, colons, and\n# underscores.\n[ metric_name_validation_scheme <string> | default \"utf8\" ]\n\n# Specifies the character escaping scheme that will be requested when scraping\n# for metric and label names that do not conform to the legacy Prometheus\n# character set. Available options are: \n#   * `allow-utf-8`: Full UTF-8 support, no escaping needed.\n```\n\n----------------------------------------\n\nTITLE: Calculating Per-Second Rate in PromQL\nDESCRIPTION: This query calculates the per-second rate for all time series with the 'http_requests_total' metric name, measured over the last 5 minutes. It demonstrates the use of the 'rate' function.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_7\n\nLANGUAGE: promql\nCODE:\n```\nrate(http_requests_total[5m])\n```\n\n----------------------------------------\n\nTITLE: Calculating Unused Memory per Instance in PromQL\nDESCRIPTION: This query calculates the unused memory in MiB for every instance by subtracting used memory from the limit and converting to MiB. It demonstrates arithmetic operations on metrics.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_9\n\nLANGUAGE: promql\nCODE:\n```\n(instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 / 1024\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Instant Query API\nDESCRIPTION: Example of querying the Prometheus instant query API endpoint (/api/v1/query) with the 'up' expression and a specific timestamp. Shows the request and JSON response format.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n$ curl 'http://localhost:9090/api/v1/query?query=up&time=2015-07-01T20:10:51.781Z'\n{\n   \"status\" : \"success\",\n   \"data\" : {\n      \"resultType\" : \"vector\",\n      \"result\" : [\n         {\n            \"metric\" : {\n               \"__name__\" : \"up\",\n               \"job\" : \"prometheus\",\n               \"instance\" : \"localhost:9090\"\n            },\n            \"value\": [ 1435781451.781, \"1\" ]\n         },\n         {\n            \"metric\" : {\n               \"__name__\" : \"up\",\n               \"job\" : \"node\",\n               \"instance\" : \"localhost:9100\"\n            },\n            \"value\" : [ 1435781451.781, \"0\" ]\n         }\n      ]\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Arithmetic Operators in PromQL\nDESCRIPTION: Core arithmetic operators supported in Prometheus Query Language for performing mathematical operations on scalars and vectors.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_0\n\nLANGUAGE: promql\nCODE:\n```\n+ (addition)\n- (subtraction)\n* (multiplication)\n/ (division)\n% (modulo)\n^ (power/exponentiation)\n```\n\n----------------------------------------\n\nTITLE: Using String Literals in PromQL\nDESCRIPTION: Demonstrates different ways to create string literals in PromQL using single quotes, double quotes, or backticks. Each style has different escaping rules, with backticks preventing escape character parsing.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_0\n\nLANGUAGE: promql\nCODE:\n```\n\"this is a string\"\n'these are unescaped: \\n \\\\ \\t'\n`these are not unescaped: \\n ' \" \\t`\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Alerting Rules in YAML for Prometheus\nDESCRIPTION: Example of a basic alerting rule configuration in YAML format. This snippet demonstrates defining an alert called 'HighRequestLatency' that triggers when the 5-minute mean request latency exceeds 0.5 seconds for 10 minutes, with custom labels and annotations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/alerting_rules.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ngroups:\n- name: example\n  labels:\n    team: myteam\n  rules:\n  - alert: HighRequestLatency\n    expr: job:request_latency_seconds:mean5m{job=\"myjob\"} > 0.5\n    for: 10m\n    keep_firing_for: 5m\n    labels:\n      severity: page\n    annotations:\n      summary: High request latency\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus to Monitor Sample Targets\nDESCRIPTION: YAML configuration to add Node Exporter targets to Prometheus, grouped into production and canary instances.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/getting_started.md#2025-04-16_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nscrape_configs:\n  - job_name:       'node'\n\n    # Override the global default and scrape targets from this job every 5 seconds.\n    scrape_interval: 5s\n\n    static_configs:\n      - targets: ['localhost:8080', 'localhost:8081']\n        labels:\n          group: 'production'\n\n      - targets: ['localhost:8082']\n        labels:\n          group: 'canary'\n```\n\n----------------------------------------\n\nTITLE: One-to-one Vector Matching Example in PromQL\nDESCRIPTION: Example of one-to-one vector matching in PromQL, showing how to calculate the fraction of HTTP requests with status code 500 for each method using the 'ignoring' keyword.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_7\n\nLANGUAGE: promql\nCODE:\n```\nmethod_code:http_errors:rate5m{code=\"500\"} / ignoring(code) method:http_requests:rate5m\n```\n\n----------------------------------------\n\nTITLE: Comparison Operators in PromQL\nDESCRIPTION: Binary comparison operators for comparing values between scalars and vectors in Prometheus Query Language.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_2\n\nLANGUAGE: promql\nCODE:\n```\n== (equal)\n!= (not-equal)\n> (greater-than)\n< (less-than)\n>= (greater-or-equal)\n<= (less-or-equal)\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Write and Read in YAML for Prometheus\nDESCRIPTION: This YAML snippet shows how to configure remote write and read features in Prometheus. These settings allow Prometheus to send and receive data from remote storage systems.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# Settings related to the remote write feature.\nremote_write:\n  [ - <remote_write> ... ]\n\n# Settings related to the remote read feature.\nremote_read:\n  [ - <remote_read> ... ]\n```\n\n----------------------------------------\n\nTITLE: Bind-mounting Prometheus Configuration File\nDESCRIPTION: Command to run Prometheus on Docker with a custom configuration file mounted from the host system. This allows using a specific prometheus.yml file with your own configuration.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/installation.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n    -p 9090:9090 \\\n    -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml \\\n    prom/prometheus\n```\n\n----------------------------------------\n\nTITLE: Aggregation Operator Syntax in PromQL\nDESCRIPTION: Syntax for using aggregation operators in PromQL. These operators can aggregate over all label dimensions or preserve distinct dimensions using 'without' or 'by' clauses.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_10\n\nLANGUAGE: promql\nCODE:\n```\n<aggr-op> [without|by (<label list>)] ([parameter,] <vector expression>)\n```\n\n----------------------------------------\n\nTITLE: Running Prometheus Using Docker Basic Command\nDESCRIPTION: Basic command to run Prometheus on Docker, exposing it on port 9090. This uses the default configuration included in the Docker image.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/installation.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 9090:9090 prom/prometheus\n```\n\n----------------------------------------\n\nTITLE: Creating Prometheus Recording Rules\nDESCRIPTION: YAML configuration for a recording rule that aggregates CPU usage metrics.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/getting_started.md#2025-04-16_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ngroups:\n- name: cpu-node\n  rules:\n  - record: job_instance_mode:node_cpu_seconds:avg_rate5m\n    expr: avg by (job, instance, mode) (rate(node_cpu_seconds_total[5m]))\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Receiver in YAML for Prometheus\nDESCRIPTION: This YAML snippet defines the configuration for the OTLP (OpenTelemetry Protocol) receiver in Prometheus. It includes settings for attribute promotion, translation strategy, and histogram conversion.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# Settings related to the OTLP receiver feature.\n# See https://prometheus.io/docs/guides/opentelemetry/ for best practices.\notlp:\n  [ promote_resource_attributes: [<string>, ...] | default = [ ] ]\n  # Configures translation of OTLP metrics when received through the OTLP metrics\n  # endpoint. Available values:\n  # - \"UnderscoreEscapingWithSuffixes\" refers to commonly agreed normalization used\n  #   by OpenTelemetry in https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/translator/prometheus\n  # - \"NoUTF8EscapingWithSuffixes\" is a mode that relies on UTF-8 support in Prometheus.\n  #   It preserves all special characters like dots, but still adds required metric name suffixes\n  #   for units and _total, as UnderscoreEscapingWithSuffixes does.\n  [ translation_strategy: <string> | default = \"UnderscoreEscapingWithSuffixes\" ]\n  # Enables adding \"service.name\", \"service.namespace\" and \"service.instance.id\"\n  # resource attributes to the \"target_info\" metric, on top of converting\n  # them into the \"instance\" and \"job\" labels.\n  [ keep_identifying_resource_attributes: <boolean> | default = false]\n  # Configures optional translation of OTLP explicit bucket histograms into native histograms with custom buckets.\n  [ convert_histograms_to_nhcb: <boolean> | default = false]\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Time Units in PromQL\nDESCRIPTION: Shows how to create complex time durations by concatenating multiple time units, which must be arranged from longest to shortest duration. Each unit can only appear once.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_5\n\nLANGUAGE: promql\nCODE:\n```\n1h30m # Equivalent to 5400s and thus 5400.\n12h34m56s # Equivalent to 45296s and thus 45296.\n54s321ms # Equivalent to 54.321.\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Service Discovery in Prometheus\nDESCRIPTION: YAML configuration for Docker SD that allows retrieving scrape targets from Docker Engine hosts. This SD discovers containers and creates targets for each network IP and exposed port with comprehensive meta labels.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\n# Address of the Docker daemon.\nhost: <string>\n```\n\n----------------------------------------\n\nTITLE: Configuring Alerting in YAML for Prometheus\nDESCRIPTION: This YAML snippet defines the alerting configuration for Prometheus. It includes settings for alert relabeling and Alertmanager configurations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# Alerting specifies settings related to the Alertmanager.\nalerting:\n  alert_relabel_configs:\n    [ - <relabel_config> ... ]\n  alertmanagers:\n    [ - <alertmanager_config> ... ]\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Metrics\nDESCRIPTION: Example PromQL queries to explore Prometheus metrics, including filtering and aggregation.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/getting_started.md#2025-04-16_snippet_3\n\nLANGUAGE: promql\nCODE:\n```\nprometheus_target_interval_length_seconds\n```\n\nLANGUAGE: promql\nCODE:\n```\nprometheus_target_interval_length_seconds{quantile=\"0.99\"}\n```\n\nLANGUAGE: promql\nCODE:\n```\ncount(prometheus_target_interval_length_seconds)\n```\n\nLANGUAGE: promql\nCODE:\n```\nrate(prometheus_tsdb_head_chunks_created_total[1m])\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Write in Prometheus YAML\nDESCRIPTION: Defines the configuration for remote write endpoints in Prometheus. Includes options for URL, timeout, headers, relabeling, and queue management. Also covers AWS Signature Verification 4, AzureAD, and Google Cloud Monitoring configurations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_48\n\nLANGUAGE: yaml\nCODE:\n```\nurl: <string>\n\n[ protobuf_message: <prometheus.WriteRequest | io.prometheus.write.v2.Request> | default = prometheus.WriteRequest ]\n\n[ remote_timeout: <duration> | default = 30s ]\n\nheaders:\n  [ <string>: <string> ... ]\n\nwrite_relabel_configs:\n  [ - <relabel_config> ... ]\n\n[ name: <string> ]\n\n[ send_exemplars: <boolean> | default = false ]\n\n[ send_native_histograms: <boolean> | default = false ]\n\n[ round_robin_dns: <boolean> | default = false ]\n\nsigv4:\n  [ region: <string> ]\n  [ access_key: <string> ]\n  [ secret_key: <secret> ]\n  [ profile: <string> ]\n  [ role_arn: <string> ]\n\nazuread:\n  [ cloud: <string> | default = AzurePublic ]\n  [ managed_identity:\n      [ client_id: <string> ] ]  \n  [ oauth:\n      [ client_id: <string> ]\n      [ client_secret: <string> ]\n      [ tenant_id: <string> ] ]\n  [ sdk:\n      [ tenant_id: <string> ] ]\n\ngoogle_iam:\n  credentials_file: <file_name>\n\nqueue_config:\n  [ capacity: <int> | default = 10000 ]\n  [ max_shards: <int> | default = 50 ]\n  [ min_shards: <int> | default = 1 ]\n  [ max_samples_per_send: <int> | default = 2000]\n  [ batch_send_deadline: <duration> | default = 5s ]\n  [ min_backoff: <duration> | default = 30ms ]\n  [ max_backoff: <duration> | default = 5s ]\n  [ retry_on_http_429: <boolean> | default = false ]\n  [ sample_age_limit: <duration> | default = 0s ]\n\nmetadata_config:\n  [ send: <boolean> | default = true ]\n  [ send_interval: <duration> | default = 1m ]\n  [ max_samples_per_send: <int> | default = 500]\n\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Static Configuration Format in YAML\nDESCRIPTION: YAML format for defining static target configurations in file-based service discovery.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\n- targets:\n  [ - '<host>' ]\n  labels:\n    [ <labelname>: <labelvalue> ... ]\n```\n\n----------------------------------------\n\nTITLE: Computing 90th Percentile with histogram_quantile for Native Histogram\nDESCRIPTION: Calculates the 90th percentile of request durations over the last 10 minutes using a native histogram\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_7\n\nLANGUAGE: promql\nCODE:\n```\nhistogram_quantile(0.9, rate(http_request_duration_seconds[10m]))\n```\n\n----------------------------------------\n\nTITLE: Complete Prometheus Configuration with Rules\nDESCRIPTION: Full Prometheus configuration including global settings, rule files, and scrape configs for Prometheus itself and Node Exporter targets.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/getting_started.md#2025-04-16_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  scrape_interval:     15s # By default, scrape targets every 15 seconds.\n  evaluation_interval: 15s # Evaluate rules every 15 seconds.\n\n  # Attach these extra labels to all timeseries collected by this Prometheus instance.\n  external_labels:\n    monitor: 'codelab-monitor'\n\nrule_files:\n  - 'prometheus.rules.yml'\n\nscrape_configs:\n  - job_name: 'prometheus'\n\n    # Override the global default and scrape targets from this job every 5 seconds.\n    scrape_interval: 5s\n\n    static_configs:\n      - targets: ['localhost:9090']\n\n  - job_name:       'node'\n\n    # Override the global default and scrape targets from this job every 5 seconds.\n    scrape_interval: 5s\n\n    static_configs:\n      - targets: ['localhost:8080', 'localhost:8081']\n        labels:\n          group: 'production'\n\n      - targets: ['localhost:8082']\n        labels:\n          group: 'canary'\n```\n\n----------------------------------------\n\nTITLE: Configuring relabel_config in Prometheus\nDESCRIPTION: YAML configuration block for relabel_config which defines how labels are processed before scraping targets. It includes options for matching, replacing, and transforming labels using various actions like replace, keep, drop, hashmod, etc.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_46\n\nLANGUAGE: yaml\nCODE:\n```\n# The source_labels tells the rule what labels to fetch from the series. Any \n# labels which do not exist get a blank value (\"\").  Their content is concatenated\n# using the configured separator and matched against the configured regular expression\n# for the replace, keep, and drop actions.\n[ source_labels: '[' <labelname> [, ...] ']' ]\n\n# Separator placed between concatenated source label values.\n[ separator: <string> | default = ; ]\n\n# Label to which the resulting value is written in a replace action.\n# It is mandatory for replace actions. Regex capture groups are available.\n[ target_label: <labelname> ]\n\n# Regular expression against which the extracted value is matched.\n[ regex: <regex> | default = (.*) ]\n\n# Modulus to take of the hash of the source label values.\n[ modulus: <int> ]\n\n# Replacement value against which a regex replace is performed if the\n# regular expression matches. Regex capture groups are available.\n[ replacement: <string> | default = $1 ]\n\n# Action to perform based on regex matching.\n[ action: <relabel_action> | default = replace ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Consul Service Discovery in Prometheus\nDESCRIPTION: YAML configuration for Consul SD that allows retrieving scrape targets from Consul's Catalog API. Includes server configuration, filtering options, and available meta labels for target relabeling.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\n# The information to access the Consul API. It is to be defined\n# as the Consul documentation requires.\n[ server: <host> | default = \"localhost:8500\" ]\n# Prefix for URIs for when consul is behind an API gateway (reverse proxy).\n[ path_prefix: <string> ]\n[ token: <secret> ]\n[ datacenter: <string> ]\n# Namespaces are only supported in Consul Enterprise.\n[ namespace: <string> ]\n# Admin Partitions are only supported in Consul Enterprise.\n[ partition: <string> ]\n[ scheme: <string> | default = \"http\" ]\n# The username and password fields are deprecated in favor of the basic_auth configuration.\n[ username: <string> ]\n[ password: <secret> ]\n\n# A list of services for which targets are retrieved. If omitted, all services\n# are scraped.\nservices:\n  [ - <string> ]\n\n# A Consul Filter expression used to filter the catalog results\n# See https://www.consul.io/api-docs/catalog#list-services to know more\n# about the filter expressions that can be used.\n[ filter: <string> ]\n\n# The `tags` and `node_meta` fields are deprecated in Consul in favor of `filter`.\n# An optional list of tags used to filter nodes for a given service. Services must contain all tags in the list.\ntags:\n  [ - <string> ]\n\n# Node metadata key/value pairs to filter nodes for a given service. As of Consul 1.14, consider `filter` instead.\n[ node_meta:\n  [ <string>: <string> ... ] ]\n\n# The string by which Consul tags are joined into the tag label.\n[ tag_separator: <string> | default = , ]\n\n# Allow stale Consul results (see https://www.consul.io/api/features/consistency.html). Will reduce load on Consul.\n[ allow_stale: <boolean> | default = true ]\n\n# The time after which the provided names are refreshed.\n# On large setup it might be a good idea to increase this value because the catalog will change all the time.\n[ refresh_interval: <duration> | default = 30s ]\n\n# HTTP client settings, including authentication methods (such as basic auth and\n# authorization), proxy configurations, TLS options, custom HTTP headers, etc.\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Selecting Time Series with Specific Labels in PromQL\nDESCRIPTION: This query returns all time series with the metric 'http_requests_total' and specific 'job' and 'handler' labels. It shows how to filter metrics based on label values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_1\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total{job=\"apiserver\", handler=\"/api/comments\"}\n```\n\n----------------------------------------\n\nTITLE: Selecting Time Series with Metric Name in PromQL\nDESCRIPTION: This query returns all time series with the metric name 'http_requests_total'. It demonstrates the basic syntax for selecting metrics in Prometheus.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_0\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total\n```\n\n----------------------------------------\n\nTITLE: Querying Current Configuration with GET API Endpoint\nDESCRIPTION: API endpoint for retrieving the currently loaded Prometheus configuration file as a YAML string.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nGET /api/v1/status/config\n```\n\n----------------------------------------\n\nTITLE: HTTP Service Discovery Base Format in JSON\nDESCRIPTION: The basic JSON schema for HTTP Service Discovery responses showing the required structure with targets array and labels object.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/http_sd.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"targets\": [ \"<host>\", ... ],\n    \"labels\": {\n      \"<labelname>\": \"<labelvalue>\", ...\n    }\n  },\n  ...\n]\n```\n\n----------------------------------------\n\nTITLE: Configuration Reload Endpoint in Prometheus HTTP API\nDESCRIPTION: Endpoint that triggers a reload of Prometheus configuration and rule files. This endpoint is disabled by default and requires the --web.enable-lifecycle flag to be enabled. Can be accessed via PUT or POST HTTP methods.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/management_api.md#2025-04-16_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nPUT  /-/reload\nPOST /-/reload\n```\n\n----------------------------------------\n\nTITLE: Querying Target Metadata with GET API Endpoint\nDESCRIPTION: API endpoint for retrieving metadata about metrics currently scraped from targets. Supports parameters for matching targets by label selectors, specifying metrics, and limiting results.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nGET /api/v1/targets/metadata\n```\n\n----------------------------------------\n\nTITLE: Alert Test Case Structure in YAML for Prometheus Rule Testing\nDESCRIPTION: Defines the structure for alert test cases in Prometheus rule testing, including evaluation time, alert name, and expected alerts.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/unit_testing_rules.md#2025-04-16_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# The time elapsed from time=0s when the alerts have to be checked.\neval_time: <duration>\n\n# Name of the alert to be tested.\nalertname: <string>\n\n# List of expected alerts which are firing under the given alertname at\n# given evaluation time. If you want to test if an alerting rule should\n# not be firing, then you can mention the above fields and leave 'exp_alerts' empty.\nexp_alerts:\n  [ - <alert> ]\n```\n\n----------------------------------------\n\nTITLE: Docker Swarm Service Discovery Configuration\nDESCRIPTION: YAML configuration for Docker Swarm service discovery, specifying host connection, target role, port settings, filters, and refresh intervals.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nhost: <string>\n\nrole: <string>\n\n[ port: <int> | default = 80 ]\n\n[ filters:\n  [ - name: <string>\n      values: <string>, [...] ]\n\n[ refresh_interval: <duration> | default = 60s ]\n\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Prometheus Release Changelog Entry 2.52.0\nDESCRIPTION: Release notes for Prometheus version 2.52.0 detailing changes to TSDB, scraping functionality, Kubernetes service discovery enhancements, and various bugfixes.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## 2.52.0 / 2024-05-07\n\n* [CHANGE] TSDB: Fix the predicate checking for blocks which are beyond the retention period to include the ones right at the retention boundary. #9633\n* [CHANGE] Scrape: Multiple samples (even with different timestamps) are treated as duplicates during one scrape.\n* [FEATURE] Kubernetes SD: Add a new metric `prometheus_sd_kubernetes_failures_total` to track failed requests to Kubernetes API. #13554\n* [FEATURE] Kubernetes SD: Add node and zone metadata labels when using the endpointslice role. #13935\n```\n\n----------------------------------------\n\nTITLE: Checking Rule Syntax with Promtool\nDESCRIPTION: Command to validate Prometheus rule file syntax using the promtool utility without starting a Prometheus server.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/recording_rules.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npromtool check rules /path/to/example.rules.yml\n```\n\n----------------------------------------\n\nTITLE: Example Recording Rule Definition\nDESCRIPTION: Simple example of a recording rule that sums HTTP requests by response code.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/recording_rules.md#2025-04-16_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ngroups:\n  - name: example\n    rules:\n    - record: code:prometheus_http_requests_total:sum\n      expr: sum by (code) (prometheus_http_requests_total)\n```\n\n----------------------------------------\n\nTITLE: Aggregating Rates by Job Label in PromQL\nDESCRIPTION: This query sums the rates of 'http_requests_total' over all instances, preserving the 'job' dimension. It shows how to use aggregation operators with label preservation.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_8\n\nLANGUAGE: promql\nCODE:\n```\nsum by (job) (\n  rate(http_requests_total[5m])\n)\n```\n\n----------------------------------------\n\nTITLE: Logical/Set Operators in PromQL\nDESCRIPTION: Binary operators for performing logical and set operations between instant vectors in Prometheus Query Language.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_3\n\nLANGUAGE: promql\nCODE:\n```\nand (intersection)\nor (union)\nunless (complement)\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Targets API\nDESCRIPTION: Example of using the /api/v1/targets endpoint to retrieve both active and dropped targets. Shows the full response structure including discovered labels, scrape configurations, and health status.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"activeTargets\": [\n      {\n        \"discoveredLabels\": {\n          \"__address__\": \"127.0.0.1:9090\",\n          \"__metrics_path__\": \"/metrics\",\n          \"__scheme__\": \"http\",\n          \"job\": \"prometheus\"\n        },\n        \"labels\": {\n          \"instance\": \"127.0.0.1:9090\",\n          \"job\": \"prometheus\"\n        },\n        \"scrapePool\": \"prometheus\",\n        \"scrapeUrl\": \"http://127.0.0.1:9090/metrics\",\n        \"globalUrl\": \"http://example-prometheus:9090/metrics\",\n        \"lastError\": \"\",\n        \"lastScrape\": \"2017-01-17T15:07:44.723715405+01:00\",\n        \"lastScrapeDuration\": 0.050688943,\n        \"health\": \"up\",\n        \"scrapeInterval\": \"1m\",\n        \"scrapeTimeout\": \"10s\"\n      }\n    ],\n    \"droppedTargets\": [\n      {\n        \"discoveredLabels\": {\n          \"__address__\": \"127.0.0.1:9100\",\n          \"__metrics_path__\": \"/metrics\",\n          \"__scheme__\": \"http\",\n          \"__scrape_interval__\": \"1m\",\n          \"__scrape_timeout__\": \"10s\",\n          \"job\": \"node\"\n        },\n        \"scrapePool\": \"node\"\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using label_join() Function\nDESCRIPTION: Demonstrates joining multiple source label values into a new destination label using a specified separator.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_17\n\nLANGUAGE: promql\nCODE:\n```\nlabel_join(up{job=\"api-server\",src1=\"a\",src2=\"b\",src3=\"c\"}, \"foo\", \",\", \"src1\", \"src2\", \"src3\")\n```\n\n----------------------------------------\n\nTITLE: Configuring TSDB in Prometheus YAML\nDESCRIPTION: Defines the runtime-reloadable configuration settings for the Time Series Database (TSDB) in Prometheus. Includes options for handling out-of-order samples.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_50\n\nLANGUAGE: yaml\nCODE:\n```\n[ out_of_order_time_window: <duration> | default = 0s ]\n```\n\n----------------------------------------\n\nTITLE: Building Docker image for Prometheus\nDESCRIPTION: Commands to build a Docker image for Prometheus locally. This process involves building the binary for Linux/AMD64 architecture and preparing the necessary license files before creating the Docker image.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake promu\npromu crossbuild -p linux/amd64\nmake npm_licenses\nmake common-docker-amd64\n```\n\n----------------------------------------\n\nTITLE: Using increase() Function in PromQL\nDESCRIPTION: Example shows how to calculate the increase in HTTP requests over a 5-minute period using the increase() function. This function is used with counters to calculate the total increase over a time range.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_12\n\nLANGUAGE: promql\nCODE:\n```\nincrease(http_requests_total{job=\"api-server\"}[5m])\n```\n\n----------------------------------------\n\nTITLE: Many-to-one Vector Matching Example in PromQL\nDESCRIPTION: Example of many-to-one vector matching in PromQL, showing how to match elements from the right side with multiple elements with the same 'method' label on the left using 'group_left'.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_9\n\nLANGUAGE: promql\nCODE:\n```\nmethod_code:http_errors:rate5m / ignoring(code) group_left method:http_requests:rate5m\n```\n\n----------------------------------------\n\nTITLE: Using delta() function in PromQL\nDESCRIPTION: The delta() function calculates the difference between the first and last value of each time series in a range vector. It's useful for gauging changes over time.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_2\n\nLANGUAGE: promql\nCODE:\n```\ndelta(cpu_temp_celsius{host=\"zeus\"}[2h])\n```\n\n----------------------------------------\n\nTITLE: File Service Discovery Configuration in YAML\nDESCRIPTION: YAML configuration for file-based service discovery, specifying file patterns and refresh intervals.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_25\n\nLANGUAGE: yaml\nCODE:\n```\nfiles:\n  [ - <filename_pattern> ... ]\n[ refresh_interval: <duration> | default = 5m ]\n```\n\n----------------------------------------\n\nTITLE: Group Modifiers in PromQL\nDESCRIPTION: Modifiers that enable many-to-one and one-to-many vector matching operations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_5\n\nLANGUAGE: promql\nCODE:\n```\ngroup_left\ngroup_right\n```\n\n----------------------------------------\n\nTITLE: Using irate() Function\nDESCRIPTION: Calculates per-second instant rate of increase for HTTP requests using the last two data points within a 5-minute range.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_16\n\nLANGUAGE: promql\nCODE:\n```\nirate(http_requests_total{job=\"api-server\"}[5m])\n```\n\n----------------------------------------\n\nTITLE: Templating Alert Labels and Annotations in Prometheus\nDESCRIPTION: Examples of templated alerts using Prometheus console templates. This snippet shows two alerts: 'InstanceDown' for detecting instances that are unreachable for more than 5 minutes, and 'APIHighRequestLatency' for instances with high median request latency, both using variable templating in annotations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/alerting_rules.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ngroups:\n- name: example\n  rules:\n\n  # Alert for any instance that is unreachable for >5 minutes.\n  - alert: InstanceDown\n    expr: up == 0\n    for: 5m\n    labels:\n      severity: page\n    annotations:\n      summary: \"Instance {{ $labels.instance }} down\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.\"\n\n  # Alert for any instance that has a median request latency >1s.\n  - alert: APIHighRequestLatency\n    expr: api_http_request_latencies_second{quantile=\"0.5\"} > 1\n    for: 10m\n    annotations:\n      summary: \"High request latency on {{ $labels.instance }}\"\n      description: \"{{ $labels.instance }} has a median request latency above 1s (current value: {{ $value }}s)\"\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTPS and Authentication for Prometheus in YAML\nDESCRIPTION: This YAML configuration defines TLS server settings, HTTP server options, and basic authentication for Prometheus. It includes detailed options for certificates, client authentication, TLS versions, cipher suites, HTTP/2 support, security headers, and user authentication.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/https.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntls_server_config:\n  cert_file: <filename>\n  key_file: <filename>\n  [ client_auth_type: <string> | default = \"NoClientCert\" ]\n  [ client_ca_file: <filename> ]\n  [ client_allowed_sans:\n    [ - <string> ] ]\n  [ min_version: <string> | default = \"TLS12\" ]\n  [ max_version: <string> | default = \"TLS13\" ]\n  [ cipher_suites:\n    [ - <string> ] ]\n  [ prefer_server_cipher_suites: <boolean> | default = true ]\n  [ curve_preferences:\n    [ - <string> ] ]\n\nhttp_server_config:\n  [ http2: <boolean> | default = true ]\n  [ headers:\n    [ Content-Security-Policy: <string> ]\n    [ X-Frame-Options: <string> ]\n    [ X-Content-Type-Options: <string> ]\n    [ X-XSS-Protection: <string> ]\n    [ Strict-Transport-Security: <string> ] ]\n\nbasic_auth_users:\n  [ <string>: <secret> ... ]\n```\n\n----------------------------------------\n\nTITLE: Alerting Rule Syntax\nDESCRIPTION: Syntax definition for creating alerting rules, including alert name, expression, duration settings, and label/annotation configurations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/recording_rules.md#2025-04-16_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# The name of the alert. Must be a valid label value.\nalert: <string>\n\n# The PromQL expression to evaluate. Every evaluation cycle this is\n# evaluated at the current time, and all resultant time series become\n# pending/firing alerts.\nexpr: <string>\n\n# Alerts are considered firing once they have been returned for this long.\n# Alerts which have not yet fired for long enough are considered pending.\n[ for: <duration> | default = 0s ]\n\n# How long an alert will continue firing after the condition that triggered it\n# has cleared.\n[ keep_firing_for: <duration> | default = 0s ]\n\n# Labels to add or overwrite for each alert.\nlabels:\n  [ <labelname>: <tmpl_string> ]\n\n# Annotations to add to each alert.\nannotations:\n  [ <labelname>: <tmpl_string> ]\n```\n\n----------------------------------------\n\nTITLE: HTTP Service Discovery Example Implementation\nDESCRIPTION: Comprehensive example showing multiple target groups with different datacenter locations and Prometheus jobs, demonstrating how to structure the response with multiple targets and labels.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/http_sd.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"targets\": [\"10.0.10.2:9100\", \"10.0.10.3:9100\", \"10.0.10.4:9100\", \"10.0.10.5:9100\"],\n        \"labels\": {\n            \"__meta_datacenter\": \"london\",\n            \"__meta_prometheus_job\": \"node\"\n        }\n    },\n    {\n        \"targets\": [\"10.0.40.2:9100\", \"10.0.40.3:9100\"],\n        \"labels\": {\n            \"__meta_datacenter\": \"london\",\n            \"__meta_prometheus_job\": \"alertmanager\"\n        }\n    },\n    {\n        \"targets\": [\"10.0.40.2:9093\", \"10.0.40.3:9093\"],\n        \"labels\": {\n            \"__meta_datacenter\": \"newyork\",\n            \"__meta_prometheus_job\": \"alertmanager\"\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Querying Series by Label Matchers - API Response Example\nDESCRIPTION: Example response from the /api/v1/series endpoint showing matched series data for 'up' and 'process_start_time_seconds' metrics. The response includes series metadata with label sets.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"status\" : \"success\",\n   \"data\" : [\n      {\n         \"__name__\" : \"up\",\n         \"job\" : \"prometheus\",\n         \"instance\" : \"localhost:9090\"\n      },\n      {\n         \"__name__\" : \"up\",\n         \"job\" : \"node\",\n         \"instance\" : \"localhost:9091\"\n      },\n      {\n         \"__name__\" : \"process_start_time_seconds\",\n         \"job\" : \"prometheus\",\n         \"instance\" : \"localhost:9090\"\n      }\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Subquery with Rate Function in PromQL\nDESCRIPTION: This query calculates the 5-minute rate of the 'http_requests_total' metric for the past 30 minutes, with a resolution of 1 minute. It shows how to use subqueries for complex time-based calculations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_5\n\nLANGUAGE: promql\nCODE:\n```\nrate(http_requests_total[5m])[30m:1m]\n```\n\n----------------------------------------\n\nTITLE: Total HTTP Requests Aggregation in PromQL\nDESCRIPTION: Example of using the 'sum' aggregation operator without any clauses to calculate the total HTTP requests across all applications.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_14\n\nLANGUAGE: promql\nCODE:\n```\nsum(http_requests_total)\n```\n\n----------------------------------------\n\nTITLE: Counting Build Versions with count_values in PromQL\nDESCRIPTION: Example of using the 'count_values' aggregation operator to count the number of binaries running each build version.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_15\n\nLANGUAGE: promql\nCODE:\n```\ncount_values(\"version\", build_version)\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Rules API\nDESCRIPTION: Example of using the /api/v1/rules endpoint to retrieve both alerting and recording rules. Shows the full response structure including active alerts and rule configurations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"data\": {\n        \"groups\": [\n            {\n                \"rules\": [\n                    {\n                        \"alerts\": [\n                            {\n                                \"activeAt\": \"2018-07-04T20:27:12.60602144+02:00\",\n                                \"annotations\": {\n                                    \"summary\": \"High request latency\"\n                                },\n                                \"labels\": {\n                                    \"alertname\": \"HighRequestLatency\",\n                                    \"severity\": \"page\"\n                                },\n                                \"state\": \"firing\",\n                                \"value\": \"1e+00\"\n                            }\n                        ],\n                        \"annotations\": {\n                            \"summary\": \"High request latency\"\n                        },\n                        \"duration\": 600,\n                        \"health\": \"ok\",\n                        \"labels\": {\n                            \"severity\": \"page\"\n                        },\n                        \"name\": \"HighRequestLatency\",\n                        \"query\": \"job:request_latency_seconds:mean5m{job=\\\"myjob\\\"} > 0.5\",\n                        \"type\": \"alerting\"\n                    },\n                    {\n                        \"health\": \"ok\",\n                        \"name\": \"job:http_inprogress_requests:sum\",\n                        \"query\": \"sum by (job) (http_inprogress_requests)\",\n                        \"type\": \"recording\"\n                    }\n                ],\n                \"file\": \"/rules.yaml\",\n                \"interval\": 60,\n                \"limit\": 0,\n                \"name\": \"example\"\n            }\n        ]\n    },\n    \"status\": \"success\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing the Discoverer Interface in Prometheus Service Discovery\nDESCRIPTION: The core interface that any service discovery mechanism must implement in Prometheus. The Run method initializes the discovery process and sends target groups to Prometheus through a channel.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/discovery/README.md#2025-04-16_snippet_0\n\nLANGUAGE: go\nCODE:\n```\ntype Discoverer interface {\n\tRun(ctx context.Context, up chan<- []*targetgroup.Group)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Federation Scrape in Prometheus YAML\nDESCRIPTION: This YAML configuration example shows how to set up a Prometheus server to federate metrics from other Prometheus servers. It defines a scrape job that targets multiple source Prometheus servers, selects specific metrics using match parameters, and ensures labels are preserved with honor_labels.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/federation.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nscrape_configs:\n  - job_name: 'federate'\n    scrape_interval: 15s\n\n    honor_labels: true\n    metrics_path: '/federate'\n\n    params:\n      'match[]':\n        - '{job=\"prometheus\"}'\n        - '{__name__=~\"job:.*\"}'\n\n    static_configs:\n      - targets:\n        - 'source-prometheus-1:9090'\n        - 'source-prometheus-2:9090'\n        - 'source-prometheus-3:9090'\n```\n\n----------------------------------------\n\nTITLE: Prometheus Release Changelog Entry 2.51.0\nDESCRIPTION: Major release notes for version 2.51.0 built with Go 1.22.1, introducing new features and optimizations including optional dedupelabels build tag.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n## 2.51.0 / 2024-03-18\n\nThis version is built with Go 1.22.1.\n\nThere is a new optional build tag \"dedupelabels\", which should reduce memory consumption (#12304).\nIt is off by default; there will be an optional alternative image to try it out.\n```\n\n----------------------------------------\n\nTITLE: Using Underscores in Numeric Literals for Readability\nDESCRIPTION: Demonstrates how underscores can be used in decimal or hexadecimal literals to improve readability of large numbers without affecting their value.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_2\n\nLANGUAGE: promql\nCODE:\n```\n1_000_000\n.123_456_789\n0x_53_AB_F3_82\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP-based Service Discovery in Prometheus\nDESCRIPTION: Configuration schema for HTTP-based service discovery, allowing Prometheus to fetch target lists from HTTP endpoints. Includes URL, refresh interval, and HTTP client settings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_29\n\nLANGUAGE: yaml\nCODE:\n```\n# URL from which the targets are fetched.\nurl: <string>\n\n# Refresh interval to re-query the endpoint.\n[ refresh_interval: <duration> | default = 60s ]\n\n# HTTP client settings, including authentication methods (such as basic auth and\n# authorization), proxy configurations, TLS options, custom HTTP headers, etc.\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Service Discovery in Prometheus YAML\nDESCRIPTION: YAML configuration for setting up Kubernetes service discovery in Prometheus. This snippet defines the structure for configuring API server addresses, roles, kubeconfig, namespace limitations, and selectors for filtered discovery of Kubernetes resources.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_32\n\nLANGUAGE: yaml\nCODE:\n```\n# The information to access the Kubernetes API.\n\n# The API server addresses. If left empty, Prometheus is assumed to run inside\n# of the cluster and will discover API servers automatically and use the pod's\n# CA certificate and bearer token file at /var/run/secrets/kubernetes.io/serviceaccount/.\n[ api_server: <host> ]\n\n# The Kubernetes role of entities that should be discovered.\n# One of endpoints, endpointslice, service, pod, node, or ingress.\nrole: <string>\n\n# Optional path to a kubeconfig file.\n# Note that api_server and kube_config are mutually exclusive.\n[ kubeconfig_file: <filename> ]\n\n# Optional namespace discovery. If omitted, all namespaces are used.\nnamespaces:\n  own_namespace: <boolean>\n  names:\n    [ - <string> ]\n\n# Optional label and field selectors to limit the discovery process to a subset of available resources.\n# See https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/\n# and https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ to learn more about the possible\n# filters that can be used. The endpoints role supports pod, service and endpoints selectors.\n# The pod role supports node selectors when configured with `attach_metadata: {node: true}`.\n```\n\n----------------------------------------\n\nTITLE: Aggregating 90th Percentile by Job for Native Histograms\nDESCRIPTION: Aggregates the 90th percentile by job label for native histograms\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_9\n\nLANGUAGE: promql\nCODE:\n```\nhistogram_quantile(0.9, sum by (job) (rate(http_request_duration_seconds[10m])))\n```\n\n----------------------------------------\n\nTITLE: Calculating HTTP Request Duration Fraction using histogram_fraction\nDESCRIPTION: Calculates the fraction of HTTP requests that took 200ms or less over the last hour using the histogram_fraction function\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_5\n\nLANGUAGE: promql\nCODE:\n```\nhistogram_fraction(0, 0.2, rate(http_request_duration_seconds[1h]))\n```\n\n----------------------------------------\n\nTITLE: Tracking RPC Latency Distributions with Histogram in Prometheus Format\nDESCRIPTION: Defines a histogram metric for RPC latency distributions with multiple buckets ranging from negative to positive values. The metric includes bucket counts, sum, total count, and creation timestamp, along with comments containing sample data.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.5mfs.om.txt#2025-04-16_snippet_2\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP rpc_durations_histogram_seconds RPC latency distributions.\n# TYPE rpc_durations_histogram_seconds histogram\nrpc_durations_histogram_seconds_bucket{le=\"-0.00099\"} 0\nrpc_durations_histogram_seconds_bucket{le=\"-0.00089\"} 0\nrpc_durations_histogram_seconds_bucket{le=\"-0.0007899999999999999\"} 0\nrpc_durations_histogram_seconds_bucket{le=\"-0.0006899999999999999\"} 0\nrpc_durations_histogram_seconds_bucket{le=\"-0.0005899999999999998\"} 0\nrpc_durations_histogram_seconds_bucket{le=\"-0.0004899999999999998\"} 0\nrpc_durations_histogram_seconds_bucket{le=\"-0.0003899999999999998\"} 0\nrpc_durations_histogram_seconds_bucket{le=\"-0.0002899999999999998\"} 3 # {dummyID=\"17783\"} -0.0003825067330956884 1.7268398142239082e+09\nrpc_durations_histogram_seconds_bucket{le=\"-0.0001899999999999998\"} 5 # {dummyID=\"84741\"} -0.00020178290006788965 1.726839814829977e+09\nrpc_durations_histogram_seconds_bucket{le=\"-8.999999999999979e-05\"} 5\nrpc_durations_histogram_seconds_bucket{le=\"1.0000000000000216e-05\"} 8 # {dummyID=\"19206\"} -4.6156147425468016e-05 1.7268398151337721e+09\nrpc_durations_histogram_seconds_bucket{le=\"0.00011000000000000022\"} 9 # {dummyID=\"3974\"} 9.528436760156754e-05 1.726839814526797e+09\nrpc_durations_histogram_seconds_bucket{le=\"0.00021000000000000023\"} 11 # {dummyID=\"29640\"} 0.00017459624183458996 1.7268398139220061e+09\nrpc_durations_histogram_seconds_bucket{le=\"0.0003100000000000002\"} 15 # {dummyID=\"9818\"} 0.0002791130914009552 1.7268398149821382e+09\nrpc_durations_histogram_seconds_bucket{le=\"0.0004100000000000002\"} 15\nrpc_durations_histogram_seconds_bucket{le=\"0.0005100000000000003\"} 15\nrpc_durations_histogram_seconds_bucket{le=\"0.0006100000000000003\"} 15\nrpc_durations_histogram_seconds_bucket{le=\"0.0007100000000000003\"} 15\nrpc_durations_histogram_seconds_bucket{le=\"0.0008100000000000004\"} 15\nrpc_durations_histogram_seconds_bucket{le=\"0.0009100000000000004\"} 15\nrpc_durations_histogram_seconds_bucket{le=\"+Inf\"} 15\nrpc_durations_histogram_seconds_sum -8.452185437166741e-05\nrpc_durations_histogram_seconds_count 15\nrpc_durations_histogram_seconds_created 1.726839813016302e+09\n```\n\n----------------------------------------\n\nTITLE: Computing 90th Percentile with histogram_quantile for Classic Histogram\nDESCRIPTION: Calculates the 90th percentile of request durations over the last 10 minutes using a classic histogram\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_6\n\nLANGUAGE: promql\nCODE:\n```\nhistogram_quantile(0.9, rate(http_request_duration_seconds_bucket[10m]))\n```\n\n----------------------------------------\n\nTITLE: Using absent() function in PromQL\nDESCRIPTION: The absent() function returns an empty vector if the input has any elements, or a 1-element vector with value 1 if the input is empty. It's useful for alerting when no time series exist for a given metric and label combination.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_0\n\nLANGUAGE: promql\nCODE:\n```\nabsent(nonexistent{job=\"myjob\"})\n# => {job=\"myjob\"}\n\nabsent(nonexistent{job=\"myjob\",instance=~\".*\"})\n# => {job=\"myjob\"}\n\nabsent(sum(nonexistent{job=\"myjob\"}))\n# => {}\n```\n\n----------------------------------------\n\nTITLE: Connecting PromQL Autocompletion to Remote Prometheus Server\nDESCRIPTION: Configuration to connect the autocompletion feature to a remote Prometheus server by providing its URL, enabling metric and label suggestions.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst promQL = new PromQLExtension().setComplete({remote: {url: 'https://prometheus.land'}})\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Read in Prometheus YAML\nDESCRIPTION: Specifies the configuration for remote read endpoints in Prometheus. Includes options for URL, timeout, required matchers, and HTTP client settings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_49\n\nLANGUAGE: yaml\nCODE:\n```\nurl: <string>\n\n[ name: <string> ]\n\nrequired_matchers:\n  [ <labelname>: <labelvalue> ... ]\n\n[ remote_timeout: <duration> | default = 1m ]\n\nheaders:\n  [ <string>: <string> ... ]\n\n[ read_recent: <boolean> | default = false ]\n\n[ filter_external_labels: <boolean> | default = true ]\n\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: HTTP Service Discovery Response Format Example\nDESCRIPTION: JSON format example for the HTTP response body expected by Prometheus's HTTP-based service discovery mechanism. Defines targets and their associated labels.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_30\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"targets\": [ \"<host>\", ... ],\n    \"labels\": {\n      \"<labelname>\": \"<labelvalue>\", ...\n    }\n  },\n  ...\n]\n```\n\n----------------------------------------\n\nTITLE: DNS Service Discovery Configuration\nDESCRIPTION: YAML configuration for DNS-based service discovery, including domain names, query types, port settings, and refresh intervals.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nnames:\n  [ - <string> ]\n\n[ type: <string> | default = 'SRV' ]\n\n[ port: <int>]\n\n[ refresh_interval: <duration> | default = 30s ]\n```\n\n----------------------------------------\n\nTITLE: Tracking Prometheus HTTP Handler Errors in Prometheus Format\nDESCRIPTION: Defines counter metrics for tracking internal errors encountered by the Prometheus HTTP metric handler. The metrics are separated by cause (encoding and gathering) and include both total counts and creation timestamps.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.5mfs.om.txt#2025-04-16_snippet_1\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP promhttp_metric_handler_errors Total number of internal errors encountered by the promhttp metric handler.\n# TYPE promhttp_metric_handler_errors counter\npromhttp_metric_handler_errors_total{cause=\"encoding\"} 0.0\npromhttp_metric_handler_errors_created{cause=\"encoding\"} 1.726839813016397e+09\npromhttp_metric_handler_errors_total{cause=\"gathering\"} 0.0\npromhttp_metric_handler_errors_created{cause=\"gathering\"} 1.726839813016395e+09\n```\n\n----------------------------------------\n\nTITLE: Running with InfluxDB Backend\nDESCRIPTION: Command to run the adapter with InfluxDB as the remote storage backend, including authentication token, organization, and bucket parameters.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/remote_storage/remote_storage_adapter/README.md#2025-04-16_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nINFLUXDB_AUTH_TOKEN=<token> ./remote_storage_adapter --influxdb-url=http://localhost:8086/ --influxdb.organization=<organization_name> --influxdb.bucket=<bucket_name>\n```\n\n----------------------------------------\n\nTITLE: Offset Modifier Usage in PromQL\nDESCRIPTION: Examples of using offset modifier to query historical data and perform temporal comparisons.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_16\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total offset 5m\n```\n\nLANGUAGE: promql\nCODE:\n```\nsum(http_requests_total{method=\"GET\"} offset 5m)\n```\n\nLANGUAGE: promql\nCODE:\n```\nrate(http_requests_total[5m] offset 1w)\n```\n\nLANGUAGE: promql\nCODE:\n```\nrate(http_requests_total[5m] offset -1w)\n```\n\n----------------------------------------\n\nTITLE: Summing HTTP Requests by Application and Group in PromQL\nDESCRIPTION: Example of using the 'sum' aggregation operator with 'without' to calculate the total number of HTTP requests per application and group over all instances.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_12\n\nLANGUAGE: promql\nCODE:\n```\nsum without (instance) (http_requests_total)\n```\n\n----------------------------------------\n\nTITLE: Example Test Configuration in YAML for Prometheus Rule Testing\nDESCRIPTION: Provides a complete example of a test configuration file (test.yml) for Prometheus rule testing, including rule files, evaluation interval, and test cases.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/unit_testing_rules.md#2025-04-16_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n# This is the main input for unit testing.\n# Only this file is passed as command line argument.\n\nrule_files:\n    - alerts.yml\n\nevaluation_interval: 1m\n\ntests:\n    # Test 1.\n    - interval: 1m\n      # Series data.\n      input_series:\n          - series: 'up{job=\"prometheus\", instance=\"localhost:9090\"}'\n            values: '0 0 0 0 0 0 0 0 0 0 0 0 0 0 0'\n          - series: 'up{job=\"node_exporter\", instance=\"localhost:9100\"}'\n            values: '1+0x6 0 0 0 0 0 0 0 0' # 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n          - series: 'go_goroutines{job=\"prometheus\", instance=\"localhost:9090\"}'\n            values: '10+10x2 30+20x5' # 10 20 30 30 50 70 90 110 130\n          - series: 'go_goroutines{job=\"node_exporter\", instance=\"localhost:9100\"}'\n            values: '10+10x7 10+30x4' # 10 20 30 40 50 60 70 80 10 40 70 100 130\n\n      # Unit test for alerting rules.\n      alert_rule_test:\n          # Unit test 1.\n          - eval_time: 10m\n            alertname: InstanceDown\n            exp_alerts:\n                # Alert 1.\n                - exp_labels:\n                      severity: page\n                      instance: localhost:9090\n                      job: prometheus\n                  exp_annotations:\n                      summary: \"Instance localhost:9090 down\"\n                      description: \"localhost:9090 of job prometheus has been down for more than 5 minutes.\"\n      # Unit tests for promql expressions.\n      promql_expr_test:\n          # Unit test 1.\n          - expr: go_goroutines > 5\n            eval_time: 4m\n            exp_samples:\n                # Sample 1.\n                - labels: 'go_goroutines{job=\"prometheus\",instance=\"localhost:9090\"}'\n                  value: 50\n                # Sample 2.\n                - labels: 'go_goroutines{job=\"node_exporter\",instance=\"localhost:9100\"}'\n                  value: 50\n```\n\n----------------------------------------\n\nTITLE: Enabling Experimental Features in Prometheus\nDESCRIPTION: Use the --enable-feature flag to enable experimental features in Prometheus. This snippet shows how to enable the remote write receiver and PromQL @ modifier features.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_14\n\nLANGUAGE: markdown\nCODE:\n```\n* [FEATURE] **experimental** API: Accept remote_write requests. Behind the --enable-feature=remote-write-receiver flag. #8424\n* [FEATURE] **experimental** PromQL: Add `@ <timestamp>` modifier. Behind the --enable-feature=promql-at-modifier flag. #8121 #8436 #8425\n```\n\n----------------------------------------\n\nTITLE: Using histogram_avg() function in PromQL\nDESCRIPTION: The histogram_avg() function calculates the arithmetic average of observed values stored in histogram samples. It's useful for computing average values from native histograms.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_3\n\nLANGUAGE: promql\nCODE:\n```\nhistogram_avg(rate(http_request_duration_seconds[5m]))\n```\n\n----------------------------------------\n\nTITLE: Finding Top CPU Users by Application and Process in PromQL\nDESCRIPTION: This query finds the top 3 CPU users grouped by application and process type. It demonstrates the use of 'topk' function with aggregation.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_11\n\nLANGUAGE: promql\nCODE:\n```\ntopk(3, sum by (app, proc) (rate(instance_cpu_time_ns[5m])))\n```\n\n----------------------------------------\n\nTITLE: Counting Running Instances per Application in PromQL\nDESCRIPTION: This query counts the number of running instances per application. It shows how to use the 'count' aggregation operator with label grouping.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_12\n\nLANGUAGE: promql\nCODE:\n```\ncount by (app) (instance_cpu_time_ns)\n```\n\n----------------------------------------\n\nTITLE: Single Value Display with Go Templates\nDESCRIPTION: Demonstrates how to query and display a single metric value with formatting. Uses the 'with' statement for error handling and the humanize function for value formatting.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/template_examples.md#2025-04-16_snippet_2\n\nLANGUAGE: go\nCODE:\n```\n{{ with query \"some_metric{instance='someinstance'}\" }}\n  {{ . | first | value | humanize }}\n{{ end }}\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS in Prometheus\nDESCRIPTION: TLS configuration for Prometheus, including CA certificates, client certificates, server name extension, verification settings, and TLS version constraints.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n# CA certificate to validate API server certificate with. At most one of ca and ca_file is allowed.\n[ ca: <string> ]\n[ ca_file: <filename> ]\n\n# Certificate and key for client cert authentication to the server.\n# At most one of cert and cert_file is allowed.\n# At most one of key and key_file is allowed.\n[ cert: <string> ]\n[ cert_file: <filename> ]\n[ key: <secret> ]\n[ key_file: <filename> ]\n\n# ServerName extension to indicate the name of the server.\n# https://tools.ietf.org/html/rfc4366#section-3.1\n[ server_name: <string> ]\n\n# Disable validation of the server certificate.\n[ insecure_skip_verify: <boolean> ]\n\n# Minimum acceptable TLS version. Accepted values: TLS10 (TLS 1.0), TLS11 (TLS\n# 1.1), TLS12 (TLS 1.2), TLS13 (TLS 1.3).\n# If unset, Prometheus will use Go default minimum version, which is TLS 1.2.\n# See MinVersion in https://pkg.go.dev/crypto/tls#Config.\n[ min_version: <string> ]\n# Maximum acceptable TLS version. Accepted values: TLS10 (TLS 1.0), TLS11 (TLS\n# 1.1), TLS12 (TLS 1.2), TLS13 (TLS 1.3).\n# If unset, Prometheus will use Go default maximum version, which is TLS 1.3.\n# See MaxVersion in https://pkg.go.dev/crypto/tls#Config.\n[ max_version: <string> ]\n```\n\n----------------------------------------\n\nTITLE: Using absent_over_time() function in PromQL\nDESCRIPTION: The absent_over_time() function is similar to absent(), but works with range vectors. It's useful for alerting when no time series exist for a given metric and label combination over a certain time period.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_1\n\nLANGUAGE: promql\nCODE:\n```\nabsent_over_time(nonexistent{job=\"myjob\"}[1h])\n# => {job=\"myjob\"}\n\nabsent_over_time(nonexistent{job=\"myjob\",instance=~\".*\"}[1h])\n# => {job=\"myjob\"}\n\nabsent_over_time(sum(nonexistent{job=\"myjob\"})[1h:])\n# => {}\n```\n\n----------------------------------------\n\nTITLE: Vector Matching Keywords in PromQL\nDESCRIPTION: Keywords used for controlling how vectors with different label sets are matched in operations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_4\n\nLANGUAGE: promql\nCODE:\n```\non\nignoring\n```\n\n----------------------------------------\n\nTITLE: Negative Regular Expression Matching in PromQL\nDESCRIPTION: This query selects all HTTP status codes except 4xx ones using a negative regular expression match. It demonstrates how to exclude certain label values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_4\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total{status!~\"4..\"}\n```\n\n----------------------------------------\n\nTITLE: Rule Group Configuration Schema\nDESCRIPTION: Detailed configuration schema for a rule group including name, interval, limits, and query offset settings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/recording_rules.md#2025-04-16_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# The name of the group. Must be unique within a file.\nname: <string>\n\n# How often rules in the group are evaluated.\n[ interval: <duration> | default = global.evaluation_interval ]\n\n# Limit the number of alerts an alerting rule and series a recording\n# rule can produce. 0 is no limit.\n[ limit: <int> | default = 0 ]\n\n# Offset the rule evaluation timestamp of this particular group by the specified duration into the past.\n[ query_offset: <duration> | default = global.rule_query_offset ]\n\n# Labels to add or overwrite before storing the result for its rules.\n# Labels defined in <rule> will override the key if it has a collision.\nlabels:\n  [ <labelname>: <labelvalue> ]\n\nrules:\n  [ - <rule> ... ]\n```\n\n----------------------------------------\n\nTITLE: Selecting a Range Vector in PromQL\nDESCRIPTION: This query selects a range of 5 minutes of data for the specified metric and labels, creating a range vector. It demonstrates how to query for historical data.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_2\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total{job=\"apiserver\", handler=\"/api/comments\"}[5m]\n```\n\n----------------------------------------\n\nTITLE: Range Vector Selection in PromQL\nDESCRIPTION: Selects values from last 5 minutes for http_requests_total metric with job label set to prometheus.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_15\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total{job=\"prometheus\"}[5m]\n```\n\n----------------------------------------\n\nTITLE: @ Modifier Usage in PromQL\nDESCRIPTION: Examples of using @ modifier to evaluate queries at specific timestamps.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_17\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total @ 1609746000\n```\n\nLANGUAGE: promql\nCODE:\n```\nsum(http_requests_total{method=\"GET\"} @ 1609746000)\n```\n\nLANGUAGE: promql\nCODE:\n```\nrate(http_requests_total[5m] @ 1609746000)\n```\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total @ 1609746000 offset 5m\n```\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total offset 5m @ 1609746000\n```\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total @ start()\n```\n\nLANGUAGE: promql\nCODE:\n```\nrate(http_requests_total[5m] @ end())\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Scrape Config for RabbitMQ Monitoring\nDESCRIPTION: YAML configuration for Prometheus to discover and scrape metrics from RabbitMQ pods in Kubernetes. Uses kubernetes_sd_configs for pod discovery and includes relabel configurations to filter for RabbitMQ pods specifically.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/kubernetes-rabbitmq/README.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nscrape_configs:\n- job_name: 'RabbitMQ'\n  kubernetes_sd_configs:\n  - role: pod\n  relabel_configs:\n  - source_labels:\n    - __meta_kubernetes_pod_label_app\n    regex: rabbitmq\n    action: keep\n```\n\n----------------------------------------\n\nTITLE: Creating Alert Templates in Prometheus YAML\nDESCRIPTION: Example showing how to define alert templates with labels and annotations in Prometheus. Demonstrates basic variable interpolation using $labels for instance and job information.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/template_examples.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nalert: InstanceDown\nexpr: up == 0\nfor: 5m\nlabels:\n  severity: page\nannotations:\n  summary: \"Instance {{$labels.instance}} down\"\n  description: \"{{$labels.instance}} of job {{$labels.job}} has been down for more than 5 minutes.\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth 2.0 Authentication in Prometheus\nDESCRIPTION: OAuth 2.0 configuration for Prometheus, supporting client credentials or password grant types. Includes client ID/secret settings, token URL configuration, scopes, and HTTP request settings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nclient_id: <string>\n[ client_secret: <secret> ]\n\n# Read the client secret from a file.\n# It is mutually exclusive with `client_secret`.\n[ client_secret_file: <filename> ]\n\n# Scopes for the token request.\nscopes:\n  [ - <string> ... ]\n\n# The URL to fetch the token from.\ntoken_url: <string>\n\n# Optional parameters to append to the token URL.\n# To set 'password' grant type, add it to params:\n# endpoint_params:\n#   grant_type: 'password'\n#   username: 'username@example.com'\n#   password: 'strongpassword'\nendpoint_params:\n  [ <string>: <string> ... ]\n\n# Configures the token request's TLS settings.\ntls_config:\n  [ <tls_config> ]\n\n# Optional proxy URL.\n[ proxy_url: <string> ]\n# Comma-separated string that can contain IPs, CIDR notation, domain names\n# that should be excluded from proxying. IP and domain names can\n# contain port numbers.\n[ no_proxy: <string> ]\n# Use proxy URL indicated by environment variables (HTTP_PROXY, https_proxy, HTTPs_PROXY, https_proxy, and no_proxy)\n[ proxy_from_environment: <boolean> | default: false ]\n# Specifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ <string>: [<secret>, ...] ] ]\n\n# Custom HTTP headers to be sent along with each request.\n# Headers that are set by Prometheus itself can't be overwritten.\nhttp_headers:\n  # Header name.\n  [ <string>:\n    # Header values.\n    [ values: [<string>, ...] ]\n    # Headers values. Hidden in configuration page.\n    [ secrets: [<secret>, ...] ]\n    # Files to read header values from.\n    [ files: [<string>, ...] ] ]\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Prometheus Docker Image with Embedded Configuration\nDESCRIPTION: Dockerfile to create a custom Prometheus image with configuration embedded in the image. This approach works well for static configurations that are consistent across environments.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/installation.md#2025-04-16_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM prom/prometheus\nADD prometheus.yml /etc/prometheus/\n```\n\n----------------------------------------\n\nTITLE: Label Replace with Basic Regex\nDESCRIPTION: Shows how to use label_replace() to create a new label based on regex matching of an existing label value.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_18\n\nLANGUAGE: promql\nCODE:\n```\nlabel_replace(up{job=\"api-server\",service=\"a:c\"}, \"foo\", \"$1\", \"service\", \"(.*):.*\")\n```\n\n----------------------------------------\n\nTITLE: Running Prometheus with Persistent Storage\nDESCRIPTION: Commands to create and use a Docker volume for persisting Prometheus data across container restarts. This prevents data loss when the container is restarted.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/installation.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Create persistent volume for your data\ndocker volume create prometheus-data\n# Start Prometheus container\ndocker run \\\n    -p 9090:9090 \\\n    -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml \\\n    -v prometheus-data:/prometheus \\\n    prom/prometheus\n```\n\n----------------------------------------\n\nTITLE: GCE Service Discovery Configuration in YAML\nDESCRIPTION: YAML configuration for Google Compute Engine service discovery, including project, zone, and filtering options.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_26\n\nLANGUAGE: yaml\nCODE:\n```\nproject: <string>\nzone: <string>\n[ filter: <string> ]\n[ refresh_interval: <duration> | default = 60s ]\n[ port: <int> | default = 80 ]\n```\n\n----------------------------------------\n\nTITLE: Matching Against Metric Names with __name__ Label\nDESCRIPTION: Demonstrates how to use the internal __name__ label to select metrics by name, including using regex patterns to match multiple metric names with a common prefix.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_12\n\nLANGUAGE: promql\nCODE:\n```\n{__name__=~\"job:.*\"}\n```\n\n----------------------------------------\n\nTITLE: Enable Auto Reload Config Flag\nDESCRIPTION: Command flag to enable automatic configuration reload\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=auto-reload-config\n```\n\n----------------------------------------\n\nTITLE: Querying Runtime Information with GET API Endpoint\nDESCRIPTION: API endpoint for retrieving various runtime information properties about the Prometheus server, available since v2.2.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\nGET /api/v1/status/runtimeinfo\n```\n\n----------------------------------------\n\nTITLE: One-to-one Vector Matching Syntax in PromQL\nDESCRIPTION: Syntax for one-to-one vector matching operations in PromQL. The 'ignoring' keyword allows ignoring certain labels when matching, while the 'on' keyword allows reducing the set of considered labels to a provided list.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_6\n\nLANGUAGE: promql\nCODE:\n```\n<vector expr> <bin-op> ignoring(<label list>) <vector expr>\n<vector expr> <bin-op> on(<label list>) <vector expr>\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Response Envelope Format for Prometheus API\nDESCRIPTION: Specifies the standard JSON response envelope format used by the Prometheus API, including status, data, error information, and optional warnings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\" | \"error\",\n  \"data\": <data>,\n\n  // Only set if status is \"error\". The data field may still hold\n  // additional data.\n  \"errorType\": \"<string>\",\n  \"error\": \"<string>\",\n\n  // Only set if there were warnings while executing the request.\n  // There will still be data in the data field.\n  \"warnings\": [\"<string>\"],\n  // Only set if there were info-level annotations while executing the request.\n  \"infos\": [\"<string>\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Memory-Mapping Full Chunks in TSDB\nDESCRIPTION: TSDB now memory-maps full chunks of Head (in-memory) block from disk, reducing memory footprint and making restarts faster.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_23\n\nLANGUAGE: go\nCODE:\n```\n// TSDB: Memory-map full chunks of Head (in-memory) block from disk\n```\n\n----------------------------------------\n\nTITLE: Setting Custom HTTP Headers for Prometheus Requests\nDESCRIPTION: Configuration to add custom HTTP headers to all requests made to the Prometheus API, useful for authentication or additional metadata.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst customHeaders = new Headers({'header-name': 'test-value'});\nconst promql = new PromQLExtension().setComplete({remote: {requestHeaders: customHeaders}})\n```\n\n----------------------------------------\n\nTITLE: Configuring Exemplars in Prometheus YAML\nDESCRIPTION: Specifies the configuration for exemplar storage in Prometheus. This feature is experimental and must be enabled explicitly. It configures the maximum size of the circular buffer for storing exemplars.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_51\n\nLANGUAGE: yaml\nCODE:\n```\n[ max_exemplars: <int> | default = 100000 ]\n```\n\n----------------------------------------\n\nTITLE: Using histogram_count() function in PromQL\nDESCRIPTION: The histogram_count() function returns the count of observations stored in histogram samples. It's useful for calculating rates from histogram series.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_4\n\nLANGUAGE: promql\nCODE:\n```\nhistogram_count(rate(http_request_duration_seconds[10m]))\n```\n\n----------------------------------------\n\nTITLE: Aggregating 90th Percentile by Job for Classic Histograms\nDESCRIPTION: Aggregates the 90th percentile by job label while maintaining the required le label for classic histograms\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_8\n\nLANGUAGE: promql\nCODE:\n```\nhistogram_quantile(0.9, sum by (job, le) (rate(http_request_duration_seconds_bucket[10m])))\n```\n\n----------------------------------------\n\nTITLE: Updating Target Groups with Changed Targets in Prometheus SD\nDESCRIPTION: Example of how to send an update when a target is removed from a target group. The entire changed target group must be sent down the channel with the same Source identifier.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/discovery/README.md#2025-04-16_snippet_2\n\nLANGUAGE: go\nCODE:\n```\n&targetgroup.Group{\n\tTargets: []model.LabelSet{\n\t\t{\n\t\t\t\"__instance__\": \"10.11.122.11:6001\",\n\t\t\t\"hostname\":     \"demo-postgres-1\",\n\t\t\t\"test\":         \"simple-test\",\n\t\t},\n\t},\n\tLabels: model.LabelSet{\n\t\t\"job\": \"postgres\",\n\t},\n\t\"Source\": \"file2\",\n}\n```\n\n----------------------------------------\n\nTITLE: Prometheus HTTP Request Duration Metrics\nDESCRIPTION: Histogram metrics capturing HTTP request duration statistics across various Prometheus API endpoints. The metrics include bucket counts for different duration thresholds, sum of durations, and total request counts per endpoint.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_5\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/labels\",le=\"120\"} 4921\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/labels\",le=\"+Inf\"} 4921\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/labels\"} 12.963155722000025\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/labels\"} 4921\n```\n\n----------------------------------------\n\nTITLE: Basic Instant Vector Selector\nDESCRIPTION: Demonstrates the simplest form of a time series selector that returns all series with the specified metric name, using the most recent sample for each.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_6\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total\n```\n\n----------------------------------------\n\nTITLE: Sampling Timeseries with limitk in PromQL\nDESCRIPTION: Example of using the 'limitk' experimental aggregation operator to sample 10 timeseries for inspection of labels and values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_17\n\nLANGUAGE: promql\nCODE:\n```\nlimitk(10, http_requests_total)\n```\n\n----------------------------------------\n\nTITLE: Aggregating Memory Usage by Application and Process in PromQL\nDESCRIPTION: This query calculates the total unused memory in MiB, aggregated by application and process. It shows how to combine aggregation with arithmetic operations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_10\n\nLANGUAGE: promql\nCODE:\n```\nsum by (app, proc) (\n  instance_memory_limit_bytes - instance_memory_usage_bytes\n) / 1024 / 1024\n```\n\n----------------------------------------\n\nTITLE: Using info() Function with Label Selector\nDESCRIPTION: Simplified version of adding data labels using the info() function with a specific label selector for k8s_cluster_name.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_14\n\nLANGUAGE: promql\nCODE:\n```\ninfo(\n  rate(http_server_request_duration_seconds_count[2m]),\n  {k8s_cluster_name=~\".+\"}\n)\n```\n\n----------------------------------------\n\nTITLE: Prometheus Operator Precedence Examples\nDESCRIPTION: Examples demonstrating operator precedence and associativity in Prometheus. Shows that operators at same level are left-associative, except for ^ which is right-associative.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_18\n\nLANGUAGE: prometheus\nCODE:\n```\n2 * 3 % 2\n```\n\nLANGUAGE: prometheus\nCODE:\n```\n(2 * 3) % 2\n```\n\nLANGUAGE: prometheus\nCODE:\n```\n2 ^ 3 ^ 2\n```\n\nLANGUAGE: prometheus\nCODE:\n```\n2 ^ (3 ^ 2)\n```\n\n----------------------------------------\n\nTITLE: Configuring EC2 Service Discovery in Prometheus\nDESCRIPTION: YAML configuration for EC2 service discovery in Prometheus. This configuration allows Prometheus to discover and scrape targets from AWS EC2 instances. It includes options for AWS authentication, region selection, refresh intervals, and instance filtering.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\n# The information to access the EC2 API.\n\n# The AWS region. If blank, the region from the instance metadata is used.\n[ region: <string> ]\n\n# Custom endpoint to be used.\n[ endpoint: <string> ]\n\n# The AWS API keys. If blank, the environment variables `AWS_ACCESS_KEY_ID`\n# and `AWS_SECRET_ACCESS_KEY` are used.\n[ access_key: <string> ]\n[ secret_key: <secret> ]\n# Named AWS profile used to connect to the API.\n[ profile: <string> ]\n\n# AWS Role ARN, an alternative to using AWS API keys.\n[ role_arn: <string> ]\n\n# Refresh interval to re-read the instance list.\n[ refresh_interval: <duration> | default = 60s ]\n\n# The port to scrape metrics from. If using the public IP address, this must\n# instead be specified in the relabeling rule.\n[ port: <int> | default = 80 ]\n\n# Filters can be used optionally to filter the instance list by other criteria.\n# Available filter criteria can be found here:\n# https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html\n# Filter API documentation: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_Filter.html\nfilters:\n  [ - name: <string>\n      values: <string>, [...] ]\n\n# HTTP client settings, including authentication methods (such as basic auth and\n# authorization), proxy configurations, TLS options, custom HTTP headers, etc.\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: PromQL Complex Aggregation Query\nDESCRIPTION: Complex expression showing rate calculations with aggregations, grouping, and binary operations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/test/expression.txt#2025-04-16_snippet_2\n\nLANGUAGE: PromQL\nCODE:\n```\nsum by(job, mode) (rate(node_cpu_seconds_total[1m])) / on(job) group_left sum by(job)(rate(node_cpu_seconds_total[1m]))\n```\n\n----------------------------------------\n\nTITLE: Example Response for TSDB Statistics\nDESCRIPTION: Sample response showing TSDB head block statistics, series counts by metric name, label value counts, memory usage, and series counts by label value pairs.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\n$ curl http://localhost:9090/api/v1/status/tsdb\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"headStats\": {\n      \"numSeries\": 508,\n      \"chunkCount\": 937,\n      \"minTime\": 1591516800000,\n      \"maxTime\": 1598896800143,\n    },\n    \"seriesCountByMetricName\": [\n      {\n        \"name\": \"net_conntrack_dialer_conn_failed_total\",\n        \"value\": 20\n      },\n      {\n        \"name\": \"prometheus_http_request_duration_seconds_bucket\",\n        \"value\": 20\n      }\n    ],\n    \"labelValueCountByLabelName\": [\n      {\n        \"name\": \"__name__\",\n        \"value\": 211\n      },\n      {\n        \"name\": \"event\",\n        \"value\": 3\n      }\n    ],\n    \"memoryInBytesByLabelName\": [\n      {\n        \"name\": \"__name__\",\n        \"value\": 8266\n      },\n      {\n        \"name\": \"instance\",\n        \"value\": 28\n      }\n    ],\n    \"seriesCountByLabelValuePair\": [\n      {\n        \"name\": \"job=prometheus\",\n        \"value\": 425\n      },\n      {\n        \"name\": \"instance=localhost:9090\",\n        \"value\": 425\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Triton Service Discovery in Prometheus\nDESCRIPTION: Configuration for Triton service discovery in Prometheus, which retrieves scrape targets from Container Monitor discovery endpoints. Includes configuration options for container and compute node discovery.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_40\n\nLANGUAGE: yaml\nCODE:\n```\n# The information to access the Triton discovery API.\n\n# The account to use for discovering new targets.\naccount: <string>\n\n# The type of targets to discover, can be set to:\n# * \"container\" to discover virtual machines (SmartOS zones, lx/KVM/bhyve branded zones) running on Triton\n# * \"cn\" to discover compute nodes (servers/global zones) making up the Triton infrastructure\n[ role : <string> | default = \"container\" ]\n\n# The DNS suffix which should be applied to target.\ndns_suffix: <string>\n\n# The Triton discovery endpoint (e.g. 'cmon.us-east-3b.triton.zone'). This is\n# often the same value as dns_suffix.\nendpoint: <string>\n\n# A list of groups for which targets are retrieved, only supported when `role` == `container`.\n# If omitted all containers owned by the requesting account are scraped.\ngroups:\n  [ - <string> ... ]\n\n# The port to use for discovery and metric scraping.\n[ port: <int> | default = 9163 ]\n\n# The interval which should be used for refreshing targets.\n[ refresh_interval: <duration> | default = 60s ]\n\n# The Triton discovery API version.\n[ version: <int> | default = 1 ]\n\n# TLS configuration.\ntls_config:\n  [ <tls_config> ]\n```\n\n----------------------------------------\n\nTITLE: Monitoring Prometheus Alertmanager Notifications Metrics\nDESCRIPTION: Metrics related to Prometheus alertmanager operations including discovered alertmanagers, notification latency, queue capacity, and alert sending statistics. These metrics help monitor the health and performance of the alert notification system.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_17\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP prometheus_notifications_alertmanagers_discovered The number of alertmanagers discovered and active.\n# TYPE prometheus_notifications_alertmanagers_discovered gauge\nprometheus_notifications_alertmanagers_discovered 1\n# HELP prometheus_notifications_dropped_total Total number of alerts dropped due to errors when sending to Alertmanager.\n# TYPE prometheus_notifications_dropped_total counter\nprometheus_notifications_dropped_total 0\n# HELP prometheus_notifications_errors_total Total number of sent alerts affected by errors.\n# TYPE prometheus_notifications_errors_total counter\nprometheus_notifications_errors_total{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\"} 0\n# HELP prometheus_notifications_latency_seconds Latency quantiles for sending alert notifications.\n# TYPE prometheus_notifications_latency_seconds summary\nprometheus_notifications_latency_seconds{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\",quantile=\"0.5\"} 0.001566044\nprometheus_notifications_latency_seconds{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\",quantile=\"0.9\"} 0.003927931\nprometheus_notifications_latency_seconds{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\",quantile=\"0.99\"} 0.013928135\nprometheus_notifications_latency_seconds_sum{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\"} 194.15032606200046\nprometheus_notifications_latency_seconds_count{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\"} 75180\n# HELP prometheus_notifications_queue_capacity The capacity of the alert notifications queue.\n# TYPE prometheus_notifications_queue_capacity gauge\nprometheus_notifications_queue_capacity 10000\n# HELP prometheus_notifications_queue_length The number of alert notifications in the queue.\n# TYPE prometheus_notifications_queue_length gauge\nprometheus_notifications_queue_length 0\n# HELP prometheus_notifications_sent_total Total number of alerts sent.\n# TYPE prometheus_notifications_sent_total counter\nprometheus_notifications_sent_total{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\"} 141616\n```\n\n----------------------------------------\n\nTITLE: Querying Label Values - API Response Example\nDESCRIPTION: Example response from the /api/v1/label/<label_name>/values endpoint showing values for the http_status_code label.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"status\" : \"success\",\n   \"data\" : [\n      \"200\",\n      \"504\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Prometheus HTTP Request Duration Metrics in Plaintext Format\nDESCRIPTION: A detailed histogram of HTTP request durations across various Prometheus API endpoints. The metrics show request count distribution across different duration buckets, along with sum and count values for each handler path. This data is useful for monitoring API performance and identifying slowdowns.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n# TYPE prometheus_http_request_duration_seconds histogram\nprometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"0.1\"} 688\nprometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"0.2\"} 688\nprometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"0.4\"} 688\nprometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"1\"} 688\nprometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"3\"} 688\nprometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"8\"} 688\nprometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"20\"} 688\nprometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"60\"} 688\nprometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"120\"} 688\nprometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"+Inf\"} 688\nprometheus_http_request_duration_seconds_sum{handler=\"/\"} 0.026826932000000022\nprometheus_http_request_duration_seconds_count{handler=\"/\"} 688\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/healthy\",le=\"0.1\"} 29524\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/healthy\",le=\"0.2\"} 29524\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/healthy\",le=\"0.4\"} 29524\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/healthy\",le=\"1\"} 29524\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/healthy\",le=\"3\"} 29524\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/healthy\",le=\"8\"} 29524\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/healthy\",le=\"20\"} 29524\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/healthy\",le=\"60\"} 29524\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/healthy\",le=\"120\"} 29524\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/healthy\",le=\"+Inf\"} 29524\nprometheus_http_request_duration_seconds_sum{handler=\"/-/healthy\"} 0.7400570460000002\nprometheus_http_request_duration_seconds_count{handler=\"/-/healthy\"} 29524\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/ready\",le=\"0.1\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/ready\",le=\"0.2\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/ready\",le=\"0.4\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/ready\",le=\"1\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/ready\",le=\"3\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/ready\",le=\"8\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/ready\",le=\"20\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/ready\",le=\"60\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/ready\",le=\"120\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/-/ready\",le=\"+Inf\"} 49\nprometheus_http_request_duration_seconds_sum{handler=\"/-/ready\"} 0.005040123000000002\nprometheus_http_request_duration_seconds_count{handler=\"/-/ready\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/alerts\",le=\"0.1\"} 48\nprometheus_http_request_duration_seconds_bucket{handler=\"/alerts\",le=\"0.2\"} 48\nprometheus_http_request_duration_seconds_bucket{handler=\"/alerts\",le=\"0.4\"} 48\nprometheus_http_request_duration_seconds_bucket{handler=\"/alerts\",le=\"1\"} 48\nprometheus_http_request_duration_seconds_bucket{handler=\"/alerts\",le=\"3\"} 48\nprometheus_http_request_duration_seconds_bucket{handler=\"/alerts\",le=\"8\"} 48\nprometheus_http_request_duration_seconds_bucket{handler=\"/alerts\",le=\"20\"} 48\nprometheus_http_request_duration_seconds_bucket{handler=\"/alerts\",le=\"60\"} 48\nprometheus_http_request_duration_seconds_bucket{handler=\"/alerts\",le=\"120\"} 48\nprometheus_http_request_duration_seconds_bucket{handler=\"/alerts\",le=\"+Inf\"} 48\nprometheus_http_request_duration_seconds_sum{handler=\"/alerts\"} 0.011801602999999999\nprometheus_http_request_duration_seconds_count{handler=\"/alerts\"} 48\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"0.1\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"0.2\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"0.4\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"1\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"3\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"8\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"20\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"60\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"120\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"+Inf\"} 27\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/*path\"} 0.001724389\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/*path\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alertmanagers\",le=\"0.1\"} 8\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alertmanagers\",le=\"0.2\"} 8\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alertmanagers\",le=\"0.4\"} 8\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alertmanagers\",le=\"1\"} 8\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alertmanagers\",le=\"3\"} 8\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alertmanagers\",le=\"8\"} 8\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alertmanagers\",le=\"20\"} 8\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alertmanagers\",le=\"60\"} 8\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alertmanagers\",le=\"120\"} 8\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alertmanagers\",le=\"+Inf\"} 8\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/alertmanagers\"} 0.042492975999999995\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/alertmanagers\"} 8\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alerts\",le=\"0.1\"} 14630\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alerts\",le=\"0.2\"} 14635\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alerts\",le=\"0.4\"} 14635\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alerts\",le=\"1\"} 14635\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alerts\",le=\"3\"} 14635\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alerts\",le=\"8\"} 14635\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alerts\",le=\"20\"} 14635\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alerts\",le=\"60\"} 14635\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alerts\",le=\"120\"} 14635\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/alerts\",le=\"+Inf\"} 14635\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/alerts\"} 19.028669391999912\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/alerts\"} 14635\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/format_query\",le=\"0.1\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/format_query\",le=\"0.2\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/format_query\",le=\"0.4\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/format_query\",le=\"1\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/format_query\",le=\"3\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/format_query\",le=\"8\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/format_query\",le=\"20\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/format_query\",le=\"60\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/format_query\",le=\"120\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/format_query\",le=\"+Inf\"} 4\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/format_query\"} 0.023786675\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/format_query\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/label/:name/values\",le=\"0.1\"} 17773\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/label/:name/values\",le=\"0.2\"} 17860\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/label/:name/values\",le=\"0.4\"} 17939\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/label/:name/values\",le=\"1\"} 17970\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/label/:name/values\",le=\"3\"} 17971\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/label/:name/values\",le=\"8\"} 17971\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/label/:name/values\",le=\"20\"} 17971\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/label/:name/values\",le=\"60\"} 17971\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/label/:name/values\",le=\"120\"} 17971\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/label/:name/values\",le=\"+Inf\"} 17971\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/label/:name/values\"} 99.95326355699925\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/label/:name/values\"} 17971\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/labels\",le=\"0.1\"} 4905\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/labels\",le=\"0.2\"} 4912\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/labels\",le=\"0.4\"} 4916\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/labels\",le=\"1\"} 4921\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/labels\",le=\"3\"} 4921\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/labels\",le=\"8\"} 4921\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/labels\",le=\"20\"} 4921\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/labels\",le=\"60\"} 4921\n```\n\n----------------------------------------\n\nTITLE: Nested Subquery with Multiple Functions in PromQL\nDESCRIPTION: This complex query demonstrates a nested subquery using multiple functions including 'max_over_time', 'deriv', and 'rate'. It shows advanced usage of subqueries and time-based functions.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_6\n\nLANGUAGE: promql\nCODE:\n```\nmax_over_time(deriv(rate(distance_covered_total[5s])[30s:5s])[10m:])\n```\n\n----------------------------------------\n\nTITLE: Prometheus Release Changelog Entry 2.50.0\nDESCRIPTION: Major release notes for version 2.50.0 introducing new features and enhancements including remote write improvements and experimental features.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n## 2.50.0 / 2024-02-22\n\n* [CHANGE] Remote Write: Error `storage.ErrTooOldSample` is now generating HTTP error 400 instead of HTTP error 500. #13335\n```\n\n----------------------------------------\n\nTITLE: Managing Dependencies with Go Modules\nDESCRIPTION: Commands for adding and updating dependencies using Go modules, including version selection and module tidying\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CONTRIBUTING.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Pick the latest tagged release.\ngo get example.com/some/module/pkg@latest\n\n# Pick a specific version.\ngo get example.com/some/module/pkg@vX.Y.Z\n```\n\nLANGUAGE: bash\nCODE:\n```\n# The GO111MODULE variable can be omitted when the code isn't located in GOPATH.\nGO111MODULE=on go mod tidy\n```\n\n----------------------------------------\n\nTITLE: Reserved Keyword Workaround with __name__ Label\nDESCRIPTION: Shows how to reference metrics whose names match reserved keywords by using the __name__ label explicitly instead of the direct metric name syntax.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_13\n\nLANGUAGE: promql\nCODE:\n```\non{} # Bad!\n```\n\n----------------------------------------\n\nTITLE: Querying Alertmanagers Status with GET API Endpoint\nDESCRIPTION: API endpoint for retrieving the current state of Prometheus alertmanager discovery, showing both active and dropped Alertmanagers.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nGET /api/v1/alertmanagers\n```\n\n----------------------------------------\n\nTITLE: Example Request for Metadata of a Specific Metric\nDESCRIPTION: Example curl request that retrieves metadata only for the http_requests_total metric using the metric parameter.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncurl -G http://localhost:9090/api/v1/metadata?metric=http_requests_total\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"http_requests_total\": [\n      {\n        \"type\": \"counter\",\n        \"help\": \"Number of HTTP requests\",\n        \"unit\": \"\"\n      },\n      {\n        \"type\": \"counter\",\n        \"help\": \"Amount of HTTP requests\",\n        \"unit\": \"\"\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Monitoring Prometheus Rule Evaluation Performance\nDESCRIPTION: Metrics related to rule evaluation performance including duration, failures, and total evaluations for different rule groups. These metrics help monitor the health and efficiency of Prometheus's rule evaluation system.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_19\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP prometheus_rule_evaluation_duration_seconds The duration for a rule to execute.\n# TYPE prometheus_rule_evaluation_duration_seconds summary\nprometheus_rule_evaluation_duration_seconds{quantile=\"0.5\"} 0.000214623\nprometheus_rule_evaluation_duration_seconds{quantile=\"0.9\"} 0.001456135\nprometheus_rule_evaluation_duration_seconds{quantile=\"0.99\"} 0.008111814\nprometheus_rule_evaluation_duration_seconds_sum 5209.704794862625\nprometheus_rule_evaluation_duration_seconds_count 7.203456e+06\n# HELP prometheus_rule_evaluation_failures_total The total number of rule evaluation failures.\n# TYPE prometheus_rule_evaluation_failures_total counter\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 0\n# HELP prometheus_rule_evaluations_total The total number of rule evaluations.\n# TYPE prometheus_rule_evaluations_total counter\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 118092\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 118090\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 1.4761e+06\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 1.476125e+06\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 649495\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 649484\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 1.358035e+06\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 1.358035e+06\n# HELP prometheus_rule_group_duration_seconds The duration of rule group evaluations.\n# TYPE prometheus_rule_group_duration_seconds summary\nprometheus_rule_group_duration_seconds{quantile=\"0.01\"} 0.000735928\nprometheus_rule_group_duration_seconds{quantile=\"0.05\"} 0.000818857\nprometheus_rule_group_duration_seconds{quantile=\"0.5\"} 0.004852081\nprometheus_rule_group_duration_seconds{quantile=\"0.9\"} 0.022897759\nprometheus_rule_group_duration_seconds{quantile=\"0.99\"} 0.069327797\nprometheus_rule_group_duration_seconds_sum 5335.451440133042\nprometheus_rule_group_duration_seconds_count 472359\n# HELP prometheus_rule_group_interval_seconds The interval of a rule group.\n```\n\n----------------------------------------\n\nTITLE: Basic Setup of PromQL Extension in CodeMirror\nDESCRIPTION: Basic initialization of a CodeMirror editor with the PromQL extension, enabling syntax highlighting, autocompletion, and linting by default.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport {PromQLExtension} from '@prometheus-io/codemirror-promql';\nimport {basicSetup} from '@codemirror/basic-setup';\nimport {EditorState} from '@codemirror/state';\nimport {EditorView} from '@codemirror/view';\n\nconst promQL = new PromQLExtension()\nnew EditorView({\n    state: EditorState.create({\n        extensions: [basicSetup, promQL.asExtension()],\n    }),\n    // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n    // tslint:disable-next-line:no-non-null-assertion\n    parent: document.getElementById('editor')!,\n});\n```\n\n----------------------------------------\n\nTITLE: Deleting Series Data in Prometheus\nDESCRIPTION: This endpoint deletes data for selected series within a specified time range. The data remains on disk until cleaned up by future compactions or explicitly via the Clean Tombstones endpoint. Requires the admin API to be enabled.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_35\n\nLANGUAGE: json\nCODE:\n```\n$ curl -X POST \\\n  -g 'http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=up&match[]=process_start_time_seconds{job=\"prometheus\"}'\n```\n\n----------------------------------------\n\nTITLE: Docker Engine Service Discovery Configuration\nDESCRIPTION: YAML configuration options for Docker Engine service discovery including port settings, host networking, filters, and refresh intervals.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\n[ port: <int> | default = 80 ]\n\n[ host_networking_host: <string> | default = \"localhost\" ]\n\n[ match_first_network: <boolean> | default = true ]\n\n[ filters:\n  [ - name: <string>\n      values: <string>, [...] ]\n\n[ refresh_interval: <duration> | default = 60s ]\n\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Targets in Prometheus\nDESCRIPTION: YAML configuration for defining static scrape targets in Prometheus. Allows specification of target hosts and common labels for all metrics scraped from these targets.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_45\n\nLANGUAGE: yaml\nCODE:\n```\ntargets:\n  [ - '<host>' ]\n\nlabels:\n  [ <labelname>: <labelvalue> ... ]\n```\n\n----------------------------------------\n\nTITLE: Promtool Metrics Check Example\nDESCRIPTION: Example showing how to check Prometheus metrics using standard input or from a URL\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/command-line/promtool.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ cat metrics.prom | promtool check metrics\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ curl -s http://localhost:9090/metrics | promtool check metrics\n```\n\n----------------------------------------\n\nTITLE: Example Alerting Rules in YAML for Prometheus\nDESCRIPTION: Demonstrates an example of Prometheus alerting rules (alerts.yml) used in conjunction with the test configuration.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/unit_testing_rules.md#2025-04-16_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n# This is the rules file.\n\ngroups:\n- name: example\n  rules:\n\n  - alert: InstanceDown\n    expr: up == 0\n    for: 5m\n    labels:\n        severity: page\n    annotations:\n        summary: \"Instance {{ $labels.instance }} down\"\n        description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.\"\n\n  - alert: AnotherInstanceDown\n    expr: up == 0\n    for: 10m\n    labels:\n        severity: page\n    annotations:\n        summary: \"Instance {{ $labels.instance }} down\"\n        description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.\"\n```\n\n----------------------------------------\n\nTITLE: Info Function Join Query Example\nDESCRIPTION: Traditional approach for adding data labels using a join query with target_info metric. Shows how to combine rate calculations with label joining.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_13\n\nLANGUAGE: promql\nCODE:\n```\nrate(http_server_request_duration_seconds_count[2m])\n* on (job, instance) group_left (k8s_cluster_name)\n  target_info\n```\n\n----------------------------------------\n\nTITLE: Valid Non-Empty Matcher Examples\nDESCRIPTION: Shows valid selectors that are guaranteed to match only non-empty label values, either through regex patterns requiring at least one character or by including additional constraints.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_11\n\nLANGUAGE: promql\nCODE:\n```\n{job=~\".+\"}              # Good!\n{job=~\".*\",method=\"get\"} # Good!\n```\n\n----------------------------------------\n\nTITLE: Building Complete Prometheus Binary with UI Assets\nDESCRIPTION: Command to build a Prometheus binary that includes compiled-in versions of both React app versions, installing dependencies and building production assets.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/README.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Prometheus Metrics - Go Runtime Statistics and Network Tracking\nDESCRIPTION: A comprehensive set of Prometheus metrics capturing Go runtime behavior including garbage collection cycles, memory usage, goroutine stats, scheduler latencies, and network connection tracking across different services\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\ngo_gc_cycles_automatic_gc_cycles_total 190932\ngo_gc_cycles_forced_gc_cycles_total 0\ngo_gc_cycles_total_gc_cycles_total 190932\ngo_gc_duration_seconds{quantile=\"0\"} 7.6238e-05\ngo_gc_duration_seconds{quantile=\"0.25\"} 0.00010188\ngo_gc_duration_seconds{quantile=\"0.5\"} 0.000135819\ngo_gc_duration_seconds{quantile=\"0.75\"} 0.000156061\ngo_gc_duration_seconds{quantile=\"1\"} 0.002389378\ngo_gc_duration_seconds_sum 44.985611511\ngo_gc_duration_seconds_count 190932\ngo_gc_gogc_percent 75\ngo_gc_gomemlimit_bytes 9.03676723e+08\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"8.999999999999998\"} 2.279966416e+09\n# ... additional metrics truncated for brevity ...\n```\n\n----------------------------------------\n\nTITLE: Time Duration Syntax in PromQL\nDESCRIPTION: Shows how to specify time durations in PromQL using various time units (ms, s, m, h, d, w, y) combined with decimal integers. Multiple units can be concatenated in decreasing order of duration.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_3\n\nLANGUAGE: promql\nCODE:\n```\n1s # Equivalent to 1.\n2m # Equivalent to 120.\n1ms # Equivalent to 0.001.\n-2h # Equivalent to -7200.\n```\n\n----------------------------------------\n\nTITLE: Tracking Prometheus System Status and Remote Storage Metrics\nDESCRIPTION: These metrics show Prometheus system readiness status and remote storage statistics including sample count, exemplars, and timestamp information.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_14\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_ready 1\nprometheus_remote_storage_exemplars_in_total 4.959738e+06\nprometheus_remote_storage_highest_timestamp_in_seconds 1.738949375e+09\nprometheus_remote_storage_histograms_in_total 0\nprometheus_remote_storage_samples_in_total 6.00447296e+08\nprometheus_remote_storage_string_interner_zero_reference_releases_total 0\n```\n\n----------------------------------------\n\nTITLE: Workaround for Reserved Keywords\nDESCRIPTION: Demonstrates the proper way to query for metrics with names that match reserved PromQL keywords by using the __name__ label matcher.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_14\n\nLANGUAGE: promql\nCODE:\n```\n{__name__=\"on\"} # Good!\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running Prometheus Server Components in Go\nDESCRIPTION: The main() function initializes and runs all Prometheus server components, connecting interdependent components and coordinating their startup and shutdown using an actor-like model.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/internal_architecture.md#2025-04-16_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nfunc main() {\n    // Define and parse command-line flags\n    // Instantiate major run-time components\n    // Connect components using channels, references, or contexts\n    // Run all components in an actor-like model\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing Prometheus Query into Abstract Syntax Tree\nDESCRIPTION: Example of using the experimental Prometheus query parsing API endpoint (/api/v1/parse_query) to convert the expression 'foo/bar' into a JSON-formatted abstract syntax tree (AST). Shows the request and JSON response format.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n$ curl 'http://localhost:9090/api/v1/parse_query?query=foo/bar'\n{\n   \"data\" : {\n      \"bool\" : false,\n      \"lhs\" : {\n         \"matchers\" : [\n            {\n               \"name\" : \"__name__\",\n               \"type\" : \"=\",\n               \"value\" : \"foo\"\n            }\n         ],\n         \"name\" : \"foo\",\n         \"offset\" : 0,\n         \"startOrEnd\" : null,\n         \"timestamp\" : null,\n         \"type\" : \"vectorSelector\"\n      },\n      \"matching\" : {\n         \"card\" : \"one-to-one\",\n         \"include\" : [],\n         \"labels\" : [],\n         \"on\" : false\n      },\n      \"op\" : \"/\",\n      \"rhs\" : {\n         \"matchers\" : [\n            {\n               \"name\" : \"__name__\",\n               \"type\" : \"=\",\n               \"value\" : \"bar\"\n            }\n         ],\n         \"name\" : \"bar\",\n         \"offset\" : 0,\n         \"startOrEnd\" : null,\n         \"timestamp\" : null,\n         \"type\" : \"vectorSelector\"\n      },\n      \"type\" : \"binaryExpr\"\n   },\n   \"status\" : \"success\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating TSDB Blocks from OpenMetrics Data\nDESCRIPTION: Command to create TSDB blocks from OpenMetrics format data files. Takes an input file and optional output directory parameter. Default output directory is ./data/.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/storage.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npromtool tsdb create-blocks-from openmetrics <input file> [<output directory>]\n```\n\n----------------------------------------\n\nTITLE: PromQL Label Matching and Operators\nDESCRIPTION: Examples of label matching operations using different comparison operators (=, !=, =~, !~).\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/test/expression.txt#2025-04-16_snippet_3\n\nLANGUAGE: PromQL\nCODE:\n```\nmetric_name{a=\"1\",b!=\"2\",c=~\"3\",d!~\"4\"}\n```\n\n----------------------------------------\n\nTITLE: Graceful Shutdown Endpoint in Prometheus HTTP API\nDESCRIPTION: Endpoint that triggers a graceful shutdown of Prometheus. This endpoint is disabled by default and requires the --web.enable-lifecycle flag to be enabled. Can be accessed via PUT or POST HTTP methods.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/management_api.md#2025-04-16_snippet_3\n\nLANGUAGE: http\nCODE:\n```\nPUT  /-/quit\nPOST /-/quit\n```\n\n----------------------------------------\n\nTITLE: Simple Query Iteration in Go Templates\nDESCRIPTION: Shows how to iterate over query results using range to display instance status. Uses the special dot (.) variable to access current sample values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/template_examples.md#2025-04-16_snippet_1\n\nLANGUAGE: go\nCODE:\n```\n{{ range query \"up\" }}\n  {{ .Labels.instance }} {{ .Value }}\n{{ end }}\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus HTTP Response Size Metrics\nDESCRIPTION: This snippet shows Prometheus metrics for HTTP response sizes. It includes bucket counts for different size ranges, as well as sum and count metrics for various handlers like /config, /consoles/*filepath, /debug/*subpath, and others.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_11\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"1e+09\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"+Inf\"} 13\nprometheus_http_response_size_bytes_sum{handler=\"/config\"} 22776\nprometheus_http_response_size_bytes_count{handler=\"/config\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"100\"} 17\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"1000\"} 17\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"10000\"} 31\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"100000\"} 34\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"1e+06\"} 34\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"1e+07\"} 34\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"1e+08\"} 34\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"1e+09\"} 34\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"+Inf\"} 34\nprometheus_http_response_size_bytes_sum{handler=\"/consoles/*filepath\"} 175556\nprometheus_http_response_size_bytes_count{handler=\"/consoles/*filepath\"} 34\n```\n\n----------------------------------------\n\nTITLE: Readiness Check Endpoint in Prometheus HTTP API\nDESCRIPTION: Endpoint that returns 200 status code when Prometheus is ready to serve traffic and respond to queries. This can be accessed via either GET or HEAD HTTP methods.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/management_api.md#2025-04-16_snippet_1\n\nLANGUAGE: http\nCODE:\n```\nGET /-/ready\nHEAD /-/ready\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Alerts API\nDESCRIPTION: Example of using the /api/v1/alerts endpoint to retrieve all active alerts. Shows the response structure for currently firing alerts.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"data\": {\n        \"alerts\": [\n            {\n                \"activeAt\": \"2018-07-04T20:27:12.60602144+02:00\",\n                \"annotations\": {},\n                \"labels\": {\n                    \"alertname\": \"my-alert\"\n                },\n                \"state\": \"firing\",\n                \"value\": \"1e+00\"\n            }\n        ]\n    },\n    \"status\": \"success\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Regular Expressions for Label Matching in PromQL\nDESCRIPTION: This query uses a regular expression to select time series with job names ending in 'server'. It shows how to use regex for flexible label matching.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_3\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total{job=~\".*server\"}\n```\n\n----------------------------------------\n\nTITLE: Loading and Parsing Prometheus Configuration in Go\nDESCRIPTION: The config.LoadFile() function reads the Prometheus configuration from a file and parses it into a config.Config structure, which represents the overall configuration of the server and its components.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/internal_architecture.md#2025-04-16_snippet_1\n\nLANGUAGE: go\nCODE:\n```\nfunc LoadFile(filename string) (*Config, error) {\n    // Read configuration file\n    // Parse into config.Config structure\n    // Validate and initialize configuration\n    return config, nil\n}\n```\n\n----------------------------------------\n\nTITLE: Sampling Time Series Using Ratio in PromQL\nDESCRIPTION: This query returns approximately 10% of the time series, providing a more evenly sampled result. It shows how to use the 'limit_ratio' function for metric exploration.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_14\n\nLANGUAGE: promql\nCODE:\n```\nlimit_ratio(0.1, app_foo_metric_bar)\n```\n\n----------------------------------------\n\nTITLE: Recording Rule Syntax\nDESCRIPTION: Syntax definition for creating recording rules, including record name, expression, and label configurations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/recording_rules.md#2025-04-16_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# The name of the time series to output to. Must be a valid metric name.\nrecord: <string>\n\n# The PromQL expression to evaluate. Every evaluation cycle this is\n# evaluated at the current time, and the result recorded as a new set of\n# time series with the metric name as given by 'record'.\nexpr: <string>\n\n# Labels to add or overwrite before storing the result.\nlabels:\n  [ <labelname>: <labelvalue> ]\n```\n\n----------------------------------------\n\nTITLE: Handling Configuration Reloads in Prometheus Server\nDESCRIPTION: The configuration reload handler listens for reload requests and re-reads the configuration file, applying the new configuration to all components that support reloading.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/internal_architecture.md#2025-04-16_snippet_2\n\nLANGUAGE: go\nCODE:\n```\n// Configuration reload handler\nfor {\n    select {\n    case <-hupReady:\n        if err := reloadConfig(cfg.configFile, cfg.enableExpandExternalLabels, logger, noStepSubqueryInterval, reloaders...);\n            err != nil {\n            level.Error(logger).Log(\"msg\", \"Error reloading config\", \"err\", err)\n        }\n    case rc := <-webHandler.Reload():\n        if err := reloadConfig(cfg.configFile, cfg.enableExpandExternalLabels, logger, noStepSubqueryInterval, reloaders...);\n            err != nil {\n            level.Error(logger).Log(\"msg\", \"Error reloading config\", \"err\", err)\n            rc <- err\n        } else {\n            rc <- nil\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Request for Querying Limited Metrics Metadata\nDESCRIPTION: Example curl request that retrieves metadata for up to 2 metrics with their respective metadata objects.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncurl -G http://localhost:9090/api/v1/metadata?limit=2\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"cortex_ring_tokens\": [\n      {\n        \"type\": \"gauge\",\n        \"help\": \"Number of tokens in the ring\",\n        \"unit\": \"\"\n      }\n    ],\n    \"http_requests_total\": [\n      {\n        \"type\": \"counter\",\n        \"help\": \"Number of HTTP requests\",\n        \"unit\": \"\"\n      },\n      {\n        \"type\": \"counter\",\n        \"help\": \"Amount of HTTP requests\",\n        \"unit\": \"\"\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Tracking RPC Latency Distributions with Summary in Prometheus Format\nDESCRIPTION: Defines summary metrics for RPC latency distributions across different services (exponential, normal, uniform). Each service has quantile values, sum, count, and creation timestamp to provide statistical information about request durations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.5mfs.om.txt#2025-04-16_snippet_3\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP rpc_durations_seconds RPC latency distributions.\n# TYPE rpc_durations_seconds summary\nrpc_durations_seconds{service=\"exponential\",quantile=\"0.5\"} 7.689368882420941e-07\nrpc_durations_seconds{service=\"exponential\",quantile=\"0.9\"} 1.6537614174305048e-06\nrpc_durations_seconds{service=\"exponential\",quantile=\"0.99\"} 2.0965499063061924e-06\nrpc_durations_seconds_sum{service=\"exponential\"} 2.0318666372575776e-05\nrpc_durations_seconds_count{service=\"exponential\"} 22\nrpc_durations_seconds_created{service=\"exponential\"} 1.7268398130168908e+09\nrpc_durations_seconds{service=\"normal\",quantile=\"0.5\"} -5.066758674917046e-06\nrpc_durations_seconds{service=\"normal\",quantile=\"0.9\"} 0.0002935723711788224\nrpc_durations_seconds{service=\"normal\",quantile=\"0.99\"} 0.0003023094636293776\nrpc_durations_seconds_sum{service=\"normal\"} -8.452185437166741e-05\nrpc_durations_seconds_count{service=\"normal\"} 15\nrpc_durations_seconds_created{service=\"normal\"} 1.726839813016714e+09\nrpc_durations_seconds{service=\"uniform\",quantile=\"0.5\"} 9.005014931474918e-05\nrpc_durations_seconds{service=\"uniform\",quantile=\"0.9\"} 0.00017801230208182325\nrpc_durations_seconds{service=\"uniform\",quantile=\"0.99\"} 0.00018641524538180192\nrpc_durations_seconds_sum{service=\"uniform\"} 0.0011666095700533677\nrpc_durations_seconds_count{service=\"uniform\"} 11\nrpc_durations_seconds_created{service=\"uniform\"} 1.72683981301684e+09\n```\n\n----------------------------------------\n\nTITLE: Basic Trigonometric Functions in PromQL\nDESCRIPTION: Standard trigonometric functions that operate on instant vectors, returning results in radians. These functions ignore histogram samples in the input vector and handle special cases according to Go's math package specifications.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_21\n\nLANGUAGE: promql\nCODE:\n```\nacos(v instant-vector)\nacosh(v instant-vector)\nasin(v instant-vector)\nasinh(v instant-vector)\natan(v instant-vector)\natanh(v instant-vector)\ncos(v instant-vector)\ncosh(v instant-vector)\nsin(v instant-vector)\nsinh(v instant-vector)\ntan(v instant-vector)\ntanh(v instant-vector)\n```\n\n----------------------------------------\n\nTITLE: Limiting Returned Time Series in PromQL\nDESCRIPTION: This query limits the number of returned time series to 10. It demonstrates the use of the 'limitk' function for exploring metrics and their labels.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/examples.md#2025-04-16_snippet_13\n\nLANGUAGE: promql\nCODE:\n```\nlimitk(10, app_foo_metric_bar)\n```\n\n----------------------------------------\n\nTITLE: Backfilling Historical Recording Rule Data\nDESCRIPTION: Command to generate historical data for recording rules. Requires start and end timestamps, Prometheus server URL, and one or more rules files. Creates TSDB blocks with historical rule evaluations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/storage.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npromtool tsdb create-blocks-from rules \\\n    --start 1617079873 \\\n    --end 1617097873 \\\n    --url http://mypromserver.com:9090 \\\n    rules.yaml rules2.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Nomad Service Discovery in Prometheus\nDESCRIPTION: Configuration for Nomad service discovery in Prometheus, which retrieves scrape targets from Nomad's Service API. Includes API access options and available meta labels.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_38\n\nLANGUAGE: yaml\nCODE:\n```\n# The information to access the Nomad API. It is to be defined\n# as the Nomad documentation requires.\n[ allow_stale: <boolean> | default = true ]\n[ namespace: <string> | default = default ]\n[ refresh_interval: <duration> | default = 60s ]\n[ region: <string> | default = global ]\n# The URL to connect to the API.\n[ server: <string> ]\n[ tag_separator: <string> | default = ,]\n\n# HTTP client settings, including authentication methods (such as basic auth and\n# authorization), proxy configurations, TLS options, custom HTTP headers, etc.\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Label Replace with Named Capture Groups\nDESCRIPTION: Demonstrates label_replace() using named capture groups in the regex pattern.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_19\n\nLANGUAGE: promql\nCODE:\n```\nlabel_replace(up{job=\"api-server\",service=\"a:c\"}, \"foo\", \"$name\", \"service\", \"(?P<name>.*):(?P<version>.*)\")\n```\n\n----------------------------------------\n\nTITLE: Configuring UTF-8 Metric Name Validation in Prometheus 3.0\nDESCRIPTION: This YAML configuration demonstrates how to set the metric name validation scheme globally and per-scrape job in Prometheus 3.0. It allows users to choose between the new UTF-8 support and the legacy validation behavior.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/migration.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  metric_name_validation_scheme: legacy\n\nscrape_configs:\n  - job_name: job1\n    metric_name_validation_scheme: utf8\n  - job_name: job2\n    metric_name_validation_scheme: legacy\n```\n\n----------------------------------------\n\nTITLE: URL Parameter Usage in Go Templates\nDESCRIPTION: Shows how to use URL parameters in console templates to generate dynamic queries. Accesses parameters via .Params and formats values using humanize1024.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/template_examples.md#2025-04-16_snippet_3\n\nLANGUAGE: go\nCODE:\n```\n{{ with printf \"node_memory_MemTotal{job='node',instance='%s'}\" .Params.instance | query }}\n  {{ . | first | value | humanize1024 }}B\n{{ end }}\n```\n\n----------------------------------------\n\nTITLE: Example Request for Querying Target Metadata with curl\nDESCRIPTION: Example curl request that retrieves metadata for the go_goroutines metric from the first two targets with job=\"prometheus\" label.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncurl -G http://localhost:9091/api/v1/targets/metadata \\\n    --data-urlencode 'metric=go_goroutines' \\\n    --data-urlencode 'match_target={job=\"prometheus\"}' \\\n    --data-urlencode 'limit=2'\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": [\n    {\n      \"target\": {\n        \"instance\": \"127.0.0.1:9090\",\n        \"job\": \"prometheus\"\n      },\n      \"type\": \"gauge\",\n      \"help\": \"Number of goroutines that currently exist.\",\n      \"unit\": \"\"\n    },\n    {\n      \"target\": {\n        \"instance\": \"127.0.0.1:9091\",\n        \"job\": \"prometheus\"\n      },\n      \"type\": \"gauge\",\n      \"help\": \"Number of goroutines that currently exist.\",\n      \"unit\": \"\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Exemplars - API Response Example\nDESCRIPTION: Example response from the experimental /api/v1/query_exemplars endpoint showing exemplar data with trace IDs and timestamps.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"status\": \"success\",\n    \"data\": [\n        {\n            \"seriesLabels\": {\n                \"__name__\": \"test_exemplar_metric_total\",\n                \"instance\": \"localhost:8090\",\n                \"job\": \"prometheus\",\n                \"service\": \"bar\"\n            },\n            \"exemplars\": [\n                {\n                    \"labels\": {\n                        \"trace_id\": \"EpTxMJ40fUus7aGY\"\n                    },\n                    \"value\": \"6\",\n                    \"timestamp\": 1600096945.479\n                }\n            ]\n        },\n        {\n            \"seriesLabels\": {\n                \"__name__\": \"test_exemplar_metric_total\",\n                \"instance\": \"localhost:8090\",\n                \"job\": \"prometheus\",\n                \"service\": \"foo\"\n            },\n            \"exemplars\": [\n                {\n                    \"labels\": {\n                        \"trace_id\": \"Olp9XHlq763ccsfa\"\n                    },\n                    \"value\": \"19\",\n                    \"timestamp\": 1600096955.479\n                },\n                {\n                    \"labels\": {\n                        \"trace_id\": \"hCtjygkIHwAN9vs4\"\n                    },\n                    \"value\": \"20\",\n                    \"timestamp\": 1600096965.489\n                }\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Local Development Server for Prometheus UI\nDESCRIPTION: Command to start a development server for the React UI outside of a running Prometheus server. This serves the UI on http://localhost:5173/ with hot-reload capabilities for code edits.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Tracking RPC Request Counts in Prometheus Format\nDESCRIPTION: Defines counter metrics for tracking the total number of RPC requests received across different services (exponential, normal, uniform). Each service has both a total count and creation timestamp.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.5mfs.om.txt#2025-04-16_snippet_4\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP rpc_requests Total number of RPC requests received.\n# TYPE rpc_requests counter\nrpc_requests_total{service=\"exponential\"} 22.0\nrpc_requests_created{service=\"exponential\"} 1.726839813016893e+09\nrpc_requests_total{service=\"normal\"} 15.0\nrpc_requests_created{service=\"normal\"} 1.726839813016717e+09\nrpc_requests_total{service=\"uniform\"} 11.0\nrpc_requests_created{service=\"uniform\"} 1.7268398130168471e+09\n# EOF\n```\n\n----------------------------------------\n\nTITLE: Starting Prometheus in Bash\nDESCRIPTION: Command to start Prometheus with a specified configuration file.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/getting_started.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start Prometheus.\n# By default, Prometheus stores its database in ./data (flag --storage.tsdb.path).\n./prometheus --config.file=prometheus.yml\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Matchers for the Same Label\nDESCRIPTION: Shows how to apply multiple filtering conditions to the same label, where all conditions must be satisfied for a time series to be included in the result.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_9\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total{replica!=\"rep-a\",replica=~\"rep.*\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus YAML File\nDESCRIPTION: Basic Prometheus configuration file (prometheus.yml) that sets up global settings and a scrape configuration for Prometheus to monitor itself.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/getting_started.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  scrape_interval:     15s # By default, scrape targets every 15 seconds.\n\n  # Attach these labels to any time series or alerts when communicating with\n  # external systems (federation, remote storage, Alertmanager).\n  external_labels:\n    monitor: 'codelab-monitor'\n\n# A scrape configuration containing exactly one endpoint to scrape:\n# Here it's Prometheus itself.\nscrape_configs:\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n  - job_name: 'prometheus'\n\n    # Override the global default and scrape targets from this job every 5 seconds.\n    scrape_interval: 5s\n\n    static_configs:\n      - targets: ['localhost:9090']\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus HTTP and Kubernetes Service Discovery Metrics\nDESCRIPTION: This snippet shows Prometheus metrics for HTTP service discovery failures and Kubernetes service discovery events and failures. It provides information on the number of events handled for different Kubernetes resources and roles.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_23\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP prometheus_sd_http_failures_total Number of HTTP service discovery refresh failures.\n# TYPE prometheus_sd_http_failures_total counter\nprometheus_sd_http_failures_total 0\n# HELP prometheus_sd_kubernetes_events_total The number of Kubernetes events handled.\n# TYPE prometheus_sd_kubernetes_events_total counter\nprometheus_sd_kubernetes_events_total{event=\"add\",role=\"endpoints\"} 0\nprometheus_sd_kubernetes_events_total{event=\"add\",role=\"endpointslice\"} 0\nprometheus_sd_kubernetes_events_total{event=\"add\",role=\"ingress\"} 0\nprometheus_sd_kubernetes_events_total{event=\"add\",role=\"node\"} 0\nprometheus_sd_kubernetes_events_total{event=\"add\",role=\"pod\"} 0\nprometheus_sd_kubernetes_events_total{event=\"add\",role=\"service\"} 0\nprometheus_sd_kubernetes_events_total{event=\"delete\",role=\"endpoints\"} 0\nprometheus_sd_kubernetes_events_total{event=\"delete\",role=\"endpointslice\"} 0\nprometheus_sd_kubernetes_events_total{event=\"delete\",role=\"ingress\"} 0\nprometheus_sd_kubernetes_events_total{event=\"delete\",role=\"node\"} 0\nprometheus_sd_kubernetes_events_total{event=\"delete\",role=\"pod\"} 0\nprometheus_sd_kubernetes_events_total{event=\"delete\",role=\"service\"} 0\nprometheus_sd_kubernetes_events_total{event=\"update\",role=\"endpoints\"} 0\nprometheus_sd_kubernetes_events_total{event=\"update\",role=\"endpointslice\"} 0\nprometheus_sd_kubernetes_events_total{event=\"update\",role=\"ingress\"} 0\nprometheus_sd_kubernetes_events_total{event=\"update\",role=\"node\"} 0\nprometheus_sd_kubernetes_events_total{event=\"update\",role=\"pod\"} 0\nprometheus_sd_kubernetes_events_total{event=\"update\",role=\"service\"} 0\n# HELP prometheus_sd_kubernetes_failures_total The number of failed WATCH/LIST requests.\n# TYPE prometheus_sd_kubernetes_failures_total counter\nprometheus_sd_kubernetes_failures_total 0\n```\n\n----------------------------------------\n\nTITLE: Example Request for Querying All Metrics for a Specific Target\nDESCRIPTION: Example curl request that retrieves metadata for all metrics from targets matching instance=\"127.0.0.1:9090\" label.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncurl -G http://localhost:9091/api/v1/targets/metadata \\\n    --data-urlencode 'match_target={instance=\"127.0.0.1:9090\"}'\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": [\n    // ...\n    {\n      \"target\": {\n        \"instance\": \"127.0.0.1:9090\",\n        \"job\": \"prometheus\"\n      },\n      \"metric\": \"prometheus_treecache_zookeeper_failures_total\",\n      \"type\": \"counter\",\n      \"help\": \"The total number of ZooKeeper failures.\",\n      \"unit\": \"\"\n    },\n    {\n      \"target\": {\n        \"instance\": \"127.0.0.1:9090\",\n        \"job\": \"prometheus\"\n      },\n      \"metric\": \"prometheus_tsdb_reloads_total\",\n      \"type\": \"counter\",\n      \"help\": \"Number of times the database reloaded block data from disk.\",\n      \"unit\": \"\"\n    },\n    // ...\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing npm Dependencies for Prometheus UI\nDESCRIPTION: Command to install all required npm package dependencies and build the local workspace npm modules for both the new and old Prometheus UI applications.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake ui-build\n```\n\n----------------------------------------\n\nTITLE: Configuring alertmanager_config in Prometheus\nDESCRIPTION: YAML configuration block for alertmanager_config which defines how Prometheus connects to Alertmanager instances. It includes settings for API version, timeout, authentication, and various service discovery mechanisms to dynamically find Alertmanager instances.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_47\n\nLANGUAGE: yaml\nCODE:\n```\n# Per-target Alertmanager timeout when pushing alerts.\n[ timeout: <duration> | default = 10s ]\n\n# The api version of Alertmanager.\n[ api_version: <string> | default = v2 ]\n\n# Prefix for the HTTP path alerts are pushed to.\n[ path_prefix: <path> | default = / ]\n\n# Configures the protocol scheme used for requests.\n[ scheme: <scheme> | default = http ]\n\n# Optionally configures AWS's Signature Verification 4 signing process to sign requests.\n# Cannot be set at the same time as basic_auth, authorization, oauth2, azuread or google_iam.\n# To use the default credentials from the AWS SDK, use `sigv4: {}`.\nsigv4:\n  # The AWS region. If blank, the region from the default credentials chain\n  # is used.\n  [ region: <string> ]\n\n  # The AWS API keys. If blank, the environment variables `AWS_ACCESS_KEY_ID`\n  # and `AWS_SECRET_ACCESS_KEY` are used.\n  [ access_key: <string> ]\n  [ secret_key: <secret> ]\n\n  # Named AWS profile used to authenticate.\n  [ profile: <string> ]\n\n  # AWS Role ARN, an alternative to using AWS API keys.\n  [ role_arn: <string> ]\n\n# HTTP client settings, including authentication methods (such as basic auth and\n# authorization), proxy configurations, TLS options, custom HTTP headers, etc.\n[ <http_config> ]\n\n# List of Azure service discovery configurations.\nazure_sd_configs:\n  [ - <azure_sd_config> ... ]\n\n# List of Consul service discovery configurations.\nconsul_sd_configs:\n  [ - <consul_sd_config> ... ]\n\n# List of DNS service discovery configurations.\ndns_sd_configs:\n  [ - <dns_sd_config> ... ]\n\n# List of EC2 service discovery configurations.\nec2_sd_configs:\n  [ - <ec2_sd_config> ... ]\n\n# List of Eureka service discovery configurations.\neureka_sd_configs:\n  [ - <eureka_sd_config> ... ]\n\n# List of file service discovery configurations.\nfile_sd_configs:\n  [ - <file_sd_config> ... ]\n\n# List of DigitalOcean service discovery configurations.\ndigitalocean_sd_configs:\n  [ - <digitalocean_sd_config> ... ]\n\n# List of Docker service discovery configurations.\ndocker_sd_configs:\n  [ - <docker_sd_config> ... ]\n\n# List of Docker Swarm service discovery configurations.\ndockerswarm_sd_configs:\n  [ - <dockerswarm_sd_config> ... ]\n\n# List of GCE service discovery configurations.\ngce_sd_configs:\n  [ - <gce_sd_config> ... ]\n\n# List of Hetzner service discovery configurations.\nhetzner_sd_configs:\n  [ - <hetzner_sd_config> ... ]\n\n# List of HTTP service discovery configurations.\nhttp_sd_configs:\n  [ - <http_sd_config> ... ]\n\n # List of IONOS service discovery configurations.\nionos_sd_configs:\n  [ - <ionos_sd_config> ... ]\n\n# List of Kubernetes service discovery configurations.\nkubernetes_sd_configs:\n  [ - <kubernetes_sd_config> ... ]\n\n# List of Lightsail service discovery configurations.\nlightsail_sd_configs:\n  [ - <lightsail_sd_config> ... ]\n\n# List of Linode service discovery configurations.\nlinode_sd_configs:\n  [ - <linode_sd_config> ... ]\n\n# List of Marathon service discovery configurations.\nmarathon_sd_configs:\n  [ - <marathon_sd_config> ... ]\n\n# List of AirBnB's Nerve service discovery configurations.\nnerve_sd_configs:\n  [ - <nerve_sd_config> ... ]\n\n# List of Nomad service discovery configurations.\nnomad_sd_configs:\n  [ - <nomad_sd_config> ... ]\n\n# List of OpenStack service discovery configurations.\nopenstack_sd_configs:\n  [ - <openstack_sd_config> ... ]\n\n# List of OVHcloud service discovery configurations.\novhcloud_sd_configs:\n  [ - <ovhcloud_sd_config> ... ]\n\n# List of PuppetDB service discovery configurations.\npuppetdb_sd_configs:\n  [ - <puppetdb_sd_config> ... ]\n\n# List of Scaleway service discovery configurations.\nscaleway_sd_configs:\n  [ - <scaleway_sd_config> ... ]\n\n# List of Zookeeper Serverset service discovery configurations.\nserverset_sd_configs:\n  [ - <serverset_sd_config> ... ]\n\n# List of Triton service discovery configurations.\ntriton_sd_configs:\n  [ - <triton_sd_config> ... ]\n\n# List of Uyuni service discovery configurations.\nuyuni_sd_configs:\n  [ - <uyuni_sd_config> ... ]\n\n# List of Vultr service discovery configurations.\nvultr_sd_configs:\n  [ - <vultr_sd_config> ... ]\n\n# List of labeled statically configured Alertmanagers.\nstatic_configs:\n  [ - <static_config> ... ]\n```\n\n----------------------------------------\n\nTITLE: Formatting Prometheus Query Expression\nDESCRIPTION: Example of using the Prometheus query formatting API endpoint (/api/v1/format_query) to prettify the expression 'foo/bar'. Shows the request and JSON response format.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n$ curl 'http://localhost:9090/api/v1/format_query?query=foo/bar'\n{\n   \"status\" : \"success\",\n   \"data\" : \"foo / bar\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring PuppetDB Service Discovery in YAML\nDESCRIPTION: YAML configuration for PuppetDB service discovery integration, specifying URL, query parameters, and resource inclusion options.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\nurl: <string>\nquery: <string>\n[ include_parameters: <boolean> | default = false ]\n[ refresh_interval: <duration> | default = 60s ]\n[ port: <int> | default = 80 ]\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus using Go tools\nDESCRIPTION: Commands to build and install Prometheus using Go's installation tools. This builds the prometheus and promtool binaries and places them in your GOPATH. Note that this requires running Prometheus from the root of the repository.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nGO111MODULE=on go install github.com/prometheus/prometheus/cmd/...\nprometheus --config.file=your_config.yml\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus HTTP Request Total Counter Metric\nDESCRIPTION: This snippet defines the 'prometheus_http_requests_total' counter metric type for tracking HTTP requests in Prometheus. It includes labels for response code and request handler.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_9\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE prometheus_http_requests_total counter\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Proxy Headers in Prometheus\nDESCRIPTION: Example of configuring HTTP proxy headers in Prometheus configuration.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_10\n\nLANGUAGE: YAML\nCODE:\n```\nproxy_from_environment: true\nno_proxy: \"example.com\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Authentication Options for Prometheus Service Discovery\nDESCRIPTION: Configuration options for token-based authentication in Prometheus service discovery. These options are mutually exclusive and can be used with other authentication mechanisms.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_36\n\nLANGUAGE: yaml\nCODE:\n```\n# Optional authentication information for token-based authentication\n# https://docs.mesosphere.com/1.11/security/ent/iam-api/#passing-an-authentication-token\n# It is mutually exclusive with `auth_token_file` and other authentication mechanisms.\n[ auth_token: <secret> ]\n\n# Optional authentication information for token-based authentication\n# https://docs.mesosphere.com/1.11/security/ent/iam-api/#passing-an-authentication-token\n# It is mutually exclusive with `auth_token` and other authentication mechanisms.\n[ auth_token_file: <filename> ]\n\n# HTTP client settings, including authentication methods (such as basic auth and\n# authorization), proxy configurations, TLS options, custom HTTP headers, etc.\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Invalid Time Duration Examples in PromQL\nDESCRIPTION: Demonstrates syntax that is not supported when specifying time durations in PromQL, including suffixing hexadecimal numbers, combining units with floating point values, or suffixing special values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_4\n\nLANGUAGE: promql\nCODE:\n```\n0xABm # No suffixing of hexadecimal numbers.\n1.5h # Time units cannot be combined with a floating point.\n+Infd # No suffixing of Inf or NaN.\n```\n\n----------------------------------------\n\nTITLE: Creating TSDB Snapshots in Prometheus\nDESCRIPTION: This endpoint creates a snapshot of all current data in the TSDB's data directory. It can optionally skip data that is only present in the head block and not yet compacted to disk. Requires the admin API to be enabled.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_34\n\nLANGUAGE: json\nCODE:\n```\n$ curl -XPOST http://localhost:9090/api/v1/admin/tsdb/snapshot\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"name\": \"20171210T211224Z-2be650b6d019eb54\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Definition in YAML for Prometheus Rule Testing\nDESCRIPTION: Specifies the format for defining expected samples in PromQL test cases, including labels and value.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/unit_testing_rules.md#2025-04-16_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n# Labels of the sample in usual series notation '<metric name>{<label name>=<label value>, ...}'\n# Examples:\n#      series_name{label1=\"value1\", label2=\"value2\"}\n#      go_goroutines{job=\"prometheus\", instance=\"localhost:9090\"}\nlabels: <string>\n\n# The expected value of the PromQL expression.\nvalue: <number>\n```\n\n----------------------------------------\n\nTITLE: Configuring Serverset Service Discovery in Prometheus\nDESCRIPTION: Configuration for Serverset service discovery in Prometheus, which retrieves scrape targets from Serversets stored in Zookeeper. Includes server configuration options and available meta labels.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_39\n\nLANGUAGE: yaml\nCODE:\n```\n# The Zookeeper servers.\nservers:\n  - <host>\n# Paths can point to a single serverset, or the root of a tree of serversets.\npaths:\n  - <string>\n[ timeout: <duration> | default = 10s ]\n```\n\n----------------------------------------\n\nTITLE: Aggregating All Classic Histograms\nDESCRIPTION: Aggregates all classic histograms while maintaining only the le label\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_10\n\nLANGUAGE: promql\nCODE:\n```\nhistogram_quantile(0.9, sum by (le) (rate(http_request_duration_seconds_bucket[10m])))\n```\n\n----------------------------------------\n\nTITLE: Viewing Prometheus HTTP Response Size Metrics\nDESCRIPTION: These metrics show HTTP response size statistics for various endpoints (/status, /targets, /tsdb-status). The data is organized in histogram buckets with corresponding sum and count values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_12\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_response_size_bytes_bucket{handler=\"/status\",le=\"100000\"} 46\nprometheus_http_response_size_bytes_bucket{handler=\"/status\",le=\"1e+06\"} 46\nprometheus_http_response_size_bytes_bucket{handler=\"/status\",le=\"1e+07\"} 46\nprometheus_http_response_size_bytes_bucket{handler=\"/status\",le=\"1e+08\"} 46\nprometheus_http_response_size_bytes_bucket{handler=\"/status\",le=\"1e+09\"} 46\nprometheus_http_response_size_bytes_bucket{handler=\"/status\",le=\"+Inf\"} 46\nprometheus_http_response_size_bytes_sum{handler=\"/status\"} 80592\nprometheus_http_response_size_bytes_count{handler=\"/status\"} 46\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"1000\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"10000\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"100000\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"1e+06\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"1e+07\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"1e+08\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"1e+09\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"+Inf\"} 39\nprometheus_http_response_size_bytes_sum{handler=\"/targets\"} 68328\nprometheus_http_response_size_bytes_count{handler=\"/targets\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"1000\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"10000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"100000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"1e+06\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"1e+07\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"1e+08\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"1e+09\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"+Inf\"} 49\nprometheus_http_response_size_bytes_sum{handler=\"/tsdb-status\"} 85848\nprometheus_http_response_size_bytes_count{handler=\"/tsdb-status\"} 49\n```\n\n----------------------------------------\n\nTITLE: Configuring Metric Name Validation and Native Histogram Settings in Prometheus\nDESCRIPTION: Configuration options for setting metric name validation schemes and native histogram parameters. Includes options for bucket limits, bucket growth factors, and conversion of classic histograms.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n[ metric_name_validation_scheme <string> | default \"utf8\" ]\n\n# Limit on total number of positive and negative buckets allowed in a single\n# native histogram. The resolution of a histogram with more buckets will be\n# reduced until the number of buckets is within the limit. If the limit cannot\n# be reached, the scrape will fail.\n# 0 means no limit.\n[ native_histogram_bucket_limit: <int> | default = 0 ]\n\n# Lower limit for the growth factor of one bucket to the next in each native\n# histogram. The resolution of a histogram with a lower growth factor will be\n# reduced as much as possible until it is within the limit.\n# To set an upper limit for the schema (equivalent to \"scale\" in OTel's\n# exponential histograms), use the following factor limits:\n# \n# +----------------------------+----------------------------+\n# |        growth factor       | resulting schema AKA scale |\n# +----------------------------+----------------------------+\n# |          65536             |             -4             |\n# +----------------------------+----------------------------+\n# |            256             |             -3             |\n# +----------------------------+----------------------------+\n# |             16             |             -2             |\n# +----------------------------+----------------------------+\n# |              4             |             -1             |\n# +----------------------------+----------------------------+\n# |              2             |              0             |\n# +----------------------------+----------------------------+\n# |              1.4           |              1             |\n# +----------------------------+----------------------------+\n# |              1.1           |              2             |\n# +----------------------------+----------------------------+\n# |              1.09          |              3             |\n# +----------------------------+----------------------------+\n# |              1.04          |              4             |\n# +----------------------------+----------------------------+\n# |              1.02          |              5             |\n# +----------------------------+----------------------------+\n# |              1.01          |              6             |\n# +----------------------------+----------------------------+\n# |              1.005         |              7             |\n# +----------------------------+----------------------------+\n# |              1.002         |              8             |\n# +----------------------------+----------------------------+\n# \n# 0 results in the smallest supported factor (which is currently ~1.0027 or\n# schema 8, but might change in the future).\n[ native_histogram_min_bucket_factor: <float> | default = 0 ]\n\n# Specifies whether to convert classic histograms into native histograms with\n# custom buckets (has no effect without --enable-feature=native-histograms).\n[ convert_classic_histograms_to_nhcb <bool> | default =\n<global.convert_classic_histograms_to_nhcb>]\n```\n\n----------------------------------------\n\nTITLE: Displaying Prometheus TSDB Metrics\nDESCRIPTION: Shows detailed Time Series Database (TSDB) metrics for Prometheus including blocks loaded, checkpoint operations, clean start status, and compaction statistics with histograms for chunk range and samples.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_29\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP prometheus_tsdb_blocks_loaded Number of currently loaded data blocks\n# TYPE prometheus_tsdb_blocks_loaded gauge\nprometheus_tsdb_blocks_loaded 17\n# HELP prometheus_tsdb_checkpoint_creations_failed_total Total number of checkpoint creations that failed.\n# TYPE prometheus_tsdb_checkpoint_creations_failed_total counter\nprometheus_tsdb_checkpoint_creations_failed_total 0\n# HELP prometheus_tsdb_checkpoint_creations_total Total number of checkpoint creations attempted.\n# TYPE prometheus_tsdb_checkpoint_creations_total counter\nprometheus_tsdb_checkpoint_creations_total 62\n# HELP prometheus_tsdb_checkpoint_deletions_failed_total Total number of checkpoint deletions that failed.\n# TYPE prometheus_tsdb_checkpoint_deletions_failed_total counter\nprometheus_tsdb_checkpoint_deletions_failed_total 0\n# HELP prometheus_tsdb_checkpoint_deletions_total Total number of checkpoint deletions attempted.\n# TYPE prometheus_tsdb_checkpoint_deletions_total counter\nprometheus_tsdb_checkpoint_deletions_total 62\n# HELP prometheus_tsdb_clean_start -1: lockfile is disabled. 0: a lockfile from a previous execution was replaced. 1: lockfile creation was clean\n# TYPE prometheus_tsdb_clean_start gauge\nprometheus_tsdb_clean_start 1\n# HELP prometheus_tsdb_compaction_chunk_range_seconds Final time range of chunks on their first compaction\n# TYPE prometheus_tsdb_compaction_chunk_range_seconds histogram\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"100\"} 86\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"400\"} 86\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"1600\"} 86\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"6400\"} 86\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"25600\"} 676\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"102400\"} 1555\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"409600\"} 2379\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"1.6384e+06\"} 34068\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"6.5536e+06\"} 4.819758e+06\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"2.62144e+07\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"+Inf\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_range_seconds_sum 8.917524773366e+12\nprometheus_tsdb_compaction_chunk_range_seconds_count 4.861956e+06\n# HELP prometheus_tsdb_compaction_chunk_samples Final number of samples on their first compaction\n# TYPE prometheus_tsdb_compaction_chunk_samples histogram\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"4\"} 1407\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"6\"} 1544\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"9\"} 1881\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"13.5\"} 2065\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"20.25\"} 2782\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"30.375\"} 4342\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"45.5625\"} 6180\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"68.34375\"} 11518\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"102.515625\"} 13890\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"153.7734375\"} 4.810155e+06\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"230.66015625\"} 4.86058e+06\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"345.990234375\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"+Inf\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_samples_sum 5.86000708e+08\nprometheus_tsdb_compaction_chunk_samples_count 4.861956e+06\n# HELP prometheus_tsdb_compaction_chunk_size_bytes Final size of chunks on their first compaction\n```\n\n----------------------------------------\n\nTITLE: Running with Graphite Backend\nDESCRIPTION: Command to run the adapter with Graphite as the remote storage backend, specifying the Graphite server address.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/remote_storage/remote_storage_adapter/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./remote_storage_adapter --graphite-address=localhost:8080\n```\n\n----------------------------------------\n\nTITLE: Defining Global Configuration in YAML for Prometheus\nDESCRIPTION: This YAML snippet defines the global configuration section for Prometheus. It includes settings for scrape intervals, timeouts, evaluation intervals, external labels, and various limits for scraping and labeling.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  # How frequently to scrape targets by default.\n  [ scrape_interval: <duration> | default = 1m ]\n\n  # How long until a scrape request times out.\n  # It cannot be greater than the scrape interval.\n  [ scrape_timeout: <duration> | default = 10s ]\n\n  # The protocols to negotiate during a scrape with the client.\n  # Supported values (case sensitive): PrometheusProto, OpenMetricsText0.0.1,\n  # OpenMetricsText1.0.0, PrometheusText0.0.4.\n  # The default value changes to [ PrometheusProto, OpenMetricsText1.0.0, OpenMetricsText0.0.1, PrometheusText0.0.4 ]\n  # when native_histogram feature flag is set.\n  [ scrape_protocols: [<string>, ...] | default = [ OpenMetricsText1.0.0, OpenMetricsText0.0.1, PrometheusText0.0.4 ] ]\n\n  # How frequently to evaluate rules.\n  [ evaluation_interval: <duration> | default = 1m ]\n\n  # Offset the rule evaluation timestamp of this particular group by the\n  # specified duration into the past to ensure the underlying metrics have\n  # been received. Metric availability delays are more likely to occur when\n  # Prometheus is running as a remote write target, but can also occur when\n  # there's anomalies with scraping.\n  [ rule_query_offset: <duration> | default = 0s ]\n\n  # The labels to add to any time series or alerts when communicating with\n  # external systems (federation, remote storage, Alertmanager). \n  # Environment variable references `${var}` or `$var` are replaced according \n  # to the values of the current environment variables. \n  # References to undefined variables are replaced by the empty string.\n  # The `$` character can be escaped by using `$$`.\n  external_labels:\n    [ <labelname>: <labelvalue> ... ]\n\n  # File to which PromQL queries are logged.\n  # Reloading the configuration will reopen the file.\n  [ query_log_file: <string> ]\n\n  # File to which scrape failures are logged.\n  # Reloading the configuration will reopen the file.\n  [ scrape_failure_log_file: <string> ]\n\n  # An uncompressed response body larger than this many bytes will cause the\n  # scrape to fail. 0 means no limit. Example: 100MB.\n  # This is an experimental feature, this behaviour could\n  # change or be removed in the future.\n  [ body_size_limit: <size> | default = 0 ]\n\n  # Per-scrape limit on the number of scraped samples that will be accepted.\n  # If more than this number of samples are present after metric relabeling\n  # the entire scrape will be treated as failed. 0 means no limit.\n  [ sample_limit: <int> | default = 0 ]\n\n  # Limit on the number of labels that will be accepted per sample. If more\n  # than this number of labels are present on any sample post metric-relabeling,\n  # the entire scrape will be treated as failed. 0 means no limit.\n  [ label_limit: <int> | default = 0 ]\n\n  # Limit on the length (in bytes) of each individual label name. If any label\n  # name in a scrape is longer than this number post metric-relabeling, the\n  # entire scrape will be treated as failed. Note that label names are UTF-8\n  # encoded, and characters can take up to 4 bytes. 0 means no limit.\n  [ label_name_length_limit: <int> | default = 0 ]\n\n  # Limit on the length (in bytes) of each individual label value. If any label\n  # value in a scrape is longer than this number post metric-relabeling, the\n  # entire scrape will be treated as failed. Note that label values are UTF-8\n  # encoded, and characters can take up to 4 bytes. 0 means no limit.\n  [ label_value_length_limit: <int> | default = 0 ]\n\n  # Limit per scrape config on number of unique targets that will be\n  # accepted. If more than this number of targets are present after target\n  # relabeling, Prometheus will mark the targets as failed without scraping them.\n  # 0 means no limit. This is an experimental feature, this behaviour could\n  # change in the future.\n  [ target_limit: <int> | default = 0 ]\n\n  # Limit per scrape config on the number of targets dropped by relabeling\n  # that will be kept in memory. 0 means no limit.\n  [ keep_dropped_targets: <int> | default = 0 ]\n\n  # Specifies the validation scheme for metric and label names. Either blank or\n  # \"utf8\" for full UTF-8 support, or \"legacy\" for letters, numbers, colons,\n  # and underscores.\n  [ metric_name_validation_scheme <string> | default \"utf8\" ]\n\n  # Specifies whether to convert all scraped classic histograms into native\n  # histograms with custom buckets.\n  [ convert_classic_histograms_to_nhcb <bool> | default = false]\n```\n\n----------------------------------------\n\nTITLE: Reusable Template Definitions in Go\nDESCRIPTION: Examples of defining and using reusable templates in Prometheus. Shows both single-argument and multi-argument template definitions using the args function.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/template_examples.md#2025-04-16_snippet_5\n\nLANGUAGE: go\nCODE:\n```\n{{/* Define the template */}}\n{{define \"myTemplate\"}}\n  do something\n{{end}}\n\n{{/* Use the template */}}\n{{template \"myTemplate\"}}\n\n{{define \"myMultiArgTemplate\"}}\n  First argument: {{.arg0}}\n  Second argument: {{.arg1}}\n{{end}}\n{{template \"myMultiArgTemplate\" (args 1 2)}}\n```\n\n----------------------------------------\n\nTITLE: Setting Cache Max Age for Prometheus Data\nDESCRIPTION: Configuration to set the maximum time that metrics and labels fetched from Prometheus are stored in the client-side cache.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nconst promQL = new PromQLExtension().setComplete({remote: {cache: {maxAge: 5 * 60 * 1000}}})\n```\n\n----------------------------------------\n\nTITLE: Configuring Hetzner Service Discovery in Prometheus\nDESCRIPTION: Configuration schema for Hetzner SD settings, allowing Prometheus to discover scrape targets from Hetzner Cloud and Robot APIs. Includes role selection, port configuration, refresh interval and HTTP client settings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_28\n\nLANGUAGE: yaml\nCODE:\n```\n# The Hetzner role of entities that should be discovered.\n# One of robot or hcloud.\nrole: <string>\n\n# The port to scrape metrics from.\n[ port: <int> | default = 80 ]\n\n# The time after which the servers are refreshed.\n[ refresh_interval: <duration> | default = 60s ]\n\n# HTTP client settings, including authentication methods (such as basic auth and\n# authorization), proxy configurations, TLS options, custom HTTP headers, etc.\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Filtering by Scrape Pool\nDESCRIPTION: Example of filtering targets using the scrapePool parameter to show targets from a specific scrape pool.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"activeTargets\": [\n      {\n        \"discoveredLabels\": {\n          \"__address__\": \"127.0.0.1:9091\",\n          \"__metrics_path__\": \"/metrics\",\n          \"__scheme__\": \"http\",\n          \"job\": \"node_exporter\"\n        },\n        \"labels\": {\n          \"instance\": \"127.0.0.1:9091\",\n          \"job\": \"node_exporter\"\n        },\n        \"scrapePool\": \"node_exporter\",\n        \"scrapeUrl\": \"http://127.0.0.1:9091/metrics\",\n        \"globalUrl\": \"http://example-prometheus:9091/metrics\",\n        \"lastError\": \"\",\n        \"lastScrape\": \"2017-01-17T15:07:44.723715405+01:00\",\n        \"lastScrapeDuration\": 50688943,\n        \"health\": \"up\"\n      }\n    ],\n    \"droppedTargets\": []\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Metrics\nDESCRIPTION: This snippet shows a collection of Prometheus metrics with their current values. It includes metrics related to TSDB operations, compactions, WAL management, and web server statistics. These metrics are used for monitoring and analyzing the performance of a Prometheus server.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_18\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_target_sync_length_seconds_count{scrape_job=\"random\"} 2953\nprometheus_template_text_expansion_failures_total 0\nprometheus_template_text_expansions_total 2.126277e+06\nprometheus_treecache_watcher_goroutines 0\nprometheus_treecache_zookeeper_failures_total 0\nprometheus_tsdb_blocks_loaded 17\nprometheus_tsdb_checkpoint_creations_failed_total 0\nprometheus_tsdb_checkpoint_creations_total 62\nprometheus_tsdb_checkpoint_deletions_failed_total 0\nprometheus_tsdb_checkpoint_deletions_total 62\nprometheus_tsdb_clean_start 1\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"100\"} 86\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"400\"} 86\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"1600\"} 86\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"6400\"} 86\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"25600\"} 676\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"102400\"} 1555\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"409600\"} 2379\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"1.6384e+06\"} 34068\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"6.5536e+06\"} 4.819758e+06\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"2.62144e+07\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"+Inf\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_range_seconds_sum 8.917524773366e+12\nprometheus_tsdb_compaction_chunk_range_seconds_count 4.861956e+06\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"4\"} 1407\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"6\"} 1544\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"9\"} 1881\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"13.5\"} 2065\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"20.25\"} 2782\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"30.375\"} 4342\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"45.5625\"} 6180\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"68.34375\"} 11518\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"102.515625\"} 13890\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"153.7734375\"} 4.810155e+06\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"230.66015625\"} 4.86058e+06\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"345.990234375\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_samples_bucket{le=\"+Inf\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_samples_sum 5.86000708e+08\nprometheus_tsdb_compaction_chunk_samples_count 4.861956e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"32\"} 1233\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"48\"} 156238\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"72\"} 2.006456e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"108\"} 3.568405e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"162\"} 3.835144e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"243\"} 4.034591e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"364.5\"} 4.505646e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"546.75\"} 4.69694e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"820.125\"} 4.78551e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"1230.1875\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"1845.28125\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"2767.921875\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"+Inf\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_size_bytes_sum 6.81852972e+08\nprometheus_tsdb_compaction_chunk_size_bytes_count 4.861956e+06\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"1\"} 61\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"2\"} 155\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"4\"} 180\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"8\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"16\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"32\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"64\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"128\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"256\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"512\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"1024\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"2048\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"4096\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"8192\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"+Inf\"} 183\nprometheus_tsdb_compaction_duration_seconds_sum 254.9095696899999\nprometheus_tsdb_compaction_duration_seconds_count 183\nprometheus_tsdb_compaction_populating_block 0\nprometheus_tsdb_compactions_failed_total 0\nprometheus_tsdb_compactions_skipped_total 0\nprometheus_tsdb_compactions_total 183\nprometheus_tsdb_compactions_triggered_total 14855\nprometheus_tsdb_data_replay_duration_seconds 5.550163617\nprometheus_tsdb_exemplar_exemplars_appended_total 0\nprometheus_tsdb_exemplar_exemplars_in_storage 0\nprometheus_tsdb_exemplar_last_exemplars_timestamp_seconds 0\nprometheus_tsdb_exemplar_max_exemplars 0\nprometheus_tsdb_exemplar_out_of_order_exemplars_total 0\nprometheus_tsdb_exemplar_series_with_exemplars_in_storage 0\nprometheus_tsdb_head_active_appenders 0\nprometheus_tsdb_head_chunks 31476\nprometheus_tsdb_head_chunks_created_total 4.893432e+06\nprometheus_tsdb_head_chunks_removed_total 4.861956e+06\nprometheus_tsdb_head_chunks_storage_size_bytes 7.237299e+06\nprometheus_tsdb_head_gc_duration_seconds_sum 4.773801686000001\nprometheus_tsdb_head_gc_duration_seconds_count 123\nprometheus_tsdb_head_max_time 1.738949375191e+12\nprometheus_tsdb_head_max_time_seconds 1.738949375e+09\nprometheus_tsdb_head_min_time 1.738944000171e+12\nprometheus_tsdb_head_min_time_seconds 1.738944e+09\nprometheus_tsdb_head_out_of_order_samples_appended_total{type=\"float\"} 0\nprometheus_tsdb_head_out_of_order_samples_appended_total{type=\"histogram\"} 0\nprometheus_tsdb_head_samples_appended_total{type=\"float\"} 5.85543187e+08\nprometheus_tsdb_head_samples_appended_total{type=\"histogram\"} 0\nprometheus_tsdb_head_series 10720\nprometheus_tsdb_head_series_created_total 18541\nprometheus_tsdb_head_series_not_found_total 0\nprometheus_tsdb_head_series_removed_total 7821\nprometheus_tsdb_head_truncations_failed_total 0\nprometheus_tsdb_head_truncations_total 123\nprometheus_tsdb_isolation_high_watermark 7.852949e+06\nprometheus_tsdb_isolation_low_watermark 7.852949e+06\nprometheus_tsdb_lowest_timestamp 1.73618640004e+12\nprometheus_tsdb_lowest_timestamp_seconds 1.7361864e+09\nprometheus_tsdb_mmap_chunk_corruptions_total 0\nprometheus_tsdb_mmap_chunks_total 4.851264e+06\nprometheus_tsdb_out_of_bound_samples_total{type=\"float\"} 0\nprometheus_tsdb_out_of_order_samples_total{type=\"float\"} 517\nprometheus_tsdb_out_of_order_samples_total{type=\"histogram\"} 0\nprometheus_tsdb_reloads_failures_total 0\nprometheus_tsdb_reloads_total 14822\nprometheus_tsdb_retention_limit_bytes 0\nprometheus_tsdb_retention_limit_seconds 2.6784e+06\nprometheus_tsdb_size_retentions_total 0\nprometheus_tsdb_snapshot_replay_error_total 0\nprometheus_tsdb_storage_blocks_bytes 2.762863592e+09\nprometheus_tsdb_symbol_table_size_bytes 10616\nprometheus_tsdb_time_retentions_total 5\nprometheus_tsdb_tombstone_cleanup_seconds_bucket{le=\"+Inf\"} 0\nprometheus_tsdb_tombstone_cleanup_seconds_sum 0\nprometheus_tsdb_tombstone_cleanup_seconds_count 0\nprometheus_tsdb_too_old_samples_total{type=\"float\"} 0\nprometheus_tsdb_vertical_compactions_total 0\nprometheus_tsdb_wal_completed_pages_total 109271\nprometheus_tsdb_wal_corruptions_total 0\nprometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.5\"} NaN\nprometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.9\"} NaN\nprometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.99\"} NaN\nprometheus_tsdb_wal_fsync_duration_seconds_sum 4.842524568000002\nprometheus_tsdb_wal_fsync_duration_seconds_count 123\nprometheus_tsdb_wal_page_flushes_total 2.293951e+06\nprometheus_tsdb_wal_segment_current 24726\nprometheus_tsdb_wal_storage_size_bytes 6.9168385e+07\nprometheus_tsdb_wal_truncate_duration_seconds_sum 121.61954577099996\nprometheus_tsdb_wal_truncate_duration_seconds_count 62\nprometheus_tsdb_wal_truncations_failed_total 0\nprometheus_tsdb_wal_truncations_total 62\nprometheus_tsdb_wal_writes_failed_total 0\nprometheus_web_federation_errors_total 0\nprometheus_web_federation_warnings_total 0\npromhttp_metric_handler_requests_in_flight 1\npromhttp_metric_handler_requests_total{code=\"200\"} 4.059092e+06\npromhttp_metric_handler_requests_total{code=\"500\"} 0\npromhttp_metric_handler_requests_total{code=\"503\"} 0\n```\n\n----------------------------------------\n\nTITLE: Starting Node Exporter Instances in Bash\nDESCRIPTION: Commands to extract and start multiple instances of Node Exporter as sample targets for Prometheus.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/getting_started.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntar -xzvf node_exporter-*.*.tar.gz\ncd node_exporter-*.*\n\n# Start 3 example targets in separate terminals:\n./node_exporter --web.listen-address 127.0.0.1:8080\n./node_exporter --web.listen-address 127.0.0.1:8081\n./node_exporter --web.listen-address 127.0.0.1:8082\n```\n\n----------------------------------------\n\nTITLE: Displaying Go Garbage Collection Metrics in Prometheus Format\nDESCRIPTION: This snippet shows various Prometheus metrics related to Go garbage collection, including cycle counts, durations, heap allocations, and memory usage. It provides detailed information about the Go runtime's memory management and performance.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP go_gc_cycles_automatic_gc_cycles_total Count of completed GC cycles generated by the Go runtime. Sourced from /gc/cycles/automatic:gc-cycles\n# TYPE go_gc_cycles_automatic_gc_cycles_total counter\ngo_gc_cycles_automatic_gc_cycles_total 190932\n# HELP go_gc_cycles_forced_gc_cycles_total Count of completed GC cycles forced by the application. Sourced from /gc/cycles/forced:gc-cycles\n# TYPE go_gc_cycles_forced_gc_cycles_total counter\ngo_gc_cycles_forced_gc_cycles_total 0\n# HELP go_gc_cycles_total_gc_cycles_total Count of all completed GC cycles. Sourced from /gc/cycles/total:gc-cycles\n# TYPE go_gc_cycles_total_gc_cycles_total counter\ngo_gc_cycles_total_gc_cycles_total 190932\n# HELP go_gc_duration_seconds A summary of the wall-time pause (stop-the-world) duration in garbage collection cycles.\n# TYPE go_gc_duration_seconds summary\ngo_gc_duration_seconds{quantile=\"0\"} 7.6238e-05\ngo_gc_duration_seconds{quantile=\"0.25\"} 0.00010188\ngo_gc_duration_seconds{quantile=\"0.5\"} 0.000135819\ngo_gc_duration_seconds{quantile=\"0.75\"} 0.000156061\ngo_gc_duration_seconds{quantile=\"1\"} 0.002389378\ngo_gc_duration_seconds_sum 44.985611511\ngo_gc_duration_seconds_count 190932\n# HELP go_gc_gogc_percent Heap size target percentage configured by the user, otherwise 100. This value is set by the GOGC environment variable, and the runtime/debug.SetGCPercent function. Sourced from /gc/gogc:percent\n# TYPE go_gc_gogc_percent gauge\ngo_gc_gogc_percent 75\n# HELP go_gc_gomemlimit_bytes Go runtime memory limit configured by the user, otherwise math.MaxInt64. This value is set by the GOMEMLIMIT environment variable, and the runtime/debug.SetMemoryLimit function. Sourced from /gc/gomemlimit:bytes\n# TYPE go_gc_gomemlimit_bytes gauge\ngo_gc_gomemlimit_bytes 9.03676723e+08\n# HELP go_gc_heap_allocs_by_size_bytes Distribution of heap allocations by approximate size. Bucket counts increase monotonically. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks. Sourced from /gc/heap/allocs-by-size:bytes\n# TYPE go_gc_heap_allocs_by_size_bytes histogram\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"8.999999999999998\"} 2.279966416e+09\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"24.999999999999996\"} 1.9429106442e+10\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"64.99999999999999\"} 3.2158220609e+10\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"144.99999999999997\"} 4.2309744198e+10\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"320.99999999999994\"} 4.3418919481e+10\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"704.9999999999999\"} 4.374631622e+10\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"1536.9999999999998\"} 4.3917578245e+10\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"3200.9999999999995\"} 4.396605609e+10\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"6528.999999999999\"} 4.4007501305e+10\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"13568.999999999998\"} 4.4020325917e+10\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"27264.999999999996\"} 4.4036187548e+10\ngo_gc_heap_allocs_by_size_bytes_bucket{le=\"+Inf\"} 4.4046288803e+10\ngo_gc_heap_allocs_by_size_bytes_sum 5.937322571024e+12\ngo_gc_heap_allocs_by_size_bytes_count 4.4046288803e+10\n# HELP go_gc_heap_allocs_bytes_total Cumulative sum of memory allocated to the heap by the application. Sourced from /gc/heap/allocs:bytes\n# TYPE go_gc_heap_allocs_bytes_total counter\ngo_gc_heap_allocs_bytes_total 5.937322571024e+12\n# HELP go_gc_heap_allocs_objects_total Cumulative count of heap allocations triggered by the application. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks. Sourced from /gc/heap/allocs:objects\n# TYPE go_gc_heap_allocs_objects_total counter\ngo_gc_heap_allocs_objects_total 4.4046288803e+10\n# HELP go_gc_heap_frees_by_size_bytes Distribution of freed heap allocations by approximate size. Bucket counts increase monotonically. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks. Sourced from /gc/heap/frees-by-size:bytes\n# TYPE go_gc_heap_frees_by_size_bytes histogram\ngo_gc_heap_frees_by_size_bytes_bucket{le=\"8.999999999999998\"} 2.279951045e+09\ngo_gc_heap_frees_by_size_bytes_bucket{le=\"24.999999999999996\"} 1.9428965693e+10\ngo_gc_heap_frees_by_size_bytes_bucket{le=\"64.99999999999999\"} 3.2157849717e+10\ngo_gc_heap_frees_by_size_bytes_bucket{le=\"144.99999999999997\"} 4.2309204178e+10\ngo_gc_heap_frees_by_size_bytes_bucket{le=\"320.99999999999994\"} 4.3418348856e+10\ngo_gc_heap_frees_by_size_bytes_bucket{le=\"704.9999999999999\"} 4.3745739652e+10\ngo_gc_heap_frees_by_size_bytes_bucket{le=\"1536.9999999999998\"} 4.3916999773e+10\ngo_gc_heap_frees_by_size_bytes_bucket{le=\"3200.9999999999995\"} 4.3965477112e+10\ngo_gc_heap_frees_by_size_bytes_bucket{le=\"6528.999999999999\"} 4.4006921621e+10\ngo_gc_heap_frees_by_size_bytes_bucket{le=\"13568.999999999998\"} 4.4019746017e+10\ngo_gc_heap_frees_by_size_bytes_bucket{le=\"27264.999999999996\"} 4.4035607466e+10\ngo_gc_heap_frees_by_size_bytes_bucket{le=\"+Inf\"} 4.4045708586e+10\ngo_gc_heap_frees_by_size_bytes_sum 5.937239047736e+12\ngo_gc_heap_frees_by_size_bytes_count 4.4045708586e+10\n# HELP go_gc_heap_frees_bytes_total Cumulative sum of heap memory freed by the garbage collector. Sourced from /gc/heap/frees:bytes\n# TYPE go_gc_heap_frees_bytes_total counter\ngo_gc_heap_frees_bytes_total 5.937239047736e+12\n# HELP go_gc_heap_frees_objects_total Cumulative count of heap allocations whose storage was freed by the garbage collector. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks. Sourced from /gc/heap/frees:objects\n# TYPE go_gc_heap_frees_objects_total counter\ngo_gc_heap_frees_objects_total 4.4045708586e+10\n# HELP go_gc_heap_goal_bytes Heap size target for the end of the GC cycle. Sourced from /gc/heap/goal:bytes\n# TYPE go_gc_heap_goal_bytes gauge\ngo_gc_heap_goal_bytes 1.0365948e+08\n# HELP go_gc_heap_live_bytes Heap memory occupied by live objects that were marked by the previous GC. Sourced from /gc/heap/live:bytes\n# TYPE go_gc_heap_live_bytes gauge\ngo_gc_heap_live_bytes 5.8791024e+07\n# HELP go_gc_heap_objects_objects Number of objects, live or unswept, occupying heap memory. Sourced from /gc/heap/objects:objects\n# TYPE go_gc_heap_objects_objects gauge\ngo_gc_heap_objects_objects 580217\n# HELP go_gc_heap_tiny_allocs_objects_total Count of small allocations that are packed together into blocks. These allocations are counted separately from other allocations because each individual allocation is not tracked by the runtime, only their block. Each block is already accounted for in allocs-by-size and frees-by-size. Sourced from /gc/heap/tiny/allocs:objects\n# TYPE go_gc_heap_tiny_allocs_objects_total counter\ngo_gc_heap_tiny_allocs_objects_total 8.306064403e+09\n# HELP go_gc_limiter_last_enabled_gc_cycle GC cycle the last time the GC CPU limiter was enabled. This metric is useful for diagnosing the root cause of an out-of-memory error, because the limiter trades memory for CPU time when the GC's CPU time gets too high. This is most likely to occur with use of SetMemoryLimit. The first GC cycle is cycle 1, so a value of 0 indicates that it was never enabled. Sourced from /gc/limiter/last-enabled:gc-cycle\n# TYPE go_gc_limiter_last_enabled_gc_cycle gauge\ngo_gc_limiter_last_enabled_gc_cycle 160896\n# HELP go_gc_pauses_seconds Deprecated. Prefer the identical /sched/pauses/total/gc:seconds. Sourced from /gc/pauses:seconds\n# TYPE go_gc_pauses_seconds histogram\ngo_gc_pauses_seconds_bucket{le=\"6.399999999999999e-08\"} 0\ngo_gc_pauses_seconds_bucket{le=\"6.399999999999999e-07\"} 0\ngo_gc_pauses_seconds_bucket{le=\"7.167999999999999e-06\"} 137212\ngo_gc_pauses_seconds_bucket{le=\"8.191999999999999e-05\"} 208425\ngo_gc_pauses_seconds_bucket{le=\"0.0009175039999999999\"} 376121\ngo_gc_pauses_seconds_bucket{le=\"0.010485759999999998\"} 381798\ngo_gc_pauses_seconds_bucket{le=\"0.11744051199999998\"} 381863\ngo_gc_pauses_seconds_bucket{le=\"+Inf\"} 381864\ngo_gc_pauses_seconds_sum 20.343611904000003\ngo_gc_pauses_seconds_count 381864\n# HELP go_gc_scan_globals_bytes The total amount of global variable space that is scannable. Sourced from /gc/scan/globals:bytes\n# TYPE go_gc_scan_globals_bytes gauge\ngo_gc_scan_globals_bytes 555824\n# HELP go_gc_scan_heap_bytes The total amount of heap space that is scannable. Sourced from /gc/scan/heap:bytes\n# TYPE go_gc_scan_heap_bytes gauge\ngo_gc_scan_heap_bytes 4.0986192e+07\n# HELP go_gc_scan_stack_bytes The number of bytes of stack that were scanned last GC cycle. Sourced from /gc/scan/stack:bytes\n# TYPE go_gc_scan_stack_bytes gauge\ngo_gc_scan_stack_bytes 477760\n# HELP go_gc_scan_total_bytes The total amount space that is scannable. Sum of all metrics in /gc/scan. Sourced from /gc/scan/total:bytes\n# TYPE go_gc_scan_total_bytes gauge\ngo_gc_scan_total_bytes 4.2019776e+07\n# HELP go_gc_stack_starting_size_bytes The stack size of new goroutines. Sourced from /gc/stack/starting-size:bytes\n# TYPE go_gc_stack_starting_size_bytes gauge\ngo_gc_stack_starting_size_bytes 4096\n# HELP go_goroutines Number of goroutines that currently exist.\n# TYPE go_goroutines gauge\ngo_goroutines 151\n# HELP go_info Information about the Go environment.\n# TYPE go_info gauge\ngo_info{version=\"go1.23.4\"} 1\n# HELP go_memstats_alloc_bytes Number of bytes allocated in heap and currently in use. Equals to /memory/classes/heap/objects:bytes.\n# TYPE go_memstats_alloc_bytes gauge\ngo_memstats_alloc_bytes 8.3523288e+07\n# HELP go_memstats_alloc_bytes_total Total number of bytes allocated in heap until now, even if released already. Equals to /gc/heap/allocs:bytes.\n# TYPE go_memstats_alloc_bytes_total counter\ngo_memstats_alloc_bytes_total 5.937322571024e+12\n# HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table. Equals to /memory/classes/profiling/buckets:bytes.\n# TYPE go_memstats_buck_hash_sys_bytes gauge\ngo_memstats_buck_hash_sys_bytes 9.35076e+06\n```\n\n----------------------------------------\n\nTITLE: Example of Range Query with Warning and Info Expectations\nDESCRIPTION: Demonstrates evaluating a range query with specific warning message and info regex matching, along with expected result series over time.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/promql/promqltest/README.md#2025-04-16_snippet_6\n\nLANGUAGE: promql\nCODE:\n```\neval range from 0 to 3m step 1m sum by (env) (my_metric)\n    expect warn msg something went wrong\n    expect info regex something went (wrong|boom)\n    {env=\"prod\"} 2 5 10 20\n    {env=\"test\"} 10 20 30 45\n```\n\n----------------------------------------\n\nTITLE: Defining Rule Files and Scrape Configs in YAML for Prometheus\nDESCRIPTION: This YAML snippet shows how to specify rule files and scrape configuration files for Prometheus. It allows for glob patterns to match multiple files.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# Rule files specifies a list of globs. Rules and alerts are read from\n# all matching files.\nrule_files:\n  [ - <filepath_glob> ... ]\n\n# Scrape config files specifies a list of globs. Scrape configs are read from\n# all matching files and appended to the list of scrape configs.\nscrape_config_files:\n  [ - <filepath_glob> ... ]\n\n# A list of scrape configurations.\nscrape_configs:\n  [ - <scrape_config> ... ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Selectors for Prometheus Service Discovery in YAML\nDESCRIPTION: This YAML snippet defines the selector configuration for Kubernetes service discovery in Prometheus. It allows specifying role-based selectors with optional label and field selectors.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_33\n\nLANGUAGE: yaml\nCODE:\n```\n[ selectors:\n  [ - role: <string>\n    [ label: <string> ]\n    [ field: <string> ] ]]\n```\n\n----------------------------------------\n\nTITLE: Basic Rule Group Structure in YAML\nDESCRIPTION: Basic YAML structure for defining rule groups in Prometheus.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/recording_rules.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ngroups:\n  [ - <rule_group> ]\n```\n\n----------------------------------------\n\nTITLE: Configuring OVHcloud Service Discovery in YAML\nDESCRIPTION: YAML configuration for OVHcloud service discovery integration with Prometheus, including authentication and service selection options.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\napplication_key: <string>\napplication_secret: <secret>\nconsumer_key: <secret>\nservice: <string>\n[ endpoint: <string> | default = \"ovh-eu\" ]\n[ refresh_interval: <duration> | default = 60s ]\n```\n\n----------------------------------------\n\nTITLE: Invalid and Valid Empty Matcher Examples\nDESCRIPTION: Demonstrates illegal syntax with only empty matchers, and shows valid alternatives that include at least one non-empty matcher or use regex patterns that won't match empty values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_10\n\nLANGUAGE: promql\nCODE:\n```\n{job=~\".*\"} # Bad!\n```\n\n----------------------------------------\n\nTITLE: PromQL Numeric and String Literals\nDESCRIPTION: Examples of different literal types in PromQL including numeric values and string literals with different quote styles (double, single, and backtick quotes).\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/test/expression.txt#2025-04-16_snippet_0\n\nLANGUAGE: PromQL\nCODE:\n```\n0.123e3\n\"test string\"\n'test string'\n`test string`\n`test\n\nstring`\n```\n\n----------------------------------------\n\nTITLE: Configuring Runtime Settings in YAML for Prometheus\nDESCRIPTION: This YAML snippet defines the runtime configuration section for Prometheus. It includes settings for the Go garbage collector.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nruntime:\n  # Configure the Go garbage collector GOGC parameter\n  # See: https://tip.golang.org/doc/gc-guide#GOGC\n  # Lowering this number increases CPU usage.\n  [ gogc: <int> | default = 75 ]\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Kuma and Other Service Discovery Metrics\nDESCRIPTION: This snippet displays Prometheus metrics for Kuma service discovery, Linode, Nomad, and general service discovery updates. It includes information on fetch durations, failures, and update events.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_24\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP prometheus_sd_kuma_fetch_duration_seconds The duration of a Kuma MADS fetch call.\n# TYPE prometheus_sd_kuma_fetch_duration_seconds summary\nprometheus_sd_kuma_fetch_duration_seconds{quantile=\"0.5\"} NaN\nprometheus_sd_kuma_fetch_duration_seconds{quantile=\"0.9\"} NaN\nprometheus_sd_kuma_fetch_duration_seconds{quantile=\"0.99\"} NaN\nprometheus_sd_kuma_fetch_duration_seconds_sum 0\nprometheus_sd_kuma_fetch_duration_seconds_count 0\n# HELP prometheus_sd_kuma_fetch_failures_total The number of Kuma MADS fetch call failures.\n# TYPE prometheus_sd_kuma_fetch_failures_total counter\nprometheus_sd_kuma_fetch_failures_total 0\n# HELP prometheus_sd_kuma_fetch_skipped_updates_total The number of Kuma MADS fetch calls that result in no updates to the targets.\n# TYPE prometheus_sd_kuma_fetch_skipped_updates_total counter\nprometheus_sd_kuma_fetch_skipped_updates_total 0\n# HELP prometheus_sd_linode_failures_total Number of Linode service discovery refresh failures.\n# TYPE prometheus_sd_linode_failures_total counter\nprometheus_sd_linode_failures_total 0\n# HELP prometheus_sd_nomad_failures_total Number of nomad service discovery refresh failures.\n# TYPE prometheus_sd_nomad_failures_total counter\nprometheus_sd_nomad_failures_total 0\n# HELP prometheus_sd_received_updates_total Total number of update events received from the SD providers.\n# TYPE prometheus_sd_received_updates_total counter\nprometheus_sd_received_updates_total{name=\"notify\"} 2\nprometheus_sd_received_updates_total{name=\"scrape\"} 11820\n# HELP prometheus_sd_updates_delayed_total Total number of update events that couldn't be sent immediately.\n# TYPE prometheus_sd_updates_delayed_total counter\nprometheus_sd_updates_delayed_total{name=\"notify\"} 0\nprometheus_sd_updates_delayed_total{name=\"scrape\"} 0\n# HELP prometheus_sd_updates_total Total number of update events sent to the SD consumers.\n# TYPE prometheus_sd_updates_total counter\nprometheus_sd_updates_total{name=\"notify\"} 1\nprometheus_sd_updates_total{name=\"scrape\"} 2953\n```\n\n----------------------------------------\n\nTITLE: Configuring Eureka Service Discovery in Prometheus\nDESCRIPTION: Configuration for Eureka service discovery in Prometheus, which retrieves scrape targets using the Eureka REST API. Includes server configuration options and available meta labels.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_41\n\nLANGUAGE: yaml\nCODE:\n```\n# The URL to connect to the Eureka server.\nserver: <string>\n\n# Refresh interval to re-read the app instance list.\n[ refresh_interval: <duration> | default = 30s ]\n\n# HTTP client settings, including authentication methods (such as basic auth and\n# authorization), proxy configurations, TLS options, custom HTTP headers, etc.\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Configuring IONOS Service Discovery in Prometheus\nDESCRIPTION: Configuration schema for IONOS Cloud service discovery, allowing Prometheus to discover scrape targets from IONOS Cloud API. Includes datacenter ID, port configuration, refresh interval and HTTP client settings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_31\n\nLANGUAGE: yaml\nCODE:\n```\n# The unique ID of the data center.\ndatacenter_id: <string>\n\n# The port to scrape metrics from.\n[ port: <int> | default = 80 ]\n\n# The time after which the servers are refreshed.\n[ refresh_interval: <duration> | default = 60s ]\n\n# HTTP client settings, including authentication methods (such as basic auth and\n# authorization), proxy configurations, TLS options, custom HTTP headers, etc.\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Running promtool for Rule Testing in Shell\nDESCRIPTION: Commands to run promtool for testing Prometheus rules using single or multiple test files.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/unit_testing_rules.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# For a single test file.\n./promtool test rules test.yml\n\n# If you have multiple test files, say test1.yml,test2.yml,test2.yml\n./promtool test rules test1.yml test2.yml test3.yml\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Flag Values with GET API Endpoint\nDESCRIPTION: API endpoint for retrieving the flag values that Prometheus was configured with, all returned as string values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nGET /api/v1/status/flags\n```\n\n----------------------------------------\n\nTITLE: Building Prometheus Alerts and Dashboards\nDESCRIPTION: Commands to generate the prometheus_alerts.yaml file containing alerts and the dashboards_out directory with Grafana dashboard JSON files using make targets.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/prometheus-mixin/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ make prometheus_alerts.yaml\n$ make dashboards_out\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenStack Service Discovery in Prometheus\nDESCRIPTION: YAML configuration for OpenStack service discovery in Prometheus. This configuration allows Prometheus to discover and scrape targets from OpenStack environments. It includes options for authentication, role selection, and other OpenStack-specific parameters.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\n# The information to access the OpenStack API.\n\n# The OpenStack role of entities that should be discovered.\nrole: <openstack_role>\n\n# The OpenStack Region.\nregion: <string>\n\n# identity_endpoint specifies the HTTP endpoint that is required to work with\n# the Identity API of the appropriate version. While it's ultimately needed by\n# all of the identity services, it will often be populated by a provider-level\n# function.\n[ identity_endpoint: <string> ]\n\n# username is required if using Identity V2 API. Consult with your provider's\n# control panel to discover your account's username. In Identity V3, either\n# userid or a combination of username and domain_id or domain_name are needed.\n[ username: <string> ]\n[ userid: <string> ]\n\n# password for the Identity V2 and V3 APIs. Consult with your provider's\n# control panel to discover your account's preferred method of authentication.\n[ password: <secret> ]\n\n# At most one of domain_id and domain_name must be provided if using username\n# with Identity V3. Otherwise, either are optional.\n[ domain_name: <string> ]\n[ domain_id: <string> ]\n\n# The project_id and project_name fields are optional for the Identity V2 API.\n# Some providers allow you to specify a project_name instead of the project_id.\n# Some require both. Your provider's authentication policies will determine\n# how these fields influence authentication.\n[ project_name: <string> ]\n[ project_id: <string> ]\n\n# The application_credential_id or application_credential_name fields are\n# required if using an application credential to authenticate. Some providers\n# allow you to create an application credential to authenticate rather than a\n# password.\n[ application_credential_name: <string> ]\n[ application_credential_id: <string> ]\n\n# The application_credential_secret field is required if using an application\n# credential to authenticate.\n[ application_credential_secret: <secret> ]\n\n# Whether the service discovery should list all instances for all projects.\n# It is only relevant for the 'instance' role and usually requires admin permissions.\n[ all_tenants: <boolean> | default: false ]\n\n# Refresh interval to re-read the instance list.\n[ refresh_interval: <duration> | default = 60s ]\n\n# The port to scrape metrics from. If using the public IP address, this must\n# instead be specified in the relabeling rule.\n[ port: <int> | default = 80 ]\n\n# The availability of the endpoint to connect to. Must be one of public, admin or internal.\n[ availability: <string> | default = \"public\" ]\n```\n\n----------------------------------------\n\nTITLE: Degree-Radian Conversion Functions in PromQL\nDESCRIPTION: Utility functions for converting between degrees and radians, including a constant function for pi. These functions operate on instant vectors and are essential for working with angular measurements.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_22\n\nLANGUAGE: promql\nCODE:\n```\ndeg(v instant-vector)\npi()\nrad(v instant-vector)\n```\n\n----------------------------------------\n\nTITLE: Querying Metric Metadata with GET API Endpoint\nDESCRIPTION: API endpoint for retrieving metadata about metrics currently scraped from targets, without providing target information. Supports limiting total metrics, per-metric limits, and filtering by metric name.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nGET /api/v1/metadata\n```\n\n----------------------------------------\n\nTITLE: Building and Running Custom Prometheus Docker Image\nDESCRIPTION: Commands to build and run a custom Prometheus Docker image with embedded configuration. This follows the Dockerfile example provided earlier.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/installation.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t my-prometheus .\ndocker run -p 9090:9090 my-prometheus\n```\n\n----------------------------------------\n\nTITLE: Prometheus TSDB Metrics Collection\nDESCRIPTION: A comprehensive collection of Prometheus TSDB metrics including counters and gauges for monitoring database operations, memory usage, storage, and HTTP request handling. Metrics cover areas such as chunk management, sample processing, block operations, WAL performance, and federation status.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_31\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP prometheus_tsdb_mmap_chunk_corruptions_total Total number of memory-mapped chunk corruptions.\n# TYPE prometheus_tsdb_mmap_chunk_corruptions_total counter\nprometheus_tsdb_mmap_chunk_corruptions_total 0\n# HELP prometheus_tsdb_mmap_chunks_total Total number of chunks that were memory-mapped.\n# TYPE prometheus_tsdb_mmap_chunks_total counter\nprometheus_tsdb_mmap_chunks_total 4.851264e+06\n# HELP prometheus_tsdb_out_of_bound_samples_total Total number of out of bound samples ingestion failed attempts with out of order support disabled.\n# TYPE prometheus_tsdb_out_of_bound_samples_total counter\nprometheus_tsdb_out_of_bound_samples_total{type=\"float\"} 0\n# HELP prometheus_tsdb_out_of_order_samples_total Total number of out of order samples ingestion failed attempts due to out of order being disabled.\n# TYPE prometheus_tsdb_out_of_order_samples_total counter\nprometheus_tsdb_out_of_order_samples_total{type=\"float\"} 517\nprometheus_tsdb_out_of_order_samples_total{type=\"histogram\"} 0\n# HELP prometheus_tsdb_reloads_failures_total Number of times the database failed to reloadBlocks block data from disk.\n# TYPE prometheus_tsdb_reloads_failures_total counter\nprometheus_tsdb_reloads_failures_total 0\n# HELP prometheus_tsdb_reloads_total Number of times the database reloaded block data from disk.\n# TYPE prometheus_tsdb_reloads_total counter\nprometheus_tsdb_reloads_total 14822\n# HELP prometheus_tsdb_retention_limit_bytes Max number of bytes to be retained in the tsdb blocks, configured 0 means disabled\n# TYPE prometheus_tsdb_retention_limit_bytes gauge\nprometheus_tsdb_retention_limit_bytes 0\n# HELP prometheus_tsdb_retention_limit_seconds How long to retain samples in storage.\n# TYPE prometheus_tsdb_retention_limit_seconds gauge\nprometheus_tsdb_retention_limit_seconds 2.6784e+06\n# HELP prometheus_tsdb_size_retentions_total The number of times that blocks were deleted because the maximum number of bytes was exceeded.\n# TYPE prometheus_tsdb_size_retentions_total counter\nprometheus_tsdb_size_retentions_total 0\n# HELP prometheus_tsdb_snapshot_replay_error_total Total number snapshot replays that failed.\n# TYPE prometheus_tsdb_snapshot_replay_error_total counter\nprometheus_tsdb_snapshot_replay_error_total 0\n# HELP prometheus_tsdb_storage_blocks_bytes The number of bytes that are currently used for local storage by all blocks.\n# TYPE prometheus_tsdb_storage_blocks_bytes gauge\nprometheus_tsdb_storage_blocks_bytes 2.762863592e+09\n# HELP prometheus_tsdb_symbol_table_size_bytes Size of symbol table in memory for loaded blocks\n# TYPE prometheus_tsdb_symbol_table_size_bytes gauge\nprometheus_tsdb_symbol_table_size_bytes 10616\n# HELP prometheus_tsdb_time_retentions_total The number of times that blocks were deleted because the maximum time limit was exceeded.\n# TYPE prometheus_tsdb_time_retentions_total counter\nprometheus_tsdb_time_retentions_total 5\n# HELP prometheus_tsdb_tombstone_cleanup_seconds The time taken to recompact blocks to remove tombstones.\n# TYPE prometheus_tsdb_tombstone_cleanup_seconds histogram\nprometheus_tsdb_tombstone_cleanup_seconds_bucket{le=\"+Inf\"} 0\nprometheus_tsdb_tombstone_cleanup_seconds_sum 0\nprometheus_tsdb_tombstone_cleanup_seconds_count 0\n# HELP prometheus_tsdb_too_old_samples_total Total number of out of order samples ingestion failed attempts with out of support enabled, but sample outside of time window.\n# TYPE prometheus_tsdb_too_old_samples_total counter\nprometheus_tsdb_too_old_samples_total{type=\"float\"} 0\n# HELP prometheus_tsdb_vertical_compactions_total Total number of compactions done on overlapping blocks.\n# TYPE prometheus_tsdb_vertical_compactions_total counter\nprometheus_tsdb_vertical_compactions_total 0\n# HELP prometheus_tsdb_wal_completed_pages_total Total number of completed pages.\n# TYPE prometheus_tsdb_wal_completed_pages_total counter\nprometheus_tsdb_wal_completed_pages_total 109271\n# HELP prometheus_tsdb_wal_corruptions_total Total number of WAL corruptions.\n# TYPE prometheus_tsdb_wal_corruptions_total counter\nprometheus_tsdb_wal_corruptions_total 0\n# HELP prometheus_tsdb_wal_fsync_duration_seconds Duration of write log fsync.\n# TYPE prometheus_tsdb_wal_fsync_duration_seconds summary\nprometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.5\"} NaN\nprometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.9\"} NaN\nprometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.99\"} NaN\nprometheus_tsdb_wal_fsync_duration_seconds_sum 4.842524568000002\nprometheus_tsdb_wal_fsync_duration_seconds_count 123\n# HELP prometheus_tsdb_wal_page_flushes_total Total number of page flushes.\n# TYPE prometheus_tsdb_wal_page_flushes_total counter\nprometheus_tsdb_wal_page_flushes_total 2.293951e+06\n# HELP prometheus_tsdb_wal_segment_current Write log segment index that TSDB is currently writing to.\n# TYPE prometheus_tsdb_wal_segment_current gauge\nprometheus_tsdb_wal_segment_current 24726\n# HELP prometheus_tsdb_wal_storage_size_bytes Size of the write log directory.\n# TYPE prometheus_tsdb_wal_storage_size_bytes gauge\nprometheus_tsdb_wal_storage_size_bytes 6.9168385e+07\n# HELP prometheus_tsdb_wal_truncate_duration_seconds Duration of WAL truncation.\n# TYPE prometheus_tsdb_wal_truncate_duration_seconds summary\nprometheus_tsdb_wal_truncate_duration_seconds_sum 121.61954577099996\nprometheus_tsdb_wal_truncate_duration_seconds_count 62\n# HELP prometheus_tsdb_wal_truncations_failed_total Total number of write log truncations that failed.\n# TYPE prometheus_tsdb_wal_truncations_failed_total counter\nprometheus_tsdb_wal_truncations_failed_total 0\n# HELP prometheus_tsdb_wal_truncations_total Total number of write log truncations attempted.\n# TYPE prometheus_tsdb_wal_truncations_total counter\nprometheus_tsdb_wal_truncations_total 62\n# HELP prometheus_tsdb_wal_writes_failed_total Total number of write log writes that failed.\n# TYPE prometheus_tsdb_wal_writes_failed_total counter\nprometheus_tsdb_wal_writes_failed_total 0\n# HELP prometheus_web_federation_errors_total Total number of errors that occurred while sending federation responses.\n# TYPE prometheus_web_federation_errors_total counter\nprometheus_web_federation_errors_total 0\n# HELP prometheus_web_federation_warnings_total Total number of warnings that occurred while sending federation responses.\n# TYPE prometheus_web_federation_warnings_total counter\nprometheus_web_federation_warnings_total 0\n# HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served.\n# TYPE promhttp_metric_handler_requests_in_flight gauge\npromhttp_metric_handler_requests_in_flight 1\n# HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.\n# TYPE promhttp_metric_handler_requests_total counter\npromhttp_metric_handler_requests_total{code=\"200\"} 4.059092e+06\npromhttp_metric_handler_requests_total{code=\"500\"} 0\npromhttp_metric_handler_requests_total{code=\"503\"} 0\n```\n\n----------------------------------------\n\nTITLE: Aggregating All Native Histograms\nDESCRIPTION: Aggregates all native histograms without any by clause\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_11\n\nLANGUAGE: promql\nCODE:\n```\nhistogram_quantile(0.9, sum(rate(http_request_duration_seconds[10m])))\n```\n\n----------------------------------------\n\nTITLE: Prometheus Rule Test File Format in YAML\nDESCRIPTION: YAML structure for defining Prometheus rule tests, including rule files, evaluation interval, group evaluation order, and test cases.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/unit_testing_rules.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# This is a list of rule files to consider for testing. Globs are supported.\nrule_files:\n  [ - <file_name> ]\n\n[ evaluation_interval: <duration> | default = 1m ]\n\n# The order in which group names are listed below will be the order of evaluation of\n# rule groups (at a given evaluation time). The order is guaranteed only for the groups mentioned below.\n# All the groups need not be mentioned below.\ngroup_eval_order:\n  [ - <group_name> ]\n\n# All the tests are listed here.\ntests:\n  [ - <test_group> ]\n```\n\n----------------------------------------\n\nTITLE: Metric Relabeling Configuration for Label Value Normalization\nDESCRIPTION: YAML configuration for metric relabeling to maintain backward compatibility with integer le and quantile label values in Prometheus v3.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/migration.md#2025-04-16_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n    metric_relabel_configs:\n      - source_labels:\n          - quantile\n        target_label: quantile\n        regex: (\\d+)\\.0+\n      - source_labels:\n          - le\n          - __name__\n        target_label: le\n        regex: (\\d+)\\.0+;.*_bucket\n```\n\n----------------------------------------\n\nTITLE: Creating Tagged Release in Bash\nDESCRIPTION: Commands to create and push a signed git tag for a new Prometheus release. The tag is generated from the VERSION file contents and signed using GPG.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/RELEASE.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntag=\"v$(< VERSION)\"\ngit tag -s \"${tag}\" -m \"${tag}\"\ngit push origin \"${tag}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Client Settings for Kubernetes Service Discovery in YAML\nDESCRIPTION: This YAML snippet indicates where to define HTTP client settings for Kubernetes service discovery in Prometheus, including authentication, proxy configurations, and TLS options.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_35\n\nLANGUAGE: yaml\nCODE:\n```\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Example Response for Configuration Status\nDESCRIPTION: Sample response showing the currently loaded configuration in YAML format (without YAML comments due to library limitations).\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n$ curl http://localhost:9090/api/v1/status/config\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"yaml\": \"<content of the loaded config file in YAML>\",\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Alternative Aggregation Operator Syntax in PromQL\nDESCRIPTION: Alternative syntax for aggregation operators in PromQL, with the 'without' or 'by' clause after the expression.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_11\n\nLANGUAGE: promql\nCODE:\n```\n<aggr-op>([parameter,] <vector expression>) [without|by (<label list>)]\n```\n\n----------------------------------------\n\nTITLE: Recompiling Protobufs for Prometheus\nDESCRIPTION: This command is used to recompile the protobufs for the Prometheus project. It should be run from the parent directory when protobuf definitions have been modified.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/prompb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake proto\n```\n\n----------------------------------------\n\nTITLE: Using Advanced Label Matching Operators\nDESCRIPTION: Demonstrates regex-based and negative label matching operators to select time series based on complex criteria across multiple label values and patterns.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_8\n\nLANGUAGE: promql\nCODE:\n```\nhttp_requests_total{environment=~\"staging|testing|development\",method!=\"GET\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Scaleway Service Discovery in Prometheus\nDESCRIPTION: YAML configuration for Scaleway service discovery, including access credentials, project settings, and filtering options. Supports both instance and baremetal target discovery with customizable refresh intervals and HTTP client settings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_42\n\nLANGUAGE: yaml\nCODE:\n```\naccess_key: <string>\n\n[ secret_key: <secret> ]\n\n[ secret_key_file: <filename> ]\n\nproject_id: <string>\n\nrole: <string>\n\n[ port: <int> | default = 80 ]\n\n[ api_url: <string> | default = \"https://api.scaleway.com\" ]\n\n[ zone: <string> | default = fr-par-1 ]\n\n[ name_filter: <string> ]\n\ntags_filter:\n[ - <string> ]\n\n[ refresh_interval: <duration> | default = 60s ]\n\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Vultr Service Discovery in Prometheus\nDESCRIPTION: YAML configuration for Vultr service discovery, defining port settings and refresh intervals for instance discovery. Includes HTTP client configuration options.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_44\n\nLANGUAGE: yaml\nCODE:\n```\n[ port: <int> | default = 80 ]\n\n[ refresh_interval: <duration> | default = 60s ]\n\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Building Prometheus with make\nDESCRIPTION: Commands to build Prometheus using make and run it with a configuration file. This method compiles in the web assets so Prometheus can be run from anywhere.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n./prometheus --config.file=your_config.yml\n```\n\n----------------------------------------\n\nTITLE: Disabling PromQL Linter and Autocompletion Features\nDESCRIPTION: Code example showing how to selectively disable the linter and autocompletion features of the PromQL extension.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst promQL = new PromQLExtension().activateLinter(false).activateCompletion(false) // here the linter and the autocomplete are deactivated\n```\n\n----------------------------------------\n\nTITLE: PromQL Comment Syntax\nDESCRIPTION: Example of line comment syntax in PromQL using the # character.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_18\n\nLANGUAGE: promql\nCODE:\n```\n# This is a comment\n```\n\n----------------------------------------\n\nTITLE: Formatting Float Literals in PromQL\nDESCRIPTION: Shows the various formats for expressing float literals in Prometheus queries, including standard decimal notation, scientific notation, hexadecimal, and special values like NaN and Infinity.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/basics.md#2025-04-16_snippet_1\n\nLANGUAGE: promql\nCODE:\n```\n23\n-2.43\n3.4e-9\n0x8f\n-Inf\nNaN\n```\n\n----------------------------------------\n\nTITLE: Example Target Groups Structure for Prometheus Service Discovery\nDESCRIPTION: An example of how target groups are structured in Prometheus service discovery. This shows two target groups with different sources, containing multiple targets with various labels.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/discovery/README.md#2025-04-16_snippet_1\n\nLANGUAGE: go\nCODE:\n```\n[]targetgroup.Group{\n\t{\n\t\tTargets: []model.LabelSet{\n\t\t\t{\n\t\t\t\t\"__instance__\": \"10.11.150.1:7870\",\n\t\t\t\t\"hostname\":     \"demo-target-1\",\n\t\t\t\t\"test\":         \"simple-test\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"__instance__\": \"10.11.150.4:7870\",\n\t\t\t\t\"hostname\":     \"demo-target-2\",\n\t\t\t\t\"test\":         \"simple-test\",\n\t\t\t},\n\t\t},\n\t\tLabels: model.LabelSet{\n\t\t\t\"job\": \"mysql\",\n\t\t},\n\t\t\"Source\": \"file1\",\n\t},\n\t{\n\t\tTargets: []model.LabelSet{\n\t\t\t{\n\t\t\t\t\"__instance__\": \"10.11.122.11:6001\",\n\t\t\t\t\"hostname\":     \"demo-postgres-1\",\n\t\t\t\t\"test\":         \"simple-test\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"__instance__\": \"10.11.122.15:6001\",\n\t\t\t\t\"hostname\":     \"demo-postgres-2\",\n\t\t\t\t\"test\":         \"simple-test\",\n\t\t\t},\n\t\t},\n\t\tLabels: model.LabelSet{\n\t\t\t\"job\": \"postgres\",\n\t\t},\n\t\t\"Source\": \"file2\",\n\t},\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling WAL Compression by Default in TSDB\nDESCRIPTION: WAL compression is now enabled by default in TSDB. This prevents downgrading to v2.10 or earlier without deleting the WAL.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_21\n\nLANGUAGE: go\nCODE:\n```\n// TSDB: WAL compression is enabled by default\n```\n\n----------------------------------------\n\nTITLE: Installing Golex Package\nDESCRIPTION: Command to install the golex package, which is required for generating lexer code from the .l definition files.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngo get -u modernc.org/golex\n```\n\n----------------------------------------\n\nTITLE: Series Definition in YAML for Prometheus Rule Testing\nDESCRIPTION: Specifies the format for defining series data in Prometheus rule tests, including series notation and value expansion syntax.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/unit_testing_rules.md#2025-04-16_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# This follows the usual series notation '<metric name>{<label name>=<label value>, ...}'\n# Examples:\n#      series_name{label1=\"value1\", label2=\"value2\"}\n#      go_goroutines{job=\"prometheus\", instance=\"localhost:9090\"}\nseries: <string>\n\n# This uses expanding notation.\n# Expanding notation:\n#     'a+bxn' becomes 'a a+b a+(2*b) a+(3*b)  a+(n*b)'\n#     Read this as series starts at a, then n further samples incrementing by b.\n#     'a-bxn' becomes 'a a-b a-(2*b) a-(3*b)  a-(n*b)'\n#     Read this as series starts at a, then n further samples decrementing by b (or incrementing by negative b).\n#     'axn' becomes 'a a a  a' (a n+1 times) - it's a shorthand for 'a+0xn'\n# There are special values to indicate missing and stale samples:\n#     '_' represents a missing sample from scrape\n#     'stale' indicates a stale sample\n# Examples:\n#     1. '-2+4x3' becomes '-2 2 6 10' - series starts at -2, then 3 further samples incrementing by 4.\n#     2. ' 1-2x4' becomes '1 -1 -3 -5 -7' - series starts at 1, then 4 further samples decrementing by 2.\n#     3. ' 1x4' becomes '1 1 1 1 1' - shorthand for '1+0x4', series starts at 1, then 4 further samples incrementing by 0.\n#     4. ' 1 _x3 stale' becomes '1 _ _ _ stale' - the missing sample cannot increment, so 3 missing samples are produced by the '_x3' expression.\n#\n# Native histogram notation:\n#     Native histograms can be used instead of floating point numbers using the following notation:\n#     {{schema:1 sum:-0.3 count:3.1 z_bucket:7.1 z_bucket_w:0.05 buckets:[5.1 10 7] offset:-3 n_buckets:[4.1 5] n_offset:-5 counter_reset_hint:gauge}}\n#     Native histograms support the same expanding notation as floating point numbers, i.e. 'axn', 'a+bxn' and 'a-bxn'.\n#     All properties are optional and default to 0. The order is not important. The following properties are supported:\n#     - schema (int): \n#         Currently valid schema numbers are -4 <= n <= 8. They are all for\n#         base-2 bucket schemas, where 1 is a bucket boundary in each case, and\n#         then each power of two is divided into 2^n logarithmic buckets.  Or\n#         in other words, each bucket boundary is the previous boundary times\n#         2^(2^-n).\n#     - sum (float): \n#         The sum of all observations, including the zero bucket.\n#     - count (non-negative float): \n#         The number of observations, including those that are NaN and including the zero bucket.\n#     - z_bucket (non-negative float): \n#         The sum of all observations in the zero bucket.\n#     - z_bucket_w (non-negative float): \n#         The width of the zero bucket. \n#         If z_bucket_w > 0, the zero bucket contains all observations -z_bucket_w <= x <= z_bucket_w.\n#         Otherwise, the zero bucket only contains observations that are exactly 0.\n#     - buckets (list of non-negative floats):\n#         Observation counts in positive buckets. Each represents an absolute count.\n#     - offset (int):\n#         The starting index of the first entry in the positive buckets.\n#     - n_buckets (list of non-negative floats):\n#         Observation counts in negative buckets. Each represents an absolute count.\n#     - n_offset (int):\n#         The starting index of the first entry in the negative buckets.\n#     - counter_reset_hint (one of 'unknown', 'reset', 'not_reset' or 'gauge')\n#         The counter reset hint associated with this histogram. Defaults to 'unknown' if not set.\nvalues: <string>\n```\n\n----------------------------------------\n\nTITLE: Example of Instant Query with Warning Expectation\nDESCRIPTION: Shows a complete example of evaluating an instant query at a specific time with expectations for warnings but no info annotations, and the expected result series.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/promql/promqltest/README.md#2025-04-16_snippet_5\n\nLANGUAGE: promql\nCODE:\n```\neval instant at 1m sum by (env) (my_metric)\n    expect warn\n    expect no_info\n    {env=\"prod\"} 5\n    {env=\"test\"} 20\n```\n\n----------------------------------------\n\nTITLE: Changing HTTP Method for Prometheus API Requests\nDESCRIPTION: Configuration to change the HTTP method used when contacting Prometheus from the default POST to GET for specific endpoints.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst promQL = new PromQLExtension().setComplete({remote: {httpMethod: 'GET'}})\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Histogram Metric for Golang Manual Histogram\nDESCRIPTION: This snippet defines a Prometheus histogram metric named 'golang_manual_histogram_seconds'. It includes bucket definitions, sum, and count for different combinations of address, generation, and port.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/1histogram.om.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP golang_manual_histogram_seconds This is a histogram with manually selected parameters\n# TYPE golang_manual_histogram_seconds histogram\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5001\",le=\"0.005\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5001\",le=\"0.01\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5001\",le=\"0.025\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5001\",le=\"0.05\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5001\",le=\"0.1\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5001\",le=\"0.25\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5001\",le=\"0.5\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5001\",le=\"1.0\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5001\",le=\"2.5\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5001\",le=\"5.0\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5001\",le=\"10.0\"} 1\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5001\",le=\"+Inf\"} 1\ngolang_manual_histogram_seconds_sum{address=\"0.0.0.0\",generation=\"20\",port=\"5001\"} 10.0\ngolang_manual_histogram_seconds_count{address=\"0.0.0.0\",generation=\"20\",port=\"5001\"} 1\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5002\",le=\"0.005\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5002\",le=\"0.01\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5002\",le=\"0.025\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5002\",le=\"0.05\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5002\",le=\"0.1\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5002\",le=\"0.25\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5002\",le=\"0.5\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5002\",le=\"1.0\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5002\",le=\"2.5\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5002\",le=\"5.0\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5002\",le=\"10.0\"} 1\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5002\",le=\"+Inf\"} 1\ngolang_manual_histogram_seconds_sum{address=\"0.0.0.0\",generation=\"20\",port=\"5002\"} 10.0\ngolang_manual_histogram_seconds_count{address=\"0.0.0.0\",generation=\"20\",port=\"5002\"} 1\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5003\",le=\"0.005\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5003\",le=\"0.01\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5003\",le=\"0.025\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5003\",le=\"0.05\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5003\",le=\"0.1\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5003\",le=\"0.25\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5003\",le=\"0.5\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5003\",le=\"1.0\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5003\",le=\"2.5\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5003\",le=\"5.0\"} 0\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5003\",le=\"10.0\"} 1\ngolang_manual_histogram_seconds_bucket{address=\"0.0.0.0\",generation=\"20\",port=\"5003\",le=\"+Inf\"} 1\ngolang_manual_histogram_seconds_sum{address=\"0.0.0.0\",generation=\"20\",port=\"5003\"} 10.0\ngolang_manual_histogram_seconds_count{address=\"0.0.0.0\",generation=\"20\",port=\"5003\"} 1\n# EOF\n```\n\n----------------------------------------\n\nTITLE: Removing a Target Group in Prometheus Service Discovery\nDESCRIPTION: When all targets in a group are removed, the SD mechanism should send a target group with empty Targets but the same Source identifier to indicate removal of the entire group.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/discovery/README.md#2025-04-16_snippet_3\n\nLANGUAGE: go\nCODE:\n```\n&targetgroup.Group{\n\tTargets:  nil,\n\t\"Source\": \"file2\",\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling HTTP/2 in Prometheus\nDESCRIPTION: HTTP/2 is disabled due to concerns with the Go HTTP/2 client implementation.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_18\n\nLANGUAGE: go\nCODE:\n```\n// Disable HTTP/2 because of concerns with the Go HTTP/2 client\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Target Scraping Metrics\nDESCRIPTION: This snippet shows Prometheus metrics for target scraping intervals and metadata cache size. It provides information on the actual intervals between scrapes for different quantiles and the total number of scrapes.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_25\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP prometheus_target_interval_length_seconds Actual intervals between scrapes.\n# TYPE prometheus_target_interval_length_seconds summary\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.01\"} 14.982591232\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.05\"} 14.997567414\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.5\"} 14.999977915\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.9\"} 15.000793403\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.99\"} 15.017607167\nprometheus_target_interval_length_seconds_sum{interval=\"15s\"} 9.742237453453667e+06\nprometheus_target_interval_length_seconds_count{interval=\"15s\"} 649482\n# HELP prometheus_target_metadata_cache_bytes The number of bytes that are currently used for storing metric metadata in the cache\n```\n\n----------------------------------------\n\nTITLE: PromQL Test Case Structure in YAML for Prometheus Rule Testing\nDESCRIPTION: Defines the structure for PromQL expression test cases in Prometheus rule testing, including the expression, evaluation time, and expected samples.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/unit_testing_rules.md#2025-04-16_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n# Expression to evaluate\nexpr: <string>\n\n# The time elapsed from time=0s when the expression has to be evaluated.\neval_time: <duration>\n\n# Expected samples at the given evaluation time.\nexp_samples:\n  [ - <sample> ]\n```\n\n----------------------------------------\n\nTITLE: Tracking Prometheus Server Status and Remote Storage Metrics\nDESCRIPTION: Metrics showing Prometheus server readiness and remote storage operations including sample counts, exemplars, histograms, and timestamp information. These metrics are crucial for monitoring the health and throughput of Prometheus's remote storage functionality.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_18\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP prometheus_ready Whether Prometheus startup was fully completed and the server is ready for normal operation.\n# TYPE prometheus_ready gauge\nprometheus_ready 1\n# HELP prometheus_remote_storage_exemplars_in_total Exemplars in to remote storage, compare to exemplars out for queue managers.\n# TYPE prometheus_remote_storage_exemplars_in_total counter\nprometheus_remote_storage_exemplars_in_total 4.959738e+06\n# HELP prometheus_remote_storage_highest_timestamp_in_seconds Highest timestamp that has come into the remote storage via the Appender interface, in seconds since epoch. Initialized to 0 when no data has been received yet.\n# TYPE prometheus_remote_storage_highest_timestamp_in_seconds gauge\nprometheus_remote_storage_highest_timestamp_in_seconds 1.738949375e+09\n# HELP prometheus_remote_storage_histograms_in_total HistogramSamples in to remote storage, compare to histograms out for queue managers.\n# TYPE prometheus_remote_storage_histograms_in_total counter\nprometheus_remote_storage_histograms_in_total 0\n# HELP prometheus_remote_storage_samples_in_total Samples in to remote storage, compare to samples out for queue managers.\n# TYPE prometheus_remote_storage_samples_in_total counter\nprometheus_remote_storage_samples_in_total 6.00447296e+08\n# HELP prometheus_remote_storage_string_interner_zero_reference_releases_total The number of times release has been called for strings that are not interned.\n# TYPE prometheus_remote_storage_string_interner_zero_reference_releases_total counter\nprometheus_remote_storage_string_interner_zero_reference_releases_total 0\n```\n\n----------------------------------------\n\nTITLE: Using Prometheus v2 as a Go library\nDESCRIPTION: Command to use Prometheus v2.35.0 as a library in a Go project. This demonstrates the alternative versioning scheme used for v2.x.x releases.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/README.md#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngo get github.com/prometheus/prometheus@v0.35.0\n```\n\n----------------------------------------\n\nTITLE: Enable Exemplar Storage Flag\nDESCRIPTION: Command flag to enable storage of OpenMetrics exemplars in a fixed-size circular buffer\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=exemplar-storage\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Rule Group Metrics\nDESCRIPTION: This snippet shows Prometheus metrics for rule groups, indicating the number of rules in each group across different file paths and configurations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_21\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE prometheus_rule_group_rules gauge\nprometheus_rule_group_rules{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 2\nprometheus_rule_group_rules{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 2\nprometheus_rule_group_rules{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 25\nprometheus_rule_group_rules{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 25\nprometheus_rule_group_rules{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 11\nprometheus_rule_group_rules{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 11\nprometheus_rule_group_rules{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 23\nprometheus_rule_group_rules{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 23\n```\n\n----------------------------------------\n\nTITLE: Overriding Prometheus API Prefix\nDESCRIPTION: Configuration to modify the default API prefix used when building queries for the Prometheus API, allowing for custom API paths.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst promql = new PromQLExtension().setComplete({remote: {apiPrefix: '/my/api/prefix'}})\n```\n\n----------------------------------------\n\nTITLE: Configuring Scrape Config Files in Prometheus\nDESCRIPTION: Example of using the new scrape_config_files option to include scrape configs from different files.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_12\n\nLANGUAGE: YAML\nCODE:\n```\nscrape_config_files:\n  - /path/to/additional/scrape_configs/*.yaml\n```\n\n----------------------------------------\n\nTITLE: Building the Remote Storage Adapter\nDESCRIPTION: Command to build the remote storage adapter binary from source code.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/remote_storage/remote_storage_adapter/README.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngo build\n```\n\n----------------------------------------\n\nTITLE: Accessing the Prometheus Remote Read API Endpoint\nDESCRIPTION: The endpoint for making Remote Read API requests to Prometheus. This endpoint is used to retrieve data from Prometheus using snappy compression.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/remote_read_api.md#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n/api/v1/read\n```\n\n----------------------------------------\n\nTITLE: Building Production-Optimized UI Versions\nDESCRIPTION: Command to build production-optimized versions of both React app versions to their respective output directories.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: Enabling OTLP Delta to Cumulative Conversion Feature Flag\nDESCRIPTION: Command line flag to enable conversion of OTLP metrics from delta temporality to cumulative temporality in Prometheus. When enabled, delta metrics will be converted instead of being dropped. Uses the OpenTelemetry Collector's deltatocumulative processor with default settings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=otlp-deltatocumulative\n```\n\n----------------------------------------\n\nTITLE: Prometheus Metrics Sample Data\nDESCRIPTION: Comprehensive set of Prometheus metrics showing connection tracking, process stats, and HTTP request durations. Includes metrics for network connections, memory usage, query performance, and HTTP endpoint response times.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_1\n\nLANGUAGE: prometheus\nCODE:\n```\nnet_conntrack_dialer_conn_established_total{dialer_name=\"random\"} 4\nnet_conntrack_dialer_conn_failed_total{dialer_name=\"alertmanager\",reason=\"refused\"} 0\nnet_conntrack_dialer_conn_failed_total{dialer_name=\"alertmanager\",reason=\"resolution\"} 0\n# ... additional metrics truncated for brevity ...\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"0.4\"} 27\n```\n\n----------------------------------------\n\nTITLE: Adding group() Aggregator in PromQL\nDESCRIPTION: A new group() aggregator has been added to PromQL for grouping results.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_22\n\nLANGUAGE: go\nCODE:\n```\n// PromQL: Added `group()` aggregator\n```\n\n----------------------------------------\n\nTITLE: Supporting Composite Durations in Prometheus\nDESCRIPTION: Prometheus now supports composite durations in PromQL, config, and UI, allowing expressions like 1h30m.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_20\n\nLANGUAGE: go\nCODE:\n```\n// Support composite durations in PromQL, config and UI, e.g. 1h30m\n```\n\n----------------------------------------\n\nTITLE: Bind-mounting Prometheus Configuration Directory\nDESCRIPTION: Command to run Prometheus on Docker with an entire configuration directory mounted from the host system. This allows mounting multiple configuration files at once.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/installation.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n    -p 9090:9090 \\\n    -v /path/to/config:/etc/prometheus \\\n    prom/prometheus\n```\n\n----------------------------------------\n\nTITLE: Simplified info() Function Usage\nDESCRIPTION: Most basic usage of info() function without label selectors to add all available data labels.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/functions.md#2025-04-16_snippet_15\n\nLANGUAGE: promql\nCODE:\n```\ninfo(rate(http_server_request_duration_seconds_count[2m]))\n```\n\n----------------------------------------\n\nTITLE: Overriding Default HTTP Client for Prometheus Requests\nDESCRIPTION: Configuration to provide a custom HTTP client for Prometheus API requests, useful for authenticated or specialized access scenarios.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst promQL = new PromQLExtension().setComplete({remote: {fetchFn: myHTTPClient}})\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Prometheus UI\nDESCRIPTION: Command to run tests for the React app and all modules in interactive watch mode. Tests will automatically re-run when source files are changed.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm test\n```\n\n----------------------------------------\n\nTITLE: Test Group Structure in YAML for Prometheus Rule Testing\nDESCRIPTION: Defines the structure of a test group in YAML, including series data, alert rule tests, PromQL expression tests, and external labels and URL.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/unit_testing_rules.md#2025-04-16_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# Series data\n[ interval: <duration> | default = evaluation_interval ]\ninput_series:\n  [ - <series> ]\n\n# Name of the test group\n[ name: <string> ]\n\n# Unit tests for the above data.\n\n# Unit tests for alerting rules. We consider the alerting rules from the input file.\nalert_rule_test:\n  [ - <alert_test_case> ]\n\n# Unit tests for PromQL expressions.\npromql_expr_test:\n  [ - <promql_test_case> ]\n\n# External labels accessible to the alert template.\nexternal_labels:\n  [ <labelname>: <string> ... ]\n\n# External URL accessible to the alert template.\n# Usually set using --web.external-url.\n  [ external_url: <string> ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Uyuni Service Discovery in Prometheus\nDESCRIPTION: YAML configuration for Uyuni service discovery, specifying server connection details, authentication credentials, and system filtering options. Includes customizable refresh intervals and HTTP client settings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_43\n\nLANGUAGE: yaml\nCODE:\n```\nserver: <string>\n\nusername: <string>\npassword: <secret>\n\n[ entitlement: <string> | default = monitoring_entitled ]\n\n[ separator: <string> | default = , ]\n\n[ refresh_interval: <duration> | default = 60s ]\n\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Handling TLS Certificate Validation in Go 1.15\nDESCRIPTION: Go 1.15 deprecates X.509 CommonName in TLS certificates validation. This affects Prometheus builds using Go 1.15.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_17\n\nLANGUAGE: go\nCODE:\n```\n// X.509 CommonName is deprecated in Go 1.15 TLS certificate validation\n```\n\n----------------------------------------\n\nTITLE: Prometheus Release Notes Markdown\nDESCRIPTION: Structured changelog entries documenting version releases, features, enhancements and bugfixes for Prometheus from version 2.33.5 to 2.39.1.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_13\n\nLANGUAGE: markdown\nCODE:\n```\n## 2.39.1 / 2022-10-07\n\n* [BUGFIX] Rules: Fix notifier relabel changing the labels on active alerts. #11427\n\n## 2.39.0 / 2022-10-05\n\n* [FEATURE] **experimental** TSDB: Add support for ingesting out-of-order samples. This is configured via `out_of_order_time_window` field in the config file; check config file docs for more info. #11075\n* [ENHANCEMENT] API: `/-/healthy` and `/-/ready` API calls now also respond to a `HEAD` request on top of existing `GET` support. #11160\n* [ENHANCEMENT] PuppetDB SD: Add `__meta_puppetdb_query` label. #11238\n* [ENHANCEMENT] AWS EC2 SD: Add `__meta_ec2_region` label. #11326\n* [ENHANCEMENT] AWS Lightsail SD: Add `__meta_lightsail_region` label. #11326\n* [ENHANCEMENT] Scrape: Optimise relabeling by re-using memory. #11147\n* [ENHANCEMENT] TSDB: Improve WAL replay timings. #10973 #11307 #11319\n* [ENHANCEMENT] TSDB: Optimise memory by not storing unnecessary data in the memory. #11280 #11288 #11296\n* [ENHANCEMENT] TSDB: Allow overlapping blocks by default. `--storage.tsdb.allow-overlapping-blocks` now has no effect. #11331\n* [ENHANCEMENT] UI: Click to copy label-value pair from query result to clipboard. #11229\n* [BUGFIX] TSDB: Turn off isolation for Head compaction to fix a memory leak. #11317\n* [BUGFIX] TSDB: Fix 'invalid magic number 0' error on Prometheus startup. #11338\n* [BUGFIX] PromQL: Properly close file descriptor when logging unfinished queries. #11148\n* [BUGFIX] Agent: Fix validation of flag options and prevent WAL from growing more than desired. #9876\n```\n\n----------------------------------------\n\nTITLE: Enable Concurrent Rule Evaluation Flag\nDESCRIPTION: Command flag to enable concurrent evaluation of independent rules\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=concurrent-rule-eval\n```\n\n----------------------------------------\n\nTITLE: Running Go Linters\nDESCRIPTION: Command to run golangci-lint for code quality checks\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CONTRIBUTING.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake lint\n```\n\n----------------------------------------\n\nTITLE: Configuring File-based Service Discovery in Prometheus YAML\nDESCRIPTION: Configuration snippet showing how to reference file_sd configuration in Prometheus config file\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/custom-sd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprometheus.yml\n```\n\n----------------------------------------\n\nTITLE: Static Configuration Format in JSON\nDESCRIPTION: JSON format for defining static target configurations in file-based service discovery.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"targets\": [ \"<host>\", ... ],\n    \"labels\": {\n      \"<labelname>\": \"<labelvalue>\", ...\n    }\n  },\n  ...\n]\n```\n\n----------------------------------------\n\nTITLE: Advanced Network Statistics Table Template\nDESCRIPTION: Complex template combining HTML and Go template syntax to create a table of network statistics. Demonstrates nested queries, multiple metrics, and rate calculations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/template_examples.md#2025-04-16_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<table>\n{{ range printf \"node_network_receive_bytes{job='node',instance='%s',device!='lo'}\" .Params.instance | query | sortByLabel \"device\"}}\n  <tr><th colspan=2>{{ .Labels.device }}</th></tr>\n  <tr>\n    <td>Received</td>\n    <td>{{ with printf \"rate(node_network_receive_bytes{job='node',instance='%s',device='%s'}[5m])\" .Labels.instance .Labels.device | query }}{{ . | first | value | humanize }}B/s{{end}}</td>\n  </tr>\n  <tr>\n    <td>Transmitted</td>\n    <td>{{ with printf \"rate(node_network_transmit_bytes{job='node',instance='%s',device='%s'}[5m])\" .Labels.instance .Labels.device | query }}{{ . | first | value | humanize }}B/s{{end}}</td>\n  </tr>{{ end }}\n</table>\n```\n\n----------------------------------------\n\nTITLE: Trigonometric Binary Operators in PromQL\nDESCRIPTION: Trigonometric function that works in radians for vector operations in Prometheus Query Language.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_1\n\nLANGUAGE: promql\nCODE:\n```\natan2\n```\n\n----------------------------------------\n\nTITLE: Configuring Native Histograms in Go Client\nDESCRIPTION: Code snippet showing how to configure native histograms in the Prometheus Go client by setting the NativeHistogramBucketFactor in HistogramOpts.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_9\n\nLANGUAGE: Go\nCODE:\n```\nHistogramOpts{\n  NativeHistogramBucketFactor: 1.1\n}\n```\n\n----------------------------------------\n\nTITLE: Using Prometheus v3 as a Go library\nDESCRIPTION: Command to use Prometheus v3.0.0 as a library in a Go project. Note that Prometheus uses a special versioning scheme for its Go modules to comply with Go mod rules.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/README.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngo get github.com/prometheus/prometheus@v0.300.0\n```\n\n----------------------------------------\n\nTITLE: Prometheus Server Metrics Definition\nDESCRIPTION: A collection of Prometheus metrics that monitor the Prometheus server's operation, including build information, configuration status, query engine performance, and request handling statistics.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_3\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE prometheus_build_info gauge\nprometheus_build_info{branch=\"HEAD\",goarch=\"amd64\",goos=\"linux\",goversion=\"go1.23.4\",revision=\"7086161a93b262aa0949dbf2aba15a5a7b13e0a3\",tags=\"netgo,builtinassets,stringlabels\",version=\"3.1.0\"} 1\n# HELP prometheus_config_last_reload_success_timestamp_seconds Timestamp of the last successful configuration reload.\n# TYPE prometheus_config_last_reload_success_timestamp_seconds gauge\nprometheus_config_last_reload_success_timestamp_seconds 1.7380636976181264e+09\n# HELP prometheus_config_last_reload_successful Whether the last configuration reload attempt was successful.\n# TYPE prometheus_config_last_reload_successful gauge\nprometheus_config_last_reload_successful 1\n# HELP prometheus_engine_queries The current number of queries being executed or waiting.\n# TYPE prometheus_engine_queries gauge\nprometheus_engine_queries 0\n# HELP prometheus_engine_queries_concurrent_max The max number of concurrent queries.\n# TYPE prometheus_engine_queries_concurrent_max gauge\nprometheus_engine_queries_concurrent_max 20\n# HELP prometheus_engine_query_duration_seconds Query timings\n# TYPE prometheus_engine_query_duration_seconds summary\nprometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 8.075e-05\nprometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.000917449\nprometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.009315769\nprometheus_engine_query_duration_seconds_sum{slice=\"inner_eval\"} 12506.67007997419\nprometheus_engine_query_duration_seconds_count{slice=\"inner_eval\"} 8.714071e+06\nprometheus_engine_query_duration_seconds{slice=\"prepare_time\",quantile=\"0.5\"} 2.228e-05\nprometheus_engine_query_duration_seconds{slice=\"prepare_time\",quantile=\"0.9\"} 6.2819e-05\nprometheus_engine_query_duration_seconds{slice=\"prepare_time\",quantile=\"0.99\"} 0.000399637\nprometheus_engine_query_duration_seconds_sum{slice=\"prepare_time\"} 490.326247638047\nprometheus_engine_query_duration_seconds_count{slice=\"prepare_time\"} 8.714071e+06\nprometheus_engine_query_duration_seconds{slice=\"queue_time\",quantile=\"0.5\"} 4.628e-06\nprometheus_engine_query_duration_seconds{slice=\"queue_time\",quantile=\"0.9\"} 1.6082e-05\nprometheus_engine_query_duration_seconds{slice=\"queue_time\",quantile=\"0.99\"} 4.1174e-05\nprometheus_engine_query_duration_seconds_sum{slice=\"queue_time\"} 8720.662071224393\nprometheus_engine_query_duration_seconds_count{slice=\"queue_time\"} 1.7428526e+07\nprometheus_engine_query_duration_seconds{slice=\"result_sort\",quantile=\"0.5\"} 7.83e-07\nprometheus_engine_query_duration_seconds{slice=\"result_sort\",quantile=\"0.9\"} 1.994e-06\nprometheus_engine_query_duration_seconds{slice=\"result_sort\",quantile=\"0.99\"} 1.458e-05\nprometheus_engine_query_duration_seconds_sum{slice=\"result_sort\"} 2.337879348999977\nprometheus_engine_query_duration_seconds_count{slice=\"result_sort\"} 1.208154e+06\n# HELP prometheus_engine_query_log_enabled State of the query log.\n# TYPE prometheus_engine_query_log_enabled gauge\nprometheus_engine_query_log_enabled 0\n# HELP prometheus_engine_query_log_failures_total The number of query log failures.\n# TYPE prometheus_engine_query_log_failures_total counter\nprometheus_engine_query_log_failures_total 0\n# HELP prometheus_engine_query_samples_total The total number of samples loaded by all queries.\n# TYPE prometheus_engine_query_samples_total counter\nprometheus_engine_query_samples_total 9.1183747239e+10\n# HELP prometheus_http_request_duration_seconds Histogram of latencies for HTTP requests.\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Data Structure in Go for Prometheus\nDESCRIPTION: Defines the 'sample' struct used as the primary data structure for handling time series data in Prometheus. It includes a map of labels and a value of any type.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/template_reference.md#2025-04-16_snippet_0\n\nLANGUAGE: go\nCODE:\n```\ntype sample struct {\n        Labels map[string]string\n        Value  interface{}\n}\n```\n\n----------------------------------------\n\nTITLE: Enable Extra Scrape Metrics Flag\nDESCRIPTION: Command flag to enable additional scrape metrics collection\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=extra-scrape-metrics\n```\n\n----------------------------------------\n\nTITLE: Defining Service Discovery Config Interface in Go\nDESCRIPTION: Core interface definition for implementing service discovery configurations in Prometheus. Includes the Config interface for naming and creating discoverers, and DiscovererOptions struct for configuration options.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/discovery/README.md#2025-04-16_snippet_4\n\nLANGUAGE: go\nCODE:\n```\ntype Config interface {\n\t// Name returns the name of the discovery mechanism.\n\tName() string\n\n\t// NewDiscoverer returns a Discoverer for the Config\n\t// with the given DiscovererOptions.\n\tNewDiscoverer(DiscovererOptions) (Discoverer, error)\n}\n\ntype DiscovererOptions struct {\n\tLogger *slog.Logger\n\n\t// A registerer for the Discoverer's metrics.\n\tRegisterer prometheus.Registerer\n\t\n\tHTTPClientOptions []config.HTTPClientOption\n}\n```\n\n----------------------------------------\n\nTITLE: Log Format Example - New Format (Prometheus v3)\nDESCRIPTION: Example of the new logging format used in Prometheus v3 with log/slog showing updated timestamp format, source path, and additional system information.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/migration.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ntime=2024-10-24T00:03:07.542+02:00 level=INFO source=/home/user/go/src/github.com/prometheus/prometheus/cmd/prometheus/main.go:640 msg=\"No time or size retention was set so using the default time retention\" duration=15d\ntime=2024-10-24T00:03:07.542+02:00 level=INFO source=/home/user/go/src/github.com/prometheus/prometheus/cmd/prometheus/main.go:681 msg=\"Starting Prometheus Server\" mode=server version=\"(version=, branch=, revision=7c7116fea8343795cae6da42960cacd0207a2af8)\"\ntime=2024-10-24T00:03:07.542+02:00 level=INFO source=/home/user/go/src/github.com/prometheus/prometheus/cmd/prometheus/main.go:686 msg=\"operational information\" build_context=\"(go=go1.23.0, platform=linux/amd64, user=, date=, tags=unknown)\" host_details=\"(Linux 5.15.0-124-generic #134-Ubuntu SMP Fri Sep 27 20:20:17 UTC 2024 x86_64 gigafips (none))\" fd_limits=\"(soft=1048576, hard=1048576)\" vm_limits=\"(soft=unlimited, hard=unlimited)\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Vite Proxy for External Prometheus Server\nDESCRIPTION: Vite configuration example showing how to proxy API requests from the development server to an external Prometheus server, including HTTPS support with origin change settings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/README.md#2025-04-16_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { defineConfig } from \"vite\";\nimport react from \"@vitejs/plugin-react\";\n\n// https://vitejs.dev/config/\nexport default defineConfig({\n  base: '',\n  plugins: [react()],\n  server: {\n    proxy: {\n      \"/api\": {\n        target: \"https://prometheus.demo.prometheus.io/\",\n        changeOrigin: true,\n      },\n      \"/-/\": {\n        target: \"https://prometheus.demo.prometheus.io/\",\n        changeOrigin: true,\n      },\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing CodeMirror-promql Package with npm\nDESCRIPTION: Command to install the CodeMirror-promql package using npm as a dependency for your project.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save @prometheus-io/codemirror-promql\n```\n\n----------------------------------------\n\nTITLE: Installing Required CodeMirror Peer Dependencies\nDESCRIPTION: Command to install the necessary CodeMirror peer dependencies required for the promql extension to function correctly.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save @codemirror/autocomplete @codemirror/language @codemirror/lint @codemirror/state @codemirror/view @lezer/common\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Requests in Prometheus\nDESCRIPTION: HTTP configuration for Prometheus, including authentication methods (basic auth, authorization headers, OAuth 2.0), redirect following, HTTP/2 settings, TLS configuration, and proxy settings.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n# Sets the `Authorization` header on every request with the\n# configured username and password.\n# username and username_file are mutually exclusive.\n# password and password_file are mutually exclusive.\nbasic_auth:\n  [ username: <string> ]\n  [ username_file: <string> ]\n  [ password: <secret> ]\n  [ password_file: <string> ]\n\n# Sets the `Authorization` header on every request with\n# the configured credentials.\nauthorization:\n  # Sets the authentication type of the request.\n  [ type: <string> | default: Bearer ]\n  # Sets the credentials of the request. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials: <secret> ]\n  # Sets the credentials of the request with the credentials read from the\n  # configured file. It is mutually exclusive with `credentials`.\n  [ credentials_file: <filename> ]\n\n# Optional OAuth 2.0 configuration.\n# Cannot be used at the same time as basic_auth or authorization.\noauth2:\n  [ <oauth2> ]\n\n# Configure whether requests follow HTTP 3xx redirects.\n[ follow_redirects: <boolean> | default = true ]\n\n# Whether to enable HTTP2.\n[ enable_http2: <boolean> | default: true ]\n\n# Configures the request's TLS settings.\ntls_config:\n  [ <tls_config> ]\n\n# Optional proxy URL.\n[ proxy_url: <string> ]\n# Comma-separated string that can contain IPs, CIDR notation, domain names\n# that should be excluded from proxying. IP and domain names can\n# contain port numbers.\n[ no_proxy: <string> ]\n# Use proxy URL indicated by environment variables (HTTP_PROXY, https_proxy, HTTPs_PROXY, https_proxy, and no_proxy)\n[ proxy_from_environment: <boolean> | default: false ]\n# Specifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ <string>: [<secret>, ...] ] ]\n\n# Custom HTTP headers to be sent along with each request.\n# Headers that are set by Prometheus itself can't be overwritten.\nhttp_headers:\n  # Header name.\n  [ <string>:\n    # Header values.\n    [ values: [<string>, ...] ]\n    # Headers values. Hidden in configuration page.\n    [ secrets: [<secret>, ...] ]\n    # Files to read header values from.\n    [ files: [<string>, ...] ] ]\n```\n\n----------------------------------------\n\nTITLE: Prometheus HTTP Duration Metrics\nDESCRIPTION: Time-series metrics data showing HTTP request duration statistics for Prometheus API endpoints. Includes histogram buckets for duration ranges, total request counts, and sum of durations for each endpoint.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_6\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/rules\"} 629.5237672670012\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/rules\"} 88662\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/scrape_pools\",le=\"0.1\"} 142\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/scrape_pools\",le=\"0.2\"} 142\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/scrape_pools\",le=\"0.4\"} 142\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/scrape_pools\",le=\"1\"} 142\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/scrape_pools\",le=\"3\"} 142\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/scrape_pools\",le=\"8\"} 142\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/scrape_pools\",le=\"20\"} 142\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/scrape_pools\",le=\"60\"} 142\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/scrape_pools\",le=\"120\"} 142\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/scrape_pools\",le=\"+Inf\"} 142\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/scrape_pools\"} 0.17501777799999996\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/scrape_pools\"} 142\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration\nDESCRIPTION: Basic YAML front matter configuration block defining the page title and sort order for documentation.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/index.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Configuration\nsort_rank: 3\n---\n```\n\n----------------------------------------\n\nTITLE: PromQL Invalid NaN and Inf Tests\nDESCRIPTION: Examples showing illegal usage of NaN and Inf as metric names with label matchers\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/test/expression.txt#2025-04-16_snippet_7\n\nLANGUAGE: promql\nCODE:\n```\nNaN{foo=\"bar\"}\n\nInf{foo=\"bar\"}\n```\n\n----------------------------------------\n\nTITLE: HTTP Request Duration Metrics for Prometheus API Endpoints\nDESCRIPTION: Prometheus histogram metrics showing request duration statistics across different API endpoints. Includes bucket counts for various duration thresholds (from 0.1s to +Inf) as well as sum and count values for each endpoint.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_3\n\nLANGUAGE: prometheus-metrics\nCODE:\n```\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/parse_query\",le=\"8\"} 265\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/parse_query\",le=\"20\"} 265\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/parse_query\",le=\"60\"} 265\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/parse_query\",le=\"120\"} 265\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/parse_query\",le=\"+Inf\"} 265\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/parse_query\"} 0.7265021480000003\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/parse_query\"} 265\n```\n\n----------------------------------------\n\nTITLE: PromQL Metric Name and Function Tests\nDESCRIPTION: Examples of metric names and binary expressions with functions\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/test/expression.txt#2025-04-16_snippet_9\n\nLANGUAGE: promql\nCODE:\n```\nsum:my_metric_name:rate5m\n\n1 + foo atan2 bar\n```\n\n----------------------------------------\n\nTITLE: Exporting Prometheus Target Metrics in Prometheus Format\nDESCRIPTION: A sample of Prometheus metrics in exposition format showing various target-related statistics. This includes cache usage metrics, scrape pool information, target counts, and error counters across different scrape jobs like alertmanager, blackbox, caddy, cadvisor, grafana, node, prometheus, and random.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_26\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE prometheus_target_metadata_cache_bytes gauge\nprometheus_target_metadata_cache_bytes{scrape_job=\"alertmanager\"} 6817\nprometheus_target_metadata_cache_bytes{scrape_job=\"blackbox\"} 661\nprometheus_target_metadata_cache_bytes{scrape_job=\"caddy\"} 2365\nprometheus_target_metadata_cache_bytes{scrape_job=\"cadvisor\"} 4547\nprometheus_target_metadata_cache_bytes{scrape_job=\"grafana\"} 37608\nprometheus_target_metadata_cache_bytes{scrape_job=\"node\"} 13900\nprometheus_target_metadata_cache_bytes{scrape_job=\"prometheus\"} 20265\nprometheus_target_metadata_cache_bytes{scrape_job=\"random\"} 792\n# HELP prometheus_target_metadata_cache_entries Total number of metric metadata entries in the cache\n# TYPE prometheus_target_metadata_cache_entries gauge\nprometheus_target_metadata_cache_entries{scrape_job=\"alertmanager\"} 86\nprometheus_target_metadata_cache_entries{scrape_job=\"blackbox\"} 13\nprometheus_target_metadata_cache_entries{scrape_job=\"caddy\"} 47\nprometheus_target_metadata_cache_entries{scrape_job=\"cadvisor\"} 93\nprometheus_target_metadata_cache_entries{scrape_job=\"grafana\"} 373\nprometheus_target_metadata_cache_entries{scrape_job=\"node\"} 293\nprometheus_target_metadata_cache_entries{scrape_job=\"prometheus\"} 237\nprometheus_target_metadata_cache_entries{scrape_job=\"random\"} 16\n# HELP prometheus_target_scrape_pool_exceeded_label_limits_total Total number of times scrape pools hit the label limits, during sync or config reload.\n# TYPE prometheus_target_scrape_pool_exceeded_label_limits_total counter\nprometheus_target_scrape_pool_exceeded_label_limits_total 0\n# HELP prometheus_target_scrape_pool_exceeded_target_limit_total Total number of times scrape pools hit the target limit, during sync or config reload.\n# TYPE prometheus_target_scrape_pool_exceeded_target_limit_total counter\nprometheus_target_scrape_pool_exceeded_target_limit_total 0\n# HELP prometheus_target_scrape_pool_reloads_failed_total Total number of failed scrape pool reloads.\n# TYPE prometheus_target_scrape_pool_reloads_failed_total counter\nprometheus_target_scrape_pool_reloads_failed_total 0\n# HELP prometheus_target_scrape_pool_reloads_total Total number of scrape pool reloads.\n# TYPE prometheus_target_scrape_pool_reloads_total counter\nprometheus_target_scrape_pool_reloads_total 0\n# HELP prometheus_target_scrape_pool_symboltable_items Current number of symbols in table for this scrape pool.\n# TYPE prometheus_target_scrape_pool_symboltable_items gauge\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"alertmanager\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"blackbox\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"caddy\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"cadvisor\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"grafana\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"node\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"prometheus\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"random\"} 0\n# HELP prometheus_target_scrape_pool_sync_total Total number of syncs that were executed on a scrape pool.\n# TYPE prometheus_target_scrape_pool_sync_total counter\nprometheus_target_scrape_pool_sync_total{scrape_job=\"alertmanager\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"blackbox\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"caddy\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"cadvisor\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"grafana\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"node\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"prometheus\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"random\"} 2953\n# HELP prometheus_target_scrape_pool_target_limit Maximum number of targets allowed in this scrape pool.\n# TYPE prometheus_target_scrape_pool_target_limit gauge\nprometheus_target_scrape_pool_target_limit{scrape_job=\"alertmanager\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"blackbox\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"caddy\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"cadvisor\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"grafana\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"node\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"prometheus\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"random\"} 0\n# HELP prometheus_target_scrape_pool_targets Current number of targets in this scrape pool.\n# TYPE prometheus_target_scrape_pool_targets gauge\nprometheus_target_scrape_pool_targets{scrape_job=\"alertmanager\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"blackbox\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"caddy\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"cadvisor\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"grafana\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"node\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"prometheus\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"random\"} 4\n# HELP prometheus_target_scrape_pools_failed_total Total number of scrape pool creations that failed.\n# TYPE prometheus_target_scrape_pools_failed_total counter\nprometheus_target_scrape_pools_failed_total 0\n# HELP prometheus_target_scrape_pools_total Total number of scrape pool creation attempts.\n# TYPE prometheus_target_scrape_pools_total counter\nprometheus_target_scrape_pools_total 8\n# HELP prometheus_target_scrapes_cache_flush_forced_total How many times a scrape cache was flushed due to getting big while scrapes are failing.\n# TYPE prometheus_target_scrapes_cache_flush_forced_total counter\nprometheus_target_scrapes_cache_flush_forced_total 0\n# HELP prometheus_target_scrapes_exceeded_body_size_limit_total Total number of scrapes that hit the body size limit\n# TYPE prometheus_target_scrapes_exceeded_body_size_limit_total counter\nprometheus_target_scrapes_exceeded_body_size_limit_total 0\n# HELP prometheus_target_scrapes_exceeded_native_histogram_bucket_limit_total Total number of scrapes that hit the native histogram bucket limit and were rejected.\n# TYPE prometheus_target_scrapes_exceeded_native_histogram_bucket_limit_total counter\nprometheus_target_scrapes_exceeded_native_histogram_bucket_limit_total 0\n# HELP prometheus_target_scrapes_exceeded_sample_limit_total Total number of scrapes that hit the sample limit and were rejected.\n# TYPE prometheus_target_scrapes_exceeded_sample_limit_total counter\nprometheus_target_scrapes_exceeded_sample_limit_total 0\n# HELP prometheus_target_scrapes_exemplar_out_of_order_total Total number of exemplar rejected due to not being out of the expected order.\n# TYPE prometheus_target_scrapes_exemplar_out_of_order_total counter\nprometheus_target_scrapes_exemplar_out_of_order_total 0\n# HELP prometheus_target_scrapes_sample_duplicate_timestamp_total Total number of samples rejected due to duplicate timestamps but different values.\n# TYPE prometheus_target_scrapes_sample_duplicate_timestamp_total counter\nprometheus_target_scrapes_sample_duplicate_timestamp_total 0\n# HELP prometheus_target_scrapes_sample_out_of_bounds_total Total number of samples rejected due to timestamp falling outside of the time bounds.\n# TYPE prometheus_target_scrapes_sample_out_of_bounds_total counter\nprometheus_target_scrapes_sample_out_of_bounds_total 0\n# HELP prometheus_target_scrapes_sample_out_of_order_total Total number of samples rejected due to not being out of the expected order.\n# TYPE prometheus_target_scrapes_sample_out_of_order_total counter\nprometheus_target_scrapes_sample_out_of_order_total 455\n# HELP prometheus_target_sync_failed_total Total number of target sync failures.\n# TYPE prometheus_target_sync_failed_total counter\nprometheus_target_sync_failed_total{scrape_job=\"alertmanager\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"blackbox\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"caddy\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"cadvisor\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"grafana\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"node\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"prometheus\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"random\"} 0\n# HELP prometheus_target_sync_length_seconds Actual interval to sync the scrape pool.\n```\n\n----------------------------------------\n\nTITLE: Prometheus Release Changelog Entry 2.51.2\nDESCRIPTION: Bugfix release notes for version 2.51.2 addressing a notifier issue that could cause hanging when using relabeling on alerts.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## 2.51.2 / 2024-04-09\n\nBugfix release.\n\n[BUGFIX] Notifier: could hang when using relabeling on alerts #13861\n```\n\n----------------------------------------\n\nTITLE: Adding HTTP Error Handler for Prometheus API Requests\nDESCRIPTION: Configuration to add a custom error handler for HTTP errors that might occur when communicating with the Prometheus API.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst promQL = new PromQLExtension().setComplete({remote: {httpErrorHandler: (error: any) => console.error(error)}})\n```\n\n----------------------------------------\n\nTITLE: Health Check Endpoint in Prometheus HTTP API\nDESCRIPTION: Endpoint that always returns 200 status code and is used to verify if Prometheus is healthy. This can be accessed via either GET or HEAD HTTP methods.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/management_api.md#2025-04-16_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET /-/healthy\nHEAD /-/healthy\n```\n\n----------------------------------------\n\nTITLE: Querying WAL Replay Statistics in Prometheus\nDESCRIPTION: This endpoint returns information about the Write-Ahead Log (WAL) replay process, including progress metrics and current state. It provides real-time monitoring of the replay progress and is available before the server is marked ready.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_33\n\nLANGUAGE: json\nCODE:\n```\n$ curl http://localhost:9090/api/v1/status/walreplay\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"min\": 2,\n    \"max\": 5,\n    \"current\": 40,\n    \"state\": \"in progress\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Mixin Dependencies\nDESCRIPTION: Command to install the required dependencies for the Prometheus Mixin using the jsonnet-bundler (jb) tool.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/prometheus-mixin/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ jb install\n```\n\n----------------------------------------\n\nTITLE: Example Response for Alertmanagers Status\nDESCRIPTION: Sample response showing the active and dropped Alertmanagers in the Prometheus setup.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\n$ curl http://localhost:9090/api/v1/alertmanagers\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"activeAlertmanagers\": [\n      {\n        \"url\": \"http://127.0.0.1:9090/api/v1/alerts\"\n      }\n    ],\n    \"droppedAlertmanagers\": [\n      {\n        \"url\": \"http://127.0.0.1:9093/api/v1/alerts\"\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: PromQL Offset Expression Tests\nDESCRIPTION: Examples of offset expressions with positive and negative duration values\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/test/expression.txt#2025-04-16_snippet_8\n\nLANGUAGE: promql\nCODE:\n```\nfoo offset -5d\n\nfoo offset - 5d\n\nfoo offset 5d\n```\n\n----------------------------------------\n\nTITLE: Accessing stripeSeries using HeadSeriesRef in Prometheus TSDB\nDESCRIPTION: This code snippet shows the usage of HeadSeriesRef in accessing stripeSeries within the Prometheus TSDB head block. HeadSeriesRef is a 64-bit counter that increments for each new series.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/refs.md#2025-04-16_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nstripeSeries\n```\n\n----------------------------------------\n\nTITLE: PromQL Functions and Time Operations\nDESCRIPTION: Examples of PromQL functions including last_over_time, sgn, and clamp, plus time-based operations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/test/expression.txt#2025-04-16_snippet_4\n\nLANGUAGE: PromQL\nCODE:\n```\nlast_over_time(data[1m])\nsgn(data)\nclamp(data,0,1)\n```\n\n----------------------------------------\n\nTITLE: Example Response for Runtime Information\nDESCRIPTION: Sample response showing runtime properties of the Prometheus server including start time, working directory, server time, configuration status, and resource usage.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\n$ curl http://localhost:9090/api/v1/status/runtimeinfo\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"startTime\": \"2019-11-02T17:23:59.301361365+01:00\",\n    \"CWD\": \"/\",\n    \"hostname\" : \"DESKTOP-717H17Q\",\n    \"serverTime\": \"2025-01-05T18:27:33Z\",\n    \"reloadConfigSuccess\": true,\n    \"lastConfigTime\": \"2019-11-02T17:23:59+01:00\",\n    \"timeSeriesCount\": 873,\n    \"corruptionCount\": 0,\n    \"goroutineCount\": 48,\n    \"GOMAXPROCS\": 4,\n    \"GOGC\": \"\",\n    \"GODEBUG\": \"\",\n    \"storageRetention\": \"15d\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Expect Syntax for Query Evaluation\nDESCRIPTION: Defines the syntax for expect lines used to validate query behaviors like errors, annotations, and result ordering with different matching types.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/promql/promqltest/README.md#2025-04-16_snippet_4\n\nLANGUAGE: promql\nCODE:\n```\nexpect <type> <match_type> <string>\n```\n\n----------------------------------------\n\nTITLE: Initializing Cache with Predefined Metric List\nDESCRIPTION: Configuration to pre-populate the cache with a list of metric names, useful when metrics are already available elsewhere in the application.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nconst promQL = new PromQLExtension().setComplete({\n    remote: {\n        cache: {\n            initialMetricList: [\n                'ALERTS',\n                'ALERTS_FOR_STATE',\n                'alertmanager_alerts',\n                'alertmanager_alerts_invalid_total',\n                'alertmanager_alerts_received_total',\n            ]\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Displaying Prometheus Data Directory Structure\nDESCRIPTION: Shows the typical directory structure of Prometheus's data storage, including block directories, chunks, tombstones, and write-ahead log files.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/storage.md#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n./data\n 01BKGV7JBM69T2G1BGBGM6KB12\n    meta.json\n 01BKGTZQ1SYQJTR4PB43C8PD98\n    chunks\n       000001\n    tombstones\n    index\n    meta.json\n 01BKGTZQ1HHWHV8FBJXW1Y3W0K\n    meta.json\n 01BKGV7JC0RY8A6MACW02A2PJD\n    chunks\n       000001\n    tombstones\n    index\n    meta.json\n chunks_head\n    000001\n wal\n     000000002\n     checkpoint.00000001\n         00000000\n```\n\n----------------------------------------\n\nTITLE: PromQL Special Values and Modifiers\nDESCRIPTION: Examples showing special values like NaN and Inf, along with various modifiers and time-based operations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/test/expression.txt#2025-04-16_snippet_5\n\nLANGUAGE: PromQL\nCODE:\n```\nNaN\nInf\n-Inf\n+Inf\nfoo @ start()\nfoo @ end()\nfoo @ 1234\n```\n\n----------------------------------------\n\nTITLE: Load Command with Series Data Example\nDESCRIPTION: Demonstrates the load command syntax with a metric series containing various data point types including regular values, expanded sequences, missing values, stale markers, and native histograms.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/promql/promqltest/README.md#2025-04-16_snippet_1\n\nLANGUAGE: promql\nCODE:\n```\nload 1m\n    my_metric{env=\"prod\"} 5 2+3x2 _ stale {{schema:1 sum:3 count:22 buckets:[5 10 7]}}\n```\n\n----------------------------------------\n\nTITLE: Monitoring HTTP Response Size Metrics in Prometheus\nDESCRIPTION: This dataset represents Prometheus metrics for HTTP response sizes across various API endpoints and static asset handlers. The data is organized in histogram buckets with different size thresholds (from 100 bytes to +Inf), along with sum and count aggregations for each endpoint.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_14\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/buildinfo\",le=\"1e+07\"} 4413\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/buildinfo\",le=\"1e+08\"} 4413\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/buildinfo\",le=\"1e+09\"} 4413\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/buildinfo\",le=\"+Inf\"} 4413\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/status/buildinfo\"} 807579\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/status/buildinfo\"} 4413\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"100\"} 1\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"1000\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"10000\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"100000\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"1e+06\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"1e+07\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"1e+08\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"1e+09\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"+Inf\"} 84723\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/status/config\"} 6.3710963e+07\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/status/config\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"1000\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"10000\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"100000\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"1e+06\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"1e+07\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"1e+08\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"1e+09\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"+Inf\"} 32\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/status/flags\"} 31968\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/status/flags\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"1000\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"10000\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"100000\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"1e+06\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"1e+07\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"1e+08\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"1e+09\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"+Inf\"} 863\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/status/runtimeinfo\"} 240400\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/status/runtimeinfo\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"1000\"} 92\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"10000\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"100000\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"1e+06\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"1e+07\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"1e+08\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"1e+09\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"+Inf\"} 94\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/status/tsdb\"} 66441\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/status/tsdb\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"100\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"1000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"10000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"100000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"1e+06\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"1e+07\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"1e+08\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"1e+09\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"+Inf\"} 49\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/status/walreplay\"} 3381\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/status/walreplay\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"1000\"} 184\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"10000\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"100000\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"1e+06\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"1e+07\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"1e+08\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"1e+09\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"+Inf\"} 191\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/targets\"} 191150\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/targets\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"100\"} 1\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"1000\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"10000\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"100000\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"1e+06\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"1e+07\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"1e+08\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"1e+09\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"+Inf\"} 18\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/targets/metadata\"} 2736\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/targets/metadata\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"100\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"1000\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"10000\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"100000\"} 142\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"1e+06\"} 958\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"1e+07\"} 2225\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"1e+08\"} 2225\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"1e+09\"} 2225\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"+Inf\"} 2225\nprometheus_http_response_size_bytes_sum{handler=\"/assets/*filepath\"} 3.525958804e+09\nprometheus_http_response_size_bytes_count{handler=\"/assets/*filepath\"} 2225\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"100\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"1000\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"10000\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"100000\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"1e+06\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"1e+07\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"1e+08\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"1e+09\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"+Inf\"} 103\nprometheus_http_response_size_bytes_sum{handler=\"/classic/static/*filepath\"} 1957\nprometheus_http_response_size_bytes_count{handler=\"/classic/static/*filepath\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"1000\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"10000\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"100000\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"1e+06\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"1e+07\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"1e+08\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"1e+09\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"+Inf\"} 13\n```\n\n----------------------------------------\n\nTITLE: Querying Build Information with GET API Endpoint\nDESCRIPTION: API endpoint for retrieving build information properties about the Prometheus server, available since v2.14.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\nGET /api/v1/status/buildinfo\n```\n\n----------------------------------------\n\nTITLE: Custom Service Discovery Implementation in Go\nDESCRIPTION: File path reference for implementing custom service discovery mechanism using Go\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/custom-sd/README.md#2025-04-16_snippet_1\n\nLANGUAGE: go\nCODE:\n```\nadapter-usage/main.go\n```\n\n----------------------------------------\n\nTITLE: Updating React Dependencies in Prometheus UI\nDESCRIPTION: Command to update npm dependencies for the Prometheus React application. This should be followed by verification that no additional node_modules directories were created and that the UI build still works.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/RELEASE.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake update-npm-deps\n```\n\n----------------------------------------\n\nTITLE: Prometheus Metrics Collection\nDESCRIPTION: A comprehensive collection of Prometheus metrics including Go runtime metrics, network connection tracking metrics, process statistics, and Prometheus-specific API metrics. The metrics use various types including histograms, counters, and gauges to track system performance and behavior.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_2\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE go_sched_pauses_total_other_seconds histogram\ngo_sched_pauses_total_other_seconds_bucket{le=\"6.399999999999999e-08\"} 0\ngo_sched_pauses_total_other_seconds_bucket{le=\"6.399999999999999e-07\"} 0\ngo_sched_pauses_total_other_seconds_bucket{le=\"7.167999999999999e-06\"} 0\ngo_sched_pauses_total_other_seconds_bucket{le=\"8.191999999999999e-05\"} 0\ngo_sched_pauses_total_other_seconds_bucket{le=\"0.0009175039999999999\"} 0\ngo_sched_pauses_total_other_seconds_bucket{le=\"0.010485759999999998\"} 0\ngo_sched_pauses_total_other_seconds_bucket{le=\"0.11744051199999998\"} 0\ngo_sched_pauses_total_other_seconds_bucket{le=\"+Inf\"} 0\ngo_sched_pauses_total_other_seconds_sum 0\ngo_sched_pauses_total_other_seconds_count 0\n# HELP go_sync_mutex_wait_total_seconds_total Approximate cumulative time goroutines have spent blocked on a sync.Mutex, sync.RWMutex, or runtime-internal lock.\ngo_sync_mutex_wait_total_seconds_total 628.29966272\n# HELP go_threads Number of OS threads created.\ngo_threads 10\n# HELP net_conntrack_dialer_conn_attempted_total Total number of connections attempted by the given dialer a given name.\nnet_conntrack_dialer_conn_attempted_total{dialer_name=\"alertmanager\"} 2\n[...additional metrics truncated for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure\nDESCRIPTION: Top-level markdown structure defining the main navigation and sections of the Prometheus server documentation\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/index.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\n# todo: internal\n---\n\n# Prometheus\n\nWelcome to the documentation of the Prometheus server.\n\nThe documentation is available alongside all the project documentation at\n[prometheus.io](https://prometheus.io/docs/prometheus/latest/).\n\n## Content\n\n- [Getting started](getting_started.md)\n- [Installation](installation.md)\n- [Configuration](configuration/configuration.md)\n- [Querying](querying/basics.md)\n- [Storage](storage.md)\n- [Federation](federation.md)\n- [Migration](migration.md)\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Log File Path in PromQL\nDESCRIPTION: The query_log_file path is now relative to the config file in PromQL.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_19\n\nLANGUAGE: go\nCODE:\n```\n// PromQL: `query_log_file` path is now relative to the config file\n```\n\n----------------------------------------\n\nTITLE: Range Query Evaluation Syntax\nDESCRIPTION: Shows the structure for evaluating a range query with start and end times and step interval, including optional expectation lines and expected result series.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/promql/promqltest/README.md#2025-04-16_snippet_3\n\nLANGUAGE: promql\nCODE:\n```\n# Range query\neval range from <start> to <end> step <step> <query>\n    <expect>\n    ...\n    <expect>\n    <series> <points>\n    ...\n    <series> <points>\n```\n\n----------------------------------------\n\nTITLE: Updating Go Dependencies in Prometheus\nDESCRIPTION: Command to update all Go dependencies in the Prometheus project. This is typically needed for '+incompatible' and 'v0.0.0' non-semver updates.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/RELEASE.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake update-all-go-deps\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write v2\nDESCRIPTION: YAML configuration for setting up remote write with Protobuf v2 message format\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/remote_storage/example_write_adapter/README.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nremote_write:\n  - url: \"http://localhost:1234/receive\"\n    protobuf_message: \"io.prometheus.write.v2.Request\"\n```\n\n----------------------------------------\n\nTITLE: Running with OpenTSDB Backend\nDESCRIPTION: Command to run the adapter with OpenTSDB as the remote storage backend, specifying the OpenTSDB URL.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/remote_storage/remote_storage_adapter/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./remote_storage_adapter --opentsdb-url=http://localhost:8081/\n```\n\n----------------------------------------\n\nTITLE: Git Alias Configuration for Release Tagging\nDESCRIPTION: Git alias configuration that creates a convenience command for tagging releases. The alias includes signing and pushing the tag in one command.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/RELEASE.md#2025-04-16_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[alias]\n  tag-release = \"!f() { tag=v${1:-$(cat VERSION)} ; git tag -s ${tag} -m ${tag} && git push origin ${tag}; }; f\"\n```\n\n----------------------------------------\n\nTITLE: Tagging Library Release in Bash\nDESCRIPTION: Commands to create and push a signed git tag for a Prometheus library release. The tag is generated using a script that determines the module version.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/RELEASE.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntag=\"v$(./scripts/get_module_version.sh)\"\ngit tag -s \"${tag}\" -m \"${tag}\"\ngit push origin \"${tag}\"\n```\n\n----------------------------------------\n\nTITLE: Monitoring Prometheus Rule Group Performance\nDESCRIPTION: These metrics show rule group performance statistics including duration, interval settings, missed iterations, and execution counts for various rule groups.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_16\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_rule_group_duration_seconds{quantile=\"0.01\"} 0.000735928\nprometheus_rule_group_duration_seconds{quantile=\"0.05\"} 0.000818857\nprometheus_rule_group_duration_seconds{quantile=\"0.5\"} 0.004852081\nprometheus_rule_group_duration_seconds{quantile=\"0.9\"} 0.022897759\nprometheus_rule_group_duration_seconds{quantile=\"0.99\"} 0.069327797\nprometheus_rule_group_duration_seconds_sum 5335.451440133042\nprometheus_rule_group_duration_seconds_count 472359\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 15\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 0\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 0\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 1\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 0\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 0\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 1\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 0\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 0\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 59046\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 59045\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 59045\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 59045\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 59045\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 59045\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 59045\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 59045\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 0.000754015\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 0.001104624\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 0.022040842\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 0.048928087\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 0.002766703\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 0.002218466\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Metrics Metadata Limit\nDESCRIPTION: Configuration to set the maximum number of metrics for which metadata is fetched, to prevent browser performance issues with large Prometheus instances.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst promQL = new PromQLExtension().setComplete({maxMetricsMetadata: 10000})\n```\n\n----------------------------------------\n\nTITLE: Example of Instant Query with Ordered Results Expectation\nDESCRIPTION: Shows an example of a query where the results are expected to be returned in a specific order.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/promql/promqltest/README.md#2025-04-16_snippet_10\n\nLANGUAGE: promql\nCODE:\n```\neval instant at 1m sum by (env) (my_metric)\nexpect ordered\n{env=\"prod\"} 5\n{env=\"test\"} 20\n```\n\n----------------------------------------\n\nTITLE: Sample Records Structure in Prometheus WAL\nDESCRIPTION: Diagram showing the encoding format for Sample records which contain series ID, timestamp, and value. The first row stores the starting ID and timestamp, with subsequent rows storing delta-encoded values relative to the first sample.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/wal.md#2025-04-16_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n\n type = 2 <1b>                                                    \n\n                \n  id <8b>             timestamp <8b>                           \n                \n  \n  id_delta <uvarint>  timestamp_delta <uvarint>  value <8b>   \n  \n                              . . .                               \n\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write v1\nDESCRIPTION: YAML configuration for setting up remote write with deprecated Protobuf v1 message format\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/remote_storage/example_write_adapter/README.md#2025-04-16_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nremote_write:\n  - url: \"http://localhost:1234/receive\"\n    protobuf_message: \"prometheus.WriteRequest\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Prometheus Target Sync Metrics in Exposition Format\nDESCRIPTION: Shows the prometheus_target_sync_length_seconds metrics for various scrape jobs including alertmanager, blackbox, caddy, and others. The data includes quantile measurements, sums, and counts for target sync operations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_27\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE prometheus_target_sync_length_seconds summary\nprometheus_target_sync_length_seconds{scrape_job=\"alertmanager\",quantile=\"0.01\"} 2.0522e-05\nprometheus_target_sync_length_seconds{scrape_job=\"alertmanager\",quantile=\"0.05\"} 2.0522e-05\nprometheus_target_sync_length_seconds{scrape_job=\"alertmanager\",quantile=\"0.5\"} 2.0522e-05\nprometheus_target_sync_length_seconds{scrape_job=\"alertmanager\",quantile=\"0.9\"} 0.000141485\nprometheus_target_sync_length_seconds{scrape_job=\"alertmanager\",quantile=\"0.99\"} 0.000141485\nprometheus_target_sync_length_seconds_sum{scrape_job=\"alertmanager\"} 0.13103036\nprometheus_target_sync_length_seconds_count{scrape_job=\"alertmanager\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"blackbox\",quantile=\"0.01\"} 3.9252e-05\nprometheus_target_sync_length_seconds{scrape_job=\"blackbox\",quantile=\"0.05\"} 3.9252e-05\nprometheus_target_sync_length_seconds{scrape_job=\"blackbox\",quantile=\"0.5\"} 3.9252e-05\nprometheus_target_sync_length_seconds{scrape_job=\"blackbox\",quantile=\"0.9\"} 6.2134e-05\nprometheus_target_sync_length_seconds{scrape_job=\"blackbox\",quantile=\"0.99\"} 6.2134e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"blackbox\"} 0.6044201539999996\nprometheus_target_sync_length_seconds_count{scrape_job=\"blackbox\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"caddy\",quantile=\"0.01\"} 1.3759e-05\nprometheus_target_sync_length_seconds{scrape_job=\"caddy\",quantile=\"0.05\"} 1.3759e-05\nprometheus_target_sync_length_seconds{scrape_job=\"caddy\",quantile=\"0.5\"} 1.3759e-05\nprometheus_target_sync_length_seconds{scrape_job=\"caddy\",quantile=\"0.9\"} 7.8256e-05\nprometheus_target_sync_length_seconds{scrape_job=\"caddy\",quantile=\"0.99\"} 7.8256e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"caddy\"} 0.10369844599999971\nprometheus_target_sync_length_seconds_count{scrape_job=\"caddy\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"cadvisor\",quantile=\"0.01\"} 2.0452e-05\nprometheus_target_sync_length_seconds{scrape_job=\"cadvisor\",quantile=\"0.05\"} 2.0452e-05\nprometheus_target_sync_length_seconds{scrape_job=\"cadvisor\",quantile=\"0.5\"} 2.0452e-05\nprometheus_target_sync_length_seconds{scrape_job=\"cadvisor\",quantile=\"0.9\"} 4.2337e-05\nprometheus_target_sync_length_seconds{scrape_job=\"cadvisor\",quantile=\"0.99\"} 4.2337e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"cadvisor\"} 0.10489659999999998\nprometheus_target_sync_length_seconds_count{scrape_job=\"cadvisor\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"grafana\",quantile=\"0.01\"} 1.4995e-05\nprometheus_target_sync_length_seconds{scrape_job=\"grafana\",quantile=\"0.05\"} 1.4995e-05\nprometheus_target_sync_length_seconds{scrape_job=\"grafana\",quantile=\"0.5\"} 1.4995e-05\nprometheus_target_sync_length_seconds{scrape_job=\"grafana\",quantile=\"0.9\"} 1.7284e-05\nprometheus_target_sync_length_seconds{scrape_job=\"grafana\",quantile=\"0.99\"} 1.7284e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"grafana\"} 0.09031192700000017\nprometheus_target_sync_length_seconds_count{scrape_job=\"grafana\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"node\",quantile=\"0.01\"} 4.1607e-05\nprometheus_target_sync_length_seconds{scrape_job=\"node\",quantile=\"0.05\"} 4.1607e-05\nprometheus_target_sync_length_seconds{scrape_job=\"node\",quantile=\"0.5\"} 4.1607e-05\nprometheus_target_sync_length_seconds{scrape_job=\"node\",quantile=\"0.9\"} 7.416e-05\nprometheus_target_sync_length_seconds{scrape_job=\"node\",quantile=\"0.99\"} 7.416e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"node\"} 0.11539821299999993\nprometheus_target_sync_length_seconds_count{scrape_job=\"node\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"prometheus\",quantile=\"0.01\"} 1.5564e-05\nprometheus_target_sync_length_seconds{scrape_job=\"prometheus\",quantile=\"0.05\"} 1.5564e-05\nprometheus_target_sync_length_seconds{scrape_job=\"prometheus\",quantile=\"0.5\"} 1.5564e-05\nprometheus_target_sync_length_seconds{scrape_job=\"prometheus\",quantile=\"0.9\"} 1.961e-05\nprometheus_target_sync_length_seconds{scrape_job=\"prometheus\",quantile=\"0.99\"} 1.961e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"prometheus\"} 0.10655758600000016\nprometheus_target_sync_length_seconds_count{scrape_job=\"prometheus\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"random\",quantile=\"0.01\"} 4.1299e-05\nprometheus_target_sync_length_seconds{scrape_job=\"random\",quantile=\"0.05\"} 4.1299e-05\nprometheus_target_sync_length_seconds{scrape_job=\"random\",quantile=\"0.5\"} 4.1299e-05\nprometheus_target_sync_length_seconds{scrape_job=\"random\",quantile=\"0.9\"} 4.8586e-05\nprometheus_target_sync_length_seconds{scrape_job=\"random\",quantile=\"0.99\"} 4.8586e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"random\"} 0.20406449899999993\nprometheus_target_sync_length_seconds_count{scrape_job=\"random\"} 2953\n```\n\n----------------------------------------\n\nTITLE: Displaying Prometheus Rule Group Metrics in Prometheus Exposition Format\nDESCRIPTION: A comprehensive set of Prometheus metrics showing various aspects of rule group evaluation performance. These metrics include interval configuration, missed iterations, total iterations, evaluation duration, sample counts, timestamps, restoration time, and rule duration sum for different rule groups.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_20\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE prometheus_rule_group_interval_seconds gauge\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 15\nprometheus_rule_group_interval_seconds{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 15\n# HELP prometheus_rule_group_iterations_missed_total The total number of rule group evaluations missed due to slow rule group evaluation.\n# TYPE prometheus_rule_group_iterations_missed_total counter\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 0\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 0\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 1\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 0\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 0\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 1\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 0\nprometheus_rule_group_iterations_missed_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 0\n# HELP prometheus_rule_group_iterations_total The total number of scheduled rule group evaluations, whether executed or missed.\n# TYPE prometheus_rule_group_iterations_total counter\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 59046\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 59045\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 59045\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 59045\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 59045\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 59045\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 59045\nprometheus_rule_group_iterations_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 59045\n# HELP prometheus_rule_group_last_duration_seconds The duration of the last rule group evaluation.\n# TYPE prometheus_rule_group_last_duration_seconds gauge\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 0.000754015\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 0.001104624\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 0.022040842\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 0.048928087\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 0.002766703\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 0.002218466\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 0.010245447\nprometheus_rule_group_last_duration_seconds{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 0.009232393\n# HELP prometheus_rule_group_last_evaluation_samples The number of samples returned during the last rule group evaluation.\n# TYPE prometheus_rule_group_last_evaluation_samples gauge\nprometheus_rule_group_last_evaluation_samples{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 2\nprometheus_rule_group_last_evaluation_samples{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 2\nprometheus_rule_group_last_evaluation_samples{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 10\nprometheus_rule_group_last_evaluation_samples{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 10\nprometheus_rule_group_last_evaluation_samples{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 11\nprometheus_rule_group_last_evaluation_samples{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 11\nprometheus_rule_group_last_evaluation_samples{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 0\nprometheus_rule_group_last_evaluation_samples{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 0\n# HELP prometheus_rule_group_last_evaluation_timestamp_seconds The timestamp of the last rule group evaluation in seconds.\n# TYPE prometheus_rule_group_last_evaluation_timestamp_seconds gauge\nprometheus_rule_group_last_evaluation_timestamp_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 1.738949373753179e+09\nprometheus_rule_group_last_evaluation_timestamp_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 1.738949366900781e+09\nprometheus_rule_group_last_evaluation_timestamp_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 1.7389493632087085e+09\nprometheus_rule_group_last_evaluation_timestamp_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 1.7389493666743164e+09\nprometheus_rule_group_last_evaluation_timestamp_seconds{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 1.7389493675395968e+09\nprometheus_rule_group_last_evaluation_timestamp_seconds{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 1.7389493623381927e+09\nprometheus_rule_group_last_evaluation_timestamp_seconds{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 1.738949366856423e+09\nprometheus_rule_group_last_evaluation_timestamp_seconds{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 1.738949369917499e+09\n# HELP prometheus_rule_group_last_restore_duration_seconds The duration of the last alert rules alerts restoration using the `ALERTS_FOR_STATE` series.\n# TYPE prometheus_rule_group_last_restore_duration_seconds gauge\nprometheus_rule_group_last_restore_duration_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 0.002032625\nprometheus_rule_group_last_restore_duration_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 9.1853e-05\nprometheus_rule_group_last_restore_duration_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 0.000166088\nprometheus_rule_group_last_restore_duration_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 0.000108127\nprometheus_rule_group_last_restore_duration_seconds{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 6.408e-06\nprometheus_rule_group_last_restore_duration_seconds{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 2.621e-06\nprometheus_rule_group_last_restore_duration_seconds{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 8.4979e-05\nprometheus_rule_group_last_restore_duration_seconds{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 0.000104444\n# HELP prometheus_rule_group_last_rule_duration_sum_seconds The sum of time in seconds it took to evaluate each rule in the group regardless of concurrency. This should be higher than the group duration if rules are evaluated concurrently.\n# TYPE prometheus_rule_group_last_rule_duration_sum_seconds gauge\nprometheus_rule_group_last_rule_duration_sum_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 0.000685176\nprometheus_rule_group_last_rule_duration_sum_seconds{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 0.001035099\nprometheus_rule_group_last_rule_duration_sum_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 0.021769662\nprometheus_rule_group_last_rule_duration_sum_seconds{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 0.042017156\nprometheus_rule_group_last_rule_duration_sum_seconds{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 0.00260535\nprometheus_rule_group_last_rule_duration_sum_seconds{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 0.002069356\nprometheus_rule_group_last_rule_duration_sum_seconds{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 0.009905481\nprometheus_rule_group_last_rule_duration_sum_seconds{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 0.009056893\n# HELP prometheus_rule_group_rules The number of rules.\n```\n\n----------------------------------------\n\nTITLE: Alert Definition in YAML for Prometheus Rule Testing\nDESCRIPTION: Specifies the format for defining expected alerts in Prometheus rule tests, including labels and annotations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/unit_testing_rules.md#2025-04-16_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# These are the expanded labels and annotations of the expected alert.\n# Note: labels also include the labels of the sample associated with the\n# alert (same as what you see in `/alerts`, without series `__name__` and `alertname`)\nexp_labels:\n  [ <labelname>: <string> ]\nexp_annotations:\n  [ <labelname>: <string> ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Storage and Tracing in YAML for Prometheus\nDESCRIPTION: This YAML snippet shows how to configure storage-related settings and tracing in Prometheus. It includes options for TSDB, exemplars, and tracing configuration.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n# Storage related settings that are runtime reloadable.\nstorage:\n  [ tsdb: <tsdb> ]\n  [ exemplars: <exemplars> ]\n\n# Configures exporting traces.\ntracing:\n  [ <tracing_config> ]\n```\n\n----------------------------------------\n\nTITLE: Enable Old UI Flag\nDESCRIPTION: Command flag to enable the old Prometheus 2.x web UI\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=old-ui\n```\n\n----------------------------------------\n\nTITLE: Displaying Prometheus Template and TreeCache Metrics\nDESCRIPTION: Shows metrics related to Prometheus template text expansions and TreeCache operations including failures, expansions, goroutines, and ZooKeeper interactions.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_28\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP prometheus_template_text_expansion_failures_total The total number of template text expansion failures.\n# TYPE prometheus_template_text_expansion_failures_total counter\nprometheus_template_text_expansion_failures_total 0\n# HELP prometheus_template_text_expansions_total The total number of template text expansions.\n# TYPE prometheus_template_text_expansions_total counter\nprometheus_template_text_expansions_total 2.126277e+06\n# HELP prometheus_treecache_watcher_goroutines The current number of watcher goroutines.\n# TYPE prometheus_treecache_watcher_goroutines gauge\nprometheus_treecache_watcher_goroutines 0\n# HELP prometheus_treecache_zookeeper_failures_total The total number of ZooKeeper failures.\n# TYPE prometheus_treecache_zookeeper_failures_total counter\nprometheus_treecache_zookeeper_failures_total 0\n```\n\n----------------------------------------\n\nTITLE: Prometheus Chunk Structure Diagram\nDESCRIPTION: Illustrates the layout of an individual chunk, including length, encoding type, payload data, and checksum.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n\n len <uvarint>  encoding <1 byte>  data <data>  checksum <4 byte> \n\n```\n\n----------------------------------------\n\nTITLE: Starting Prometheus Server\nDESCRIPTION: Command to start Prometheus with metadata WAL records feature enabled\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/remote_storage/example_write_adapter/README.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n./prometheus --enable-feature=metadata-wal-records\n```\n\n----------------------------------------\n\nTITLE: Enable Native Histograms Flag\nDESCRIPTION: Command flag to enable experimental native histogram support\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=native-histograms\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus HTTP Response Size Metrics\nDESCRIPTION: This snippet shows Prometheus metrics for HTTP response sizes across various handlers. It includes bucket counts for different size ranges, as well as sum and total count values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_15\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_response_size_bytes_sum{handler=\"/config\"} 22776\nprometheus_http_response_size_bytes_count{handler=\"/config\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"100\"} 17\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"1000\"} 17\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"10000\"} 31\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"100000\"} 34\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"1e+06\"} 34\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"1e+07\"} 34\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"1e+08\"} 34\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"1e+09\"} 34\nprometheus_http_response_size_bytes_bucket{handler=\"/consoles/*filepath\",le=\"+Inf\"} 34\nprometheus_http_response_size_bytes_sum{handler=\"/consoles/*filepath\"} 175556\nprometheus_http_response_size_bytes_count{handler=\"/consoles/*filepath\"} 34\n```\n\n----------------------------------------\n\nTITLE: Instant Query Evaluation Syntax\nDESCRIPTION: Shows the structure for evaluating an instant query at a specific timestamp with expected results and optional expectation lines for annotations, errors, or ordering.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/promql/promqltest/README.md#2025-04-16_snippet_2\n\nLANGUAGE: promql\nCODE:\n```\n# Instant query\neval instant at <time> <query>\n    <expect>\n    ...\n    <expect>\n    <series> <points>\n    ...\n    <series> <points>\n```\n\n----------------------------------------\n\nTITLE: Tracking HTTP Response Size Metrics in Prometheus\nDESCRIPTION: This metric data shows HTTP response size distributions across various Prometheus API endpoints. The data is formatted as histogram buckets with size thresholds ranging from 100 bytes to +Inf, plus sum and count values. This helps monitor API response sizes for performance analysis and capacity planning.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_10\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/buildinfo\",le=\"1e+06\"} 4413\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/buildinfo\",le=\"1e+07\"} 4413\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/buildinfo\",le=\"1e+08\"} 4413\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/buildinfo\",le=\"1e+09\"} 4413\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/buildinfo\",le=\"+Inf\"} 4413\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/status/buildinfo\"} 807579\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/status/buildinfo\"} 4413\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"100\"} 1\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"1000\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"10000\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"100000\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"1e+06\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"1e+07\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"1e+08\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"1e+09\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/config\",le=\"+Inf\"} 84723\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/status/config\"} 6.3710963e+07\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/status/config\"} 84723\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"1000\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"10000\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"100000\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"1e+06\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"1e+07\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"1e+08\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"1e+09\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/flags\",le=\"+Inf\"} 32\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/status/flags\"} 31968\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/status/flags\"} 32\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"1000\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"10000\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"100000\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"1e+06\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"1e+07\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"1e+08\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"1e+09\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/runtimeinfo\",le=\"+Inf\"} 863\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/status/runtimeinfo\"} 240400\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/status/runtimeinfo\"} 863\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"1000\"} 92\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"10000\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"100000\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"1e+06\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"1e+07\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"1e+08\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"1e+09\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/tsdb\",le=\"+Inf\"} 94\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/status/tsdb\"} 66441\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/status/tsdb\"} 94\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"100\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"1000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"10000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"100000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"1e+06\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"1e+07\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"1e+08\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"1e+09\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/status/walreplay\",le=\"+Inf\"} 49\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/status/walreplay\"} 3381\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/status/walreplay\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"1000\"} 184\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"10000\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"100000\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"1e+06\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"1e+07\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"1e+08\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"1e+09\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets\",le=\"+Inf\"} 191\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/targets\"} 191150\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/targets\"} 191\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"100\"} 1\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"1000\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"10000\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"100000\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"1e+06\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"1e+07\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"1e+08\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"1e+09\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/targets/metadata\",le=\"+Inf\"} 18\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/targets/metadata\"} 2736\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/targets/metadata\"} 18\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"100\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"1000\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"10000\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"100000\"} 142\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"1e+06\"} 958\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"1e+07\"} 2225\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"1e+08\"} 2225\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"1e+09\"} 2225\nprometheus_http_response_size_bytes_bucket{handler=\"/assets/*filepath\",le=\"+Inf\"} 2225\nprometheus_http_response_size_bytes_sum{handler=\"/assets/*filepath\"} 3.525958804e+09\nprometheus_http_response_size_bytes_count{handler=\"/assets/*filepath\"} 2225\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"100\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"1000\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"10000\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"100000\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"1e+06\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"1e+07\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"1e+08\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"1e+09\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/classic/static/*filepath\",le=\"+Inf\"} 103\nprometheus_http_response_size_bytes_sum{handler=\"/classic/static/*filepath\"} 1957\nprometheus_http_response_size_bytes_count{handler=\"/classic/static/*filepath\"} 103\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"1000\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"10000\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"100000\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"1e+06\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"1e+07\"} 13\nprometheus_http_response_size_bytes_bucket{handler=\"/config\",le=\"1e+08\"} 13\n```\n\n----------------------------------------\n\nTITLE: Prometheus HTTP Request Duration Metrics Data\nDESCRIPTION: Histogram metrics showing HTTP request duration statistics for Prometheus API endpoints. The data includes bucket counts for different duration thresholds, along with sum and count values for each endpoint. The metrics use the prometheus_http_request_duration_seconds metric name with various handler labels.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_2\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"1\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"3\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"8\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"20\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"60\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"120\"} 27\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/*path\",le=\"+Inf\"} 27\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/*path\"} 0.001724389\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/*path\"} 27\n```\n\n----------------------------------------\n\nTITLE: Prometheus TSDB Metrics in Exposition Format\nDESCRIPTION: Complete set of Prometheus TSDB metrics in exposition format. Includes metrics for compaction, head block management, exemplar storage, isolation watermarks, and timestamps. Each metric includes type information (gauge, counter, histogram, or summary) and help text describing its purpose.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_30\n\nLANGUAGE: promql\nCODE:\n```\n# TYPE prometheus_tsdb_compaction_chunk_size_bytes histogram\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"32\"} 1233\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"48\"} 156238\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"72\"} 2.006456e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"108\"} 3.568405e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"162\"} 3.835144e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"243\"} 4.034591e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"364.5\"} 4.505646e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"546.75\"} 4.69694e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"820.125\"} 4.78551e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"1230.1875\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"1845.28125\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"2767.921875\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_size_bytes_bucket{le=\"+Inf\"} 4.861956e+06\nprometheus_tsdb_compaction_chunk_size_bytes_sum 6.81852972e+08\nprometheus_tsdb_compaction_chunk_size_bytes_count 4.861956e+06\n# HELP prometheus_tsdb_compaction_duration_seconds Duration of compaction runs\n# TYPE prometheus_tsdb_compaction_duration_seconds histogram\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"1\"} 61\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"2\"} 155\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"4\"} 180\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"8\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"16\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"32\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"64\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"128\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"256\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"512\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"1024\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"2048\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"4096\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"8192\"} 183\nprometheus_tsdb_compaction_duration_seconds_bucket{le=\"+Inf\"} 183\nprometheus_tsdb_compaction_duration_seconds_sum 254.9095696899999\nprometheus_tsdb_compaction_duration_seconds_count 183\n# HELP prometheus_tsdb_compaction_populating_block Set to 1 when a block is currently being written to the disk.\n# TYPE prometheus_tsdb_compaction_populating_block gauge\nprometheus_tsdb_compaction_populating_block 0\n# HELP prometheus_tsdb_compactions_failed_total Total number of compactions that failed for the partition.\n# TYPE prometheus_tsdb_compactions_failed_total counter\nprometheus_tsdb_compactions_failed_total 0\n# HELP prometheus_tsdb_compactions_skipped_total Total number of skipped compactions due to disabled auto compaction.\n# TYPE prometheus_tsdb_compactions_skipped_total counter\nprometheus_tsdb_compactions_skipped_total 0\n# HELP prometheus_tsdb_compactions_total Total number of compactions that were executed for the partition.\n# TYPE prometheus_tsdb_compactions_total counter\nprometheus_tsdb_compactions_total 183\n# HELP prometheus_tsdb_compactions_triggered_total Total number of triggered compactions for the partition.\n# TYPE prometheus_tsdb_compactions_triggered_total counter\nprometheus_tsdb_compactions_triggered_total 14855\n# HELP prometheus_tsdb_data_replay_duration_seconds Time taken to replay the data on disk.\n# TYPE prometheus_tsdb_data_replay_duration_seconds gauge\nprometheus_tsdb_data_replay_duration_seconds 5.550163617\n# HELP prometheus_tsdb_exemplar_exemplars_appended_total Total number of appended exemplars.\n# TYPE prometheus_tsdb_exemplar_exemplars_appended_total counter\nprometheus_tsdb_exemplar_exemplars_appended_total 0\n# HELP prometheus_tsdb_exemplar_exemplars_in_storage Number of exemplars currently in circular storage.\n# TYPE prometheus_tsdb_exemplar_exemplars_in_storage gauge\nprometheus_tsdb_exemplar_exemplars_in_storage 0\n# HELP prometheus_tsdb_exemplar_last_exemplars_timestamp_seconds The timestamp of the oldest exemplar stored in circular storage. Useful to check for what timerange the current exemplar buffer limit allows. This usually means the last timestampfor all exemplars for a typical setup. This is not true though if one of the series timestamp is in future compared to rest series.\n# TYPE prometheus_tsdb_exemplar_last_exemplars_timestamp_seconds gauge\nprometheus_tsdb_exemplar_last_exemplars_timestamp_seconds 0\n# HELP prometheus_tsdb_exemplar_max_exemplars Total number of exemplars the exemplar storage can store, resizeable.\n# TYPE prometheus_tsdb_exemplar_max_exemplars gauge\nprometheus_tsdb_exemplar_max_exemplars 0\n# HELP prometheus_tsdb_exemplar_out_of_order_exemplars_total Total number of out of order exemplar ingestion failed attempts.\n# TYPE prometheus_tsdb_exemplar_out_of_order_exemplars_total counter\nprometheus_tsdb_exemplar_out_of_order_exemplars_total 0\n# HELP prometheus_tsdb_exemplar_series_with_exemplars_in_storage Number of series with exemplars currently in circular storage.\n# TYPE prometheus_tsdb_exemplar_series_with_exemplars_in_storage gauge\nprometheus_tsdb_exemplar_series_with_exemplars_in_storage 0\n# HELP prometheus_tsdb_head_active_appenders Number of currently active appender transactions\n# TYPE prometheus_tsdb_head_active_appenders gauge\nprometheus_tsdb_head_active_appenders 0\n# HELP prometheus_tsdb_head_chunks Total number of chunks in the head block.\n# TYPE prometheus_tsdb_head_chunks gauge\nprometheus_tsdb_head_chunks 31476\n# HELP prometheus_tsdb_head_chunks_created_total Total number of chunks created in the head\n# TYPE prometheus_tsdb_head_chunks_created_total counter\nprometheus_tsdb_head_chunks_created_total 4.893432e+06\n# HELP prometheus_tsdb_head_chunks_removed_total Total number of chunks removed in the head\n# TYPE prometheus_tsdb_head_chunks_removed_total counter\nprometheus_tsdb_head_chunks_removed_total 4.861956e+06\n# HELP prometheus_tsdb_head_chunks_storage_size_bytes Size of the chunks_head directory.\n# TYPE prometheus_tsdb_head_chunks_storage_size_bytes gauge\nprometheus_tsdb_head_chunks_storage_size_bytes 7.237299e+06\n# HELP prometheus_tsdb_head_gc_duration_seconds Runtime of garbage collection in the head block.\n# TYPE prometheus_tsdb_head_gc_duration_seconds summary\nprometheus_tsdb_head_gc_duration_seconds_sum 4.773801686000001\nprometheus_tsdb_head_gc_duration_seconds_count 123\n# HELP prometheus_tsdb_head_max_time Maximum timestamp of the head block. The unit is decided by the library consumer.\n# TYPE prometheus_tsdb_head_max_time gauge\nprometheus_tsdb_head_max_time 1.738949375191e+12\n# HELP prometheus_tsdb_head_max_time_seconds Maximum timestamp of the head block.\n# TYPE prometheus_tsdb_head_max_time_seconds gauge\nprometheus_tsdb_head_max_time_seconds 1.738949375e+09\n# HELP prometheus_tsdb_head_min_time Minimum time bound of the head block. The unit is decided by the library consumer.\n# TYPE prometheus_tsdb_head_min_time gauge\nprometheus_tsdb_head_min_time 1.738944000171e+12\n# HELP prometheus_tsdb_head_min_time_seconds Minimum time bound of the head block.\n# TYPE prometheus_tsdb_head_min_time_seconds gauge\nprometheus_tsdb_head_min_time_seconds 1.738944e+09\n# HELP prometheus_tsdb_head_out_of_order_samples_appended_total Total number of appended out of order samples.\n# TYPE prometheus_tsdb_head_out_of_order_samples_appended_total counter\nprometheus_tsdb_head_out_of_order_samples_appended_total{type=\"float\"} 0\nprometheus_tsdb_head_out_of_order_samples_appended_total{type=\"histogram\"} 0\n# HELP prometheus_tsdb_head_samples_appended_total Total number of appended samples.\n# TYPE prometheus_tsdb_head_samples_appended_total counter\nprometheus_tsdb_head_samples_appended_total{type=\"float\"} 5.85543187e+08\nprometheus_tsdb_head_samples_appended_total{type=\"histogram\"} 0\n# HELP prometheus_tsdb_head_series Total number of series in the head block.\n# TYPE prometheus_tsdb_head_series gauge\nprometheus_tsdb_head_series 10720\n# HELP prometheus_tsdb_head_series_created_total Total number of series created in the head\n# TYPE prometheus_tsdb_head_series_created_total counter\nprometheus_tsdb_head_series_created_total 18541\n# HELP prometheus_tsdb_head_series_not_found_total Total number of requests for series that were not found.\n# TYPE prometheus_tsdb_head_series_not_found_total counter\nprometheus_tsdb_head_series_not_found_total 0\n# HELP prometheus_tsdb_head_series_removed_total Total number of series removed in the head\n# TYPE prometheus_tsdb_head_series_removed_total counter\nprometheus_tsdb_head_series_removed_total 7821\n# HELP prometheus_tsdb_head_truncations_failed_total Total number of head truncations that failed.\n# TYPE prometheus_tsdb_head_truncations_failed_total counter\nprometheus_tsdb_head_truncations_failed_total 0\n# HELP prometheus_tsdb_head_truncations_total Total number of head truncations attempted.\n# TYPE prometheus_tsdb_head_truncations_total counter\nprometheus_tsdb_head_truncations_total 123\n# HELP prometheus_tsdb_isolation_high_watermark The highest TSDB append ID that has been given out.\n# TYPE prometheus_tsdb_isolation_high_watermark gauge\nprometheus_tsdb_isolation_high_watermark 7.852949e+06\n# HELP prometheus_tsdb_isolation_low_watermark The lowest TSDB append ID that is still referenced.\n# TYPE prometheus_tsdb_isolation_low_watermark gauge\nprometheus_tsdb_isolation_low_watermark 7.852949e+06\n# HELP prometheus_tsdb_lowest_timestamp Lowest timestamp value stored in the database. The unit is decided by the library consumer.\n# TYPE prometheus_tsdb_lowest_timestamp gauge\nprometheus_tsdb_lowest_timestamp 1.73618640004e+12\n# HELP prometheus_tsdb_lowest_timestamp_seconds Lowest timestamp value stored in the database.\n# TYPE prometheus_tsdb_lowest_timestamp_seconds gauge\nprometheus_tsdb_lowest_timestamp_seconds 1.7361864e+09\n```\n\n----------------------------------------\n\nTITLE: Illustrating Series Section Structure in ASCII Art\nDESCRIPTION: This ASCII art diagram shows the structure of the Series section in the index file, including individual series entries.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/index.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n```\n\n  \n    series_1                         \n  \n                  . . .              \n  \n    series_n                         \n  \n\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus YAML for Remote Storage\nDESCRIPTION: YAML configuration to add to prometheus.yml that configures remote write for all supported backends and remote read for InfluxDB, specifying the adapter's endpoints.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/remote_storage/remote_storage_adapter/README.md#2025-04-16_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# Remote write configuration (for Graphite, OpenTSDB, or InfluxDB).\nremote_write:\n  - url: \"http://localhost:9201/write\"\n\n# Remote read configuration (for InfluxDB only at the moment).\nremote_read:\n  - url: \"http://localhost:9201/read\"\n```\n\n----------------------------------------\n\nTITLE: Building Prometheus with Prebuilt UI Assets\nDESCRIPTION: Command to build Prometheus using prebuilt UI assets by passing a parameter to make specifying the static directory location.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/README.md#2025-04-16_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmake PREBUILT_ASSETS_STATIC_DIR=web/ui/static build\n```\n\n----------------------------------------\n\nTITLE: Prometheus HTTP Request Total Metric Samples\nDESCRIPTION: This section provides multiple samples of the 'prometheus_http_requests_total' metric with different label combinations for various API endpoints and UI paths. Each line represents a unique combination of response code and handler path.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_10\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_requests_total{code=\"200\",handler=\"/\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/-/healthy\"} 29524\nprometheus_http_requests_total{code=\"200\",handler=\"/-/quit\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/-/ready\"} 49\nprometheus_http_requests_total{code=\"200\",handler=\"/-/reload\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/alertmanager-discovery\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/alerts\"} 48\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/*path\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/admin/tsdb/clean_tombstones\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/admin/tsdb/delete_series\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/admin/tsdb/snapshot\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/alertmanagers\"} 8\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/alerts\"} 14635\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/format_query\"} 3\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/label/:name/values\"} 14679\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/labels\"} 4915\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/metadata\"} 2093\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/notifications\"} 12\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/notifications/live\"} 4529\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/otlp/v1/metrics\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/parse_query\"} 247\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/query\"} 326411\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/query_exemplars\"} 841\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/query_range\"} 1.183734e+06\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/read\"} 16\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/rules\"} 88658\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/scrape_pools\"} 142\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/series\"} 1235\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/status/buildinfo\"} 4413\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/status/config\"} 84722\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/status/flags\"} 32\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/status/runtimeinfo\"} 863\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/status/tsdb\"} 94\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/status/walreplay\"} 49\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/targets\"} 191\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/targets/metadata\"} 16\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/write\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/assets/*filepath\"} 2213\nprometheus_http_requests_total{code=\"200\",handler=\"/classic/static/*filepath\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/config\"} 13\nprometheus_http_requests_total{code=\"200\",handler=\"/consoles/*filepath\"} 17\nprometheus_http_requests_total{code=\"200\",handler=\"/debug/*subpath\"} 3\nprometheus_http_requests_total{code=\"200\",handler=\"/favicon.ico\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/favicon.svg\"} 769\nprometheus_http_requests_total{code=\"200\",handler=\"/federate\"} 4\nprometheus_http_requests_total{code=\"200\",handler=\"/flags\"} 9\nprometheus_http_requests_total{code=\"200\",handler=\"/graph\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/manifest.json\"} 0\nprometheus_http_requests_total{code=\"200\",handler=\"/metrics\"} 4.059092e+06\nprometheus_http_requests_total{code=\"200\",handler=\"/query\"} 1774\nprometheus_http_requests_total{code=\"200\",handler=\"/rules\"} 8673\nprometheus_http_requests_total{code=\"200\",handler=\"/service-discovery\"} 20\nprometheus_http_requests_total{code=\"200\",handler=\"/status\"} 46\nprometheus_http_requests_total{code=\"200\",handler=\"/targets\"} 39\nprometheus_http_requests_total{code=\"200\",handler=\"/tsdb-status\"} 49\nprometheus_http_requests_total{code=\"200\",handler=\"/version\"} 0\nprometheus_http_requests_total{code=\"204\",handler=\"/api/v1/*path\"} 27\nprometheus_http_requests_total{code=\"204\",handler=\"/api/v1/notifications/live\"} 5\nprometheus_http_requests_total{code=\"301\",handler=\"/debug/*subpath\"} 1\nprometheus_http_requests_total{code=\"302\",handler=\"/\"} 688\nprometheus_http_requests_total{code=\"302\",handler=\"/graph\"} 751\nprometheus_http_requests_total{code=\"400\",handler=\"/api/v1/format_query\"} 1\nprometheus_http_requests_total{code=\"400\",handler=\"/api/v1/label/:name/values\"} 3292\nprometheus_http_requests_total{code=\"400\",handler=\"/api/v1/labels\"} 6\nprometheus_http_requests_total{code=\"400\",handler=\"/api/v1/parse_query\"} 18\nprometheus_http_requests_total{code=\"400\",handler=\"/api/v1/query\"} 155\nprometheus_http_requests_total{code=\"400\",handler=\"/api/v1/query_range\"} 263\nprometheus_http_requests_total{code=\"400\",handler=\"/api/v1/rules\"} 4\nprometheus_http_requests_total{code=\"400\",handler=\"/api/v1/series\"} 11\nprometheus_http_requests_total{code=\"400\",handler=\"/api/v1/targets/metadata\"} 2\nprometheus_http_requests_total{code=\"404\",handler=\"/assets/*filepath\"} 12\nprometheus_http_requests_total{code=\"404\",handler=\"/classic/static/*filepath\"} 103\nprometheus_http_requests_total{code=\"404\",handler=\"/consoles/*filepath\"} 17\nprometheus_http_requests_total{code=\"404\",handler=\"/favicon.ico\"} 177\nprometheus_http_requests_total{code=\"416\",handler=\"/favicon.svg\"} 1\nprometheus_http_requests_total{code=\"422\",handler=\"/api/v1/query\"} 114\nprometheus_http_requests_total{code=\"422\",handler=\"/api/v1/query_range\"} 31\nprometheus_http_requests_total{code=\"499\",handler=\"/api/v1/query\"} 7\nprometheus_http_requests_total{code=\"499\",handler=\"/api/v1/query_range\"} 20\nprometheus_http_requests_total{code=\"503\",handler=\"/api/v1/query\"} 7\nprometheus_http_requests_total{code=\"503\",handler=\"/api/v1/query_range\"} 4\nprometheus_http_requests_total{code=\"503\",handler=\"/api/v1/status/config\"} 1\n```\n\n----------------------------------------\n\nTITLE: Example of Instant Query Expected to Fail with Exact Message\nDESCRIPTION: Shows an example of a query that is expected to fail with an exact error message match.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/promql/promqltest/README.md#2025-04-16_snippet_8\n\nLANGUAGE: promql\nCODE:\n```\neval instant at 1m ceil({__name__=~'testmetric1|testmetric2'})\nexpect fail msg \"vector cannot contain metrics with the same labelset\"\n```\n\n----------------------------------------\n\nTITLE: Installing Remote Write protobuf as Go library\nDESCRIPTION: Command to install the Prometheus Remote Write protobuf implementation as a Go library from buf.build. This is marked as experimental functionality for developers who need to integrate with Prometheus Remote Write.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngo get buf.build/gen/go/prometheus/prometheus/protocolbuffers/go@latest\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Service Discovery in Prometheus\nDESCRIPTION: YAML configuration for Azure SD that allows retrieving scrape targets from Azure VMs. Includes authentication options, subscription settings, and available meta labels for target relabeling.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n# The information to access the Azure API.\n# The Azure environment.\n[ environment: <string> | default = AzurePublicCloud ]\n\n# The authentication method, either OAuth, ManagedIdentity or SDK.\n# See https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview\n# SDK authentication method uses environment variables by default.\n# See https://learn.microsoft.com/en-us/azure/developer/go/azure-sdk-authentication\n[ authentication_method: <string> | default = OAuth]\n# The subscription ID. Always required.\nsubscription_id: <string>\n# Optional tenant ID. Only required with authentication_method OAuth.\n[ tenant_id: <string> ]\n# Optional client ID. Only required with authentication_method OAuth.\n[ client_id: <string> ]\n# Optional client secret. Only required with authentication_method OAuth.\n[ client_secret: <secret> ]\n\n# Optional resource group name. Limits discovery to this resource group.\n[ resource_group: <string> ]\n\n# Refresh interval to re-read the instance list.\n[ refresh_interval: <duration> | default = 300s ]\n\n# The port to scrape metrics from. If using the public IP address, this must\n# instead be specified in the relabeling rule.\n[ port: <int> | default = 80 ]\n\n# HTTP client settings, including authentication methods (such as basic auth and\n# authorization), proxy configurations, TLS options, custom HTTP headers, etc.\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Enable Delayed Name Removal Flag\nDESCRIPTION: Command flag to enable delayed __name__ label removal in PromQL\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=promql-delayed-name-removal\n```\n\n----------------------------------------\n\nTITLE: Adding TLS and Basic Authentication to HTTP Endpoints\nDESCRIPTION: This feature allows adding TLS and basic authentication to Prometheus HTTP endpoints, enhancing security.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_15\n\nLANGUAGE: markdown\nCODE:\n```\n* [FEATURE] Add TLS and basic authentication to HTTP endpoints. #8316\n```\n\n----------------------------------------\n\nTITLE: Illustrating Label Index Structure in ASCII Art\nDESCRIPTION: This ASCII art diagram shows the structure of the Label Index section in the index file, including length fields, number of names and entries, and reference values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/index.md#2025-04-16_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n```\n\n len <4b>       #names <4b>     #entries <4b>  \n\n  \n  ref(value_0) <4b>                            \n  \n  ...                                          \n  \n  ref(value_n) <4b>                            \n  \n                      . . .                      \n\n CRC32 <4b>                                      \n\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookback Interval for Prometheus Metadata Retrieval\nDESCRIPTION: Setting a custom time interval for retrieving metrics and labels from Prometheus, which determines how far back the data should be considered.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst promQL = new PromQLExtension().setComplete({remote: {lookbackInterval: 12 * 60 * 60 * 1000}})\n```\n\n----------------------------------------\n\nTITLE: Installing CodeMirror Basic Setup\nDESCRIPTION: Command to install the CodeMirror basic-setup package, which simplifies setting up CodeMirror with standard functionality.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save @codemirror/basic-setup\n```\n\n----------------------------------------\n\nTITLE: Streaming Live Notifications from Prometheus\nDESCRIPTION: This experimental endpoint streams live notifications as they occur using Server-Sent Events. It provides real-time updates about server status changes and includes both new notifications and deleted ones (marked with active: false).\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_38\n\nLANGUAGE: json\nCODE:\n```\n$ curl http://localhost:9090/api/v1/notifications/live\ndata: {\n  \"status\": \"success\",\n  \"data\": [\n    {\n      \"text\": \"Prometheus is shutting down and gracefully stopping all operations.\",\n      \"date\": \"2024-10-07T12:33:08.551376578+02:00\",\n      \"active\": true\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Log Format Example - Old Format (Prometheus v2)\nDESCRIPTION: Example of the logging format used in Prometheus v2 with go-kit/log showing timestamp, caller, level and message structure.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/migration.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nts=2024-10-23T22:01:06.074Z caller=main.go:627 level=info msg=\"No time or size retention was set so using the default time retention\" duration=15d\nts=2024-10-23T22:01:06.074Z caller=main.go:671 level=info msg=\"Starting Prometheus Server\" mode=server version=\"(version=, branch=, revision=91d80252c3e528728b0f88d254dd720f6be07cb8-modified)\"\nts=2024-10-23T22:01:06.074Z caller=main.go:676 level=info build_context=\"(go=go1.23.0, platform=linux/amd64, user=, date=, tags=unknown)\"\nts=2024-10-23T22:01:06.074Z caller=main.go:677 level=info host_details=\"(Linux 5.15.0-124-generic #134-Ubuntu SMP Fri Sep 27 20:20:17 UTC 2024 x86_64 gigafips (none))\"\n```\n\n----------------------------------------\n\nTITLE: Configuring DigitalOcean Service Discovery in Prometheus\nDESCRIPTION: YAML configuration for DigitalOcean SD that allows retrieving scrape targets from DigitalOcean's Droplets API. Includes port configuration, refresh interval, and available meta labels for target relabeling.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\n# The port to scrape metrics from.\n[ port: <int> | default = 80 ]\n\n# The time after which the droplets are refreshed.\n[ refresh_interval: <duration> | default = 60s ]\n\n# HTTP client settings, including authentication methods (such as basic auth and\n# authorization), proxy configurations, TLS options, custom HTTP headers, etc.\n[ <http_config> ]\n```\n\n----------------------------------------\n\nTITLE: Displaying Help Information\nDESCRIPTION: Command to display all available flags and options for the remote storage adapter.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/remote_storage/remote_storage_adapter/README.md#2025-04-16_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n./remote_storage_adapter -h\n```\n\n----------------------------------------\n\nTITLE: Renaming Remote Write Metrics\nDESCRIPTION: This change renames and modifies several metrics related to remote write functionality in Prometheus.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_16\n\nLANGUAGE: markdown\nCODE:\n```\n* [CHANGE] Remote write: The following metrics were removed/renamed in remote write. #6815\n  * `prometheus_remote_storage_succeeded_samples_total` was removed and `prometheus_remote_storage_samples_total` was introduced for all the samples attempted to send.\n  * `prometheus_remote_storage_sent_bytes_total` was removed and replaced with `prometheus_remote_storage_samples_bytes_total` and `prometheus_remote_storage_metadata_bytes_total`.\n  * `prometheus_remote_storage_failed_samples_total` -> `prometheus_remote_storage_samples_failed_total` .\n  * `prometheus_remote_storage_retried_samples_total` -> `prometheus_remote_storage_samples_retried_total`.\n  * `prometheus_remote_storage_dropped_samples_total` -> `prometheus_remote_storage_samples_dropped_total`.\n  * `prometheus_remote_storage_pending_samples` -> `prometheus_remote_storage_samples_pending`.\n```\n\n----------------------------------------\n\nTITLE: Histogram Spans Data Format Diagram\nDESCRIPTION: Details the format of positive and negative spans in histogram chunks, showing span counts, lengths and offsets.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n\n num_spans <varbit_uint>  length_0 <varbit_uint>  offset_0 <varbit_int>  length_1 <varbit_uint>  offset_1 <varbit_int>  ...  length_n <varbit_uint>  offset_n <varbit_int> \n\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Autocompletion Strategy\nDESCRIPTION: Configuration to provide a custom implementation of the autocompletion strategy, overriding the default implementation entirely.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nconst promQL = new PromQLExtension().setComplete({completeStrategy: myCustomImpl})\n```\n\n----------------------------------------\n\nTITLE: Histogram Samples Container Structure Diagram\nDESCRIPTION: Shows the container structure for multiple histogram samples, with each sample having its own data structure.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n\n    sample_0 <data>       \n\n    sample_1 <data>       \n\n    sample_2 <data>       \n\n          ...             \n\n    sample_n <data>       \n\n```\n\n----------------------------------------\n\nTITLE: Building and Testing Prometheus\nDESCRIPTION: Commands for building Prometheus from source and running the test suite. Includes basic build command and test execution using make.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CONTRIBUTING.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# For building.\ngo build ./cmd/prometheus/\n./prometheus\n\n# For testing.\nmake test         # Make sure all the tests pass before you commit and push :)\n```\n\n----------------------------------------\n\nTITLE: Displaying Prometheus HTTP Response Size Metrics\nDESCRIPTION: Metrics showing HTTP response size buckets for various Prometheus API endpoints including /status, /targets, and /tsdb-status. The metrics are organized in histogram buckets with different size thresholds.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_16\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_response_size_bytes_bucket{handler=\"/status\",le=\"1e+07\"} 46\nprometheus_http_response_size_bytes_bucket{handler=\"/status\",le=\"1e+08\"} 46\nprometheus_http_response_size_bytes_bucket{handler=\"/status\",le=\"1e+09\"} 46\nprometheus_http_response_size_bytes_bucket{handler=\"/status\",le=\"+Inf\"} 46\nprometheus_http_response_size_bytes_sum{handler=\"/status\"} 80592\nprometheus_http_response_size_bytes_count{handler=\"/status\"} 46\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"1000\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"10000\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"100000\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"1e+06\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"1e+07\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"1e+08\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"1e+09\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/targets\",le=\"+Inf\"} 39\nprometheus_http_response_size_bytes_sum{handler=\"/targets\"} 68328\nprometheus_http_response_size_bytes_count{handler=\"/targets\"} 39\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"1000\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"10000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"100000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"1e+06\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"1e+07\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"1e+08\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"1e+09\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/tsdb-status\",le=\"+Inf\"} 49\nprometheus_http_response_size_bytes_sum{handler=\"/tsdb-status\"} 85848\nprometheus_http_response_size_bytes_count{handler=\"/tsdb-status\"} 49\n```\n\n----------------------------------------\n\nTITLE: Configuring Nerve Service Discovery in Prometheus\nDESCRIPTION: Configuration for Nerve service discovery in Prometheus, which retrieves scrape targets from AirBnB's Nerve stored in Zookeeper. Includes server configurations and available meta labels.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_37\n\nLANGUAGE: yaml\nCODE:\n```\n# The Zookeeper servers.\nservers:\n  - <host>\n# Paths can point to a single service, or the root of a tree of services.\npaths:\n  - <string>\n[ timeout: <duration> | default = 10s ]\n```\n\n----------------------------------------\n\nTITLE: Basic Comment Syntax in PromQL Test Scripts\nDESCRIPTION: Shows how to write comments in PromQL test scripts by prefixing text with a # character.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/promql/promqltest/README.md#2025-04-16_snippet_0\n\nLANGUAGE: promql\nCODE:\n```\n# This is a comment.\n```\n\n----------------------------------------\n\nTITLE: Building Remote Write Adapter in Go\nDESCRIPTION: Commands to build and run the remote write adapter server\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/examples/remote_storage/example_write_adapter/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngo build\n\n./example_write_adapter\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Histogram Encoding Structure in Plaintext\nDESCRIPTION: This code snippet represents the binary layout format for histograms in Prometheus, showing how bucket values are stored. It includes fields for negative spans, positive buckets, negative buckets, and custom values, along with their respective counts and encoding formats.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/wal.md#2025-04-16_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\n  \n  . . .                                                              \n  \n  negative_spans_num <uvarint> = 0                                   \n  \n  positive_bkts_num <uvarint>                                        \n  \n  positive_bkt_1 (float) <8b>  . . .  positive_bkt_n (float) <8b>  \n  \n  negative_bkts_num <uvarint> = 0                                    \n  \n  custom_values_num <uvarint>                                        \n  \n  custom_value_1 (float) <8b>  . . .  custom_value_n (float) <8b>  \n  \n                              . . .                                    \n\n```\n\n----------------------------------------\n\nTITLE: Installing Jsonnet and Jsonnet Bundler with Go\nDESCRIPTION: Commands to install jsonnet, jsonnetfmt, and jb using Go's package manager. These tools are required for working with the Prometheus Mixin.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/documentation/prometheus-mixin/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ go install github.com/google/go-jsonnet/cmd/jsonnet@latest\n$ go install github.com/google/go-jsonnet/cmd/jsonnetfmt@latest\n$ go install github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb@latest\n```\n\n----------------------------------------\n\nTITLE: Retrieving Active Notifications from Prometheus\nDESCRIPTION: This experimental endpoint returns a list of all currently active notifications used in the web UI. It provides information about the server's status notifications.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_37\n\nLANGUAGE: json\nCODE:\n```\n$ curl http://localhost:9090/api/v1/notifications\n{\n  \"status\": \"success\",\n  \"data\": [\n    {\n      \"text\": \"Prometheus is shutting down and gracefully stopping all operations.\",\n      \"date\": \"2024-10-07T12:33:08.551376578+02:00\",\n      \"active\": true\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Prometheus Release Changelog Entry 2.49.1\nDESCRIPTION: Bugfix release notes for version 2.49.1 fixing a TSDB scrape accept header issue.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n## 2.49.1 / 2024-01-15\n\n* [BUGFIX] TSDB: Fixed a wrong `q=` value in scrape accept header #13313\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Service Discovery Metrics\nDESCRIPTION: This snippet displays various Prometheus metrics related to service discovery, including Azure, Consul, DNS, discovered targets, and file-based service discovery. It provides information on cache hits, failures, RPC durations, and file operations.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_22\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP prometheus_sd_azure_cache_hit_total Number of cache hit during refresh.\n# TYPE prometheus_sd_azure_cache_hit_total counter\nprometheus_sd_azure_cache_hit_total 0\n# HELP prometheus_sd_azure_failures_total Number of Azure service discovery refresh failures.\n# TYPE prometheus_sd_azure_failures_total counter\nprometheus_sd_azure_failures_total 0\n# HELP prometheus_sd_consul_rpc_duration_seconds The duration of a Consul RPC call in seconds.\n# TYPE prometheus_sd_consul_rpc_duration_seconds summary\nprometheus_sd_consul_rpc_duration_seconds{call=\"service\",endpoint=\"catalog\",quantile=\"0.5\"} NaN\nprometheus_sd_consul_rpc_duration_seconds{call=\"service\",endpoint=\"catalog\",quantile=\"0.9\"} NaN\nprometheus_sd_consul_rpc_duration_seconds{call=\"service\",endpoint=\"catalog\",quantile=\"0.99\"} NaN\nprometheus_sd_consul_rpc_duration_seconds_sum{call=\"service\",endpoint=\"catalog\"} 0\nprometheus_sd_consul_rpc_duration_seconds_count{call=\"service\",endpoint=\"catalog\"} 0\nprometheus_sd_consul_rpc_duration_seconds{call=\"services\",endpoint=\"catalog\",quantile=\"0.5\"} NaN\nprometheus_sd_consul_rpc_duration_seconds{call=\"services\",endpoint=\"catalog\",quantile=\"0.9\"} NaN\nprometheus_sd_consul_rpc_duration_seconds{call=\"services\",endpoint=\"catalog\",quantile=\"0.99\"} NaN\nprometheus_sd_consul_rpc_duration_seconds_sum{call=\"services\",endpoint=\"catalog\"} 0\nprometheus_sd_consul_rpc_duration_seconds_count{call=\"services\",endpoint=\"catalog\"} 0\n# HELP prometheus_sd_consul_rpc_failures_total The number of Consul RPC call failures.\n# TYPE prometheus_sd_consul_rpc_failures_total counter\nprometheus_sd_consul_rpc_failures_total 0\n# HELP prometheus_sd_discovered_targets Current number of discovered targets.\n# TYPE prometheus_sd_discovered_targets gauge\nprometheus_sd_discovered_targets{config=\"alertmanager\",name=\"scrape\"} 1\nprometheus_sd_discovered_targets{config=\"blackbox\",name=\"scrape\"} 1\nprometheus_sd_discovered_targets{config=\"caddy\",name=\"scrape\"} 1\nprometheus_sd_discovered_targets{config=\"cadvisor\",name=\"scrape\"} 1\nprometheus_sd_discovered_targets{config=\"config-0\",name=\"notify\"} 1\nprometheus_sd_discovered_targets{config=\"grafana\",name=\"scrape\"} 1\nprometheus_sd_discovered_targets{config=\"node\",name=\"scrape\"} 1\nprometheus_sd_discovered_targets{config=\"prometheus\",name=\"scrape\"} 1\nprometheus_sd_discovered_targets{config=\"random\",name=\"scrape\"} 4\n# HELP prometheus_sd_dns_lookup_failures_total The number of DNS-SD lookup failures.\n# TYPE prometheus_sd_dns_lookup_failures_total counter\nprometheus_sd_dns_lookup_failures_total 0\n# HELP prometheus_sd_dns_lookups_total The number of DNS-SD lookups.\n# TYPE prometheus_sd_dns_lookups_total counter\nprometheus_sd_dns_lookups_total 0\n# HELP prometheus_sd_failed_configs Current number of service discovery configurations that failed to load.\n# TYPE prometheus_sd_failed_configs gauge\nprometheus_sd_failed_configs{name=\"notify\"} 0\nprometheus_sd_failed_configs{name=\"scrape\"} 0\n# HELP prometheus_sd_file_mtime_seconds Timestamp (mtime) of files read by FileSD. Timestamp is set at read time.\n# TYPE prometheus_sd_file_mtime_seconds gauge\nprometheus_sd_file_mtime_seconds{filename=\"/etc/prometheus/file_sd/alertmanager.yml\"} 1.701295557e+09\nprometheus_sd_file_mtime_seconds{filename=\"/etc/prometheus/file_sd/cadvisor.yml\"} 1.705433682e+09\nprometheus_sd_file_mtime_seconds{filename=\"/etc/prometheus/file_sd/node.yml\"} 1.701295553e+09\nprometheus_sd_file_mtime_seconds{filename=\"/etc/prometheus/file_sd/random.yml\"} 1.579263729e+09\n# HELP prometheus_sd_file_read_errors_total The number of File-SD read errors.\n# TYPE prometheus_sd_file_read_errors_total counter\nprometheus_sd_file_read_errors_total 0\n# HELP prometheus_sd_file_scan_duration_seconds The duration of the File-SD scan in seconds.\n# TYPE prometheus_sd_file_scan_duration_seconds summary\nprometheus_sd_file_scan_duration_seconds{quantile=\"0.5\"} 9.5384e-05\nprometheus_sd_file_scan_duration_seconds{quantile=\"0.9\"} 0.000275679\nprometheus_sd_file_scan_duration_seconds{quantile=\"0.99\"} 0.000275679\nprometheus_sd_file_scan_duration_seconds_sum 4.751347856999997\nprometheus_sd_file_scan_duration_seconds_count 11812\n# HELP prometheus_sd_file_watcher_errors_total The number of File-SD errors caused by filesystem watch failures.\n# TYPE prometheus_sd_file_watcher_errors_total counter\nprometheus_sd_file_watcher_errors_total 0\n```\n\n----------------------------------------\n\nTITLE: Prometheus HTTP Duration Metrics for API Endpoints\nDESCRIPTION: Histogram metrics showing HTTP request durations across various API endpoints including status, targets, assets, and config paths. The metrics are organized in buckets ranging from 0.1 to 120 seconds, with sums and counts for each handler.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_4\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/status/config\",le=\"60\"} 84723\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/status/config\",le=\"120\"} 84723\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/status/config\",le=\"+Inf\"} 84723\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/status/config\"} 228.93575751199964\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/status/config\"} 84723\n```\n\n----------------------------------------\n\nTITLE: Enable Metadata WAL Records Flag\nDESCRIPTION: Command flag to enable storing metadata in WAL records\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=metadata-wal-records\n```\n\n----------------------------------------\n\nTITLE: Tombstone Record Format in Prometheus Memory Snapshot\nDESCRIPTION: Structure of a tombstone record in the Prometheus memory snapshot format. A single record is written for all tombstones, using the same encoding as the tombstone file in blocks.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/memory_snapshot.md#2025-04-16_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n\n                        Record Type <byte>                       \n\n len(Encoded Tombstones) <uvarint>  Encoded Tombstones <bytes>  \n\n```\n\n----------------------------------------\n\nTITLE: Go Runtime Memory and Scheduler Metrics in Prometheus Format\nDESCRIPTION: Prometheus metrics exposition format showing various Go runtime statistics including memory allocation, garbage collection timings, scheduler latencies, and system resource utilization. Each metric includes help text describing its purpose and type definition.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_1\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP go_memstats_frees_total Total number of heap objects frees. Equals to /gc/heap/frees:objects + /gc/heap/tiny/allocs:objects.\n# TYPE go_memstats_frees_total counter\ngo_memstats_frees_total 5.2351772989e+10\n# HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata. Equals to /memory/classes/metadata/other:bytes.\n# TYPE go_memstats_gc_sys_bytes gauge\ngo_memstats_gc_sys_bytes 5.450544e+06\n# HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and currently in use, same as go_memstats_alloc_bytes. Equals to /memory/classes/heap/objects:bytes.\n# TYPE go_memstats_heap_alloc_bytes gauge\ngo_memstats_heap_alloc_bytes 8.3523288e+07\n# HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used. Equals to /memory/classes/heap/released:bytes + /memory/classes/heap/free:bytes.\n# TYPE go_memstats_heap_idle_bytes gauge\ngo_memstats_heap_idle_bytes 1.23691008e+08\n# HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use. Equals to /memory/classes/heap/objects:bytes + /memory/classes/heap/unused:bytes\n# TYPE go_memstats_heap_inuse_bytes gauge\ngo_memstats_heap_inuse_bytes 9.56416e+07\n```\n\n----------------------------------------\n\nTITLE: XOR Chunk Data Format Diagram\nDESCRIPTION: Details the structure of XOR-encoded time series data, showing timestamp and value encoding for efficient storage.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n\n num_samples <uint16>  ts_0 <varint>  v_0 <float64>  ts_1_delta <uvarint>  v_1_xor <varbit_xor>  ts_2_dod <varbit_ts>  v_2_xor <varbit_xor>  ...  ts_n_dod <varbit_ts>  v_n_xor <varbit_xor>  padding <x bits> \n\n```\n\n----------------------------------------\n\nTITLE: Summing HTTP Requests by Application and Group with 'by' in PromQL\nDESCRIPTION: Equivalent example using the 'by' clause instead of 'without' to calculate the total number of HTTP requests per application and group.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/operators.md#2025-04-16_snippet_13\n\nLANGUAGE: promql\nCODE:\n```\nsum by (application, group) (http_requests_total)\n```\n\n----------------------------------------\n\nTITLE: Prometheus HTTP Request Duration Metrics\nDESCRIPTION: Histogram metrics showing the distribution of HTTP request durations across different endpoints. Each metric includes bucket counts for different duration thresholds, along with sum and count values. The metrics use the prometheus_http_request_duration_seconds prefix and include labels for different handlers.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_5\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_request_duration_seconds_bucket{handler=\"/config\",le=\"120\"} 13\nprometheus_http_request_duration_seconds_bucket{handler=\"/config\",le=\"+Inf\"} 13\nprometheus_http_request_duration_seconds_sum{handler=\"/config\"} 0.0024490289999999997\nprometheus_http_request_duration_seconds_count{handler=\"/config\"} 13\n# Additional metrics for other handlers...\n```\n\n----------------------------------------\n\nTITLE: Using Custom Prometheus Client Implementation\nDESCRIPTION: Configuration to provide a completely custom Prometheus client implementation instead of using the built-in one.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/codemirror-promql/README.md#2025-04-16_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nconst promQL = new PromQLExtension().setComplete({remote: MyPrometheusClient})\n```\n\n----------------------------------------\n\nTITLE: Example of Instant Query Expected to Fail\nDESCRIPTION: Shows an example of a query that is expected to fail without specifying a specific error message.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/promql/promqltest/README.md#2025-04-16_snippet_7\n\nLANGUAGE: promql\nCODE:\n```\neval instant at 1m ceil({__name__=~'testmetric1|testmetric2'})\nexpect fail\n```\n\n----------------------------------------\n\nTITLE: Enable Delayed Compaction Flag\nDESCRIPTION: Command flag to enable delayed head compaction start time\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=delayed-compaction\n```\n\n----------------------------------------\n\nTITLE: Label Offset Table Binary Layout\nDESCRIPTION: Specifies the binary format for storing label offset entries that track label index sections. Contains length, entries count, label name, offset values, and CRC32 checksum. Note: This format is no longer in use.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/index.md#2025-04-16_snippet_7\n\nLANGUAGE: ascii-art\nCODE:\n```\n\n len <4b>             #entries <4b>        \n\n  \n   n = 1 <1b>                             \n  \n  len(name) <uvarint>   name <bytes>     \n  \n   offset <uvarint64>                     \n  \n                    . . .                   \n\n  CRC32 <4b>                                \n\n```\n\n----------------------------------------\n\nTITLE: Prometheus HTTP Request Duration Metrics Data\nDESCRIPTION: Collection of Prometheus metrics showing HTTP request durations across various endpoints. The metrics include histogram buckets with different latency thresholds (from 0.1s to +Inf), sum values representing total duration, and count values showing the number of requests.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_7\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/targets\"} 0.5432347889999999\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/targets\"} 191\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/targets/metadata\",le=\"0.1\"} 18\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/targets/metadata\",le=\"0.2\"} 18\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/targets/metadata\",le=\"0.4\"} 18\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/targets/metadata\",le=\"1\"} 18\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/targets/metadata\",le=\"3\"} 18\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/targets/metadata\",le=\"8\"} 18\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/targets/metadata\",le=\"20\"} 18\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/targets/metadata\",le=\"60\"} 18\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/targets/metadata\",le=\"120\"} 18\nprometheus_http_request_duration_seconds_bucket{handler=\"/api/v1/targets/metadata\",le=\"+Inf\"} 18\nprometheus_http_request_duration_seconds_sum{handler=\"/api/v1/targets/metadata\"} 0.043789973\nprometheus_http_request_duration_seconds_count{handler=\"/api/v1/targets/metadata\"} 18\nprometheus_http_request_duration_seconds_bucket{handler=\"/assets/*filepath\",le=\"0.1\"} 385\nprometheus_http_request_duration_seconds_bucket{handler=\"/assets/*filepath\",le=\"0.2\"} 481\nprometheus_http_request_duration_seconds_bucket{handler=\"/assets/*filepath\",le=\"0.4\"} 528\nprometheus_http_request_duration_seconds_bucket{handler=\"/assets/*filepath\",le=\"1\"} 612\nprometheus_http_request_duration_seconds_bucket{handler=\"/assets/*filepath\",le=\"3\"} 634\nprometheus_http_request_duration_seconds_bucket{handler=\"/assets/*filepath\",le=\"8\"} 640\nprometheus_http_request_duration_seconds_bucket{handler=\"/assets/*filepath\",le=\"20\"} 641\nprometheus_http_request_duration_seconds_bucket{handler=\"/assets/*filepath\",le=\"60\"} 642\nprometheus_http_request_duration_seconds_bucket{handler=\"/assets/*filepath\",le=\"120\"} 643\nprometheus_http_request_duration_seconds_bucket{handler=\"/assets/*filepath\",le=\"+Inf\"} 643\nprometheus_http_request_duration_seconds_sum{handler=\"/assets/*filepath\"} 280.0660388799997\nprometheus_http_request_duration_seconds_count{handler=\"/assets/*filepath\"} 643\nprometheus_http_request_duration_seconds_bucket{handler=\"/classic/static/*filepath\",le=\"0.1\"} 103\nprometheus_http_request_duration_seconds_bucket{handler=\"/classic/static/*filepath\",le=\"0.2\"} 103\nprometheus_http_request_duration_seconds_bucket{handler=\"/classic/static/*filepath\",le=\"0.4\"} 103\nprometheus_http_request_duration_seconds_bucket{handler=\"/classic/static/*filepath\",le=\"1\"} 103\nprometheus_http_request_duration_seconds_bucket{handler=\"/classic/static/*filepath\",le=\"3\"} 103\nprometheus_http_request_duration_seconds_bucket{handler=\"/classic/static/*filepath\",le=\"8\"} 103\nprometheus_http_request_duration_seconds_bucket{handler=\"/classic/static/*filepath\",le=\"20\"} 103\nprometheus_http_request_duration_seconds_bucket{handler=\"/classic/static/*filepath\",le=\"60\"} 103\nprometheus_http_request_duration_seconds_bucket{handler=\"/classic/static/*filepath\",le=\"120\"} 103\nprometheus_http_request_duration_seconds_bucket{handler=\"/classic/static/*filepath\",le=\"+Inf\"} 103\nprometheus_http_request_duration_seconds_sum{handler=\"/classic/static/*filepath\"} 0.006113046000000001\nprometheus_http_request_duration_seconds_count{handler=\"/classic/static/*filepath\"} 103\nprometheus_http_request_duration_seconds_bucket{handler=\"/config\",le=\"0.1\"} 13\nprometheus_http_request_duration_seconds_bucket{handler=\"/config\",le=\"0.2\"} 13\nprometheus_http_request_duration_seconds_bucket{handler=\"/config\",le=\"0.4\"} 13\nprometheus_http_request_duration_seconds_bucket{handler=\"/config\",le=\"1\"} 13\nprometheus_http_request_duration_seconds_bucket{handler=\"/config\",le=\"3\"} 13\nprometheus_http_request_duration_seconds_bucket{handler=\"/config\",le=\"8\"} 13\nprometheus_http_request_duration_seconds_bucket{handler=\"/config\",le=\"20\"} 13\nprometheus_http_request_duration_seconds_bucket{handler=\"/config\",le=\"60\"} 13\nprometheus_http_request_duration_seconds_bucket{handler=\"/config\",le=\"120\"} 13\nprometheus_http_request_duration_seconds_bucket{handler=\"/config\",le=\"+Inf\"} 13\nprometheus_http_request_duration_seconds_sum{handler=\"/config\"} 0.0024490289999999997\nprometheus_http_request_duration_seconds_count{handler=\"/config\"} 13\nprometheus_http_request_duration_seconds_bucket{handler=\"/consoles/*filepath\",le=\"0.1\"} 33\nprometheus_http_request_duration_seconds_bucket{handler=\"/consoles/*filepath\",le=\"0.2\"} 34\nprometheus_http_request_duration_seconds_bucket{handler=\"/consoles/*filepath\",le=\"0.4\"} 34\nprometheus_http_request_duration_seconds_bucket{handler=\"/consoles/*filepath\",le=\"1\"} 34\nprometheus_http_request_duration_seconds_bucket{handler=\"/consoles/*filepath\",le=\"3\"} 34\nprometheus_http_request_duration_seconds_bucket{handler=\"/consoles/*filepath\",le=\"8\"} 34\nprometheus_http_request_duration_seconds_bucket{handler=\"/consoles/*filepath\",le=\"20\"} 34\nprometheus_http_request_duration_seconds_bucket{handler=\"/consoles/*filepath\",le=\"60\"} 34\nprometheus_http_request_duration_seconds_bucket{handler=\"/consoles/*filepath\",le=\"120\"} 34\nprometheus_http_request_duration_seconds_bucket{handler=\"/consoles/*filepath\",le=\"+Inf\"} 34\nprometheus_http_request_duration_seconds_sum{handler=\"/consoles/*filepath\"} 0.5689515199999999\nprometheus_http_request_duration_seconds_count{handler=\"/consoles/*filepath\"} 34\nprometheus_http_request_duration_seconds_bucket{handler=\"/debug/*subpath\",le=\"0.1\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/debug/*subpath\",le=\"0.2\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/debug/*subpath\",le=\"0.4\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/debug/*subpath\",le=\"1\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/debug/*subpath\",le=\"3\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/debug/*subpath\",le=\"8\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/debug/*subpath\",le=\"20\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/debug/*subpath\",le=\"60\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/debug/*subpath\",le=\"120\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/debug/*subpath\",le=\"+Inf\"} 4\nprometheus_http_request_duration_seconds_sum{handler=\"/debug/*subpath\"} 0.086499352\nprometheus_http_request_duration_seconds_count{handler=\"/debug/*subpath\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.ico\",le=\"0.1\"} 177\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.ico\",le=\"0.2\"} 177\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.ico\",le=\"0.4\"} 177\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.ico\",le=\"1\"} 177\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.ico\",le=\"3\"} 177\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.ico\",le=\"8\"} 177\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.ico\",le=\"20\"} 177\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.ico\",le=\"60\"} 177\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.ico\",le=\"120\"} 177\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.ico\",le=\"+Inf\"} 177\nprometheus_http_request_duration_seconds_sum{handler=\"/favicon.ico\"} 0.05591882500000002\nprometheus_http_request_duration_seconds_count{handler=\"/favicon.ico\"} 177\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.svg\",le=\"0.1\"} 770\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.svg\",le=\"0.2\"} 770\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.svg\",le=\"0.4\"} 770\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.svg\",le=\"1\"} 770\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.svg\",le=\"3\"} 770\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.svg\",le=\"8\"} 770\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.svg\",le=\"20\"} 770\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.svg\",le=\"60\"} 770\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.svg\",le=\"120\"} 770\nprometheus_http_request_duration_seconds_bucket{handler=\"/favicon.svg\",le=\"+Inf\"} 770\nprometheus_http_request_duration_seconds_sum{handler=\"/favicon.svg\"} 0.3058455699999999\nprometheus_http_request_duration_seconds_count{handler=\"/favicon.svg\"} 770\nprometheus_http_request_duration_seconds_bucket{handler=\"/federate\",le=\"0.1\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/federate\",le=\"0.2\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/federate\",le=\"0.4\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/federate\",le=\"1\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/federate\",le=\"3\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/federate\",le=\"8\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/federate\",le=\"20\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/federate\",le=\"60\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/federate\",le=\"120\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/federate\",le=\"+Inf\"} 4\nprometheus_http_request_duration_seconds_sum{handler=\"/federate\"} 0.05996980699999999\nprometheus_http_request_duration_seconds_count{handler=\"/federate\"} 4\nprometheus_http_request_duration_seconds_bucket{handler=\"/flags\",le=\"0.1\"} 9\nprometheus_http_request_duration_seconds_bucket{handler=\"/flags\",le=\"0.2\"} 9\nprometheus_http_request_duration_seconds_bucket{handler=\"/flags\",le=\"0.4\"} 9\nprometheus_http_request_duration_seconds_bucket{handler=\"/flags\",le=\"1\"} 9\nprometheus_http_request_duration_seconds_bucket{handler=\"/flags\",le=\"3\"} 9\nprometheus_http_request_duration_seconds_bucket{handler=\"/flags\",le=\"8\"} 9\n```\n\n----------------------------------------\n\nTITLE: PromQL Number Literal Tests\nDESCRIPTION: Examples of positive and negative number literals in PromQL syntax\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/test/expression.txt#2025-04-16_snippet_6\n\nLANGUAGE: promql\nCODE:\n```\n-42\n\n+42\n```\n\n----------------------------------------\n\nTITLE: Histogram Second Sample Format Diagram\nDESCRIPTION: Shows the format of the second histogram sample, using delta encoding relative to the first sample for timestamps and values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n\n ts_delta <varbit_int>  count_delta <varbit_int>  zero_count_delta <varbit_int>  sum_xor <varbit_xor>  pos_bucket_0_delta <varbit_int>  ...  pos_bucket_n_delta <varbit_int>  neg_bucket_0_delta <varbit_int>  ...  neg_bucket_n_delta <varbit_int> \n\n```\n\n----------------------------------------\n\nTITLE: Prometheus HTTP Request Totals\nDESCRIPTION: Total HTTP requests metrics broken down by endpoint and response code. Includes counts for various API endpoints, static assets, and system endpoints.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_7\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_requests_total{code=\"200\",handler=\"/-/healthy\"} 29524\nprometheus_http_requests_total{code=\"200\",handler=\"/api/v1/query\"} 326411\nprometheus_http_requests_total{code=\"200\",handler=\"/metrics\"} 4.059092e+06\n```\n\n----------------------------------------\n\nTITLE: Prometheus Release Notes\nDESCRIPTION: Markdown formatted release notes documenting changes across multiple Prometheus versions from 2.6.0 to 2.8.0, including major changes, features, enhancements and bug fixes\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_24\n\nLANGUAGE: markdown\nCODE:\n```\n## 2.8.0 / 2019-03-12\n\nThis release uses Write-Ahead Logging (WAL) for the remote_write API. This currently causes a slight increase in memory usage, which will be addressed in future releases.\n\n* [CHANGE] Default time retention is used only when no size based retention is specified. These are flags where time retention is specified by the flag `--storage.tsdb.retention` and size retention by `--storage.tsdb.retention.size`. #5216\n* [CHANGE] `prometheus_tsdb_storage_blocks_bytes_total` is now `prometheus_tsdb_storage_blocks_bytes`. prometheus/tsdb#506\n* [FEATURE] [EXPERIMENTAL] Time overlapping blocks are now allowed; vertical compaction and vertical query merge. It is an optional feature which is controlled by the `--storage.tsdb.allow-overlapping-blocks` flag, disabled by default. prometheus/tsdb#370\n* [ENHANCEMENT] Use the WAL for remote_write API. #4588\n* [ENHANCEMENT] Query performance improvements. prometheus/tsdb#531\n* [ENHANCEMENT] UI enhancements with upgrade to Bootstrap 4. #5226\n* [ENHANCEMENT] Reduce time that Alertmanagers are in flux when reloaded. #5126\n* [ENHANCEMENT] Limit number of metrics displayed on UI to 10000. #5139\n* [ENHANCEMENT] (1) Remember All/Unhealthy choice on target-overview when reloading page. (2) Resize text-input area on Graph page on mouseclick. #5201\n* [ENHANCEMENT] In `histogram_quantile` merge buckets with equivalent le values. #5158.\n* [ENHANCEMENT] Show list of offending labels in the error message in many-to-many scenarios. #5189\n* [ENHANCEMENT] Show `Storage Retention` criteria in effect on `/status` page. #5322\n* [BUGFIX] Fix sorting of rule groups. #5260\n* [BUGFIX] Fix support for password_file and bearer_token_file in Kubernetes SD. #5211\n* [BUGFIX] Scrape: catch errors when creating HTTP clients #5182. Adds new metrics:\n  * `prometheus_target_scrape_pools_total`\n  * `prometheus_target_scrape_pools_failed_total`\n  * `prometheus_target_scrape_pool_reloads_total`\n  * `prometheus_target_scrape_pool_reloads_failed_total`\n* [BUGFIX] Fix panic when aggregator param is not a literal. #5290\n```\n\n----------------------------------------\n\nTITLE: Extracting Prometheus Archive in Bash\nDESCRIPTION: Commands to extract the Prometheus archive and change to the resulting directory.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/getting_started.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntar xvfz prometheus-*.tar.gz\ncd prometheus-*\n```\n\n----------------------------------------\n\nTITLE: Bumping UI Module Version During Release Preparation\nDESCRIPTION: Command to increment the version of the UI module as part of the release preparation process, which should be run after updating the VERSION file and CHANGELOG.md.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/RELEASE.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake ui-bump-version\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus HTTP Request Duration Metrics\nDESCRIPTION: These metrics represent histogram buckets for HTTP request durations in seconds. They are broken down by handler and include sum and count values. The data provides insights into the performance of various Prometheus endpoints.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_8\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_request_duration_seconds_bucket{handler=\"/flags\",le=\"20\"} 9\nprometheus_http_request_duration_seconds_bucket{handler=\"/flags\",le=\"60\"} 9\nprometheus_http_request_duration_seconds_bucket{handler=\"/flags\",le=\"120\"} 9\nprometheus_http_request_duration_seconds_bucket{handler=\"/flags\",le=\"+Inf\"} 9\nprometheus_http_request_duration_seconds_sum{handler=\"/flags\"} 0.001586781\nprometheus_http_request_duration_seconds_count{handler=\"/flags\"} 9\n\n# ... (similar metrics for other handlers)\n\nprometheus_http_request_duration_seconds_bucket{handler=\"/tsdb-status\",le=\"0.1\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/tsdb-status\",le=\"0.2\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/tsdb-status\",le=\"0.4\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/tsdb-status\",le=\"1\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/tsdb-status\",le=\"3\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/tsdb-status\",le=\"8\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/tsdb-status\",le=\"20\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/tsdb-status\",le=\"60\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/tsdb-status\",le=\"120\"} 49\nprometheus_http_request_duration_seconds_bucket{handler=\"/tsdb-status\",le=\"+Inf\"} 49\nprometheus_http_request_duration_seconds_sum{handler=\"/tsdb-status\"} 0.018204165\nprometheus_http_request_duration_seconds_count{handler=\"/tsdb-status\"} 49\n# HELP prometheus_http_requests_total Counter of HTTP requests.\n```\n\n----------------------------------------\n\nTITLE: Defining Go Build Information Metrics in Prometheus Format\nDESCRIPTION: Defines a gauge metric for Go build information with build details as labels. This metric provides version information about the main Go module used in the application.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.5mfs.om.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP go_build_info Build information about the main Go module.\n# TYPE go_build_info gauge\ngo_build_info{checksum=\"\",path=\"\",version=\"\"} 1.0\n```\n\n----------------------------------------\n\nTITLE: Integer Native Histogram Record Format (Exponential Bucketing)\nDESCRIPTION: Record format for type 7 integer native histograms using exponential bucketing. Includes fields for id, timestamp, schema, counts, thresholds, and bucket spans.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/wal.md#2025-04-16_snippet_7\n\nLANGUAGE: ascii-art\nCODE:\n```\n\n type = 7 <1b>                                                         \n\n                     \n  id <8b>             timestamp <8b>                                \n                     \n  \n  id_delta <uvarint>  timestamp_delta <uvarint>                     \n  \n  counter_reset_hint <1b>  schema <varint>                          \n  \n  zero_threshold (float) <8b>     zero_count <uvarint>              \n  \n  count <uvarint>  sum (float) <8b>                                 \n  \n  positive_spans_num <uvarint>                                       \n  \n  positive_span_offset_1 <varint>  positive_span_len_1 <uvarint32>  \n  \n  . . .                                                              \n  \n  negative_spans_num <uvarint>                                       \n  \n  negative_span_offset <varint>  negative_span_len <uvarint32>      \n  \n  . . .                                                              \n  \n  positive_bkts_num <uvarint>                                        \n  \n  positive_bkt_1 <varint>  . . .  positive_bkt_n <varint>          \n  \n  negative_bkts_num <uvarint>                                        \n  \n  negative_bkt_1 <varint>  . . .  negative_bkt_n <varint>          \n  \n                              . . .                                    \n\n```\n\n----------------------------------------\n\nTITLE: Configuring Metadata Attachment for Kubernetes Service Discovery in YAML\nDESCRIPTION: This YAML snippet defines the metadata attachment configuration for Kubernetes service discovery in Prometheus. It allows attaching node metadata to discovered targets for specific roles.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_34\n\nLANGUAGE: yaml\nCODE:\n```\nattach_metadata:\n  [ node: <boolean> | default = false ]\n```\n\n----------------------------------------\n\nTITLE: Prometheus HTTP Duration Metrics\nDESCRIPTION: HTTP request duration metrics for various endpoints including /rules, /service-discovery, /status, /targets, and /tsdb-status. Shows histogram bucket data with different duration thresholds.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_6\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_request_duration_seconds_bucket{handler=\"/rules\",le=\"+Inf\"} 8673\nprometheus_http_request_duration_seconds_sum{handler=\"/rules\"} 2.776021043000005\nprometheus_http_request_duration_seconds_count{handler=\"/rules\"} 8673\n```\n\n----------------------------------------\n\nTITLE: Histogram Chunk Data Format Diagram\nDESCRIPTION: Shows the structure of histogram-encoded data chunks, including sample count, flags, threshold, and schema.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n\n num_samples <uint16>  histogram_flags <1 byte>  zero_threshold <1 or 9 bytes>  schema <varbit_int>  pos_spans <data>  neg_spans <data>  custom_values <data>  samples <data>  padding <x bits> \n\n```\n\n----------------------------------------\n\nTITLE: Displaying Prometheus HTTP Response Size Metrics in Prometheus Format\nDESCRIPTION: This shows HTTP response size histogram metrics for various Prometheus endpoints. Each metric includes bucket counters across different size thresholds, along with summary statistics (sum and count). The metrics help monitor response size distributions for different API endpoints.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_12\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE prometheus_http_response_size_bytes histogram\nprometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"100\"} 688\nprometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"1000\"} 688\nprometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"10000\"} 688\nprometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"100000\"} 688\nprometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"1e+06\"} 688\nprometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"1e+07\"} 688\nprometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"1e+08\"} 688\nprometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"1e+09\"} 688\nprometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"+Inf\"} 688\nprometheus_http_response_size_bytes_sum{handler=\"/\"} 19952\nprometheus_http_response_size_bytes_count{handler=\"/\"} 688\nprometheus_http_response_size_bytes_bucket{handler=\"/-/healthy\",le=\"100\"} 29524\nprometheus_http_response_size_bytes_bucket{handler=\"/-/healthy\",le=\"1000\"} 29524\nprometheus_http_response_size_bytes_bucket{handler=\"/-/healthy\",le=\"10000\"} 29524\nprometheus_http_response_size_bytes_bucket{handler=\"/-/healthy\",le=\"100000\"} 29524\nprometheus_http_response_size_bytes_bucket{handler=\"/-/healthy\",le=\"1e+06\"} 29524\nprometheus_http_response_size_bytes_bucket{handler=\"/-/healthy\",le=\"1e+07\"} 29524\nprometheus_http_response_size_bytes_bucket{handler=\"/-/healthy\",le=\"1e+08\"} 29524\nprometheus_http_response_size_bytes_bucket{handler=\"/-/healthy\",le=\"1e+09\"} 29524\nprometheus_http_response_size_bytes_bucket{handler=\"/-/healthy\",le=\"+Inf\"} 29524\nprometheus_http_response_size_bytes_sum{handler=\"/-/healthy\"} 885720\nprometheus_http_response_size_bytes_count{handler=\"/-/healthy\"} 29524\nprometheus_http_response_size_bytes_bucket{handler=\"/-/ready\",le=\"100\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/-/ready\",le=\"1000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/-/ready\",le=\"10000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/-/ready\",le=\"100000\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/-/ready\",le=\"1e+06\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/-/ready\",le=\"1e+07\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/-/ready\",le=\"1e+08\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/-/ready\",le=\"1e+09\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/-/ready\",le=\"+Inf\"} 49\nprometheus_http_response_size_bytes_sum{handler=\"/-/ready\"} 1372\nprometheus_http_response_size_bytes_count{handler=\"/-/ready\"} 49\nprometheus_http_response_size_bytes_bucket{handler=\"/alerts\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/alerts\",le=\"1000\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/alerts\",le=\"10000\"} 48\nprometheus_http_response_size_bytes_bucket{handler=\"/alerts\",le=\"100000\"} 48\nprometheus_http_response_size_bytes_bucket{handler=\"/alerts\",le=\"1e+06\"} 48\nprometheus_http_response_size_bytes_bucket{handler=\"/alerts\",le=\"1e+07\"} 48\nprometheus_http_response_size_bytes_bucket{handler=\"/alerts\",le=\"1e+08\"} 48\nprometheus_http_response_size_bytes_bucket{handler=\"/alerts\",le=\"1e+09\"} 48\nprometheus_http_response_size_bytes_bucket{handler=\"/alerts\",le=\"+Inf\"} 48\nprometheus_http_response_size_bytes_sum{handler=\"/alerts\"} 84096\nprometheus_http_response_size_bytes_count{handler=\"/alerts\"} 48\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/*path\",le=\"100\"} 27\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/*path\",le=\"1000\"} 27\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/*path\",le=\"10000\"} 27\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/*path\",le=\"100000\"} 27\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/*path\",le=\"1e+06\"} 27\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/*path\",le=\"1e+07\"} 27\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/*path\",le=\"1e+08\"} 27\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/*path\",le=\"1e+09\"} 27\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/*path\",le=\"+Inf\"} 27\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/*path\"} 0\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/*path\"} 27\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alertmanagers\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alertmanagers\",le=\"1000\"} 8\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alertmanagers\",le=\"10000\"} 8\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alertmanagers\",le=\"100000\"} 8\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alertmanagers\",le=\"1e+06\"} 8\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alertmanagers\",le=\"1e+07\"} 8\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alertmanagers\",le=\"1e+08\"} 8\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alertmanagers\",le=\"1e+09\"} 8\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alertmanagers\",le=\"+Inf\"} 8\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/alertmanagers\"} 1064\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/alertmanagers\"} 8\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alerts\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alerts\",le=\"1000\"} 14635\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alerts\",le=\"10000\"} 14635\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alerts\",le=\"100000\"} 14635\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alerts\",le=\"1e+06\"} 14635\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alerts\",le=\"1e+07\"} 14635\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alerts\",le=\"1e+08\"} 14635\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alerts\",le=\"1e+09\"} 14635\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/alerts\",le=\"+Inf\"} 14635\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/alerts\"} 1.0260926e+07\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/alerts\"} 14635\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/format_query\",le=\"100\"} 0\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/format_query\",le=\"1000\"} 4\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/format_query\",le=\"10000\"} 4\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/format_query\",le=\"100000\"} 4\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/format_query\",le=\"1e+06\"} 4\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/format_query\",le=\"1e+07\"} 4\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/format_query\",le=\"1e+08\"} 4\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/format_query\",le=\"1e+09\"} 4\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/format_query\",le=\"+Inf\"} 4\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/format_query\"} 495\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/format_query\"} 4\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/label/:name/values\",le=\"100\"} 8167\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/label/:name/values\",le=\"1000\"} 15939\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/label/:name/values\",le=\"10000\"} 17901\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/label/:name/values\",le=\"100000\"} 17971\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/label/:name/values\",le=\"1e+06\"} 17971\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/label/:name/values\",le=\"1e+07\"} 17971\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/label/:name/values\",le=\"1e+08\"} 17971\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/label/:name/values\",le=\"1e+09\"} 17971\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/label/:name/values\",le=\"+Inf\"} 17971\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/label/:name/values\"} 2.0009454e+07\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/label/:name/values\"} 17971\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/labels\",le=\"100\"} 102\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/labels\",le=\"1000\"} 4908\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/labels\",le=\"10000\"} 4921\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/labels\",le=\"100000\"} 4921\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/labels\",le=\"1e+06\"} 4921\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/labels\",le=\"1e+07\"} 4921\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/labels\",le=\"1e+08\"} 4921\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/labels\",le=\"1e+09\"} 4921\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/labels\",le=\"+Inf\"} 4921\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/labels\"} 3.16046e+06\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/labels\"} 4921\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/metadata\",le=\"100\"} 170\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/metadata\",le=\"1000\"} 1368\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/metadata\",le=\"10000\"} 1368\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/metadata\",le=\"100000\"} 2093\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/metadata\",le=\"1e+06\"} 2093\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/metadata\",le=\"1e+07\"} 2093\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/metadata\",le=\"1e+08\"} 2093\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/metadata\",le=\"1e+09\"} 2093\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/metadata\",le=\"+Inf\"} 2093\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/metadata\"} 1.5950362e+07\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/metadata\"} 2093\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"100\"} 12\n```\n\n----------------------------------------\n\nTITLE: Series Record Format in Prometheus Memory Snapshot\nDESCRIPTION: Detailed structure of a series record in the Prometheus memory snapshot format. Each record contains a single series with its metadata, series reference, labels, chunk range, chunk data (if exists), and sample buffer of the last 4 samples in memory.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/memory_snapshot.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n\n     Record Type <byte>      Series Ref <uint64>      \n\n               Number of Labels <uvarint>              \n\n     len(name_1) <uvarint>        name_1 <bytes>      \n\n     len(val_1) <uvarint>         val_1 <bytes>       \n\n                         . . .                         \n\n     len(name_N) <uvarint>        name_N <bytes>      \n\n     len(val_N) <uvarint>         val_N <bytes>       \n\n                  Chunk Range <int64>                  \n\n                 Chunk Exists <uvarint>                \n # 1 if head chunk exists, 0 otherwise to detect a nil |\n| # chunk. Below fields exists only when it's 1 here.   \n\n     Chunk Mint <int64>        Chunk Maxt <int64>     \n\n                 Chunk Encoding <byte>                 \n\n      len(Chunk) <uvarint>        Chunk <bytes>       \n\n|  sampleBuf[0].t <int64>  |  sampleBuf[0].v <float64>  | \n\n|  sampleBuf[1].t <int64>  |  sampleBuf[1].v <float64>  | \n\n|  sampleBuf[2].t <int64>  |  sampleBuf[2].v <float64>  | \n\n|  sampleBuf[3].t <int64>  |  sampleBuf[3].v <float64>  | \n\n```\n\n----------------------------------------\n\nTITLE: Float Native Histogram Record Format (Exponential Bucketing)\nDESCRIPTION: Record format for type 8 float native histograms using exponential bucketing. Similar to type 7 but uses float values for counts and buckets.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/wal.md#2025-04-16_snippet_8\n\nLANGUAGE: ascii-art\nCODE:\n```\n\n type = 8 <1b>                                                         \n\n                     \n  id <8b>             timestamp <8b>                                \n                     \n  \n  id_delta <uvarint>  timestamp_delta <uvarint>                     \n  \n  counter_reset_hint <1b>  schema <varint>                          \n  \n  zero_threshold (float) <8b>     zero_count (float) <8b>           \n  \n  count (float) <8b>  sum (float) <8b>                              \n  \n  positive_spans_num <uvarint>                                       \n  \n  positive_span_offset_1 <varint>  positive_span_len_1 <uvarint32>  \n  \n  . . .                                                              \n  \n  negative_spans_num <uvarint>                                       \n  \n  negative_span_offset <varint>  negative_span_len <uvarint32>      \n  \n  . . .                                                              \n  \n  positive_bkts_num <uvarint>                                        \n  \n  positive_bkt_1 (float) <8b>  . . .  positive_bkt_n (float) <8b>  \n  \n  negative_bkts_num <uvarint>                                        \n  \n  negative_bkt_1 (float) <8b>  . . .  negative_bkt_n (float) <8b>  \n  \n                              . . .                                    \n\n```\n\n----------------------------------------\n\nTITLE: Exemplar Record Format in Prometheus Memory Snapshot\nDESCRIPTION: Structure of an exemplar record in the Prometheus memory snapshot format. Each record can contain multiple exemplars encoded similarly to WAL format but with a different record type.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/memory_snapshot.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n\n                      Record Type <byte>                           \n\n                 \n  series ref <8b>     timestamp <8b>                            \n                 \n  \n  ref_delta <uvarint>  timestamp_delta <uvarint>  value <8b>   \n  \n   n = len(labels) <uvarint>                                     \n  \n      len(str_1) <uvarint>             str_1 <bytes>            \n  \n                               ...                               \n  \n      len(str_2n) <uvarint>            str_2n <bytes>           \n  \n                               . . .                               \n\n```\n\n----------------------------------------\n\nTITLE: Configuring Tracing with OTLP in Prometheus (YAML)\nDESCRIPTION: YAML configuration block for setting up tracing in Prometheus using the OTLP protocol. Defines parameters for client type, endpoint, sampling fraction, security settings, headers, compression, timeout, and TLS configuration. This is an experimental feature that allows traces to be exported to a tracing backend.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_52\n\nLANGUAGE: yaml\nCODE:\n```\n# Client used to export the traces. Options are 'http' or 'grpc'.\n[ client_type: <string> | default = grpc ]\n\n# Endpoint to send the traces to. Should be provided in format <host>:<port>.\n[ endpoint: <string> ]\n\n# Sets the probability a given trace will be sampled. Must be a float from 0 through 1.\n[ sampling_fraction: <float> | default = 0 ]\n\n# If disabled, the client will use a secure connection.\n[ insecure: <boolean> | default = false ]\n\n# Key-value pairs to be used as headers associated with gRPC or HTTP requests.\nheaders:\n  [ <string>: <string> ... ]\n\n# Compression key for supported compression types. Supported compression: gzip.\n[ compression: <string> ]\n\n# Maximum time the exporter will wait for each batch export.\n[ timeout: <duration> | default = 10s ]\n\n# TLS configuration.\ntls_config:\n  [ <tls_config> ]\n```\n\n----------------------------------------\n\nTITLE: Example of Instant Query Expected to Fail with Regex Match\nDESCRIPTION: Shows an example of a query that is expected to fail with a regex pattern matching the error message.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/promql/promqltest/README.md#2025-04-16_snippet_9\n\nLANGUAGE: promql\nCODE:\n```\neval instant at 1m ceil({__name__=~'testmetric1|testmetric2'})\nexpect fail regex \"vector cannot contain metrics .*|something else went wrong\"\n```\n\n----------------------------------------\n\nTITLE: Histogram Custom Values Format Diagram\nDESCRIPTION: Shows the structure for custom values in histograms, used primarily for schema -53 with custom bucket boundaries.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n\n num_values <varbit_uint>  value_0 <custom>  value_1 <custom>  ...  value_n <custom> \n\n```\n\n----------------------------------------\n\nTITLE: Querying HTTP Response Size Metrics in Prometheus\nDESCRIPTION: This snippet demonstrates the structure of Prometheus metrics for HTTP response sizes. It includes histogram buckets, sum, and count for different API endpoints, allowing analysis of response size distributions.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_13\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"1000\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"10000\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"100000\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"1e+06\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"1e+07\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"1e+08\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"1e+09\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"+Inf\"} 12\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/notifications\"} 360\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/notifications\"} 12\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus HTTP Response Size Bytes Histogram Metric\nDESCRIPTION: This snippet defines the 'prometheus_http_response_size_bytes' histogram metric for measuring the size of HTTP response bodies in Prometheus.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.prom.txt#2025-04-16_snippet_11\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP prometheus_http_response_size_bytes Histogram of response size for HTTP requests.\n```\n\n----------------------------------------\n\nTITLE: Histogram First Sample Format Diagram\nDESCRIPTION: Details the format of the first histogram sample, containing absolute values for timestamp, count, and bucket values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n\n ts <varbit_int>  count <varbit_uint>  zero_count <varbit_uint>  sum <float64>  pos_bucket_0 <varbit_int>  ...  pos_bucket_n <varbit_int>  neg_bucket_0 <varbit_int>  ...  neg_bucket_n <varbit_int> \n\n```\n\n----------------------------------------\n\nTITLE: Exemplar Records Structure in Prometheus WAL\nDESCRIPTION: Diagram showing the encoding format for Exemplar records which contain series ID, timestamp, value, and a list of labels. Similar to Sample records, they use delta encoding for series IDs and timestamps relative to the first exemplar.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/wal.md#2025-04-16_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n\n type = 4 <1b>                                                    \n\n                \n  id <8b>             timestamp <8b>                           \n                \n  \n  id_delta <uvarint>  timestamp_delta <uvarint>  value <8b>   \n  \n   n = len(labels) <uvarint>                                    \n  \n  len(str_1) <uvarint>  str_1 <bytes>                          \n  \n   ...                                                          \n  \n  len(str_2n) <uvarint>  str_2n <bytes>                       \n  \n                              . . .                               \n\n```\n\n----------------------------------------\n\nTITLE: Prometheus Chunks File Structure Diagram\nDESCRIPTION: Shows the overall structure of a chunks file, including magic number, version, padding, and chunks.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n\n  magic(0x85BD40DD) <4 byte>  \n\n    version(1) <1 byte>       \n\n    padding(0) <3 byte>       \n\n  \n          Chunk 1           \n  \n           ...              \n  \n          Chunk N           \n  \n\n```\n\n----------------------------------------\n\nTITLE: Postings Offset Table Binary Layout\nDESCRIPTION: Details the binary structure for storing postings offset entries, sorted by label name and value. Includes length, entries count, label name/value pairs, offset values, and CRC32 checksum.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/index.md#2025-04-16_snippet_8\n\nLANGUAGE: ascii-art\nCODE:\n```\n\n len <4b>             #entries <4b>        \n\n  \n   n = 2 <1b>                             \n  \n  len(name) <uvarint>   name <bytes>     \n  \n  len(value) <uvarint>  value <bytes>    \n  \n   offset <uvarint64>                     \n  \n                    . . .                   \n\n  CRC32 <4b>                                \n\n```\n\n----------------------------------------\n\nTITLE: Prometheus HTTP Metrics Data Sample\nDESCRIPTION: A collection of Prometheus metrics showing HTTP response size distributions and request counts across different API endpoints. The data includes histogram buckets for response sizes with different size thresholds and request counts for various HTTP status codes.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_8\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_requests_total{code=\"503\",handler=\"/api/v1/query\"} 7\nprometheus_http_requests_total{code=\"503\",handler=\"/api/v1/query_range\"} 4\nprometheus_http_requests_total{code=\"503\",handler=\"/api/v1/status/config\"} 1\nprometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"100\"} 688\nprometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"1000\"} 688\nprometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"10000\"} 688\n```\n\n----------------------------------------\n\nTITLE: Illustrating Records Encoding Structure in Prometheus WAL\nDESCRIPTION: Diagram showing how each record fragment is encoded in the WAL with a type byte, length field, CRC32 checksum, and data payload. The type byte includes reserved bits, compression flags, and type flags.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/wal.md#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n\n type <1b>  len <2b>  CRC32 <4b>  data <bytes> \n\n```\n\n----------------------------------------\n\nTITLE: Prometheus Metrics Snapshot\nDESCRIPTION: A snapshot of Prometheus metrics showing the current state of service discovery, target scraping intervals, metadata cache usage, and other internal operational metrics. The metrics are labeled with various dimensions such as scrape job names, intervals, and quantiles for histograms.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_17\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_sd_received_updates_total{name=\"scrape\"} 11820\nprometheus_sd_updates_delayed_total{name=\"notify\"} 0\nprometheus_sd_updates_delayed_total{name=\"scrape\"} 0\nprometheus_sd_updates_total{name=\"notify\"} 1\nprometheus_sd_updates_total{name=\"scrape\"} 2953\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.01\"} 14.982591232\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.05\"} 14.997567414\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.5\"} 14.999977915\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.9\"} 15.000793403\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.99\"} 15.017607167\nprometheus_target_interval_length_seconds_sum{interval=\"15s\"} 9.742237453453667e+06\nprometheus_target_interval_length_seconds_count{interval=\"15s\"} 649482\nprometheus_target_metadata_cache_bytes{scrape_job=\"alertmanager\"} 6817\nprometheus_target_metadata_cache_bytes{scrape_job=\"blackbox\"} 661\nprometheus_target_metadata_cache_bytes{scrape_job=\"caddy\"} 2365\nprometheus_target_metadata_cache_bytes{scrape_job=\"cadvisor\"} 4547\nprometheus_target_metadata_cache_bytes{scrape_job=\"grafana\"} 37608\nprometheus_target_metadata_cache_bytes{scrape_job=\"node\"} 13900\nprometheus_target_metadata_cache_bytes{scrape_job=\"prometheus\"} 20265\nprometheus_target_metadata_cache_bytes{scrape_job=\"random\"} 792\nprometheus_target_metadata_cache_entries{scrape_job=\"alertmanager\"} 86\nprometheus_target_metadata_cache_entries{scrape_job=\"blackbox\"} 13\nprometheus_target_metadata_cache_entries{scrape_job=\"caddy\"} 47\nprometheus_target_metadata_cache_entries{scrape_job=\"cadvisor\"} 93\nprometheus_target_metadata_cache_entries{scrape_job=\"grafana\"} 373\nprometheus_target_metadata_cache_entries{scrape_job=\"node\"} 293\nprometheus_target_metadata_cache_entries{scrape_job=\"prometheus\"} 237\nprometheus_target_metadata_cache_entries{scrape_job=\"random\"} 16\nprometheus_target_scrape_pool_exceeded_label_limits_total 0\nprometheus_target_scrape_pool_exceeded_target_limit_total 0\nprometheus_target_scrape_pool_reloads_failed_total 0\nprometheus_target_scrape_pool_reloads_total 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"alertmanager\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"blackbox\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"caddy\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"cadvisor\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"grafana\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"node\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"prometheus\"} 0\nprometheus_target_scrape_pool_symboltable_items{scrape_job=\"random\"} 0\nprometheus_target_scrape_pool_sync_total{scrape_job=\"alertmanager\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"blackbox\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"caddy\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"cadvisor\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"grafana\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"node\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"prometheus\"} 2953\nprometheus_target_scrape_pool_sync_total{scrape_job=\"random\"} 2953\nprometheus_target_scrape_pool_target_limit{scrape_job=\"alertmanager\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"blackbox\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"caddy\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"cadvisor\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"grafana\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"node\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"prometheus\"} 0\nprometheus_target_scrape_pool_target_limit{scrape_job=\"random\"} 0\nprometheus_target_scrape_pool_targets{scrape_job=\"alertmanager\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"blackbox\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"caddy\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"cadvisor\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"grafana\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"node\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"prometheus\"} 1\nprometheus_target_scrape_pool_targets{scrape_job=\"random\"} 4\nprometheus_target_scrape_pools_failed_total 0\nprometheus_target_scrape_pools_total 8\nprometheus_target_scrapes_cache_flush_forced_total 0\nprometheus_target_scrapes_exceeded_body_size_limit_total 0\nprometheus_target_scrapes_exceeded_native_histogram_bucket_limit_total 0\nprometheus_target_scrapes_exceeded_sample_limit_total 0\nprometheus_target_scrapes_exemplar_out_of_order_total 0\nprometheus_target_scrapes_sample_duplicate_timestamp_total 0\nprometheus_target_scrapes_sample_out_of_bounds_total 0\nprometheus_target_scrapes_sample_out_of_order_total 455\nprometheus_target_sync_failed_total{scrape_job=\"alertmanager\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"blackbox\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"caddy\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"cadvisor\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"grafana\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"node\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"prometheus\"} 0\nprometheus_target_sync_failed_total{scrape_job=\"random\"} 0\nprometheus_target_sync_length_seconds{scrape_job=\"alertmanager\",quantile=\"0.01\"} 2.0522e-05\nprometheus_target_sync_length_seconds{scrape_job=\"alertmanager\",quantile=\"0.05\"} 2.0522e-05\nprometheus_target_sync_length_seconds{scrape_job=\"alertmanager\",quantile=\"0.5\"} 2.0522e-05\nprometheus_target_sync_length_seconds{scrape_job=\"alertmanager\",quantile=\"0.9\"} 0.000141485\nprometheus_target_sync_length_seconds{scrape_job=\"alertmanager\",quantile=\"0.99\"} 0.000141485\nprometheus_target_sync_length_seconds_sum{scrape_job=\"alertmanager\"} 0.13103036\nprometheus_target_sync_length_seconds_count{scrape_job=\"alertmanager\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"blackbox\",quantile=\"0.01\"} 3.9252e-05\nprometheus_target_sync_length_seconds{scrape_job=\"blackbox\",quantile=\"0.05\"} 3.9252e-05\nprometheus_target_sync_length_seconds{scrape_job=\"blackbox\",quantile=\"0.5\"} 3.9252e-05\nprometheus_target_sync_length_seconds{scrape_job=\"blackbox\",quantile=\"0.9\"} 6.2134e-05\nprometheus_target_sync_length_seconds{scrape_job=\"blackbox\",quantile=\"0.99\"} 6.2134e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"blackbox\"} 0.6044201539999996\nprometheus_target_sync_length_seconds_count{scrape_job=\"blackbox\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"caddy\",quantile=\"0.01\"} 1.3759e-05\nprometheus_target_sync_length_seconds{scrape_job=\"caddy\",quantile=\"0.05\"} 1.3759e-05\nprometheus_target_sync_length_seconds{scrape_job=\"caddy\",quantile=\"0.5\"} 1.3759e-05\nprometheus_target_sync_length_seconds{scrape_job=\"caddy\",quantile=\"0.9\"} 7.8256e-05\nprometheus_target_sync_length_seconds{scrape_job=\"caddy\",quantile=\"0.99\"} 7.8256e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"caddy\"} 0.10369844599999971\nprometheus_target_sync_length_seconds_count{scrape_job=\"caddy\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"cadvisor\",quantile=\"0.01\"} 2.0452e-05\nprometheus_target_sync_length_seconds{scrape_job=\"cadvisor\",quantile=\"0.05\"} 2.0452e-05\nprometheus_target_sync_length_seconds{scrape_job=\"cadvisor\",quantile=\"0.5\"} 2.0452e-05\nprometheus_target_sync_length_seconds{scrape_job=\"cadvisor\",quantile=\"0.9\"} 4.2337e-05\nprometheus_target_sync_length_seconds{scrape_job=\"cadvisor\",quantile=\"0.99\"} 4.2337e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"cadvisor\"} 0.10489659999999998\nprometheus_target_sync_length_seconds_count{scrape_job=\"cadvisor\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"grafana\",quantile=\"0.01\"} 1.4995e-05\nprometheus_target_sync_length_seconds{scrape_job=\"grafana\",quantile=\"0.05\"} 1.4995e-05\nprometheus_target_sync_length_seconds{scrape_job=\"grafana\",quantile=\"0.5\"} 1.4995e-05\nprometheus_target_sync_length_seconds{scrape_job=\"grafana\",quantile=\"0.9\"} 1.7284e-05\nprometheus_target_sync_length_seconds{scrape_job=\"grafana\",quantile=\"0.99\"} 1.7284e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"grafana\"} 0.09031192700000017\nprometheus_target_sync_length_seconds_count{scrape_job=\"grafana\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"node\",quantile=\"0.01\"} 4.1607e-05\nprometheus_target_sync_length_seconds{scrape_job=\"node\",quantile=\"0.05\"} 4.1607e-05\nprometheus_target_sync_length_seconds{scrape_job=\"node\",quantile=\"0.5\"} 4.1607e-05\nprometheus_target_sync_length_seconds{scrape_job=\"node\",quantile=\"0.9\"} 7.416e-05\nprometheus_target_sync_length_seconds{scrape_job=\"node\",quantile=\"0.99\"} 7.416e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"node\"} 0.11539821299999993\nprometheus_target_sync_length_seconds_count{scrape_job=\"node\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"prometheus\",quantile=\"0.01\"} 1.5564e-05\nprometheus_target_sync_length_seconds{scrape_job=\"prometheus\",quantile=\"0.05\"} 1.5564e-05\nprometheus_target_sync_length_seconds{scrape_job=\"prometheus\",quantile=\"0.5\"} 1.5564e-05\nprometheus_target_sync_length_seconds{scrape_job=\"prometheus\",quantile=\"0.9\"} 1.961e-05\nprometheus_target_sync_length_seconds{scrape_job=\"prometheus\",quantile=\"0.99\"} 1.961e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"prometheus\"} 0.10655758600000016\nprometheus_target_sync_length_seconds_count{scrape_job=\"prometheus\"} 2953\nprometheus_target_sync_length_seconds{scrape_job=\"random\",quantile=\"0.01\"} 4.1299e-05\nprometheus_target_sync_length_seconds{scrape_job=\"random\",quantile=\"0.05\"} 4.1299e-05\nprometheus_target_sync_length_seconds{scrape_job=\"random\",quantile=\"0.5\"} 4.1299e-05\nprometheus_target_sync_length_seconds{scrape_job=\"random\",quantile=\"0.9\"} 4.8586e-05\nprometheus_target_sync_length_seconds{scrape_job=\"random\",quantile=\"0.99\"} 4.8586e-05\nprometheus_target_sync_length_seconds_sum{scrape_job=\"random\"} 0.20406449899999993\n```\n\n----------------------------------------\n\nTITLE: Cloning the Prometheus repository\nDESCRIPTION: Commands to clone the Prometheus repository from GitHub and navigate into the project directory. This is the first step for building Prometheus from source.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/prometheus/prometheus.git\ncd prometheus\n```\n\n----------------------------------------\n\nTITLE: Table of Contents Binary Layout\nDESCRIPTION: Defines the binary structure for the index file's entry point that contains references to all major sections. Includes references to symbols, series, label indices, offset tables, and CRC32 checksum.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/index.md#2025-04-16_snippet_9\n\nLANGUAGE: ascii-art\nCODE:\n```\n\n ref(symbols) <8b>                       \n\n ref(series) <8b>                        \n\n ref(label indices start) <8b>           \n\n ref(label offset table) <8b>            \n\n ref(postings start) <8b>                \n\n ref(postings offset table) <8b>         \n\n CRC32 <4b>                              \n\n```\n\n----------------------------------------\n\nTITLE: Tombstones File Structure Format in Prometheus\nDESCRIPTION: Diagram showing the binary structure of a tombstones file. The file begins with a magic number (0x0130BA30) and version byte, followed by multiple tombstone entries, and ends with a CRC checksum. The last 8 bytes of the file specify the offset to the stones section, which is padded to a multiple of 4 bytes.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/tombstones.md#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n\n magic(0x0130BA30) <4b>      version(1) <1 byte> \n\n  \n                 Tombstone 1                    \n  \n                       ...                      \n  \n                 Tombstone N                    \n  \n                   CRC<4b>                      \n  \n\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus HTTP Response Size Metrics\nDESCRIPTION: This snippet demonstrates the structure of Prometheus metrics for HTTP response sizes. It includes histogram buckets, sums, and counts for various API endpoints, providing detailed insights into response size distributions.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_9\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/metadata\"} 2093\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"100\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"1000\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"10000\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"100000\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"1e+06\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"1e+07\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"1e+08\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"1e+09\"} 12\nprometheus_http_response_size_bytes_bucket{handler=\"/api/v1/notifications\",le=\"+Inf\"} 12\nprometheus_http_response_size_bytes_sum{handler=\"/api/v1/notifications\"} 360\nprometheus_http_response_size_bytes_count{handler=\"/api/v1/notifications\"} 12\n```\n\n----------------------------------------\n\nTITLE: Head Chunks File Structure ASCII Diagram\nDESCRIPTION: Visual representation of the head chunks file format showing the magic number (0x0130BC91), version byte, padding, and sequential chunks layout.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/head_chunks.md#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n\n  magic(0x0130BC91) <4 byte>  \n\n    version(1) <1 byte>       \n\n    padding(0) <3 byte>       \n\n  \n          Chunk 1           \n  \n           ...              \n  \n          Chunk N           \n  \n\n```\n\n----------------------------------------\n\nTITLE: PromQL Quoted Identifier Tests\nDESCRIPTION: Examples of quoted metric names and label matchers using string literals\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/test/expression.txt#2025-04-16_snippet_10\n\nLANGUAGE: promql\nCODE:\n```\n{\"metric_name\"}\n\n{\"foo\"=\"bar\"}\n\n{\"metric_name\", \"foo\"=\"bar\"}\n```\n\n----------------------------------------\n\nTITLE: Enable Per-Step Stats Flag\nDESCRIPTION: Command flag to enable per-step query statistics\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=promql-per-step-stats\n```\n\n----------------------------------------\n\nTITLE: Illustrating Symbol Table Structure in ASCII Art\nDESCRIPTION: This ASCII art diagram shows the structure of the Symbol Table section in the index file, including length fields, number of symbols, and individual string entries.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/index.md#2025-04-16_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```\n\n len <4b>            #symbols <4b>       \n\n  \n  len(str_1) <uvarint>  str_1 <bytes>  \n  \n                 . . .                  \n  \n  len(str_n) <uvarint>  str_n <bytes>  \n  \n\n CRC32 <4b>                               \n\n```\n```\n\n----------------------------------------\n\nTITLE: Illustrating Index File Structure in ASCII Art\nDESCRIPTION: This ASCII art diagram shows the overall structure of the index file, including its main sections like Symbol Table, Series, Label Indexes, Postings, and TOC.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/index.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```\n\n magic(0xBAAAD700) <4b>      version(1) <1 byte> \n\n  \n                  Symbol Table                  \n  \n                     Series                     \n  \n                  Label Index 1                 \n  \n                       ...                      \n  \n                  Label Index N                 \n  \n                    Postings 1                  \n  \n                       ...                      \n  \n                    Postings N                  \n  \n                Label Offset Table              \n  \n              Postings Offset Table             \n  \n                       TOC                      \n  \n\n```\n```\n\n----------------------------------------\n\nTITLE: Structure of HeadChunkRef in Prometheus TSDB\nDESCRIPTION: This snippet details the structure of HeadChunkRef in Prometheus TSDB. It's an 8-byte integer packing a 5-byte HeadSeriesRef and a 3-byte HeadChunkID, with implications on series and chunk limits.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/refs.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nA `HeadChunkRef` is an 8 byte integer that packs together:\n\n* 5 Bytes for `HeadSeriesRef`.\n* 3 Bytes for `HeadChunkID` (uint64) (see below).\n```\n\n----------------------------------------\n\nTITLE: Monitoring Prometheus Notification Metrics\nDESCRIPTION: These metrics detail the Prometheus alerting notification system including discovered alertmanagers, queue capacity, error rates, and latency statistics for alert delivery.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_13\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_notifications_alertmanagers_discovered 1\nprometheus_notifications_dropped_total 0\nprometheus_notifications_errors_total{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\"} 0\nprometheus_notifications_latency_seconds{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\",quantile=\"0.5\"} 0.001566044\nprometheus_notifications_latency_seconds{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\",quantile=\"0.9\"} 0.003927931\nprometheus_notifications_latency_seconds{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\",quantile=\"0.99\"} 0.013928135\nprometheus_notifications_latency_seconds_sum{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\"} 194.15032606200046\nprometheus_notifications_latency_seconds_count{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\"} 75180\nprometheus_notifications_queue_capacity 10000\nprometheus_notifications_queue_length 0\nprometheus_notifications_sent_total{alertmanager=\"http://demo.do.prometheus.io:9093/api/v2/alerts\"} 141616\n```\n\n----------------------------------------\n\nTITLE: Illustrating Series Entry Structure in ASCII Art\nDESCRIPTION: This ASCII art diagram shows the detailed structure of a single series entry, including labels, chunks count, and chunk metadata.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/index.md#2025-04-16_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n```\n\n len <uvarint>                                                            \n\n  \n                      labels count <uvarint64>                          \n  \n                          \n                ref(l_i.name) <uvarint32>                             \n                          \n                ref(l_i.value) <uvarint32>                            \n                          \n                              ...                                       \n  \n                      chunks count <uvarint64>                          \n  \n                          \n                c_0.mint <varint64>                                   \n                          \n                c_0.maxt - c_0.mint <uvarint64>                       \n                          \n                ref(c_0.data) <uvarint64>                             \n                          \n                          \n                c_i.mint - c_i-1.maxt <uvarint64>                     \n                          \n                c_i.maxt - c_i.mint <uvarint64>                       \n                          \n                ref(c_i.data) - ref(c_i-1.data) <varint64>            \n                          \n                              ...                                       \n  \n\n CRC32 <4b>                                                               \n\n```\n```\n\n----------------------------------------\n\nTITLE: Enable Experimental PromQL Functions Flag\nDESCRIPTION: Command flag to enable experimental PromQL functions\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=promql-experimental-functions\n```\n\n----------------------------------------\n\nTITLE: Sample 1 Data Structure\nDESCRIPTION: ASCII diagram showing the layout of the second sample data with delta encodings and XOR values\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_12\n\nLANGUAGE: ascii-diagram\nCODE:\n```\n\n ts_delta <varbit_int>  count_xor <varbit_xor>  zero_count_xor <varbit_xor>  sum_xor <varbit_xor>  pos_bucket_0_xor <varbit_xor>  ...  pos_bucket_n_xor <varbit_xor>  neg_bucket_0_xor <varbit_xor>  ...  neg_bucket_n_xor <varbit_xor> \n\n```\n\n----------------------------------------\n\nTITLE: Analyzing Prometheus Rule Evaluation Metrics\nDESCRIPTION: These metrics provide detailed statistics about Prometheus rule evaluations, including duration, failure counts, and total evaluations per rule group.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/testdata/alltypes.237mfs.nometa.prom.txt#2025-04-16_snippet_15\n\nLANGUAGE: prometheus\nCODE:\n```\nprometheus_rule_evaluation_duration_seconds{quantile=\"0.5\"} 0.000214623\nprometheus_rule_evaluation_duration_seconds{quantile=\"0.9\"} 0.001456135\nprometheus_rule_evaluation_duration_seconds{quantile=\"0.99\"} 0.008111814\nprometheus_rule_evaluation_duration_seconds_sum 5209.704794862625\nprometheus_rule_evaluation_duration_seconds_count 7.203456e+06\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 0\nprometheus_rule_evaluation_failures_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 0\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/ansible_managed.rules;ansible managed alert rules\"} 118092\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/ansible_managed.yml;ansible managed alert rules\"} 118090\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/node_alerts.rules;node-exporter\"} 1.4761e+06\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/node_alerts.yaml;node-exporter\"} 1.476125e+06\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/node_rules.rules;node-exporter.rules\"} 649495\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/node_rules.yaml;node-exporter.rules\"} 649484\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.rules;prometheus\"} 1.358035e+06\nprometheus_rule_evaluations_total{rule_group=\"/etc/prometheus/rules/prometheus_alerts.yaml;prometheus\"} 1.358035e+06\n```\n\n----------------------------------------\n\nTITLE: Calculating BlockSeriesRef in Prometheus TSDB\nDESCRIPTION: This snippet explains how BlockSeriesRef is calculated in Prometheus TSDB. It's derived from the byte offset in the index file, divided by 16 due to alignment. This limits the index size to 64 GB.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/refs.md#2025-04-16_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nBlockSeriesRef are only 32 bits for now, because 64 bits would slow down the postings lists disk access. (note: this limits the index size to 2^32 * 16 = 64 GB)\n```\n\n----------------------------------------\n\nTITLE: Enable Created Timestamps Flag\nDESCRIPTION: Command flag to enable created timestamp ingestion\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=created-timestamp-zero-ingestion\n```\n\n----------------------------------------\n\nTITLE: Illustrating Label Index Example in ASCII Art\nDESCRIPTION: This ASCII art diagram shows an example of a Label Index section for a single label name with 4 different values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/index.md#2025-04-16_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n```\n\n 24  1  4  ref(value_0) | ref(value_1) | ref(value_2) | ref(value_3) | CRC32 |\n\n```\n```\n\n----------------------------------------\n\nTITLE: Series Records Structure in Prometheus WAL\nDESCRIPTION: Diagram showing the encoding format for Series records which contain a series ID and a list of label key-value pairs. The record type is 1 and includes the series ID followed by the number of labels and their string values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/wal.md#2025-04-16_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n\n type = 1 <1b>                              \n\n  \n  id <8b>  n = len(labels) <uvarint>     \n  \n  len(str_1) <uvarint>  str_1 <bytes>    \n  \n   ...                                    \n  \n  len(str_2n) <uvarint>  str_2n <bytes>  \n  \n                  . . .                     \n\n```\n\n----------------------------------------\n\nTITLE: Histogram Subsequent Samples Format Diagram\nDESCRIPTION: Details the format of the third and subsequent histogram samples, using double-delta encoding for further compression.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n\n ts_dod <varbit_int>  count_dod <varbit_int>  zero_count_dod <varbit_int>  sum_xor <varbit_xor>  pos_bucket_0_dod <varbit_int>  ...  pos_bucket_n_dod <varbit_int>  neg_bucket_0_dod <varbit_int>  ...  neg_bucket_n_dod <varbit_int> \n\n```\n\n----------------------------------------\n\nTITLE: Enable Memory Snapshot Flag\nDESCRIPTION: Command flag to enable taking memory snapshots during shutdown for faster startup\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/feature_flags.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n--enable-feature=memory-snapshot-on-shutdown\n```\n\n----------------------------------------\n\nTITLE: Tombstone Records Structure in Prometheus WAL\nDESCRIPTION: Diagram showing the encoding format for Tombstone records which specify intervals for which samples of a series were deleted. Each tombstone includes a series ID, minimum time, and maximum time to define the deleted interval.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/wal.md#2025-04-16_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n\n type = 3 <1b>                                       \n\n  \n  id <8b>  min_time <varint>  max_time <varint>  \n  \n                        . . .                        \n\n```\n\n----------------------------------------\n\nTITLE: Sample 0 Data Structure\nDESCRIPTION: ASCII diagram showing the layout of the initial sample data including timestamp, counts, and bucket values\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_11\n\nLANGUAGE: ascii-diagram\nCODE:\n```\n\n ts <varbit_int>  count <float64>  zero_count <float64>  sum <float64>  pos_bucket_0 <float64>  ...  pos_bucket_n <float64>  neg_bucket_0 <float64>  ...  neg_bucket_n <float64> \n\n```\n\n----------------------------------------\n\nTITLE: Tombstone Record Format in Prometheus\nDESCRIPTION: Diagram showing the structure of an individual tombstone record. Each record contains a series reference stored as an unsigned variable-length integer (uvarint64), followed by minimum and maximum timestamps (mint and maxt) stored as signed variable-length integers (varint64).\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/tombstones.md#2025-04-16_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n\nseries ref <uvarint64>  mint <varint64>  maxt <varint64>\n\n```\n\n----------------------------------------\n\nTITLE: Structure of BlockChunkRef in Prometheus TSDB\nDESCRIPTION: This snippet explains the structure of BlockChunkRef in Prometheus TSDB. It's an 8-byte integer packing a 4-byte chunk file index and a 4-byte byte offset within the file.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/refs.md#2025-04-16_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\nA `BlockChunkRef` is an 8 byte integer.  Unlike `HeadChunkRef`, it is static and independent of factors such as Prometheus restarting.\n\nIt packs together:\n\n* 4 Bytes for chunk file index in the block. This number just increments. Filenames [start at 1](https://ganeshvernekar.com/blog/prometheus-tsdb-persistent-block-and-its-index/#contents-of-a-block)\nbut the `BlockChunkRef` start at 0.\n* 4 Bytes for the byte offset within the file.\n```\n\n----------------------------------------\n\nTITLE: Configuring Tag Separator in Prometheus\nDESCRIPTION: Configuration option for specifying a custom tag separator used when concatenating tags in Prometheus. The default separator is a comma.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/configuration/configuration.md#2025-04-16_snippet_27\n\nLANGUAGE: yaml\nCODE:\n```\n# The tag separator is used to separate the tags on concatenation\n[ tag_separator: <string> | default = , ]\n```\n\n----------------------------------------\n\nTITLE: Installing lezer-promql NPM Package\nDESCRIPTION: Command to install the @prometheus-io/lezer-promql package using npm. It also mentions the need to manually install lezer dependencies.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save @prometheus-io/lezer-promql\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save @lezer/lr @lezer/highlight\n```\n\n----------------------------------------\n\nTITLE: Float Histogram Sample Data Structure\nDESCRIPTION: ASCII diagram showing the basic layout of histogram samples with multiple sample entries\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_10\n\nLANGUAGE: ascii-diagram\nCODE:\n```\n\n    sample_0 <data>       \n\n    sample_1 <data>       \n\n    sample_2 <data>       \n\n          ...             \n\n    sample_n <data>       \n\n```\n\n----------------------------------------\n\nTITLE: Building and Testing lezer-promql Project\nDESCRIPTION: Commands for building and testing the lezer-promql project. It includes installing dependencies, running the build process, and executing tests.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm i\nnpm run build\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm run test\n```\n\n----------------------------------------\n\nTITLE: Prometheus Main Changelog Entry\nDESCRIPTION: Complete changelog entries for Prometheus showing version history and changes. Includes unreleased changes, version 3.2.1, 3.2.0, 3.1.0, and 3.0.1 with their respective changes, features, enhancements, performance improvements and bug fixes.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Changelog\n\n## unreleased\n\n* [CHANGE] Make setting out-of-order native histograms feature (`--enable-feature=ooo-native-histograms`) a no-op. Out-of-order native histograms are now always enabled when `out_of_order_time_window` is greater than zero and `--enable-feature=native-histograms` is set. #16207\n* [FEATURE] OTLP translate: Add feature flag for optionally translating OTel explicit bucket histograms into native histograms with custom buckets. #15850\n* [ENHANCEMENT] TSDB: add `prometheus_tsdb_wal_replay_unknown_refs_total` and `prometheus_tsdb_wbl_replay_unknown_refs_total` metrics to track unknown series references during WAL/WBL replay. #16166\n* [BUGFIX] TSDB: fix unknown series errors and possible lost data during WAL replay when series are removed from the head due to inactivity and reappear before the next WAL checkpoint. #16060\n\n## 3.2.1 / 2025-02-25\n\n* [BUGFIX] Don't send Accept` header `escape=allow-utf-8` when `metric_name_validation_scheme: legacy` is configured. #16061\n\n## 3.2.0 / 2025-02-17\n\n* [CHANGE] relabel: Replace actions can now use UTF-8 characters in `targetLabel` field. Note that `$<chars>` or `${<chars>}` will be expanded. This also apply to `replacement` field for `LabelMap` action. #15851\n* [CHANGE] rulefmt: Rule names can use UTF-8 characters, except `{` and `}` characters (due to common mistake checks). #15851\n* [FEATURE] remote/otlp: Add feature flag `otlp-deltatocumulative` to support conversion from delta to cumulative. #15165\n* [ENHANCEMENT] openstack SD: Discover Octavia loadbalancers. #15539\n* [ENHANCEMENT] scrape: Add metadata for automatic metrics to WAL for `metadata-wal-records` feature. #15837\n* [ENHANCEMENT] promtool: Support linting of scrape interval, through lint option `too-long-scrape-interval`. #15719\n* [ENHANCEMENT] promtool: Add --ignore-unknown-fields option. #15706\n* [ENHANCEMENT] ui: Make \"hide empty rules\" and hide empty rules\" persistent #15807\n* [ENHANCEMENT] web/api: Add a limit parameter to `/query` and `/query_range`. #15552\n* [ENHANCEMENT] api: Add fields Node and ServerTime to `/status`. #15784\n* [PERF] Scraping: defer computing labels for dropped targets until they are needed by the UI.  #15261\n* [BUGFIX] remotewrite2: Fix invalid metadata bug for metrics without metadata. #15829\n* [BUGFIX] remotewrite2: Fix the unit field propagation. #15825\n* [BUGFIX] scrape: Fix WAL metadata for histograms and summaries. #15832\n* [BUGFIX] ui: Merge duplicate \"Alerts page settings\" sections. #15810\n* [BUGFIX] PromQL: Fix `<aggr_over_time>` functions with histograms. #15711\n\n## 3.1.0 / 2025-01-02\n\n * [SECURITY] upgrade golang.org/x/crypto to address reported CVE-2024-45337. #15691\n * [CHANGE] Notifier: Increment prometheus_notifications_errors_total by the number of affected alerts rather than per batch. #15428\n * [CHANGE] API: list rules field \"groupNextToken:omitempty\" renamed to \"groupNextToken\". #15400\n * [ENHANCEMENT] OTLP translate: keep identifying attributes in target_info. #15448\n * [ENHANCEMENT] Paginate rule groups, add infinite scroll to rules within groups. #15677\n * [ENHANCEMENT] TSDB: Improve calculation of space used by labels. #13880\n * [ENHANCEMENT] Rules: new metric rule_group_last_rule_duration_sum_seconds. #15672\n * [ENHANCEMENT] Observability: Export 'go_sync_mutex_wait_total_seconds_total' metric. #15339\n * [ENHANCEMEN] Remote-Write: optionally use a DNS resolver that picks a random IP. #15329\n * [PERF] Optimize `l=~\".+\"` matcher. #15474, #15684\n * [PERF] TSDB: Cache all symbols for compaction . #15455\n * [PERF] TSDB: MemPostings: keep a map of label values slices. #15426\n * [PERF] Remote-Write: Remove interning hook. #15456\n * [PERF] Scrape: optimize string manipulation for experimental native histograms with custom buckets. #15453\n * [PERF] TSDB: reduce memory allocations. #15465, #15427\n * [PERF] Storage: Implement limit in mergeGenericQuerier. #14489\n * [PERF] TSDB: Optimize inverse matching. #14144\n * [PERF] Regex: use stack memory for lowercase copy of string. #15210\n * [PERF] TSDB: When deleting from postings index, pause to unlock and let readers read. #15242\n * [BUGFIX] Main: Avoid possible segfault at exit. (#15724)\n * [BUGFIX] Rules: Do not run rules concurrently if uncertain about dependencies. #15560\n * [BUGFIX] PromQL: Adds test for `absent`, `absent_over_time` and `deriv` func with histograms. #15667\n * [BUGFIX] PromQL: Fix various bugs related to quoting UTF-8 characters. #15531\n * [BUGFIX] Scrape: fix nil panic after scrape loop reload. #15563\n * [BUGFIX] Remote-write: fix panic on repeated log message. #15562\n * [BUGFIX] Scrape: reload would ignore always_scrape_classic_histograms and convert_classic_histograms_to_nhcb configs. #15489\n * [BUGFIX] TSDB: fix data corruption in experimental native histograms. #15482\n * [BUGFIX] PromQL: Ignore histograms in all time related functions. #15479\n * [BUGFIX] OTLP receiver: Convert metric metadata. #15416\n * [BUGFIX] PromQL: Fix `resets` function for histograms. #15527\n * [BUGFIX] PromQL: Fix behaviour of `changes()` for mix of histograms and floats. #15469\n * [BUGFIX] PromQL: Fix behaviour of some aggregations with histograms. #15432\n * [BUGFIX] allow quoted exemplar keys in openmetrics text format. #15260\n * [BUGFIX] TSDB: fixes for rare conditions when loading write-behind-log (WBL). #15380\n * [BUGFIX] `round()` function did not remove `__name__` label. #15250\n * [BUGFIX] Promtool: analyze block shows metric name with 0 cardinality. #15438\n * [BUGFIX] PromQL: Fix `count_values` for histograms. #15422\n * [BUGFIX] PromQL: fix issues with comparison binary operations with `bool` modifier and native histograms. #15413\n * [BUGFIX] PromQL: fix incorrect \"native histogram ignored in aggregation\" annotations. #15414\n * [BUGFIX] PromQL: Corrects the behaviour of some operator and aggregators with Native Histograms. #15245\n * [BUGFIX] TSDB: Always return unknown hint for first sample in non-gauge histogram chunk. #15343\n * [BUGFIX] PromQL: Clamp functions: Ignore any points with native histograms. #15169\n * [BUGFIX] TSDB: Fix race on stale values in headAppender. #15322\n * [BUGFIX] UI: Fix selector / series formatting for empty metric names. #15340\n * [BUGFIX] OTLP receiver: Allow colons in non-standard units. #15710\n\n## 3.0.1 / 2024-11-28\n\nThe first bug fix release for Prometheus 3.\n\n* [BUGFIX] Promql: Make subqueries left open. #15431\n* [BUGFIX] Fix memory leak when query log is enabled. #15434\n* [BUGFIX] Support utf8 names on /v1/label/:name/values endpoint. #15399\n```\n\n----------------------------------------\n\nTITLE: Running Tests Once for CI Environments\nDESCRIPTION: Command to run tests only once and then exit, using the CI environment variable. Useful for continuous integration workflows.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nCI=true npm test\n```\n\n----------------------------------------\n\nTITLE: Sample 2+ Data Structure\nDESCRIPTION: ASCII diagram showing the layout of subsequent samples with delta-of-delta timestamp encoding\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/chunks.md#2025-04-16_snippet_13\n\nLANGUAGE: ascii-diagram\nCODE:\n```\n\n ts_dod <varbit_int>  count_xor <varbit_xor>  zero_count_xor <varbit_xor>  sum_xor <varbit_xor>  pos_bucket_0_xor <varbit_xor>  ...  pos_bucket_n_xor <varbit_xor>  neg_bucket_0_xor <varbit_xor>  ...  neg_bucket_n_xor <varbit_xor> \n\n```\n\n----------------------------------------\n\nTITLE: Using Prebuilt UI Assets in Prometheus Build\nDESCRIPTION: Commands to extract prebuilt UI assets and build Prometheus using them, avoiding the need to install npm or rebuild the frontend from source.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/README.md#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntar -xvf prometheus-web-ui-<version>.tar.gz -C web/ui\n```\n\n----------------------------------------\n\nTITLE: Prometheus Release Changelog Entry 2.50.1\nDESCRIPTION: Bugfix release notes for version 2.50.1 addressing an API metadata field naming issue.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n## 2.50.1 / 2024-02-26\n\n* [BUGFIX] API: Fix metadata API using wrong field names. #13633\n```\n\n----------------------------------------\n\nTITLE: Postings Section Binary Layout\nDESCRIPTION: Defines the binary structure for storing monotonically increasing lists of series references containing label pairs. Includes length, number of entries, series references, and CRC32 checksum.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/index.md#2025-04-16_snippet_6\n\nLANGUAGE: ascii-art\nCODE:\n```\n\n len <4b>            #entries <4b>      \n\n  \n  ref(series_1) <4b>                   \n  \n  ...                                  \n  \n  ref(series_n) <4b>                   \n  \n\n CRC32 <4b>                              \n\n```\n\n----------------------------------------\n\nTITLE: Integer Native Histogram Record Format (Custom Bucketing - NHCB)\nDESCRIPTION: Record format for type 9 integer native histograms using custom bucketing (NHCB). Backwards compatible with type 7 and includes custom value fields.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/wal.md#2025-04-16_snippet_9\n\nLANGUAGE: ascii-art\nCODE:\n```\n\n type = 9 <1b>                                                         \n\n                     \n  id <8b>             timestamp <8b>                                \n                     \n  \n  id_delta <uvarint>  timestamp_delta <uvarint>                     \n  \n  counter_reset_hint <1b>  schema <varint>                          \n  \n  zero_threshold (float) <8b>     zero_count <uvarint>              \n  \n  count <uvarint>  sum (float) <8b>                                 \n  \n  positive_spans_num <uvarint>                                       \n  \n  positive_span_offset_1 <varint>  positive_span_len_1 <uvarint32>  \n  \n  . . .                                                              \n  \n  negative_spans_num <uvarint> = 0                                   \n  \n  positive_bkts_num <uvarint>                                        \n  \n  positive_bkt_1 <varint>  . . .  positive_bkt_n <varint>          \n  \n  negative_bkts_num <uvarint> = 0                                    \n  \n  custom_values_num <uvarint>                                        \n  \n  custom_value_1 (float) <8b>  . . .  custom_value_n (float) <8b>  \n  \n                              . . .                                    \n\n```\n\n----------------------------------------\n\nTITLE: Filtering Active Targets\nDESCRIPTION: Example of filtering targets using the state parameter to show only active targets. Demonstrates the response structure when droppedTargets is empty.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"activeTargets\": [\n      {\n        \"discoveredLabels\": {\n          \"__address__\": \"127.0.0.1:9090\",\n          \"__metrics_path__\": \"/metrics\",\n          \"__scheme__\": \"http\",\n          \"job\": \"prometheus\"\n        },\n        \"labels\": {\n          \"instance\": \"127.0.0.1:9090\",\n          \"job\": \"prometheus\"\n        },\n        \"scrapePool\": \"prometheus\",\n        \"scrapeUrl\": \"http://127.0.0.1:9090/metrics\",\n        \"globalUrl\": \"http://example-prometheus:9090/metrics\",\n        \"lastError\": \"\",\n        \"lastScrape\": \"2017-01-17T15:07:44.723715405+01:00\",\n        \"lastScrapeDuration\": 50688943,\n        \"health\": \"up\"\n      }\n    ],\n    \"droppedTargets\": []\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Individual Chunk Format ASCII Diagram\nDESCRIPTION: Detailed structure of individual chunks showing series reference, timestamps, encoding, length, data, and CRC32 checksum fields with their respective sizes.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/head_chunks.md#2025-04-16_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n\n| series ref <8 byte> | mint <8 byte, uint64> | maxt <8 byte, uint64> | encoding <1 byte> | len <uvarint> | data <bytes>  CRC32 <4 byte> \n\n```\n\n----------------------------------------\n\nTITLE: Illustrating Type Byte Structure in Prometheus WAL Records\nDESCRIPTION: Diagram showing the bit-level composition of the type byte in WAL records, including reserved bits, compression flags for zstd and snappy, and the type flag indicating the record fragment type.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/wal.md#2025-04-16_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n\n reserved <3bit>  zstd_flag <1bit>  snappy_flag <1bit>  type_flag <3bit> \n\n```\n\n----------------------------------------\n\nTITLE: Metadata Records Structure in Prometheus WAL\nDESCRIPTION: Diagram showing the encoding format for Metadata records which contain metadata updates associated with a series. Each record includes a series ID, metric type, and a list of metadata field name-value pairs.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/wal.md#2025-04-16_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n\n type = 6 <1b>                              \n\n  \n  series_id <uvarint>                     \n  \n  metric_type <1b>                        \n  \n  num_fields <uvarint>                    \n  \n  len(name_1) <uvarint>  name_1 <bytes>  \n  \n  len(val_1) <uvarint>   val_1 <bytes>   \n  \n                 . . .                    \n  \n  len(name_n) <uvarint>  name_n <bytes>  \n  \n  len(val_n) <uvarint>   val_n <bytes>   \n  \n                  . . .                     \n\n```\n\n----------------------------------------\n\nTITLE: Configuring PromQL Parser Debug Settings\nDESCRIPTION: Code snippet showing how to enable verbose debugging output in the generated PromQL parser\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CONTRIBUTING.md#2025-04-16_snippet_3\n\nLANGUAGE: golang\nCODE:\n```\n// As of writing this was somewhere around line 600.\nvar (\n\tyyDebug        = 0 // This can be a number 0 -> 5.\n\tyyErrorVerbose = false  // This can be set to true.\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Go Code from Lexer Files with Golex\nDESCRIPTION: Command to generate Go code from the lexer definition file. This should be run after making changes to promlex.l or openmetricslex.l files to update the corresponding Go implementation.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/model/textparse/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngolex -o=promlex.l.go promlex.l\n```\n\n----------------------------------------\n\nTITLE: Structure of ChunkDiskMapperRef in Prometheus TSDB\nDESCRIPTION: This snippet details the structure of ChunkDiskMapperRef in Prometheus TSDB. It's an 8-byte integer with 4 bytes for the chunks file number and 4 bytes for the byte offset, similar to BlockChunkRef.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/tsdb/docs/refs.md#2025-04-16_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n[`ChunkDiskMapperRef`](https://pkg.go.dev/github.com/prometheus/prometheus/tsdb/chunks#ChunkDiskMapperRef) is an 8 Byte integer.\n  4 Bytes are used to refer to a chunks file number and 4 bytes serve as byte offset (similar to `BlockChunkRef`).  `mmappedChunk` provide this value such that callers can load the mmapped chunk from disk.\n```\n\n----------------------------------------\n\nTITLE: Cleaning Tombstones in Prometheus TSDB\nDESCRIPTION: This endpoint removes deleted data from disk and cleans up existing tombstones to free up space. It can be used after deleting series. Requires the admin API to be enabled.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_36\n\nLANGUAGE: json\nCODE:\n```\n$ curl -XPOST http://localhost:9090/api/v1/admin/tsdb/clean_tombstones\n```\n\n----------------------------------------\n\nTITLE: Prometheus Release Changelog Entry 2.51.1\nDESCRIPTION: Bugfix release notes for version 2.51.1 covering fixes for PromQL label validation and native histogram handling.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n## 2.51.1 / 2024-03-27\n\nBugfix release.\n\n* [BUGFIX] PromQL: Re-instate validation of label_join destination label #13803\n* [BUGFIX] Scraping (experimental native histograms): Fix handling of the min bucket factor on sync of targets #13846\n* [BUGFIX] PromQL: Some queries could return the same series twice (library use only) #13845\n```\n\n----------------------------------------\n\nTITLE: Getting Label Names - API Response Example\nDESCRIPTION: Example response from the /api/v1/labels endpoint showing all available label names in the Prometheus server.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"status\": \"success\",\n    \"data\": [\n        \"__name__\",\n        \"call\",\n        \"code\",\n        \"config\",\n        \"dialer_name\",\n        \"endpoint\",\n        \"event\",\n        \"goversion\",\n        \"handler\",\n        \"instance\",\n        \"interval\",\n        \"job\",\n        \"le\",\n        \"listener_name\",\n        \"name\",\n        \"quantile\",\n        \"reason\",\n        \"role\",\n        \"scrape_job\",\n        \"slice\",\n        \"version\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Request for Limited Metadata Entries Per Metric\nDESCRIPTION: Example curl request that retrieves only one metadata entry for each metric using the limit_per_metric parameter.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ncurl -G http://localhost:9090/api/v1/metadata?limit_per_metric=1\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"cortex_ring_tokens\": [\n      {\n        \"type\": \"gauge\",\n        \"help\": \"Number of tokens in the ring\",\n        \"unit\": \"\"\n      }\n    ],\n    \"http_requests_total\": [\n      {\n        \"type\": \"counter\",\n        \"help\": \"Number of HTTP requests\",\n        \"unit\": \"\"\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying TSDB Statistics with GET API Endpoint\nDESCRIPTION: API endpoint for retrieving cardinality statistics about the Prometheus Time Series Database (TSDB), available since v2.15.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\nGET /api/v1/status/tsdb\n```\n\n----------------------------------------\n\nTITLE: Enabling Native Histograms in Prometheus\nDESCRIPTION: Command line flag to enable experimental native histogram support in Prometheus. This switches the preferred exposition format to protobuf.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\n--enable-feature=native-histograms\n```\n\n----------------------------------------\n\nTITLE: Example Response for Build Information\nDESCRIPTION: Sample response showing Prometheus version, revision, branch, build user and date, and Go version information.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\n$ curl http://localhost:9090/api/v1/status/buildinfo\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"version\": \"2.13.1\",\n    \"revision\": \"cb7cbad5f9a2823a622aaa668833ca04f50a0ea7\",\n    \"branch\": \"master\",\n    \"buildUser\": \"julius@desktop\",\n    \"buildDate\": \"20191102-16:19:59\",\n    \"goVersion\": \"go1.13.1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Response for Flag Values\nDESCRIPTION: Sample response showing various Prometheus configuration flag values that were set during Prometheus startup.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/docs/querying/api.md#2025-04-16_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\n$ curl http://localhost:9090/api/v1/status/flags\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"alertmanager.notification-queue-capacity\": \"10000\",\n    \"alertmanager.timeout\": \"10s\",\n    \"log.level\": \"info\",\n    \"query.lookback-delta\": \"5m\",\n    \"query.max-concurrency\": \"20\",\n    ...\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: PromQL Basic Arithmetic Operations\nDESCRIPTION: Simple arithmetic operation example showing addition of numeric values.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/web/ui/module/lezer-promql/test/expression.txt#2025-04-16_snippet_1\n\nLANGUAGE: PromQL\nCODE:\n```\n1 + 2\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS Version in Prometheus\nDESCRIPTION: Example of setting the maximum TLS version in Prometheus TLS configuration.\nSOURCE: https://github.com/prometheus/prometheus/blob/main/CHANGELOG.md#2025-04-16_snippet_11\n\nLANGUAGE: YAML\nCODE:\n```\ntls_config:\n  max_version: TLS13\n```"
  }
]
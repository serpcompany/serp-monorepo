[
  {
    "owner": "databricks",
    "repo": "databricks-sdk-py",
    "content": "TITLE: Installing Databricks SDK\nDESCRIPTION: This command installs the Databricks SDK for Python using pip, the Python package installer. It downloads and installs the latest version of the SDK and its dependencies.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/getting-started.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install databricks-sdk\n```\n\n----------------------------------------\n\nTITLE: Initializing WorkspaceClient and Listing Clusters\nDESCRIPTION: This snippet demonstrates how to instantiate the `WorkspaceClient` and list all clusters in the workspace. It requires the `databricks-sdk` package to be installed and properly configured authentication.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient()\nfor c in w.clusters.list():\n    print(c.cluster_name)\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing SQL Warehouses with Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to create a SQL warehouse using the Databricks SDK for Python, retrieve its information, and then delete it. It uses the `WorkspaceClient` to interact with the Databricks API, setting various parameters such as cluster size, maximum number of clusters, and auto-stop time. The snippet also includes setting custom tags for the warehouse.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/warehouses.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\ncreated = w.warehouses.create(\n    name=f\"sdk-{time.time_ns()}\",\n    cluster_size=\"2X-Small\",\n    max_num_clusters=1,\n    auto_stop_mins=10,\n    tags=sql.EndpointTags(\n        custom_tags=[sql.EndpointTagPair(key=\"Owner\", value=\"eng-dev-ecosystem-team_at_databricks.com\")]\n    ),\n).result()\n\nwh = w.warehouses.get(id=created.id)\n\n# cleanup\nw.warehouses.delete(id=created.id)\n```\n\n----------------------------------------\n\nTITLE: Creating a Catalog in Databricks with Python SDK\nDESCRIPTION: This code snippet demonstrates how to create a catalog in Databricks using the Python SDK. It imports necessary libraries, initializes the WorkspaceClient, creates a catalog with a unique name, and then cleans up by deleting the created catalog. The name includes a timestamp to ensure uniqueness.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/catalogs.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\n# cleanup\nw.catalogs.delete(name=created_catalog.name, force=True)\n```\n\n----------------------------------------\n\nTITLE: Add and Delete Secrets in Databricks\nDESCRIPTION: This snippet demonstrates how to create a secret scope, add a secret to it, and then clean up by deleting both the secret and the scope using the Databricks SDK. It requires the `databricks-sdk` library.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/secrets.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nkey_name = f\"sdk-{time.time_ns()}\"\n\nscope_name = f\"sdk-{time.time_ns()}\"\n\nw.secrets.create_scope(scope=scope_name)\n\nw.secrets.put_secret(scope=scope_name, key=key_name, string_value=f\"sdk-{time.time_ns()}\")\n\n# cleanup\nw.secrets.delete_secret(scope=scope_name, key=key_name)\nw.secrets.delete_scope(scope=scope_name)\n```\n\n----------------------------------------\n\nTITLE: Initializing WorkspaceClient with Azure Client Secret Authentication - Python\nDESCRIPTION: This snippet shows how to initialize the Databricks WorkspaceClient using Azure client secret authentication. It requires the Databricks Workspace URL, Azure Resource ID, AAD Tenant ID, AAD Client ID, and AAD Client Secret as input.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/authentication.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(host=input('Databricks Workspace URL: '),\n                    azure_workspace_resource_id=input('Azure Resource ID: '),\n                    azure_tenant_id=input('AAD Tenant ID: '),\n                    azure_client_id=input('AAD Client ID: '),\n                    azure_client_secret=input('AAD Client Secret: '))\n```\n\n----------------------------------------\n\nTITLE: Submitting and Waiting for a Job in Databricks SDK\nDESCRIPTION: This example demonstrates how to submit a Databricks job and wait for its completion using the `jobs.submit` method. It creates a dummy Python file on DBFS, submits a job to run it, and then waits for the job to reach either the 'TERMINATED' or 'SKIPPED' state. A callback function is used to print the intermediate status of the job.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\nimport logging\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nimport databricks.sdk.service.jobs as j\n\nw = WorkspaceClient()\n\n# create a dummy file on DBFS that just sleeps for 10 seconds\npy_on_dbfs = f'/home/{w.current_user.me().user_name}/sample.py'\nwith w.dbfs.open(py_on_dbfs, write=True, overwrite=True) as f:\n    f.write(b'import time; time.sleep(10); print(\"Hello, World!\")')\n\n# trigger one-time-run job and get waiter object\nwaiter = w.jobs.submit(run_name=f'py-sdk-run-{time.time()}', tasks=[\n    j.RunSubmitTaskSettings(\n        task_key='hello_world',\n        new_cluster=j.BaseClusterInfo(\n            spark_version=w.clusters.select_spark_version(long_term_support=True),\n            node_type_id=w.clusters.select_node_type(local_disk=True),\n            num_workers=1\n        ),\n        spark_python_task=j.SparkPythonTask(\n            python_file=f'dbfs:{py_on_dbfs}'\n        ),\n    )\n])\n\nlogging.info(f'starting to poll: {waiter.run_id}')\n\n# callback, that receives a polled entity between state updates\ndef print_status(run: j.Run):\n    statuses = [f'{t.task_key}: {t.state.life_cycle_state}' for t in run.tasks]\n    logging.info(f'workflow intermediate status: {\", \".join(statuses)}')\n\n# If you want to perform polling in a separate thread, process, or service,\n# you can use w.jobs.wait_get_run_job_terminated_or_skipped(\n#   run_id=waiter.run_id,\n#   timeout=datetime.timedelta(minutes=15),\n#   callback=print_status) to achieve the same results.\n#\n# Waiter interface allows for `w.jobs.submit(..).result()` simplicity in\n# the scenarios, where you need to block the calling thread for the job to finish.\nrun = waiter.result(timeout=datetime.timedelta(minutes=15),\n                    callback=print_status)\n\nlogging.info(f'job finished: {run.run_page_url}')\n```\n\n----------------------------------------\n\nTITLE: Get SQL Query by ID with Databricks SDK (Python)\nDESCRIPTION: This snippet shows how to retrieve a SQL query by its ID using the Databricks SDK for Python. It first creates a query, then retrieves it using the `get` method, and finally deletes it.  It depends on the `WorkspaceClient` and the `sql` service.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/queries.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\nsrcs = w.data_sources.list()\n\nquery = w.queries.create(\n    query=sql.CreateQueryRequestQuery(\n        display_name=f\"sdk-{time.time_ns()}\",\n        warehouse_id=srcs[0].warehouse_id,\n        description=\"test query from Go SDK\",\n        query_text=\"SHOW TABLES\",\n    )\n)\n\nby_id = w.queries.get(id=query.id)\n\n# cleanup\nw.queries.delete(id=query.id)\n```\n\n----------------------------------------\n\nTITLE: Restarting a Cluster in Databricks with Python\nDESCRIPTION: This snippet illustrates how to restart a Databricks cluster using the Databricks SDK for Python. It first creates a cluster, restarts the created cluster, and then deletes it. The environment variable TEST_INSTANCE_POOL_ID must be set.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nlatest = w.clusters.select_spark_version(latest=True, long_term_support=True)\n\ncluster_name = f\"sdk-{time.time_ns()}\"\n\nclstr = w.clusters.create(\n    cluster_name=cluster_name,\n    spark_version=latest,\n    instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n    autotermination_minutes=15,\n    num_workers=1,\n).result()\n\n_ = w.clusters.restart(cluster_id=clstr.cluster_id).result()\n\n# cleanup\nw.clusters.permanent_delete(cluster_id=clstr.cluster_id)\n```\n\n----------------------------------------\n\nTITLE: Creating a SQL Warehouse with Databricks SDK in Python\nDESCRIPTION: This code snippet demonstrates how to create a SQL warehouse using the Databricks SDK for Python. It imports the necessary modules, initializes the WorkspaceClient, and uses the warehouses.create method with specified parameters such as name, cluster size, and auto-stop minutes. Finally, it demonstrates how to delete the warehouse after creation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/warehouses.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\ncreated = w.warehouses.create(\n    name=f\"sdk-{time.time_ns()}\",\n    cluster_size=\"2X-Small\",\n    max_num_clusters=1,\n    auto_stop_mins=10,\n    tags=sql.EndpointTags(\n        custom_tags=[sql.EndpointTagPair(key=\"Owner\", value=\"eng-dev-ecosystem-team_at_databricks.com\")]\n    ),\n).result()\n\n# cleanup\nw.warehouses.delete(id=created.id)\n```\n\n----------------------------------------\n\nTITLE: Creating and Deleting a Pipeline in Databricks\nDESCRIPTION: This snippet demonstrates how to create a Databricks pipeline using the Databricks SDK for Python, retrieve pipeline events, and then delete the created pipeline. It uses the `WorkspaceClient` to interact with the Pipelines API. It relies on an environment variable TEST_INSTANCE_POOL_ID to be present.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/pipelines/pipelines.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import pipelines\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncreated = w.pipelines.create(\n    continuous=False,\n    name=f\"sdk-{time.time_ns()}\",\n    libraries=[pipelines.PipelineLibrary(notebook=pipelines.NotebookLibrary(path=notebook_path))],\n    clusters=[\n        pipelines.PipelineCluster(\n            instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n            label=\"default\",\n            num_workers=1,\n            custom_tags={\n                \"cluster_type\": \"default\",\n            },\n        )\n    ],\n)\n\nevents = w.pipelines.list_pipeline_events(pipeline_id=created.pipeline_id)\n\n# cleanup\nw.pipelines.delete(pipeline_id=created.pipeline_id)\n```\n\n----------------------------------------\n\nTITLE: Creating a Cluster and Waiting for it to be Running - Python\nDESCRIPTION: This snippet creates a Databricks cluster and waits for it to reach the `RUNNING` state. It uses the `WorkspaceClient` to create the cluster and the `result()` method with a timeout to wait for the cluster to be ready. The `timeout` parameter specifies the maximum time to wait for the cluster to reach the `RUNNING` state. It imports the necessary modules and sets up basic cluster configuration.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/wait.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\nimport logging\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient()\ninfo = w.clusters.create(cluster_name='Created cluster',\n                         spark_version='12.0.x-scala2.12',\n                         node_type_id='m5d.large',\n                         autotermination_minutes=10,\n                         num_workers=1).result(timeout=datetime.timedelta(minutes=10))\nlogging.info(f'Created: {info}')\n```\n\n----------------------------------------\n\nTITLE: Listing Clusters with Pagination\nDESCRIPTION: This Python code snippet shows how to list all clusters in the workspace using the `w.clusters.list()` method, which returns a generator for paginated responses.  It iterates through the cluster list and prints each cluster's name and number of workers.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/getting-started.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor cluster in w.clusters.list():\n    print(f'cluster {cluster.cluster_name} has {cluster.num_workers} workers')\n```\n\n----------------------------------------\n\nTITLE: Listing Databricks Clusters using Azure CLI Authentication (Python)\nDESCRIPTION: This code snippet demonstrates how to authenticate with Azure Databricks using credentials provided by the Azure CLI (`az login`). It initializes a `WorkspaceClient` with the Databricks host and retrieves a list of clusters, printing their names and states.\nDependencies: databricks-sdk\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(host='https://adb-30....azuredatabricks.net')\nclusters = w.clusters.list()\nfor cl in clusters:\n    print(f' - {cl.cluster_name} is {cl.state}')\n```\n\n----------------------------------------\n\nTITLE: Databricks Token Authentication in Python\nDESCRIPTION: This snippet demonstrates how to authenticate with Databricks using a token. It requires the `databricks.sdk` package and prompts the user for the Databricks Workspace URL and token.  The `WorkspaceClient` is initialized with the provided credentials.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(host=input('Databricks Workspace URL: '), token=input('Token: '))\n```\n\n----------------------------------------\n\nTITLE: Creating an App in Databricks\nDESCRIPTION: Creates a new app in the Databricks workspace.  The app name must be unique and contain only lowercase alphanumeric characters and hyphens. Returns a long-running operation waiter for the App object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nw.apps.create(name: str [, description: Optional[str]]) -> Wait[App]\n```\n\n----------------------------------------\n\nTITLE: Change Cluster Owner using Databricks SDK\nDESCRIPTION: This snippet demonstrates how to change the owner of a Databricks cluster using the Databricks SDK for Python. It includes creating a user, a cluster, changing the owner, and cleaning up resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nlatest = w.clusters.select_spark_version(latest=True, long_term_support=True)\n\ncluster_name = f\"sdk-{time.time_ns()}\"\n\nother_owner = w.users.create(user_name=f\"sdk-{time.time_ns()}@example.com\")\n\nclstr = w.clusters.create(\n    cluster_name=cluster_name,\n    spark_version=latest,\n    instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n    autotermination_minutes=15,\n    num_workers=1,\n).result()\n\nw.clusters.change_owner(cluster_id=clstr.cluster_id, owner_username=other_owner.user_name)\n\n# cleanup\nw.users.delete(id=other_owner.id)\nw.clusters.permanent_delete(cluster_id=clstr.cluster_id)\n```\n\n----------------------------------------\n\nTITLE: Initializing WorkspaceClient with Default Authentication - Python\nDESCRIPTION: This snippet initializes the Databricks WorkspaceClient using the default authentication flow. It relies on configuration profiles or environment variables for authentication. No explicit credentials are provided in the code.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/authentication.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient()\nw. # press <TAB> for autocompletion\n```\n\n----------------------------------------\n\nTITLE: Databricks Volume Usage Example\nDESCRIPTION: This code snippet demonstrates a full workflow for creating and managing Databricks volumes using the Databricks SDK for Python.  It covers creating storage credentials, external locations, catalogs, schemas, volumes, reading volume details, updating volume comment, and cleaning up all the created resources. The snippet requires the `TEST_METASTORE_DATA_ACCESS_ARN` and `TEST_BUCKET` environment variables to be set.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/volumes.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\nstorage_credential = w.storage_credentials.create(\n    name=f\"sdk-{time.time_ns()}\",\n    aws_iam_role=catalog.AwsIamRoleRequest(role_arn=os.environ[\"TEST_METASTORE_DATA_ACCESS_ARN\"]),\n    comment=\"created via SDK\",\n)\n\nexternal_location = w.external_locations.create(\n    name=f\"sdk-{time.time_ns()}\",\n    credential_name=storage_credential.name,\n    comment=\"created via SDK\",\n    url=\"s3://\" + os.environ[\"TEST_BUCKET\"] + \"/\" + f\"sdk-{time.time_ns()}\",\n)\n\ncreated_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated_schema = w.schemas.create(name=f\"sdk-{time.time_ns()}\", catalog_name=created_catalog.name)\n\ncreated_volume = w.volumes.create(\n    catalog_name=created_catalog.name,\n    schema_name=created_schema.name,\n    name=f\"sdk-{time.time_ns()}\",\n    storage_location=external_location.url,\n    volume_type=catalog.VolumeType.EXTERNAL,\n)\n\nloaded_volume = w.volumes.read(name=created_volume.full_name)\n\n_ = w.volumes.update(name=loaded_volume.full_name, comment=\"Updated volume comment\")\n\n# cleanup\nw.storage_credentials.delete(name=storage_credential.name)\nw.external_locations.delete(name=external_location.name)\nw.schemas.delete(full_name=created_schema.full_name)\nw.catalogs.delete(name=created_catalog.name, force=True)\nw.volumes.delete(name=created_volume.full_name)\n```\n\n----------------------------------------\n\nTITLE: Listing All Workspaces with Databricks AccountClient in Python\nDESCRIPTION: This snippet shows how to retrieve a list of all workspaces associated with an account using the `AccountClient` from the `databricks.sdk`. It initializes the client and then calls `a.workspaces.list()` to retrieve the list of workspaces. The result is an iterator over `Workspace` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/workspaces.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nall = a.workspaces.list()\n```\n\n----------------------------------------\n\nTITLE: Updating User Details using Databricks SDK in Python\nDESCRIPTION: Updates user details in the Databricks workspace using the WorkspaceClient's patch method. Creates a user, then updates the user's active status. It depends on the `databricks.sdk` and `databricks.sdk.service.iam` libraries.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/users.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import iam\n\nw = WorkspaceClient()\n\nuser = w.users.create(\n    display_name=f\"sdk-{time.time_ns()}\",\n    user_name=f\"sdk-{time.time_ns()}@example.com\",\n)\n\nw.users.patch(\n    id=user.id,\n    operations=[iam.Patch(op=iam.PatchOp.REPLACE, path=\"active\", value=\"false\")],\n    schemas=[iam.PatchSchema.URN_IETF_PARAMS_SCIM_API_MESSAGES_2_0_PATCH_OP],\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Configuration of Databricks Serving Endpoint\nDESCRIPTION: This method updates the configuration of a Databricks serving endpoint, including served entities, compute configurations, and traffic configuration. An endpoint undergoing an update cannot be updated again until the current update completes or fails. The `auto_capture_config` is deprecated, and AI Gateway is recommended for managing inference tables.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: update_config(name: str [, auto_capture_config: Optional[AutoCaptureConfigInput], served_entities: Optional[List[ServedEntityInput]], served_models: Optional[List[ServedModelInput]], traffic_config: Optional[TrafficConfig]]) -> Wait[ServingEndpointDetailed]\n\n        Update config of a serving endpoint.\n\n        Updates any combination of the serving endpoint's served entities, the compute configuration of those\\n        served entities, and the endpoint's traffic config. An endpoint that already has an update in progress\\n        can not be updated until the current update completes or fails.\n\n        :param name: str\n          The name of the serving endpoint to update. This field is required.\n        :param auto_capture_config: :class:`AutoCaptureConfigInput` (optional)\n          Configuration for Inference Tables which automatically logs requests and responses to Unity Catalog.\\n          Note: this field is deprecated for creating new provisioned throughput endpoints, or updating\\n          existing provisioned throughput endpoints that never have inference table configured; in these cases\\n          please use AI Gateway to manage inference tables.\n        :param served_entities: List[:class:`ServedEntityInput`] (optional)\n          The list of served entities under the serving endpoint config.\n        :param served_models: List[:class:`ServedModelInput`] (optional)\n          (Deprecated, use served_entities instead) The list of served models under the serving endpoint\\n          config.\n        :param traffic_config: :class:`TrafficConfig` (optional)\n          The traffic configuration associated with the serving endpoint config.\n\n        :returns:\\n          Long-running operation waiter for :class:`ServingEndpointDetailed`\\n          See :method:wait_get_serving_endpoint_not_updating for more details.\n        \n```\n\n----------------------------------------\n\nTITLE: Editing a Cluster Policy in Databricks with Python SDK\nDESCRIPTION: This code snippet showcases how to edit an existing cluster policy using the Databricks SDK for Python. It retrieves a policy, modifies its definition to set `spark_conf.spark.databricks.delta.preview.enabled` to false, and then updates the policy. A temporary policy is created initially and then deleted as part of the cleanup process.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/cluster_policies.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.cluster_policies.create(\n    name=f\"sdk-{time.time_ns()}\",\n    definition=\"\"\"{\n            \"spark_conf.spark.databricks.delta.preview.enabled\": {\n                \"type\": \"fixed\",\n                \"value\": true\n            }\n        }\n\"\"\",\n)\n\npolicy = w.cluster_policies.get(policy_id=created.policy_id)\n\nw.cluster_policies.edit(\n    policy_id=policy.policy_id,\n    name=policy.name,\n    definition=\"\"\"{\n            \"spark_conf.spark.databricks.delta.preview.enabled\": {\n                \"type\": \"fixed\",\n                \"value\": false\n            }\n        }\n\"\"\",\n)\n\n# cleanup\nw.cluster_policies.delete(policy_id=created.policy_id)\n```\n\n----------------------------------------\n\nTITLE: Creating a Storage Credential (Python)\nDESCRIPTION: This method creates a new storage credential within a specified metastore. The credential information is cloud-specific, accepting AwsIamRole, AzureServicePrincipal, or GcpServiceAccountKey. The caller must be a metastore admin with the CREATE_STORAGE_CREDENTIAL privilege.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/catalog/storage_credentials.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmetastore_id: str [, credential_info: Optional[CreateStorageCredential]]\n```\n\n----------------------------------------\n\nTITLE: Export Job Run Example (Python)\nDESCRIPTION: This code snippet demonstrates how to export and retrieve a job run using the Databricks SDK for Python. It creates a job, runs it, exports the view (code), and then cleans up by deleting the job. It depends on the `databricks.sdk` library and requires setting the `DATABRICKS_CLUSTER_ID` environment variable.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\ncreated_job = w.jobs.create(\n    name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.Task(\n            description=\"test\",\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=\"test\",\n            timeout_seconds=0,\n        )\n    ],\n)\n\nrun_by_id = w.jobs.run_now(job_id=created_job.job_id).result()\n\nexported_view = w.jobs.export_run(run_id=run_by_id.tasks[0].run_id, views_to_export=\"CODE\")\n\n# cleanup\nw.jobs.delete(job_id=created_job.job_id)\n```\n\n----------------------------------------\n\nTITLE: Submitting a Job and Waiting for Completion - Python\nDESCRIPTION: This snippet demonstrates how to submit a Databricks job and wait for it to complete. It creates a simple Python file on DBFS, submits a job that executes the file, and then uses the `result()` method on the waiter object to wait for the job to finish.  It also includes a callback function to log the intermediate status of the job. The `timeout` parameter specifies the maximum time to wait for the job to reach the TERMINATED or SKIPPED state.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/wait.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\nimport logging\nimport time\nfrom databricks.sdk import WorkspaceClient\nimport databricks.sdk.service.jobs as j\nw = WorkspaceClient()\n# create a dummy file on DBFS that just sleeps for 10 seconds\npy_on_dbfs = f'/home/{w.current_user.me().user_name}/sample.py'\nwith w.dbfs.open(py_on_dbfs, write=True, overwrite=True) as f:\n    f.write(b'import time; time.sleep(10); print(\"Hello, World!\")')\n# trigger one-time-run job and get waiter object\nwaiter = w.jobs.submit(run_name=f'py-sdk-run-{time.time()}', tasks=[\n    j.RunSubmitTaskSettings(\n        task_key='hello_world',\n        new_cluster=j.BaseClusterInfo(\n            spark_version=w.clusters.select_spark_version(long_term_support=True),\n            node_type_id=w.clusters.select_node_type(local_disk=True),\n            num_workers=1\n        ),\n        spark_python_task=j.SparkPythonTask(\n            python_file=f'dbfs:{py_on_dbfs}'\n        ),\n    )\n])\nlogging.info(f'starting to poll: {waiter.run_id}')\n# callback, that receives a polled entity between state updates\ndef print_status(run: j.Run):\n    statuses = [f'{t.task_key}: {t.state.life_cycle_state}' for t in run.tasks]\n    logging.info(f'workflow intermediate status: {\", \".join(statuses)}')\n# If you want to perform polling in a separate thread, process, or service,\n# you can use w.jobs.wait_get_run_job_terminated_or_skipped(\n#   run_id=waiter.run_id,\n#   timeout=datetime.timedelta(minutes=15),\n#   callback=print_status) to achieve the same results.\n#\n# Waiter interface allows for `w.jobs.submit(..).result()` simplicity in\n# the scenarios, where you need to block the calling thread for the job to finish.\nrun = waiter.result(timeout=datetime.timedelta(minutes=15),\n                    callback=print_status)\nlogging.info(f'job finished: {run.run_page_url}')\n```\n\n----------------------------------------\n\nTITLE: Get a Metastore by ID in Python\nDESCRIPTION: This snippet shows how to retrieve a specific metastore by its ID using the Databricks SDK.  It first creates a metastore, then retrieves it by its metastore ID, and finally cleans up by deleting the created metastore. It relies on the WorkspaceClient from the databricks.sdk.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/metastores.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.metastores.create(\n    name=f\"sdk-{time.time_ns()}\",\n    storage_root=\"s3://%s/%s\" % (os.environ[\"TEST_BUCKET\"], f\"sdk-{time.time_ns()}\"),\n)\n\n_ = w.metastores.get(id=created.metastore_id)\n\n# cleanup\nw.metastores.delete(id=created.metastore_id, force=True)\n```\n\n----------------------------------------\n\nTITLE: Get Table Example - Databricks SDK\nDESCRIPTION: This snippet demonstrates how to retrieve a table using the Databricks SDK. It includes setting up the WorkspaceClient, creating a catalog and schema, executing a SQL statement to create a table, getting the table information, and finally cleaning up the created resources. The code relies on the 'TEST_DEFAULT_WAREHOUSE_ID' environment variable.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/tables.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ntable_name = f\"sdk-{time.time_ns()}\"\n\ncreated_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated_schema = w.schemas.create(name=f\"sdk-{time.time_ns()}\", catalog_name=created_catalog.name)\n\n_ = w.statement_execution.execute(\n    warehouse_id=os.environ[\"TEST_DEFAULT_WAREHOUSE_ID\"],\n    catalog=created_catalog.name,\n    schema=created_schema.name,\n    statement=\"CREATE TABLE %s AS SELECT 2+2 as four\" % (table_name),\n).result()\n\ntable_full_name = \"%s.%s.%s\" % (\n    created_catalog.name,\n    created_schema.name,\n    table_name,\n)\n\ncreated_table = w.tables.get(full_name=table_full_name)\n\n# cleanup\nw.schemas.delete(full_name=created_schema.full_name)\nw.catalogs.delete(name=created_catalog.name, force=True)\nw.tables.delete(full_name=table_full_name)\n```\n\n----------------------------------------\n\nTITLE: Submit Databricks Job and Get Run Output in Python\nDESCRIPTION: This code snippet demonstrates how to submit a Databricks job and retrieve the run output using the Python SDK. It creates a notebook task and submits it to a Databricks cluster.  The snippet also shows how to clean up resources by deleting the run after completion.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\nrun = w.jobs.submit(\n    run_name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.SubmitTask(\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=f\"sdk-{time.time_ns()}\",\n        )\n    ],\n).result()\n\noutput = w.jobs.get_run_output(run_id=run.tasks[0].run_id)\n\n# cleanup\nw.jobs.delete_run(run_id=run.run_id)\n```\n\n----------------------------------------\n\nTITLE: Creating and Deleting a Cluster in Databricks with Python\nDESCRIPTION: This snippet demonstrates how to create a new Databricks cluster using the Databricks SDK for Python.  It first retrieves the latest Spark version, then creates a cluster with a specified name, Spark version, instance pool ID, autotermination time, and number of workers. Finally, it retrieves the cluster details by ID and deletes the cluster. The environment variable TEST_INSTANCE_POOL_ID must be set.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nlatest = w.clusters.select_spark_version(latest=True, long_term_support=True)\n\ncluster_name = f\"sdk-{time.time_ns()}\"\n\nclstr = w.clusters.create(\n    cluster_name=cluster_name,\n    spark_version=latest,\n    instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n    autotermination_minutes=15,\n    num_workers=1,\n).result()\n\nby_id = w.clusters.get(cluster_id=clstr.cluster_id)\n\n# cleanup\nw.clusters.permanent_delete(cluster_id=clstr.cluster_id)\n```\n\n----------------------------------------\n\nTITLE: Creating a User using Databricks SDK in Python\nDESCRIPTION: Creates a new user in the Databricks workspace using the WorkspaceClient.  It takes a user_name as input, creates the user, and then deletes the created user. It depends on the `databricks.sdk` library.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/users.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nother_owner = w.users.create(user_name=f\"sdk-{time.time_ns()}@example.com\")\n\n# cleanup\nw.users.delete(id=other_owner.id)\n```\n\n----------------------------------------\n\nTITLE: Updating a rule set - Python\nDESCRIPTION: This method replaces the rules of an existing rule set.  It is recommended to first retrieve the current version of the rule set using the 'get_rule_set' method before updating it. The 'name' of the rule set and a RuleSetUpdateRequest object are required. The method returns a RuleSetResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/access_control.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nAccountAccessControlAPI.update_rule_set(name: str, rule_set: RuleSetUpdateRequest) -> RuleSetResponse\n```\n\n----------------------------------------\n\nTITLE: Updating a Catalog in Databricks with Python SDK\nDESCRIPTION: This snippet demonstrates how to update a catalog's properties, specifically the isolation mode, using the Databricks Python SDK. It creates a catalog, updates its isolation mode using `w.catalogs.update()`, and then cleans up by deleting the catalog. The example sets the isolation mode to `ISOLATED`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/catalogs.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\ncreated = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\n_ = w.catalogs.update(name=created.name, isolation_mode=catalog.CatalogIsolationMode.ISOLATED)\n\n# cleanup\nw.catalogs.delete(name=created.name, force=True)\n```\n\n----------------------------------------\n\nTITLE: Create User with AccountClient in Python\nDESCRIPTION: This code snippet demonstrates how to create a new user in a Databricks account using the AccountClient and the `create` method. It includes setting a display name and user name.  It also includes clean up by deleting the user. It requires the `databricks.sdk` library.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/users.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nuser = a.users.create(\n    display_name=f\"sdk-{time.time_ns()}\",\n    user_name=f\"sdk-{time.time_ns()}@example.com\",\n)\n\n# cleanup\na.users.delete(id=user.id)\n```\n\n----------------------------------------\n\nTITLE: Updating a Legacy Query in Databricks SQL\nDESCRIPTION: This method modifies an existing query definition in Databricks SQL. It allows updating parameters such as data_source_id, description, name, query text, and tags.  The method takes the query_id and optional parameters for updating the query definition.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/queries_legacy.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(query_id: str [, data_source_id: Optional[str], description: Optional[str], name: Optional[str], options: Optional[Any], query: Optional[str], run_as_role: Optional[RunAsRole], tags: Optional[List[str]]]) -> LegacyQuery\n```\n\n----------------------------------------\n\nTITLE: Update Global Init Script in Databricks\nDESCRIPTION: This code snippet shows how to update an existing global init script using the Databricks SDK. It creates a script, then updates it with a new name and script content. The updated script now contains \"echo 2\" instead of \"echo 1\". Finally, the script is deleted.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/global_init_scripts.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.global_init_scripts.create(\n    name=f\"sdk-{time.time_ns()}\",\n    script=base64.b64encode((\"echo 1\").encode()).decode(),\n    enabled=True,\n    position=10,\n)\n\nw.global_init_scripts.update(\n    script_id=created.script_id,\n    name=f\"sdk-{time.time_ns()}\",\n    script=base64.b64encode((\"echo 2\").encode()).decode(),\n)\n\n# cleanup\nw.global_init_scripts.delete(script_id=created.script_id)\n```\n\n----------------------------------------\n\nTITLE: Creating a Registered Model with databricks-sdk-py\nDESCRIPTION: Creates a new registered model in Unity Catalog.  Requires USE_CATALOG privilege on the parent catalog, USE_SCHEMA privilege on the parent schema and CREATE MODEL or CREATE FUNCTION privilege on the parent schema.  File storage for model versions will be located in the default location specified by the parent schema, catalog or Metastore.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/registered_models.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(catalog_name: str, schema_name: str, name: str [, comment: Optional[str], storage_location: Optional[str]]) -> RegisteredModelInfo\n\n    Create a Registered Model.\n\n    Creates a new registered model in Unity Catalog.\n\n    File storage for model versions in the registered model will be located in the default location which\n    is specified by the parent schema, or the parent catalog, or the Metastore.\n\n    For registered model creation to succeed, the user must satisfy the following conditions: - The caller\n    must be a metastore admin, or be the owner of the parent catalog and schema, or have the\n    **USE_CATALOG** privilege on the parent catalog and the **USE_SCHEMA** privilege on the parent schema.\n    - The caller must have the **CREATE MODEL** or **CREATE FUNCTION** privilege on the parent schema.\n\n    :param catalog_name: str\n      The name of the catalog where the schema and the registered model reside\n    :param schema_name: str\n      The name of the schema where the registered model resides\n    :param name: str\n      The name of the registered model\n    :param comment: str (optional)\n      The comment attached to the registered model\n    :param storage_location: str (optional)\n      The storage location on the cloud under which model version data files are stored\n\n    :returns: :class:`RegisteredModelInfo`\n```\n\n----------------------------------------\n\nTITLE: Waiting for App to be Idle in Databricks\nDESCRIPTION: Waits for the specified app to reach an idle state. Includes an optional timeout and callback function.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nw.apps.wait_get_app_idle(name: str, timeout: datetime.timedelta = 0:20:00, callback: Optional[Callable[[App], None]]) -> App\n```\n\n----------------------------------------\n\nTITLE: Enforce Job Policy Compliance using Databricks SDK (Python)\nDESCRIPTION: Updates a job so the job clusters are compliant with the current versions of their cluster policies. All-purpose clusters used in the job will not be updated. It takes the job ID as a parameter and an optional validate_only flag to preview changes without updating the job. Returns an EnforcePolicyComplianceResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/policy_compliance_for_jobs.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: enforce_compliance(job_id: int [, validate_only: Optional[bool]]) -> EnforcePolicyComplianceResponse\n\n    Enforce job policy compliance.\n\n    Updates a job so the job clusters that are created when running the job (specified in `new_cluster`)\n    are compliant with the current versions of their respective cluster policies. All-purpose clusters\n    used in the job will not be updated.\n\n    :param job_id: int\n      The ID of the job you want to enforce policy compliance on.\n    :param validate_only: bool (optional)\n      If set, previews changes made to the job to comply with its policy, but does not update the job.\n\n    :returns: :class:`EnforcePolicyComplianceResponse`\n```\n\n----------------------------------------\n\nTITLE: Listing Catalogs in Databricks with Python SDK\nDESCRIPTION: This code demonstrates how to list all accessible catalogs using the Databricks Python SDK. It initializes the WorkspaceClient and then calls `w.catalogs.list()` to retrieve the list of catalogs. It imports the `catalog` service to specify the request type.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/catalogs.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\nall = w.catalogs.list(catalog.ListCatalogsRequest())\n```\n\n----------------------------------------\n\nTITLE: Listing Queries with Filters - Python\nDESCRIPTION: This code snippet demonstrates how to list query history using the `list` method of the `QueryHistoryAPI` within the Databricks SDK. It shows how to filter queries by specifying a time range using the `QueryFilter` and `TimeRange` classes. It requires the `WorkspaceClient` from the `databricks.sdk` and relevant classes from `databricks.sdk.service.sql`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/query_history.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\n_ = w.query_history.list(\n    filter_by=sql.QueryFilter(\n        query_start_time_range=sql.TimeRange(start_time_ms=1690243200000, end_time_ms=1690329600000)\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting Private Endpoint Rule in Databricks\nDESCRIPTION: This method initiates the deletion of a private endpoint rule. The function accepts the network connectivity configuration ID and the private endpoint rule ID. If the connection state is PENDING or EXPIRED, the private endpoint is immediately deleted; otherwise, it's deactivated for seven days before deletion.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/network_connectivity.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nNetworkConnectivityAPI.delete_private_endpoint_rule(network_connectivity_config_id: str, private_endpoint_rule_id: str) -> NccAzurePrivateEndpointRule\n```\n\n----------------------------------------\n\nTITLE: Download Billable Usage Logs - Python\nDESCRIPTION: Downloads billable usage logs in CSV format for a specified account and date range using the Databricks AccountClient. Requires the `databricks.sdk` package.  The `start_month` and `end_month` parameters are mandatory, specifying the date range in `YYYY-MM` format.  The optional `personal_data` parameter controls the inclusion of personally identifiable information.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/billable_usage.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nresp = a.billable_usage.download(start_month=\"2024-08\", end_month=\"2024-09\")\n```\n\n----------------------------------------\n\nTITLE: Get Job by ID Example (Python)\nDESCRIPTION: This code snippet demonstrates how to retrieve a job by its ID using the Databricks SDK for Python. It creates a job, retrieves it using `w.jobs.get`, and then cleans up by deleting the job. It depends on the `databricks.sdk` library and requires setting the `DATABRICKS_CLUSTER_ID` environment variable.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\ncreated_job = w.jobs.create(\n    name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.Task(\n            description=\"test\",\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=\"test\",\n            timeout_seconds=0,\n        )\n    ],\n)\n\nby_id = w.jobs.get(job_id=created_job.job_id)\n\n# cleanup\nw.jobs.delete(job_id=created_job.job_id)\n```\n\n----------------------------------------\n\nTITLE: Getting a Pipeline by ID with Databricks SDK in Python\nDESCRIPTION: This snippet demonstrates how to retrieve an existing Delta Live Tables pipeline by its ID using the Databricks SDK for Python. It first creates a pipeline, retrieves it using its ID, and then deletes it for cleanup. This showcases the usage of `WorkspaceClient` and the `pipelines.get` method.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/pipelines/pipelines.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import pipelines\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncreated = w.pipelines.create(\n    continuous=False,\n    name=f\"sdk-{time.time_ns()}\",\n    libraries=[pipelines.PipelineLibrary(notebook=pipelines.NotebookLibrary(path=notebook_path))],\n    clusters=[\n        pipelines.PipelineCluster(\n            instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n            label=\"default\",\n            num_workers=1,\n            custom_tags={\n                \"cluster_type\": \"default\",\n            },\n        )\n    ],\n)\n\nby_id = w.pipelines.get(pipeline_id=created.pipeline_id)\n\n# cleanup\nw.pipelines.delete(pipeline_id=created.pipeline_id)\n```\n\n----------------------------------------\n\nTITLE: Listing Recipients in Python\nDESCRIPTION: This code snippet demonstrates how to list all recipients using the `list` method. It uses `WorkspaceClient` from the `databricks.sdk` to interact with the Databricks workspace and `ListRecipientsRequest` from `databricks.sdk.service.sharing` to specify the listing parameters. It initializes a `WorkspaceClient` and calls the `list` method on the `recipients` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/recipients.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sharing\n\nw = WorkspaceClient()\n\nall = w.recipients.list(sharing.ListRecipientsRequest())\n```\n\n----------------------------------------\n\nTITLE: List All Tokens with Databricks SDK in Python\nDESCRIPTION: This snippet lists all tokens associated with the workspace using the Databricks SDK. It initializes a `WorkspaceClient` and then calls the `list` method of the `token_management` API to retrieve all tokens. No clean up is needed as it only lists existing tokens.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/token_management.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import settings\n\nw = WorkspaceClient()\n\nall = w.token_management.list(settings.ListTokenManagementRequest())\n```\n\n----------------------------------------\n\nTITLE: Deleting a Dashboard using Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to delete a dashboard using the Databricks SDK. It creates a dashboard, then deletes it using the `delete` method, moving it to the trash. It also includes a redundant cleanup step to ensure the dashboard is removed.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/dashboards.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.dashboards.create(name=f\"sdk-{time.time_ns()}\")\n\nw.dashboards.delete(dashboard_id=created.id)\n\n# cleanup\nw.dashboards.delete(dashboard_id=created.id)\n```\n\n----------------------------------------\n\nTITLE: Starting an App in Databricks\nDESCRIPTION: Starts the last active deployment of the app in the workspace and returns the AppDeployment object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nw.apps.start(name: str) -> AppDeployment\n```\n\n----------------------------------------\n\nTITLE: Delete Listing in Databricks Marketplace (Python)\nDESCRIPTION: Deletes an existing listing from the Databricks Marketplace.  It takes the listing ID (str) as input. This operation removes the listing from the marketplace.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_listings.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nw.provider_listings.delete(id: str)\n```\n\n----------------------------------------\n\nTITLE: Querying a Serving Endpoint in Databricks\nDESCRIPTION: This method allows querying a deployed serving endpoint with various input types. It supports different data formats such as Pandas DataFrames (records and split orientations) and tensor-based inputs (columnar and row formats). It also allows for specifying parameters for completions, chat, and embeddings external & foundation model serving endpoints.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: query(name: str [, dataframe_records: Optional[List[Any]], dataframe_split: Optional[DataframeSplitInput], extra_params: Optional[Dict[str, str]], input: Optional[Any], inputs: Optional[Any], instances: Optional[List[Any]], max_tokens: Optional[int], messages: Optional[List[ChatMessage]], n: Optional[int], prompt: Optional[Any], stop: Optional[List[str]], stream: Optional[bool], temperature: Optional[float]]) -> QueryEndpointResponse\n\n        Query a serving endpoint.\n\n        :param name: str\n          The name of the serving endpoint. This field is required.\n        :param dataframe_records: List[Any] (optional)\n          Pandas Dataframe input in the records orientation.\n        :param dataframe_split: :class:`DataframeSplitInput` (optional)\n          Pandas Dataframe input in the split orientation.\n        :param extra_params: Dict[str,str] (optional)\n          The extra parameters field used ONLY for __completions, chat,__ and __embeddings external &\\n          foundation model__ serving endpoints. This is a map of strings and should only be used with other\\n          external/foundation model query fields.\n        :param input: Any (optional)\n          The input string (or array of strings) field used ONLY for __embeddings external & foundation\\n          model__ serving endpoints and is the only field (along with extra_params if needed) used by\\n          embeddings queries.\n        :param inputs: Any (optional)\n          Tensor-based input in columnar format.\n        :param instances: List[Any] (optional)\n          Tensor-based input in row format.\n        :param max_tokens: int (optional)\n          The max tokens field used ONLY for __completions__ and __chat external & foundation model__ serving\\n          endpoints. This is an integer and should only be used with other chat/completions query fields.\n        :param messages: List[:class:`ChatMessage`] (optional)\n          The messages field used ONLY for __chat external & foundation model__ serving endpoints. This is a\\n          map of strings and should only be used with other chat query fields.\n        :param n: int (optional)\n          The n (number of candidates) field used ONLY for __completions__ and __chat external & foundation\\n          model__ serving endpoints. This is an integer between 1 and 5 with a default of 1 and should only be\\n          used with other chat/completions query fields.\n        :param prompt: Any (optional)\n          The prompt string (or array of strings) field used ONLY for __completions external & foundation\\n          model__ serving endpoints and should only be used with other completions query fields.\n        :param stop: List[str] (optional)\n          The stop sequences field used ONLY for __completions__ and __chat external & foundation model__\\n          serving endpoints. This is a list of strings and should only be used with other chat/completions\\n          query fields.\n        :param stream: bool (optional)\n          The stream field used ONLY for __completions__ and __chat external & foundation model__ serving\\n          endpoints. This is a boolean defaulting to false and should only be used with other chat/completions\\n          query fields.\n        :param temperature: float (optional)\n          The temperature field used ONLY for __completions__ and __chat external & foundation model__ serving\\n          endpoints. This is a float between 0.0 and 2.0 with a default of 1.0 and should only be used with\\n          other chat/completions query fields.\n\n        :returns: :class:`QueryEndpointResponse`\n        \n```\n\n----------------------------------------\n\nTITLE: Creating a Vector Search Index\nDESCRIPTION: This method creates a new vector search index. It requires the index name, endpoint name, primary key, and index type. Optionally, it takes specifications for Delta Sync or Direct Vector Access indexes based on the index type.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_indexes.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create_index(name: str, endpoint_name: str, primary_key: str, index_type: VectorIndexType [, delta_sync_index_spec: Optional[DeltaSyncVectorIndexSpecRequest], direct_access_index_spec: Optional[DirectAccessVectorIndexSpec]]) -> CreateVectorIndexResponse\n\n        Create an index.\n\n        Create a new index.\n\n        :param name: str\n          Name of the index\n        :param endpoint_name: str\n          Name of the endpoint to be used for serving the index\n        :param primary_key: str\n          Primary key of the index\n        :param index_type: :class:`VectorIndexType`\n          There are 2 types of Vector Search indexes:\n\n          - `DELTA_SYNC`: An index that automatically syncs with a source Delta Table, automatically and\n          incrementally updating the index as the underlying data in the Delta Table changes. -\n          `DIRECT_ACCESS`: An index that supports direct read and write of vectors and metadata through our\n          REST and SDK APIs. With this model, the user manages index updates.\n        :param delta_sync_index_spec: :class:`DeltaSyncVectorIndexSpecRequest` (optional)\n          Specification for Delta Sync Index. Required if `index_type` is `DELTA_SYNC`.\n        :param direct_access_index_spec: :class:`DirectAccessVectorIndexSpec` (optional)\n          Specification for Direct Vector Access Index. Required if `index_type` is `DIRECT_ACCESS`.\n\n        :returns: :class:`CreateVectorIndexResponse`\n```\n\n----------------------------------------\n\nTITLE: Starting a Terminated Cluster with WorkspaceClient\nDESCRIPTION: This code snippet demonstrates how to start a terminated Databricks cluster using the `start` method of the `WorkspaceClient`. It includes the setup of a cluster before starting it and also the clean up after. It utilizes `os` for accessing environment variables and `time` to generate unique cluster names.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nlatest = w.clusters.select_spark_version(latest=True, long_term_support=True)\n\ncluster_name = f\"sdk-{time.time_ns()}\"\n\nclstr = w.clusters.create(\n    cluster_name=cluster_name,\n    spark_version=latest,\n    instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n    autotermination_minutes=15,\n    num_workers=1,\n).result()\n\n_ = w.clusters.start(cluster_id=clstr.cluster_id).result()\n\n# cleanup\nw.clusters.permanent_delete(cluster_id=clstr.cluster_id)\n```\n\n----------------------------------------\n\nTITLE: Running a Job with Databricks Jobs API in Python\nDESCRIPTION: This code snippet demonstrates how to use the Databricks SDK to run a job in Databricks. It includes creating a workspace client, ensuring a cluster is running, creating a job, running the job, and then cleaning up by deleting the created job. It depends on the databricks-sdk package and accesses environment variables for configuration.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\ncreated_job = w.jobs.create(\n    name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.Task(\n            description=\"test\",\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=\"test\",\n            timeout_seconds=0,\n        )\n    ],\n)\n\nrun_by_id = w.jobs.run_now(job_id=created_job.job_id).result()\n\n# cleanup\nw.jobs.delete(job_id=created_job.job_id)\n```\n\n----------------------------------------\n\nTITLE: Get Permissions on Securable - Python\nDESCRIPTION: This code snippet demonstrates how to retrieve permissions for a securable object, specifically a table, using the `get_effective` method of the `GrantsAPI`. It first creates a catalog, schema, and table, then retrieves the effective permissions for the created table. Finally, it cleans up the created resources. Requires the Databricks SDK for Python and the `TEST_DEFAULT_WAREHOUSE_ID` environment variable.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/grants.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\ntable_name = f\"sdk-{time.time_ns()}\"\n\ncreated_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated_schema = w.schemas.create(name=f\"sdk-{time.time_ns()}\", catalog_name=created_catalog.name)\n\n_ = w.statement_execution.execute(\n    warehouse_id=os.environ[\"TEST_DEFAULT_WAREHOUSE_ID\"],\n    catalog=created_catalog.name,\n    schema=created_schema.name,\n    statement=\"CREATE TABLE %s AS SELECT 2+2 as four\" % (table_name),\n).result()\n\ntable_full_name = \"%s.%s.%s\" % (\n    created_catalog.name,\n    created_schema.name,\n    table_name,\n)\n\ncreated_table = w.tables.get(full_name=table_full_name)\n\ngrants = w.grants.get_effective(\n    securable_type=catalog.SecurableType.TABLE,\n    full_name=created_table.full_name,\n)\n\n# cleanup\nw.schemas.delete(full_name=created_schema.full_name)\nw.catalogs.delete(name=created_catalog.name, force=True)\nw.tables.delete(full_name=table_full_name)\n```\n\n----------------------------------------\n\nTITLE: Update Custom OAuth App Integration (Python)\nDESCRIPTION: Updates an existing Custom OAuth App Integration, identified by its integration ID. It allows updating redirect URLs, OAuth scopes, token access policy and user-authorized scopes.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/custom_app_integration.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(integration_id: str [, redirect_urls: Optional[List[str]], scopes: Optional[List[str]], token_access_policy: Optional[TokenAccessPolicy], user_authorized_scopes: Optional[List[str]]])\n\n    Updates Custom OAuth App Integration.\n\n    Updates an existing custom OAuth App Integration. You can retrieve the custom OAuth app integration\n    via :method:CustomAppIntegration/get.\n\n    :param integration_id: str\n    :param redirect_urls: List[str] (optional)\n      List of OAuth redirect urls to be updated in the custom OAuth app integration\n    :param scopes: List[str] (optional)\n      List of OAuth scopes to be updated in the custom OAuth app integration, similar to redirect URIs\n      this will fully replace the existing values instead of appending\n    :param token_access_policy: :class:`TokenAccessPolicy` (optional)\n      Token access policy to be updated in the custom OAuth app integration\n    :param user_authorized_scopes: List[str] (optional)\n      Scopes that will need to be consented by end user to mint the access token. If the user does not\n      authorize the access token will not be minted. Must be a subset of scopes.\n```\n\n----------------------------------------\n\nTITLE: Getting Cluster Policy Compliance Status with Python\nDESCRIPTION: This method returns the policy compliance status of a specified cluster. It allows users to check if a cluster is out of compliance due to policy updates after the cluster was last edited.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/policy_compliance_for_clusters.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_compliance(cluster_id: str) -> GetClusterComplianceResponse\n```\n\n----------------------------------------\n\nTITLE: Update catalog workspace bindings using Databricks SDK\nDESCRIPTION: This snippet shows how to update the workspace bindings of a catalog using the Databricks SDK. It creates a catalog, assigns it to a specific workspace using the `update` method, and then deletes the catalog.  The environment variable `THIS_WORKSPACE_ID` must be set to the workspace ID to assign the catalog to.  Requires Databricks SDK and a WorkspaceClient instance.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/workspace_bindings.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nthis_workspace_id = os.environ[\"THIS_WORKSPACE_ID\"]\n\ncreated = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\n_ = w.workspace_bindings.update(name=created.name, assign_workspaces=[this_workspace_id])\n\n# cleanup\nw.catalogs.delete(name=created.name, force=True)\n```\n\n----------------------------------------\n\nTITLE: Create and Delete Cluster\nDESCRIPTION: This code snippet demonstrates how to create a Databricks cluster using the WorkspaceClient and then delete it. It uses environment variables for instance pool ID and configures the cluster with specific settings like spark version, number of workers, and autotermination minutes. A `permanent_delete` is invoked after a regular `delete`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nlatest = w.clusters.select_spark_version(latest=True, long_term_support=True)\n\ncluster_name = f\"sdk-{time.time_ns()}\"\n\nclstr = w.clusters.create(\n    cluster_name=cluster_name,\n    spark_version=latest,\n    instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n    autotermination_minutes=15,\n    num_workers=1,\n).result()\n\n_ = w.clusters.delete(cluster_id=clstr.cluster_id).result()\n\n# cleanup\nw.clusters.permanent_delete(cluster_id=clstr.cluster_id)\n```\n\n----------------------------------------\n\nTITLE: Creating a Clean Room - Python\nDESCRIPTION: Creates a new clean room with specified collaborators. The caller must be a metastore admin or have the **CREATE_CLEAN_ROOM** privilege on the metastore. Requires name and remote_detailed_info parameters. Returns a CleanRoomInfo object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/clean_rooms.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(name: str, remote_detailed_info: CentralCleanRoomInfo [, comment: Optional[str]]) -> CleanRoomInfo\n\n    Create a clean room.\n    \n    Creates a new clean room with specified colaborators. The caller must be a metastore admin or have the\n    **CREATE_CLEAN_ROOM** privilege on the metastore.\n    \n    :param name: str\n      Name of the clean room.\n    :param remote_detailed_info: :class:`CentralCleanRoomInfo`\n      Central clean room details.\n    :param comment: str (optional)\n      User-provided free-form text description.\n    \n    :returns: :class:`CleanRoomInfo`\n```\n\n----------------------------------------\n\nTITLE: Get IP Access List by ID with Databricks SDK\nDESCRIPTION: This code shows how to retrieve an IP access list by its ID using the Databricks SDK.  It creates a new IP access list, retrieves it by its ID, and then deletes it. It demonstrates how to use the `get` method with the IP access list ID.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/ip_access_lists.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import settings\n\nw = WorkspaceClient()\n\ncreated = w.ip_access_lists.create(\n    label=f\"sdk-{time.time_ns()}\",\n    ip_addresses=[\"1.0.0.0/16\"],\n    list_type=settings.ListType.BLOCK,\n)\n\nby_id = w.ip_access_lists.get(ip_access_list_id=created.ip_access_list.list_id)\n\n# cleanup\nw.ip_access_lists.delete(ip_access_list_id=created.ip_access_list.list_id)\n```\n\n----------------------------------------\n\nTITLE: Validating a Credential in Databricks (Python)\nDESCRIPTION: This method validates a credential. For service credentials, either the credential name or cloud-specific credentials must be provided. For storage credentials, either the external location name or URL must be provided. The caller must have the appropriate metastore permissions.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/credentials.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: validate_credential( [, aws_iam_role: Optional[AwsIamRole], azure_managed_identity: Optional[AzureManagedIdentity], credential_name: Optional[str], databricks_gcp_service_account: Optional[DatabricksGcpServiceAccount], external_location_name: Optional[str], purpose: Optional[CredentialPurpose], read_only: Optional[bool], url: Optional[str]]) -> ValidateCredentialResponse\n\n        Validate a credential.\n\n        Validates a credential.\n\n        For service credentials (purpose is **SERVICE**), either the __credential_name__ or the cloud-specific\n        credential must be provided.\n\n        For storage credentials (purpose is **STORAGE**), at least one of __external_location_name__ and\n        __url__ need to be provided. If only one of them is provided, it will be used for validation. And if\n        both are provided, the __url__ will be used for validation, and __external_location_name__ will be\n        ignored when checking overlapping urls. Either the __credential_name__ or the cloud-specific\n        credential must be provided.\n\n        The caller must be a metastore admin or the credential owner or have the required permission on the\n        metastore and the credential (e.g., **CREATE_EXTERNAL_LOCATION** when purpose is **STORAGE**).\n\n        :param aws_iam_role: :class:`AwsIamRole` (optional)\n          The AWS IAM role configuration\n        :param azure_managed_identity: :class:`AzureManagedIdentity` (optional)\n```\n\n----------------------------------------\n\nTITLE: Initializing WorkspaceClient with Config Profile - Python\nDESCRIPTION: This code snippet shows how to initialize the Databricks WorkspaceClient using a specific profile defined in the `.databrickscfg` file.  The profile argument specifies the name of the profile to use.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/authentication.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(profile='MYPROFILE')\n# Now call the Databricks workspace APIs as desired...\n```\n\n----------------------------------------\n\nTITLE: Editing a SQL Warehouse with Databricks SDK in Python\nDESCRIPTION: This code snippet demonstrates how to edit an existing SQL warehouse using the Databricks SDK for Python. It first creates a SQL warehouse, then uses the warehouses.edit method to modify parameters such as name, cluster size, and auto-stop minutes. It also shows how to delete the warehouse as cleanup. The code relies on the WorkspaceClient and the sql module.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/warehouses.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\ncreated = w.warehouses.create(\n    name=f\"sdk-{time.time_ns()}\",\n    cluster_size=\"2X-Small\",\n    max_num_clusters=1,\n    auto_stop_mins=10,\n    tags=sql.EndpointTags(\n        custom_tags=[sql.EndpointTagPair(key=\"Owner\", value=\"eng-dev-ecosystem-team_at_databricks.com\")]\n    ),\n).result()\n\n_ = w.warehouses.edit(\n    id=created.id,\n    name=f\"sdk-{time.time_ns()}\",\n    cluster_size=\"2X-Small\",\n    max_num_clusters=1,\n    auto_stop_mins=10,\n)\n\n# cleanup\nw.warehouses.delete(id=created.id)\n```\n\n----------------------------------------\n\nTITLE: Resizing a Cluster in Databricks with Python\nDESCRIPTION: This snippet shows how to resize a Databricks cluster using the Databricks SDK for Python. It creates a cluster, resizes it to one worker, and then deletes it. The environment variable TEST_INSTANCE_POOL_ID must be set.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nlatest = w.clusters.select_spark_version(latest=True, long_term_support=True)\n\ncluster_name = f\"sdk-{time.time_ns()}\"\n\nclstr = w.clusters.create(\n    cluster_name=cluster_name,\n    spark_version=latest,\n    instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n    autotermination_minutes=15,\n    num_workers=1,\n).result()\n\nby_id = w.clusters.resize(cluster_id=clstr.cluster_id, num_workers=1).result()\n\n# cleanup\nw.clusters.permanent_delete(cluster_id=clstr.cluster_id)\n```\n\n----------------------------------------\n\nTITLE: Create a Metastore in Python\nDESCRIPTION: This snippet shows how to create a new metastore using the Databricks SDK. It generates a unique name for the metastore using the current timestamp, and it constructs the storage root path using environment variables. After creating the metastore, the code cleans up by deleting it. This operation requires the `WorkspaceClient` from the databricks SDK.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/metastores.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.metastores.create(\n    name=f\"sdk-{time.time_ns()}\",\n    storage_root=\"s3://%s/%s\" % (os.environ[\"TEST_BUCKET\"], f\"sdk-{time.time_ns()}\"),\n)\n\n# cleanup\nw.metastores.delete(id=created.metastore_id, force=True)\n```\n\n----------------------------------------\n\nTITLE: Update Permissions on Securable - Python\nDESCRIPTION: This code snippet demonstrates how to update permissions for a securable object, specifically a table, using the `update` method of the `GrantsAPI`. It grants MODIFY and SELECT privileges to a specified group. It first creates a catalog, schema, and table, then updates the table's permissions. Finally, it cleans up the created resources. Requires the Databricks SDK for Python, the `TEST_DEFAULT_WAREHOUSE_ID` and `TEST_DATA_ENG_GROUP` environment variables.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/grants.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\ntable_name = f\"sdk-{time.time_ns()}\"\n\ncreated_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated_schema = w.schemas.create(name=f\"sdk-{time.time_ns()}\", catalog_name=created_catalog.name)\n\n_ = w.statement_execution.execute(\n    warehouse_id=os.environ[\"TEST_DEFAULT_WAREHOUSE_ID\"],\n    catalog=created_catalog.name,\n    schema=created_schema.name,\n    statement=\"CREATE TABLE %s AS SELECT 2+2 as four\" % (table_name),\n).result()\n\ntable_full_name = \"%s.%s.%s\" % (\n    created_catalog.name,\n    created_schema.name,\n    table_name,\n)\n\naccount_level_group_name = os.environ[\"TEST_DATA_ENG_GROUP\"]\n\ncreated_table = w.tables.get(full_name=table_full_name)\n\nx = w.grants.update(\n    full_name=created_table.full_name,\n    securable_type=catalog.SecurableType.TABLE,\n    changes=[\n        catalog.PermissionsChange(\n            add=[catalog.Privilege.MODIFY, catalog.Privilege.SELECT],\n            principal=account_level_group_name,\n        )\n    ],\n)\n\n# cleanup\nw.schemas.delete(full_name=created_schema.full_name)\nw.catalogs.delete(name=created_catalog.name, force=True)\nw.tables.delete(full_name=table_full_name)\n```\n\n----------------------------------------\n\nTITLE: Create SQL Query with Databricks SDK (Python)\nDESCRIPTION: This snippet demonstrates how to create a new SQL query using the Databricks SDK for Python. It initializes a WorkspaceClient, retrieves data sources to obtain a warehouse ID, and then creates a query with a display name, warehouse ID, description, and query text. After creation, the query is immediately deleted.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/queries.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\nsrcs = w.data_sources.list()\n\nquery = w.queries.create(\n    query=sql.CreateQueryRequestQuery(\n        display_name=f\"sdk-{time.time_ns()}\",\n        warehouse_id=srcs[0].warehouse_id,\n        description=\"test query from Go SDK\",\n        query_text=\"SHOW TABLES\",\n    )\n)\n\n# cleanup\nw.queries.delete(id=query.id)\n```\n\n----------------------------------------\n\nTITLE: Create On-Behalf-Of Token with Databricks SDK in Python\nDESCRIPTION: This snippet demonstrates how to create an on-behalf-of (OBO) token using the Databricks SDK. It first creates a service principal, then creates an OBO token associated with that service principal with a specified lifetime. Finally, it cleans up by deleting the service principal and the token.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/token_management.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import iam\n\nw = WorkspaceClient()\n\ngroups = w.groups.group_display_name_to_id_map(iam.ListGroupsRequest())\n\nspn = w.service_principals.create(\n    display_name=f\"sdk-{time.time_ns()}\",\n    groups=[iam.ComplexValue(value=groups[\"admins\"])],\n)\n\nobo = w.token_management.create_obo_token(application_id=spn.application_id, lifetime_seconds=60)\n\n# cleanup\nw.service_principals.delete(id=spn.id)\nw.token_management.delete(token_id=obo.token_info.token_id)\n```\n\n----------------------------------------\n\nTITLE: Resetting Job Settings with Databricks SDK in Python\nDESCRIPTION: This snippet demonstrates how to reset all settings for an existing job using the Databricks SDK for Python. It creates a job, retrieves its settings, updates the job name, and then uses the `reset` method to apply the new settings.  Finally the example cleans up by deleting the job.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\ncreated_job = w.jobs.create(\n    name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.Task(\n            description=\"test\",\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=\"test\",\n            timeout_seconds=0,\n        )\n    ],\n)\n\nnew_name = f\"sdk-{time.time_ns()}\"\n\nby_id = w.jobs.get(job_id=created_job.job_id)\n\nw.jobs.reset(\n    job_id=by_id.job_id,\n    new_settings=jobs.JobSettings(name=new_name, tasks=by_id.settings.tasks),\n)\n\n# cleanup\nw.jobs.delete(job_id=created_job.job_id)\n```\n\n----------------------------------------\n\nTITLE: Listing Providers with Databricks SDK in Python\nDESCRIPTION: This code snippet shows how to list all available data providers using the Databricks SDK. It initializes a WorkspaceClient and then calls the list method on the providers object to retrieve all providers. It imports the sharing module to explicitly reference ListProvidersRequest.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/providers.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sharing\n\nw = WorkspaceClient()\n\nall = w.providers.list(sharing.ListProvidersRequest())\n```\n\n----------------------------------------\n\nTITLE: Get a Serving Endpoint - Python\nDESCRIPTION: Retrieves the details of a single serving endpoint given its name.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(name: str) -> ServingEndpointDetailed\n\n    Get a single serving endpoint.\n\n    Retrieves the details for a single serving endpoint.\n\n    :param name: str\n      The name of the serving endpoint. This field is required.\n\n    :returns: :class:`ServingEndpointDetailed`\n```\n\n----------------------------------------\n\nTITLE: Listing Secrets in a Secret Scope in Databricks using Python SDK\nDESCRIPTION: This code snippet illustrates how to list the secret keys stored within a specific secret scope in Databricks using the Python SDK. It initializes a WorkspaceClient, creates a secret scope, lists the secrets using `w.secrets.list_secrets()`, and performs cleanup by deleting a dummy secret and the scope itself. The operation is metadata-only; secret data cannot be retrieved.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/secrets.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nkey_name = f\"sdk-{time.time_ns()}\"\n\nscope_name = f\"sdk-{time.time_ns()}\"\n\nw.secrets.create_scope(scope=scope_name)\n\nscrts = w.secrets.list_secrets(scope=scope_name)\n\n# cleanup\nw.secrets.delete_secret(scope=scope_name, key=key_name)\nw.secrets.delete_scope(scope=scope_name)\n```\n\n----------------------------------------\n\nTITLE: Get Current User Info with WorkspaceClient in Python\nDESCRIPTION: This code snippet demonstrates how to get the current user's information using the Databricks SDK for Python. It initializes a WorkspaceClient and calls the me() method on the current_user attribute to retrieve the user details. The expected output is a User object containing information about the current user.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/current_user.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nme = w.current_user.me()\n```\n\n----------------------------------------\n\nTITLE: Running Monitor Refresh in Python\nDESCRIPTION: Queues a metric refresh on the monitor for the specified table, executing in the background.  Requires specific permissions on the table's parent catalog and schema, and must be called from the workspace where the monitor was created.  Takes table_name as input and returns a MonitorRefreshInfo object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/lakehouse_monitors.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: run_refresh(table_name: str) -> MonitorRefreshInfo\n\n        Queue a metric refresh for a monitor.\n        \n        Queues a metric refresh on the monitor for the specified table. The refresh will execute in the\n        background.\n        \n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n        - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent schema - be an\n        owner of the table\n        \n        Additionally, the call must be made from the workspace where the monitor was created.\n        \n        :param table_name: str\n          Full name of the table.\n        \n        :returns: :class:`MonitorRefreshInfo`\n```\n\n----------------------------------------\n\nTITLE: Delete a file (Python)\nDESCRIPTION: Deletes the file specified by the file_path. If the request is successful, there is no response body.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/files/files.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(file_path: str)\n\n    Delete a file.\n\n    Deletes a file. If the request is successful, there is no response body.\n\n    :param file_path: str\n      The absolute path of the file.\n```\n\n----------------------------------------\n\nTITLE: Create Service Principal Federation Policy (Python)\nDESCRIPTION: Creates a service principal federation policy. It requires the service principal ID and optional parameters for the policy itself and a policy identifier. The identifier must contain only lowercase alphanumeric characters, numbers, hyphens, and slashes. Returns the created FederationPolicy object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/service_principal_federation_policy.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(service_principal_id: int [, policy: Optional[FederationPolicy], policy_id: Optional[str]]) -> FederationPolicy\n\n    Create service principal federation policy.\n\n    :param service_principal_id: int\n      The service principal id for the federation policy.\n    :param policy: :class:`FederationPolicy` (optional)\n    :param policy_id: str (optional)\n      The identifier for the federation policy. The identifier must contain only lowercase alphanumeric\n      characters, numbers, hyphens, and slashes. If unspecified, the id will be assigned by Databricks.\n\n    :returns: :class:`FederationPolicy`\n```\n\n----------------------------------------\n\nTITLE: Initializing WorkspaceClient with Credentials\nDESCRIPTION: This Python code snippet initializes a `WorkspaceClient` with explicit host, username, and password credentials. This method is useful when running the SDK outside of a Databricks notebook environment.  Replace placeholders with actual values.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/getting-started.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(\n    host='http://my-databricks-instance.com',\n    username='my-user',\n    password='my-password')\n```\n\n----------------------------------------\n\nTITLE: Submit Job Run Example (Python)\nDESCRIPTION: This code snippet demonstrates how to submit a job run using the Databricks SDK for Python.  It assumes that the notebook path is already available. It depends on the `databricks.sdk` library and requires setting the `DATABRICKS_CLUSTER_ID` environment variable.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\nrun = w.jobs.submit(\n    run_name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.SubmitTask(\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n\n```\n\n----------------------------------------\n\nTITLE: Patching Service Principal Details (Python)\nDESCRIPTION: This snippet shows how to partially update a service principal's details using the `patch` method of the Databricks SDK. It creates a service principal, retrieves it by ID, then patches the `active` status to `false`, and finally deletes the service principal.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/service_principals.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import iam\n\nw = WorkspaceClient()\n\ncreated = w.service_principals.create(display_name=f\"sdk-{time.time_ns()}\")\n\nby_id = w.service_principals.get(id=created.id)\n\nw.service_principals.patch(\n    id=by_id.id,\n    operations=[iam.Patch(op=iam.PatchOp.REPLACE, path=\"active\", value=\"false\")],\n    schemas=[iam.PatchSchema.URN_IETF_PARAMS_SCIM_API_MESSAGES_2_0_PATCH_OP],\n)\n\n# cleanup\nw.service_principals.delete(id=created.id)\n```\n\n----------------------------------------\n\nTITLE: Deleting an App in Databricks using Python\nDESCRIPTION: This code snippet shows how to delete an existing app using the `delete` method.  It requires the name of the app to be deleted as a string. The method returns an `App` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(name: str) -> App\n\n        Delete an app.\n\n        Deletes an app.\n\n        :param name: str\n          The name of the app.\n\n        :returns: :class:`App`\n```\n\n----------------------------------------\n\nTITLE: Set Object Permissions using Permissions API in Python\nDESCRIPTION: This snippet demonstrates how to set permissions on a Databricks object (notebook). It creates a group, retrieves the object status to get its ID, and then uses the `set` method of the `permissions` API to set the access control list, granting the created group `CAN_RUN` permission. Finally, the group is deleted.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/permissions.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import iam\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ngroup = w.groups.create(display_name=f\"sdk-{time.time_ns()}\")\n\nobj = w.workspace.get_status(path=notebook_path)\n\n_ = w.permissions.set(\n    request_object_type=\"notebooks\",\n    request_object_id=\"%d\" % (obj.object_id),\n    access_control_list=[\n        iam.AccessControlRequest(\n            group_name=group.display_name,\n            permission_level=iam.PermissionLevel.CAN_RUN,\n        )\n    ],\n)\n\n# cleanup\nw.groups.delete(id=group.id)\n```\n\n----------------------------------------\n\nTITLE: Getting Personal Compute Setting with ETag - Python\nDESCRIPTION: This method retrieves the current value of the Personal Compute setting. It supports an optional etag parameter, allowing the client to specify a version for optimistic concurrency. The response is guaranteed to be at least as fresh as the provided etag.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/personal_compute.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsettings.personal_compute.get([, etag: Optional[str]]) -> PersonalComputeSetting\n```\n\n----------------------------------------\n\nTITLE: Querying a Serving Endpoint with DataPlaneAPI\nDESCRIPTION: This method allows querying a serving endpoint with various input parameters for different use cases like embeddings, completions, and chat models. The endpoint is identified by its name, and the query can be customized using parameters such as dataframe records, dataframe split, extra parameters, input, inputs, instances, max tokens, messages, n, prompt, stop, stream, and temperature. The response is a QueryEndpointResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints_data_plane.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nw.serving_endpoints_data_plane.query(name: str [, dataframe_records: Optional[List[Any]], dataframe_split: Optional[DataframeSplitInput], extra_params: Optional[Dict[str, str]], input: Optional[Any], inputs: Optional[Any], instances: Optional[List[Any]], max_tokens: Optional[int], messages: Optional[List[ChatMessage]], n: Optional[int], prompt: Optional[Any], stop: Optional[List[str]], stream: Optional[bool], temperature: Optional[float]]) -> QueryEndpointResponse\n```\n\n----------------------------------------\n\nTITLE: Editing Cluster Configuration Python\nDESCRIPTION: This code demonstrates how to update a Databricks cluster configuration, including the Spark version, name, instance pool, autotermination minutes, and number of workers. It creates a cluster, edits its configuration, and then permanently deletes it.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncluster_name = f\"sdk-{time.time_ns()}\"\n\nlstest = w.clusters.select_spark_version(latest=True, long_term_support=True)\n\nclstr = w.clusters.create(\n    cluster_name=cluster_name,\n    spark_version=lstest,\n    instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n    autotermination_minutes=15,\n    num_workers=1,\n).result()\n\n_ = w.clusters.edit(\n    cluster_id=clstr.cluster_id,\n    spark_version=lstest,\n    cluster_name=cluster_name,\n    instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n    autotermination_minutes=10,\n    num_workers=2,\n).result()\n\n# cleanup\nw.clusters.permanent_delete(cluster_id=clstr.cluster_id)\n```\n\n----------------------------------------\n\nTITLE: Delete Service Principal Secret (Python)\nDESCRIPTION: Deletes a specified secret from a service principal. The `service_principal_id` parameter identifies the service principal, and `secret_id` specifies the secret to delete.  No return value is explicitly stated.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/service_principal_secrets.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(service_principal_id: int, secret_id: str)\n\n    Delete service principal secret.\n\n    Delete a secret from the given service principal.\n\n    :param service_principal_id: int\n      The service principal ID.\n    :param secret_id: str\n      The secret ID.\n```\n\n----------------------------------------\n\nTITLE: Create External Location with Storage Credential in Databricks using Python SDK\nDESCRIPTION: This code snippet demonstrates how to create an external location with an associated storage credential using the Databricks SDK for Python. It defines the storage credential using an AWS IAM role and creates the external location using the created credential and S3 bucket. This example showcases the `create` methods of `StorageCredentialsAPI` and `ExternalLocationsAPI`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/external_locations.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\ncredential = w.storage_credentials.create(\n    name=f\"sdk-{time.time_ns()}\",\n    aws_iam_role=catalog.AwsIamRole(role_arn=os.environ[\"TEST_METASTORE_DATA_ACCESS_ARN\"]),\n)\n\ncreated = w.external_locations.create(\n    name=f\"sdk-{time.time_ns()}\",\n    credential_name=credential.name,\n    url=f's3://{os.environ[\"TEST_BUCKET\"]}/sdk-{time.time_ns()}',\n)\n\n# cleanup\nw.storage_credentials.delete(delete=credential.name)\nw.external_locations.delete(delete=created.name)\n```\n\n----------------------------------------\n\nTITLE: Selecting Smallest Node Type with WorkspaceClient\nDESCRIPTION: This code snippet demonstrates how to use the `select_node_type` method from the `WorkspaceClient` to select the smallest available node type based on specified conditions. In this case, it selects a node type that supports local disk.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nsmallest = w.clusters.select_node_type(local_disk=True)\n```\n\n----------------------------------------\n\nTITLE: Get catalog workspace bindings using Databricks SDK\nDESCRIPTION: This snippet demonstrates how to retrieve the workspace bindings of a catalog using the Databricks SDK. It first creates a catalog, then retrieves its bindings using the `get` method, and finally cleans up by deleting the catalog. This requires the Databricks SDK and a WorkspaceClient instance.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/workspace_bindings.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\nbindings = w.workspace_bindings.get(name=created.name)\n\n# cleanup\nw.catalogs.delete(name=created.name, force=True)\n```\n\n----------------------------------------\n\nTITLE: Creating a Group with Groups API in Python\nDESCRIPTION: This snippet demonstrates how to create a new group within a Databricks workspace using the `create` method of the `GroupsAPI` class. It requires the `databricks-sdk` library and uses `WorkspaceClient` to interact with the Databricks API. A unique display name is generated using the current time.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/groups.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ngroup = w.groups.create(display_name=f\"sdk-{time.time_ns()}\")\n\n# cleanup\nw.groups.delete(id=group.id)\n```\n\n----------------------------------------\n\nTITLE: Create a Serving Endpoint - Python\nDESCRIPTION: Creates a new serving endpoint with the given name and optional configurations such as AI Gateway, budget policy, core config, rate limits, route optimization, and tags. It returns a long-running operation waiter for the ServingEndpointDetailed object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(name: str [, ai_gateway: Optional[AiGatewayConfig], budget_policy_id: Optional[str], config: Optional[EndpointCoreConfigInput], rate_limits: Optional[List[RateLimit]], route_optimized: Optional[bool], tags: Optional[List[EndpointTag]]]) -> Wait[ServingEndpointDetailed]\n\n    Create a new serving endpoint.\n\n    :param name: str\n      The name of the serving endpoint. This field is required and must be unique across a Databricks\n      workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores.\n    :param ai_gateway: :class:`AiGatewayConfig` (optional)\n      The AI Gateway configuration for the serving endpoint. NOTE: External model, provisioned throughput,\n      and pay-per-token endpoints are fully supported; agent endpoints currently only support inference\n      tables.\n    :param budget_policy_id: str (optional)\n      The budget policy to be applied to the serving endpoint.\n    :param config: :class:`EndpointCoreConfigInput` (optional)\n      The core config of the serving endpoint.\n    :param rate_limits: List[:class:`RateLimit`] (optional)\n      Rate limits to be applied to the serving endpoint. NOTE: this field is deprecated, please use AI\n      Gateway to manage rate limits.\n    :param route_optimized: bool (optional)\n      Enable route optimization for the serving endpoint.\n    :param tags: List[:class:`EndpointTag`] (optional)\n      Tags to be attached to the serving endpoint and automatically propagated to billing logs.\n\n    :returns:\n      Long-running operation waiter for :class:`ServingEndpointDetailed`.\n      See :method:wait_get_serving_endpoint_not_updating for more details.\n```\n\n----------------------------------------\n\nTITLE: Listing Volumes in Databricks with Python SDK\nDESCRIPTION: This snippet shows how to list volumes within a specified catalog and schema using the Databricks Python SDK. It first creates a catalog and schema, then retrieves a list of volumes within that schema. Finally, it cleans up by deleting the created schema and catalog.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/volumes.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated_schema = w.schemas.create(name=f\"sdk-{time.time_ns()}\", catalog_name=created_catalog.name)\n\nall_volumes = w.volumes.list(catalog_name=created_catalog.name, schema_name=created_schema.name)\n\n# cleanup\nw.schemas.delete(full_name=created_schema.full_name)\nw.catalogs.delete(name=created_catalog.name, force=True)\n```\n\n----------------------------------------\n\nTITLE: Starting an App in Databricks using Python\nDESCRIPTION: This code snippet illustrates how to start the last active deployment of an app using the `start` method. It requires the name of the app. The method returns a long-running operation waiter for the app.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: start(name: str) -> Wait[App]\n\n        Start an app.\n\n        Start the last active deployment of the app in the workspace.\n\n        :param name: str\n          The name of the app.\n\n        :returns:\n          Long-running operation waiter for :class:`App`.\n          See :method:wait_get_app_active for more details.\n```\n\n----------------------------------------\n\nTITLE: Creating Notification Destination in Python\nDESCRIPTION: Creates a new notification destination with the specified configuration and display name. Requires workspace admin permissions. The configuration must wrap exactly one of the nested configs. Returns a NotificationDestination object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/notification_destinations.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create( [, config: Optional[Config], display_name: Optional[str]]) -> NotificationDestination\n\n    Create a notification destination.\n\n    Creates a notification destination. Requires workspace admin permissions.\n\n    :param config: :class:`Config` (optional)\n      The configuration for the notification destination. Must wrap EXACTLY one of the nested configs.\n    :param display_name: str (optional)\n      The display name for the notification destination.\n\n    :returns: :class:`NotificationDestination`\n```\n\n----------------------------------------\n\nTITLE: Updating Storage Credential in Databricks\nDESCRIPTION: This snippet demonstrates how to update an existing storage credential in Databricks using the WorkspaceClient. It begins by creating a storage credential with an AWS IAM role, updates the credential by adding a comment and reaffirms the AWS IAM role and concludes by deleting the updated credential.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/storage_credentials.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\ncreated = w.storage_credentials.create(\n    name=f\"sdk-{time.time_ns()}\",\n    aws_iam_role=catalog.AwsIamRoleRequest(role_arn=os.environ[\"TEST_METASTORE_DATA_ACCESS_ARN\"]),\n)\n\n_ = w.storage_credentials.update(\n    name=created.name,\n    comment=f\"sdk-{time.time_ns()}\",\n    aws_iam_role=catalog.AwsIamRoleRequest(role_arn=os.environ[\"TEST_METASTORE_DATA_ACCESS_ARN\"]),\n)\n\n# cleanup\nw.storage_credentials.delete(name=created.name)\n```\n\n----------------------------------------\n\nTITLE: Creating Lakeview Dashboard\nDESCRIPTION: Creates a new draft Lakeview dashboard using the LakeviewAPI.  This method allows specifying dashboard properties during creation. It returns the created Dashboard object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: create( [, dashboard: Optional[Dashboard]]) -> Dashboard\n\n    Create dashboard.\n\n    Create a draft dashboard.\n\n    :param dashboard: :class:`Dashboard` (optional)\n\n    :returns: :class:`Dashboard`\n```\n\n----------------------------------------\n\nTITLE: Updating a Storage Credential (Python)\nDESCRIPTION: This method updates an existing storage credential in a metastore. The caller must be the owner of the storage credential. Metastore admins can only change the owner of the credential.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/catalog/storage_credentials.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmetastore_id: str, storage_credential_name: str [, credential_info: Optional[UpdateStorageCredential]]\n```\n\n----------------------------------------\n\nTITLE: Updating Personal Compute Setting - Python\nDESCRIPTION: This method updates the value of the Personal Compute setting. It requires the `allow_missing` parameter (set to true), the `setting` object containing the desired state, and a `field_mask` specifying the fields to update.  The field mask ensures only the specified fields are updated, avoiding unintended changes.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/personal_compute.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsettings.personal_compute.update(allow_missing: bool, setting: PersonalComputeSetting, field_mask: str) -> PersonalComputeSetting\n```\n\n----------------------------------------\n\nTITLE: Create Endpoint in Databricks\nDESCRIPTION: Creates a new vector search endpoint within Databricks. Requires the endpoint name and type. Returns a long-running operation waiter for EndpointInfo.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_endpoints.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nw.vector_search_endpoints.create_endpoint(name: str, endpoint_type: EndpointType) -> Wait[EndpointInfo]\n```\n\n----------------------------------------\n\nTITLE: Create Listing in Databricks Marketplace (Python)\nDESCRIPTION: Creates a new listing in the Databricks Marketplace. It takes a Listing object as input and returns a CreateListingResponse object. This operation allows providers to publish their products for consumption in the marketplace.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_listings.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nw.provider_listings.create(listing: Listing) -> CreateListingResponse\n```\n\n----------------------------------------\n\nTITLE: List All Metastores in Python\nDESCRIPTION: This snippet demonstrates how to list all available metastores using the Databricks SDK. It initializes a `WorkspaceClient` and calls the `list()` method on the `metastores` object to retrieve an iterator over all metastores. The WorkspaceClient comes from the databricks.sdk.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/metastores.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nall = w.metastores.list()\n```\n\n----------------------------------------\n\nTITLE: Downloading a notebook or file from Databricks Workspace in Python\nDESCRIPTION: This code snippet demonstrates how to download a file from Databricks workspace using the WorkspaceClient.  It uploads a small python file, then downloads it and asserts that the downloaded content matches the original.  It then deletes the temporary file.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/workspace.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport io\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.workspace import ImportFormat\n\nw = WorkspaceClient()\n\npy_file = f\"/Users/{w.current_user.me().user_name}/file-{time.time_ns()}.py\"\n\nw.workspace.upload(py_file, io.BytesIO(b\"print(1)\"), format=ImportFormat.AUTO)\nwith w.workspace.download(py_file) as f:\n    content = f.read()\n    assert content == b\"print(1)\"\n\nw.workspace.delete(py_file)\n```\n\n----------------------------------------\n\nTITLE: Exporting a workspace object in Python\nDESCRIPTION: This code snippet demonstrates how to export a workspace object (notebook) from a Databricks workspace using the WorkspaceClient. It creates a temporary notebook path and then exports the object using the `export` method. Requires databricks SDK.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/workspace.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import workspace\n\nw = WorkspaceClient()\n\nnotebook = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\nexport_response = w.workspace.export(format=workspace.ExportFormat.SOURCE, path=notebook)\n```\n\n----------------------------------------\n\nTITLE: Updating Job Settings with Databricks Jobs API\nDESCRIPTION: This snippet illustrates how to update the settings of an existing Databricks job. It first creates a job with a notebook task, then updates the job settings using the `update` method, changing the name and maximum concurrent runs.  Finally cleans up the job after the update.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\nnew_name = f\"sdk-{time.time_ns()}\"\n\ncreated_job = w.jobs.create(\n    name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.Task(\n            description=\"test\",\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=\"test\",\n            timeout_seconds=0,\n        )\n    ],\n)\n\nw.jobs.update(\n    job_id=created_job.job_id,\n    new_settings=jobs.JobSettings(name=new_name, max_concurrent_runs=5),\n)\n\n# cleanup\nw.jobs.delete(job_id=created_job.job_id)\n```\n\n----------------------------------------\n\nTITLE: Upload a file (Python)\nDESCRIPTION: Uploads a file with a maximum size of 5 GiB to the specified file_path. The file contents are sent as raw bytes in the request body. An optional overwrite parameter allows overwriting existing files.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/files/files.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: upload(file_path: str, contents: BinaryIO [, overwrite: Optional[bool]])\n\n    Upload a file.\n\n    Uploads a file of up to 5 GiB. The file contents should be sent as the request body as raw bytes (an\n    octet stream); do not encode or otherwise modify the bytes before sending. The contents of the\n    resulting file will be exactly the bytes sent in the request body. If the request is successful, there\n    is no response body.\n\n    :param file_path: str\n      The absolute path of the file.\n    :param contents: BinaryIO\n    :param overwrite: bool (optional)\n      If true, an existing file will be overwritten.\n```\n\n----------------------------------------\n\nTITLE: Add Instance Profile using Databricks SDK in Python\nDESCRIPTION: Registers a new instance profile with Databricks, allowing it to be selected when launching clusters. This method is available to admin users. It demonstrates how to use the WorkspaceClient and the instance_profiles.add method with parameters for instance profile ARN, skipping validation, and specifying the IAM role ARN.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/instance_profiles.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\narn = \"arn:aws:iam::000000000000:instance-profile/abc\"\n\nw.instance_profiles.add(\n    instance_profile_arn=arn,\n    skip_validation=True,\n    iam_role_arn=\"arn:aws:iam::000000000000:role/bcd\",\n)\n```\n\n----------------------------------------\n\nTITLE: List Tokens Example (Python)\nDESCRIPTION: This code snippet shows how to list all valid tokens for a user-workspace pair using the Databricks SDK.  Requires the Databricks SDK.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/tokens.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nall = w.tokens.list()\n```\n\n----------------------------------------\n\nTITLE: Listing Instance Pools in Databricks with Python SDK\nDESCRIPTION: This code snippet demonstrates how to list instance pools in Databricks using the Databricks SDK for Python. It initializes a WorkspaceClient and then iterates through the list of instance pools obtained using the `list()` method. No specific dependencies beyond the Databricks SDK are required. The output is an iterator over `InstancePoolAndStats` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/instance_pools.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nall = w.instance_pools.list()\n```\n\n----------------------------------------\n\nTITLE: Getting a Recipient in Python\nDESCRIPTION: This code snippet showcases how to retrieve a recipient's information using the `get` method after creating it with the `create` method. It uses `WorkspaceClient` from the `databricks.sdk` to interact with the Databricks workspace. The recipient's name is dynamically generated to ensure uniqueness. It imports the `time` module to generate a unique name.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/recipients.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.recipients.create(name=f\"sdk-{time.time_ns()}\")\n\n_ = w.recipients.get(name=created.name)\n\n# cleanup\nw.recipients.delete(name=created.name)\n```\n\n----------------------------------------\n\nTITLE: Listing Exchange Filters using ProviderExchangeFiltersAPI in Python\nDESCRIPTION: Lists existing exchange filters associated with a specific exchange using the ``list`` method. It takes the ``exchange_id`` as a string, and optionally ``page_size`` (int) and ``page_token`` (str) for pagination. Returns an iterator over ``ExchangeFilter`` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_exchange_filters.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nw.provider_exchange_filters.list(exchange_id: str [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[ExchangeFilter]\n```\n\n----------------------------------------\n\nTITLE: Transfer Object Ownership - Python\nDESCRIPTION: Transfers ownership of a dashboard, query, or alert to a new owner. The object_type specifies the type of the object, and object_id is the ID of the object to transfer. The new_owner parameter takes the email address of the new owner, who must already exist in the workspace. This method requires an admin API key.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/dbsql_permissions.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: transfer_ownership(object_type: OwnableObjectType, object_id: TransferOwnershipObjectId [, new_owner: Optional[str]]) -> Success\n\n    Transfer object ownership.\n\n    Transfers ownership of a dashboard, query, or alert to an active user. Requires an admin API key.\n\n    **Note**: A new version of the Databricks SQL API is now available. For queries and alerts, please use\n    :method:queries/update and :method:alerts/update respectively instead. [Learn more]\n\n    [Learn more]: https://docs.databricks.com/en/sql/dbsql-api-latest.html\n\n    :param object_type: :class:`OwnableObjectType`\n      The type of object on which to change ownership.\n    :param object_id: :class:`TransferOwnershipObjectId`\n      The ID of the object on which to change ownership.\n    :param new_owner: str (optional)\n      Email address for the new owner, who must exist in the workspace.\n\n    :returns: :class:`Success`\n```\n\n----------------------------------------\n\nTITLE: Patch User with AccountClient in Python\nDESCRIPTION: This code snippet demonstrates how to partially update a user's details in a Databricks account using the AccountClient and the `patch` method. It adds the `account_admin` role to the user. It requires `databricks.sdk` and `databricks.sdk.service.iam`. The code also handles user creation and deletion for a complete example.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/users.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import iam\n\na = AccountClient()\n\nuser = a.users.create(\n    display_name=f\"sdk-{time.time_ns()}\",\n    user_name=f\"sdk-{time.time_ns()}@example.com\",\n)\n\na.users.patch(\n    id=user.id,\n    schemas=[iam.PatchSchema.URN_IETF_PARAMS_SCIM_API_MESSAGES_2_0_PATCH_OP],\n    operations=[\n        iam.Patch(\n            op=iam.PatchOp.ADD,\n            value=iam.User(roles=[iam.ComplexValue(value=\"account_admin\")]),\n        )\n    ],\n)\n\n# cleanup\na.users.delete(id=user.id)\n```\n\n----------------------------------------\n\nTITLE: Submitting a One-Time Run with Databricks Jobs API\nDESCRIPTION: This snippet demonstrates how to submit a one-time run using the Databricks Jobs API. It creates a task with a notebook task, specifies an existing cluster, and submits the job. The result is a long-running operation waiter for the Run. Finally cleans up the run after execution.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\nrun = w.jobs.submit(\n    run_name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.SubmitTask(\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=f\"sdk-{time.time_ns()}\",\n        )\n    ],\n).result()\n\n# cleanup\nw.jobs.delete_run(run_id=run.run_id)\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline with Databricks SDK in Python\nDESCRIPTION: This snippet demonstrates how to create a new Delta Live Tables pipeline using the Databricks SDK for Python. It defines the pipeline configuration, including the notebook library, cluster settings, and pipeline name, and then uses the `WorkspaceClient` to create the pipeline. Finally, it shows how to clean up by deleting the created pipeline.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/pipelines/pipelines.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import pipelines\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncreated = w.pipelines.create(\n    continuous=False,\n    name=f\"sdk-{time.time_ns()}\",\n    libraries=[pipelines.PipelineLibrary(notebook=pipelines.NotebookLibrary(path=notebook_path))],\n    clusters=[\n        pipelines.PipelineCluster(\n            instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n            label=\"default\",\n            num_workers=1,\n            custom_tags={\n                \"cluster_type\": \"default\",\n            },\n        )\n    ],\n)\n\n# cleanup\nw.pipelines.delete(pipeline_id=created.pipeline_id)\n```\n\n----------------------------------------\n\nTITLE: Updating an App in Databricks using Python\nDESCRIPTION: This code snippet demonstrates how to update an existing app using the `update` method.  It requires the app name and an optional `App` object containing the updated information. The method returns the updated `App` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(name: str [, app: Optional[App]]) -> App\n\n        Update an app.\n\n        Updates the app with the supplied name.\n\n        :param name: str\n          The name of the app. The name must contain only lowercase alphanumeric characters and hyphens. It\n          must be unique within the workspace.\n        :param app: :class:`App` (optional)\n\n        :returns: :class:`App`\n```\n\n----------------------------------------\n\nTITLE: Creating Usage Dashboard with Databricks SDK (Python)\nDESCRIPTION: This code snippet demonstrates the 'create' method of the UsageDashboardsAPI. It allows you to create a new usage dashboard, specifying optional parameters like 'dashboard_type' (either Workspace or Global) and 'workspace_id'. The method returns a CreateBillingUsageDashboardResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/usage_dashboards.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\na.usage_dashboards.create(dashboard_type: Optional[UsageDashboardType], workspace_id: Optional[int]) -> CreateBillingUsageDashboardResponse\n```\n\n----------------------------------------\n\nTITLE: Editing an Instance Pool with Databricks SDK\nDESCRIPTION: This code snippet shows how to edit an existing instance pool. It first creates an instance pool, then modifies its name and node type. After the edit operation, the instance pool is deleted. This example highlights the edit functionality of the Databricks Instance Pools API.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/instance_pools.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nsmallest = w.clusters.select_node_type(local_disk=True)\n\ncreated = w.instance_pools.create(instance_pool_name=f\"sdk-{time.time_ns()}\", node_type_id=smallest)\n\nw.instance_pools.edit(\n    instance_pool_id=created.instance_pool_id,\n    instance_pool_name=f\"sdk-{time.time_ns()}\",\n    node_type_id=smallest,\n)\n\n# cleanup\nw.instance_pools.delete(instance_pool_id=created.instance_pool_id)\n```\n\n----------------------------------------\n\nTITLE: Configuring AAD Service Principal (Terraform)\nDESCRIPTION: This Terraform configuration creates an Azure AD Service Principal with a secret. It defines an application, a service principal, creates a service principal password, and outputs the client ID and client secret.\nDependencies: AzureAD and Time providers\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_7\n\nLANGUAGE: hcl\nCODE:\n```\nvariable \"name\" {\n  type = string\n}\n\nresource \"azuread_application\" \"this\" {\n  display_name = var.name\n}\n\nresource \"azuread_service_principal\" \"this\" {\n  application_id = azuread_application.this.application_id\n}\n\nresource \"time_rotating\" \"month\" {\n  rotation_days = 30\n}\n\nresource \"azuread_service_principal_password\" \"this\" {\n  service_principal_id = azuread_service_principal.this.object_id\n  rotate_when_changed = {\n    rotation = time_rotating.month.id\n  }\n}\n\noutput \"client_id\" {\n  description = \"value for ARM_CLIENT_ID environment variable\"\n  value       = azuread_application.this.application_id\n}\n\noutput \"client_secret\" {\n  description = \"value for ARM_CLIENT_SECRET environment variable\"\n  value       = azuread_service_principal_password.this.value\n  sensitive   = true\n}\n```\n\n----------------------------------------\n\nTITLE: Updating an Installation using ConsumerInstallationsAPI in Python\nDESCRIPTION: Updates an existing installation. The `update` method takes the listing ID, installation ID, and an InstallationDetail object as parameters. An optional rotate_token parameter is also accepted to rotate the token. It returns an UpdateInstallationResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_installations.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nw.consumer_installations.update(listing_id: str, installation_id: str, installation: InstallationDetail [, rotate_token: Optional[bool]]) -> UpdateInstallationResponse\n```\n\n----------------------------------------\n\nTITLE: Deleting a Vector Search Index\nDESCRIPTION: This method deletes a vector search index. It requires the index name as input.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_indexes.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete_index(index_name: str)\n\n        Delete an index.\n\n        Delete an index.\n\n        :param index_name: str\n          Name of the index\n```\n\n----------------------------------------\n\nTITLE: Listing Databricks Clusters using Public Client OAuth (Python)\nDESCRIPTION: This code snippet demonstrates how to authenticate with Azure Databricks using the Public Client OAuth flow. It initializes a `WorkspaceClient` with the Databricks host, client ID (obtained from the Terraform output), and sets `auth_type` to 'external-browser', triggering a browser-based login flow.\nDependencies: databricks-sdk\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(host='https://adb-30....azuredatabricks.net',\n                    client_id='>>>value from public_client_id output<<<<',\n                    auth_type='external-browser')\nclusters = w.clusters.list()\nfor cl in clusters:\n  print(f' - {cl.cluster_name} is {cl.state}')\n```\n\n----------------------------------------\n\nTITLE: Azure Client Secret Authentication in Python\nDESCRIPTION: This snippet shows how to authenticate with Azure Databricks using a client secret. It requires the `databricks.sdk` package and prompts the user for the Databricks Workspace URL, Azure Resource ID, AAD Tenant ID, AAD Client ID, and AAD Client Secret. The `WorkspaceClient` is initialized with these Azure-specific credentials.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(host=input('Databricks Workspace URL: '),\n                    azure_workspace_resource_id=input('Azure Resource ID: '),\n                    azure_tenant_id=input('AAD Tenant ID: '),\n                    azure_client_id=input('AAD Client ID: '),\n                    azure_client_secret=input('AAD Client Secret: '))\n```\n\n----------------------------------------\n\nTITLE: Updating Configuration and Waiting for Databricks Endpoint\nDESCRIPTION: This method updates the configuration of a serving endpoint and waits for the operation to complete. It accepts parameters for auto-capture configuration, served entities, served models, and traffic configuration, and a timeout duration.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: update_config_and_wait(name: str [, auto_capture_config: Optional[AutoCaptureConfigInput], served_entities: Optional[List[ServedEntityInput]], served_models: Optional[List[ServedModelInput]], traffic_config: Optional[TrafficConfig], timeout: datetime.timedelta = 0:20:00]) -> ServingEndpointDetailed\n\n\n```\n\n----------------------------------------\n\nTITLE: Updating an App in Databricks\nDESCRIPTION: Updates the app with the supplied name. The app name must be unique and contain only lowercase alphanumeric characters and hyphens. Returns the updated App object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nw.apps.update(name: str [, description: Optional[str]]) -> App\n```\n\n----------------------------------------\n\nTITLE: Creating Online Table\nDESCRIPTION: This method allows the creation of a new online table.  It takes an optional OnlineTable object as input, which contains the table configuration. The method returns a long-running operation waiter for the created OnlineTable.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/online_tables.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create( [, table: Optional[OnlineTable]]) -> Wait[OnlineTable]\n\n    Create an Online Table.\n\n    Create a new Online Table.\n\n    :param table: :class:`OnlineTable` (optional)\n      Online Table information.\n\n    :returns:\n      Long-running operation waiter for :class:`OnlineTable`.\n      See :method:wait_get_online_table_active for more details.\n```\n\n----------------------------------------\n\nTITLE: Configuring WorkspaceClient with Debug Headers in Python\nDESCRIPTION: This code snippet demonstrates how to initialize the Databricks WorkspaceClient with debug_headers enabled.  Setting debug_headers to True will output the HTTP headers in debug logs, which can be useful for troubleshooting. However, it's important to be aware that headers may contain sensitive data such as access tokens.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/authentication.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(debug_headers=True)\n# Now call the Databricks workspace APIs as desired...\n```\n\n----------------------------------------\n\nTITLE: Get Build Logs for Served Model - Python\nDESCRIPTION: Retrieves the build logs associated with a specific served model within a serving endpoint. It requires the endpoint name and served model name as input parameters.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: build_logs(name: str, served_model_name: str) -> BuildLogsResponse\n\n    Get build logs for a served model.\n\n    Retrieves the build logs associated with the provided served model.\n\n    :param name: str\n      The name of the serving endpoint that the served model belongs to. This field is required.\n    :param served_model_name: str\n      The name of the served model that build logs will be retrieved for. This field is required.\n\n    :returns: :class:`BuildLogsResponse`\n```\n\n----------------------------------------\n\nTITLE: SSO for Local Scripts with External Browser Authentication\nDESCRIPTION: This Python code demonstrates how to use the `auth_type='external-browser'` utility in the Databricks SDK for Python to authenticate local scripts using SSO.  It prompts the user for the Databricks host, initializes a `WorkspaceClient` with the specified authentication type, and lists the clusters in the workspace. It requires the Databricks SDK library.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nhost = input('Enter Databricks host: ')\nw = WorkspaceClient(host=host, auth_type='external-browser')\nclusters = w.clusters.list()\nfor cl in clusters:\n    print(f' - {cl.cluster_name} is {cl.state}')\n```\n\n----------------------------------------\n\nTITLE: List Metastores - Python\nDESCRIPTION: Retrieves all Unity Catalog metastores associated with an account. Returns an iterator over `MetastoreInfo` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/catalog/metastores.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list() -> Iterator[MetastoreInfo]\n\n    Get all metastores associated with an account.\n\n    Gets all Unity Catalog metastores associated with an account specified by ID.\n\n    :returns: Iterator over :class:`MetastoreInfo`\n```\n\n----------------------------------------\n\nTITLE: Listing Alerts with Databricks SDK\nDESCRIPTION: This snippet demonstrates how to list Databricks SQL alerts using the Databricks SDK for Python. It retrieves all alerts accessible to the user. It showcases basic usage of the list method.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/alerts.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\nall = w.alerts.list(sql.ListAlertsRequest())\n```\n\n----------------------------------------\n\nTITLE: Updating Git Credentials using Databricks SDK\nDESCRIPTION: This code snippet illustrates how to update existing Git credentials using the Databricks SDK. It creates credentials, updates their username and personal access token using the `update` method, and then cleans up by deleting the credentials. It uses time-based values to ensure uniqueness for demonstration.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/git_credentials.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncr = w.git_credentials.create(git_provider=\"gitHub\", git_username=\"test\", personal_access_token=\"test\")\n\nw.git_credentials.update(\n    credential_id=cr.credential_id,\n    git_provider=\"gitHub\",\n    git_username=f\"sdk-{time.time_ns()}@example.com\",\n    personal_access_token=f\"sdk-{time.time_ns()}\",\n)\n\n# cleanup\nw.git_credentials.delete(credential_id=cr.credential_id)\n```\n\n----------------------------------------\n\nTITLE: SQL Statement with Parameters\nDESCRIPTION: This example demonstrates how to pass parameters to a SQL statement using the Databricks SQL Statement Execution API. It includes the SQL statement with parameter markers and the corresponding JSON structure for passing the parameter values and types.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/statement_execution.rst#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ ..., \"statement\": \"SELECT * FROM my_table WHERE name = :my_name AND date = :my_date\",\n\"parameters\": [ { \"name\": \"my_name\", \"value\": \"the name\" }, { \"name\": \"my_date\", \"value\":\n\"2020-01-01\", \"type\": \"DATE\" } ] }\n```\n\n----------------------------------------\n\nTITLE: Create Token Example (Python)\nDESCRIPTION: This code snippet demonstrates how to create a personal access token using the Databricks SDK. It sets a comment and lifetime for the token, and then cleans up by deleting it. Requires the Databricks SDK.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/tokens.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ntoken = w.tokens.create(comment=f\"sdk-{time.time_ns()}\", lifetime_seconds=300)\n\n# cleanup\nw.tokens.delete(token_id=token.token_info.token_id)\n```\n\n----------------------------------------\n\nTITLE: Deploying an App in Databricks using Python\nDESCRIPTION: This code snippet explains how to create a new app deployment using the `deploy` method.  It takes the app name and an optional `AppDeployment` object as parameters.  The method returns a long-running operation waiter for the deployment.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: deploy(app_name: str [, app_deployment: Optional[AppDeployment]]) -> Wait[AppDeployment]\n\n        Create an app deployment.\n\n        Creates an app deployment for the app with the supplied name.\n\n        :param app_name: str\n          The name of the app.\n        :param app_deployment: :class:`AppDeployment` (optional)\n\n        :returns:\n          Long-running operation waiter for :class:`AppDeployment`.\n          See :method:wait_get_deployment_app_succeeded for more details.\n```\n\n----------------------------------------\n\nTITLE: Delete Share Recipient Databricks SDK (Python)\nDESCRIPTION: This snippet demonstrates deleting a share recipient using the Databricks SDK for Python. It calls the `w.recipients.delete()` method, passing the name of the recipient to be deleted. This is typically used for cleanup after the recipient is no longer needed. `w` likely represents a client object for interacting with Databricks.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/recipients.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# cleanup\nw.recipients.delete(name=created.name)\n```\n\n----------------------------------------\n\nTITLE: Getting a Catalog in Databricks with Python SDK\nDESCRIPTION: This snippet shows how to retrieve a specific catalog by name using the Databricks Python SDK. It first creates a catalog, then retrieves it using `w.catalogs.get()`, and finally deletes the catalog. This tests both the creation and retrieval functionality.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/catalogs.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\n_ = w.catalogs.get(name=created.name)\n\n# cleanup\nw.catalogs.delete(name=created.name, force=True)\n```\n\n----------------------------------------\n\nTITLE: Getting Group Details in Databricks (Python)\nDESCRIPTION: Retrieves information for a specific group in the Databricks account. The id parameter is the unique ID for a group in the Databricks account. Returns a Group object containing the details.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/groups.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(id: str) -> Group\n\n        Get group details.\n\n        Gets the information for a specific group in the Databricks account.\n\n        :param id: str\n          Unique ID for a group in the Databricks account.\n\n        :returns: :class:`Group`\n```\n\n----------------------------------------\n\nTITLE: Upserting Data into a Direct Vector Access Index\nDESCRIPTION: This method upserts data into a specified Direct Vector Access Index. It requires the index name and a JSON string representing the data to be upserted.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_indexes.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: upsert_data_vector_index(index_name: str, inputs_json: str) -> UpsertDataVectorIndexResponse\n\n        Upsert data into an index.\n\n        Handles the upserting of data into a specified vector index.\n\n        :param index_name: str\n          Name of the vector index where data is to be upserted. Must be a Direct Vector Access Index.\n        :param inputs_json: str\n          JSON string representing the data to be upserted.\n\n        :returns: :class:`UpsertDataVectorIndexResponse`\n```\n\n----------------------------------------\n\nTITLE: Adding Serving Endpoint Permissions Methods - Python\nDESCRIPTION: This snippet introduces methods for managing serving endpoint permissions within the Databricks workspace via the `w.serving_endpoints` service. The added methods are: `get_serving_endpoint_permission_levels()`, `get_serving_endpoint_permissions()`, `set_serving_endpoint_permissions()`, and `update_serving_endpoint_permissions()`. These methods facilitate the management of access control for serving endpoints.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nw.serving_endpoints.get_serving_endpoint_permission_levels()\nw.serving_endpoints.get_serving_endpoint_permissions()\nw.serving_endpoints.set_serving_endpoint_permissions()\nw.serving_endpoints.update_serving_endpoint_permissions()\n```\n\n----------------------------------------\n\nTITLE: Updating a Function in Databricks\nDESCRIPTION: Updates a function's owner. Only the owner can be updated. If the user is not a metastore admin, they must be a member of the new owner's group. Requires appropriate privileges on the function, schema, and catalog.  name is the fully-qualified function name, and owner is the username of the new owner.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/functions.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(name: str [, owner: Optional[str]]) -> FunctionInfo\n\n    Update a function.\n\n    Updates the function that matches the supplied name. Only the owner of the function can be updated. If\n    the user is not a metastore admin, the user must be a member of the group that is the new function\n    owner. - Is a metastore admin - Is the owner of the function's parent catalog - Is the owner of the\n    function's parent schema and has the **USE_CATALOG** privilege on its parent catalog - Is the owner of\n    the function itself and has the **USE_CATALOG** privilege on its parent catalog as well as the\n    **USE_SCHEMA** privilege on the function's parent schema.\n\n    :param name: str\n      The fully-qualified name of the function (of the form\n      __catalog_name__.__schema_name__.__function__name__).\n    :param owner: str (optional)\n      Username of current owner of function.\n\n    :returns: :class:`FunctionInfo`\n```\n\n----------------------------------------\n\nTITLE: Selecting Latest Spark Version with WorkspaceClient\nDESCRIPTION: This code snippet shows how to use the `select_spark_version` method from the `WorkspaceClient` to select the latest Databricks Runtime version with long term support. The snippet initializes a `WorkspaceClient` and calls the `select_spark_version` with the specified parameters.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nlatest = w.clusters.select_spark_version(latest=True, long_term_support=True)\n```\n\n----------------------------------------\n\nTITLE: Querying a Vector Search Index\nDESCRIPTION: This method queries a specified vector search index. It requires the index name and a list of column names to include in the response. It also accepts optional parameters for reranking, filtering, number of results, query text, query type, query vector, and score threshold.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_indexes.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: query_index(index_name: str, columns: List[str] [, columns_to_rerank: Optional[List[str]], filters_json: Optional[str], num_results: Optional[int], query_text: Optional[str], query_type: Optional[str], query_vector: Optional[List[float]], score_threshold: Optional[float]]) -> QueryVectorIndexResponse\n\n        Query an index.\n\n        Query the specified vector index.\n\n        :param index_name: str\n          Name of the vector index to query.\n        :param columns: List[str]\n          List of column names to include in the response.\n        :param columns_to_rerank: List[str] (optional)\n          Column names used to retrieve data to send to the reranker.\n        :param filters_json: str (optional)\n          JSON string representing query filters.\n\n          Example filters: - `{\"id <\": 5}`: Filter for id less than 5. - `{\"id >\": 5}`: Filter for id greater\n          than 5. - `{\"id <=\": 5}`: Filter for id less than equal to 5. - `{\"id >=\": 5}`: Filter for id\n          greater than equal to 5. - `{\"id\": 5}`: Filter for id equal to 5.\n        :param num_results: int (optional)\n          Number of results to return. Defaults to 10.\n        :param query_text: str (optional)\n          Query text. Required for Delta Sync Index using model endpoint.\n        :param query_type: str (optional)\n          The query type to use. Choices are `ANN` and `HYBRID`. Defaults to `ANN`.\n        :param query_vector: List[float] (optional)\n          Query vector. Required for Direct Vector Access Index and Delta Sync Index using self-managed\n          vectors.\n        :param score_threshold: float (optional)\n          Threshold for the approximate nearest neighbor search. Defaults to 0.0.\n\n        :returns: :class:`QueryVectorIndexResponse`\n```\n\n----------------------------------------\n\nTITLE: Check Access Policy Method - Python\nDESCRIPTION: This method checks the access policy for a given actor, permission, and resource. It takes parameters such as actor, permission, resource, consistency token, authorization identity, and optional resource info. The method returns a CheckPolicyResponse indicating whether the actor has the specified permission on the resource.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/access_control.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: check_policy(actor: Actor, permission: str, resource: str, consistency_token: ConsistencyToken, authz_identity: RequestAuthzIdentity [, resource_info: Optional[ResourceInfo]]) -> CheckPolicyResponse\n\n        Check access policy to a resource.\n\n        :param actor: :class:`Actor`\n        :param permission: str\n        :param resource: str\n          Ex: (servicePrincipal/use, accounts/<account-id>/servicePrincipals/<sp-id>) Ex:\n          (servicePrincipal.ruleSet/update, accounts/<account-id>/servicePrincipals/<sp-id>/ruleSets/default)\n        :param consistency_token: :class:`ConsistencyToken`\n        :param authz_identity: :class:`RequestAuthzIdentity`\n        :param resource_info: :class:`ResourceInfo` (optional)\n\n        :returns: :class:`CheckPolicyResponse`\n```\n\n----------------------------------------\n\nTITLE: List Listings - Python\nDESCRIPTION: Lists all published listings in the Databricks Marketplace that the consumer has access to. Supports filtering based on various parameters such as asset types, categories, free listings, private exchange listings, staff picks, provider IDs and tags. Returns an iterator over `Listing` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_listings.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list( [, assets: Optional[List[AssetType]], categories: Optional[List[Category]], is_free: Optional[bool], is_private_exchange: Optional[bool], is_staff_pick: Optional[bool], page_size: Optional[int], page_token: Optional[str], provider_ids: Optional[List[str]], tags: Optional[List[ListingTag]]]) -> Iterator[Listing]\n\n    List listings.\n\n    List all published listings in the Databricks Marketplace that the consumer has access to.\n\n    :param assets: List[:class:`AssetType`] (optional)\n      Matches any of the following asset types\n    :param categories: List[:class:`Category`] (optional)\n      Matches any of the following categories\n    :param is_free: bool (optional)\n      Filters each listing based on if it is free.\n    :param is_private_exchange: bool (optional)\n      Filters each listing based on if it is a private exchange.\n    :param is_staff_pick: bool (optional)\n      Filters each listing based on whether it is a staff pick.\n    :param page_size: int (optional)\n    :param page_token: str (optional)\n    :param provider_ids: List[str] (optional)\n      Matches any of the following provider ids\n    :param tags: List[:class:`ListingTag`] (optional)\n      Matches any of the following tags\n\n    :returns: Iterator over :class:`Listing`\n```\n\n----------------------------------------\n\nTITLE: Updating Service Principal Details\nDESCRIPTION: Demonstrates how to update a service principal's details using the `update` method. The example creates a new service principal, retrieves it, updates it using `a.service_principals.update()`, and then cleans up by deleting the service principal.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/service_principals.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nsp_create = a.service_principals.create(active=True, display_name=f\"sdk-{time.time_ns()}\")\n\nsp = a.service_principals.get(id=sp_create.id)\n\na.service_principals.update(active=True, display_name=sp.display_name, id=sp.id)\n\n# cleanup\na.service_principals.delete(id=sp_create.id)\n```\n\n----------------------------------------\n\nTITLE: Creating Service Principal with AccountClient\nDESCRIPTION: This snippet demonstrates how to create a service principal using the AccountClient from the databricks-sdk. It imports necessary modules, initializes the AccountClient, and creates a service principal with a unique display name based on the current timestamp. The AccountClient needs to be configured with the appropriate credentials to interact with the Databricks account.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/service_principals.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nspn = a.service_principals.create(display_name=f\"sdk-{time.time_ns()}\")\n```\n\n----------------------------------------\n\nTITLE: Getting a Provider with Databricks SDK in Python\nDESCRIPTION: This snippet demonstrates how to retrieve a specific data provider using the Databricks SDK. It begins by creating a provider similar to the previous example. After the provider is created, the code retrieves the provider using the get method and then deletes it for cleanup. It shows how to retrieve provider details by name.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/providers.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\npublic_share_recipient = \"\"\"{\n        \"shareCredentialsVersion\":1,\n        \"bearerToken\":\"dapiabcdefghijklmonpqrstuvwxyz\",\n        \"endpoint\":\"https://sharing.delta.io/delta-sharing/\"\n    }\n\"\"\"\n\ncreated = w.providers.create(name=f\"sdk-{time.time_ns()}\", recipient_profile_str=public_share_recipient)\n\n_ = w.providers.get(name=created.name)\n\n# cleanup\nw.providers.delete(name=created.name)\n```\n\n----------------------------------------\n\nTITLE: Get OpenAPI Schema for Serving Endpoint - Python\nDESCRIPTION: Retrieves the query schema of a serving endpoint in OpenAPI format, providing information about supported paths, input/output formats, and datatypes. It requires the name of the serving endpoint.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_open_api(name: str) -> GetOpenApiResponse\n\n    Get the schema for a serving endpoint.\n\n    Get the query schema of the serving endpoint in OpenAPI format. The schema contains information for\n    the supported paths, input and output format and datatypes.\n\n    :param name: str\n      The name of the serving endpoint that the served model belongs to. This field is required.\n\n    :returns: :class:`GetOpenApiResponse`\n```\n\n----------------------------------------\n\nTITLE: Listing Workspaces with AccountClient\nDESCRIPTION: This Python code snippet demonstrates how to list all workspaces in a Databricks account using the `AccountClient`. It initializes the client, iterates through the workspaces, and prints the workspace name and creation time. It requires the `databricks.sdk` package.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/getting-started.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Authenticate as described above\nfrom databricks.sdk import AccountClient\na = AccountClient()\nfor workspace in a.workspaces.list():\n    print(f'workspace {workspace.workspace_name} was created at {workspace.creation_time}')\n```\n\n----------------------------------------\n\nTITLE: List Models Usage Example (Python)\nDESCRIPTION: This example demonstrates how to list all available models using the Databricks SDK. It initializes a `WorkspaceClient` and then uses the `list_models` method to retrieve a list of models. The example requires the `databricks-sdk` library and utilizes the `ml.ListModelsRequest` class.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/model_registry.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import ml\n\nw = WorkspaceClient()\n\nall = w.model_registry.list_models(ml.ListModelsRequest())\n```\n\n----------------------------------------\n\nTITLE: Create Command Execution Context in Databricks\nDESCRIPTION: This snippet demonstrates how to create a command execution context on a Databricks cluster using the Databricks SDK. It initializes a WorkspaceClient, retrieves a cluster ID from an environment variable, creates a context, and then cleans up by deleting the context. The language is set to Python.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/command_execution.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import compute\n\nw = WorkspaceClient()\n\ncluster_id = os.environ[\"TEST_DEFAULT_CLUSTER_ID\"]\n\ncontext = w.command_execution.create(cluster_id=cluster_id, language=compute.Language.PYTHON).result()\n\n# cleanup\nw.command_execution.destroy(cluster_id=cluster_id, context_id=context.id)\n```\n\n----------------------------------------\n\nTITLE: Getting a Registered Model with databricks-sdk-py\nDESCRIPTION: Retrieves a registered model. The caller must be a metastore admin or an owner of (or have the EXECUTE privilege on) the registered model, and must also have the USE_CATALOG privilege on the parent catalog and the USE_SCHEMA privilege on the parent schema.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/registered_models.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(full_name: str [, include_aliases: Optional[bool], include_browse: Optional[bool]]) -> RegisteredModelInfo\n\n    Get a Registered Model.\n\n    Get a registered model.\n\n    The caller must be a metastore admin or an owner of (or have the **EXECUTE** privilege on) the\n    registered model. For the latter case, the caller must also be the owner or have the **USE_CATALOG**\n    privilege on the parent catalog and the **USE_SCHEMA** privilege on the parent schema.\n\n    :param full_name: str\n      The three-level (fully qualified) name of the registered model\n    :param include_aliases: bool (optional)\n      Whether to include registered model aliases in the response\n    :param include_browse: bool (optional)\n      Whether to include registered models in the response for which the principal can only access\n      selective metadata for\n\n    :returns: :class:`RegisteredModelInfo`\n```\n\n----------------------------------------\n\nTITLE: Creating a Group in Databricks (Python)\nDESCRIPTION: Creates a new group in the Databricks account with a unique name, using the supplied group details. The display_name parameter is a human-readable group name, and entitlements are assigned to the group. See the Databricks documentation for a list of supported entitlement values.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/groups.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create( [, display_name: Optional[str], entitlements: Optional[List[ComplexValue]], external_id: Optional[str], groups: Optional[List[ComplexValue]], id: Optional[str], members: Optional[List[ComplexValue]], meta: Optional[ResourceMeta], roles: Optional[List[ComplexValue]], schemas: Optional[List[GroupSchema]]]) -> Group\n\n        Create a new group.\n\n        Creates a group in the Databricks account with a unique name, using the supplied group details.\n\n        :param display_name: str (optional)\n          String that represents a human-readable group name\n        :param entitlements: List[:class:`ComplexValue`] (optional)\n          Entitlements assigned to the group. See [assigning entitlements] for a full list of supported\n          values.\n\n          [assigning entitlements]: https://docs.databricks.com/administration-guide/users-groups/index.html#assigning-entitlements\n        :param external_id: str (optional)\n        :param groups: List[:class:`ComplexValue`] (optional)\n        :param id: str (optional)\n          Databricks group ID\n        :param members: List[:class:`ComplexValue`] (optional)\n        :param meta: :class:`ResourceMeta` (optional)\n          Container for the group identifier. Workspace local versus account.\n        :param roles: List[:class:`ComplexValue`] (optional)\n          Corresponds to AWS instance profile/arn role.\n        :param schemas: List[:class:`GroupSchema`] (optional)\n          The schema of the group.\n\n        :returns: :class:`Group`\n```\n\n----------------------------------------\n\nTITLE: Get all cluster statuses - Python\nDESCRIPTION: Retrieves the status of all libraries on all Databricks clusters. This method returns an iterator over `ClusterLibraryStatuses` objects, providing details about the libraries installed on each cluster via the API or the UI.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/libraries.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: all_cluster_statuses() -> Iterator[ClusterLibraryStatuses]\n\n    Get all statuses.\n\n    Get the status of all libraries on all clusters. A status is returned for all libraries installed on\n    this cluster via the API or the libraries UI.\n\n    :returns: Iterator over :class:`ClusterLibraryStatuses`\n```\n\n----------------------------------------\n\nTITLE: Update Experiment with Databricks SDK in Python\nDESCRIPTION: This code snippet demonstrates how to update an experiment's name using the Databricks SDK for Python. It initializes a WorkspaceClient, creates a new experiment, updates its name, and then cleans up by deleting the experiment.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/experiments.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nexperiment = w.experiments.create_experiment(name=f\"sdk-{time.time_ns()}\")\n\nw.experiments.update_experiment(new_name=f\"sdk-{time.time_ns()}\", experiment_id=experiment.experiment_id)\n\n# cleanup\nw.experiments.delete_experiment(experiment_id=experiment.experiment_id)\n```\n\n----------------------------------------\n\nTITLE: Updating a Pipeline in Databricks\nDESCRIPTION: This code snippet shows how to update an existing Databricks pipeline. First it creates a pipeline, and then it updates it using the 'update' method. It uses the WorkspaceClient to interact with the Databricks workspace. It requires the TEST_INSTANCE_POOL_ID environment variable to be set.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/pipelines/pipelines.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import pipelines\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncreated = w.pipelines.create(\n    continuous=False,\n    name=f\"sdk-{time.time_ns()}\",\n    libraries=[pipelines.PipelineLibrary(notebook=pipelines.NotebookLibrary(path=notebook_path))],\n    clusters=[\n        pipelines.PipelineCluster(\n            instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n            label=\"default\",\n            num_workers=1,\n            custom_tags={\n                \"cluster_type\": \"default\",\n            },\n        )\n    ],\n)\n\nw.pipelines.update(\n    pipeline_id=created.pipeline_id,\n    name=f\"sdk-{time.time_ns()}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Reading a Volume in Databricks with Python SDK\nDESCRIPTION: This snippet demonstrates how to read a volume in Databricks using the Python SDK. It begins by creating the necessary resources like storage credentials, external locations, catalogs, schemas, and a volume.  It then reads the volume by its full name. Finally, it performs cleanup by deleting all the created resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/volumes.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\nstorage_credential = w.storage_credentials.create(\n    name=f\"sdk-{time.time_ns()}\",\n    aws_iam_role=catalog.AwsIamRoleRequest(role_arn=os.environ[\"TEST_METASTORE_DATA_ACCESS_ARN\"]),\n    comment=\"created via SDK\",\n)\n\nexternal_location = w.external_locations.create(\n    name=f\"sdk-{time.time_ns()}\",\n    credential_name=storage_credential.name,\n    comment=\"created via SDK\",\n    url=\"s3://\" + os.environ[\"TEST_BUCKET\"] + \"/\" + f\"sdk-{time.time_ns()}\",\n)\n\ncreated_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated_schema = w.schemas.create(name=f\"sdk-{time.time_ns()}\", catalog_name=created_catalog.name)\n\ncreated_volume = w.volumes.create(\n    catalog_name=created_catalog.name,\n    schema_name=created_schema.name,\n    name=f\"sdk-{time.time_ns()}\",\n    storage_location=external_location.url,\n    volume_type=catalog.VolumeType.EXTERNAL,\n)\n\nloaded_volume = w.volumes.read(name=created_volume.full_name)\n\n# cleanup\nw.storage_credentials.delete(name=storage_credential.name)\nw.external_locations.delete(name=external_location.name)\nw.schemas.delete(full_name=created_schema.full_name)\n```\n\n----------------------------------------\n\nTITLE: Listing Personalization Requests with Pagination - Python\nDESCRIPTION: This method retrieves all personalization requests across all listings for a given provider. It supports pagination using `page_size` and `page_token` parameters. The method returns an iterator over `PersonalizationRequest` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_personalization_requests.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list( [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[PersonalizationRequest]\n\n    All personalization requests across all listings.\n\n    List personalization requests to this provider. This will return all personalization requests,\n    regardless of which listing they are for.\n\n    :param page_size: int (optional)\n    :param page_token: str (optional)\n\n    :returns: Iterator over :class:`PersonalizationRequest`\n```\n\n----------------------------------------\n\nTITLE: Creating Private Endpoint Rule in Databricks\nDESCRIPTION: This method creates a private endpoint rule for a specified network connectivity configuration. It requires the network connectivity configuration ID, the Azure resource ID of the target resource, and the sub-resource type (group ID) of the target resource. After creation, the private endpoint needs approval via the Azure portal.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/network_connectivity.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nNetworkConnectivityAPI.create_private_endpoint_rule(network_connectivity_config_id: str, resource_id: str, group_id: CreatePrivateEndpointRuleRequestGroupId) -> NccAzurePrivateEndpointRule\n```\n\n----------------------------------------\n\nTITLE: Creating Storage Credential using AwsIamRole in Databricks\nDESCRIPTION: This snippet demonstrates how to create a storage credential in Databricks using the WorkspaceClient and specifying an AWS IAM role for authentication. It initializes the WorkspaceClient, creates a storage credential with a unique name and the provided IAM role ARN, and then cleans up by deleting the created credential.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/storage_credentials.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\ncreated = w.storage_credentials.create(\n    name=f\"sdk-{time.time_ns()}\",\n    aws_iam_role=catalog.AwsIamRole(role_arn=os.environ[\"TEST_METASTORE_DATA_ACCESS_ARN\"]),\n)\n\n# cleanup\nw.storage_credentials.delete(delete=created.name)\n```\n\n----------------------------------------\n\nTITLE: Repairing a Job Run with Databricks SDK in Python\nDESCRIPTION: This snippet demonstrates how to repair a job run using the Databricks SDK for Python. It cancels a run, then repairs it by re-running a specific task, using the `repair_run` method. The example includes job creation, running, and subsequent cleanup.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\ncreated_job = w.jobs.create(\n    name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.Task(\n            description=\"test\",\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=\"test\",\n            timeout_seconds=0,\n        )\n    ],\n)\n\nrun_now_response = w.jobs.run_now(job_id=created_job.job_id)\n\ncancelled_run = w.jobs.cancel_run(run_id=run_now_response.response.run_id).result()\n\nrepaired_run = w.jobs.repair_run(\n    rerun_tasks=[cancelled_run.tasks[0].task_key],\n    run_id=run_now_response.response.run_id,\n).result()\n\n# cleanup\nw.jobs.delete(job_id=created_job.job_id)\n```\n\n----------------------------------------\n\nTITLE: Updating a Provider with Databricks SDK in Python\nDESCRIPTION: This snippet illustrates how to update the information of an existing data provider using the Databricks SDK. It creates a provider, updates its comment using the update method, and then deletes the provider. This demonstrates how to modify provider details such as the comment field.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/providers.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\npublic_share_recipient = \"\"\"{\n        \"shareCredentialsVersion\":1,\n        \"bearerToken\":\"dapiabcdefghijklmonpqrstuvwxyz\",\n        \"endpoint\":\"https://sharing.delta.io/delta-sharing/\"\n    }\n\"\"\"\n\ncreated = w.providers.create(name=f\"sdk-{time.time_ns()}\", recipient_profile_str=public_share_recipient)\n\n_ = w.providers.update(name=created.name, comment=\"Comment for update\")\n\n# cleanup\nw.providers.delete(name=created.name)\n```\n\n----------------------------------------\n\nTITLE: Getting a Table Monitor in Python\nDESCRIPTION: Retrieves a monitor for the specified table. The caller must have necessary permissions on the table's parent catalog and schema and SELECT privilege on the table. Returns configuration values and information on assets created by the monitor as a MonitorInfo object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/lakehouse_monitors.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(table_name: str) -> MonitorInfo\n\n        Get a table monitor.\n        \n        Gets a monitor for the specified table.\n        \n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema. 3. have the following\n        permissions: - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent\n        schema - **SELECT** privilege on the table.\n        \n        The returned information includes configuration values, as well as information on assets created by\n        the monitor. Some information (e.g., dashboard) may be filtered out if the caller is in a different\n        workspace than where the monitor was created.\n        \n        :param table_name: str\n          Full name of the table.\n        \n        :returns: :class:`MonitorInfo`\n```\n\n----------------------------------------\n\nTITLE: Listing Monitor Refreshes - Databricks SDK (Python)\nDESCRIPTION: This method retrieves the history of the most recent refreshes (up to 25) for a given table.  The caller needs specific permissions including ownership or USE privileges on the catalog and schema, along with SELECT privilege on the table. The call must originate from the workspace where the monitor was created.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/quality_monitors.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list_refreshes(table_name: str) -> MonitorRefreshListResponse\n\n        List refreshes.\n\n        Gets an array containing the history of the most recent refreshes (up to 25) for this table.\n\n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n        - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent schema -\n        **SELECT** privilege on the table.\n\n        Additionally, the call must be made from the workspace where the monitor was created.\n\n        :param table_name: str\n          Full name of the table.\n\n        :returns: :class:`MonitorRefreshListResponse`\n```\n\n----------------------------------------\n\nTITLE: Deleting an Installation using ConsumerInstallationsAPI in Python\nDESCRIPTION: Uninstalls an installation associated with a Databricks Marketplace listing. The `delete` method takes both the listing ID and the installation ID as parameters.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_installations.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nw.consumer_installations.delete(listing_id: str, installation_id: str)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Registered Model Alias with databricks-sdk-py\nDESCRIPTION: Deletes a registered model alias. The caller must be a metastore admin or an owner of the registered model. For the latter case, the caller must also be the owner or have the USE_CATALOG privilege on the parent catalog and the USE_SCHEMA privilege on the parent schema.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/registered_models.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete_alias(full_name: str, alias: str)\n\n    Delete a Registered Model Alias.\n\n    Deletes a registered model alias.\n\n    The caller must be a metastore admin or an owner of the registered model. For the latter case, the\n    caller must also be the owner or have the **USE_CATALOG** privilege on the parent catalog and the\n    **USE_SCHEMA** privilege on the parent schema.\n\n    :param full_name: str\n      The three-level (fully qualified) name of the registered model\n    :param alias: str\n      The name of the alias\n```\n\n----------------------------------------\n\nTITLE: Creating custom OAuth application with Databricks SDK\nDESCRIPTION: This snippet demonstrates how to create a custom OAuth application using the `account_client.custom_app_integration.create` API in the Databricks SDK for Python. It requires `AccountClient` from the `databricks.sdk` and the user provides account ID, username and password for authentication. The code also enrolls all published apps for OAuth and retrieves its status.  It logs the created client ID and secret.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport logging, getpass\nfrom databricks.sdk import AccountClient\naccount_client = AccountClient(host='https://accounts.cloud.databricks.com',\n                               account_id=input('Databricks Account ID: '),\n                               username=input('Username: '),\n                               password=getpass.getpass('Password: '))\n\nlogging.info('Enrolling all published apps...')\naccount_client.o_auth_enrollment.create(enable_all_published_apps=True)\n\nstatus = account_client.o_auth_enrollment.get()\nlogging.info(f'Enrolled all published apps: {status}')\n\ncustom_app = account_client.custom_app_integration.create(\n    name='awesome-app',\n    redirect_urls=[f'https://host.domain/path/to/callback'],\n    confidential=True)\nlogging.info(f'Created new custom app: '\n             f'--client_id {custom_app.client_id} '\n             f'--client_secret {custom_app.client_secret}')\n```\n\n----------------------------------------\n\nTITLE: Creating a Databricks Repo in Python\nDESCRIPTION: This snippet demonstrates how to create a new Databricks Repo using the WorkspaceClient and the repos.create method. It sets the path, URL, and provider for the new repo, then cleans up by deleting the repo.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/repos.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nroot = f\"sdk-{time.time_ns()}\"\n\nri = w.repos.create(\n    path=root,\n    url=\"https://github.com/shreyas-goenka/empty-repo.git\",\n    provider=\"github\",\n)\n\n# cleanup\nw.repos.delete(repo_id=ri.id)\n```\n\n----------------------------------------\n\nTITLE: Creating a Databricks Pipeline\nDESCRIPTION: This snippet demonstrates creating a Databricks pipeline with a notebook library and a cluster configuration. It also includes cleanup by deleting the created pipeline. Requires the databricks-sdk-py library and the TEST_INSTANCE_POOL_ID environment variable.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/pipelines/pipelines.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipelines.PipelineCreateRequest(\n                  libraries=[pipelines.PipelineLibrary(notebook=pipelines.NotebookLibrary(path=notebook_path))],\n                clusters=[\n                    pipelines.PipelineCluster(\n                        instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n                        label=\"default\",\n                        num_workers=1,\n                        custom_tags={\n                            \"cluster_type\": \"default\",\n                        },\n                    )\n                ],\n            )\n            \n            # cleanup\n            w.pipelines.delete(pipeline_id=created.pipeline_id)\n```\n\n----------------------------------------\n\nTITLE: Iterating over TableSummary with pagination - Python\nDESCRIPTION: This method demonstrates how to iterate over a collection of `TableSummary` objects using pagination. The parameters `page_token`, `schema_name_pattern`, and `table_name_pattern` allow for filtering and controlling the pagination process. The page length is determined by configuration, with a server-configured maximum of 10000.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/tables.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n        :param page_token: str (optional)\n          Opaque pagination token to go to next page based on previous query.\n        :param schema_name_pattern: str (optional)\n          A sql LIKE pattern (% and _) for schema names. All schemas will be returned if not set or empty.\n        :param table_name_pattern: str (optional)\n          A sql LIKE pattern (% and _) for table names. All tables will be returned if not set or empty.\n\n        :returns: Iterator over :class:`TableSummary`\n```\n\n----------------------------------------\n\nTITLE: Get Encryption Key Configuration by ID in Databricks\nDESCRIPTION: This code retrieves an encryption key configuration by its ID using the AccountClient.  It first creates an encryption key, then fetches it using the get method.  Finally, it deletes the created key for cleanup. Environment variables for KMS key ARN and alias are expected to be set.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/encryption_keys.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import provisioning\n\na = AccountClient()\n\ncreated = a.encryption_keys.create(\n    aws_key_info=provisioning.CreateAwsKeyInfo(\n        key_arn=os.environ[\"TEST_MANAGED_KMS_KEY_ARN\"],\n        key_alias=os.environ[\"TEST_STORAGE_KMS_KEY_ALIAS\"],\n    ),\n    use_cases=[provisioning.KeyUseCase.MANAGED_SERVICES],\n)\n\nby_id = a.encryption_keys.get(customer_managed_key_id=created.customer_managed_key_id)\n\n# cleanup\na.encryption_keys.delete(customer_managed_key_id=created.customer_managed_key_id)\n```\n\n----------------------------------------\n\nTITLE: Update Serving Endpoint AI Gateway - Python\nDESCRIPTION: Updates the AI Gateway configuration of a serving endpoint. Requires the name of the serving endpoint and optional parameters for fallback configuration, guardrails, inference table configuration, rate limits, and usage tracking configuration.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: put_ai_gateway(name: str [, fallback_config: Optional[FallbackConfig], guardrails: Optional[AiGatewayGuardrails], inference_table_config: Optional[AiGatewayInferenceTableConfig], rate_limits: Optional[List[AiGatewayRateLimit]], usage_tracking_config: Optional[AiGatewayUsageTrackingConfig]]) -> PutAiGatewayResponse\n\n    Update AI Gateway of a serving endpoint.\n\n    Used to update the AI Gateway of a serving endpoint. NOTE: External model, provisioned throughput, and\n    pay-per-token endpoints are fully supported; agent endpoints currently only support inference tables.\n\n    :param name: str\n      The name of the serving endpoint whose AI Gateway is being updated. This field is required.\n    :param fallback_config: :class:`FallbackConfig` (optional)\n      Configuration for traffic fallback which auto fallbacks to other served entities if the request to a\n      served entity fails with certain error codes, to increase availability.\n    :param guardrails: :class:`AiGatewayGuardrails` (optional)\n```\n\n----------------------------------------\n\nTITLE: Get Model Usage Example (Python)\nDESCRIPTION: This example demonstrates how to retrieve model details using the Databricks SDK. It first creates a model, then retrieves it by name using the `get_model` method. The example requires the `databricks-sdk` library and utilizes the `WorkspaceClient` to interact with the Databricks workspace.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/model_registry.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.model_registry.create_model(name=f\"sdk-{time.time_ns()}\")\n\nmodel = w.model_registry.get_model(name=created.registered_model.name)\n```\n\n----------------------------------------\n\nTITLE: Getting a Cluster Policy in Databricks with Python SDK\nDESCRIPTION: This code retrieves a cluster policy after creating it using the Databricks SDK for Python. The example creates a cluster policy with a specific configuration, retrieves it by ID, and then deletes it, demonstrating a typical workflow for policy management. The `definition` parameter is used to set Spark configurations within the policy.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/cluster_policies.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.cluster_policies.create(\n    name=f\"sdk-{time.time_ns()}\",\n    definition=\"\"\"{\n            \"spark_conf.spark.databricks.delta.preview.enabled\": {\n                \"type\": \"fixed\",\n                \"value\": true\n            }\n        }\n\"\"\",\n)\n\npolicy = w.cluster_policies.get(policy_id=created.policy_id)\n\n# cleanup\nw.cluster_policies.delete(policy_id=created.policy_id)\n```\n\n----------------------------------------\n\nTITLE: Updating a Table Monitor in Python\nDESCRIPTION: Updates a monitor for the specified table. Takes various optional parameters to modify the monitor's configuration.  The method returns a MonitorInfo object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/lakehouse_monitors.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(table_name: str, output_schema_name: str [, baseline_table_name: Optional[str], custom_metrics: Optional[List[MonitorMetric]], data_classification_config: Optional[MonitorDataClassificationConfig], inference_log: Optional[MonitorInferenceLog], notifications: Optional[MonitorNotifications], schedule: Optional[MonitorCronSchedule], slicing_exprs: Optional[List[str]], snapshot: Optional[MonitorSnapshot], time_series: Optional[MonitorTimeSeries]]) -> MonitorInfo\n```\n\n----------------------------------------\n\nTITLE: Listing Service Principals (Python)\nDESCRIPTION: This snippet demonstrates how to list all service principals in a Databricks workspace using the Databricks SDK. It initializes a WorkspaceClient and uses the `list` method to retrieve all service principals.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/service_principals.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import iam\n\nw = WorkspaceClient()\n\nall = w.service_principals.list(iam.ListServicePrincipalsRequest())\n```\n\n----------------------------------------\n\nTITLE: Update External Location in Databricks using Python SDK\nDESCRIPTION: This code snippet demonstrates how to update an external location in Databricks using the Python SDK. It showcases the `update` method of the `ExternalLocationsAPI`. The example creates a storage credential and an external location, updates the external location's URL, and then cleans up by deleting the created resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/external_locations.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\ncredential = w.storage_credentials.create(\n    name=f\"sdk-{time.time_ns()}\",\n    aws_iam_role=catalog.AwsIamRole(role_arn=os.environ[\"TEST_METASTORE_DATA_ACCESS_ARN\"]),\n)\n\ncreated = w.external_locations.create(\n    name=f\"sdk-{time.time_ns()}\",\n    credential_name=credential.name,\n    url=f's3://{os.environ[\"TEST_BUCKET\"]}/sdk-{time.time_ns()}',\n)\n\n_ = w.external_locations.update(\n    name=created.name,\n    credential_name=credential.name,\n    url=f's3://{os.environ[\"TEST_BUCKET\"]}/sdk-{time.time_ns()}',\n)\n\n# cleanup\nw.storage_credentials.delete(delete=credential.name)\nw.external_locations.delete(delete=created.name)\n```\n\n----------------------------------------\n\nTITLE: Initializing WorkspaceClient in Notebook\nDESCRIPTION: This Python code snippet shows how to initialize a `WorkspaceClient` within a Databricks notebook. When no authentication parameters are provided, the client automatically pulls authentication information from the notebook's execution context.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/getting-started.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient()\n```\n\n----------------------------------------\n\nTITLE: Creating Service Principal in Databricks (Python)\nDESCRIPTION: This snippet demonstrates how to create a service principal using the Databricks SDK. It initializes a WorkspaceClient, creates a service principal with a unique display name, and then cleans up by deleting the created service principal.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/service_principals.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.service_principals.create(display_name=f\"sdk-{time.time_ns()}\")\n\n# cleanup\nw.service_principals.delete(id=created.id)\n```\n\n----------------------------------------\n\nTITLE: Updating Workspace Configuration with Databricks AccountClient in Python\nDESCRIPTION: This snippet demonstrates how to update a workspace configuration using the `AccountClient` from the `databricks.sdk`. It includes creating credentials, updating the workspace with the new credentials, and cleaning up the created credentials.  It uses environment variables (e.g., `os.environ[\"TEST_CROSSACCOUNT_ARN\"]`) for configuration. The `workspace_id` is needed to identify the workspace to be updated, and `credentials_id` is required for updating the credential configuration.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/workspaces.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import provisioning\n\na = AccountClient()\n\nupdate_role = a.credentials.create(\n    credentials_name=f\"sdk-{time.time_ns()}\",\n    aws_credentials=provisioning.CreateCredentialAwsCredentials(\n        sts_role=provisioning.CreateCredentialStsRole(role_arn=os.environ[\"TEST_CROSSACCOUNT_ARN\"])\n    ),\n)\n\ncreated = a.waiter.get()\n\n_ = a.workspaces.update(\n    workspace_id=created.workspace_id,\n    credentials_id=update_role.credentials_id,\n).result()\n\n# cleanup\na.credentials.delete(credentials_id=update_role.credentials_id)\n```\n\n----------------------------------------\n\nTITLE: Creating a Run with the MLflow Experiments API in Python\nDESCRIPTION: This snippet showcases how to create a run within an MLflow experiment using the Databricks SDK. It starts by creating an experiment, then creates a run within that experiment and adds a tag to it. Finally, it cleans up by deleting the run and the experiment.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/experiments.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import ml\n\nw = WorkspaceClient()\n\nexperiment = w.experiments.create_experiment(name=f\"sdk-{time.time_ns()}\")\n\ncreated = w.experiments.create_run(\n    experiment_id=experiment.experiment_id,\n    tags=[ml.RunTag(key=\"foo\", value=\"bar\")],\n)\n\n# cleanup\nw.experiments.delete_experiment(experiment_id=experiment.experiment_id)\nw.experiments.delete_run(run_id=created.run.info.run_id)\n```\n\n----------------------------------------\n\nTITLE: Delete Endpoint in Databricks\nDESCRIPTION: Deletes an existing vector search endpoint.  Requires the name of the endpoint to be deleted.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_endpoints.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nw.vector_search_endpoints.delete_endpoint(endpoint_name: str)\n```\n\n----------------------------------------\n\nTITLE: Update Run with Databricks SDK in Python\nDESCRIPTION: This code snippet demonstrates how to update a run's status using the Databricks SDK for Python. It initializes a WorkspaceClient, creates an experiment and a run within that experiment, updates the run status to KILLED, and then cleans up by deleting the run and the experiment.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/experiments.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import ml\n\nw = WorkspaceClient()\n\nexperiment = w.experiments.create_experiment(name=f\"sdk-{time.time_ns()}\")\n\ncreated = w.experiments.create_run(\n    experiment_id=experiment.experiment_id,\n    tags=[ml.RunTag(key=\"foo\", value=\"bar\")],\n)\n\n_ = w.experiments.update_run(run_id=created.run.info.run_id, status=ml.UpdateRunStatus.KILLED)\n\n# cleanup\nw.experiments.delete_experiment(experiment_id=experiment.experiment_id)\nw.experiments.delete_run(run_id=created.run.info.run_id)\n```\n\n----------------------------------------\n\nTITLE: Updating Notification Destination in Python\nDESCRIPTION: Updates an existing notification destination with the specified configuration and display name. Requires workspace admin permissions. At least one field must be specified in the request body. Returns a NotificationDestination object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/notification_destinations.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(id: str [, config: Optional[Config], display_name: Optional[str]]) -> NotificationDestination\n\n    Update a notification destination.\n\n    Updates a notification destination. Requires workspace admin permissions. At least one field is\n    required in the request body.\n\n    :param id: str\n      UUID identifying notification destination.\n    :param config: :class:`Config` (optional)\n      The configuration for the notification destination. Must wrap EXACTLY one of the nested configs.\n    :param display_name: str (optional)\n      The display name for the notification destination.\n\n    :returns: :class:`NotificationDestination`\n```\n\n----------------------------------------\n\nTITLE: Cancelling a Specific Job Run with Databricks SDK\nDESCRIPTION: This snippet illustrates how to cancel a specific run of a Databricks job using the Python SDK. It requires the run_id of the run to be cancelled. The cancel_run method is called with the run_id, and the result method is used to wait for the cancellation to complete.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\ncreated_job = w.jobs.create(\n    name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.Task(\n            description=\"test\",\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=\"test\",\n            timeout_seconds=0,\n        )\n    ],\n)\n\nrun_now_response = w.jobs.run_now(job_id=created_job.job_id)\n\ncancelled_run = w.jobs.cancel_run(run_id=run_now_response.response.run_id).result()\n\n# cleanup\nw.jobs.delete(job_id=created_job.job_id)\n```\n\n----------------------------------------\n\nTITLE: Adding HTTP Request to Serving Endpoints in Python\nDESCRIPTION: This snippet reflects the addition of the `serving.http_request` function to the Databricks SDK for Python, enabling calls to external functions from serving endpoints. It enhances the functionality of serving endpoints by allowing integration with external services via HTTP requests. The Databricks SDK for Python is required.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nserving.http_request\n```\n\n----------------------------------------\n\nTITLE: List External Locations in Databricks using Python SDK\nDESCRIPTION: This code snippet demonstrates how to list all external locations in Databricks using the Python SDK.  It showcases the `list` method of the `ExternalLocationsAPI`, returning an iterator over `ExternalLocationInfo` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/external_locations.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\nall = w.external_locations.list(catalog.ListExternalLocationsRequest())\n```\n\n----------------------------------------\n\nTITLE: Get Object Permission Levels using Permissions API in Python\nDESCRIPTION: This snippet retrieves the permission levels that a user can have on a specific object (notebook). It utilizes the `get_permission_levels` method of the `permissions` API after obtaining the object's ID by creating a notebook path and retrieving its status.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/permissions.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\nobj = w.workspace.get_status(path=notebook_path)\n\nlevels = w.permissions.get_permission_levels(request_object_type=\"notebooks\", request_object_id=\"%d\" % (obj.object_id))\n```\n\n----------------------------------------\n\nTITLE: Getting Git Credentials by ID using Databricks SDK\nDESCRIPTION: This code snippet shows how to retrieve Git credentials by their ID using the Databricks SDK. It creates credentials, retrieves them using the `get` method, and then cleans up by deleting the credentials. This exemplifies how to fetch specific credential entries using their unique identifier.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/git_credentials.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncr = w.git_credentials.create(git_provider=\"gitHub\", git_username=\"test\", personal_access_token=\"test\")\n\nby_id = w.git_credentials.get(credential_id=cr.credential_id)\n\n# cleanup\nw.git_credentials.delete(credential_id=cr.credential_id)\n```\n\n----------------------------------------\n\nTITLE: Get Listing in Databricks Marketplace (Python)\nDESCRIPTION: Retrieves a specific listing from the Databricks Marketplace. It requires the listing ID (str) as input and returns a GetListingResponse object containing the listing details.  This allows providers to inspect the configuration of their listings.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_listings.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nw.provider_listings.get(id: str) -> GetListingResponse\n```\n\n----------------------------------------\n\nTITLE: Listing Databricks Clusters using Private OAuth (Python)\nDESCRIPTION: This code snippet demonstrates how to authenticate with Azure Databricks using the Private OAuth flow. It initializes a `WorkspaceClient` with the Databricks host, client ID, client secret (both obtained from the Terraform output), and sets `auth_type` to 'external-browser', triggering a browser-based login flow.\nDependencies: databricks-sdk\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient(host='https://adb-30....azuredatabricks.net',\n                    client_id='>>> value from private_client_id <<<',\n                    client_secret='>>> value from private_client_secret <<<',\n                    auth_type='external-browser')\nclusters = w.clusters.list()\nfor cl in clusters:\n    print(f' - {cl.cluster_name} is {cl.state}')\n```\n\n----------------------------------------\n\nTITLE: Creating a Model Version in Model Registry with Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to create a model version using the Databricks SDK. It first creates a registered model and then creates a version of that model, specifying the source URI for the model artifacts.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/model_registry.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nmodel = w.model_registry.create_model(name=f\"sdk-{time.time_ns()}\")\n\nmv = w.model_registry.create_model_version(name=model.registered_model.name, source=\"dbfs:/tmp\")\n```\n\n----------------------------------------\n\nTITLE: Patching a Group with Groups API in Python\nDESCRIPTION: This snippet demonstrates how to update the details of a group using the `patch` method of the `GroupsAPI` class. It adds a user as a member to the group. It requires the `databricks-sdk` library and relies on `WorkspaceClient` for API interaction. The snippet also imports `iam` to work with patching operations.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/groups.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import iam\n\nw = WorkspaceClient()\n\ngroup = w.groups.create(display_name=f\"sdk-{time.time_ns()}-group\")\nuser = w.users.create(\n    display_name=f\"sdk-{time.time_ns()}-user\",\n    user_name=f\"sdk-{time.time_ns()}@example.com\",\n)\n\nw.groups.patch(\n    id=group.id,\n    operations=[\n        iam.Patch(\n            op=iam.PatchOp.ADD,\n            value={\n                \"members\": [\n                    {\n                        \"value\": user.id,\n                    }\n                ]\n            },\n        )\n    ],\n    schemas=[iam.PatchSchema.URN_IETF_PARAMS_SCIM_API_MESSAGES_2_0_PATCH_OP],\n)\n\n# cleanup\nw.users.delete(id=user.id)\nw.groups.delete(id=group.id)\n```\n\n----------------------------------------\n\nTITLE: Update Metastore - Python\nDESCRIPTION: Updates an existing Unity Catalog metastore. Takes the metastore ID and an optional `UpdateMetastore` object as input. Returns an `AccountsMetastoreInfo` object representing the updated metastore.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/catalog/metastores.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(metastore_id: str [, metastore_info: Optional[UpdateMetastore]]) -> AccountsMetastoreInfo\n\n    Update a metastore.\n\n    Updates an existing Unity Catalog metastore.\n\n    :param metastore_id: str\n      Unity Catalog metastore ID\n    :param metastore_info: :class:`UpdateMetastore` (optional)\n\n    :returns: :class:`AccountsMetastoreInfo`\n```\n\n----------------------------------------\n\nTITLE: Wait for Endpoint Online in Databricks\nDESCRIPTION: Waits for a vector search endpoint to become online. Requires the endpoint name, an optional timeout, and an optional callback function. Returns the EndpointInfo object once the endpoint is online.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_endpoints.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nw.vector_search_endpoints.wait_get_endpoint_vector_search_endpoint_online(endpoint_name: str, timeout: datetime.timedelta = 0:20:00, callback: Optional[Callable[[EndpointInfo], None]]) -> EndpointInfo\n```\n\n----------------------------------------\n\nTITLE: Creating Personalization Request with ConsumerTerms - Python\nDESCRIPTION: Creates a personalization request for a specific marketplace listing. Requires the listing ID, intended use, and accepted consumer terms. Optionally accepts comment, company, first name, is_from_lighthouse, last name, and recipient_type. Returns a CreatePersonalizationRequestResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_personalization_requests.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nw.consumer_personalization_requests.create(listing_id: str, intended_use: str, accepted_consumer_terms: ConsumerTerms [, comment: Optional[str], company: Optional[str], first_name: Optional[str], is_from_lighthouse: Optional[bool], last_name: Optional[str], recipient_type: Optional[DeltaSharingRecipientType]]) -> CreatePersonalizationRequestResponse\n```\n\n----------------------------------------\n\nTITLE: Deleting Budget Configuration\nDESCRIPTION: This code snippet demonstrates how to delete a budget configuration using the Databricks SDK for Python. It uses the `budgets.delete` method to remove a budget by its ID.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/budgets.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\na.budgets.delete(budget_id=created.budget.budget_configuration_id)\n```\n\n----------------------------------------\n\nTITLE: Getting Storage Credential by Name in Databricks\nDESCRIPTION: This snippet shows how to retrieve a storage credential from Databricks by its name using the WorkspaceClient. It first creates a storage credential with an AWS IAM role. Then, it retrieves the credential by its name and finally deletes the created credential.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/storage_credentials.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\ncreated = w.storage_credentials.create(\n    name=f\"sdk-{time.time_ns()}\",\n    aws_iam_role=catalog.AwsIamRole(role_arn=os.environ[\"TEST_METASTORE_DATA_ACCESS_ARN\"]),\n)\n\nby_name = w.storage_credentials.get(get=created.name)\n\n# cleanup\nw.storage_credentials.delete(delete=created.name)\n```\n\n----------------------------------------\n\nTITLE: Updating Service Principal Details (Python)\nDESCRIPTION: This snippet demonstrates how to update a service principal's details using the `update` method of the Databricks SDK. It creates a service principal, then updates its display name and roles, and finally deletes the service principal.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/service_principals.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import iam\n\nw = WorkspaceClient()\n\ncreated = w.service_principals.create(display_name=f\"sdk-{time.time_ns()}\")\n\nw.service_principals.update(\n    id=created.id,\n    display_name=f\"sdk-{time.time_ns()}\",\n    roles=[iam.ComplexValue(value=\"xyz\")],\n)\n\n# cleanup\nw.service_principals.delete(id=created.id)\n```\n\n----------------------------------------\n\nTITLE: Getting Schema Information in Databricks\nDESCRIPTION: This code snippet illustrates how to retrieve information about a specific schema using its full name with the Databricks SDK for Python. It showcases the creation of a catalog and schema, retrieval of the schema using `w.schemas.get`, and subsequent cleanup by deleting the created resources. The WorkspaceClient is required to interact with the Databricks workspace.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/schemas.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nnew_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated = w.schemas.create(name=f\"sdk-{time.time_ns()}\", catalog_name=new_catalog.name)\n\n_ = w.schemas.get(full_name=created.full_name)\n\n# cleanup\nw.catalogs.delete(name=new_catalog.name, force=True)\nw.schemas.delete(full_name=created.full_name)\n```\n\n----------------------------------------\n\nTITLE: Update Account IP Access Toggle Setting (Python)\nDESCRIPTION: Updates the value of the account IP access toggle setting. Requires `allow_missing` to be set to `true`, the `setting` as an `AccountIpAccessEnable` object, and a `field_mask` string specifying the fields to update.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/enable_ip_access_lists.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\na.settings.enable_ip_access_lists.update(allow_missing: bool, setting: AccountIpAccessEnable, field_mask: str) -> AccountIpAccessEnable\n```\n\n----------------------------------------\n\nTITLE: Deleting a Table Monitor - Databricks SDK (Python)\nDESCRIPTION: This method deletes a monitor for a specified table.  The caller must have necessary ownership or USE privileges on the catalog and schema. The call must be made from the workspace where the monitor was created. It is important to note that the associated metric tables and dashboard are not automatically deleted and must be cleaned up manually.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/quality_monitors.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(table_name: str)\n\n        Delete a table monitor.\n\n        Deletes a monitor for the specified table.\n\n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n        - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent schema - be an\n        owner of the table.\n\n        Additionally, the call must be made from the workspace where the monitor was created.\n\n        Note that the metric tables and dashboard will not be deleted as part of this call; those assets must\n        be manually cleaned up (if desired).\n\n        :param table_name: str\n          Full name of the table.\n```\n\n----------------------------------------\n\nTITLE: List Shares in Databricks\nDESCRIPTION: This snippet demonstrates how to list all shares using the SharesAPI. It initializes the WorkspaceClient and calls the list method, passing in a ListSharesRequest object. The max_results parameter controls the maximum number of shares to return per page, and the page_token parameter can be used for pagination.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/shares.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sharing\n\nw = WorkspaceClient()\n\nall = w.shares.list(sharing.ListSharesRequest())\n```\n\n----------------------------------------\n\nTITLE: Creating and Deleting Instance Pool with Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to create an instance pool using the Databricks SDK, then immediately deletes it. It initializes a WorkspaceClient, selects a suitable node type, creates the instance pool with a unique name, and then deletes it using the returned instance_pool_id. This showcases the basic create and delete workflow.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/instance_pools.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nsmallest = w.clusters.select_node_type(local_disk=True)\n\ncreated = w.instance_pools.create(instance_pool_name=f\"sdk-{time.time_ns()}\", node_type_id=smallest)\n\n# cleanup\nw.instance_pools.delete(instance_pool_id=created.instance_pool_id)\n```\n\n----------------------------------------\n\nTITLE: List Tables Example - Databricks SDK\nDESCRIPTION: This snippet demonstrates how to list tables using the Databricks SDK. It sets up the WorkspaceClient, creates a catalog and schema, lists the tables summaries and cleans up the created resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/tables.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated_schema = w.schemas.create(name=f\"sdk-{time.time_ns()}\", catalog_name=created_catalog.name)\n\nsummaries = w.tables.list_summaries(catalog_name=created_catalog.name, schema_name_pattern=created_schema.name)\n\n# cleanup\nw.schemas.delete(full_name=created_schema.full_name)\nw.catalogs.delete(name=created_catalog.name, force=True)\n```\n\n----------------------------------------\n\nTITLE: Listing Dashboards using Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to list dashboard objects using the Databricks SDK. It imports the necessary modules, initializes the WorkspaceClient, and retrieves a paginated list of dashboard objects using the `list` method.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/dashboards.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\nall = w.dashboards.list(sql.ListDashboardsRequest())\n```\n\n----------------------------------------\n\nTITLE: Assign Metastore to Workspace in Python\nDESCRIPTION: This snippet demonstrates how to assign a metastore to a specific workspace using the Databricks SDK. It retrieves the workspace ID from an environment variable, creates a new metastore, assigns it to the workspace, and then cleans up by deleting the created metastore. It relies on the WorkspaceClient from the databricks.sdk.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/metastores.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nworkspace_id = os.environ[\"DUMMY_WORKSPACE_ID\"]\n\ncreated = w.metastores.create(\n    name=f\"sdk-{time.time_ns()}\",\n    storage_root=\"s3://%s/%s\" % (os.environ[\"TEST_BUCKET\"], f\"sdk-{time.time_ns()}\"),\n)\n\nw.metastores.assign(metastore_id=created.metastore_id, workspace_id=workspace_id)\n\n# cleanup\nw.metastores.delete(id=created.metastore_id, force=True)\n```\n\n----------------------------------------\n\nTITLE: Querying Next Page of Vector Search Results\nDESCRIPTION: This method queries the next page of results from a previous query to a vector search index.  It requires the index name and optionally accepts the endpoint name and a page token from the previous query.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_indexes.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: query_next_page(index_name: str [, endpoint_name: Optional[str], page_token: Optional[str]]) -> QueryVectorIndexResponse\n\n        Query next page.\n\n        Use `next_page_token` returned from previous `QueryVectorIndex` or `QueryVectorIndexNextPage` request\n        to fetch next page of results.\n\n        :param index_name: str\n          Name of the vector index to query.\n        :param endpoint_name: str (optional)\n          Name of the endpoint.\n        :param page_token: str (optional)\n          Page token returned from previous `QueryVectorIndex` or `QueryVectorIndexNextPage` API.\n\n        :returns: :class:`QueryVectorIndexResponse`\n```\n\n----------------------------------------\n\nTITLE: Execute Command in Databricks Context\nDESCRIPTION: This snippet shows how to execute a command within a Databricks command execution context. It creates a context (as in the previous example), then executes a simple Python command ('print(1)') within that context.  Finally, it cleans up the created context. The example requires a running Databricks cluster and appropriate environment variables set.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/command_execution.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import compute\n\nw = WorkspaceClient()\n\ncluster_id = os.environ[\"TEST_DEFAULT_CLUSTER_ID\"]\n\ncontext = w.command_execution.create(cluster_id=cluster_id, language=compute.Language.PYTHON).result()\n\ntext_results = w.command_execution.execute(\n    cluster_id=cluster_id,\n    context_id=context.id,\n    language=compute.Language.PYTHON,\n    command=\"print(1)\",\n).result()\n\n# cleanup\nw.command_execution.destroy(cluster_id=cluster_id, context_id=context.id)\n```\n\n----------------------------------------\n\nTITLE: Listing Credentials in Databricks (Python)\nDESCRIPTION: This method retrieves a list of credentials. The array is limited to credentials that the caller has permission to access. Metastore admins have unrestricted access.  The maximum number of results and a page token can be provided for pagination.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/credentials.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list_credentials( [, max_results: Optional[int], page_token: Optional[str], purpose: Optional[CredentialPurpose]]) -> Iterator[CredentialInfo]\n\n        List credentials.\n\n        Gets an array of credentials (as __CredentialInfo__ objects).\n\n        The array is limited to only the credentials that the caller has permission to access. If the caller\n        is a metastore admin, retrieval of credentials is unrestricted. There is no guarantee of a specific\n        ordering of the elements in the array.\n\n        :param max_results: int (optional)\n          Maximum number of credentials to return. - If not set, the default max page size is used. - When set\n          to a value greater than 0, the page length is the minimum of this value and a server-configured\n          value. - When set to 0, the page length is set to a server-configured value (recommended). - When\n          set to a value less than 0, an invalid parameter error is returned.\n        :param page_token: str (optional)\n          Opaque token to retrieve the next page of results.\n        :param purpose: :class:`CredentialPurpose` (optional)\n          Return only credentials for the specified purpose.\n\n        :returns: Iterator over :class:`CredentialInfo`\n```\n\n----------------------------------------\n\nTITLE: Waiting for Genie Message Completion in Python\nDESCRIPTION: This method waits for the completion of a Genie message. It requires the conversation ID, message ID, and space ID.  An optional callback function can be provided to execute upon completion.  It returns a GenieMessage object once completed or after the timeout.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/genie.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: wait_get_message_genie_completed(conversation_id: str, message_id: str, space_id: str, timeout: datetime.timedelta = 0:20:00, callback: Optional[Callable[[GenieMessage], None]]) -> GenieMessage\n```\n\n----------------------------------------\n\nTITLE: Creating a Table Monitor in Python\nDESCRIPTION: Creates a new monitor for the specified table. The caller needs to have the appropriate permissions on the table's parent catalog and schema. Workspace assets, such as the dashboard, will be created in the workspace where the call was made. Returns a MonitorInfo object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/lakehouse_monitors.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(table_name: str, assets_dir: str, output_schema_name: str [, baseline_table_name: Optional[str], custom_metrics: Optional[List[MonitorMetric]], data_classification_config: Optional[MonitorDataClassificationConfig], inference_log: Optional[MonitorInferenceLog], notifications: Optional[MonitorNotifications], schedule: Optional[MonitorCronSchedule], skip_builtin_dashboard: Optional[bool], slicing_exprs: Optional[List[str]], snapshot: Optional[MonitorSnapshot], time_series: Optional[MonitorTimeSeries], warehouse_id: Optional[str]]) -> MonitorInfo\n\n        Create a table monitor.\n        \n        Creates a new monitor for the specified table.\n        \n        The caller must either: 1. be an owner of the table's parent catalog, have **USE_SCHEMA** on the\n        table's parent schema, and have **SELECT** access on the table 2. have **USE_CATALOG** on the table's\n        parent catalog, be an owner of the table's parent schema, and have **SELECT** access on the table. 3.\n        have the following permissions: - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on\n        the table's parent schema - be an owner of the table.\n        \n        Workspace assets, such as the dashboard, will be created in the workspace where this call was made.\n        \n        :param table_name: str\n          Full name of the table.\n        :param assets_dir: str\n          The directory to store monitoring assets (e.g. dashboard, metric tables).\n        :param output_schema_name: str\n          Schema where output metric tables are created.\n        :param baseline_table_name: str (optional)\n          Name of the baseline table from which drift metrics are computed from. Columns in the monitored\n          table should also be present in the baseline table.\n        :param custom_metrics: List[:class:`MonitorMetric`] (optional)\n          Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics\n          (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).\n        :param data_classification_config: :class:`MonitorDataClassificationConfig` (optional)\n          The data classification config for the monitor.\n        :param inference_log: :class:`MonitorInferenceLog` (optional)\n          Configuration for monitoring inference logs.\n        :param notifications: :class:`MonitorNotifications` (optional)\n          The notification settings for the monitor.\n        :param schedule: :class:`MonitorCronSchedule` (optional)\n          The schedule for automatically updating and refreshing metric tables.\n        :param skip_builtin_dashboard: bool (optional)\n          Whether to skip creating a default dashboard summarizing data quality metrics.\n        :param slicing_exprs: List[str] (optional)\n          List of column expressions to slice data with for targeted analysis. The data is grouped by each\n          expression independently, resulting in a separate slice for each predicate and its complements. For\n          high-cardinality columns, only the top 100 unique values by frequency will generate slices.\n        :param snapshot: :class:`MonitorSnapshot` (optional)\n          Configuration for monitoring snapshot tables.\n        :param time_series: :class:`MonitorTimeSeries` (optional)\n          Configuration for monitoring time series tables.\n        :param warehouse_id: str (optional)\n          Optional argument to specify the warehouse for dashboard creation. If not specified, the first\n          running warehouse will be used.\n        \n        :returns: :class:`MonitorInfo`\n```\n\n----------------------------------------\n\nTITLE: Creating a Cluster Policy in Databricks with Python SDK\nDESCRIPTION: This code snippet demonstrates how to create a new cluster policy using the Databricks SDK for Python. It sets the `spark_conf.spark.databricks.delta.preview.enabled` property to true within the policy definition. The policy is created with a unique name based on the current timestamp and is subsequently deleted.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/cluster_policies.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.cluster_policies.create(\n    name=f\"sdk-{time.time_ns()}\",\n    definition=\"\"\"{\n            \"spark_conf.spark.databricks.delta.preview.enabled\": {\n                \"type\": \"fixed\",\n                \"value\": true\n            }\n        }\n\"\"\",\n)\n\n# cleanup\nw.cluster_policies.delete(policy_id=created.policy_id)\n```\n\n----------------------------------------\n\nTITLE: Creating a Clean Room in Python\nDESCRIPTION: Creates a new clean room with the specified collaborators. This method is asynchronous; the returned name field inside the clean_room field can be used to poll the clean room status. The caller must be a metastore admin or have the **CREATE_CLEAN_ROOM** privilege on the metastore. Takes an optional CleanRoom object as input and returns a CleanRoom object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/cleanrooms/clean_rooms.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create( [, clean_room: Optional[CleanRoom]]) -> CleanRoom\n\n        Create a clean room.\n\n        Create a new clean room with the specified collaborators. This method is asynchronous; the returned\n        name field inside the clean_room field can be used to poll the clean room status, using the\n        :method:cleanrooms/get method. When this method returns, the clean room will be in a PROVISIONING\n        state, with only name, owner, comment, created_at and status populated. The clean room will be usable\n        once it enters an ACTIVE state.\n\n        The caller must be a metastore admin or have the **CREATE_CLEAN_ROOM** privilege on the metastore.\n\n        :param clean_room: :class:`CleanRoom` (optional)\n\n        :returns: :class:`CleanRoom`\n```\n\n----------------------------------------\n\nTITLE: Update Listing in Databricks Marketplace (Python)\nDESCRIPTION: Updates an existing listing in the Databricks Marketplace.  It takes the listing ID (str) and a Listing object as input.  It returns an UpdateListingResponse object. This allows providers to modify existing published listings.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_listings.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nw.provider_listings.update(id: str, listing: Listing) -> UpdateListingResponse\n```\n\n----------------------------------------\n\nTITLE: Uploading a Notebook to Databricks Workspace in Python\nDESCRIPTION: This code snippet demonstrates how to upload a simple Python notebook to a Databricks workspace using the `WorkspaceClient` from the `databricks.sdk` library. It includes creating a workspace client, defining the notebook path, uploading the notebook content, downloading it for verification, and then deleting it. It requires the `io` and `time` modules.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/workspace.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport io\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nnotebook = f\"/Users/{w.current_user.me().user_name}/notebook-{time.time_ns()}.py\"\n\nw.workspace.upload(notebook, io.BytesIO(b\"print(1)\"))\nwith w.workspace.download(notebook) as f:\n    content = f.read()\n    assert content == b\"# Databricks notebook source\\nprint(1)\"\n\nw.workspace.delete(notebook)\n```\n\n----------------------------------------\n\nTITLE: OAuth Authorization Code Flow with PKCE using Flask\nDESCRIPTION: This Python code demonstrates how to implement the OAuth Authorization Code flow with PKCE security using the Flask web framework and the Databricks SDK for Python. It initializes an `OAuthClient`, defines callback and index routes, and uses `RefreshableCredentials` to persist and restore user sessions. It requires the Flask and Databricks SDK libraries.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk.oauth import OAuthClient\noauth_client = OAuthClient(host='<workspace-url>',\n                           client_id='<oauth client ID>',\n                           redirect_url=f'http://host.domain/callback',\n                           scopes=['clusters'])\nimport secrets\nfrom flask import Flask, render_template_string, request, redirect, url_for, session\nAPP_NAME = 'flask-demo'\napp = Flask(APP_NAME)\napp.secret_key = secrets.token_urlsafe(32)\n@app.route('/callback')\ndef callback():\n    from databricks.sdk.oauth import Consent\n    consent = Consent.from_dict(oauth_client, session['consent'])\n    session['creds'] = consent.exchange_callback_parameters(request.args).as_dict()\n    return redirect(url_for('index'))\n@app.route('/')\ndef index():\n    if 'creds' not in session:\n        consent = oauth_client.initiate_consent()\n        session['consent'] = consent.as_dict()\n        return redirect(consent.auth_url)\n    from databricks.sdk import WorkspaceClient\n    from databricks.sdk.oauth import SessionCredentials\n    credentials_provider = SessionCredentials.from_dict(oauth_client, session['creds'])\n    workspace_client = WorkspaceClient(host=oauth_client.host,\n                                       product=APP_NAME,\n                                       credentials_provider=credentials_provider)\n    return render_template_string('...', w=workspace_client)\n```\n\n----------------------------------------\n\nTITLE: Creating a Model in Model Registry with Databricks SDK\nDESCRIPTION: This snippet shows how to create a new registered model with the Databricks SDK. It uses the WorkspaceClient to interact with the model registry and creates a model with a unique name based on the current timestamp.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/model_registry.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nmodel = w.model_registry.create_model(name=f\"sdk-{time.time_ns()}\")\n```\n\n----------------------------------------\n\nTITLE: Getting Log Delivery Configuration by ID in Databricks using Python\nDESCRIPTION: This snippet demonstrates how to retrieve a log delivery configuration by its ID in Databricks using the AccountClient and the LogDeliveryAPI. It first creates a log delivery configuration and then retrieves it by its ID.  It requires the databricks-sdk library and environment variables for AWS credentials.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/log_delivery.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import billing, provisioning\n\na = AccountClient()\n\nbucket = a.storage.create(\n    storage_configuration_name=f\"sdk-{time.time_ns()}\",\n    root_bucket_info=provisioning.RootBucketInfo(bucket_name=f\"sdk-{time.time_ns()}\"),\n)\n\ncreds = a.credentials.create(\n    credentials_name=f\"sdk-{time.time_ns()}\",\n    aws_credentials=provisioning.CreateCredentialAwsCredentials(\n        sts_role=provisioning.CreateCredentialStsRole(role_arn=os.environ[\"TEST_LOGDELIVERY_ARN\"])\n    ),\n)\n\ncreated = a.log_delivery.create(\n    log_delivery_configuration=billing.CreateLogDeliveryConfigurationParams(\n        config_name=f\"sdk-{time.time_ns()}\",\n        credentials_id=creds.credentials_id,\n        storage_configuration_id=bucket.storage_configuration_id,\n        log_type=billing.LogType.AUDIT_LOGS,\n        output_format=billing.OutputFormat.JSON,\n    )\n)\n\nby_id = a.log_delivery.get(log_delivery_configuration_id=created.log_delivery_configuration.config_id)\n\n# cleanup\na.storage.delete(storage_configuration_id=bucket.storage_configuration_id)\na.credentials.delete(credentials_id=creds.credentials_id)\na.log_delivery.patch_status(\n    log_delivery_configuration_id=created.log_delivery_configuration.config_id,\n    status=billing.LogDeliveryConfigStatus.DISABLED,\n)\n```\n\n----------------------------------------\n\nTITLE: Get External Location in Databricks using Python SDK\nDESCRIPTION: This code snippet demonstrates how to retrieve an external location in Databricks using the Python SDK. It showcases the `get` method of the `ExternalLocationsAPI` after creating a storage credential and external location.  It involves creating a storage credential and external location, fetching it by name, and then cleaning up by deleting the created resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/external_locations.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\ncredential = w.storage_credentials.create(\n    name=f\"sdk-{time.time_ns()}\",\n    aws_iam_role=catalog.AwsIamRole(role_arn=os.environ[\"TEST_METASTORE_DATA_ACCESS_ARN\"]),\n)\n\ncreated = w.external_locations.create(\n    name=f\"sdk-{time.time_ns()}\",\n    credential_name=credential.name,\n    url=f's3://{os.environ[\"TEST_BUCKET\"]}/sdk-{time.time_ns()}',\n)\n\n_ = w.external_locations.get(get=created.name)\n\n# cleanup\nw.storage_credentials.delete(delete=credential.name)\nw.external_locations.delete(delete=created.name)\n```\n\n----------------------------------------\n\nTITLE: Get Provider by ID in Databricks Marketplace (Python)\nDESCRIPTION: This method retrieves a single provider from the Databricks Marketplace using its unique ID. It returns a GetProviderResponse object containing the provider's details. The 'id' parameter is the string identifier of the provider to retrieve.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_providers.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: get(id: str) -> GetProviderResponse\n\n    Get a provider.\n\n    Get a provider in the Databricks Marketplace with at least one visible listing.\n\n    :param id: str\n\n    :returns: :class:`GetProviderResponse`\n```\n\n----------------------------------------\n\nTITLE: Updating an Account IP Access List in Python\nDESCRIPTION: Updates an existing IP access list, specified by its ID. A list can include allow lists and block lists. For all allow lists and block lists combined, the API supports a maximum of 1000 IP/CIDR values. If the updated list would block the calling user's current IP, error 400 is returned. It can take a few minutes for the changes to take effect.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/ip_access_lists.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(ip_access_list_id: str [, enabled: Optional[bool], ip_addresses: Optional[List[str]], label: Optional[str], list_type: Optional[ListType]])\n\n    Update access list.\n\n    Updates an existing IP access list, specified by its ID.\n\n    A list can include allow lists and block lists. See the top of this file for a description of how the\n    server treats allow lists and block lists at run time.\n\n    When updating an IP access list:\n\n    * For all allow lists and block lists combined, the API supports a maximum of 1000 IP/CIDR values,\n    where one CIDR counts as a single value. Attempts to exceed that number return error 400 with\n    `error_code` value `QUOTA_EXCEEDED`. * If the updated list would block the calling user's current IP,\n    error 400 is returned with `error_code` value `INVALID_STATE`.\n\n    It can take a few minutes for the changes to take effect.\n\n    :param ip_access_list_id: str\n      The ID for the corresponding IP access list\n    :param enabled: bool (optional)\n      Specifies whether this IP access list is enabled.\n    :param ip_addresses: List[str] (optional)\n    :param label: str (optional)\n      Label for the IP access list. This **cannot** be empty.\n    :param list_type: :class:`ListType` (optional)\n      Type of IP access list. Valid values are as follows and are case-sensitive:\n\n      * `ALLOW`: An allow list. Include this IP or range. * `BLOCK`: A block list. Exclude this IP or\n      range. IP addresses in the block list are excluded even if they are included in an allow list.\n```\n\n----------------------------------------\n\nTITLE: Deleting Notification Destination in Python\nDESCRIPTION: Deletes an existing notification destination, identified by its ID. Requires workspace admin permissions. No return value.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/notification_destinations.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(id: str)\n\n    Delete a notification destination.\n\n    Deletes a notification destination. Requires workspace admin permissions.\n\n    :param id: str\n```\n\n----------------------------------------\n\nTITLE: Getting a Vector Search Index\nDESCRIPTION: This method retrieves a vector search index. It requires the index name as input.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_indexes.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_index(index_name: str) -> VectorIndex\n\n        Get an index.\n\n        Get an index.\n\n        :param index_name: str\n          Name of the index\n\n        :returns: :class:`VectorIndex`\n```\n\n----------------------------------------\n\nTITLE: Delete a directory (Python)\nDESCRIPTION: Deletes an empty directory at the specified directory_path. To delete a non-empty directory, you must first delete all of its contents.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/files/files.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete_directory(directory_path: str)\n\n    Delete a directory.\n\n    Deletes an empty directory.\n\n    To delete a non-empty directory, first delete all of its contents. This can be done by listing the\n    directory contents and deleting each file and subdirectory recursively.\n\n    :param directory_path: str\n      The absolute path of a directory.\n```\n\n----------------------------------------\n\nTITLE: Get Serving Endpoint Permissions - Python\nDESCRIPTION: Retrieves the permissions of a serving endpoint, which can inherit permissions from their root object. The method takes the serving endpoint ID as a parameter.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_permissions(serving_endpoint_id: str) -> ServingEndpointPermissions\n\n    Get serving endpoint permissions.\n\n    Gets the permissions of a serving endpoint. Serving endpoints can inherit permissions from their root\n    object.\n\n    :param serving_endpoint_id: str\n      The serving endpoint for which to get or manage permissions.\n\n    :returns: :class:`ServingEndpointPermissions`\n```\n\n----------------------------------------\n\nTITLE: Defining ColumnTypeName Enum in Python\nDESCRIPTION: This code defines an enumeration `ColumnTypeName` representing various column data types. This includes types like ARRAY, BINARY, BOOLEAN, BYTE, CHAR, DATE, DECIMAL, DOUBLE, FLOAT, GEOGRAPHY, GEOMETRY, INT, INTERVAL, LONG, MAP, NULL, SHORT, STRING, STRUCT, TABLE_TYPE, TIMESTAMP, TIMESTAMP_NTZ, USER_DEFINED_TYPE, and VARIANT. The purpose is to provide a standardized list of data types for columns in tables.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ColumnTypeName\n\n   .. py:attribute:: ARRAY\n      :value: \"ARRAY\"\n\n   .. py:attribute:: BINARY\n      :value: \"BINARY\"\n\n   .. py:attribute:: BOOLEAN\n      :value: \"BOOLEAN\"\n\n   .. py:attribute:: BYTE\n      :value: \"BYTE\"\n\n   .. py:attribute:: CHAR\n      :value: \"CHAR\"\n\n   .. py:attribute:: DATE\n      :value: \"DATE\"\n\n   .. py:attribute:: DECIMAL\n      :value: \"DECIMAL\"\n\n   .. py:attribute:: DOUBLE\n      :value: \"DOUBLE\"\n\n   .. py:attribute:: FLOAT\n      :value: \"FLOAT\"\n\n   .. py:attribute:: GEOGRAPHY\n      :value: \"GEOGRAPHY\"\n\n   .. py:attribute:: GEOMETRY\n      :value: \"GEOMETRY\"\n\n   .. py:attribute:: INT\n      :value: \"INT\"\n\n   .. py:attribute:: INTERVAL\n      :value: \"INTERVAL\"\n\n   .. py:attribute:: LONG\n      :value: \"LONG\"\n\n   .. py:attribute:: MAP\n      :value: \"MAP\"\n\n   .. py:attribute:: NULL\n      :value: \"NULL\"\n\n   .. py:attribute:: SHORT\n      :value: \"SHORT\"\n\n   .. py:attribute:: STRING\n      :value: \"STRING\"\n\n   .. py:attribute:: STRUCT\n      :value: \"STRUCT\"\n\n   .. py:attribute:: TABLE_TYPE\n      :value: \"TABLE_TYPE\"\n\n   .. py:attribute:: TIMESTAMP\n      :value: \"TIMESTAMP\"\n\n   .. py:attribute:: TIMESTAMP_NTZ\n      :value: \"TIMESTAMP_NTZ\"\n\n   .. py:attribute:: USER_DEFINED_TYPE\n      :value: \"USER_DEFINED_TYPE\"\n\n   .. py:attribute:: VARIANT\n      :value: \"VARIANT\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Webhook in Model Registry with Databricks SDK\nDESCRIPTION: This snippet creates a webhook for a model registry event using the Databricks SDK. It configures the webhook to trigger on the MODEL_VERSION_CREATED event and specifies an HTTP URL for the webhook to call.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/model_registry.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import ml\n\nw = WorkspaceClient()\n\ncreated = w.model_registry.create_webhook(\n    description=f\"sdk-{time.time_ns()}\",\n    events=[ml.RegistryWebhookEvent.MODEL_VERSION_CREATED],\n    http_url_spec=ml.HttpUrlSpec(url=w.config.host),\n)\n\n# cleanup\nw.model_registry.delete_webhook(id=created.webhook.id)\n```\n\n----------------------------------------\n\nTITLE: Rotating a Token in Python\nDESCRIPTION: This code snippet illustrates how to rotate a recipient's token using the `rotate_token` method after creating it with the `create` method. It uses `WorkspaceClient` from the `databricks.sdk` to interact with the Databricks workspace. The recipient's name is dynamically generated, and the `existing_token_expire_in_seconds` parameter is set to 0 to expire the existing token immediately. It imports the `time` module to generate a unique name.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/recipients.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.recipients.create(name=f\"sdk-{time.time_ns()}\")\n\nrecipient_info = w.recipients.rotate_token(name=created.name, existing_token_expire_in_seconds=0)\n\n# cleanup\nw.recipients.delete(name=created.name)\n```\n\n----------------------------------------\n\nTITLE: List Databricks Job Runs in Python\nDESCRIPTION: This code snippet demonstrates how to list Databricks job runs using the Python SDK. Similar to the 'List Databricks Jobs' example, it sets up a basic job, lists associated runs, and then cleans up resources.  This is a demonstration of listing runs for a specific job.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\ncreated_job = w.jobs.create(\n    name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.Task(\n            description=\"test\",\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=\"test\",\n            timeout_seconds=0,\n        )\n    ],\n)\n\nrun_list = w.jobs.list_runs(job_id=created_job.job_id)\n\n# cleanup\nw.jobs.delete(job_id=created_job.job_id)\n```\n\n----------------------------------------\n\nTITLE: Creating Network Connectivity Configuration in Databricks\nDESCRIPTION: This method creates a network connectivity configuration within Databricks. It requires a name and a region. The name must be between 3 and 30 characters, containing alphanumeric characters, hyphens, and underscores, matching the regular expression `^[0-9a-zA-Z-_]{3,30}$`. The region specifies where the configuration is located.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/network_connectivity.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nNetworkConnectivityAPI.create_network_connectivity_configuration(name: str, region: str) -> NetworkConnectivityConfiguration\n```\n\n----------------------------------------\n\nTITLE: Updating a Table Owner - Python\nDESCRIPTION: This method allows for updating the owner of a table. The caller must have the necessary privileges, such as ownership of the parent catalog and schema, or ownership of the table itself, along with the required USE privileges. The `full_name` parameter specifies the table to update, and the `owner` parameter specifies the new owner.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/tables.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: update(full_name: str [, owner: Optional[str]])\n\n        Update a table owner.\n\n        Change the owner of the table. The caller must be the owner of the parent catalog, have the\n        **USE_CATALOG** privilege on the parent catalog and be the owner of the parent schema, or be the owner\n        of the table and have the **USE_CATALOG** privilege on the parent catalog and the **USE_SCHEMA**\n        privilege on the parent schema.\n\n        :param full_name: str\n          Full name of the table.\n        :param owner: str (optional)\n```\n\n----------------------------------------\n\nTITLE: Getting Service Principal Details\nDESCRIPTION: This snippet shows how to retrieve details of a service principal using its ID. It first creates a service principal, then retrieves its details using `a.service_principals.get(id=sp_create.id)`. Finally, it cleans up by deleting the created service principal.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/service_principals.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nsp_create = a.service_principals.create(active=True, display_name=f\"sdk-{time.time_ns()}\")\n\nsp = a.service_principals.get(id=sp_create.id)\n\n# cleanup\na.service_principals.delete(id=sp_create.id)\n```\n\n----------------------------------------\n\nTITLE: Listing Storage Credentials in Databricks\nDESCRIPTION: This snippet illustrates how to list all storage credentials accessible to the user using the WorkspaceClient. It initializes the WorkspaceClient and then calls the `list` method of the `storage_credentials` API to retrieve all available storage credentials.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/storage_credentials.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\nall = w.storage_credentials.list(catalog.ListStorageCredentialsRequest())\n```\n\n----------------------------------------\n\nTITLE: Listing VPC Endpoint Configurations with AccountClient in Python\nDESCRIPTION: This code lists all VPC endpoint configurations for an account using the AccountClient.  It iterates through the results and demonstrates retrieving all the existing configurations.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/vpc_endpoints.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nall = a.vpc_endpoints.list()\n```\n\n----------------------------------------\n\nTITLE: Edit Instance Profile using Databricks SDK in Python\nDESCRIPTION: Edits an existing instance profile within Databricks, specifically to update the associated IAM role ARN. This is particularly useful when the role and instance profile names do not match and the profile is used with Databricks SQL Serverless.  The example shows how to instantiate a WorkspaceClient and use the instance_profiles.edit method with the instance profile ARN and the updated IAM role ARN.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/instance_profiles.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\narn = \"arn:aws:iam::000000000000:instance-profile/abc\"\n\nw.instance_profiles.edit(\n    instance_profile_arn=arn,\n    iam_role_arn=\"arn:aws:iam::000000000000:role/bcdf\",\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Databricks Clusters using Profile (Python)\nDESCRIPTION: This code snippet demonstrates how to authenticate with Azure Databricks using a profile defined in the Databricks configuration file. It initializes a `WorkspaceClient` with the `profile` argument referencing the section in the configuration file.\nDependencies: databricks-sdk, ~/.databrickscfg\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient(profile='spn')\nclusters = w.clusters.list()\nfor cl in clusters:\n    print(f' - {cl.cluster_name} is {cl.state}')\n```\n\n----------------------------------------\n\nTITLE: Listing Vector Search Indexes\nDESCRIPTION: This method lists all vector search indexes in a given endpoint. It requires the endpoint name as input and optionally accepts a page token for pagination.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_indexes.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list_indexes(endpoint_name: str [, page_token: Optional[str]]) -> Iterator[MiniVectorIndex]\n\n        List indexes.\n\n        List all indexes in the given endpoint.\n\n        :param endpoint_name: str\n          Name of the endpoint\n        :param page_token: str (optional)\n          Token for pagination\n\n        :returns: Iterator over :class:`MiniVectorIndex`\n```\n\n----------------------------------------\n\nTITLE: Replace IP Access List with Databricks SDK\nDESCRIPTION: This code shows how to replace an IP access list using the Databricks SDK. It creates a new IP access list, then replaces it with a new configuration.  Finally, it deletes the IP Access List.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/ip_access_lists.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import settings\n\nw = WorkspaceClient()\n\ncreated = w.ip_access_lists.create(\n    label=f\"sdk-{time.time_ns()}\",\n    ip_addresses=[\"1.0.0.0/16\"],\n    list_type=settings.ListType.BLOCK,\n)\n\nw.ip_access_lists.replace(\n    ip_access_list_id=created.ip_access_list.list_id,\n    label=f\"sdk-{time.time_ns()}\",\n    ip_addresses=[\"1.0.0.0/24\"],\n    list_type=settings.ListType.BLOCK,\n    enabled=False,\n)\n\n# cleanup\nw.ip_access_lists.delete(ip_access_list_id=created.ip_access_list.list_id)\n```\n\n----------------------------------------\n\nTITLE: Exception Handling with DatabricksError subtypes in Python\nDESCRIPTION: This code demonstrates improved exception handling using specific subtypes of `DatabricksError` in the Databricks SDK for Python. Instead of catching the generic `DatabricksError` and checking the `error_code`, it catches specific exceptions like `PermissionDenied`, `FeatureDisabled`, and `NotFound`. This makes the code more readable, maintainable, and easier to handle different error scenarios.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntry:\n  return (self._ws\n    .permissions\n    .get(object_type, object_id))\nexcept DatabricksError as e:\n  if e.error_code in [\n    \"RESOURCE_DOES_NOT_EXIST\",\n    \"RESOURCE_NOT_FOUND\",\n    \"PERMISSION_DENIED\",\n    \"FEATURE_DISABLED\",\n    \"BAD_REQUEST\"]:\n    logger.warning(...)\n    return None\n  raise RetryableError(...) from e\n```\n\nLANGUAGE: python\nCODE:\n```\ntry:\n  return (self._ws\n    .permissions\n    .get(object_type, object_id))\nexcept PermissionDenied, FeatureDisabled:\n  logger.warning(...)\n  return None\nexcept NotFound:\n  raise RetryableError(...)\n```\n\n----------------------------------------\n\nTITLE: Replacing an Account IP Access List in Python\nDESCRIPTION: Replaces an IP access list, specified by its ID. A list can include allow lists and block lists. For all allow lists and block lists combined, the API supports a maximum of 1000 IP/CIDR values, where one CIDR counts as a single value. If the resulting list would block the calling user's current IP, error 400 is returned. It can take a few minutes for the changes to take effect.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/ip_access_lists.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: replace(ip_access_list_id: str, label: str, list_type: ListType, enabled: bool [, ip_addresses: Optional[List[str]]])\n\n    Replace access list.\n\n    Replaces an IP access list, specified by its ID.\n\n    A list can include allow lists and block lists. See the top of this file for a description of how the\n    server treats allow lists and block lists at run time. When replacing an IP access list: * For all\n    allow lists and block lists combined, the API supports a maximum of 1000 IP/CIDR values, where one\n    CIDR counts as a single value. Attempts to exceed that number return error 400 with `error_code` value\n    `QUOTA_EXCEEDED`. * If the resulting list would block the calling user's current IP, error 400 is\n    returned with `error_code` value `INVALID_STATE`. It can take a few minutes for the changes to take\n    effect.\n\n    :param ip_access_list_id: str\n      The ID for the corresponding IP access list\n    :param label: str\n      Label for the IP access list. This **cannot** be empty.\n    :param list_type: :class:`ListType`\n      Type of IP access list. Valid values are as follows and are case-sensitive:\n\n      * `ALLOW`: An allow list. Include this IP or range. * `BLOCK`: A block list. Exclude this IP or\n      range. IP addresses in the block list are excluded even if they are included in an allow list.\n    :param enabled: bool\n      Specifies whether this IP access list is enabled.\n    :param ip_addresses: List[str] (optional)\n```\n\n----------------------------------------\n\nTITLE: Update a Share in Databricks\nDESCRIPTION: This snippet demonstrates how to update a share in Databricks using the SharesAPI. It creates a catalog, schema, and table, then creates a share and adds the table to the share using the update method. The updates parameter takes a list of SharedDataObjectUpdate objects. The code also includes cleanup operations to delete the created resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/shares.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sharing\n\nw = WorkspaceClient()\n\ntable_name = f\"sdk-{time.time_ns()}\"\n\ncreated_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated_schema = w.schemas.create(name=f\"sdk-{time.time_ns()}\", catalog_name=created_catalog.name)\n\n_ = w.statement_execution.execute(\n    warehouse_id=os.environ[\"TEST_DEFAULT_WAREHOUSE_ID\"],\n    catalog=created_catalog.name,\n    schema=created_schema.name,\n    statement=\"CREATE TABLE %s TBLPROPERTIES (delta.enableDeletionVectors=false) AS SELECT 2+2 as four\" % (table_name),\n).result()\n\ntable_full_name = \"%s.%s.%s\" % (\n    created_catalog.name,\n    created_schema.name,\n    table_name,\n)\n\ncreated_share = w.shares.create(name=f\"sdk-{time.time_ns()}\")\n\n_ = w.shares.update(\n    name=created_share.name,\n    updates=[\n        sharing.SharedDataObjectUpdate(\n            action=sharing.SharedDataObjectUpdateAction.ADD,\n            data_object=sharing.SharedDataObject(name=table_full_name, data_object_type=\"TABLE\"),\n        )\n    ],\n)\n\n# cleanup\nw.schemas.delete(full_name=created_schema.full_name)\nw.catalogs.delete(name=created_catalog.name, force=True)\nw.tables.delete(full_name=table_full_name)\nw.shares.delete(name=created_share.name)\n```\n\n----------------------------------------\n\nTITLE: Setting App Permissions in Databricks using Python\nDESCRIPTION: This code snippet demonstrates how to set permissions on an app using the `set_permissions` method. It accepts the app name and an optional list of `AppAccessControlRequest` objects representing the access control list. The method returns an `AppPermissions` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: set_permissions(app_name: str [, access_control_list: Optional[List[AppAccessControlRequest]]]) -> AppPermissions\n\n        Set app permissions.\n\n        Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct\n        permissions if none are specified. Objects can inherit permissions from their root object.\n\n        :param app_name: str\n          The app for which to get or manage permissions.\n        :param access_control_list: List[:class:`AppAccessControlRequest`] (optional)\n\n        :returns: :class:`AppPermissions`\n```\n\n----------------------------------------\n\nTITLE: Listing workspace objects in Python\nDESCRIPTION: This code snippet demonstrates how to list workspace objects in a Databricks workspace using the WorkspaceClient. It iterates through the objects under a specified path and appends their paths to a list. Asserts that the length of the names array is greater than 0. Requires databricks SDK.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/workspace.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nnames = []\nfor i in w.workspace.list(f\"/Users/{w.current_user.me().user_name}\", recursive=True):\n    names.append(i.path)\nassert len(names) > 0\n```\n\n----------------------------------------\n\nTITLE: Migrating Permissions with PermissionMigrationAPI in Python\nDESCRIPTION: This snippet describes the `migrate_permissions` method within the PermissionMigrationAPI. It migrates permissions from a workspace group to an account group within a Databricks workspace.  The function takes workspace ID, source workspace group name, destination account group name, and an optional size parameter to limit the number of migrated permissions.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/permission_migration.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: migrate_permissions(workspace_id: int, from_workspace_group_name: str, to_account_group_name: str [, size: Optional[int]]) -> MigratePermissionsResponse\n\n        Migrate Permissions.\n\n        :param workspace_id: int\n          WorkspaceId of the associated workspace where the permission migration will occur.\n        :param from_workspace_group_name: str\n          The name of the workspace group that permissions will be migrated from.\n        :param to_account_group_name: str\n          The name of the account group that permissions will be migrated to.\n        :param size: int (optional)\n          The maximum number of permissions that will be migrated.\n\n        :returns: :class:`MigratePermissionsResponse`\n```\n\n----------------------------------------\n\nTITLE: Creating a Dashboard using Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to create a dashboard using the Databricks SDK. It imports the necessary modules, initializes the WorkspaceClient, creates a dashboard with a unique name, and then cleans up by deleting the created dashboard. The dashboard's name is dynamically generated using the current timestamp.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/dashboards.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.dashboards.create(name=f\"sdk-{time.time_ns()}\")\n\n# cleanup\nw.dashboards.delete(dashboard_id=created.id)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Personalization Request - Python\nDESCRIPTION: Retrieves an existing personalization request for a specific marketplace listing, identified by its listing ID. Each consumer can have at most one personalization request per listing. Returns a GetPersonalizationRequestResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_personalization_requests.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nw.consumer_personalization_requests.get(listing_id: str) -> GetPersonalizationRequestResponse\n```\n\n----------------------------------------\n\nTITLE: Listing Secrets Using dbutils from runtime in Python\nDESCRIPTION: This snippet demonstrates how to list secret scopes and their metadata using `dbutils.secrets.listScopes()` and `dbutils.secrets.list()` methods. It imports `dbutils` from `databricks.sdk.runtime` and iterates through the secret scopes to print the names of the secrets found. All configuration must be present in environment variables.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbutils.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk.runtime import dbutils\nfor secret_scope in dbutils.secrets.listScopes():\n    for secret_metadata in dbutils.secrets.list(secret_scope.name):\n        print(f'found {secret_metadata.key} secret in {secret_scope.name} scope')\n```\n\n----------------------------------------\n\nTITLE: Adding Pipeline Permissions Methods - Python\nDESCRIPTION: This snippet introduces methods for managing pipeline permissions within the Databricks workspace via the `w.pipelines` service.  The added methods are: `get_pipeline_permission_levels()`, `get_pipeline_permissions()`, `set_pipeline_permissions()`, and `update_pipeline_permissions()`. These methods enable users to retrieve, set, and update permissions for data pipelines.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nw.pipelines.get_pipeline_permission_levels()\nw.pipelines.get_pipeline_permissions()\nw.pipelines.set_pipeline_permissions()\nw.pipelines.update_pipeline_permissions()\n```\n\n----------------------------------------\n\nTITLE: Create Network Configuration with AccountClient in Python\nDESCRIPTION: This code snippet demonstrates how to create a Databricks network configuration using the AccountClient. It initializes the AccountClient, then calls the networks.create method with necessary parameters like network_name, vpc_id, subnet_ids, and security_group_ids. It leverages the time module to generate unique names and IDs.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/networks.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nnetw = a.networks.create(\n    network_name=f\"sdk-{time.time_ns()}\",\n    vpc_id=hex(time.time_ns())[2:],\n    subnet_ids=[hex(time.time_ns())[2:], hex(time.time_ns())[2:]],\n    security_group_ids=[hex(time.time_ns())[2:]],\n)\n```\n\n----------------------------------------\n\nTITLE: Getting a Full Query Result Download in Genie using Python\nDESCRIPTION: This method retrieves the result of a full SQL query download from Genie, using the provided download ID. It allows polling of the download progress and retrieving external links to the query result upon completion. Databricks strongly recommends protecting the returned URLs. The method returns a GenieGetDownloadFullQueryResultResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/genie.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: get_download_full_query_result(space_id: str, conversation_id: str, message_id: str, attachment_id: str, download_id: str) -> GenieGetDownloadFullQueryResultResponse\n\n        Get download full query result.\n\n        After [Generating a Full Query Result Download](:method:genie/getdownloadfullqueryresult) and\n        successfully receiving a `download_id`, use this API to Poll download progress and retrieve the SQL\n        query result external link(s) upon completion. Warning: Databricks strongly recommends that you\n        protect the URLs that are returned by the `EXTERNAL_LINKS` disposition. When you use the\n        `EXTERNAL_LINKS` disposition, a short-lived, presigned URL is generated, which can be used to download\n        the results directly from Amazon S3. As a short-lived access credential is embedded in this presigned\n        URL, you should protect the URL. Because presigned URLs are already generated with embedded temporary\n        access credentials, you must not set an Authorization header in the download requests. See [Execute\n        Statement](:method:statementexecution/executestatement) for more details.\n\n        :param space_id: str\n          Space ID\n        :param conversation_id: str\n          Conversation ID\n        :param message_id: str\n          Message ID\n        :param attachment_id: str\n          Attachment ID\n        :param download_id: str\n          Download ID. This ID is provided by the [Generate Download\n          endpoint](:method:genie/generateDownloadFullQueryResult)\n\n        :returns: :class:`GenieGetDownloadFullQueryResultResponse`\n```\n\n----------------------------------------\n\nTITLE: Listing SQL Warehouses with WorkspaceClient in Python\nDESCRIPTION: This code snippet demonstrates how to retrieve a list of SQL warehouses using the `WorkspaceClient` and the `data_sources.list()` method in the Databricks SDK. It showcases the necessary imports and initialization steps.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/data_sources.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nsrcs = w.data_sources.list()\n```\n\n----------------------------------------\n\nTITLE: Delete Account IP Access Toggle Setting (Python)\nDESCRIPTION: Deletes the account IP access toggle setting, reverting it to the default ON state. Accepts an optional etag for optimistic concurrency control to prevent overwriting during simultaneous writes.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/enable_ip_access_lists.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\na.settings.enable_ip_access_lists.delete(etag: Optional[str]) -> DeleteAccountIpAccessEnableResponse\n```\n\n----------------------------------------\n\nTITLE: Defining Enum: EgressNetworkPolicyInternetAccessPolicyLogOnlyModeLogOnlyModeType - Python\nDESCRIPTION: This Python enum defines the types of services to be logged in log-only mode for Internet access policies.  It allows for logging all services or only selected services, providing flexibility in monitoring and auditing network access.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EgressNetworkPolicyInternetAccessPolicyLogOnlyModeLogOnlyModeType\n\n   .. py:attribute:: ALL_SERVICES\n      :value: \"ALL_SERVICES\"\n\n   .. py:attribute:: SELECTED_SERVICES\n      :value: \"SELECTED_SERVICES\"\n```\n\n----------------------------------------\n\nTITLE: Creating Budget Configuration with Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to create a new budget configuration using the `create` method of the `BudgetsAPI` class. It includes setting up budget filters and alert configurations. A cleanup step is included to delete the created budget.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/budgets.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import billing\n\na = AccountClient()\n\ncreated = a.budgets.create(\n    budget=billing.CreateBudgetConfigurationBudget(\n        display_name=f\"sdk-{time.time_ns()}\",\n        filter=billing.BudgetConfigurationFilter(\n            tags=[\n                billing.BudgetConfigurationFilterTagClause(\n                    key=\"tagName\",\n                    value=billing.BudgetConfigurationFilterClause(\n                        operator=billing.BudgetConfigurationFilterOperator.IN,\n                        values=[\"all\"],\n                    ),\n                )\n            ]\n        ),\n        alert_configurations=[\n            billing.CreateBudgetConfigurationBudgetAlertConfigurations(\n                time_period=billing.AlertConfigurationTimePeriod.MONTH,\n                quantity_type=billing.AlertConfigurationQuantityType.LIST_PRICE_DOLLARS_USD,\n                trigger_type=billing.AlertConfigurationTriggerType.CUMULATIVE_SPENDING_EXCEEDED,\n                quantity_threshold=\"100\",\n                action_configurations=[\n                    billing.CreateBudgetConfigurationBudgetActionConfigurations(\n                        action_type=billing.ActionConfigurationType.EMAIL_NOTIFICATION,\n                        target=\"admin@example.com\",\n                    )\n                ],\n            )\n        ],\n    )\n)\n\n# cleanup\na.budgets.delete(budget_id=created.budget.budget_configuration_id)\n```\n\n----------------------------------------\n\nTITLE: Getting Group Details with Groups API in Python\nDESCRIPTION: This snippet demonstrates how to retrieve details of a specific group from a Databricks workspace using the `get` method of the `GroupsAPI` class. It requires the `databricks-sdk` library and uses the `WorkspaceClient` to interact with the Databricks API. The snippet first creates a group, then retrieves its details using its ID, and finally deletes the group.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/groups.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ngroup = w.groups.create(display_name=f\"sdk-{time.time_ns()}\")\n\nfetch = w.groups.get(id=group.id)\n\n# cleanup\nw.groups.delete(id=group.id)\n```\n\n----------------------------------------\n\nTITLE: Create Service Principal Secret (Python)\nDESCRIPTION: Creates a secret for a given service principal.  The `service_principal_id` parameter specifies the ID of the service principal. The optional `lifetime` parameter sets the secret's lifespan in seconds, defaulting to 730 days if not provided. Returns a `CreateServicePrincipalSecretResponse` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/service_principal_secrets.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(service_principal_id: int [, lifetime: Optional[str]]) -> CreateServicePrincipalSecretResponse\n\n    Create service principal secret.\n\n    Create a secret for the given service principal.\n\n    :param service_principal_id: int\n      The service principal ID.\n    :param lifetime: str (optional)\n      The lifetime of the secret in seconds. If this parameter is not provided, the secret will have a\n      default lifetime of 730 days (63072000s).\n\n    :returns: :class:`CreateServicePrincipalSecretResponse`\n```\n\n----------------------------------------\n\nTITLE: Settings API Property: aibi_dashboard_embedding_access_policy\nDESCRIPTION: This property controls whether AI/BI published dashboard embedding is enabled, conditionally enabled, or disabled at the workspace level. The default setting is conditionally enabled (ALLOW_APPROVED_DOMAINS).\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/settings.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:property:: aibi_dashboard_embedding_access_policy\n    :type: AibiDashboardEmbeddingAccessPolicyAPI\n\n    Controls whether AI/BI published dashboard embedding is enabled, conditionally enabled, or disabled at the\n    workspace level. By default, this setting is conditionally enabled (ALLOW_APPROVED_DOMAINS).\n```\n\n----------------------------------------\n\nTITLE: Retrieving Listing Content Metadata with ConsumerFulfillments.get() in Python\nDESCRIPTION: This method retrieves a high-level preview of the metadata of installable content for a given listing ID. It returns an iterator over SharedDataObject instances, providing metadata about the listing's installable content. Optional parameters include page_size and page_token for pagination.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_fulfillments.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get(listing_id: str [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[SharedDataObject]:\n    \"\"\"Get listing content metadata.\n\n    Get a high level preview of the metadata of listing installable content.\n\n    :param listing_id: str\n    :param page_size: int (optional)\n    :param page_token: str (optional)\n\n    :returns: Iterator over :class:`SharedDataObject`\n    \"\"\n```\n\n----------------------------------------\n\nTITLE: Export Metrics for Serving Endpoint - Python\nDESCRIPTION: Retrieves the metrics associated with a serving endpoint in either Prometheus or OpenMetrics exposition format. The method takes the serving endpoint name as a parameter.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: export_metrics(name: str) -> ExportMetricsResponse\n\n    Get metrics of a serving endpoint.\n\n    Retrieves the metrics associated with the provided serving endpoint in either Prometheus or\n    OpenMetrics exposition format.\n\n    :param name: str\n      The name of the serving endpoint to retrieve metrics for. This field is required.\n\n    :returns: :class:`ExportMetricsResponse`\n```\n\n----------------------------------------\n\nTITLE: Getting App Permissions in Databricks using Python\nDESCRIPTION: This code snippet retrieves the permissions of an app using the `get_permissions` method. It accepts the app name as a parameter and returns an `AppPermissions` object representing the app's permissions.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_permissions(app_name: str) -> AppPermissions\n\n        Get app permissions.\n\n        Gets the permissions of an app. Apps can inherit permissions from their root object.\n\n        :param app_name: str\n          The app for which to get or manage permissions.\n\n        :returns: :class:`AppPermissions`\n```\n\n----------------------------------------\n\nTITLE: Creating IP Access List with Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to create an IP access list using the Databricks SDK. It initializes a WorkspaceClient, creates an IP access list with a specified label, IP address range, and list type (BLOCK), and then cleans up by deleting the created list.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/ip_access_lists.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import settings\n\nw = WorkspaceClient()\n\ncreated = w.ip_access_lists.create(\n    label=f\"sdk-{time.time_ns()}\",\n    ip_addresses=[\"1.0.0.0/16\"],\n    list_type=settings.ListType.BLOCK,\n)\n\n# cleanup\nw.ip_access_lists.delete(ip_access_list_id=created.ip_access_list.list_id)\n```\n\n----------------------------------------\n\nTITLE: Getting Private Endpoint Rule in Databricks\nDESCRIPTION: This method retrieves a private endpoint rule. It takes the network connectivity configuration ID and the private endpoint rule ID as parameters. It returns the NccAzurePrivateEndpointRule object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/network_connectivity.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nNetworkConnectivityAPI.get_private_endpoint_rule(network_connectivity_config_id: str, private_endpoint_rule_id: str) -> NccAzurePrivateEndpointRule\n```\n\n----------------------------------------\n\nTITLE: Adding Model Registry Permissions Methods - Python\nDESCRIPTION: This snippet introduces methods for managing registered model permissions within the Databricks workspace. Specifically, it adds `set_registered_model_permissions()` and `update_registered_model_permissions()` to the `w.model_registry` service. These methods allow users to set and update permissions for registered models.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nw.model_registry.set_registered_model_permissions()\nw.model_registry.update_registered_model_permissions()\n```\n\n----------------------------------------\n\nTITLE: Delete a Serving Endpoint - Python\nDESCRIPTION: Deletes an existing serving endpoint. It requires the name of the endpoint to be deleted.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(name: str)\n\n    Delete a serving endpoint.\n\n    :param name: str\n```\n\n----------------------------------------\n\nTITLE: Publishing Lakeview Dashboard\nDESCRIPTION: Publishes a draft Lakeview dashboard using the LakeviewAPI. Requires the dashboard ID. It allows specifying if the publisher's credentials should be embedded and overriding the warehouse ID. It returns the published Dashboard object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: publish(dashboard_id: str [, embed_credentials: Optional[bool], warehouse_id: Optional[str]]) -> PublishedDashboard\n\n    Publish dashboard.\n\n    Publish the current draft dashboard.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard to be published.\n    :param embed_credentials: bool (optional)\n      Flag to indicate if the publisher's credentials should be embedded in the published dashboard. These\n      embedded credentials will be used to execute the published dashboard's queries.\n    :param warehouse_id: str (optional)\n      The ID of the warehouse that can be used to override the warehouse which was set in the draft.\n\n    :returns: :class:`PublishedDashboard`\n```\n\n----------------------------------------\n\nTITLE: Updating Budget Configuration with Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to update an existing budget configuration using the `update` method of the `BudgetsAPI` class. It includes updating the display name, filter, and alert configurations. A cleanup step to delete the created budget is included.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/budgets.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import billing\n\na = AccountClient()\n\ncreated = a.budgets.create(\n    budget=billing.CreateBudgetConfigurationBudget(\n        display_name=f\"sdk-{time.time_ns()}\",\n        filter=billing.BudgetConfigurationFilter(\n            tags=[\n                billing.BudgetConfigurationFilterTagClause(\n                    key=\"tagName\",\n                    value=billing.BudgetConfigurationFilterClause(\n                        operator=billing.BudgetConfigurationFilterOperator.IN,\n                        values=[\"all\"],\n                    ),\n                )\n            ]\n        ),\n        alert_configurations=[\n            billing.CreateBudgetConfigurationBudgetAlertConfigurations(\n                time_period=billing.AlertConfigurationTimePeriod.MONTH,\n                quantity_type=billing.AlertConfigurationQuantityType.LIST_PRICE_DOLLARS_USD,\n                trigger_type=billing.AlertConfigurationTriggerType.CUMULATIVE_SPENDING_EXCEEDED,\n                quantity_threshold=\"100\",\n                action_configurations=[\n                    billing.CreateBudgetConfigurationBudgetActionConfigurations(\n                        action_type=billing.ActionConfigurationType.EMAIL_NOTIFICATION,\n                        target=\"admin@example.com\",\n                    )\n                ],\n            )\n        ],\n    )\n)\n\n_ = a.budgets.update(\n    budget_id=created.budget.budget_configuration_id,\n    budget=billing.UpdateBudgetConfigurationBudget(\n        budget_configuration_id=created.budget.budget_configuration_id,\n        display_name=f\"sdk-{time.time_ns()}\",\n        filter=billing.BudgetConfigurationFilter(\n            tags=[\n                billing.BudgetConfigurationFilterTagClause(\n                    key=\"tagName\",\n                    value=billing.BudgetConfigurationFilterClause(\n                        operator=billing.BudgetConfigurationFilterOperator.IN,\n                        values=[\"all\"],\n                    ),\n                )\n            ]\n        ),\n        alert_configurations=[\n            billing.AlertConfiguration(\n                alert_configuration_id=created.budget.alert_configurations[0].alert_configuration_id,\n                time_period=billing.AlertConfigurationTimePeriod.MONTH,\n                quantity_type=billing.AlertConfigurationQuantityType.LIST_PRICE_DOLLARS_USD,\n                trigger_type=billing.AlertConfigurationTriggerType.CUMULATIVE_SPENDING_EXCEEDED,\n                quantity_threshold=\"50\",\n                action_configurations=created.budget.alert_configurations[0].action_configurations,\n            )\n        ],\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Lakeview Dashboard\nDESCRIPTION: Updates a draft Lakeview dashboard using the LakeviewAPI. Requires the dashboard ID. It allows specifying dashboard properties to update. It returns the updated Dashboard object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: update(dashboard_id: str [, dashboard: Optional[Dashboard]]) -> Dashboard\n\n    Update dashboard.\n\n    Update a draft dashboard.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard.\n    :param dashboard: :class:`Dashboard` (optional)\n\n    :returns: :class:`Dashboard`\n```\n\n----------------------------------------\n\nTITLE: Delete Provider - Python\nDESCRIPTION: Deletes an existing provider from the Databricks Marketplace. It requires the provider's ID as a string. No return value.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_providers.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(id: str)\n\n    Delete provider.\n\n    Delete provider\n\n    :param id: str\n```\n\n----------------------------------------\n\nTITLE: Download a file (Python)\nDESCRIPTION: Downloads a file from the specified file_path. The file contents are returned as the response body and it supports Range and If-Unmodified-Since HTTP headers.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/files/files.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: download(file_path: str) -> DownloadResponse\n\n    Download a file.\n\n    Downloads a file. The file contents are the response body. This is a standard HTTP file download, not\n    a JSON RPC. It supports the Range and If-Unmodified-Since HTTP headers.\n\n    :param file_path: str\n      The absolute path of the file.\n\n    :returns: :class:`DownloadResponse`\n```\n\n----------------------------------------\n\nTITLE: Creating a Legacy Query in Databricks SQL\nDESCRIPTION: This method creates a new query definition in Databricks SQL. It requires the data_source_id which corresponds to the ID of the SQL warehouse, along with optional parameters like description, name, query text, and tags.  The newly created query belongs to the authenticated user.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/queries_legacy.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create( [, data_source_id: Optional[str], description: Optional[str], name: Optional[str], options: Optional[Any], parent: Optional[str], query: Optional[str], run_as_role: Optional[RunAsRole], tags: Optional[List[str]]]) -> LegacyQuery\n```\n\n----------------------------------------\n\nTITLE: Deleting a Function in Databricks\nDESCRIPTION: Deletes a specified function from Unity Catalog. The user must be the owner of the function, its parent schema (with USE_CATALOG privilege), or its parent catalog. The name parameter specifies the fully-qualified name of the function. The force parameter is optional and allows deletion even if the function is not empty.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/functions.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(name: str [, force: Optional[bool]])\n\n    Delete a function.\n\n    Deletes the function that matches the supplied name. For the deletion to succeed, the user must\n    satisfy one of the following conditions: - Is the owner of the function's parent catalog - Is the\n    owner of the function's parent schema and have the **USE_CATALOG** privilege on its parent catalog -\n    Is the owner of the function itself and have both the **USE_CATALOG** privilege on its parent catalog\n    and the **USE_SCHEMA** privilege on its parent schema\n\n    :param name: str\n      The fully-qualified name of the function (of the form\n      __catalog_name__.__schema_name__.__function__name__).\n    :param force: bool (optional)\n      Force deletion even if the function is notempty.\n```\n\n----------------------------------------\n\nTITLE: Getting assignable roles for a resource - Python\nDESCRIPTION: This method retrieves all the roles that can be granted on a specific account-level resource. The resource name must be provided as input, and the method returns a GetAssignableRolesForResourceResponse object containing the list of assignable roles.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/access_control.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nAccountAccessControlAPI.get_assignable_roles_for_resource(resource: str) -> GetAssignableRolesForResourceResponse\n```\n\n----------------------------------------\n\nTITLE: Unpinning a Cluster with WorkspaceClient\nDESCRIPTION: This code snippet demonstrates how to unpin a Databricks cluster using the `unpin` method of the `WorkspaceClient`. It involves creating a cluster, unpinning it, and then deleting the cluster to clean up. It utilizes `os` for accessing environment variables and `time` to generate unique cluster names.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nlatest = w.clusters.select_spark_version(latest=True, long_term_support=True)\n\ncluster_name = f\"sdk-{time.time_ns()}\"\n\nclstr = w.clusters.create(\n    cluster_name=cluster_name,\n    spark_version=latest,\n    instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n    autotermination_minutes=15,\n    num_workers=1,\n).result()\n\nw.clusters.unpin(cluster_id=clstr.cluster_id)\n\n# cleanup\nw.clusters.permanent_delete(cluster_id=clstr.cluster_id)\n```\n\n----------------------------------------\n\nTITLE: Get Endpoint in Databricks\nDESCRIPTION: Retrieves information about a specific vector search endpoint. Requires the endpoint name. Returns an EndpointInfo object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_endpoints.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nw.vector_search_endpoints.get_endpoint(endpoint_name: str) -> EndpointInfo\n```\n\n----------------------------------------\n\nTITLE: Listing Experiments with WorkspaceClient in Python\nDESCRIPTION: This code snippet demonstrates how to list experiments using the `WorkspaceClient` from the `databricks.sdk` and the `ListExperimentsRequest` from `databricks.sdk.service.ml`. It initializes the workspace client, creates a `ListExperimentsRequest` and retrieves all the experiments.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/experiments.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import ml\n\nw = WorkspaceClient()\n\nall = w.experiments.list_experiments(ml.ListExperimentsRequest())\n```\n\n----------------------------------------\n\nTITLE: List Databricks Jobs in Python\nDESCRIPTION: This code snippet shows how to list Databricks jobs using the Python SDK. It creates a job, lists the runs associated with that job, and then cleans up by deleting the created job. This provides a basic example of listing jobs after creation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\ncreated_job = w.jobs.create(\n    name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.Task(\n            description=\"test\",\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=\"test\",\n            timeout_seconds=0,\n        )\n    ],\n)\n\nrun_list = w.jobs.list_runs(job_id=created_job.job_id)\n\n# cleanup\nw.jobs.delete(job_id=created_job.job_id)\n```\n\n----------------------------------------\n\nTITLE: Listing Pipelines in Databricks\nDESCRIPTION: This snippet shows how to list pipelines within a Databricks workspace using the Databricks SDK for Python. It initializes a `WorkspaceClient` and uses the `list_pipelines` method to retrieve a list of pipelines. The result is an iterator over `PipelineStateInfo` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/pipelines/pipelines.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import pipelines\n\nw = WorkspaceClient()\n\nall = w.pipelines.list_pipelines(pipelines.ListPipelinesRequest())\n```\n\n----------------------------------------\n\nTITLE: Get Workspace Configuration Status in Databricks\nDESCRIPTION: Retrieves the configuration status for a Databricks workspace using the `get_status` method of the `WorkspaceConfAPI`. It requires the Databricks SDK and a `WorkspaceClient` instance. The method takes a string `keys` as input, specifying the configuration keys to retrieve. The method returns a `Dict[str, str]` containing the configuration status.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/workspace_conf.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nconf = w.workspace_conf.get_status(keys=\"enableWorkspaceFilesystem\")\n```\n\n----------------------------------------\n\nTITLE: Querying Endpoints in Control and Data Plane with WorkspaceClient in Python\nDESCRIPTION: This code snippet demonstrates how to use the WorkspaceClient to query serving endpoints both in the Control Plane and the Data Plane. It initializes a WorkspaceClient and then calls the `query` method on both `serving_endpoints` (Control Plane) and `serving_endpoints_data_plane` (Data Plane).\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dataplane.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n# Control Plane\nw = WorkspaceClient()\nw.serving_endpoints.query(...)\n# Data Plane\nw.serving_endpoints_data_plane.query(...)\n```\n\n----------------------------------------\n\nTITLE: Example Databricks SDK Log Output - Text\nDESCRIPTION: This is an example of the log output generated by the Databricks SDK when debug logging is enabled. It shows a GET request to the `/api/2.0/clusters/list` endpoint and the corresponding 200 OK response. The response includes a JSON payload with cluster details.  It demonstrates how the SDK formats request and response messages, and shows indication of truncation when responses are too large.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/logging.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n2023-03-22 21:19:21,702 [databricks.sdk][DEBUG] GET /api/2.0/clusters/list\n< 200 OK\n< {\n<   \"clusters\": [\n<     {\n<       \"autotermination_minutes\": 60,\n<       \"cluster_id\": \"1109-115255-s1w13zjj\",\n<       \"cluster_name\": \"DEFAULT Test Cluster\",\n<       ... truncated for brevity\n<     },\n<     \"... (47 additional elements)\"\n<   ]\n< }\n```\n\n----------------------------------------\n\nTITLE: Update Automatic Cluster Update Setting - Python\nDESCRIPTION: Updates the automatic cluster update setting for the workspace. Requires a fresh etag and a field mask specifying the fields to update. It uses a PATCH request and handles potential concurrency conflicts.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/automatic_cluster_update.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(allow_missing: bool, setting: AutomaticClusterUpdateSetting, field_mask: str) -> AutomaticClusterUpdateSetting\n\n    Update the automatic cluster update setting.\n\n    Updates the automatic cluster update setting for the workspace. A fresh etag needs to be provided in\n    `PATCH` requests (as part of the setting field). The etag can be retrieved by making a `GET` request\n    before the `PATCH` request. If the setting is updated concurrently, `PATCH` fails with 409 and the\n    request must be retried by using the fresh etag in the 409 response.\n\n    :param allow_missing: bool\n      This should always be set to true for Settings API. Added for AIP compliance.\n    :param setting: :class:`AutomaticClusterUpdateSetting`\n    :param field_mask: str\n      The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n      field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n      `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n      the entire collection field can be specified. Field names must exactly match the resource field\n      names.\n\n      A field mask of `*` indicates full replacement. It’s recommended to always explicitly list the\n      fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n      changes in the future.\n\n    :returns: :class:`AutomaticClusterUpdateSetting`\n```\n\n----------------------------------------\n\nTITLE: Listing App Deployments in Databricks using Python\nDESCRIPTION: This code snippet demonstrates how to list all app deployments for a specific app using the `list_deployments` method. It takes the app name and optional pagination parameters. The method returns an iterator over `AppDeployment` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list_deployments(app_name: str [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[AppDeployment]\n\n        List app deployments.\n\n        Lists all app deployments for the app with the supplied name.\n\n        :param app_name: str\n          The name of the app.\n        :param page_size: int (optional)\n          Upper bound for items returned.\n        :param page_token: str (optional)\n          Pagination token to go to the next page of apps. Requests first page if absent.\n\n        :returns: Iterator over :class:`AppDeployment`\n```\n\n----------------------------------------\n\nTITLE: Listing Databricks Clusters using PKCE OAuth (Python)\nDESCRIPTION: This code snippet demonstrates how to authenticate with Azure Databricks using the PKCE OAuth flow. It initializes a `WorkspaceClient` with the Databricks host, client ID (obtained from the Terraform output), and sets `auth_type` to 'external-browser', triggering a browser-based login flow.\nDependencies: databricks-sdk\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(host='https://adb-30....azuredatabricks.net',\n                    client_id='>>>value_from_pkce_app_client_id output<<<<',\n                    auth_type='external-browser')\nclusters = w.clusters.list()\nfor cl in clusters:\n    print(f' - {cl.cluster_name} is {cl.state}')\n```\n\n----------------------------------------\n\nTITLE: Updating a Clean Room - Python\nDESCRIPTION: Updates the clean room with the changes and data objects in the request. The caller must be the owner of the clean room or a metastore admin. Owner updates are restricted to metastore admins.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/clean_rooms.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(name: str [, catalog_updates: Optional[List[CleanRoomCatalogUpdate]], comment: Optional[str], owner: Optional[str]]) -> CleanRoomInfo\n\n    Update a clean room.\n    \n    Updates the clean room with the changes and data objects in the request. The caller must be the owner\n    of the clean room or a metastore admin.\n    \n    When the caller is a metastore admin, only the __owner__ field can be updated.\n    \n    In the case that the clean room name is changed **updateCleanRoom** requires that the caller is both\n    the clean room owner and a metastore admin.\n    \n    For each table that is added through this method, the clean room owner must also have **SELECT**\n    privilege on the table. The privilege must be maintained indefinitely for recipients to be able to\n    access the table. Typically, you should use a group as the clean room owner.\n    \n    Table removals through **update** do not require additional privileges.\n    \n    :param name: str\n      The name of the clean room.\n    :param catalog_updates: List[:class:`CleanRoomCatalogUpdate`] (optional)\n      Array of shared data object updates.\n    :param comment: str (optional)\n      User-provided free-form text description.\n    :param owner: str (optional)\n      Username of current owner of clean room.\n    \n    :returns: :class:`CleanRoomInfo`\n```\n\n----------------------------------------\n\nTITLE: Listing Users using Databricks SDK in Python\nDESCRIPTION: Lists all users associated with a Databricks workspace using the WorkspaceClient. It filters and sorts results and depends on the `databricks.sdk` library.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/users.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import iam\n\nw = WorkspaceClient()\n\nall_users = w.users.list(\n    attributes=\"id,userName\",\n    sort_by=\"userName\",\n    sort_order=iam.ListSortOrder.DESCENDING,\n)\n```\n\n----------------------------------------\n\nTITLE: List Instance Profiles using Databricks SDK in Python\nDESCRIPTION: Retrieves a list of instance profiles accessible to the calling user for launching clusters. This method is available to all users and demonstrates the use of WorkspaceClient to access the instance_profiles.list method. The return value is an iterator over InstanceProfile objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/instance_profiles.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nall = w.instance_profiles.list()\n```\n\n----------------------------------------\n\nTITLE: Deleting an App in Databricks\nDESCRIPTION: Deletes an app from the Databricks workspace based on the provided app name.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nw.apps.delete(name: str)\n```\n\n----------------------------------------\n\nTITLE: SSO for Local Scripts with External Browser Auth\nDESCRIPTION: This example shows how to use the Databricks SDK for Python with `auth_type='external-browser'` to enable Single-Sign-On (SSO) for local scripts. The user is prompted to enter their Databricks host, and the SDK opens a browser for authentication. It then lists the clusters in the workspace and prints their names and states.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nhost = input('Enter Databricks host: ')\n\nw = WorkspaceClient(host=host, auth_type='external-browser')\nclusters = w.clusters.list()\n\nfor cl in clusters:\n    print(f' - {cl.cluster_name} is {cl.state}')\n```\n\n----------------------------------------\n\nTITLE: Creating Log Delivery Configuration in Databricks using Python\nDESCRIPTION: This snippet demonstrates how to create a new log delivery configuration in Databricks using the AccountClient and the LogDeliveryAPI.  It creates a storage configuration, credential configuration, and finally the log delivery configuration itself. It requires the databricks-sdk library and environment variables for AWS credentials.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/log_delivery.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import billing, provisioning\n\na = AccountClient()\n\nbucket = a.storage.create(\n    storage_configuration_name=f\"sdk-{time.time_ns()}\",\n    root_bucket_info=provisioning.RootBucketInfo(bucket_name=f\"sdk-{time.time_ns()}\"),\n)\n\ncreds = a.credentials.create(\n    credentials_name=f\"sdk-{time.time_ns()}\",\n    aws_credentials=provisioning.CreateCredentialAwsCredentials(\n        sts_role=provisioning.CreateCredentialStsRole(role_arn=os.environ[\"TEST_LOGDELIVERY_ARN\"])\n    ),\n)\n\ncreated = a.log_delivery.create(\n    log_delivery_configuration=billing.CreateLogDeliveryConfigurationParams(\n        config_name=f\"sdk-{time.time_ns()}\",\n        credentials_id=creds.credentials_id,\n        storage_configuration_id=bucket.storage_configuration_id,\n        log_type=billing.LogType.AUDIT_LOGS,\n        output_format=billing.OutputFormat.JSON,\n    )\n)\n\n# cleanup\na.storage.delete(storage_configuration_id=bucket.storage_configuration_id)\na.credentials.delete(credentials_id=creds.credentials_id)\na.log_delivery.patch_status(\n    log_delivery_configuration_id=created.log_delivery_configuration.config_id,\n    status=billing.LogDeliveryConfigStatus.DISABLED,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating an Installation using ConsumerInstallationsAPI in Python\nDESCRIPTION: Creates an installation from a listing. The `create` method takes the listing ID as a required parameter, and optional parameters such as accepted consumer terms, catalog name, recipient type, repo details, and share name. It returns an Installation object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_installations.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nw.consumer_installations.create(listing_id: str [, accepted_consumer_terms: Optional[ConsumerTerms], catalog_name: Optional[str], recipient_type: Optional[DeltaSharingRecipientType], repo_detail: Optional[RepoInstallation], share_name: Optional[str]]) -> Installation\n```\n\n----------------------------------------\n\nTITLE: Wait for Workspace Running Method Signature (Python)\nDESCRIPTION: This is the method signature for waiting until a Databricks workspace status becomes 'RUNNING'. It includes the workspace ID, an optional timeout, and an optional callback function. The method returns a Workspace object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/workspaces.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwait_get_workspace_running(workspace_id: int, timeout: datetime.timedelta = 0:20:00, callback: Optional[Callable[[Workspace], None]]) -> Workspace\n```\n\n----------------------------------------\n\nTITLE: Workspace Update Method Signature (Python)\nDESCRIPTION: This is the method signature for updating a Databricks workspace. It includes parameters for workspace ID, AWS region, credentials ID, custom tags, managed services customer managed key ID, network connectivity config ID, network ID, private access settings ID, storage configuration ID, and storage customer managed key ID. It also includes an optional timeout parameter.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/workspaces.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nupdate_and_wait(workspace_id: int [, aws_region: Optional[str], credentials_id: Optional[str], custom_tags: Optional[Dict[str, str]], managed_services_customer_managed_key_id: Optional[str], network_connectivity_config_id: Optional[str], network_id: Optional[str], private_access_settings_id: Optional[str], storage_configuration_id: Optional[str], storage_customer_managed_key_id: Optional[str], timeout: datetime.timedelta = 0:20:00]) -> Workspace\n```\n\n----------------------------------------\n\nTITLE: List directory contents (Python)\nDESCRIPTION: Lists the contents of a directory. Returns an iterator of DirectoryEntry objects. The number of entries returned can be controlled using the page_size parameter and pagination is supported using the page_token.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/files/files.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list_directory_contents(directory_path: str [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[DirectoryEntry]\n\n    List directory contents.\n\n    Returns the contents of a directory. If there is no directory at the specified path, the API returns a\n    HTTP 404 error.\n\n    :param directory_path: str\n      The absolute path of a directory.\n    :param page_size: int (optional)\n      The maximum number of directory entries to return. The response may contain fewer entries. If the\n      response contains a `next_page_token`, there may be more entries, even if fewer than `page_size`\n      entries are in the response.\n\n      We recommend not to set this value unless you are intentionally listing less than the complete\n      directory contents.\n\n      If unspecified, at most 1000 directory entries will be returned. The maximum value is 1000. Values\n      above 1000 will be coerced to 1000.\n    :param page_token: str (optional)\n      An opaque page token which was the `next_page_token` in the response of the previous request to list\n      the contents of this directory. Provide this token to retrieve the next page of directory entries.\n      When providing a `page_token`, all other parameters provided to the request must match the previous\n      request. To list all of the entries in a directory, it is necessary to continue requesting pages of\n      entries until the response contains no `next_page_token`. Note that the number of entries returned\n      must not be used to determine when the listing is complete.\n\n    :returns: Iterator over :class:`DirectoryEntry`\n```\n\n----------------------------------------\n\nTITLE: Get Enable Results Downloading Setting - Python\nDESCRIPTION: This method retrieves the current 'Enable Results Downloading' setting from Databricks. It returns an `EnableResultsDownloading` object representing the current state.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/enable_results_downloading.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: get_enable_results_downloading() -> EnableResultsDownloading\n\n    Get the Enable Results Downloading setting.\n\n    Gets the Enable Results Downloading setting.\n\n    :returns: :class:`EnableResultsDownloading`\n```\n\n----------------------------------------\n\nTITLE: Cancel Published Query Execution - Python\nDESCRIPTION: This method cancels the results for a query associated with a published, embedded dashboard. It requires the dashboard name and revision ID. An optional list of tokens can be provided for authorization.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/query_execution.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef cancel_published_query_execution(dashboard_name: str, dashboard_revision_id: str [, tokens: Optional[List[str]]]) -> CancelQueryExecutionResponse:\n    \"\"\"Cancel the results for the a query for a published, embedded dashboard.\n\n    :param dashboard_name: str\n    :param dashboard_revision_id: str\n    :param tokens: List[str] (optional)\n      Example: EC0A..ChAB7WCEn_4Qo4vkLqEbXsxxEgh3Y2pbWw45WhoQXgZSQo9aS5q2ZvFcbvbx9CgA-PAEAQ\n\n    :returns: :class:`CancelQueryExecutionResponse`\n    \"\"\n```\n\n----------------------------------------\n\nTITLE: Executing a Message Attachment Query in Genie using Python\nDESCRIPTION: This method executes the SQL query associated with a specific attachment of a message in a Genie conversation. It is used when the query attachment has expired and needs to be re-executed. The method returns a GenieGetMessageQueryResultResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/genie.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: execute_message_attachment_query(space_id: str, conversation_id: str, message_id: str, attachment_id: str) -> GenieGetMessageQueryResultResponse\n\n        Execute message attachment SQL query.\n\n        Execute the SQL for a message query attachment. Use this API when the query attachment has expired and\n        needs to be re-executed.\n\n        :param space_id: str\n          Genie space ID\n        :param conversation_id: str\n          Conversation ID\n        :param message_id: str\n          Message ID\n        :param attachment_id: str\n          Attachment ID\n\n        :returns: :class:`GenieGetMessageQueryResultResponse`\n```\n\n----------------------------------------\n\nTITLE: Deleting a Registered Model with databricks-sdk-py\nDESCRIPTION: Deletes a registered model and all its model versions from the specified parent catalog and schema. Requires the caller to be a metastore admin or an owner of the registered model, and in the latter case, also the owner or have the USE_CATALOG privilege on the parent catalog and the USE_SCHEMA privilege on the parent schema.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/registered_models.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(full_name: str)\n\n    Delete a Registered Model.\n\n    Deletes a registered model and all its model versions from the specified parent catalog and schema.\n\n    The caller must be a metastore admin or an owner of the registered model. For the latter case, the\n    caller must also be the owner or have the **USE_CATALOG** privilege on the parent catalog and the\n    **USE_SCHEMA** privilege on the parent schema.\n\n    :param full_name: str\n      The three-level (fully qualified) name of the registered model\n```\n\n----------------------------------------\n\nTITLE: Get Federation Policy (Python)\nDESCRIPTION: Retrieves an account federation policy based on its `policy_id`. Returns a `FederationPolicy` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/federation_policy.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(policy_id: str) -> FederationPolicy\n\n    Get account federation policy.\n\n    :param policy_id: str\n      The identifier for the federation policy.\n\n    :returns: :class:`FederationPolicy`\n```\n\n----------------------------------------\n\nTITLE: Deleting Data from a Direct Vector Access Index\nDESCRIPTION: This method deletes data from a specified Direct Vector Access Index. It requires the index name and a list of primary keys corresponding to the data to be deleted.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_indexes.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete_data_vector_index(index_name: str, primary_keys: List[str]) -> DeleteDataVectorIndexResponse\n\n        Delete data from index.\n\n        Handles the deletion of data from a specified vector index.\n\n        :param index_name: str\n          Name of the vector index where data is to be deleted. Must be a Direct Vector Access Index.\n        :param primary_keys: List[str]\n          List of primary keys for the data to be deleted.\n\n        :returns: :class:`DeleteDataVectorIndexResponse`\n```\n\n----------------------------------------\n\nTITLE: Databricks Configuration File Example (Text)\nDESCRIPTION: This is an example of a Databricks configuration file (`~/.databrickscfg`) containing Azure Service Principal credentials.\nDependencies: None\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n[spn]\nhost=https://adb-30....azuredatabricks.net\nazure_client_id=00000000-0000-0000-0000-000000000001\nazure_client_secret=00000000-0000-0000-0000-000000000002\nazure_tenant_id=00000000-0000-0000-0000-000000000003\n```\n\n----------------------------------------\n\nTITLE: UpdateInfoCause Enum Definition in Python\nDESCRIPTION: Defines an enumeration for the cause that triggered a pipeline update, mirroring the StartUpdateCause. It contains options such as API_CALL, JOB_TASK, RETRY_ON_FAILURE, SCHEMA_CHANGE, SERVICE_UPGRADE, and USER_ACTION, specifying why the update was initiated.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: UpdateInfoCause\n\n   What triggered this update.\n\n   .. py:attribute:: API_CALL\n      :value: \"API_CALL\"\n\n   .. py:attribute:: JOB_TASK\n      :value: \"JOB_TASK\"\n\n   .. py:attribute:: RETRY_ON_FAILURE\n      :value: \"RETRY_ON_FAILURE\"\n\n   .. py:attribute:: SCHEMA_CHANGE\n      :value: \"SCHEMA_CHANGE\"\n\n   .. py:attribute:: SERVICE_UPGRADE\n      :value: \"SERVICE_UPGRADE\"\n\n   .. py:attribute:: USER_ACTION\n      :value: \"USER_ACTION\"\n```\n\n----------------------------------------\n\nTITLE: Get Listing by ID - Python\nDESCRIPTION: Retrieves a specific listing from the Databricks Marketplace using its ID. Requires the listing ID as a parameter. Returns a `GetListingResponse` object containing the listing details.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_listings.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(id: str) -> GetListingResponse\n\n    Get listing.\n\n    Get a published listing in the Databricks Marketplace that the consumer has access to.\n\n    :param id: str\n\n    :returns: :class:`GetListingResponse`\n```\n\n----------------------------------------\n\nTITLE: Get Logs for Served Model - Python\nDESCRIPTION: Retrieves the latest service logs associated with a provided served model. Requires the name of the serving endpoint and the served model.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: logs(name: str, served_model_name: str) -> ServerLogsResponse\n\n    Get the latest logs for a served model.\n\n    Retrieves the service logs associated with the provided served model.\n\n    :param name: str\n      The name of the serving endpoint that the served model belongs to. This field is required.\n    :param served_model_name: str\n      The name of the served model that logs will be retrieved for. This field is required.\n\n    :returns: :class:`ServerLogsResponse`\n```\n\n----------------------------------------\n\nTITLE: List Table Summaries Example - Databricks SDK\nDESCRIPTION: This snippet demonstrates how to list table summaries using the Databricks SDK. It includes setting up the WorkspaceClient, creating a catalog and schema, listing table summaries and cleaning up created resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/tables.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated_schema = w.schemas.create(name=f\"sdk-{time.time_ns()}\", catalog_name=created_catalog.name)\n\nsummaries = w.tables.list_summaries(catalog_name=created_catalog.name, schema_name_pattern=created_schema.name)\n\n# cleanup\nw.schemas.delete(full_name=created_schema.full_name)\nw.catalogs.delete(name=created_catalog.name, force=True)\n```\n\n----------------------------------------\n\nTITLE: Getting a Legacy Alert in Databricks SQL\nDESCRIPTION: This method retrieves an alert from Databricks SQL by its ID. It takes the alert ID as a parameter and returns the corresponding LegacyAlert object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/alerts_legacy.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nw.alerts_legacy.get(alert_id: str) -> LegacyAlert\n```\n\n----------------------------------------\n\nTITLE: Getting an Experiment with the MLflow Experiments API in Python\nDESCRIPTION: This code snippet demonstrates how to retrieve an MLflow experiment using the Databricks SDK. It first creates an experiment, retrieves it using its ID, and then deletes the experiment as a cleanup step.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/experiments.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nexperiment = w.experiments.create_experiment(name=f\"sdk-{time.time_ns()}\")\n\n_ = w.experiments.get_experiment(experiment_id=experiment.experiment_id)\n\n# cleanup\nw.experiments.delete_experiment(experiment_id=experiment.experiment_id)\n```\n\n----------------------------------------\n\nTITLE: Listing Account IP Access Lists in Python\nDESCRIPTION: Gets all IP access lists for the specified account. Returns an iterator over `IpAccessListInfo` objects, providing information about each access list.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/ip_access_lists.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list() -> Iterator[IpAccessListInfo]\n\n    Get access lists.\n\n    Gets all IP access lists for the specified account.\n\n    :returns: Iterator over :class:`IpAccessListInfo`\n```\n\n----------------------------------------\n\nTITLE: Creating VPC Endpoint Configuration with AccountClient in Python\nDESCRIPTION: This code creates a VPC endpoint configuration using the AccountClient. It requires setting environment variables for `TEST_RELAY_VPC_ENDPOINT` and `AWS_REGION`. After creating the endpoint, it is immediately deleted. It uses `os` for accessing environment variables and `time` for generating a unique VPC endpoint name.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/vpc_endpoints.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\ncreated = a.vpc_endpoints.create(\n    aws_vpc_endpoint_id=os.environ[\"TEST_RELAY_VPC_ENDPOINT\"],\n    region=os.environ[\"AWS_REGION\"],\n    vpc_endpoint_name=f\"sdk-{time.time_ns()}\",\n)\n\n# cleanup\na.vpc_endpoints.delete(vpc_endpoint_id=created.vpc_endpoint_id)\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Logging with Databricks SDK - Python\nDESCRIPTION: This code snippet demonstrates how to enable debug logging in a Databricks Python project using the Databricks SDK. It configures the logging level to DEBUG, allowing detailed logging of requests and responses from the Databricks SDK. It also includes instantiating a WorkspaceClient to interact with Databricks clusters and logging their names. The `debug_truncate_bytes` parameter allows configuration of maximum bytes to log, and `debug_headers` is set to false, enabling header redaction.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/logging.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging, sys\nlogging.basicConfig(stream=sys.stderr,\n                    level=logging.INFO,\n                    format='%(asctime)s [%(name)s][%(levelname)s] %(message)s')\nlogging.getLogger('databricks.sdk').setLevel(logging.DEBUG)\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(debug_truncate_bytes=1024, debug_headers=False)\nfor cluster in w.clusters.list():\n    logging.info(f'Found cluster: {cluster.cluster_name}')\n```\n\n----------------------------------------\n\nTITLE: Listing Service Principals with filter\nDESCRIPTION: Demonstrates how to list service principals using a filter. A service principal is first created, then its details are retrieved and used to filter the list of service principals based on the display name. Finally, the created service principal is deleted.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/service_principals.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nsp_create = a.service_principals.create(active=True, display_name=f\"sdk-{time.time_ns()}\")\n\nsp = a.service_principals.get(id=sp_create.id)\n\nsp_list = a.service_principals.list(filter=\"displayName eq %v\" % (sp.display_name))\n\n# cleanup\na.service_principals.delete(id=sp_create.id)\n```\n\n----------------------------------------\n\nTITLE: Creating Private Access Settings with AccountClient in Python\nDESCRIPTION: This code snippet demonstrates how to create a private access settings object using the AccountClient in the Databricks SDK for Python. It sets the private access settings name and AWS region, and then cleans up by deleting the created settings. It depends on the 'os' and 'time' modules.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/private_access.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\ncreated = a.private_access.create(\n    private_access_settings_name=f\"sdk-{time.time_ns()}\",\n    region=os.environ[\"AWS_REGION\"],\n)\n\n# cleanup\na.private_access.delete(private_access_settings_id=created.private_access_settings_id)\n```\n\n----------------------------------------\n\nTITLE: Delete Service Principal Federation Policy (Python)\nDESCRIPTION: Deletes a service principal federation policy, identified by the service principal ID and the policy ID. The policy ID is a string identifier for the federation policy.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/service_principal_federation_policy.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(service_principal_id: int, policy_id: str)\n\n    Delete service principal federation policy.\n\n    :param service_principal_id: int\n      The service principal id for the federation policy.\n    :param policy_id: str\n      The identifier for the federation policy.\n```\n\n----------------------------------------\n\nTITLE: Listing Databricks Clusters using AAD SPN (Python)\nDESCRIPTION: This code snippet demonstrates how to authenticate with Azure Databricks using an Azure AD Service Principal. It initializes a `WorkspaceClient` with the Databricks host, azure client ID, azure client secret, and azure tenant ID (all obtained from the Terraform output or environment variables).\nDependencies: databricks-sdk\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient(host='https://adb-30....azuredatabricks.net',\n                    azure_client_id='>>> value from client_id <<<',\n                    azure_client_secret='>>> value from client_secret <<<',\n                    azure_tenant_id='>>> your Azure Tenant ID <<<')\nclusters = w.clusters.list()\nfor cl in clusters:\n    print(f' - {cl.cluster_name} is {cl.state}')\n```\n\n----------------------------------------\n\nTITLE: Updating a Model Version in Databricks with Python\nDESCRIPTION: Updates a specific model version.  Currently, only the comment of the model version can be updated. The caller must have appropriate privileges on the parent registered model. Parameters include the full name of the model version and its version number. The comment is optional.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/model_versions.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nw.model_versions.update(full_name: str, version: int [, comment: Optional[str]]) -> ModelVersionInfo\n```\n\n----------------------------------------\n\nTITLE: Get User with AccountClient in Python\nDESCRIPTION: This code snippet retrieves user details from a Databricks account using the AccountClient and the `get` method. It requires a valid user ID. The code also handles user creation and deletion for a complete example. It requires the `databricks.sdk` library.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/users.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nuser = a.users.create(\n    display_name=f\"sdk-{time.time_ns()}\",\n    user_name=f\"sdk-{time.time_ns()}@example.com\",\n)\n\nby_id = a.users.get(id=user.id)\n\n# cleanup\na.users.delete(id=user.id)\n```\n\n----------------------------------------\n\nTITLE: Enable System Schema in Databricks\nDESCRIPTION: Enables a system schema within a specified metastore. Requires account admin or metastore admin permissions. Takes the metastore ID and schema name as parameters and adds the schema to the system catalog.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/system_schemas.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nw.system_schemas.enable(metastore_id: str, schema_name: str)\n```\n\n----------------------------------------\n\nTITLE: Listing Log Delivery Configurations in Databricks (Python)\nDESCRIPTION: This code snippet demonstrates how to list all log delivery configurations associated with a Databricks account using the AccountClient.  It imports the necessary modules from the Databricks SDK and initializes an AccountClient.  It then calls the `list` method on the `log_delivery` object, which returns an iterator of LogDeliveryConfiguration objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/log_delivery.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import billing\n\na = AccountClient()\n\nall = a.log_delivery.list(billing.ListLogDeliveryRequest())\n```\n\n----------------------------------------\n\nTITLE: Listing Marketplace Files in Python\nDESCRIPTION: This Python method lists files associated with a specific parent entity in the Databricks Marketplace. It supports pagination through optional page_size and page_token parameters. It returns an iterator over FileInfo objects, providing metadata for each file.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_files.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list(file_parent: FileParent [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[FileInfo]\n\n        List files.\n\n        List files attached to a parent entity.\n\n        :param file_parent: :class:`FileParent`\n        :param page_size: int (optional)\n        :param page_token: str (optional)\n\n        :returns: Iterator over :class:`FileInfo`\n```\n\n----------------------------------------\n\nTITLE: Creating a Table Monitor - Databricks SDK (Python)\nDESCRIPTION: This method creates a new monitor for the specified table. It requires the caller to have appropriate permissions on the table, schema, and catalog.  Assets such as the dashboard will be created in the workspace where the call is made. It allows specifying various configurations such as custom metrics, data classification, inference logs, notifications, and schedules.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/quality_monitors.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(table_name: str, assets_dir: str, output_schema_name: str [, baseline_table_name: Optional[str], custom_metrics: Optional[List[MonitorMetric]], data_classification_config: Optional[MonitorDataClassificationConfig], inference_log: Optional[MonitorInferenceLog], notifications: Optional[MonitorNotifications], schedule: Optional[MonitorCronSchedule], skip_builtin_dashboard: Optional[bool], slicing_exprs: Optional[List[str]], snapshot: Optional[MonitorSnapshot], time_series: Optional[MonitorTimeSeries], warehouse_id: Optional[str]]) -> MonitorInfo\n\n        Create a table monitor.\n\n        Creates a new monitor for the specified table.\n\n        The caller must either: 1. be an owner of the table's parent catalog, have **USE_SCHEMA** on the\n        table's parent schema, and have **SELECT** access on the table 2. have **USE_CATALOG** on the table's\n        parent catalog, be an owner of the table's parent schema, and have **SELECT** access on the table. 3.\n        have the following permissions: - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on\n        the table's parent schema - be an owner of the table.\n\n        Workspace assets, such as the dashboard, will be created in the workspace where this call was made.\n\n        :param table_name: str\n          Full name of the table.\n        :param assets_dir: str\n          The directory to store monitoring assets (e.g. dashboard, metric tables).\n        :param output_schema_name: str\n          Schema where output metric tables are created.\n        :param baseline_table_name: str (optional)\n          Name of the baseline table from which drift metrics are computed from. Columns in the monitored\n          table should also be present in the baseline table.\n        :param custom_metrics: List[:class:`MonitorMetric`] (optional)\n          Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics\n          (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).\n        :param data_classification_config: :class:`MonitorDataClassificationConfig` (optional)\n          The data classification config for the monitor.\n        :param inference_log: :class:`MonitorInferenceLog` (optional)\n          Configuration for monitoring inference logs.\n        :param notifications: :class:`MonitorNotifications` (optional)\n          The notification settings for the monitor.\n        :param schedule: :class:`MonitorCronSchedule` (optional)\n          The schedule for automatically updating and refreshing metric tables.\n        :param skip_builtin_dashboard: bool (optional)\n          Whether to skip creating a default dashboard summarizing data quality metrics.\n        :param slicing_exprs: List[str] (optional)\n          List of column expressions to slice data with for targeted analysis. The data is grouped by each\n          expression independently, resulting in a separate slice for each predicate and its complements. For\n          high-cardinality columns, only the top 100 unique values by frequency will generate slices.\n        :param snapshot: :class:`MonitorSnapshot` (optional)\n          Configuration for monitoring snapshot tables.\n        :param time_series: :class:`MonitorTimeSeries` (optional)\n          Configuration for monitoring time series tables.\n        :param warehouse_id: str (optional)\n          Optional argument to specify the warehouse for dashboard creation. If not specified, the first\n          running warehouse will be used.\n\n        :returns: :class:`MonitorInfo`\n```\n\n----------------------------------------\n\nTITLE: Listing App Deployments in Databricks\nDESCRIPTION: Lists all app deployments for the app with the supplied name. Supports pagination through optional page_size and page_token parameters. Returns an iterator over AppDeployment objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nw.apps.list_deployments(app_name: str [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[AppDeployment]\n```\n\n----------------------------------------\n\nTITLE: Creating Forecasting Experiment using Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to create a forecasting experiment using the `create_experiment` method of the `ForecastingAPI` class in the Databricks SDK. It takes parameters such as training data path, target column, time column, forecast granularity, and forecast horizon. Optional parameters like custom weights column, experiment path, holiday regions, include features, max runtime, prediction data path, primary metric, register to, split column, timeseries identifier columns, and training frameworks can also be specified.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/forecasting.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nw.forecasting.create_experiment(train_data_path: str, target_column: str, time_column: str, forecast_granularity: str, forecast_horizon: int [, custom_weights_column: Optional[str], experiment_path: Optional[str], holiday_regions: Optional[List[str]], include_features: Optional[List[str]], max_runtime: Optional[int], prediction_data_path: Optional[str], primary_metric: Optional[str], register_to: Optional[str], split_column: Optional[str], timeseries_identifier_columns: Optional[List[str]], training_frameworks: Optional[List[str]]]) -> Wait[ForecastingExperiment]\n```\n\n----------------------------------------\n\nTITLE: Listing SQL Warehouses using Databricks SDK\nDESCRIPTION: This code snippet shows how to list all SQL warehouses using the Databricks SDK for Python. It uses the `WorkspaceClient` and `warehouses.list` to retrieve a list of warehouses. It also imports necessary modules from the SDK, which allows the programmatic access to Databricks resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/warehouses.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\nall = w.warehouses.list(sql.ListWarehousesRequest())\n```\n\n----------------------------------------\n\nTITLE: Listing Repositories with Databricks SDK\nDESCRIPTION: This snippet demonstrates how to list repositories in a Databricks workspace using the Databricks SDK for Python. It initializes a WorkspaceClient and iterates through the repos, logging the path of each repository. It requires the databricks-sdk library.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/pagination.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient()\nfor repo in w.repos.list():\n    logging.info(f'Found repo: {repo.path}')\n```\n\n----------------------------------------\n\nTITLE: Creating a Budget Policy (Python)\nDESCRIPTION: Creates a new budget policy using the `create` method of the `BudgetPolicyAPI`. Requires a `BudgetPolicy` object as input. The `policy_id` within the input policy should be empty, and `policy_name` must be provided. A unique `request_id` can be provided for idempotent requests.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/budget_policy.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nBudgetPolicyAPI.create(policy: Optional[BudgetPolicy], request_id: Optional[str]]) -> BudgetPolicy\n```\n\n----------------------------------------\n\nTITLE: Listing Schedule Subscriptions\nDESCRIPTION: Lists subscriptions for a specific schedule of a Lakeview dashboard using the LakeviewAPI. Requires dashboard ID and schedule ID. Supports pagination. Returns an iterator over Subscription objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: list_subscriptions(dashboard_id: str, schedule_id: str [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[Subscription]\n\n    List schedule subscriptions.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard which the subscriptions belongs.\n    :param schedule_id: str\n      UUID identifying the schedule which the subscriptions belongs.\n    :param page_size: int (optional)\n      The number of subscriptions to return per page.\n    :param page_token: str (optional)\n      A page token, received from a previous `ListSubscriptions` call. Use this to retrieve the subsequent\n      page.\n\n    :returns: Iterator over :class:`Subscription`\n```\n\n----------------------------------------\n\nTITLE: Listing Registered Models with databricks-sdk-py\nDESCRIPTION: Lists registered models. The returned models are filtered based on the privileges of the calling user. The metastore admin can list all models. A regular user needs to be the owner or have the EXECUTE privilege on the registered model to receive the registered models in the response. They must also have the USE_CATALOG privilege on the parent catalog and the USE_SCHEMA privilege on the parent schema.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/registered_models.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list( [, catalog_name: Optional[str], include_browse: Optional[bool], max_results: Optional[int], page_token: Optional[str], schema_name: Optional[str]]) -> Iterator[RegisteredModelInfo]\n\n    List Registered Models.\n\n    List registered models. You can list registered models under a particular schema, or list all\n    registered models in the current metastore.\n\n    The returned models are filtered based on the privileges of the calling user. For example, the\n    metastore admin is able to list all the registered models. A regular user needs to be the owner or\n    have the **EXECUTE** privilege on the registered model to recieve the registered models in the\n    response. For the latter case, the caller must also be the owner or have the **USE_CATALOG** privilege\n    on the parent catalog and the **USE_SCHEMA** privilege on the parent schema.\n\n    There is no guarantee of a specific ordering of the elements in the response.\n\n    :param catalog_name: str (optional)\n      The identifier of the catalog under which to list registered models. If specified, schema_name must\n      be specified.\n    :param include_browse: bool (optional)\n      Whether to include registered models in the response for which the principal can only access\n      selective metadata for\n    :param max_results: int (optional)\n      Max number of registered models to return.\n\n      If both catalog and schema are specified: - when max_results is not specified, the page length is\n      set to a server configured value (10000, as of 4/2/2024). - when set to a value greater than 0, the\n      page length is the minimum of this value and a server configured value (10000, as of 4/2/2024); -\n      when set to 0, the page length is set to a server configured value (10000, as of 4/2/2024); - when\n      set to a value less than 0, an invalid parameter error is returned;\n\n      If neither schema nor catalog is specified: - when max_results is not specified, the page length is\n      set to a server configured value (100, as of 4/2/2024). - when set to a value greater than 0, the\n      page length is the minimum of this value and a server configured value (1000, as of 4/2/2024); -\n      when set to 0, the page length is set to a server configured value (100, as of 4/2/2024); - when set\n      to a value less than 0, an invalid parameter error is returned;\n    :param page_token: str (optional)\n      Opaque token to send for the next page of results (pagination).\n    :param schema_name: str (optional)\n      The identifier of the schema under which to list registered models. If specified, catalog_name must\n      be specified.\n\n    :returns: Iterator over :class:`RegisteredModelInfo`\n```\n\n----------------------------------------\n\nTITLE: Defining Privileges in Python\nDESCRIPTION: This code defines the Privilege class, enumerating available privileges within the Databricks environment.  Each privilege is represented as a class attribute with a string value. These privileges govern access and actions that can be performed on securable objects. It includes privileges like ACCESS, CREATE, SELECT, MODIFY, USAGE, and more.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass Privilege:\n   ACCESS = \"ACCESS\"\n\n   ALL_PRIVILEGES = \"ALL_PRIVILEGES\"\n\n   APPLY_TAG = \"APPLY_TAG\"\n\n   BROWSE = \"BROWSE\"\n\n   CREATE = \"CREATE\"\n\n   CREATE_CATALOG = \"CREATE_CATALOG\"\n\n   CREATE_CONNECTION = \"CREATE_CONNECTION\"\n\n   CREATE_EXTERNAL_LOCATION = \"CREATE_EXTERNAL_LOCATION\"\n\n   CREATE_EXTERNAL_TABLE = \"CREATE_EXTERNAL_TABLE\"\n\n   CREATE_EXTERNAL_VOLUME = \"CREATE_EXTERNAL_VOLUME\"\n\n   CREATE_FOREIGN_CATALOG = \"CREATE_FOREIGN_CATALOG\"\n\n   CREATE_FOREIGN_SECURABLE = \"CREATE_FOREIGN_SECURABLE\"\n\n   CREATE_FUNCTION = \"CREATE_FUNCTION\"\n\n   CREATE_MANAGED_STORAGE = \"CREATE_MANAGED_STORAGE\"\n\n   CREATE_MATERIALIZED_VIEW = \"CREATE_MATERIALIZED_VIEW\"\n\n   CREATE_MODEL = \"CREATE_MODEL\"\n\n   CREATE_PROVIDER = \"CREATE_PROVIDER\"\n\n   CREATE_RECIPIENT = \"CREATE_RECIPIENT\"\n\n   CREATE_SCHEMA = \"CREATE_SCHEMA\"\n\n   CREATE_SERVICE_CREDENTIAL = \"CREATE_SERVICE_CREDENTIAL\"\n\n   CREATE_SHARE = \"CREATE_SHARE\"\n\n   CREATE_STORAGE_CREDENTIAL = \"CREATE_STORAGE_CREDENTIAL\"\n\n   CREATE_TABLE = \"CREATE_TABLE\"\n\n   CREATE_VIEW = \"CREATE_VIEW\"\n\n   CREATE_VOLUME = \"CREATE_VOLUME\"\n\n   EXECUTE = \"EXECUTE\"\n\n   MANAGE = \"MANAGE\"\n\n   MANAGE_ALLOWLIST = \"MANAGE_ALLOWLIST\"\n\n   MODIFY = \"MODIFY\"\n\n   READ_FILES = \"READ_FILES\"\n\n   READ_PRIVATE_FILES = \"READ_PRIVATE_FILES\"\n\n   READ_VOLUME = \"READ_VOLUME\"\n\n   REFRESH = \"REFRESH\"\n\n   SELECT = \"SELECT\"\n\n   SET_SHARE_PERMISSION = \"SET_SHARE_PERMISSION\"\n\n   USAGE = \"USAGE\"\n\n   USE_CATALOG = \"USE_CATALOG\"\n\n   USE_CONNECTION = \"USE_CONNECTION\"\n\n   USE_MARKETPLACE_ASSETS = \"USE_MARKETPLACE_ASSETS\"\n\n   USE_PROVIDER = \"USE_PROVIDER\"\n\n   USE_RECIPIENT = \"USE_RECIPIENT\"\n\n   USE_SCHEMA = \"USE_SCHEMA\"\n\n   USE_SHARE = \"USE_SHARE\"\n\n   WRITE_FILES = \"WRITE_FILES\"\n\n   WRITE_PRIVATE_FILES = \"WRITE_PRIVATE_FILES\"\n\n   WRITE_VOLUME = \"WRITE_VOLUME\"\n```\n\n----------------------------------------\n\nTITLE: Listing Git Credentials using Databricks SDK\nDESCRIPTION: This snippet demonstrates how to list all Git credentials for a user using the Databricks SDK. It initializes a WorkspaceClient and calls the `list` method of the `git_credentials` API. This method retrieves an iterator over the user's credentials.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/git_credentials.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nlist = w.git_credentials.list()\n```\n\n----------------------------------------\n\nTITLE: Get directory metadata (Python)\nDESCRIPTION: Retrieves the metadata of a directory at the specified directory_path. The metadata is included in the response HTTP headers, with no response body. Useful for checking directory existence and access permissions.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/files/files.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_directory_metadata(directory_path: str)\n\n    Get directory metadata.\n\n    Get the metadata of a directory. The response HTTP headers contain the metadata. There is no response\n    body.\n\n    This method is useful to check if a directory exists and the caller has access to it.\n\n    If you wish to ensure the directory exists, you can instead use `PUT`, which will create the directory\n    if it does not exist, and is idempotent (it will succeed if the directory already exists).\n\n    :param directory_path: str\n      The absolute path of a directory.\n```\n\n----------------------------------------\n\nTITLE: Getting an App in Databricks\nDESCRIPTION: Retrieves information for the app with the supplied name and returns the App object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nw.apps.get(name: str) -> App\n```\n\n----------------------------------------\n\nTITLE: Listing Node Types in Databricks with Python\nDESCRIPTION: This snippet showcases how to list available node types in a Databricks workspace using the Databricks SDK for Python. It initializes a WorkspaceClient and then calls the list_node_types() method to retrieve the list of node types.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nnodes = w.clusters.list_node_types()\n```\n\n----------------------------------------\n\nTITLE: Getting a Table Monitor - Databricks SDK (Python)\nDESCRIPTION: This method retrieves a monitor for a specified table. The caller requires either ownership of the table's parent catalog or USE privileges on the catalog and schema, along with SELECT privilege on the table.  The returned information includes configuration values and asset information, with potential filtering based on the workspace of origin.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/quality_monitors.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(table_name: str) -> MonitorInfo\n\n        Get a table monitor.\n\n        Gets a monitor for the specified table.\n\n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema. 3. have the following\n        permissions: - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent\n        schema - **SELECT** privilege on the table.\n\n        The returned information includes configuration values, as well as information on assets created by\n        the monitor. Some information (e.g., dashboard) may be filtered out if the caller is in a different\n        workspace than where the monitor was created.\n\n        :param table_name: str\n          Full name of the table.\n\n        :returns: :class:`MonitorInfo`\n```\n\n----------------------------------------\n\nTITLE: Getting Private Access Settings by ID with AccountClient in Python\nDESCRIPTION: This code snippet demonstrates how to retrieve a private access settings object by its ID using the AccountClient in the Databricks SDK for Python. It first creates a private access setting, then retrieves it by ID, and finally cleans up by deleting the created setting.  It depends on the 'os' and 'time' modules.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/private_access.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\ncreated = a.private_access.create(\n    private_access_settings_name=f\"sdk-{time.time_ns()}\",\n    region=os.environ[\"AWS_REGION\"],\n)\n\nby_id = a.private_access.get(private_access_settings_id=created.private_access_settings_id)\n\n# cleanup\na.private_access.delete(private_access_settings_id=created.private_access_settings_id)\n```\n\n----------------------------------------\n\nTITLE: Listing Job Runs with Pagination and Summarization\nDESCRIPTION: This example retrieves and summarizes job run information using the Databricks SDK. It lists all jobs, then iterates through the runs for each job, calculating the average duration and identifying the latest run. It outputs a summary containing the job name, last status, last finished time, and average duration.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom collections import defaultdict\nfrom datetime import datetime, timezone\nfrom databricks.sdk import WorkspaceClient\n\nlatest_state = {}\nall_jobs = {}\ndurations = defaultdict(list)\n\nw = WorkspaceClient()\nfor job in w.jobs.list():\n    all_jobs[job.job_id] = job\n    for run in w.jobs.list_runs(job_id=job.job_id, expand_tasks=False):\n        durations[job.job_id].append(run.run_duration)\n        if job.job_id not in latest_state:\n            latest_state[job.job_id] = run\n            continue\n        if run.end_time < latest_state[job.job_id].end_time:\n            continue\n        latest_state[job.job_id] = run\n\nsummary = []\nfor job_id, run in latest_state.items():\n    summary.append({\n        'job_name': all_jobs[job_id].settings.name,\n        'last_status': run.state.result_state,\n        'last_finished': datetime.fromtimestamp(run.end_time/1000, timezone.utc),\n        'average_duration': sum(durations[job_id]) / len(durations[job_id])\n    })\n\nfor line in sorted(summary, key=lambda s: s['last_finished'], reverse=True):\n    logging.info(f'Latest: {line}')\n```\n\n----------------------------------------\n\nTITLE: Getting Service Principal Details by ID (Python)\nDESCRIPTION: This snippet shows how to retrieve details of a service principal by its ID using the Databricks SDK. It first creates a service principal, then retrieves its details using the `get` method, and finally deletes the service principal.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/service_principals.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.service_principals.create(display_name=f\"sdk-{time.time_ns()}\")\n\nby_id = w.service_principals.get(id=created.id)\n\n# cleanup\nw.service_principals.delete(id=created.id)\n```\n\n----------------------------------------\n\nTITLE: Execute Published Dashboard Query - Python\nDESCRIPTION: This method executes a query for a published dashboard. It requires the dashboard name and revision ID to retrieve the published dataset's data model. An optional warehouse ID can be overridden for the query execution.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/query_execution.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef execute_published_dashboard_query(dashboard_name: str, dashboard_revision_id: str [, override_warehouse_id: Optional[str]]):\n    \"\"\"Execute a query for a published dashboard.\n\n    :param dashboard_name: str\n      Dashboard name and revision_id is required to retrieve PublishedDatasetDataModel which contains the\n      list of datasets, warehouse_id, and embedded_credentials\n    :param dashboard_revision_id: str\n    :param override_warehouse_id: str (optional)\n      A dashboard schedule can override the warehouse used as compute for processing the published\n      dashboard queries\n    \"\"\n```\n\n----------------------------------------\n\nTITLE: Getting a Storage Configuration by ID with Databricks Account API in Python\nDESCRIPTION: This code snippet illustrates how to retrieve a storage configuration by its ID using the Databricks Account API.  It first creates a new storage configuration and then retrieves it using the returned ID. This demonstrates how to use the get method of the storage API.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/storage.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import provisioning\n\na = AccountClient()\n\nstorage = a.storage.create(\n    storage_configuration_name=f\"sdk-{time.time_ns()}\",\n    root_bucket_info=provisioning.RootBucketInfo(bucket_name=f\"sdk-{time.time_ns()}\"),\n)\n\nby_id = a.storage.get(storage_configuration_id=storage.storage_configuration_id)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Visualization in Databricks (Python)\nDESCRIPTION: This method removes a visualization from a query in Databricks. It takes a single parameter, `id`, which is a string representing the unique identifier of the visualization to be deleted. No return value is explicitly mentioned.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/query_visualizations.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(id: str)\n\n    Remove a visualization.\n\n    Removes a visualization.\n\n    :param id: str\n```\n\n----------------------------------------\n\nTITLE: Listing Private Endpoint Rules in Databricks\nDESCRIPTION: This method lists private endpoint rules for a given network connectivity configuration. It accepts the network connectivity configuration ID and an optional page token for pagination. It returns an iterator over NccAzurePrivateEndpointRule objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/network_connectivity.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nNetworkConnectivityAPI.list_private_endpoint_rules(network_connectivity_config_id: str [, page_token: Optional[str]]) -> Iterator[NccAzurePrivateEndpointRule]\n```\n\n----------------------------------------\n\nTITLE: Defining TerminationCodeCode Enum in Python\nDESCRIPTION: Defines the `TerminationCodeCode` enumeration, representing codes that indicate why a Databricks run was terminated.  It lists numerous reasons from `SUCCESS` to `MAX_JOB_QUEUE_SIZE_EXCEEDED` including user cancellations, errors during execution, and resource limitations. Used for debugging and understanding run failures in Databricks Jobs.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n.. py:attribute:: BUDGET_POLICY_LIMIT_EXCEEDED\n      :value: \"BUDGET_POLICY_LIMIT_EXCEEDED\"\n\n   .. py:attribute:: CANCELED\n      :value: \"CANCELED\"\n\n   .. py:attribute:: CLOUD_FAILURE\n      :value: \"CLOUD_FAILURE\"\n\n   .. py:attribute:: CLUSTER_ERROR\n      :value: \"CLUSTER_ERROR\"\n\n   .. py:attribute:: CLUSTER_REQUEST_LIMIT_EXCEEDED\n      :value: \"CLUSTER_REQUEST_LIMIT_EXCEEDED\"\n\n   .. py:attribute:: DRIVER_ERROR\n      :value: \"DRIVER_ERROR\"\n\n   .. py:attribute:: FEATURE_DISABLED\n      :value: \"FEATURE_DISABLED\"\n\n   .. py:attribute:: INTERNAL_ERROR\n      :value: \"INTERNAL_ERROR\"\n\n   .. py:attribute:: INVALID_CLUSTER_REQUEST\n      :value: \"INVALID_CLUSTER_REQUEST\"\n\n   .. py:attribute:: INVALID_RUN_CONFIGURATION\n      :value: \"INVALID_RUN_CONFIGURATION\"\n\n   .. py:attribute:: LIBRARY_INSTALLATION_ERROR\n      :value: \"LIBRARY_INSTALLATION_ERROR\"\n\n   .. py:attribute:: MAX_CONCURRENT_RUNS_EXCEEDED\n      :value: \"MAX_CONCURRENT_RUNS_EXCEEDED\"\n\n   .. py:attribute:: MAX_JOB_QUEUE_SIZE_EXCEEDED\n      :value: \"MAX_JOB_QUEUE_SIZE_EXCEEDED\"\n\n   .. py:attribute:: MAX_SPARK_CONTEXTS_EXCEEDED\n      :value: \"MAX_SPARK_CONTEXTS_EXCEEDED\"\n\n   .. py:attribute:: REPOSITORY_CHECKOUT_FAILED\n      :value: \"REPOSITORY_CHECKOUT_FAILED\"\n\n   .. py:attribute:: RESOURCE_NOT_FOUND\n      :value: \"RESOURCE_NOT_FOUND\"\n\n   .. py:attribute:: RUN_EXECUTION_ERROR\n      :value: \"RUN_EXECUTION_ERROR\"\n\n   .. py:attribute:: SKIPPED\n      :value: \"SKIPPED\"\n\n   .. py:attribute:: STORAGE_ACCESS_ERROR\n      :value: \"STORAGE_ACCESS_ERROR\"\n\n   .. py:attribute:: SUCCESS\n      :value: \"SUCCESS\"\n\n   .. py:attribute:: UNAUTHORIZED_ERROR\n      :value: \"UNAUTHORIZED_ERROR\"\n\n   .. py:attribute:: USER_CANCELED\n      :value: \"USER_CANCELED\"\n\n   .. py:attribute:: WORKSPACE_RUN_LIMIT_EXCEEDED\n      :value: \"WORKSPACE_RUN_LIMIT_EXCEEDED\"\n```\n\n----------------------------------------\n\nTITLE: Stopping an App in Databricks using Python\nDESCRIPTION: This code snippet shows how to stop the active deployment of an app using the `stop` method. It requires the app name.  The method returns a long-running operation waiter for the app.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: stop(name: str) -> Wait[App]\n\n        Stop an app.\n\n        Stops the active deployment of the app in the workspace.\n\n        :param name: str\n          The name of the app.\n\n        :returns:\n          Long-running operation waiter for :class:`App`.\n          See :method:wait_get_app_stopped for more details.\n```\n\n----------------------------------------\n\nTITLE: List IP Access Lists with Databricks SDK\nDESCRIPTION: This example demonstrates how to list all IP access lists in a workspace using the Databricks SDK. It initializes a WorkspaceClient and calls the `list` method to retrieve an iterator over all IP access lists.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/ip_access_lists.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nall = w.ip_access_lists.list()\n```\n\n----------------------------------------\n\nTITLE: Overriding .databrickscfg Profile in Python\nDESCRIPTION: This snippet demonstrates how to override the default `.databrickscfg` profile using the `profile` argument. It requires the `databricks.sdk` package and initializes the `WorkspaceClient` with a specified profile name, e.g., 'MYPROFILE'.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(profile='MYPROFILE')\n# Now call the Databricks workspace APIs as desired...\n```\n\n----------------------------------------\n\nTITLE: OAuth Authorization Code Flow with PKCE in Flask App\nDESCRIPTION: This example demonstrates how to implement the OAuth Authorization Code Flow with PKCE security in a Flask web application using the Databricks SDK for Python. It initializes an OAuthClient, defines routes for the callback and index pages, and handles the consent and token exchange processes.  The code showcases how to persist and restore RefreshableCredentials within a web app session using `Consent` and `SessionCredentials`\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk.oauth import OAuthClient\n\noauth_client = OAuthClient(host='<workspace-url>',\n                           client_id='<oauth client ID>',\n                           redirect_url=f'http://host.domain/callback',\n                           scopes=['clusters'])\n\nimport secrets\nfrom flask import Flask, render_template_string, request, redirect, url_for, session\n\nAPP_NAME = 'flask-demo'\napp = Flask(APP_NAME)\napp.secret_key = secrets.token_urlsafe(32)\n\n\n@app.route('/callback')\ndef callback():\n   from databricks.sdk.oauth import Consent\n   consent = Consent.from_dict(oauth_client, session['consent'])\n   session['creds'] = consent.exchange_callback_parameters(request.args).as_dict()\n   return redirect(url_for('index'))\n\n\n@app.route('/')\ndef index():\n   if 'creds' not in session:\n      consent = oauth_client.initiate_consent()\n      session['consent'] = consent.as_dict()\n      return redirect(consent.auth_url)\n\n   from databricks.sdk import WorkspaceClient\n   from databricks.sdk.oauth import SessionCredentials\n\n   credentials_provider = SessionCredentials.from_dict(oauth_client, session['creds'])\n   workspace_client = WorkspaceClient(host=oauth_client.host,\n                                      product=APP_NAME,\n                                      credentials_provider=credentials_provider)\n\n   return render_template_string('...', w=workspace_client)\n```\n\n----------------------------------------\n\nTITLE: Listing Notification Destinations in Python\nDESCRIPTION: Lists notification destinations with optional pagination parameters. Returns an iterator over ListNotificationDestinationsResult objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/notification_destinations.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list( [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[ListNotificationDestinationsResult]\n\n    List notification destinations.\n\n    Lists notification destinations.\n\n    :param page_size: int (optional)\n    :param page_token: str (optional)\n\n    :returns: Iterator over :class:`ListNotificationDestinationsResult`\n```\n\n----------------------------------------\n\nTITLE: Updating Dashboard Schedule\nDESCRIPTION: Updates a schedule for a Lakeview dashboard using the LakeviewAPI. Requires the dashboard ID and schedule ID. It allows specifying schedule properties to update. It returns the updated Schedule object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: update_schedule(dashboard_id: str, schedule_id: str [, schedule: Optional[Schedule]]) -> Schedule\n\n    Update dashboard schedule.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard to which the schedule belongs.\n    :param schedule_id: str\n      UUID identifying the schedule.\n    :param schedule: :class:`Schedule` (optional)\n\n    :returns: :class:`Schedule`\n```\n\n----------------------------------------\n\nTITLE: Deleting a Group with Groups API in Python\nDESCRIPTION: This snippet demonstrates how to delete an existing group from a Databricks workspace using the `delete` method of the `GroupsAPI` class. It requires the `databricks-sdk` library and utilizes the `WorkspaceClient` to connect to the Databricks API. It assumes that a group with a specific ID already exists.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/groups.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ngroup = w.groups.create(display_name=f\"sdk-{time.time_ns()}\")\n\nw.groups.delete(id=group.id)\n```\n\n----------------------------------------\n\nTITLE: Getting Restrict Workspace Admins Setting in Python\nDESCRIPTION: Retrieves the restrict workspace admins setting.  An optional etag can be provided for versioning to ensure the response is at least as fresh as the provided etag.  This method is typically called before DELETE or PATCH requests to obtain the latest etag.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/restrict_workspace_admins.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsettings.restrict_workspace_admins.get(etag: Optional[str]) -> RestrictWorkspaceAdminsSetting\n```\n\n----------------------------------------\n\nTITLE: Deleting a Table Monitor in Python\nDESCRIPTION: Deletes a monitor for the specified table.  Requires specific permissions on the table's parent catalog and schema, and must be called from the workspace where the monitor was created.  The metric tables and dashboard are not deleted as part of this call and must be manually cleaned up. Takes the table_name as an input.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/lakehouse_monitors.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(table_name: str)\n\n        Delete a table monitor.\n        \n        Deletes a monitor for the specified table.\n        \n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n        - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent schema - be an\n        owner of the table.\n        \n        Additionally, the call must be made from the workspace where the monitor was created.\n        \n        Note that the metric tables and dashboard will not be deleted as part of this call; those assets must\n        be manually cleaned up (if desired).\n        \n        :param table_name: str\n          Full name of the table.\n```\n\n----------------------------------------\n\nTITLE: Getting a Legacy Query Definition in Databricks SQL\nDESCRIPTION: This method retrieves a specific query definition from Databricks SQL. It takes the query_id as a parameter and returns the query object along with contextual permissions for the authenticated user.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/queries_legacy.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(query_id: str) -> LegacyQuery\n```\n\n----------------------------------------\n\nTITLE: Listing Cluster Policies in Databricks\nDESCRIPTION: This code snippet demonstrates how to list cluster policies using the Databricks SDK for Python. It initializes a WorkspaceClient and calls the `list` method on the `cluster_policies` object. It shows how to import necessary modules and create a WorkspaceClient instance.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/cluster_policies.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import compute\n\nw = WorkspaceClient()\n\nall = w.cluster_policies.list(compute.ListClusterPoliciesRequest())\n```\n\n----------------------------------------\n\nTITLE: Unassign Metastore from Workspace in Python\nDESCRIPTION: This snippet demonstrates how to unassign a metastore from a workspace using the Databricks SDK. It retrieves the workspace ID from an environment variable, creates a metastore, unassigns it from the workspace, and then cleans up by deleting the created metastore. It leverages the WorkspaceClient from the databricks.sdk.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/metastores.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nworkspace_id = os.environ[\"DUMMY_WORKSPACE_ID\"]\n\ncreated = w.metastores.create(\n    name=f\"sdk-{time.time_ns()}\",\n    storage_root=\"s3://%s/%s\" % (os.environ[\"TEST_BUCKET\"], f\"sdk-{time.time_ns()}\"),\n)\n\nw.metastores.unassign(metastore_id=created.metastore_id, workspace_id=workspace_id)\n\n# cleanup\nw.metastores.delete(id=created.metastore_id, force=True)\n```\n\n----------------------------------------\n\nTITLE: Updating CspEnablementSetting Python\nDESCRIPTION: Updates the Compliance Security Profile setting for the workspace. Requires a fresh etag for optimistic concurrency control. The `field_mask` parameter specifies which fields of the setting are to be updated, and `allow_missing` should always be set to true. Returns the updated `CspEnablementSetting` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/csp_enablement.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(allow_missing: bool, setting: CspEnablementSetting, field_mask: str) -> CspEnablementSetting\n\n    Update the compliance security profile setting.\n        \n    Updates the compliance security profile setting for the workspace. A fresh etag needs to be provided\n    in `PATCH` requests (as part of the setting field). The etag can be retrieved by making a `GET`\n    request before the `PATCH` request. If the setting is updated concurrently, `PATCH` fails with 409 and\n    the request must be retried by using the fresh etag in the 409 response.\n        \n    :param allow_missing: bool\n      This should always be set to true for Settings API. Added for AIP compliance.\n    :param setting: :class:`CspEnablementSetting`\n    :param field_mask: str\n      Field mask is required to be passed into the PATCH request. Field mask specifies which fields of the\n      setting payload will be updated. The field mask needs to be supplied as single string. To specify\n      multiple fields in the field mask, use comma as the separator (no space).\n        \n    :returns: :class:`CspEnablementSetting`\n```\n\n----------------------------------------\n\nTITLE: Creating an Output Catalog in Python\nDESCRIPTION: Creates an output catalog for the clean room. Requires the clean_room_name as a string and an optional CleanRoomOutputCatalog object. Returns a CreateCleanRoomOutputCatalogResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/cleanrooms/clean_rooms.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create_output_catalog(clean_room_name: str [, output_catalog: Optional[CleanRoomOutputCatalog]]) -> CreateCleanRoomOutputCatalogResponse\n\n        Create an output catalog.\n\n        Create the output catalog of the clean room.\n\n        :param clean_room_name: str\n          Name of the clean room.\n        :param output_catalog: :class:`CleanRoomOutputCatalog` (optional)\n\n        :returns: :class:`CreateCleanRoomOutputCatalogResponse`\n```\n\n----------------------------------------\n\nTITLE: Getting an Endpoint in Databricks\nDESCRIPTION: This code snippet describes the method for retrieving an Endpoint object using the Databricks SDK. It takes the name of the Endpoint as a string parameter and returns the Endpoint object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/endpoints.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(name: str) -> Endpoint\n\n    Get an Endpoint.\n        \n    :param name: str\n        \n    :returns: :class:`Endpoint`\n```\n\n----------------------------------------\n\nTITLE: Listing Monitor Refreshes in Python\nDESCRIPTION: Retrieves a history of the most recent refreshes (up to 25) for a given table.  Requires specific permissions on the table's parent catalog and schema, and must be called from the workspace where the monitor was created. Returns an iterator over MonitorRefreshInfo objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/lakehouse_monitors.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list_refreshes(table_name: str) -> Iterator[MonitorRefreshInfo]\n\n        List refreshes.\n        \n        Gets an array containing the history of the most recent refreshes (up to 25) for this table.\n        \n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n        - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent schema -\n        **SELECT** privilege on the table.\n        \n        Additionally, the call must be made from the workspace where the monitor was created.\n        \n        :param table_name: str\n          Full name of the table.\n        \n        :returns: Iterator over :class:`MonitorRefreshInfo`\n```\n\n----------------------------------------\n\nTITLE: Defining AppDeploymentMode Enum (Python)\nDESCRIPTION: This Python code defines an enumeration `AppDeploymentMode` with two possible values: `AUTO_SYNC` and `SNAPSHOT`. It specifies the mode used for deploying an application.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/apps.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: AppDeploymentMode\n\n   .. py:attribute:: AUTO_SYNC\n      :value: \"AUTO_SYNC\"\n\n   .. py:attribute:: SNAPSHOT\n      :value: \"SNAPSHOT\"\n```\n\n----------------------------------------\n\nTITLE: Updating an Alert with Databricks SDK\nDESCRIPTION: This snippet demonstrates how to update a Databricks SQL alert using the Databricks SDK for Python. It creates a query and an alert, then updates the alert's display name. Finally, it cleans up by deleting the query and the alert.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/alerts.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\nsrcs = w.data_sources.list()\n\nquery = w.queries.create(\n    query=sql.CreateQueryRequestQuery(\n        display_name=f\"sdk-{time.time_ns()}\",\n        warehouse_id=srcs[0].warehouse_id,\n        description=\"test query from Go SDK\",\n        query_text=\"SELECT 1\",\n    )\n)\n\nalert = w.alerts.create(\n    alert=sql.CreateAlertRequestAlert(\n        condition=sql.AlertCondition(\n            operand=sql.AlertConditionOperand(column=sql.AlertOperandColumn(name=\"1\")),\n            op=sql.AlertOperator.EQUAL,\n            threshold=sql.AlertConditionThreshold(value=sql.AlertOperandValue(double_value=1)),\n        ),\n        display_name=f\"sdk-{time.time_ns()}\",\n        query_id=query.id,\n    )\n)\n\n_ = w.alerts.update(\n    id=alert.id,\n    alert=sql.UpdateAlertRequestAlert(display_name=f\"sdk-{time.time_ns()}\"),\n    update_mask=\"display_name\",\n)\n\n# cleanup\nw.queries.delete(id=query.id)\nw.alerts.delete(id=alert.id)\n```\n\n----------------------------------------\n\nTITLE: Create Encryption Key Configuration in Databricks with AccountClient\nDESCRIPTION: This snippet demonstrates how to create an encryption key configuration using the AccountClient in the Databricks SDK. It sets up the necessary AWS key information and specifies the use case as MANAGED_SERVICES. The code also includes a cleanup step to delete the created key.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/encryption_keys.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import provisioning\n\na = AccountClient()\n\ncreated = a.encryption_keys.create(\n    aws_key_info=provisioning.CreateAwsKeyInfo(\n        key_arn=os.environ[\"TEST_MANAGED_KMS_KEY_ARN\"],\n        key_alias=os.environ[\"TEST_STORAGE_KMS_KEY_ALIAS\"],\n    ),\n    use_cases=[provisioning.KeyUseCase.MANAGED_SERVICES],\n)\n\n# cleanup\na.encryption_keys.delete(customer_managed_key_id=created.customer_managed_key_id)\n```\n\n----------------------------------------\n\nTITLE: Upgrading Databricks SDK in Notebook\nDESCRIPTION: This command upgrades the Databricks SDK for Python within a Databricks notebook environment.  It uses the %pip magic command to upgrade the package and then restarts the Python interpreter to load the new version.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/getting-started.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%pip install --upgrade databricks-sdk\n```\n\n----------------------------------------\n\nTITLE: Creating and Waiting for an App\nDESCRIPTION: Creates a new app and waits for it to become idle. Includes an optional timeout parameter to specify how long to wait.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nw.apps.create_and_wait(name: str [, description: Optional[str], timeout: datetime.timedelta = 0:20:00]) -> App\n```\n\n----------------------------------------\n\nTITLE: Migrating Permissions API Calls in Databricks SDK (Python)\nDESCRIPTION: This snippet demonstrates how to migrate permissions API calls from the old w.permissions.get() method to the new service-specific methods in the Databricks SDK. It outlines the changes required for getting, setting, and updating permissions, as well as retrieving permission levels for various services.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nw.permissions.get() --> w.<Service>.get_<Service>_permissions()\nw.permissions.get_by_request_object_id_and_request_object_type() --> w.<Service>.get_<Service>_permissions()\nw.permissions.get_permission_levels() --> w.<Service>.get_<Service>_permission_levels()\nw.permissions.set() --> w.<Service>.set_<Service>_permissions()\nw.permissions.update() --> w.<Service>.update_<Service>_permissions()\n```\n\n----------------------------------------\n\nTITLE: Updating CSP Enablement Account Setting - Python\nDESCRIPTION: This method updates the Compliance Security Profile (CSP) setting for new workspaces. It requires `allow_missing` (always set to true), a `CspEnablementAccountSetting` object, and a `field_mask` string to specify which fields to update. It returns the updated `CspEnablementAccountSetting` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/csp_enablement_account.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(allow_missing: bool, setting: CspEnablementAccountSetting, field_mask: str) -> CspEnablementAccountSetting\n\n    Update the compliance security profile setting for new workspaces.\n\n    Updates the value of the compliance security profile setting for new workspaces.\n\n    :param allow_missing: bool\n      This should always be set to true for Settings API. Added for AIP compliance.\n    :param setting: :class:`CspEnablementAccountSetting`\n    :param field_mask: str\n      The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n      field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n      `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n      the entire collection field can be specified. Field names must exactly match the resource field\n      names.\n\n      A field mask of `*` indicates full replacement. It’s recommended to always explicitly list the\n      fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n      changes in the future.\n\n    :returns: :class:`CspEnablementAccountSetting`\n```\n\n----------------------------------------\n\nTITLE: Creating an Alert with Databricks SDK\nDESCRIPTION: This snippet demonstrates how to create a Databricks SQL alert using the Databricks SDK for Python. It first creates a query and then uses the query ID to create an alert with a specified condition. It cleans up by deleting the created query and alert.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/alerts.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\nsrcs = w.data_sources.list()\n\nquery = w.queries.create(\n    query=sql.CreateQueryRequestQuery(\n        display_name=f\"sdk-{time.time_ns()}\",\n        warehouse_id=srcs[0].warehouse_id,\n        description=\"test query from Go SDK\",\n        query_text=\"SELECT 1\",\n    )\n)\n\nalert = w.alerts.create(\n    alert=sql.CreateAlertRequestAlert(\n        condition=sql.AlertCondition(\n            operand=sql.AlertConditionOperand(column=sql.AlertOperandColumn(name=\"1\")),\n            op=sql.AlertOperator.EQUAL,\n            threshold=sql.AlertConditionThreshold(value=sql.AlertOperandValue(double_value=1)),\n        ),\n        display_name=f\"sdk-{time.time_ns()}\",\n        query_id=query.id,\n    )\n)\n\n# cleanup\nw.queries.delete(id=query.id)\nw.alerts.delete(id=alert.id)\n```\n\n----------------------------------------\n\nTITLE: Creating and Waiting for Online Table\nDESCRIPTION: This method creates an online table and waits for it to become active. It optionally accepts an OnlineTable object for configuration and a timeout duration for the wait operation. It returns the active OnlineTable object after successful creation and activation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/online_tables.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create_and_wait( [, table: Optional[OnlineTable], timeout: datetime.timedelta = 0:20:00]) -> OnlineTable\n```\n\n----------------------------------------\n\nTITLE: Restoring a Dashboard using Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to restore a deleted dashboard using the Databricks SDK. It creates a dashboard, restores it using the `restore` method, and then cleans up by deleting the created dashboard.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/dashboards.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.dashboards.create(name=f\"sdk-{time.time_ns()}\")\n\nw.dashboards.restore(dashboard_id=created.id)\n\n# cleanup\nw.dashboards.delete(dashboard_id=created.id)\n```\n\n----------------------------------------\n\nTITLE: Getting Monitor Refresh Info in Python\nDESCRIPTION: Gets information about a specific monitor refresh given a table name and refresh ID. Requires specific permissions on the table's parent catalog and schema, and must be called from the workspace where the monitor was created. Returns a MonitorRefreshInfo object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/lakehouse_monitors.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_refresh(table_name: str, refresh_id: str) -> MonitorRefreshInfo\n\n        Get refresh.\n        \n        Gets info about a specific monitor refresh using the given refresh ID.\n        \n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n        - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent schema -\n        **SELECT** privilege on the table.\n        \n        Additionally, the call must be made from the workspace where the monitor was created.\n        \n        :param table_name: str\n          Full name of the table.\n        :param refresh_id: str\n          ID of the refresh.\n        \n        :returns: :class:`MonitorRefreshInfo`\n```\n\n----------------------------------------\n\nTITLE: Using dbutils with Databricks SDK\nDESCRIPTION: This snippet demonstrates how to use the `dbutils` property on the `WorkspaceClient` to interact with Databricks utilities. It retrieves the list of files in the root directory and prints the number of files. The `WorkspaceClient` is initialized, and then the `dbutils` object is accessed from it.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\ndbutils = w.dbutils\n\nfiles_in_root = dbutils.fs.ls('/')\nprint(f'number of files in root: {len(files_in_root)}')\n```\n\n----------------------------------------\n\nTITLE: Defining ConditionTaskOp Enum Python\nDESCRIPTION: This code snippet defines an enumeration called `ConditionTaskOp` representing operators for comparing task values within condition tasks. It includes `EQUAL_TO`, `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL`, and `NOT_EQUAL`. Comparison details are provided in the description.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ConditionTaskOp\n\n   * `EQUAL_TO`, `NOT_EQUAL` operators perform string comparison of their operands. This means that `“12.0” == “12”` will evaluate to `false`. * `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL` operators perform numeric comparison of their operands. `“12.0” >= “12”` will evaluate to `true`, `“10.0” >= “12”` will evaluate to `false`.\n   The boolean comparison to task values can be implemented with operators `EQUAL_TO`, `NOT_EQUAL`. If a task value was set to a boolean value, it will be serialized to `“true”` or `“false”` for the comparison.\n\n   .. py:attribute:: EQUAL_TO\n      :value: \"EQUAL_TO\"\n\n   .. py:attribute:: GREATER_THAN\n      :value: \"GREATER_THAN\"\n\n   .. py:attribute:: GREATER_THAN_OR_EQUAL\n      :value: \"GREATER_THAN_OR_EQUAL\"\n\n   .. py:attribute:: LESS_THAN\n      :value: \"LESS_THAN\"\n\n   .. py:attribute:: LESS_THAN_OR_EQUAL\n      :value: \"LESS_THAN_OR_EQUAL\"\n\n   .. py:attribute:: NOT_EQUAL\n      :value: \"NOT_EQUAL\"\n```\n\n----------------------------------------\n\nTITLE: Adding Warehouse Permissions Methods - Python\nDESCRIPTION: This snippet introduces methods for managing warehouse permissions within the Databricks workspace via the `w.warehouses` service. The added methods are: `get_warehouse_permission_levels()`, `get_warehouse_permissions()`, `set_warehouse_permissions()`, and `update_warehouse_permissions()`. These methods enable the management of permissions for SQL warehouses.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nw.warehouses.get_warehouse_permission_levels()\nw.warehouses.get_warehouse_permissions()\nw.warehouses.set_warehouse_permissions()\nw.warehouses.update_warehouse_permissions()\n```\n\n----------------------------------------\n\nTITLE: Creating a Storage Configuration with Databricks Account API in Python\nDESCRIPTION: This code snippet demonstrates how to create a new storage configuration using the Databricks Account API. It initializes the AccountClient, then creates a storage configuration using a unique name and RootBucketInfo, which includes the bucket name sourced from an environment variable. Finally, it cleans up by deleting the created storage configuration.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/storage.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import provisioning\n\na = AccountClient()\n\nstorage = a.storage.create(\n    storage_configuration_name=f\"sdk-{time.time_ns()}\",\n    root_bucket_info=provisioning.RootBucketInfo(bucket_name=os.environ[\"TEST_ROOT_BUCKET\"]),\n)\n\n# cleanup\na.storage.delete(storage_configuration_id=storage.storage_configuration_id)\n```\n\n----------------------------------------\n\nTITLE: Delete Legacy Access Disablement (Python)\nDESCRIPTION: Deletes the legacy access disablement status. It is recommended to use the etag obtained from a GET request to prevent race conditions.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/disable_legacy_access.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete( [, etag: Optional[str]]) -> DeleteDisableLegacyAccessResponse\n\n    Delete Legacy Access Disablement Status.\n\n    Deletes legacy access disablement status.\n\n    :param etag: str (optional)\n      etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n      optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n      each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n      to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n      request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n    :returns: :class:`DeleteDisableLegacyAccessResponse`\n```\n\n----------------------------------------\n\nTITLE: List Service Principal Secrets (Python)\nDESCRIPTION: Lists all secrets associated with a given service principal. The `service_principal_id` parameter specifies the service principal. The `page_token` parameter is used for pagination. Returns an iterator over `SecretInfo` objects, but does not include the secret values.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/service_principal_secrets.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list(service_principal_id: int [, page_token: Optional[str]]) -> Iterator[SecretInfo]\n\n    List service principal secrets.\n\n    List all secrets associated with the given service principal. This operation only returns information\n    about the secrets themselves and does not include the secret values.\n\n    :param service_principal_id: int\n      The service principal ID.\n    :param page_token: str (optional)\n      An opaque page token which was the `next_page_token` in the response of the previous request to list\n      the secrets for this service principal. Provide this token to retrieve the next page of secret\n      entries. When providing a `page_token`, all other parameters provided to the request must match the\n      previous request. To list all of the secrets for a service principal, it is necessary to continue\n      requesting pages of entries until the response contains no `next_page_token`. Note that the number\n      of entries returned must not be used to determine when the listing is complete.\n\n    :returns: Iterator over :class:`SecretInfo`\n```\n\n----------------------------------------\n\nTITLE: Updating Model Example in Model Registry - Python\nDESCRIPTION: This code snippet demonstrates how to update a model's description in the Databricks Model Registry using the Databricks SDK for Python. It initializes a WorkspaceClient, creates a model, retrieves it, and then updates its description using the 'update_model' method.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/model_registry.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.model_registry.create_model(name=f\"sdk-{time.time_ns()}\")\n\nmodel = w.model_registry.get_model(name=created.registered_model.name)\n\nw.model_registry.update_model(\n    name=model.registered_model_databricks.name,\n    description=f\"sdk-{time.time_ns()}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Root Files Using WorkspaceClient in Python\nDESCRIPTION: This snippet demonstrates how to list files in the root directory of the Databricks file system using the `dbutils.fs.ls()` method accessed through the `WorkspaceClient`. It initializes a `WorkspaceClient`, retrieves the `dbutils` object, and then lists the files. This operation requires a Databricks cluster.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbutils.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient()\ndbutils = w.dbutils\nfiles_in_root = dbutils.fs.ls('/')\nprint(f'number of files in root: {len(files_in_root)}')\n```\n\n----------------------------------------\n\nTITLE: Listing SQL Queries with WorkspaceClient\nDESCRIPTION: This Python code snippet demonstrates how to list all SQL queries in a Databricks workspace using the `WorkspaceClient`. It initializes the client, iterates through the queries, and prints the name and creation time of each query. It depends on the `databricks.sdk` package.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/getting-started.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Authenticate as described above\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient()\nfor query in w.queries.list():\n    print(f'query {query.name} was created at {query.created_at}')\n```\n\n----------------------------------------\n\nTITLE: Get Rule Set - Python\nDESCRIPTION: Retrieves a rule set by its name. Rule sets are attached to a resource and contain access rules for that resource. Etag is used for optimistic concurrency control during updates.\n\nParameters:\nname (str): The ruleset name associated with the request.\netag (str): Etag used for versioning.\n\nReturns: RuleSetResponse\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/account_access_control_proxy.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_rule_set(name: str, etag: str) -> RuleSetResponse\n\n    Get a rule set.\n\n    Get a rule set by its name. A rule set is always attached to a resource and contains a list of access\n    rules on the said resource. Currently only a default rule set for each resource is supported.\n\n    :param name: str\n      The ruleset name associated with the request.\n    :param etag: str\n      Etag used for versioning. The response is at least as fresh as the eTag provided. Etag is used for\n      optimistic concurrency control as a way to help prevent simultaneous updates of a rule set from\n      overwriting each other. It is strongly suggested that systems make use of the etag in the read ->\n      modify -> write pattern to perform rule set updates in order to avoid race conditions that is get an\n      etag from a GET rule set request, and pass it with the PUT update request to identify the rule set\n      version you are updating.\n\n    :returns: :class:`RuleSetResponse`\n```\n\n----------------------------------------\n\nTITLE: Updating a Credential in Databricks (Python)\nDESCRIPTION: This method updates a service or storage credential. The caller must be the owner of the credential, a metastore admin, or have the 'MANAGE' permission. Metastore admins can only change the owner. The 'force' parameter allows updates even with dependent services/locations.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/credentials.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update_credential(name_arg: str [, aws_iam_role: Optional[AwsIamRole], azure_managed_identity: Optional[AzureManagedIdentity], azure_service_principal: Optional[AzureServicePrincipal], comment: Optional[str], databricks_gcp_service_account: Optional[DatabricksGcpServiceAccount], force: Optional[bool], isolation_mode: Optional[IsolationMode], new_name: Optional[str], owner: Optional[str], read_only: Optional[bool], skip_validation: Optional[bool]]) -> CredentialInfo\n\n        Update a credential.\n\n        Updates a service or storage credential on the metastore.\n\n        The caller must be the owner of the credential or a metastore admin or have the `MANAGE` permission.\n        If the caller is a metastore admin, only the __owner__ field can be changed.\n\n        :param name_arg: str\n          Name of the credential.\n        :param aws_iam_role: :class:`AwsIamRole` (optional)\n          The AWS IAM role configuration\n        :param azure_managed_identity: :class:`AzureManagedIdentity` (optional)\n          The Azure managed identity configuration.\n        :param azure_service_principal: :class:`AzureServicePrincipal` (optional)\n          The Azure service principal configuration. Only applicable when purpose is **STORAGE**.\n        :param comment: str (optional)\n          Comment associated with the credential.\n        :param databricks_gcp_service_account: :class:`DatabricksGcpServiceAccount` (optional)\n          GCP long-lived credential. Databricks-created Google Cloud Storage service account.\n        :param force: bool (optional)\n          Force an update even if there are dependent services (when purpose is **SERVICE**) or dependent\n          external locations and external tables (when purpose is **STORAGE**).\n        :param isolation_mode: :class:`IsolationMode` (optional)\n          Whether the current securable is accessible from all workspaces or a specific set of workspaces.\n        :param new_name: str (optional)\n          New name of credential.\n        :param owner: str (optional)\n          Username of current owner of credential.\n        :param read_only: bool (optional)\n          Whether the credential is usable only for read operations. Only applicable when purpose is\n          **STORAGE**.\n        :param skip_validation: bool (optional)\n          Supply true to this argument to skip validation of the updated credential.\n\n        :returns: :class:`CredentialInfo`\n```\n\n----------------------------------------\n\nTITLE: Listing Clean Room Task Runs in Python\nDESCRIPTION: This snippet describes the method for listing notebook task runs within a specified clean room using the `list` method of the `CleanRoomTaskRunsAPI`. It takes the clean room name as a required parameter and optional parameters for notebook name, page size, and page token for pagination.  It returns an iterator over `CleanRoomNotebookTaskRun` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/cleanrooms/clean_room_task_runs.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list(clean_room_name: str [, notebook_name: Optional[str], page_size: Optional[int], page_token: Optional[str]]) -> Iterator[CleanRoomNotebookTaskRun]\n\n    List notebook task runs.\n\n    List all the historical notebook task runs in a clean room.\n\n    :param clean_room_name: str\n      Name of the clean room.\n    :param notebook_name: str (optional)\n      Notebook name\n    :param page_size: int (optional)\n      The maximum number of task runs to return. Currently ignored - all runs will be returned.\n    :param page_token: str (optional)\n      Opaque pagination token to go to next page based on previous query.\n\n    :returns: Iterator over :class:`CleanRoomNotebookTaskRun`\n```\n\n----------------------------------------\n\nTITLE: Get Job Policy Compliance Status with Databricks SDK (Python)\nDESCRIPTION: Returns the policy compliance status of a job, given its ID. Jobs could be out of compliance if a cluster policy they use was updated after the job was last edited and some of its job clusters no longer comply. Returns a GetPolicyComplianceResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/policy_compliance_for_jobs.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: get_compliance(job_id: int) -> GetPolicyComplianceResponse\n\n    Get job policy compliance.\n\n    Returns the policy compliance status of a job. Jobs could be out of compliance if a cluster policy\n    they use was updated after the job was last edited and some of its job clusters no longer comply with\n    their updated policies.\n\n    :param job_id: int\n      The ID of the job whose compliance status you are requesting.\n\n    :returns: :class:`GetPolicyComplianceResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining ApplicationState Enum (Python)\nDESCRIPTION: This Python code defines an enumeration `ApplicationState` with values representing the possible states of an application, such as `CRASHED`, `DEPLOYING`, `RUNNING`, and `UNAVAILABLE`. This enumeration provides the status of the application.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/apps.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ApplicationState\n\n   .. py:attribute:: CRASHED\n      :value: \"CRASHED\"\n\n   .. py:attribute:: DEPLOYING\n      :value: \"DEPLOYING\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n\n   .. py:attribute:: UNAVAILABLE\n      :value: \"UNAVAILABLE\"\n```\n\n----------------------------------------\n\nTITLE: Updating a Databricks Repo in Python\nDESCRIPTION: This snippet shows how to update a Databricks Repo using the WorkspaceClient and the repos.update method. It first creates a repo, then updates it to a specific branch, and finally cleans up by deleting the repo.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/repos.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nroot = f\"sdk-{time.time_ns()}\"\n\nri = w.repos.create(\n    path=root,\n    url=\"https://github.com/shreyas-goenka/empty-repo.git\",\n    provider=\"github\",\n)\n\nw.repos.update(repo_id=ri.id, branch=\"foo\")\n\n# cleanup\nw.repos.delete(repo_id=ri.id)\n```\n\n----------------------------------------\n\nTITLE: Update Federation Policy (Python)\nDESCRIPTION: Updates an existing account federation policy, identified by its `policy_id`.  Allows specifying which fields to update using the `update_mask`. Returns the updated `FederationPolicy` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/federation_policy.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(policy_id: str [, policy: Optional[FederationPolicy], update_mask: Optional[str]]) -> FederationPolicy\n\n    Update account federation policy.\n\n    :param policy_id: str\n      The identifier for the federation policy.\n    :param policy: :class:`FederationPolicy` (optional)\n    :param update_mask: str (optional)\n      The field mask specifies which fields of the policy to update. To specify multiple fields in the\n      field mask, use comma as the separator (no space). The special value '*' indicates that all fields\n      should be updated (full replacement). If unspecified, all fields that are set in the policy provided\n      in the update request will overwrite the corresponding fields in the existing policy. Example value:\n      'description,oidc_policy.audiences'.\n\n    :returns: :class:`FederationPolicy`\n```\n\n----------------------------------------\n\nTITLE: Replacing Private Access Settings with AccountClient in Python\nDESCRIPTION: This code snippet demonstrates how to replace (update) a private access settings object using the AccountClient in the Databricks SDK for Python. It first creates a private access setting, then replaces its properties, and finally cleans up by deleting the setting. It depends on the 'os' and 'time' modules.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/private_access.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\ncreated = a.private_access.create(\n    private_access_settings_name=f\"sdk-{time.time_ns()}\",\n    region=os.environ[\"AWS_REGION\"],\n)\n\na.private_access.replace(\n    private_access_settings_id=created.private_access_settings_id,\n    private_access_settings_name=f\"sdk-{time.time_ns()}\",\n    region=os.environ[\"AWS_REGION\"],\n)\n\n# cleanup\na.private_access.delete(private_access_settings_id=created.private_access_settings_id)\n```\n\n----------------------------------------\n\nTITLE: Generating a Full Query Result Download in Genie using Python\nDESCRIPTION: This method initiates a full SQL query result download from Genie and returns a download ID. This ID can be used to track the download progress and retrieve the result's external link. Databricks strongly recommends protecting the returned URLs. The method returns a GenieGenerateDownloadFullQueryResultResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/genie.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: generate_download_full_query_result(space_id: str, conversation_id: str, message_id: str, attachment_id: str) -> GenieGenerateDownloadFullQueryResultResponse\n\n        Generate full query result download.\n\n        Initiate full SQL query result download and obtain a `download_id` to track the download progress.\n        This call initiates a new SQL execution to generate the query result. The result is stored in an\n        external link can be retrieved using the [Get Download Full Query\n        Result](:method:genie/getdownloadfullqueryresult) API. Warning: Databricks strongly recommends that\n        you protect the URLs that are returned by the `EXTERNAL_LINKS` disposition. See [Execute\n        Statement](:method:statementexecution/executestatement) for more details.\n\n        :param space_id: str\n          Space ID\n        :param conversation_id: str\n          Conversation ID\n        :param message_id: str\n          Message ID\n        :param attachment_id: str\n          Attachment ID\n\n        :returns: :class:`GenieGenerateDownloadFullQueryResultResponse`\n```\n\n----------------------------------------\n\nTITLE: Getting Enable Export Notebook Setting - Python\nDESCRIPTION: This snippet demonstrates how to retrieve the current 'Enable Export Notebook' setting using the `get_enable_export_notebook()` method. This method returns an `EnableExportNotebook` object representing the setting's state.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/enable_export_notebook.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: get_enable_export_notebook() -> EnableExportNotebook\n\n        Get the Enable Export Notebook setting.\n\n        Gets the Enable Export Notebook setting.\n\n        :returns: :class:`EnableExportNotebook`\n```\n\n----------------------------------------\n\nTITLE: Getting VPC Endpoint Configuration by ID with AccountClient in Python\nDESCRIPTION: This code retrieves a VPC endpoint configuration using its ID. It assumes a VPC endpoint already exists. It first creates a new VPC endpoint, then retrieves it using `vpc_endpoints.get`, and finally cleans up by deleting the created endpoint. It relies on environment variables `TEST_RELAY_VPC_ENDPOINT` and `AWS_REGION` for creation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/vpc_endpoints.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\ncreated = a.vpc_endpoints.create(\n    aws_vpc_endpoint_id=os.environ[\"TEST_RELAY_VPC_ENDPOINT\"],\n    region=os.environ[\"AWS_REGION\"],\n    vpc_endpoint_name=f\"sdk-{time.time_ns()}\",\n)\n\nby_id = a.vpc_endpoints.get(vpc_endpoint_id=created.vpc_endpoint_id)\n\n# cleanup\na.vpc_endpoints.delete(vpc_endpoint_id=created.vpc_endpoint_id)\n```\n\n----------------------------------------\n\nTITLE: Getting Lakeview Dashboard\nDESCRIPTION: Retrieves a specific Lakeview dashboard using the LakeviewAPI.  The dashboard is identified by its UUID. It returns the Dashboard object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: get(dashboard_id: str) -> Dashboard\n\n    Get dashboard.\n\n    Get a draft dashboard.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard.\n\n    :returns: :class:`Dashboard`\n```\n\n----------------------------------------\n\nTITLE: Pinning a Cluster in Databricks with Python\nDESCRIPTION: This snippet demonstrates how to pin a Databricks cluster using the Databricks SDK for Python. It creates a cluster, pins it, then deletes it. The environment variable TEST_INSTANCE_POOL_ID must be set.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nlatest = w.clusters.select_spark_version(latest=True, long_term_support=True)\n\ncluster_name = f\"sdk-{time.time_ns()}\"\n\nclstr = w.clusters.create(\n    cluster_name=cluster_name,\n    spark_version=latest,\n    instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n    autotermination_minutes=15,\n    num_workers=1,\n).result()\n\nw.clusters.pin(cluster_id=clstr.cluster_id)\n\n# cleanup\nw.clusters.permanent_delete(cluster_id=clstr.cluster_id)\n```\n\n----------------------------------------\n\nTITLE: Get file metadata (Python)\nDESCRIPTION: Retrieves metadata for a file at the provided file_path. The response HTTP headers contain the metadata. There is no response body.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/files/files.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_metadata(file_path: str) -> GetMetadataResponse\n\n    Get file metadata.\n\n    Get the metadata of a file. The response HTTP headers contain the metadata. There is no response body.\n\n    :param file_path: str\n      The absolute path of the file.\n\n    :returns: :class:`GetMetadataResponse`\n```\n\n----------------------------------------\n\nTITLE: List Custom OAuth App Integrations (Python)\nDESCRIPTION: Retrieves a list of custom OAuth app integrations. It supports pagination through optional parameters like page_size and page_token. The include_creator_username parameter can be used.  Returns an iterator over GetCustomAppIntegrationOutput objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/custom_app_integration.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list( [, include_creator_username: Optional[bool], page_size: Optional[int], page_token: Optional[str]]) -> Iterator[GetCustomAppIntegrationOutput]\n\n    Get custom oauth app integrations.\n\n    Get the list of custom OAuth app integrations for the specified Databricks account\n\n    :param include_creator_username: bool (optional)\n    :param page_size: int (optional)\n    :param page_token: str (optional)\n\n    :returns: Iterator over :class:`GetCustomAppIntegrationOutput`\n```\n\n----------------------------------------\n\nTITLE: Defining CommandStatus Enum in Python\nDESCRIPTION: This code defines an enumeration (`CommandStatus`) for the status of a command executed in a Databricks notebook context.  The possible states are CANCELLED, CANCELLING, ERROR, FINISHED, QUEUED, and RUNNING, representing the different phases of command execution.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: CommandStatus\n\n   .. py:attribute:: CANCELLED\n      :value: \"CANCELLED\"\n\n   .. py:attribute:: CANCELLING\n      :value: \"CANCELLING\"\n\n   .. py:attribute:: ERROR\n      :value: \"ERROR\"\n\n   .. py:attribute:: FINISHED\n      :value: \"FINISHED\"\n\n   .. py:attribute:: QUEUED\n      :value: \"QUEUED\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n```\n\n----------------------------------------\n\nTITLE: List Providers - Python\nDESCRIPTION: Lists provider profiles for the account in the Databricks Marketplace. It supports optional parameters for pagination: `page_size` (int) to limit the number of results per page, and `page_token` (str) for retrieving the next page of results. The method returns an iterator over ProviderInfo objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_providers.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list( [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[ProviderInfo]\n\n    List providers.\n\n    List provider profiles for account.\n\n    :param page_size: int (optional)\n    :param page_token: str (optional)\n\n    :returns: Iterator over :class:`ProviderInfo`\n```\n\n----------------------------------------\n\nTITLE: Defining RedashConfigAPI Class (Python)\nDESCRIPTION: This code defines the `RedashConfigAPI` class within the `databricks.sdk.service.sql` module. It includes the `get_config()` method, which retrieves the workspace configuration for Redash V2 and returns a `ClientConfig` object. The class serves as an interface for interacting with the internal Redash V2 configuration service.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/redash_config.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: RedashConfigAPI\n\n    Redash V2 service for workspace configurations (internal)\n\n    .. py:method:: get_config() -> ClientConfig\n\n        Read workspace configuration for Redash-v2.\n\n        :returns: :class:`ClientConfig`\n```\n\n----------------------------------------\n\nTITLE: Update ESM Enablement Setting (Python)\nDESCRIPTION: Updates the enhanced security monitoring setting using the update method of the EsmEnablementAPI. It requires parameters for allowing missing settings, the setting object itself, and a field mask to specify which fields are being updated. A fresh etag is required to prevent concurrent update conflicts.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/esm_enablement.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(allow_missing: bool, setting: EsmEnablementSetting, field_mask: str) -> EsmEnablementSetting\n\n    Update the enhanced security monitoring setting.\n        \n    Updates the enhanced security monitoring setting for the workspace. A fresh etag needs to be provided\n    in `PATCH` requests (as part of the setting field). The etag can be retrieved by making a `GET`\n    request before the `PATCH` request. If the setting is updated concurrently, `PATCH` fails with 409 and\n    the request must be retried by using the fresh etag in the 409 response.\n        \n    :param allow_missing: bool\n      This should always be set to true for Settings API. Added for AIP compliance.\n    :param setting: :class:`EsmEnablementSetting`\n    :param field_mask: str\n      Field mask is required to be passed into the PATCH request. Field mask specifies which fields of the\n      setting payload will be updated. The field mask needs to be supplied as single string. To specify\n      multiple fields in the field mask, use comma as the separator (no space).\n        \n    :returns: :class:`EsmEnablementSetting`\n```\n\n----------------------------------------\n\nTITLE: Handling ResourceNotFoundException in Python\nDESCRIPTION: This code snippet demonstrates how to handle the ResourceDoesNotExist exception when attempting to retrieve a non-existent job by ID using the Databricks SDK for Python. It shows the updated error handling approach after a behavior change where InvalidParameterValue exceptions are now ResourceDoesNotExist exceptions for non-existent resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ntry:\n    w.jobs.get_by_id(\"123\")\nexcept e as ResourceDoesNotExist:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Creating a Visualization in Databricks SQL (Legacy)\nDESCRIPTION: Creates a new visualization for a given query. This method takes the query ID, visualization type, and options as input. It is recommended to use the new `queryvisualizations/create` endpoint instead of this legacy API. Returns a `LegacyVisualization` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/query_visualizations_legacy.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef create(query_id: str, type: str, options: Any [, description: Optional[str], name: Optional[str]]) -> LegacyVisualization:\n    \"\"\"Add visualization to a query.\n\n    Creates visualization in the query.\n\n    **Note**: A new version of the Databricks SQL API is now available. Please use\n    :method:queryvisualizations/create instead. [Learn more]\n\n    [Learn more]: https://docs.databricks.com/en/sql/dbsql-api-latest.html\n\n    :param query_id: str\n      The identifier returned by :method:queries/create\n    :param type: str\n      The type of visualization: chart, table, pivot table, and so on.\n    :param options: Any\n      The options object varies widely from one visualization type to the next and is unsupported.\n      Databricks does not recommend modifying visualization settings in JSON.\n    :param description: str (optional)\n      A short description of this visualization. This is not displayed in the UI.\n    :param name: str (optional)\n      The name of the visualization that appears on dashboards and the query screen.\n\n    :returns: :class:`LegacyVisualization`\n    \"\"\n```\n\n----------------------------------------\n\nTITLE: Create Metastore - Python\nDESCRIPTION: Creates a Unity Catalog metastore. Takes an optional `CreateMetastore` object as input. Returns an `AccountsMetastoreInfo` object representing the newly created metastore.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/catalog/metastores.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create( [, metastore_info: Optional[CreateMetastore]]) -> AccountsMetastoreInfo\n\n    Create metastore.\n\n    Creates a Unity Catalog metastore.\n\n    :param metastore_info: :class:`CreateMetastore` (optional)\n\n    :returns: :class:`AccountsMetastoreInfo`\n```\n\n----------------------------------------\n\nTITLE: List Global Init Scripts in Databricks\nDESCRIPTION: This code snippet demonstrates how to list all global init scripts in a Databricks workspace using the Databricks SDK. It retrieves a list of all scripts, but not their content. For content retrieval, the `get` method should be used.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/global_init_scripts.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nall = w.global_init_scripts.list()\n```\n\n----------------------------------------\n\nTITLE: Waiting for Online Table to Become Active\nDESCRIPTION: This method waits for an online table to become active.  It takes the table's name, an optional timeout duration, and an optional callback function that is executed periodically during the waiting process. It returns the OnlineTable object once it becomes active.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/online_tables.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: wait_get_online_table_active(name: str, timeout: datetime.timedelta = 0:20:00, callback: Optional[Callable[[OnlineTable], None]]) -> OnlineTable\n```\n\n----------------------------------------\n\nTITLE: Get Compliance Security Profile Setting (Python)\nDESCRIPTION: Retrieves the current compliance security profile setting for the workspace. It utilizes an optional etag for optimistic concurrency control. The etag should be obtained from a previous GET request and passed to subsequent requests to prevent overwriting changes.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/compliance_security_profile.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get( [, etag: Optional[str]]) -> ComplianceSecurityProfileSetting\n\n    Get the compliance security profile setting.\n\n    Gets the compliance security profile setting.\n\n    :param etag: str (optional)\n      etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n      optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n      each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n      to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n      request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n    :returns: :class:`ComplianceSecurityProfileSetting`\n```\n\n----------------------------------------\n\nTITLE: Get Policy Family Information - Databricks SDK Python\nDESCRIPTION: This code snippet demonstrates how to retrieve information for a specific policy family using its ID with the Databricks SDK for Python. It requires the `databricks-sdk` package to be installed and configured with appropriate credentials to access the Databricks workspace. The code first initializes a `WorkspaceClient`, then retrieves the ID of the first available policy family, and finally fetches the policy family's details using its ID.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/policy_families.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import compute\n\nw = WorkspaceClient()\n\nall = w.policy_families.list(compute.ListPolicyFamiliesRequest())\n\nfirst_family = w.policy_families.get(policy_family_id=all[0].policy_family_id)\n```\n\n----------------------------------------\n\nTITLE: CleanRoomStatusEnum Enum (Python)\nDESCRIPTION: Defines the CleanRoomStatusEnum enumeration. Includes constants like ACTIVE, DELETED, FAILED and PROVISIONING.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: CleanRoomStatusEnum\n\n   .. py:attribute:: ACTIVE\n      :value: \"ACTIVE\"\n\n   .. py:attribute:: DELETED\n      :value: \"DELETED\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: PROVISIONING\n      :value: \"PROVISIONING\"\n```\n\n----------------------------------------\n\nTITLE: Update Serving Endpoint Tags - Python\nDESCRIPTION: Updates tags of a serving endpoint by adding and deleting tags in a single API call. Requires the name of the serving endpoint and lists of tags to add and/or delete.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: patch(name: str [, add_tags: Optional[List[EndpointTag]], delete_tags: Optional[List[str]]]) -> EndpointTags\n\n    Update tags of a serving endpoint.\n\n    Used to batch add and delete tags from a serving endpoint with a single API call.\n\n    :param name: str\n      The name of the serving endpoint who's tags to patch. This field is required.\n    :param add_tags: List[:class:`EndpointTag`] (optional)\n      List of endpoint tags to add\n    :param delete_tags: List[str] (optional)\n      List of tag keys to delete\n\n    :returns: :class:`EndpointTags`\n```\n\n----------------------------------------\n\nTITLE: List Network Configurations with AccountClient in Python\nDESCRIPTION: This code snippet demonstrates how to retrieve a list of all Databricks network configurations using the AccountClient. It initializes the AccountClient and then calls the networks.list method to retrieve an iterator over the network configurations.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/networks.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nconfigs = a.networks.list()\n```\n\n----------------------------------------\n\nTITLE: List Policy Families - Databricks SDK Python\nDESCRIPTION: This code snippet demonstrates how to list all available policy families using the Databricks SDK for Python. It requires the `databricks-sdk` package to be installed and configured with appropriate credentials. The code initializes a `WorkspaceClient` and then retrieves a list of policy families using the `list` method.  The `compute.ListPolicyFamiliesRequest()` is used as an argument to the list method to specify request parameters.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/policy_families.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import compute\n\nw = WorkspaceClient()\n\nall = w.policy_families.list(compute.ListPolicyFamiliesRequest())\n```\n\n----------------------------------------\n\nTITLE: Deleting Clean Room Assets with Python\nDESCRIPTION: Deletes an existing clean room asset, effectively unsharing or removing the asset from the specified clean room based on its type and full name.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/cleanrooms/clean_room_assets.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(clean_room_name: str, asset_type: CleanRoomAssetAssetType, asset_full_name: str)\n\n        Delete an asset.\n\n        Delete a clean room asset - unshare/remove the asset from the clean room\n\n        :param clean_room_name: str\n          Name of the clean room.\n        :param asset_type: :class:`CleanRoomAssetAssetType`\n          The type of the asset.\n        :param asset_full_name: str\n          The fully qualified name of the asset, it is same as the name field in CleanRoomAsset.\n```\n\n----------------------------------------\n\nTITLE: UpdateInfoState Enum Definition in Python\nDESCRIPTION: Defines an enumeration representing the various states an update to a Delta Live Tables pipeline can be in. It includes states like CANCELED, COMPLETED, CREATED, FAILED, INITIALIZING, RUNNING, and WAITING_FOR_RESOURCES, providing a comprehensive view of the update's progress.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: UpdateInfoState\n\n   The update state.\n\n   .. py:attribute:: CANCELED\n      :value: \"CANCELED\"\n\n   .. py:attribute:: COMPLETED\n      :value: \"COMPLETED\"\n\n   .. py:attribute:: CREATED\n      :value: \"CREATED\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: INITIALIZING\n      :value: \"INITIALIZING\"\n\n   .. py:attribute:: QUEUED\n      :value: \"QUEUED\"\n\n   .. py:attribute:: RESETTING\n      :value: \"RESETTING\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n\n   .. py:attribute:: SETTING_UP_TABLES\n      :value: \"SETTING_UP_TABLES\"\n\n   .. py:attribute:: STOPPING\n      :value: \"STOPPING\"\n\n   .. py:attribute:: WAITING_FOR_RESOURCES\n      :value: \"WAITING_FOR_RESOURCES\"\n```\n\n----------------------------------------\n\nTITLE: Deploying and Waiting for an App\nDESCRIPTION: Deploys an app and waits for the deployment to succeed. Includes an optional timeout parameter to specify how long to wait.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nw.apps.deploy_and_wait(app_name: str, source_code_path: str, mode: AppDeploymentMode, timeout: datetime.timedelta = 0:20:00) -> AppDeployment\n```\n\n----------------------------------------\n\nTITLE: Enforcing Cluster Policy Compliance with Python\nDESCRIPTION: This method updates a cluster to be compliant with the current version of its policy. It restarts running clusters and applies changes upon the next start for terminated clusters. Clusters created by Databricks Jobs, DLT, or Models services cannot be enforced using this API.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/policy_compliance_for_clusters.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef enforce_compliance(cluster_id: str [, validate_only: Optional[bool]]) -> EnforceClusterComplianceResponse\n```\n\n----------------------------------------\n\nTITLE: Getting CSP Enablement Account Setting - Python\nDESCRIPTION: This method retrieves the current Compliance Security Profile (CSP) setting for new workspaces. It accepts an optional `etag` parameter for versioning and optimistic concurrency control. It returns a `CspEnablementAccountSetting` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/csp_enablement_account.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get( [, etag: Optional[str]]) -> CspEnablementAccountSetting\n\n    Get the compliance security profile setting for new workspaces.\n\n    Gets the compliance security profile setting for new workspaces.\n\n    :param etag: str (optional)\n      etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n      optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n      each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n      to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n      request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n    :returns: :class:`CspEnablementAccountSetting`\n```\n\n----------------------------------------\n\nTITLE: Make HTTP Request via UC Connection - Python\nDESCRIPTION: Makes an external service call using credentials stored in a UC Connection. Requires the connection name, HTTP method, and path.  Headers, JSON payload, and query parameters can be optionally provided.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: http_request(conn: str, method: ExternalFunctionRequestHttpMethod, path: str [, headers: typing.Dict[str, str], json: typing.Dict[str, str], params: typing.Dict[str, str]]) -> Response\n\n    Make external services call using the credentials stored in UC Connection.\n    **NOTE:** Experimental: This API may change or be removed in a future release without warning.\n    :param conn: str\n      The connection name to use. This is required to identify the external connection.\n    :param method: :class:`ExternalFunctionRequestHttpMethod`\n      The HTTP method to use (e.g., 'GET', 'POST'). This is required.\n    :param path: str\n      The relative path for the API endpoint. This is required.\n    :param headers: Dict[str,str] (optional)\n      Additional headers for the request. If not provided, only auth headers from connections would be\n      passed.\n    :param json: Dict[str,str] (optional)\n      JSON payload for the request.\n    :param params: Dict[str,str] (optional)\n      Query parameters for the request.\n    :returns: :class:`Response`\n```\n\n----------------------------------------\n\nTITLE: Update Service Principal Federation Policy (Python)\nDESCRIPTION: Updates a service principal federation policy.  It requires the service principal ID, policy ID, and optional parameters for the updated policy and an update mask. The update mask specifies which fields of the policy to update, separated by commas. Returns the updated FederationPolicy object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/service_principal_federation_policy.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(service_principal_id: int, policy_id: str [, policy: Optional[FederationPolicy], update_mask: Optional[str]]) -> FederationPolicy\n\n    Update service principal federation policy.\n\n    :param service_principal_id: int\n      The service principal id for the federation policy.\n    :param policy_id: str\n      The identifier for the federation policy.\n    :param policy: :class:`FederationPolicy` (optional)\n    :param update_mask: str (optional)\n      The field mask specifies which fields of the policy to update. To specify multiple fields in the\n      field mask, use comma as the separator (no space). The special value '*' indicates that all fields\n      should be updated (full replacement). If unspecified, all fields that are set in the policy provided\n      in the update request will overwrite the corresponding fields in the existing policy. Example value:\n      'description,oidc_policy.audiences'.\n\n    :returns: :class:`FederationPolicy`\n```\n\n----------------------------------------\n\nTITLE: Defining Provisioning Info States in Python\nDESCRIPTION: This code defines the ProvisioningInfoState class, which represents the possible states of provisioning information. Each state is a class attribute with a string value. The states include ACTIVE, DEGRADED, DELETING, FAILED, PROVISIONING, and UPDATING.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass ProvisioningInfoState:\n   ACTIVE = \"ACTIVE\"\n\n   DEGRADED = \"DEGRADED\"\n\n   DELETING = \"DELETING\"\n\n   FAILED = \"FAILED\"\n\n   PROVISIONING = \"PROVISIONING\"\n\n   UPDATING = \"UPDATING\"\n```\n\n----------------------------------------\n\nTITLE: Listing Clean Rooms in Python\nDESCRIPTION: Lists all clean rooms of the metastore that the caller has access to. Takes optional page_size (int) and page_token (str) parameters for pagination. Returns an iterator over CleanRoom objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/cleanrooms/clean_rooms.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list( [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[CleanRoom]\n\n        List clean rooms.\n\n        Get a list of all clean rooms of the metastore. Only clean rooms the caller has access to are\n        returned.\n\n        :param page_size: int (optional)\n          Maximum number of clean rooms to return (i.e., the page length). Defaults to 100.\n        :param page_token: str (optional)\n          Opaque pagination token to go to next page based on previous query.\n\n        :returns: Iterator over :class:`CleanRoom`\n```\n\n----------------------------------------\n\nTITLE: Defining Views to Export in Python\nDESCRIPTION: Defines the options for exporting views of a Databricks item.  Includes exporting the code view, all dashboard views, or all views. These constants determine what content is included when exporting a Databricks item, such as a notebook.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n.. py:attribute:: ALL\n   :value: \"ALL\"\n\n.. py:attribute:: CODE\n   :value: \"CODE\"\n\n.. py:attribute:: DASHBOARDS\n   :value: \"DASHBOARDS\"\n```\n\n----------------------------------------\n\nTITLE: Update SQL Query with Databricks SDK (Python)\nDESCRIPTION: This snippet demonstrates how to update an existing SQL query using the Databricks SDK for Python. It creates a query, updates its display name, description, and query text, and then deletes it. The `update_mask` parameter specifies which fields to update. It depends on `WorkspaceClient` and `sql` service.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/queries.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\nsrcs = w.data_sources.list()\n\nquery = w.queries.create(\n    query=sql.CreateQueryRequestQuery(\n        display_name=f\"sdk-{time.time_ns()}\",\n        warehouse_id=srcs[0].warehouse_id,\n        description=\"test query from Go SDK\",\n        query_text=\"SHOW TABLES\",\n    )\n)\n\nupdated = w.queries.update(\n    id=query.id,\n    query=sql.UpdateQueryRequestQuery(\n        display_name=f\"sdk-{time.time_ns()}\",\n        description=\"UPDATED: test query from Go SDK\",\n        query_text=\"SELECT 2+2\",\n    ),\n    update_mask=\"display_name,description,query_text\",\n)\n\n# cleanup\nw.queries.delete(id=query.id)\n```\n\n----------------------------------------\n\nTITLE: Setting Object ACL - Python\nDESCRIPTION: Sets the access control list (ACL) for a specified object, overwriting the existing ACL. The object_type parameter defines the type of object to set permissions on, and object_id identifies the specific object. An optional access_control_list parameter specifies the new ACL to apply.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/dbsql_permissions.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: set(object_type: ObjectTypePlural, object_id: str [, access_control_list: Optional[List[AccessControl]]]) -> SetResponse\n\n    Set object ACL.\n\n    Sets the access control list (ACL) for a specified object. This operation will complete rewrite the\n    ACL.\n\n    **Note**: A new version of the Databricks SQL API is now available. Please use\n    :method:workspace/setpermissions instead. [Learn more]\n\n    [Learn more]: https://docs.databricks.com/en/sql/dbsql-api-latest.html\n\n    :param object_type: :class:`ObjectTypePlural`\n      The type of object permission to set.\n    :param object_id: str\n      Object ID. The ACL for the object with this UUID is overwritten by this request's POST content.\n    :param access_control_list: List[:class:`AccessControl`] (optional)\n\n    :returns: :class:`SetResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining Azure Instance Pool Availability in Python\nDESCRIPTION: This code defines the availability types for Azure instances in an instance pool. It includes ON_DEMAND_AZURE and SPOT_AZURE as possible choices for instance provisioning.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: InstancePoolAzureAttributesAvailability\n\n   The set of Azure availability types supported when setting up nodes for a cluster.\n\n   .. py:attribute:: ON_DEMAND_AZURE\n      :value: \"ON_DEMAND_AZURE\"\n\n   .. py:attribute:: SPOT_AZURE\n      :value: \"SPOT_AZURE\"\n```\n\n----------------------------------------\n\nTITLE: Configuring PKCE OAuth Application (Terraform)\nDESCRIPTION: This Terraform configuration creates an Azure AD application configured for the PKCE (Proof Key for Code Exchange) 3-legged OAuth flow. It defines a single-page application with a redirect URI and outputs the application's client ID.\nDependencies: AzureAD provider\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_1\n\nLANGUAGE: hcl\nCODE:\n```\ndata \"azuread_client_config\" \"current\" {}\n\nresource \"azuread_application\" \"pkce\" {\n  display_name     = \"sample-oauth-app-pkce\"\n  owners           = [data.azuread_client_config.current.object_id]\n  sign_in_audience = \"AzureADMyOrg\"\n  single_page_application {\n    redirect_uris = [\"http://localhost:8080/\"]\n  }\n}\n\noutput \"pkce_app_client_id\" {\n  value = azuread_application.pkce.application_id\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Success Message in Python\nDESCRIPTION: This snippet defines a success message with a single attribute, SUCCESS. This likely represents a generic success indicator for various operations within the Databricks platform. The attribute is defined as a string constant.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n   .. py:attribute:: SUCCESS\n      :value: \"SUCCESS\"\n```\n\n----------------------------------------\n\nTITLE: Creating Dashboard Schedule\nDESCRIPTION: Creates a schedule for a given Lakeview dashboard using the LakeviewAPI. The dashboard is identified by its UUID. It returns the created Schedule object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: create_schedule(dashboard_id: str [, schedule: Optional[Schedule]]) -> Schedule\n\n    Create dashboard schedule.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard to which the schedule belongs.\n    :param schedule: :class:`Schedule` (optional)\n\n    :returns: :class:`Schedule`\n```\n\n----------------------------------------\n\nTITLE: Signing Off Commits with DCO\nDESCRIPTION: This code snippet illustrates how to sign off a commit to comply with the Developer Certificate of Origin (DCO) by adding a 'Signed-off-by' line to the commit message.  It is essential to use your real name for the signature. This certification confirms that you have the right to contribute the code under the project's license.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nSigned-off-by: Joe Smith <joe.smith@email.com>\n```\n\n----------------------------------------\n\nTITLE: Getting Object ACL - Python\nDESCRIPTION: Retrieves the access control list (ACL) for a specified object given its type and ID.  This function is part of the DbsqlPermissionsAPI and returns a JSON representation of the ACL. The object_type parameter specifies the type of object, and object_id is the unique identifier of the object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/dbsql_permissions.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(object_type: ObjectTypePlural, object_id: str) -> GetResponse\n\n    Get object ACL.\n\n    Gets a JSON representation of the access control list (ACL) for a specified object.\n\n    **Note**: A new version of the Databricks SQL API is now available. Please use\n    :method:workspace/getpermissions instead. [Learn more]\n\n    [Learn more]: https://docs.databricks.com/en/sql/dbsql-api-latest.html\n\n    :param object_type: :class:`ObjectTypePlural`\n      The type of object permissions to check.\n    :param object_id: str\n      Object ID. An ACL is returned for the object with this UUID.\n\n    :returns: :class:`GetResponse`\n```\n\n----------------------------------------\n\nTITLE: Getting Clean Room Assets with Python\nDESCRIPTION: Retrieves the details of a specific clean room asset based on its type and fully qualified name.  Returns a CleanRoomAsset object representing the asset.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/cleanrooms/clean_room_assets.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(clean_room_name: str, asset_type: CleanRoomAssetAssetType, asset_full_name: str) -> CleanRoomAsset\n\n        Get an asset.\n\n        Get the details of a clean room asset by its type and full name.\n\n        :param clean_room_name: str\n          Name of the clean room.\n        :param asset_type: :class:`CleanRoomAssetAssetType`\n          The type of the asset.\n        :param asset_full_name: str\n          The fully qualified name of the asset, it is same as the name field in CleanRoomAsset.\n\n        :returns: :class:`CleanRoomAsset`\n```\n\n----------------------------------------\n\nTITLE: Stopping an App in Databricks\nDESCRIPTION: Stops the active deployment of the app in the workspace.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nw.apps.stop(name: str)\n```\n\n----------------------------------------\n\nTITLE: Create Databricks Workspace with Account API in Python\nDESCRIPTION: This code snippet demonstrates how to create a Databricks workspace using the Account API. It initializes the AccountClient, creates storage and credentials configurations, and then creates the workspace with specified AWS region, credentials ID, and storage configuration ID. Finally, it includes cleanup steps to delete the created resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/workspaces.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import provisioning\n\na = AccountClient()\n\nstorage = a.storage.create(\n    storage_configuration_name=f\"sdk-{time.time_ns()}\",\n    root_bucket_info=provisioning.RootBucketInfo(bucket_name=os.environ[\"TEST_ROOT_BUCKET\"]),\n)\n\nrole = a.credentials.create(\n    credentials_name=f\"sdk-{time.time_ns()}\",\n    aws_credentials=provisioning.CreateCredentialAwsCredentials(\n        sts_role=provisioning.CreateCredentialStsRole(role_arn=os.environ[\"TEST_CROSSACCOUNT_ARN\"])\n    ),\n)\n\nwaiter = a.workspaces.create(\n    workspace_name=f\"sdk-{time.time_ns()}\",\n    aws_region=os.environ[\"AWS_REGION\"],\n    credentials_id=role.credentials_id,\n    storage_configuration_id=storage.storage_configuration_id,\n)\n\n# cleanup\na.storage.delete(storage_configuration_id=storage.storage_configuration_id)\na.credentials.delete(credentials_id=role.credentials_id)\na.workspaces.delete(workspace_id=waiter.workspace_id)\n```\n\n----------------------------------------\n\nTITLE: Updating a Group in Databricks (Python)\nDESCRIPTION: Updates the details of a group by replacing the entire group entity. The id parameter is the Databricks group ID. Other parameters allow modifying display name, entitlements, and other group attributes.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/groups.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(id: str [, display_name: Optional[str], entitlements: Optional[List[ComplexValue]], external_id: Optional[str], groups: Optional[List[ComplexValue]], members: Optional[List[ComplexValue]], meta: Optional[ResourceMeta], roles: Optional[List[ComplexValue]], schemas: Optional[List[GroupSchema]]])\n\n        Replace a group.\n\n        Updates the details of a group by replacing the entire group entity.\n\n        :param id: str\n          Databricks group ID\n        :param display_name: str (optional)\n          String that represents a human-readable group name\n        :param entitlements: List[:class:`ComplexValue`] (optional)\n          Entitlements assigned to the group. See [assigning entitlements] for a full list of supported\n          values.\n\n          [assigning entitlements]: https://docs.databricks.com/administration-guide/users-groups/index.html#assigning-entitlements\n        :param external_id: str (optional)\n        :param groups: List[:class:`ComplexValue`] (optional)\n        :param members: List[:class:`ComplexValue`] (optional)\n        :param meta: :class:`ResourceMeta` (optional)\n          Container for the group identifier. Workspace local versus account.\n        :param roles: List[:class:`ComplexValue`] (optional)\n          Corresponds to AWS instance profile/arn role.\n        :param schemas: List[:class:`GroupSchema`] (optional)\n          The schema of the group.\n```\n\n----------------------------------------\n\nTITLE: Autogenerating Documentation for SuccessStatus Class in Python\nDESCRIPTION: This code snippet uses the 'autoclass' directive to automatically generate documentation for the SuccessStatus class. The ':members:' option includes all public members in the documentation, while ':undoc-members:' excludes undocumented members. This is part of the Databricks SDK documentation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: SuccessStatus\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Update Rule Set - Python\nDESCRIPTION: Updates a rule set by replacing its rules. It's recommended to first retrieve the current rule set using GET to avoid conflicts. Requires the name of the ruleset and a RuleSetUpdateRequest object.\n\nParameters:\nname (str): Name of the rule set.\nrule_set (RuleSetUpdateRequest): The RuleSetUpdateRequest object.\n\nReturns: RuleSetResponse\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/account_access_control_proxy.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update_rule_set(name: str, rule_set: RuleSetUpdateRequest) -> RuleSetResponse\n\n    Update a rule set.\n\n    Replace the rules of a rule set. First, use a GET rule set request to read the current version of the\n    rule set before modifying it. This pattern helps prevent conflicts between concurrent updates.\n\n    :param name: str\n      Name of the rule set.\n    :param rule_set: :class:`RuleSetUpdateRequest`\n\n    :returns: :class:`RuleSetResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining Privileges for Delta Sharing in Python\nDESCRIPTION: This code defines an enumeration of privileges that can be granted in Delta Sharing. It includes privileges such as ACCESS, CREATE, SELECT, USAGE, and MODIFY.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sharing.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Privilege:\n    ACCESS = \"ACCESS\"\n    ALL_PRIVILEGES = \"ALL_PRIVILEGES\"\n    APPLY_TAG = \"APPLY_TAG\"\n    CREATE = \"CREATE\"\n    CREATE_CATALOG = \"CREATE_CATALOG\"\n    CREATE_CONNECTION = \"CREATE_CONNECTION\"\n    CREATE_EXTERNAL_LOCATION = \"CREATE_EXTERNAL_LOCATION\"\n    CREATE_EXTERNAL_TABLE = \"CREATE_EXTERNAL_TABLE\"\n    CREATE_EXTERNAL_VOLUME = \"CREATE_EXTERNAL_VOLUME\"\n    CREATE_FOREIGN_CATALOG = \"CREATE_FOREIGN_CATALOG\"\n    CREATE_FOREIGN_SECURABLE = \"CREATE_FOREIGN_SECURABLE\"\n    CREATE_FUNCTION = \"CREATE_FUNCTION\"\n    CREATE_MANAGED_STORAGE = \"CREATE_MANAGED_STORAGE\"\n    CREATE_MATERIALIZED_VIEW = \"CREATE_MATERIALIZED_VIEW\"\n    CREATE_MODEL = \"CREATE_MODEL\"\n    CREATE_PROVIDER = \"CREATE_PROVIDER\"\n    CREATE_RECIPIENT = \"CREATE_RECIPIENT\"\n    CREATE_SCHEMA = \"CREATE_SCHEMA\"\n    CREATE_SERVICE_CREDENTIAL = \"CREATE_SERVICE_CREDENTIAL\"\n    CREATE_SHARE = \"CREATE_SHARE\"\n    CREATE_STORAGE_CREDENTIAL = \"CREATE_STORAGE_CREDENTIAL\"\n    CREATE_TABLE = \"CREATE_TABLE\"\n    CREATE_VIEW = \"CREATE_VIEW\"\n    CREATE_VOLUME = \"CREATE_VOLUME\"\n    EXECUTE = \"EXECUTE\"\n    MANAGE = \"MANAGE\"\n    MANAGE_ALLOWLIST = \"MANAGE_ALLOWLIST\"\n    MODIFY = \"MODIFY\"\n    READ_FILES = \"READ_FILES\"\n    READ_PRIVATE_FILES = \"READ_PRIVATE_FILES\"\n    READ_VOLUME = \"READ_VOLUME\"\n    REFRESH = \"REFRESH\"\n    SELECT = \"SELECT\"\n    SET_SHARE_PERMISSION = \"SET_SHARE_PERMISSION\"\n    USAGE = \"USAGE\"\n    USE_CATALOG = \"USE_CATALOG\"\n    USE_CONNECTION = \"USE_CONNECTION\"\n    USE_MARKETPLACE_ASSETS = \"USE_MARKETPLACE_ASSETS\"\n    USE_PROVIDER = \"USE_PROVIDER\"\n    USE_RECIPIENT = \"USE_RECIPIENT\"\n    USE_SCHEMA = \"USE_SCHEMA\"\n    USE_SHARE = \"USE_SHARE\"\n    WRITE_FILES = \"WRITE_FILES\"\n    WRITE_PRIVATE_FILES = \"WRITE_PRIVATE_FILES\"\n    WRITE_VOLUME = \"WRITE_VOLUME\"\n```\n\n----------------------------------------\n\nTITLE: Defining RunLifeCycleState Constants in Python\nDESCRIPTION: This snippet defines constants representing the lifecycle state of a job run, such as QUEUED, PENDING, RUNNING, TERMINATING, TERMINATED, SKIPPED, INTERNAL_ERROR, BLOCKED, and WAITING_FOR_RETRY.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nBLOCKED = \"BLOCKED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nINTERNAL_ERROR = \"INTERNAL_ERROR\"\n```\n\nLANGUAGE: python\nCODE:\n```\nPENDING = \"PENDING\"\n```\n\nLANGUAGE: python\nCODE:\n```\nQUEUED = \"QUEUED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nRUNNING = \"RUNNING\"\n```\n\nLANGUAGE: python\nCODE:\n```\nSKIPPED = \"SKIPPED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nTERMINATED = \"TERMINATED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nTERMINATING = \"TERMINATING\"\n```\n\nLANGUAGE: python\nCODE:\n```\nWAITING_FOR_RETRY = \"WAITING_FOR_RETRY\"\n```\n\n----------------------------------------\n\nTITLE: Signing Commits Automatically with Git\nDESCRIPTION: This command demonstrates how to automatically sign commits using the `git commit -s` option. It requires configuring `user.name` and `user.email` in Git.  Signing confirms compliance with the Developer Certificate of Origin (DCO).\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit commit -s -m \"Your commit message\"\n```\n\n----------------------------------------\n\nTITLE: Listing Storage Configurations with Databricks Account API in Python\nDESCRIPTION: This code snippet shows how to list all storage configurations associated with a Databricks account using the Account API. It initializes an AccountClient and then calls the list method of the storage API to retrieve an iterator over the storage configurations.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/storage.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nconfigs = a.storage.list()\n```\n\n----------------------------------------\n\nTITLE: Updating a Clean Room in Python\nDESCRIPTION: Updates a clean room. The caller must be the owner of the clean room, have **MODIFY_CLEAN_ROOM** privilege, or be metastore admin. When the caller is a metastore admin, only the __owner__ field can be updated. Takes the clean room name as a string and an optional CleanRoom object. Returns a CleanRoom object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/cleanrooms/clean_rooms.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(name: str [, clean_room: Optional[CleanRoom]]) -> CleanRoom\n\n        Update a clean room.\n\n        Update a clean room. The caller must be the owner of the clean room, have **MODIFY_CLEAN_ROOM**\n        privilege, or be metastore admin.\n\n        When the caller is a metastore admin, only the __owner__ field can be updated.\n\n        :param name: str\n          Name of the clean room.\n        :param clean_room: :class:`CleanRoom` (optional)\n\n        :returns: :class:`CleanRoom`\n```\n\n----------------------------------------\n\nTITLE: Updating Clean Room Assets with Python\nDESCRIPTION: Updates the metadata of a clean room asset, allowing changes to the content of a notebook or the shared partitions of a table.  Requires the clean room name, asset type, and name for identification.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/cleanrooms/clean_room_assets.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(clean_room_name: str, asset_type: CleanRoomAssetAssetType, name: str [, asset: Optional[CleanRoomAsset]]) -> CleanRoomAsset\n\n        Update an asset.\n\n        Update a clean room asset. For example, updating the content of a notebook; changing the shared\n        partitions of a table; etc.\n\n        :param clean_room_name: str\n          Name of the clean room.\n        :param asset_type: :class:`CleanRoomAssetAssetType`\n          The type of the asset.\n        :param name: str\n          A fully qualified name that uniquely identifies the asset within the clean room. This is also the\n          name displayed in the clean room UI.\n\n          For UC securable assets (tables, volumes, etc.), the format is\n          *shared_catalog*.*shared_schema*.*asset_name*\n\n          For notebooks, the name is the notebook file name.\n        :param asset: :class:`CleanRoomAsset` (optional)\n          Metadata of the clean room asset\n\n        :returns: :class:`CleanRoomAsset`\n```\n\n----------------------------------------\n\nTITLE: Defining WorkspaceBindingBindingType enum in Python\nDESCRIPTION: Defines a Python enum `WorkspaceBindingBindingType` with values BINDING_TYPE_READ_ONLY and BINDING_TYPE_READ_WRITE. This represents the type of binding a workspace has, either read-only or read-write.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: WorkspaceBindingBindingType\n\n   .. py:attribute:: BINDING_TYPE_READ_ONLY\n      :value: \"BINDING_TYPE_READ_ONLY\"\n\n   .. py:attribute:: BINDING_TYPE_READ_WRITE\n      :value: \"BINDING_TYPE_READ_WRITE\"\n```\n\n----------------------------------------\n\nTITLE: Unpublishing Lakeview Dashboard\nDESCRIPTION: Unpublishes a published Lakeview dashboard using the LakeviewAPI. Requires the dashboard ID.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: unpublish(dashboard_id: str)\n\n    Unpublish dashboard.\n\n    Unpublish the dashboard.\n\n    :param dashboard_id: str\n      UUID identifying the published dashboard.\n```\n\n----------------------------------------\n\nTITLE: Deleting a Clean Room in Python\nDESCRIPTION: Deletes a clean room. After deletion, the clean room will be removed from the metastore. If the other collaborators have not deleted the clean room, they will still have the clean room in their metastore, but it will be in a DELETED state. Takes the clean room name as a string.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/cleanrooms/clean_rooms.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(name: str)\n\n        Delete a clean room.\n\n        Delete a clean room. After deletion, the clean room will be removed from the metastore. If the other\n        collaborators have not deleted the clean room, they will still have the clean room in their metastore,\n        but it will be in a DELETED state and no operations other than deletion can be performed on it.\n\n        :param name: str\n          Name of the clean room.\n```\n\n----------------------------------------\n\nTITLE: Defining GetWarehouseResponseWarehouseType Enum (Python)\nDESCRIPTION: Defines an enumeration representing warehouse types for a Databricks SQL warehouse. The types are CLASSIC, PRO, and TYPE_UNSPECIFIED. The documentation emphasizes setting the type to `PRO` and `enable_serverless_compute` to `true` when using serverless compute.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: GetWarehouseResponseWarehouseType\n\n   Warehouse type: `PRO` or `CLASSIC`. If you want to use serverless compute, you must set to `PRO` and also set the field `enable_serverless_compute` to `true`.\n\n   .. py:attribute:: CLASSIC\n      :value: \"CLASSIC\"\n\n   .. py:attribute:: PRO\n      :value: \"PRO\"\n\n   .. py:attribute:: TYPE_UNSPECIFIED\n```\n\n----------------------------------------\n\nTITLE: StartUpdateCause Enum Definition in Python\nDESCRIPTION: Defines an enumeration for the possible causes that initiated a pipeline update. It includes causes such as API_CALL, JOB_TASK, RETRY_ON_FAILURE, SCHEMA_CHANGE, SERVICE_UPGRADE, and USER_ACTION, offering context on why an update was triggered.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: StartUpdateCause\n\n   .. py:attribute:: API_CALL\n      :value: \"API_CALL\"\n\n   .. py:attribute:: JOB_TASK\n      :value: \"JOB_TASK\"\n\n   .. py:attribute:: RETRY_ON_FAILURE\n      :value: \"RETRY_ON_FAILURE\"\n\n   .. py:attribute:: SCHEMA_CHANGE\n      :value: \"SCHEMA_CHANGE\"\n\n   .. py:attribute:: SERVICE_UPGRADE\n      :value: \"SERVICE_UPGRADE\"\n\n   .. py:attribute:: USER_ACTION\n      :value: \"USER_ACTION\"\n```\n\n----------------------------------------\n\nTITLE: Getting a Clean Room in Python\nDESCRIPTION: Retrieves the details of a clean room given its name. Takes the clean room name as a string and returns a CleanRoom object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/cleanrooms/clean_rooms.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(name: str) -> CleanRoom\n\n        Get a clean room.\n\n        Get the details of a clean room given its name.\n\n        :param name: str\n\n        :returns: :class:`CleanRoom`\n```\n\n----------------------------------------\n\nTITLE: Defining JobEditMode Enum Python\nDESCRIPTION: This code snippet defines an enumeration called `JobEditMode` representing the edit mode of a job. Possible values include `EDITABLE` and `UI_LOCKED`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: JobEditMode\n\n   Edit mode of the job.\n   * `UI_LOCKED`: The job is in a locked UI state and cannot be modified. * `EDITABLE`: The job is in an editable state and can be modified.\n\n   .. py:attribute:: EDITABLE\n      :value: \"EDITABLE\"\n\n   .. py:attribute:: UI_LOCKED\n      :value: \"UI_LOCKED\"\n```\n\n----------------------------------------\n\nTITLE: Get Provider - Python\nDESCRIPTION: Retrieves a provider's profile from the Databricks Marketplace. It requires the provider's ID as a string and returns a GetProviderResponse object containing the provider's details.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_providers.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(id: str) -> GetProviderResponse\n\n    Get provider.\n\n    Get provider profile\n\n    :param id: str\n\n    :returns: :class:`GetProviderResponse`\n```\n\n----------------------------------------\n\nTITLE: Disabling Asynchronous Token Refresh - Environment Variable\nDESCRIPTION: This snippet outlines the usage of the `DATABRICKS_DISABLE_ASYNC_TOKEN_REFRESH` environment variable to disable asynchronous token refresh in the Databricks SDK for Python.  Setting this environment variable to `true` before initializing the Databricks client will disable the feature.  This provides a global way to control the refresh behavior without modifying code.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/NEXT_CHANGELOG.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Creating an Account IP Access List in Python\nDESCRIPTION: Creates an IP access list for the account. A list can be an allow list or a block list. The API supports a maximum of 1000 IP/CIDR values combined for all allow and block lists. If the new list would block the calling user's current IP, an error is returned. Changes take a few minutes to apply.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/ip_access_lists.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(label: str, list_type: ListType [, ip_addresses: Optional[List[str]]]) -> CreateIpAccessListResponse\n\n    Create access list.\n\n    Creates an IP access list for the account.\n\n    A list can be an allow list or a block list. See the top of this file for a description of how the\n    server treats allow lists and block lists at runtime.\n\n    When creating or updating an IP access list:\n\n    * For all allow lists and block lists combined, the API supports a maximum of 1000 IP/CIDR values,\n    where one CIDR counts as a single value. Attempts to exceed that number return error 400 with\n    `error_code` value `QUOTA_EXCEEDED`. * If the new list would block the calling user's current IP,\n    error 400 is returned with `error_code` value `INVALID_STATE`.\n\n    It can take a few minutes for the changes to take effect.\n\n    :param label: str\n      Label for the IP access list. This **cannot** be empty.\n    :param list_type: :class:`ListType`\n      Type of IP access list. Valid values are as follows and are case-sensitive:\n\n      * `ALLOW`: An allow list. Include this IP or range. * `BLOCK`: A block list. Exclude this IP or\n      range. IP addresses in the block list are excluded even if they are included in an allow list.\n    :param ip_addresses: List[str] (optional)\n\n    :returns: :class:`CreateIpAccessListResponse`\n```\n\n----------------------------------------\n\nTITLE: Updating Exchange Filter using ProviderExchangeFiltersAPI in Python\nDESCRIPTION: Updates an existing exchange filter using the ``update`` method. Requires the ``id`` of the filter to be updated (string) and an ``ExchangeFilter`` object containing the updated properties. Returns an ``UpdateExchangeFilterResponse`` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_exchange_filters.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nw.provider_exchange_filters.update(id: str, filter: ExchangeFilter) -> UpdateExchangeFilterResponse\n```\n\n----------------------------------------\n\nTITLE: Defining CreateWarehouseRequestWarehouseType Enum (Python)\nDESCRIPTION: Defines an enumeration representing warehouse types for creating a Databricks SQL warehouse. The types are CLASSIC, PRO, and TYPE_UNSPECIFIED. The documentation emphasizes setting the type to `PRO` and `enable_serverless_compute` to `true` when using serverless compute.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CreateWarehouseRequestWarehouseType\n\n   Warehouse type: `PRO` or `CLASSIC`. If you want to use serverless compute, you must set to `PRO` and also set the field `enable_serverless_compute` to `true`.\n\n   .. py:attribute:: CLASSIC\n      :value: \"CLASSIC\"\n\n   .. py:attribute:: PRO\n      :value: \"PRO\"\n\n   .. py:attribute:: TYPE_UNSPECIFIED\n      :value: \"TYPE_UNSPECIFIED\"\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_ATTACH_TO attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_ATTACH_TO attribute for the PermissionLevel class. It represents a permission level that allows attaching to resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_ATTACH_TO\n      :value: \"CAN_ATTACH_TO\"\n```\n\n----------------------------------------\n\nTITLE: Updating a Visualization in Databricks (Python)\nDESCRIPTION: This method updates an existing visualization in Databricks. It requires the `id` of the visualization to update, an `update_mask` specifying the fields to update, and an optional `UpdateVisualizationRequestVisualization` object with the new values. The method returns a `Visualization` object representing the updated visualization.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/query_visualizations.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(id: str, update_mask: str [, visualization: Optional[UpdateVisualizationRequestVisualization]]) -> Visualization\n\n    Update a visualization.\n\n    Updates a visualization.\n\n    :param id: str\n    :param update_mask: str\n      The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n      field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n      `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n      the entire collection field can be specified. Field names must exactly match the resource field\n      names.\n\n      A field mask of `*` indicates full replacement. It’s recommended to always explicitly list the\n      fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n      changes in the future.\n    :param visualization: :class:`UpdateVisualizationRequestVisualization` (optional)\n\n    :returns: :class:`Visualization`\n```\n\n----------------------------------------\n\nTITLE: Marketplace Category Enum Definition in Python\nDESCRIPTION: Defines the different categories that a listing can belong to in the Databricks Marketplace. These categories represent various industries and domains such as advertising, climate, commerce, and education. Each attribute represents a specific category.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: Category\n\n   .. py:attribute:: ADVERTISING_AND_MARKETING\n      :value: \"ADVERTISING_AND_MARKETING\"\n\n   .. py:attribute:: CLIMATE_AND_ENVIRONMENT\n      :value: \"CLIMATE_AND_ENVIRONMENT\"\n\n   .. py:attribute:: COMMERCE\n      :value: \"COMMERCE\"\n\n   .. py:attribute:: DEMOGRAPHICS\n      :value: \"DEMOGRAPHICS\"\n\n   .. py:attribute:: ECONOMICS\n      :value: \"ECONOMICS\"\n\n   .. py:attribute:: EDUCATION\n      :value: \"EDUCATION\"\n\n   .. py:attribute:: ENERGY\n      :value: \"ENERGY\"\n\n   .. py:attribute:: FINANCIAL\n      :value: \"FINANCIAL\"\n\n   .. py:attribute:: GAMING\n      :value: \"GAMING\"\n\n   .. py:attribute:: GEOSPATIAL\n      :value: \"GEOSPATIAL\"\n\n   .. py:attribute:: HEALTH\n      :value: \"HEALTH\"\n\n   .. py:attribute:: LOOKUP_TABLES\n      :value: \"LOOKUP_TABLES\"\n\n   .. py:attribute:: MANUFACTURING\n      :value: \"MANUFACTURING\"\n\n   .. py:attribute:: MEDIA\n      :value: \"MEDIA\"\n\n   .. py:attribute:: OTHER\n      :value: \"OTHER\"\n\n   .. py:attribute:: PUBLIC_SECTOR\n      :value: \"PUBLIC_SECTOR\"\n\n   .. py:attribute:: RETAIL\n      :value: \"RETAIL\"\n\n   .. py:attribute:: SCIENCE_AND_RESEARCH\n      :value: \"SCIENCE_AND_RESEARCH\"\n\n   .. py:attribute:: SECURITY\n      :value: \"SECURITY\"\n\n   .. py:attribute:: SPORTS\n      :value: \"SPORTS\"\n\n   .. py:attribute:: TRANSPORTATION_AND_LOGISTICS\n      :value: \"TRANSPORTATION_AND_LOGISTICS\"\n\n   .. py:attribute:: TRAVEL_AND_TOURISM\n      :value: \"TRAVEL_AND_TOURISM\"\n```\n\n----------------------------------------\n\nTITLE: Get Metastore - Python\nDESCRIPTION: Retrieves a Unity Catalog metastore from an account, specified by ID. Requires a metastore ID. Returns an `AccountsMetastoreInfo` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/catalog/metastores.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(metastore_id: str) -> AccountsMetastoreInfo\n\n    Get a metastore.\n\n    Gets a Unity Catalog metastore from an account, both specified by ID.\n\n    :param metastore_id: str\n      Unity Catalog metastore ID\n\n    :returns: :class:`AccountsMetastoreInfo`\n```\n\n----------------------------------------\n\nTITLE: Defining Permission Levels for Registered Models in Python\nDESCRIPTION: These attributes define the different permission levels that a user can have on a registered model, ranging from read-only access to full management capabilities. Each attribute represents a specific level of access control.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n.. py:attribute:: CAN_EDIT\n   :value: \"CAN_EDIT\"\n\n.. py:attribute:: CAN_MANAGE\n   :value: \"CAN_MANAGE\"\n\n.. py:attribute:: CAN_MANAGE_PRODUCTION_VERSIONS\n   :value: \"CAN_MANAGE_PRODUCTION_VERSIONS\"\n\n.. py:attribute:: CAN_MANAGE_STAGING_VERSIONS\n   :value: \"CAN_MANAGE_STAGING_VERSIONS\"\n\n.. py:attribute:: CAN_READ\n   :value: \"CAN_READ\"\n```\n\n----------------------------------------\n\nTITLE: FileStatus Enum Definition in Python\nDESCRIPTION: Defines the possible statuses for a file on the Databricks Marketplace. These statuses include 'FILE_STATUS_PUBLISHED', 'FILE_STATUS_SANITIZATION_FAILED', 'FILE_STATUS_SANITIZING', and 'FILE_STATUS_STAGING', indicating the file's state in the publication process.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: FileStatus\n\n   .. py:attribute:: FILE_STATUS_PUBLISHED\n      :value: \"FILE_STATUS_PUBLISHED\"\n\n   .. py:attribute:: FILE_STATUS_SANITIZATION_FAILED\n      :value: \"FILE_STATUS_SANITIZATION_FAILED\"\n\n   .. py:attribute:: FILE_STATUS_SANITIZING\n      :value: \"FILE_STATUS_SANITIZING\"\n\n   .. py:attribute:: FILE_STATUS_STAGING\n      :value: \"FILE_STATUS_STAGING\"\n```\n\n----------------------------------------\n\nTITLE: Defining GitProvider Enum Python\nDESCRIPTION: This code snippet defines an enumeration called `GitProvider` representing different Git providers supported by Databricks jobs. It includes `AWS_CODE_COMMIT`, `AZURE_DEV_OPS_SERVICES`, `BITBUCKET_CLOUD`, `BITBUCKET_SERVER`, `GIT_HUB`, `GIT_HUB_ENTERPRISE`, `GIT_LAB`, and `GIT_LAB_ENTERPRISE_EDITION`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: GitProvider\n\n   .. py:attribute:: AWS_CODE_COMMIT\n      :value: \"AWS_CODE_COMMIT\"\n\n   .. py:attribute:: AZURE_DEV_OPS_SERVICES\n      :value: \"AZURE_DEV_OPS_SERVICES\"\n\n   .. py:attribute:: BITBUCKET_CLOUD\n      :value: \"BITBUCKET_CLOUD\"\n\n   .. py:attribute:: BITBUCKET_SERVER\n      :value: \"BITBUCKET_SERVER\"\n\n   .. py:attribute:: GIT_HUB\n      :value: \"GIT_HUB\"\n\n   .. py:attribute:: GIT_HUB_ENTERPRISE\n      :value: \"GIT_HUB_ENTERPRISE\"\n\n   .. py:attribute:: GIT_LAB\n      :value: \"GIT_LAB\"\n\n   .. py:attribute:: GIT_LAB_ENTERPRISE_EDITION\n      :value: \"GIT_LAB_ENTERPRISE_EDITION\"\n```\n\n----------------------------------------\n\nTITLE: Setting User Agent with Databricks SDK\nDESCRIPTION: This snippet shows how to use the `with_partner()` and `with_product()` functions from the `databricks.sdk.useragent` module to add metadata to the `User-Agent` header. The `with_partner()` function is used to indicate that the code should be attributed to a specific partner, while the `with_product()` function defines the name and version of the product built with the Databricks SDK for Python.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import useragent\nuseragent.with_product(\"partner-abc\")\nuseragent.with_partner(\"partner-xyz\")\n```\n\n----------------------------------------\n\nTITLE: Listing Legacy Queries in Databricks SQL\nDESCRIPTION: This method retrieves a list of queries from Databricks SQL. It supports optional filtering by search term and pagination.  The method includes parameters for ordering, page number, page size, and a full-text search term.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/queries_legacy.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list( [, order: Optional[str], page: Optional[int], page_size: Optional[int], q: Optional[str]]) -> Iterator[LegacyQuery]\n```\n\n----------------------------------------\n\nTITLE: Get Current Metastore Assignment in Python\nDESCRIPTION: This snippet demonstrates how to retrieve the metastore assignment for the current workspace using the Databricks SDK. It initializes a `WorkspaceClient` and then calls the `current()` method on the `metastores` object to retrieve the metastore assignment.  It leverages the WorkspaceClient from the databricks.sdk.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/metastores.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncurrent_metastore = w.metastores.current()\n```\n\n----------------------------------------\n\nTITLE: Listing Workspace Assignments with AccountClient in Python\nDESCRIPTION: This code snippet demonstrates how to list permission assignments for a specified Databricks workspace using the AccountClient from the Databricks SDK. It retrieves the workspace ID from an environment variable and iterates through the assignments.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/workspace_assignment.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nworkspace_id = os.environ[\"TEST_WORKSPACE_ID\"]\n\nall = a.workspace_assignment.list(list=workspace_id)\n```\n\n----------------------------------------\n\nTITLE: Defining PersonalComputeMessageEnum Enum in Python\nDESCRIPTION: This code defines an enumeration `PersonalComputeMessageEnum` representing the personal compute setting. The possible values are `ON` and `DELEGATE`. `ON` grants access to all users, while `DELEGATE` moves access control to individual workspaces.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PersonalComputeMessageEnum\n\n   ON: Grants all users in all workspaces access to the Personal Compute default policy, allowing all users to create single-machine compute resources. DELEGATE: Moves access control for the Personal Compute default policy to individual workspaces and requires a workspace’s users or groups to be added to the ACLs of that workspace’s Personal Compute default policy before they will be able to create compute resources through that policy.\n\n   .. py:attribute:: DELEGATE\n      :value: \"DELEGATE\"\n\n   .. py:attribute:: ON\n      :value: \"ON\"\n```\n\n----------------------------------------\n\nTITLE: List Listings in Databricks Marketplace (Python)\nDESCRIPTION: Lists all listings owned by the provider in the Databricks Marketplace.  It supports optional parameters for pagination: page_size (int) and page_token (str). It returns an iterator over Listing objects. This allows providers to retrieve all their current listings.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_listings.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nw.provider_listings.list( [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[Listing]\n```\n\n----------------------------------------\n\nTITLE: DeltaSharingRecipientType Enum Definition in Python\nDESCRIPTION: Defines the types of recipients for Delta Sharing on the Databricks Marketplace. The possible values are 'DELTA_SHARING_RECIPIENT_TYPE_DATABRICKS' and 'DELTA_SHARING_RECIPIENT_TYPE_OPEN', indicating whether the recipient is a Databricks user or an external user.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: DeltaSharingRecipientType\n\n   .. py:attribute:: DELTA_SHARING_RECIPIENT_TYPE_DATABRICKS\n      :value: \"DELTA_SHARING_RECIPIENT_TYPE_DATABRICKS\"\n\n   .. py:attribute:: DELTA_SHARING_RECIPIENT_TYPE_OPEN\n      :value: \"DELTA_SHARING_RECIPIENT_TYPE_OPEN\"\n```\n\n----------------------------------------\n\nTITLE: Ensure Cluster is Running\nDESCRIPTION: This snippet demonstrates how to ensure that a given Databricks cluster is running using the `ensure_cluster_is_running` method from the Databricks SDK. It first sets up a command execution context and then ensures the cluster is running, finally cleaning up the command execution context. It depends on the `WorkspaceClient` and `compute` modules from the `databricks.sdk`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import compute\n\nw = WorkspaceClient()\n\ncluster_id = os.environ[\"TEST_DEFAULT_CLUSTER_ID\"]\n\ncontext = w.command_execution.create(cluster_id=cluster_id, language=compute.Language.PYTHON).result()\n\nw.clusters.ensure_cluster_is_running(cluster_id)\n\n# cleanup\nw.command_execution.destroy(cluster_id=cluster_id, context_id=context.id)\n```\n\n----------------------------------------\n\nTITLE: Defining Message Error Type Constants in Python\nDESCRIPTION: This code defines a class `MessageErrorType` with several constants representing different types of errors that can occur during message processing. These constants provide specific error codes for various issues such as chat completion failures, SQL execution errors, and rate limit exceptions. These are used to provide more detailed error reporting when interacting with the Dashboards API.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: MessageErrorType\n\n   .. py:attribute:: BLOCK_MULTIPLE_EXECUTIONS_EXCEPTION\n      :value: \"BLOCK_MULTIPLE_EXECUTIONS_EXCEPTION\"\n\n   .. py:attribute:: CHAT_COMPLETION_CLIENT_EXCEPTION\n      :value: \"CHAT_COMPLETION_CLIENT_EXCEPTION\"\n\n   .. py:attribute:: CHAT_COMPLETION_CLIENT_TIMEOUT_EXCEPTION\n      :value: \"CHAT_COMPLETION_CLIENT_TIMEOUT_EXCEPTION\"\n\n   .. py:attribute:: CHAT_COMPLETION_NETWORK_EXCEPTION\n      :value: \"CHAT_COMPLETION_NETWORK_EXCEPTION\"\n\n   .. py:attribute:: CONTENT_FILTER_EXCEPTION\n      :value: \"CONTENT_FILTER_EXCEPTION\"\n\n   .. py:attribute:: CONTEXT_EXCEEDED_EXCEPTION\n      :value: \"CONTEXT_EXCEEDED_EXCEPTION\"\n\n   .. py:attribute:: COULD_NOT_GET_MODEL_DEPLOYMENTS_EXCEPTION\n      :value: \"COULD_NOT_GET_MODEL_DEPLOYMENTS_EXCEPTION\"\n\n   .. py:attribute:: COULD_NOT_GET_UC_SCHEMA_EXCEPTION\n      :value: \"COULD_NOT_GET_UC_SCHEMA_EXCEPTION\"\n\n   .. py:attribute:: DEPLOYMENT_NOT_FOUND_EXCEPTION\n      :value: \"DEPLOYMENT_NOT_FOUND_EXCEPTION\"\n\n   .. py:attribute:: FUNCTIONS_NOT_AVAILABLE_EXCEPTION\n      :value: \"FUNCTIONS_NOT_AVAILABLE_EXCEPTION\"\n\n   .. py:attribute:: FUNCTION_ARGUMENTS_INVALID_EXCEPTION\n      :value: \"FUNCTION_ARGUMENTS_INVALID_EXCEPTION\"\n\n   .. py:attribute:: FUNCTION_ARGUMENTS_INVALID_JSON_EXCEPTION\n      :value: \"FUNCTION_ARGUMENTS_INVALID_JSON_EXCEPTION\"\n\n   .. py:attribute:: FUNCTION_ARGUMENTS_INVALID_TYPE_EXCEPTION\n      :value: \"FUNCTION_ARGUMENTS_INVALID_TYPE_EXCEPTION\"\n\n   .. py:attribute:: FUNCTION_CALL_MISSING_PARAMETER_EXCEPTION\n      :value: \"FUNCTION_CALL_MISSING_PARAMETER_EXCEPTION\"\n\n   .. py:attribute:: GENERATED_SQL_QUERY_TOO_LONG_EXCEPTION\n      :value: \"GENERATED_SQL_QUERY_TOO_LONG_EXCEPTION\"\n\n   .. py:attribute:: GENERIC_CHAT_COMPLETION_EXCEPTION\n      :value: \"GENERIC_CHAT_COMPLETION_EXCEPTION\"\n\n   .. py:attribute:: GENERIC_CHAT_COMPLETION_SERVICE_EXCEPTION\n      :value: \"GENERIC_CHAT_COMPLETION_SERVICE_EXCEPTION\"\n\n   .. py:attribute:: GENERIC_SQL_EXEC_API_CALL_EXCEPTION\n      :value: \"GENERIC_SQL_EXEC_API_CALL_EXCEPTION\"\n\n   .. py:attribute:: ILLEGAL_PARAMETER_DEFINITION_EXCEPTION\n      :value: \"ILLEGAL_PARAMETER_DEFINITION_EXCEPTION\"\n\n   .. py:attribute:: INVALID_CERTIFIED_ANSWER_FUNCTION_EXCEPTION\n      :value: \"INVALID_CERTIFIED_ANSWER_FUNCTION_EXCEPTION\"\n\n   .. py:attribute:: INVALID_CERTIFIED_ANSWER_IDENTIFIER_EXCEPTION\n      :value: \"INVALID_CERTIFIED_ANSWER_IDENTIFIER_EXCEPTION\"\n\n   .. py:attribute:: INVALID_CHAT_COMPLETION_JSON_EXCEPTION\n      :value: \"INVALID_CHAT_COMPLETION_JSON_EXCEPTION\"\n\n   .. py:attribute:: INVALID_COMPLETION_REQUEST_EXCEPTION\n      :value: \"INVALID_COMPLETION_REQUEST_EXCEPTION\"\n\n   .. py:attribute:: INVALID_FUNCTION_CALL_EXCEPTION\n      :value: \"INVALID_FUNCTION_CALL_EXCEPTION\"\n\n   .. py:attribute:: INVALID_TABLE_IDENTIFIER_EXCEPTION\n      :value: \"INVALID_TABLE_IDENTIFIER_EXCEPTION\"\n\n   .. py:attribute:: LOCAL_CONTEXT_EXCEEDED_EXCEPTION\n      :value: \"LOCAL_CONTEXT_EXCEEDED_EXCEPTION\"\n\n   .. py:attribute:: MESSAGE_CANCELLED_WHILE_EXECUTING_EXCEPTION\n      :value: \"MESSAGE_CANCELLED_WHILE_EXECUTING_EXCEPTION\"\n\n   .. py:attribute:: MESSAGE_DELETED_WHILE_EXECUTING_EXCEPTION\n      :value: \"MESSAGE_DELETED_WHILE_EXECUTING_EXCEPTION\"\n\n   .. py:attribute:: MESSAGE_UPDATED_WHILE_EXECUTING_EXCEPTION\n      :value: \"MESSAGE_UPDATED_WHILE_EXECUTING_EXCEPTION\"\n\n   .. py:attribute:: MISSING_SQL_QUERY_EXCEPTION\n      :value: \"MISSING_SQL_QUERY_EXCEPTION\"\n\n   .. py:attribute:: NO_DEPLOYMENTS_AVAILABLE_TO_WORKSPACE\n      :value: \"NO_DEPLOYMENTS_AVAILABLE_TO_WORKSPACE\"\n\n   .. py:attribute:: NO_QUERY_TO_VISUALIZE_EXCEPTION\n      :value: \"NO_QUERY_TO_VISUALIZE_EXCEPTION\"\n\n   .. py:attribute:: NO_TABLES_TO_QUERY_EXCEPTION\n      :value: \"NO_TABLES_TO_QUERY_EXCEPTION\"\n\n   .. py:attribute:: RATE_LIMIT_EXCEEDED_GENERIC_EXCEPTION\n      :value: \"RATE_LIMIT_EXCEEDED_GENERIC_EXCEPTION\"\n\n   .. py:attribute:: RATE_LIMIT_EXCEEDED_SPECIFIED_WAIT_EXCEPTION\n      :value: \"RATE_LIMIT_EXCEEDED_SPECIFIED_WAIT_EXCEPTION\"\n\n   .. py:attribute:: REPLY_PROCESS_TIMEOUT_EXCEPTION\n      :value: \"REPLY_PROCESS_TIMEOUT_EXCEPTION\"\n\n   .. py:attribute:: RETRYABLE_PROCESSING_EXCEPTION\n      :value: \"RETRYABLE_PROCESSING_EXCEPTION\"\n\n   .. py:attribute:: SQL_EXECUTION_EXCEPTION\n      :value: \"SQL_EXECUTION_EXCEPTION\"\n\n   .. py:attribute:: STOP_PROCESS_DUE_TO_AUTO_REGENERATE\n      :value: \"STOP_PROCESS_DUE_TO_AUTO_REGENERATE\"\n\n   .. py:attribute:: TABLES_MISSING_EXCEPTION\n      :value: \"TABLES_MISSING_EXCEPTION\"\n\n   .. py:attribute:: TOO_MANY_CERTIFIED_ANSWERS_EXCEPTION\n      :value: \"TOO_MANY_CERTIFIED_ANSWERS_EXCEPTION\"\n\n   .. py:attribute:: TOO_MANY_TABLES_EXCEPTION\n      :value: \"TOO_MANY_TABLES_EXCEPTION\"\n\n   .. py:attribute:: UNEXPECTED_REPLY_PROCESS_EXCEPTION\n      :value: \"UNEXPECTED_REPLY_PROCESS_EXCEPTION\"\n\n   .. py:attribute:: UNKNOWN_AI_MODEL\n      :value: \"UNKNOWN_AI_MODEL\"\n\n   .. py:attribute:: WAREHOUSE_ACCESS_MISSING_EXCEPTION\n      :value: \"WAREHOUSE_ACCESS_MISSING_EXCEPTION\"\n\n   .. py:attribute:: WAREHOUSE_NOT_FOUND_EXCEPTION\n      :value: \"WAREHOUSE_NOT_FOUND_EXCEPTION\"\n```\n\n----------------------------------------\n\nTITLE: Marketplace AssetType Enum Definition in Python\nDESCRIPTION: Defines the possible types of assets that can be listed on the Databricks Marketplace. These include application, data table, git repository, media, model, notebook and integration assets. Each attribute represents a specific asset type.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: AssetType\n\n   .. py:attribute:: ASSET_TYPE_APP\n      :value: \"ASSET_TYPE_APP\"\n\n   .. py:attribute:: ASSET_TYPE_DATA_TABLE\n      :value: \"ASSET_TYPE_DATA_TABLE\"\n\n   .. py:attribute:: ASSET_TYPE_GIT_REPO\n      :value: \"ASSET_TYPE_GIT_REPO\"\n\n   .. py:attribute:: ASSET_TYPE_MEDIA\n      :value: \"ASSET_TYPE_MEDIA\"\n\n   .. py:attribute:: ASSET_TYPE_MODEL\n      :value: \"ASSET_TYPE_MODEL\"\n\n   .. py:attribute:: ASSET_TYPE_NOTEBOOK\n      :value: \"ASSET_TYPE_NOTEBOOK\"\n\n   .. py:attribute:: ASSET_TYPE_PARTNER_INTEGRATION\n      :value: \"ASSET_TYPE_PARTNER_INTEGRATION\"\n```\n\n----------------------------------------\n\nTITLE: Getting a Budget Policy (Python)\nDESCRIPTION: Retrieves a budget policy by its ID using the `get` method of the `BudgetPolicyAPI`. Requires the `policy_id` of the policy to be retrieved as input. Returns a `BudgetPolicy` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/budget_policy.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nBudgetPolicyAPI.get(policy_id: str) -> BudgetPolicy\n```\n\n----------------------------------------\n\nTITLE: Defining Format Enum Python\nDESCRIPTION: This code snippet defines an enumeration called `Format` with possible values `MULTI_TASK` and `SINGLE_TASK`, representing different formats for Databricks jobs.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: Format\n\n   .. py:attribute:: MULTI_TASK\n      :value: \"MULTI_TASK\"\n\n   .. py:attribute:: SINGLE_TASK\n      :value: \"SINGLE_TASK\"\n```\n\n----------------------------------------\n\nTITLE: Synchronizing a Delta Sync Index\nDESCRIPTION: This method triggers a synchronization process for a specified Delta Sync Index. It requires the index name as input.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_indexes.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: sync_index(index_name: str)\n\n        Synchronize an index.\n\n        Triggers a synchronization process for a specified vector index.\n\n        :param index_name: str\n          Name of the vector index to synchronize. Must be a Delta Sync Index.\n```\n\n----------------------------------------\n\nTITLE: VpcStatus Enum Definition (Python)\nDESCRIPTION: Defines an enumeration representing the status of a network configuration object in relation to its usage in a workspace, part of the `databricks.sdk.service.provisioning` module. Possible states include `UNATTACHED`, `VALID`, `BROKEN`, and `WARNED`. This indicates if a network configuration is ready for use, has issues, or is simply not associated with any workspace.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/provisioning.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: VpcStatus\n\n   The status of this network configuration object in terms of its use in a workspace: * `UNATTACHED`: Unattached. * `VALID`: Valid. * `BROKEN`: Broken. * `WARNED`: Warned.\n\n   .. py:attribute:: BROKEN\n      :value: \"BROKEN\"\n\n   .. py:attribute:: UNATTACHED\n      :value: \"UNATTACHED\"\n\n   .. py:attribute:: VALID\n      :value: \"VALID\"\n\n   .. py:attribute:: WARNED\n      :value: \"WARNED\"\n```\n\n----------------------------------------\n\nTITLE: Getting the status of a workspace object in Python\nDESCRIPTION: This code snippet demonstrates how to get the status of a workspace object (notebook) from a Databricks workspace using the WorkspaceClient. It creates a temporary notebook path and then gets the status using the `get_status` method. The databricks SDK is required.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/workspace.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nnotebook = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\nget_status_response = w.workspace.get_status(path=notebook)\n```\n\n----------------------------------------\n\nTITLE: Defining VectorIndexType Enum in Python\nDESCRIPTION: Defines an enumeration `VectorIndexType` representing the type of Vector Search index. It can be either DELTA_SYNC (automatically syncs with a Delta Table) or DIRECT_ACCESS (supports direct read and write operations managed by the user).\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/vectorsearch.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: VectorIndexType\n\n   There are 2 types of Vector Search indexes:\n   - `DELTA_SYNC`: An index that automatically syncs with a source Delta Table, automatically and incrementally updating the index as the underlying data in the Delta Table changes. - `DIRECT_ACCESS`: An index that supports direct read and write of vectors and metadata through our REST and SDK APIs. With this model, the user manages index updates.\n\n   .. py:attribute:: DELTA_SYNC\n      :value: \"DELTA_SYNC\"\n\n   .. py:attribute:: DIRECT_ACCESS\n      :value: \"DIRECT_ACCESS\"\n```\n\n----------------------------------------\n\nTITLE: Getting Default Namespace Setting - Python\nDESCRIPTION: Gets the default namespace setting. An optional etag can be provided for optimistic concurrency control.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/default_namespace.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n    .. py:method:: get( [, etag: Optional[str]]) -> DefaultNamespaceSetting\n\n        Get the default namespace setting.\n\n        Gets the default namespace setting.\n\n        :param etag: str (optional)\n          etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n          optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n          each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n          to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n          request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n        :returns: :class:`DefaultNamespaceSetting`\n```\n\n----------------------------------------\n\nTITLE: Updating a Schema in Databricks\nDESCRIPTION: This code snippet demonstrates updating the comment of an existing schema using the Databricks SDK for Python. It first creates a catalog and a schema within that catalog. Then it updates the schema's comment using `w.schemas.update`. Finally it cleans up by deleting the schema and the catalog. WorkspaceClient is necessary for interacting with the Databricks workspace.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/schemas.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nnew_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated = w.schemas.create(name=f\"sdk-{time.time.ns()}\", catalog_name=new_catalog.name)\n\n_ = w.schemas.update(full_name=created.full_name, comment=f\"sdk-{time.time_ns()}\")\n\n# cleanup\nw.catalogs.delete(name=new_catalog.name, force=True)\nw.schemas.delete(full_name=created.full_name)\n```\n\n----------------------------------------\n\nTITLE: Getting App Permission Levels in Databricks using Python\nDESCRIPTION: This snippet shows how to retrieve the permission levels that a user can have on an app using the `get_permission_levels` method. It takes the app name as input and returns a `GetAppPermissionLevelsResponse` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_permission_levels(app_name: str) -> GetAppPermissionLevelsResponse\n\n        Get app permission levels.\n\n        Gets the permission levels that a user can have on an object.\n\n        :param app_name: str\n          The app for which to get or manage permissions.\n\n        :returns: :class:`GetAppPermissionLevelsResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining RepairHistoryItemType Constants in Python\nDESCRIPTION: This snippet defines constants ORIGINAL and REPAIR, representing the repair history item type.  These constants indicate whether a run is the original run or a repair run.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nORIGINAL = \"ORIGINAL\"\n```\n\nLANGUAGE: python\nCODE:\n```\nREPAIR = \"REPAIR\"\n```\n\n----------------------------------------\n\nTITLE: Get Resource Quota Information - Python\nDESCRIPTION: This method retrieves information for a specific resource quota in Unity Catalog based on the parent securable type, parent full name, and quota name. It refreshes the quota count asynchronously if it's outdated. The updated count might not be immediately available.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/resource_quotas.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_quota(parent_securable_type: str, parent_full_name: str, quota_name: str) -> GetQuotaResponse\n\n    Get information for a single resource quota.\n\n    The GetQuota API returns usage information for a single resource quota, defined as a child-parent\n    pair. This API also refreshes the quota count if it is out of date. Refreshes are triggered\n    asynchronously. The updated count might not be returned in the first call.\n\n    :param parent_securable_type: str\n      Securable type of the quota parent.\n    :param parent_full_name: str\n      Full name of the parent resource. Provide the metastore ID if the parent is a metastore.\n    :param quota_name: str\n      Name of the quota. Follows the pattern of the quota type, with \"-quota\" added as a suffix.\n\n    :returns: :class:`GetQuotaResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining AwsAvailability Enum in Python\nDESCRIPTION: This code defines an enumeration (`AwsAvailability`) for specifying the availability type of AWS nodes in a Databricks cluster. It provides options for ON_DEMAND, SPOT, and SPOT_WITH_FALLBACK instances. The availability type is used for subsequent nodes past the `first_on_demand` nodes.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: AwsAvailability\n\n   Availability type used for all subsequent nodes past the `first_on_demand` ones.\n   Note: If `first_on_demand` is zero, this availability type will be used for the entire cluster.\n\n   .. py:attribute:: ON_DEMAND\n      :value: \"ON_DEMAND\"\n\n   .. py:attribute:: SPOT\n      :value: \"SPOT\"\n\n   .. py:attribute:: SPOT_WITH_FALLBACK\n      :value: \"SPOT_WITH_FALLBACK\"\n```\n\n----------------------------------------\n\nTITLE: Defining EBS Volume Types in Python\nDESCRIPTION: This code defines the supported EBS volume types for Databricks. It includes GENERAL_PURPOSE_SSD and THROUGHPUT_OPTIMIZED_HDD as possible volume types.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: EbsVolumeType\n\n   All EBS volume types that Databricks supports. See https://aws.amazon.com/ebs/details/ for details.\n\n   .. py:attribute:: GENERAL_PURPOSE_SSD\n      :value: \"GENERAL_PURPOSE_SSD\"\n\n   .. py:attribute:: THROUGHPUT_OPTIMIZED_HDD\n      :value: \"THROUGHPUT_OPTIMIZED_HDD\"\n```\n\n----------------------------------------\n\nTITLE: Defining CleanRoomTaskRunResultState Enum Python\nDESCRIPTION: This code snippet defines an enumeration called `CleanRoomTaskRunResultState` representing the result state of a clean room task run. Possible values include `CANCELED`, `DISABLED`, `EVICTED`, `EXCLUDED`, `FAILED`, `MAXIMUM_CONCURRENT_RUNS_REACHED`, `RUN_RESULT_STATE_UNSPECIFIED`, `SUCCESS`, `SUCCESS_WITH_FAILURES`, `TIMEDOUT`, `UPSTREAM_CANCELED`, `UPSTREAM_EVICTED`, and `UPSTREAM_FAILED`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CleanRoomTaskRunResultState\n\n   Copied from elastic-spark-common/api/messages/runs.proto. Using the original definition to avoid cyclic dependency.\n\n   .. py:attribute:: CANCELED\n      :value: \"CANCELED\"\n\n   .. py:attribute:: DISABLED\n      :value: \"DISABLED\"\n\n   .. py:attribute:: EVICTED\n      :value: \"EVICTED\"\n\n   .. py:attribute:: EXCLUDED\n      :value: \"EXCLUDED\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: MAXIMUM_CONCURRENT_RUNS_REACHED\n      :value: \"MAXIMUM_CONCURRENT_RUNS_REACHED\"\n\n   .. py:attribute:: RUN_RESULT_STATE_UNSPECIFIED\n      :value: \"RUN_RESULT_STATE_UNSPECIFIED\"\n\n   .. py:attribute:: SUCCESS\n      :value: \"SUCCESS\"\n\n   .. py:attribute:: SUCCESS_WITH_FAILURES\n      :value: \"SUCCESS_WITH_FAILURES\"\n\n   .. py:attribute:: TIMEDOUT\n      :value: \"TIMEDOUT\"\n\n   .. py:attribute:: UPSTREAM_CANCELED\n      :value: \"UPSTREAM_CANCELED\"\n\n   .. py:attribute:: UPSTREAM_EVICTED\n      :value: \"UPSTREAM_EVICTED\"\n\n   .. py:attribute:: UPSTREAM_FAILED\n      :value: \"UPSTREAM_FAILED\"\n```\n\n----------------------------------------\n\nTITLE: EndpointUseCase Enum Definition (Python)\nDESCRIPTION: Defines an enumeration for specifying the type of Databricks VPC endpoint service used when creating a VPC endpoint.  It lists possible values such as `DATAPLANE_RELAY_ACCESS` and `WORKSPACE_ACCESS`.  This enum is used in the `databricks.sdk.service.provisioning` module.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/provisioning.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: EndpointUseCase\n\n   This enumeration represents the type of Databricks VPC [endpoint service] that was used when creating this VPC endpoint.\n   [endpoint service]: https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html\n\n   .. py:attribute:: DATAPLANE_RELAY_ACCESS\n      :value: \"DATAPLANE_RELAY_ACCESS\"\n\n   .. py:attribute:: WORKSPACE_ACCESS\n      :value: \"WORKSPACE_ACCESS\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Volume in Databricks with Python SDK\nDESCRIPTION: This snippet demonstrates how to create a volume in Databricks using the Python SDK. It sets up necessary prerequisites like storage credentials, external locations, catalogs, and schemas before creating the volume. The volume is then created as an external volume using the specified storage location. Finally, it cleans up the created resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/volumes.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import catalog\n\nw = WorkspaceClient()\n\nstorage_credential = w.storage_credentials.create(\n    name=f\"sdk-{time.time_ns()}\",\n    aws_iam_role=catalog.AwsIamRoleRequest(role_arn=os.environ[\"TEST_METASTORE_DATA_ACCESS_ARN\"]),\n    comment=\"created via SDK\",\n)\n\nexternal_location = w.external_locations.create(\n    name=f\"sdk-{time.time_ns()}\",\n    credential_name=storage_credential.name,\n    comment=\"created via SDK\",\n    url=\"s3://\" + os.environ[\"TEST_BUCKET\"] + \"/\" + f\"sdk-{time.time_ns()}\",\n)\n\ncreated_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated_schema = w.schemas.create(name=f\"sdk-{time.time_ns()}\", catalog_name=created_catalog.name)\n\ncreated_volume = w.volumes.create(\n    catalog_name=created_catalog.name,\n    schema_name=created_schema.name,\n    name=f\"sdk-{time.time_ns()}\",\n    storage_location=external_location.url,\n    volume_type=catalog.VolumeType.EXTERNAL,\n)\n\n# cleanup\nw.storage_credentials.delete(name=storage_credential.name)\nw.external_locations.delete(name=external_location.name)\nw.schemas.delete(full_name=created_schema.full_name)\nw.catalogs.delete(name=created_catalog.name, force=True)\nw.volumes.delete(name=created_volume.full_name)\n```\n\n----------------------------------------\n\nTITLE: Defining Enum: ComplianceStandard - Python\nDESCRIPTION: This Python enum defines compliance standards for SHIELD customers. These standards include options like CANADA_PROTECTED_B, CYBER_ESSENTIAL_PLUS, FEDRAMP (HIGH, IL5, MODERATE), HIPAA, HITRUST, IRAP_PROTECTED, ISMAP, ITAR_EAR, NONE and PCI_DSS. This enumeration allows specifying compliance standards for Databricks.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ComplianceStandard\n\n   Compliance stardard for SHIELD customers\n\n   .. py:attribute:: CANADA_PROTECTED_B\n      :value: \"CANADA_PROTECTED_B\"\n\n   .. py:attribute:: CYBER_ESSENTIAL_PLUS\n      :value: \"CYBER_ESSENTIAL_PLUS\"\n\n   .. py:attribute:: FEDRAMP_HIGH\n      :value: \"FEDRAMP_HIGH\"\n\n   .. py:attribute:: FEDRAMP_IL5\n      :value: \"FEDRAMP_IL5\"\n\n   .. py:attribute:: FEDRAMP_MODERATE\n      :value: \"FEDRAMP_MODERATE\"\n\n   .. py:attribute:: HIPAA\n      :value: \"HIPAA\"\n\n   .. py:attribute:: HITRUST\n      :value: \"HITRUST\"\n\n   .. py:attribute:: IRAP_PROTECTED\n      :value: \"IRAP_PROTECTED\"\n\n   .. py:attribute:: ISMAP\n      :value: \"ISMAP\"\n\n   .. py:attribute:: ITAR_EAR\n      :value: \"ITAR_EAR\"\n\n   .. py:attribute:: NONE\n      :value: \"NONE\"\n\n   .. py:attribute:: PCI_DSS\n      :value: \"PCI_DSS\"\n```\n\n----------------------------------------\n\nTITLE: Updating Enable Export Notebook Setting - Python\nDESCRIPTION: This snippet illustrates updating the 'Enable Export Notebook' setting using the `patch_enable_export_notebook()` method.  It requires specifying `allow_missing` (always true), a `setting` object of type `EnableExportNotebook`, and a `field_mask` string indicating the fields to update.  The function returns the updated `EnableExportNotebook` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/enable_export_notebook.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: patch_enable_export_notebook(allow_missing: bool, setting: EnableExportNotebook, field_mask: str) -> EnableExportNotebook\n\n        Update the Enable Export Notebook setting.\n\n        Updates the Enable Export Notebook setting. The model follows eventual consistency, which means the\n        get after the update operation might receive stale values for some time.\n\n        :param allow_missing: bool\n          This should always be set to true for Settings API. Added for AIP compliance.\n        :param setting: :class:`EnableExportNotebook`\n        :param field_mask: str\n          The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n          field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n          `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n          the entire collection field can be specified. Field names must exactly match the resource field\n          names.\n\n          A field mask of `*` indicates full replacement. It’s recommended to always explicitly list the\n          fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n          changes in the future.\n\n        :returns: :class:`EnableExportNotebook`\n```\n\n----------------------------------------\n\nTITLE: Deleting a Table Constraint in Databricks\nDESCRIPTION: This code snippet shows how to delete a table constraint. The user must have USE_CATALOG and USE_SCHEMA privileges and be the table owner. The 'cascade' parameter controls whether child constraints are also deleted, requiring similar privileges on child tables if set to true. The method takes the full table name, constraint name, and cascade flag as input.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/table_constraints.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: delete(full_name: str, constraint_name: str, cascade: bool)\n\n        Delete a table constraint.\n\n        Deletes a table constraint.\n\n        For the table constraint deletion to succeed, the user must satisfy both of these conditions: - the\n        user must have the **USE_CATALOG** privilege on the table's parent catalog, the **USE_SCHEMA**\n        privilege on the table's parent schema, and be the owner of the table. - if __cascade__ argument is\n        **true**, the user must have the following permissions on all of the child tables: the **USE_CATALOG**\n        privilege on the table's catalog, the **USE_SCHEMA** privilege on the table's schema, and be the owner\n        of the table.\n\n        :param full_name: str\n          Full name of the table referenced by the constraint.\n        :param constraint_name: str\n          The name of the constraint to delete.\n        :param cascade: bool\n          If true, try deleting all child constraints of the current constraint. If false, reject this\n          operation if the current constraint has any child constraints.\n\n\n        \n```\n\n----------------------------------------\n\nTITLE: Creating a Function in Databricks\nDESCRIPTION: Creates a new User-Defined Function (UDF) in Unity Catalog. Requires USE_CATALOG on the function's parent catalog, and USE_SCHEMA and CREATE_FUNCTION on the function's parent schema. The function_info parameter is a CreateFunction object that specifies the function to be created.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/functions.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(function_info: CreateFunction) -> FunctionInfo\n\n    Create a function.\n\n    **WARNING: This API is experimental and will change in future versions**\n\n    Creates a new function\n\n    The user must have the following permissions in order for the function to be created: -\n    **USE_CATALOG** on the function's parent catalog - **USE_SCHEMA** and **CREATE_FUNCTION** on the\n    function's parent schema\n\n    :param function_info: :class:`CreateFunction`\n      Partial __FunctionInfo__ specifying the function to be created.\n\n    :returns: :class:`FunctionInfo`\n```\n\n----------------------------------------\n\nTITLE: Get Object Permissions using Permissions API in Python\nDESCRIPTION: This snippet demonstrates how to retrieve the permissions of a specific object in Databricks, in this case, a notebook. It first creates a notebook path, retrieves the object status to get its ID, and then uses the `get` method of the `permissions` API to fetch the permissions.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/permissions.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\nobj = w.workspace.get_status(path=notebook_path)\n\n_ = w.permissions.get(request_object_type=\"notebooks\", request_object_id=\"%d\" % (obj.object_id))\n```\n\n----------------------------------------\n\nTITLE: ArtifactCredentialType Constants Definition in Python\nDESCRIPTION: Defines constants for types of artifact access credentials. These constants specify the type of security credential, like AWS presigned URLs or Azure SAS URIs, used to access artifacts associated with ML models.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ArtifactCredentialType\n\n   The type of a given artifact access credential\n\n   .. py:attribute:: AWS_PRESIGNED_URL\n      :value: \"AWS_PRESIGNED_URL\"\n\n   .. py:attribute:: AZURE_ADLS_GEN2_SAS_URI\n      :value: \"AZURE_ADLS_GEN2_SAS_URI\"\n\n   .. py:attribute:: AZURE_SAS_URI\n      :value: \"AZURE_SAS_URI\"\n\n   .. py:attribute:: GCP_SIGNED_URL\n      :value: \"GCP_SIGNED_URL\"\n```\n\n----------------------------------------\n\nTITLE: Updating a Visualization in Databricks SQL (Legacy)\nDESCRIPTION: Updates an existing visualization. This method takes the visualization ID and optional parameters to update. It is recommended to use the new `queryvisualizations/update` endpoint instead of this legacy API. Returns a `LegacyVisualization` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/query_visualizations_legacy.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef update(id: str [, created_at: Optional[str], description: Optional[str], name: Optional[str], options: Optional[Any], query: Optional[LegacyQuery], type: Optional[str], updated_at: Optional[str]]) -> LegacyVisualization:\n    \"\"\"Edit existing visualization.\n\n    Updates visualization in the query.\n\n    **Note**: A new version of the Databricks SQL API is now available. Please use\n    :method:queryvisualizations/update instead. [Learn more]\n\n    [Learn more]: https://docs.databricks.com/en/sql/dbsql-api-latest.html\n\n    :param id: str\n      The UUID for this visualization.\n    :param created_at: str (optional)\n    :param description: str (optional)\n      A short description of this visualization. This is not displayed in the UI.\n    :param name: str (optional)\n      The name of the visualization that appears on dashboards and the query screen.\n    :param options: Any (optional)\n      The options object varies widely from one visualization type to the next and is unsupported.\n      Databricks does not recommend modifying visualization settings in JSON.\n    :param query: :class:`LegacyQuery` (optional)\n    :param type: str (optional)\n      The type of visualization: chart, table, pivot table, and so on.\n    :param updated_at: str (optional)\n\n    :returns: :class:`LegacyVisualization`\n    \"\"\n```\n\n----------------------------------------\n\nTITLE: Deleting a Budget Policy (Python)\nDESCRIPTION: Deletes an existing budget policy using the `delete` method of the `BudgetPolicyAPI`. Requires the `policy_id` of the policy to be deleted as input.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/budget_policy.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nBudgetPolicyAPI.delete(policy_id: str)\n```\n\n----------------------------------------\n\nTITLE: Defining Statement Execution States in Python\nDESCRIPTION: This snippet defines the possible states of a statement execution, including PENDING, RUNNING, SUCCEEDED, FAILED, CANCELED, and CLOSED. These states represent the lifecycle of a SQL statement executed on a Databricks warehouse. Each state is represented as a string constant.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n   .. py:attribute:: CANCELED\n      :value: \"CANCELED\"\n\n   .. py:attribute:: CLOSED\n      :value: \"CLOSED\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: PENDING\n      :value: \"PENDING\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n\n   .. py:attribute:: SUCCEEDED\n      :value: \"SUCCEEDED\"\n```\n\n----------------------------------------\n\nTITLE: Updating AI/BI Dashboard Embedding Access Policy in Python\nDESCRIPTION: Updates the AI/BI dashboard embedding access policy at the workspace level. The allow_missing parameter should always be set to true.  The setting parameter is an AibiDashboardEmbeddingAccessPolicySetting object, and field_mask specifies the fields to update. The function returns an AibiDashboardEmbeddingAccessPolicySetting object representing the updated policy.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/aibi_dashboard_embedding_access_policy.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n    .. py:method:: update(allow_missing: bool, setting: AibiDashboardEmbeddingAccessPolicySetting, field_mask: str) -> AibiDashboardEmbeddingAccessPolicySetting\n\n        Update the AI/BI dashboard embedding access policy.\n\n        Updates the AI/BI dashboard embedding access policy at the workspace level.\n\n        :param allow_missing: bool\n          This should always be set to true for Settings API. Added for AIP compliance.\n        :param setting: :class:`AibiDashboardEmbeddingAccessPolicySetting`\n        :param field_mask: str\n          The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n          field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n          `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n          the entire collection field can be specified. Field names must exactly match the resource field\n          names.\n\n          A field mask of `*` indicates full replacement. It’s recommended to always explicitly list the\n          fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n          changes in the future.\n\n        :returns: :class:`AibiDashboardEmbeddingAccessPolicySetting`\n```\n\n----------------------------------------\n\nTITLE: Defining AppResourceJobJobPermission Enum (Python)\nDESCRIPTION: This Python code defines an enumeration `AppResourceJobJobPermission` with values representing the permission level that can be granted on a Job resource associated with an App, such as `CAN_MANAGE`, `CAN_MANAGE_RUN`, `CAN_VIEW`, and `IS_OWNER`. These permissions control access and actions allowed on the job.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/apps.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: AppResourceJobJobPermission\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_MANAGE_RUN\n      :value: \"CAN_MANAGE_RUN\"\n\n   .. py:attribute:: CAN_VIEW\n      :value: \"CAN_VIEW\"\n\n   .. py:attribute:: IS_OWNER\n      :value: \"IS_OWNER\"\n```\n\n----------------------------------------\n\nTITLE: Defining AiGatewayRateLimitKey Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `AiGatewayRateLimitKey` with possible values ENDPOINT and USER. It specifies the key used for rate limiting in the AI Gateway.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: AiGatewayRateLimitKey\n\n   .. py:attribute:: ENDPOINT\n      :value: \"ENDPOINT\"\n\n   .. py:attribute:: USER\n      :value: \"USER\"\n```\n\n----------------------------------------\n\nTITLE: Delete Custom OAuth App Integration (Python)\nDESCRIPTION: Deletes an existing Custom OAuth App Integration, identified by its integration ID. The integration can be retrieved prior to deletion using the get method.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/custom_app_integration.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(integration_id: str)\n\n    Delete Custom OAuth App Integration.\n\n    Delete an existing Custom OAuth App Integration. You can retrieve the custom OAuth app integration via\n    :method:CustomAppIntegration/get.\n\n    :param integration_id: str\n```\n\n----------------------------------------\n\nTITLE: Defining QueryEndpointResponseObject Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `QueryEndpointResponseObject` with possible values CHAT_COMPLETION, LIST, and TEXT_COMPLETION.  It indicates the type of object returned by the serving endpoint.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: QueryEndpointResponseObject\n\n   The type of object returned by the __external/foundation model__ serving endpoint, one of [text_completion, chat.completion, list (of embeddings)].\n\n   .. py:attribute:: CHAT_COMPLETION\n      :value: \"CHAT_COMPLETION\"\n\n   .. py:attribute:: LIST\n      :value: \"LIST\"\n\n   .. py:attribute:: TEXT_COMPLETION\n      :value: \"TEXT_COMPLETION\"\n```\n\n----------------------------------------\n\nTITLE: Patching Service Principal Details\nDESCRIPTION: This snippet demonstrates how to partially update service principal details using the `patch` method. It creates a service principal, retrieves its ID, and then patches the `active` status using a `Patch` operation.  It also includes the necessary schema for the patch operation. Finally, it cleans up by deleting the service principal.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/service_principals.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import iam\n\na = AccountClient()\n\nsp_create = a.service_principals.create(active=True, display_name=f\"sdk-{time.time_ns()}\")\n\nsp = a.service_principals.get(id=sp_create.id)\n\na.service_principals.patch(\n    id=sp.id,\n    operations=[iam.Patch(op=iam.PatchOp.REPLACE, path=\"active\", value=\"false\")],\n    schemas=[iam.PatchSchema.URN_IETF_PARAMS_SCIM_API_MESSAGES_2_0_PATCH_OP],\n)\n\n# cleanup\na.service_principals.delete(id=sp_create.id)\n```\n\n----------------------------------------\n\nTITLE: ErrorType Enum Definition (Python)\nDESCRIPTION: Defines an enumeration representing AWS resource types associated with an error in the `databricks.sdk.service.provisioning` module.  Possible values include `CREDENTIALS`, `NETWORK_ACL`, `SECURITY_GROUP`, `SUBNET`, and `VPC`. It helps categorize errors during resource provisioning.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/provisioning.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ErrorType\n\n   The AWS resource associated with this error: credentials, VPC, subnet, security group, or network ACL.\n\n   .. py:attribute:: CREDENTIALS\n      :value: \"CREDENTIALS\"\n\n   .. py:attribute:: NETWORK_ACL\n      :value: \"NETWORK_ACL\"\n\n   .. py:attribute:: SECURITY_GROUP\n      :value: \"SECURITY_GROUP\"\n\n   .. py:attribute:: SUBNET\n      :value: \"SUBNET\"\n\n   .. py:attribute:: VPC\n      :value: \"VPC\"\n```\n\n----------------------------------------\n\nTITLE: Update Compliance Security Profile Setting (Python)\nDESCRIPTION: Updates the compliance security profile setting. Requires a fresh etag obtained via a GET request. The `field_mask` parameter specifies which fields to update. The `allow_missing` parameter is required for AIP compliance and must be set to true.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/compliance_security_profile.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(allow_missing: bool, setting: ComplianceSecurityProfileSetting, field_mask: str) -> ComplianceSecurityProfileSetting\n\n    Update the compliance security profile setting.\n\n    Updates the compliance security profile setting for the workspace. A fresh etag needs to be provided\n    in `PATCH` requests (as part of the setting field). The etag can be retrieved by making a `GET`\n    request before the `PATCH` request. If the setting is updated concurrently, `PATCH` fails with 409 and\n    the request must be retried by using the fresh etag in the 409 response.\n\n    :param allow_missing: bool\n      This should always be set to true for Settings API. Added for AIP compliance.\n    :param setting: :class:`ComplianceSecurityProfileSetting`\n    :param field_mask: str\n      The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n      field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n      `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n      the entire collection field can be specified. Field names must exactly match the resource field\n      names.\n\n      A field mask of `*` indicates full replacement. It’s recommended to always explicitly list the\n      fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n      changes in the future.\n\n    :returns: :class:`ComplianceSecurityProfileSetting`\n```\n\n----------------------------------------\n\nTITLE: Creating a Credential in Databricks (Python)\nDESCRIPTION: This method creates a new credential within the Databricks metastore. The type of credential is determined by the 'purpose' parameter, which can be either 'SERVICE' or 'STORAGE'. The caller must have the appropriate metastore privileges to execute this function.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/credentials.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create_credential(name: str [, aws_iam_role: Optional[AwsIamRole], azure_managed_identity: Optional[AzureManagedIdentity], azure_service_principal: Optional[AzureServicePrincipal], comment: Optional[str], databricks_gcp_service_account: Optional[DatabricksGcpServiceAccount], purpose: Optional[CredentialPurpose], read_only: Optional[bool], skip_validation: Optional[bool]]) -> CredentialInfo\n\n        Create a credential.\n\n        Creates a new credential. The type of credential to be created is determined by the **purpose** field,\n        which should be either **SERVICE** or **STORAGE**.\n\n        The caller must be a metastore admin or have the metastore privilege **CREATE_STORAGE_CREDENTIAL** for\n        storage credentials, or **CREATE_SERVICE_CREDENTIAL** for service credentials.\n\n        :param name: str\n          The credential name. The name must be unique among storage and service credentials within the\n          metastore.\n        :param aws_iam_role: :class:`AwsIamRole` (optional)\n          The AWS IAM role configuration\n        :param azure_managed_identity: :class:`AzureManagedIdentity` (optional)\n          The Azure managed identity configuration.\n        :param azure_service_principal: :class:`AzureServicePrincipal` (optional)\n          The Azure service principal configuration. Only applicable when purpose is **STORAGE**.\n        :param comment: str (optional)\n          Comment associated with the credential.\n        :param databricks_gcp_service_account: :class:`DatabricksGcpServiceAccount` (optional)\n          GCP long-lived credential. Databricks-created Google Cloud Storage service account.\n        :param purpose: :class:`CredentialPurpose` (optional)\n          Indicates the purpose of the credential.\n        :param read_only: bool (optional)\n          Whether the credential is usable only for read operations. Only applicable when purpose is\n          **STORAGE**.\n        :param skip_validation: bool (optional)\n          Optional. Supplying true to this argument skips validation of the created set of credentials.\n\n        :returns: :class:`CredentialInfo`\n```\n\n----------------------------------------\n\nTITLE: Get Assignable Roles for Resource - Python\nDESCRIPTION: Retrieves all roles that can be granted on a specified account-level resource. A role is grantable if the rule set on the resource can contain an access rule of that role.\n\nParameters:\nresource (str): The resource name for which assignable roles will be listed.\n\nReturns: GetAssignableRolesForResourceResponse\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/account_access_control_proxy.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_assignable_roles_for_resource(resource: str) -> GetAssignableRolesForResourceResponse\n\n    Get assignable roles for a resource.\n\n    Gets all the roles that can be granted on an account-level resource. A role is grantable if the rule\n    set on the resource can contain an access rule of the role.\n\n    :param resource: str\n      The resource name for which assignable roles will be listed.\n\n    :returns: :class:`GetAssignableRolesForResourceResponse`\n```\n\n----------------------------------------\n\nTITLE: Deleting a Legacy Alert in Databricks SQL\nDESCRIPTION: This method deletes an existing alert in Databricks SQL. Deleted alerts are permanently removed and cannot be restored. It takes the alert ID as a parameter to identify the alert to be deleted.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/alerts_legacy.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nw.alerts_legacy.delete(alert_id: str)\n```\n\n----------------------------------------\n\nTITLE: Getting a Function in Databricks\nDESCRIPTION: Retrieves a function from Unity Catalog based on its name.  The user must be a metastore admin, owner of the function's parent catalog, or have the necessary privileges on the catalog, schema, and function (USE_CATALOG, USE_SCHEMA, EXECUTE). The name parameter is the fully-qualified function name. The include_browse parameter is optional.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/functions.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(name: str [, include_browse: Optional[bool]]) -> FunctionInfo\n\n    Get a function.\n\n    Gets a function from within a parent catalog and schema. For the fetch to succeed, the user must\n    satisfy one of the following requirements: - Is a metastore admin - Is an owner of the function's\n    parent catalog - Have the **USE_CATALOG** privilege on the function's parent catalog and be the owner\n    of the function - Have the **USE_CATALOG** privilege on the function's parent catalog, the\n    **USE_SCHEMA** privilege on the function's parent schema, and the **EXECUTE** privilege on the\n    function itself\n\n    :param name: str\n      The fully-qualified name of the function (of the form\n      __catalog_name__.__schema_name__.__function__name__).\n    :param include_browse: bool (optional)\n      Whether to include functions in the response for which the principal can only access selective\n      metadata for\n\n    :returns: :class:`FunctionInfo`\n```\n\n----------------------------------------\n\nTITLE: Getting Forecasting Experiment using Databricks SDK\nDESCRIPTION: This code snippet shows how to retrieve a forecasting experiment using its unique ID with the `get_experiment` method of the `ForecastingAPI` class. The experiment ID is required as a parameter. The method returns a `ForecastingExperiment` object containing the details of the forecasting experiment.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/forecasting.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nw.forecasting.get_experiment(experiment_id: str) -> ForecastingExperiment\n```\n\n----------------------------------------\n\nTITLE: Defining ExternalModelProvider Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `ExternalModelProvider` with possible values AI21LABS, AMAZON_BEDROCK, ANTHROPIC, COHERE, CUSTOM, DATABRICKS_MODEL_SERVING, GOOGLE_CLOUD_VERTEX_AI, OPENAI, and PALM. It specifies the provider of an external model.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ExternalModelProvider\n\n   .. py:attribute:: AI21LABS\n      :value: \"AI21LABS\"\n\n   .. py:attribute:: AMAZON_BEDROCK\n      :value: \"AMAZON_BEDROCK\"\n\n   .. py:attribute:: ANTHROPIC\n      :value: \"ANTHROPIC\"\n\n   .. py:attribute:: COHERE\n      :value: \"COHERE\"\n\n   .. py:attribute:: CUSTOM\n      :value: \"CUSTOM\"\n\n   .. py:attribute:: DATABRICKS_MODEL_SERVING\n      :value: \"DATABRICKS_MODEL_SERVING\"\n\n   .. py:attribute:: GOOGLE_CLOUD_VERTEX_AI\n      :value: \"GOOGLE_CLOUD_VERTEX_AI\"\n\n   .. py:attribute:: OPENAI\n      :value: \"OPENAI\"\n\n   .. py:attribute:: PALM\n      :value: \"PALM\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Approved Domains (Python)\nDESCRIPTION: Retrieves the list of domains approved to host embedded AI/BI dashboards. It accepts an optional 'etag' parameter for optimistic concurrency control.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/aibi_dashboard_embedding_approved_domains.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get( [, etag: Optional[str]]) -> AibiDashboardEmbeddingApprovedDomainsSetting\n\n    Retrieve the list of domains approved to host embedded AI/BI dashboards.\n\n    Retrieves the list of domains approved to host embedded AI/BI dashboards.\n\n    :param etag: str (optional)\n      etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n      optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n      each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n      to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n      request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n    :returns: :class:`AibiDashboardEmbeddingApprovedDomainsSetting`\n```\n\n----------------------------------------\n\nTITLE: Defining UpdateRunStatus in Python\nDESCRIPTION: These attributes define the possible statuses for a run, including FAILED, FINISHED, KILLED, RUNNING, and SCHEDULED. These statuses describe the lifecycle of a run during model training and evaluation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: UpdateRunStatus\n\n   Status of a run.\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: FINISHED\n      :value: \"FINISHED\"\n\n   .. py:attribute:: KILLED\n      :value: \"KILLED\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n\n   .. py:attribute:: SCHEDULED\n      :value: \"SCHEDULED\"\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_EDIT attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_EDIT attribute for the PermissionLevel class. It represents a permission level that allows editing resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_EDIT\n      :value: \"CAN_EDIT\"\n```\n\n----------------------------------------\n\nTITLE: Defining ServingModelWorkloadType Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `ServingModelWorkloadType` with possible values CPU, GPU_LARGE, GPU_MEDIUM, GPU_SMALL and MULTIGPU_MEDIUM. It indicates the type of workload for a serving model.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ServingModelWorkloadType\n\n   Please keep this in sync with with workload types in InferenceEndpointEntities.scala\n\n   .. py:attribute:: CPU\n      :value: \"CPU\"\n\n   .. py:attribute:: GPU_LARGE\n      :value: \"GPU_LARGE\"\n\n   .. py:attribute:: GPU_MEDIUM\n      :value: \"GPU_MEDIUM\"\n\n   .. py:attribute:: GPU_SMALL\n      :value: \"GPU_SMALL\"\n\n   .. py:attribute:: MULTIGPU_MEDIUM\n      :value: \"MULTIGPU_MEDIUM\"\n```\n\n----------------------------------------\n\nTITLE: Defining TerminationReasonCode Enum (Python)\nDESCRIPTION: Defines an enumeration `TerminationReasonCode` that specifies the reasons for cluster termination. It provides a set of attributes representing different termination reasons, such as `ABUSE_DETECTED`, `AWS_AUTHORIZATION_FAILURE`, etc.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: TerminationReasonCode\n\n   The status code indicating why the cluster was terminated\n\n   .. py:attribute:: ABUSE_DETECTED\n      :value: \"ABUSE_DETECTED\"\n\n   .. py:attribute:: ACCESS_TOKEN_FAILURE\n      :value: \"ACCESS_TOKEN_FAILURE\"\n\n   .. py:attribute:: ALLOCATION_TIMEOUT\n      :value: \"ALLOCATION_TIMEOUT\"\n\n   .. py:attribute:: ALLOCATION_TIMEOUT_NODE_DAEMON_NOT_READY\n      :value: \"ALLOCATION_TIMEOUT_NODE_DAEMON_NOT_READY\"\n\n   .. py:attribute:: ALLOCATION_TIMEOUT_NO_HEALTHY_AND_WARMED_UP_CLUSTERS\n      :value: \"ALLOCATION_TIMEOUT_NO_HEALTHY_AND_WARMED_UP_CLUSTERS\"\n\n   .. py:attribute:: ALLOCATION_TIMEOUT_NO_HEALTHY_CLUSTERS\n      :value: \"ALLOCATION_TIMEOUT_NO_HEALTHY_CLUSTERS\"\n\n   .. py:attribute:: ALLOCATION_TIMEOUT_NO_MATCHED_CLUSTERS\n      :value: \"ALLOCATION_TIMEOUT_NO_MATCHED_CLUSTERS\"\n\n   .. py:attribute:: ALLOCATION_TIMEOUT_NO_READY_CLUSTERS\n      :value: \"ALLOCATION_TIMEOUT_NO_READY_CLUSTERS\"\n\n   .. py:attribute:: ALLOCATION_TIMEOUT_NO_UNALLOCATED_CLUSTERS\n      :value: \"ALLOCATION_TIMEOUT_NO_UNALLOCATED_CLUSTERS\"\n\n   .. py:attribute:: ALLOCATION_TIMEOUT_NO_WARMED_UP_CLUSTERS\n      :value: \"ALLOCATION_TIMEOUT_NO_WARMED_UP_CLUSTERS\"\n\n   .. py:attribute:: ATTACH_PROJECT_FAILURE\n      :value: \"ATTACH_PROJECT_FAILURE\"\n\n   .. py:attribute:: AWS_AUTHORIZATION_FAILURE\n      :value: \"AWS_AUTHORIZATION_FAILURE\"\n\n   .. py:attribute:: AWS_INACCESSIBLE_KMS_KEY_FAILURE\n      :value: \"AWS_INACCESSIBLE_KMS_KEY_FAILURE\"\n\n   .. py:attribute:: AWS_INSTANCE_PROFILE_UPDATE_FAILURE\n      :value: \"AWS_INSTANCE_PROFILE_UPDATE_FAILURE\"\n\n   .. py:attribute:: AWS_INSUFFICIENT_FREE_ADDRESSES_IN_SUBNET_FAILURE\n      :value: \"AWS_INSUFFICIENT_FREE_ADDRESSES_IN_SUBNET_FAILURE\"\n\n   .. py:attribute:: AWS_INSUFFICIENT_INSTANCE_CAPACITY_FAILURE\n      :value: \"AWS_INSUFFICIENT_INSTANCE_CAPACITY_FAILURE\"\n\n   .. py:attribute:: AWS_INVALID_KEY_PAIR\n      :value: \"AWS_INVALID_KEY_PAIR\"\n\n   .. py:attribute:: AWS_INVALID_KMS_KEY_STATE\n      :value: \"AWS_INVALID_KMS_KEY_STATE\"\n\n   .. py:attribute:: AWS_MAX_SPOT_INSTANCE_COUNT_EXCEEDED_FAILURE\n      :value: \"AWS_MAX_SPOT_INSTANCE_COUNT_EXCEEDED_FAILURE\"\n\n   .. py:attribute:: AWS_REQUEST_LIMIT_EXCEEDED\n      :value: \"AWS_REQUEST_LIMIT_EXCEEDED\"\n\n   .. py:attribute:: AWS_RESOURCE_QUOTA_EXCEEDED\n      :value: \"AWS_RESOURCE_QUOTA_EXCEEDED\"\n\n   .. py:attribute:: AWS_UNSUPPORTED_FAILURE\n      :value: \"AWS_UNSUPPORTED_FAILURE\"\n\n   .. py:attribute:: AZURE_BYOK_KEY_PERMISSION_FAILURE\n      :value: \"AZURE_BYOK_KEY_PERMISSION_FAILURE\"\n\n   .. py:attribute:: AZURE_EPHEMERAL_DISK_FAILURE\n      :value: \"AZURE_EPHEMERAL_DISK_FAILURE\"\n\n   .. py:attribute:: AZURE_INVALID_DEPLOYMENT_TEMPLATE\n      :value: \"AZURE_INVALID_DEPLOYMENT_TEMPLATE\"\n\n   .. py:attribute:: AZURE_OPERATION_NOT_ALLOWED_EXCEPTION\n      :value: \"AZURE_OPERATION_NOT_ALLOWED_EXCEPTION\"\n\n   .. py:attribute:: AZURE_PACKED_DEPLOYMENT_PARTIAL_FAILURE\n      :value: \"AZURE_PACKED_DEPLOYMENT_PARTIAL_FAILURE\"\n\n   .. py:attribute:: AZURE_QUOTA_EXCEEDED_EXCEPTION\n      :value: \"AZURE_QUOTA_EXCEEDED_EXCEPTION\"\n\n   .. py:attribute:: AZURE_RESOURCE_MANAGER_THROTTLING\n      :value: \"AZURE_RESOURCE_MANAGER_THROTTLING\"\n\n   .. py:attribute:: AZURE_RESOURCE_PROVIDER_THROTTLING\n      :value: \"AZURE_RESOURCE_PROVIDER_THROTTLING\"\n\n   .. py:attribute:: AZURE_UNEXPECTED_DEPLOYMENT_TEMPLATE_FAILURE\n      :value: \"AZURE_UNEXPECTED_DEPLOYMENT_TEMPLATE_FAILURE\"\n\n   .. py:attribute:: AZURE_VM_EXTENSION_FAILURE\n      :value: \"AZURE_VM_EXTENSION_FAILURE\"\n\n   .. py:attribute:: AZURE_VNET_CONFIGURATION_FAILURE\n      :value: \"AZURE_VNET_CONFIGURATION_FAILURE\"\n\n   .. py:attribute:: BOOTSTRAP_TIMEOUT\n      :value: \"BOOTSTRAP_TIMEOUT\"\n\n   .. py:attribute:: BOOTSTRAP_TIMEOUT_CLOUD_PROVIDER_EXCEPTION\n      :value: \"BOOTSTRAP_TIMEOUT_CLOUD_PROVIDER_EXCEPTION\"\n\n   .. py:attribute:: BOOTSTRAP_TIMEOUT_DUE_TO_MISCONFIG\n      :value: \"BOOTSTRAP_TIMEOUT_DUE_TO_MISCONFIG\"\n\n   .. py:attribute:: BUDGET_POLICY_LIMIT_ENFORCEMENT_ACTIVATED\n      :value: \"BUDGET_POLICY_LIMIT_ENFORCEMENT_ACTIVATED\"\n\n   .. py:attribute:: BUDGET_POLICY_RESOLUTION_FAILURE\n      :value: \"BUDGET_POLICY_RESOLUTION_FAILURE\"\n\n   .. py:attribute:: CLOUD_ACCOUNT_SETUP_FAILURE\n      :value: \"CLOUD_ACCOUNT_SETUP_FAILURE\"\n\n   .. py:attribute:: CLOUD_OPERATION_CANCELLED\n      :value: \"CLOUD_OPERATION_CANCELLED\"\n\n   .. py:attribute:: CLOUD_PROVIDER_DISK_SETUP_FAILURE\n      :value: \"CLOUD_PROVIDER_DISK_SETUP_FAILURE\"\n\n   .. py:attribute:: CLOUD_PROVIDER_INSTANCE_NOT_LAUNCHED\n      :value: \"CLOUD_PROVIDER_INSTANCE_NOT_LAUNCHED\"\n\n   .. py:attribute:: CLOUD_PROVIDER_LAUNCH_FAILURE\n      :value: \"CLOUD_PROVIDER_LAUNCH_FAILURE\"\n\n   .. py:attribute:: CLOUD_PROVIDER_LAUNCH_FAILURE_DUE_TO_MISCONFIG\n      :value: \"CLOUD_PROVIDER_LAUNCH_FAILURE_DUE_TO_MISCONFIG\"\n\n   .. py:attribute:: CLOUD_PROVIDER_RESOURCE_STOCKOUT\n      :value: \"CLOUD_PROVIDER_RESOURCE_STOCKOUT\"\n\n   .. py:attribute:: CLOUD_PROVIDER_RESOURCE_STOCKOUT_DUE_TO_MISCONFIG\n      :value: \"CLOUD_PROVIDER_RESOURCE_STOCKOUT_DUE_TO_MISCONFIG\"\n\n   .. py:attribute:: CLOUD_PROVIDER_SHUTDOWN\n      :value: \"CLOUD_PROVIDER_SHUTDOWN\"\n\n   .. py:attribute:: CLUSTER_OPERATION_THROTTLED\n      :value: \"CLUSTER_OPERATION_THROTTLED\"\n\n   .. py:attribute:: CLUSTER_OPERATION_TIMEOUT\n      :value: \"CLUSTER_OPERATION_TIMEOUT\"\n\n   .. py:attribute:: COMMUNICATION_LOST\n      :value: \"COMMUNICATION_LOST\"\n\n   .. py:attribute:: CONTAINER_LAUNCH_FAILURE\n      :value: \"CONTAINER_LAUNCH_FAILURE\"\n\n   .. py:attribute:: CONTROL_PLANE_REQUEST_FAILURE\n```\n\n----------------------------------------\n\nTITLE: Creating or Updating an ACL in Databricks using Python SDK\nDESCRIPTION: This code snippet shows how to create or update an Access Control List (ACL) for a given principal (user or group) on a specified secret scope in Databricks using the Python SDK. It initializes a WorkspaceClient, creates a group, creates a secret scope, sets the ACL using `w.secrets.put_acl()`, and then cleans up by deleting the group, a secret, and the scope.  This requires the workspace.AclPermission enum.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/secrets.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import workspace\n\nw = WorkspaceClient()\n\nkey_name = f\"sdk-{time.time_ns()}\"\n\ngroup = w.groups.create(display_name=f\"sdk-{time.time_ns()}\")\n\nscope_name = f\"sdk-{time.time_ns()}\"\n\nw.secrets.create_scope(scope=scope_name)\n\nw.secrets.put_acl(\n    scope=scope_name,\n    permission=workspace.AclPermission.MANAGE,\n    principal=group.display_name,\n)\n\n# cleanup\nw.groups.delete(id=group.id)\nw.secrets.delete_secret(scope=scope_name, key=key_name)\nw.secrets.delete_scope(scope=scope_name)\n```\n\n----------------------------------------\n\nTITLE: Getting Budget Configuration by ID with Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to retrieve a budget configuration by its ID using the `get` method of the `BudgetsAPI` class. It assumes a budget has already been created. A cleanup step is included to delete the created budget.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/budgets.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import billing\n\na = AccountClient()\n\ncreated = a.budgets.create(\n    budget=billing.CreateBudgetConfigurationBudget(\n        display_name=f\"sdk-{time.time_ns()}\",\n        filter=billing.BudgetConfigurationFilter(\n            tags=[\n                billing.BudgetConfigurationFilterTagClause(\n                    key=\"tagName\",\n                    value=billing.BudgetConfigurationFilterClause(\n                        operator=billing.BudgetConfigurationFilterOperator.IN,\n                        values=[\"all\"],\n                    ),\n                )\n            ]\n        ),\n        alert_configurations=[\n            billing.CreateBudgetConfigurationBudgetAlertConfigurations(\n                time_period=billing.AlertConfigurationTimePeriod.MONTH,\n                quantity_type=billing.AlertConfigurationQuantityType.LIST_PRICE_DOLLARS_USD,\n                trigger_type=billing.AlertConfigurationTriggerType.CUMULATIVE_SPENDING_EXCEEDED,\n                quantity_threshold=\"100\",\n                action_configurations=[\n                    billing.CreateBudgetConfigurationBudgetActionConfigurations(\n                        action_type=billing.ActionConfigurationType.EMAIL_NOTIFICATION,\n                        target=\"admin@example.com\",\n                    )\n                ],\n            )\n        ],\n    )\n)\n\nby_id = a.budgets.get(budget_id=created.budget.budget_configuration_id)\n\n# cleanup\na.budgets.delete(budget_id=created.budget.budget_configuration_id)\n```\n\n----------------------------------------\n\nTITLE: Deleting AI/BI Dashboard Embedding Access Policy in Python\nDESCRIPTION: Deletes the AI/BI dashboard embedding access policy, reverting back to the default. An optional etag can be provided for optimistic concurrency control to prevent simultaneous writes from overwriting each other.  The function returns a DeleteAibiDashboardEmbeddingAccessPolicySettingResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/aibi_dashboard_embedding_access_policy.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n    .. py:method:: delete( [, etag: Optional[str]]) -> DeleteAibiDashboardEmbeddingAccessPolicySettingResponse\n\n        Delete the AI/BI dashboard embedding access policy.\n\n        Delete the AI/BI dashboard embedding access policy, reverting back to the default.\n\n        :param etag: str (optional)\n          etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n          optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n          each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n          to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n          request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n        :returns: :class:`DeleteAibiDashboardEmbeddingAccessPolicySettingResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining RunLifecycleStateV2State Constants in Python\nDESCRIPTION: This snippet defines constants representing the current state of a run (V2), such as BLOCKED, PENDING, QUEUED, RUNNING, TERMINATED, TERMINATING, and WAITING.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nBLOCKED = \"BLOCKED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nPENDING = \"PENDING\"\n```\n\nLANGUAGE: python\nCODE:\n```\nQUEUED = \"QUEUED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nRUNNING = \"RUNNING\"\n```\n\nLANGUAGE: python\nCODE:\n```\nTERMINATED = \"TERMINATED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nTERMINATING = \"TERMINATING\"\n```\n\nLANGUAGE: python\nCODE:\n```\nWAITING = \"WAITING\"\n```\n\n----------------------------------------\n\nTITLE: Defining System Schema Info States in Python\nDESCRIPTION: This code defines the SystemSchemaInfoState class, representing the possible enablement states for the system schema. Each state is represented as a class attribute with a string value. These states includes AVAILABLE, DISABLE_INITIALIZED, ENABLE_COMPLETED, ENABLE_INITIALIZED and UNAVAILABLE.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass SystemSchemaInfoState:\n   \"\"\"The current state of enablement for the system schema. An empty string means the system schema is available and ready for opt-in.\"\"\"\n\n   AVAILABLE = \"AVAILABLE\"\n\n   DISABLE_INITIALIZED = \"DISABLE_INITIALIZED\"\n\n   ENABLE_COMPLETED = \"ENABLE_COMPLETED\"\n\n   ENABLE_INITIALIZED = \"ENABLE_INITIALIZED\"\n\n   UNAVAILABLE = \"UNAVAILABLE\"\n```\n\n----------------------------------------\n\nTITLE: Defining DAYS, HOURS and WEEKS Constants for PeriodicTriggerConfigurationTimeUnit in Python\nDESCRIPTION: This snippet defines constants DAYS, HOURS, and WEEKS to represent time units for periodic trigger configurations.  These constants are used when defining periodic triggers for job runs.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nDAYS = \"DAYS\"\n```\n\nLANGUAGE: python\nCODE:\n```\nHOURS = \"HOURS\"\n```\n\nLANGUAGE: python\nCODE:\n```\nWEEKS = \"WEEKS\"\n```\n\n----------------------------------------\n\nTITLE: Getting a rule set - Python\nDESCRIPTION: This method retrieves a rule set by its name. The ruleset name and an etag are required as inputs. The etag is used for optimistic concurrency control to prevent simultaneous updates from overwriting each other. The method returns a RuleSetResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/access_control.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nAccountAccessControlAPI.get_rule_set(name: str, etag: str) -> RuleSetResponse\n```\n\n----------------------------------------\n\nTITLE: Defining ExternalFunctionRequestHttpMethod Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `ExternalFunctionRequestHttpMethod` with possible values DELETE, GET, PATCH, POST, and PUT. It specifies the HTTP method used for external function requests.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ExternalFunctionRequestHttpMethod\n\n   .. py:attribute:: DELETE\n      :value: \"DELETE\"\n\n   .. py:attribute:: GET\n      :value: \"GET\"\n\n   .. py:attribute:: PATCH\n      :value: \"PATCH\"\n\n   .. py:attribute:: POST\n      :value: \"POST\"\n\n   .. py:attribute:: PUT\n      :value: \"PUT\"\n```\n\n----------------------------------------\n\nTITLE: Updating Permissions on Databricks Serving Endpoint\nDESCRIPTION: This method updates the permissions on a Databricks serving endpoint. Serving endpoints can inherit permissions from their root object. The method takes the serving endpoint ID and an access control list as parameters.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: update_permissions(serving_endpoint_id: str [, access_control_list: Optional[List[ServingEndpointAccessControlRequest]]]) -> ServingEndpointPermissions\n\n        Update serving endpoint permissions.\n\n        Updates the permissions on a serving endpoint. Serving endpoints can inherit permissions from their\\n        root object.\n\n        :param serving_endpoint_id: str\n          The serving endpoint for which to get or manage permissions.\n        :param access_control_list: List[:class:`ServingEndpointAccessControlRequest`] (optional)\n\n        :returns: :class:`ServingEndpointPermissions`\n        \n```\n\n----------------------------------------\n\nTITLE: Defining Init Script Execution Status in Python\nDESCRIPTION: This code defines the possible execution statuses for init scripts. It includes statuses such as FAILED_EXECUTION, SUCCEEDED, SKIPPED, and UNKNOWN to indicate the result of the script execution.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: InitScriptExecutionDetailsInitScriptExecutionStatus\n\n   Result of attempted script execution\n\n   .. py:attribute:: FAILED_EXECUTION\n      :value: \"FAILED_EXECUTION\"\n\n   .. py:attribute:: FAILED_FETCH\n      :value: \"FAILED_FETCH\"\n\n   .. py:attribute:: FUSE_MOUNT_FAILED\n      :value: \"FUSE_MOUNT_FAILED\"\n\n   .. py:attribute:: NOT_EXECUTED\n      :value: \"NOT_EXECUTED\"\n\n   .. py:attribute:: SKIPPED\n      :value: \"SKIPPED\"\n\n   .. py:attribute:: SUCCEEDED\n      :value: \"SUCCEEDED\"\n\n   .. py:attribute:: UNKNOWN\n      :value: \"UNKNOWN\"\n```\n\n----------------------------------------\n\nTITLE: Defining DESCENDING attribute for GetSortOrder in Python\nDESCRIPTION: Defines the DESCENDING attribute for the GetSortOrder class. It represents the descending sort order with a string value.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: GetSortOrder\n\n   .. py:attribute:: DESCENDING\n      :value: \"DESCENDING\"\n```\n\n----------------------------------------\n\nTITLE: Create and Wait for Endpoint in Databricks\nDESCRIPTION: Creates a vector search endpoint and waits for it to become available. Takes the endpoint name, type, and an optional timeout. Returns the EndpointInfo once the endpoint is ready.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_endpoints.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nw.vector_search_endpoints.create_endpoint_and_wait(name: str, endpoint_type: EndpointType, timeout: datetime.timedelta = 0:20:00) -> EndpointInfo\n```\n\n----------------------------------------\n\nTITLE: PipelinePermissionLevel Enum Definition in Python\nDESCRIPTION: Defines an enumeration of permission levels for accessing and managing Delta Live Tables pipelines. It includes options like CAN_MANAGE, CAN_RUN, CAN_VIEW, and IS_OWNER, which dictate the level of control and access a user has over the pipeline.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: PipelinePermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_RUN\n      :value: \"CAN_RUN\"\n\n   .. py:attribute:: CAN_VIEW\n      :value: \"CAN_VIEW\"\n\n   .. py:attribute:: IS_OWNER\n      :value: \"IS_OWNER\"\n```\n\n----------------------------------------\n\nTITLE: PipelineStateInfoHealth Enum Definition in Python\nDESCRIPTION: Defines an enumeration indicating the health of the pipeline based on its state information. It includes HEALTHY and UNHEALTHY, providing a status based on the pipeline's current state.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: PipelineStateInfoHealth\n\n   The health of a pipeline.\n\n   .. py:attribute:: HEALTHY\n      :value: \"HEALTHY\"\n\n   .. py:attribute:: UNHEALTHY\n      :value: \"UNHEALTHY\"\n```\n\n----------------------------------------\n\nTITLE: Updating Provider Analytics Dashboard with Databricks SDK\nDESCRIPTION: Updates a provider analytics dashboard using the `update()` method. The `id` parameter is immutable and cannot be updated. The `version` parameter specifies the version of the dashboard template to update to. The return type is a `UpdateProviderAnalyticsDashboardResponse` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_provider_analytics_dashboards.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nw.provider_provider_analytics_dashboards.update(id: str [, version: Optional[int]]) -> UpdateProviderAnalyticsDashboardResponse\n```\n\n----------------------------------------\n\nTITLE: Marketplace Cost Enum Definition in Python\nDESCRIPTION: Defines the cost options for listings on the Databricks Marketplace. The options are either 'FREE' or 'PAID'. This enum indicates whether a listing requires payment or is available without charge.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: Cost\n\n   .. py:attribute:: FREE\n      :value: \"FREE\"\n\n   .. py:attribute:: PAID\n      :value: \"PAID\"\n```\n\n----------------------------------------\n\nTITLE: Defining ListClustersSortByDirection Enum (Python)\nDESCRIPTION: Defines an enumeration `ListClustersSortByDirection` to specify the sorting direction for listing clusters. It provides two attributes, `ASC` and `DESC`, representing ascending and descending order respectively.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ListClustersSortByDirection\n\n   .. py:attribute:: ASC\n      :value: \"ASC\"\n\n   .. py:attribute:: DESC\n      :value: \"DESC\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Table Constraint in Databricks\nDESCRIPTION: This code snippet represents the method for creating a table constraint. It requires the user to have USE_CATALOG, USE_SCHEMA privileges, and be the owner of the table. If creating a ForeignKeyConstraint, similar privileges are needed on the parent table. The method takes the full table name and the constraint definition as input and returns the created TableConstraint object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/table_constraints.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: create(full_name_arg: str, constraint: TableConstraint) -> TableConstraint\n\n        Create a table constraint.\n\n        Creates a new table constraint.\n\n        For the table constraint creation to succeed, the user must satisfy both of these conditions: - the\n        user must have the **USE_CATALOG** privilege on the table's parent catalog, the **USE_SCHEMA**\n        privilege on the table's parent schema, and be the owner of the table. - if the new constraint is a\n        __ForeignKeyConstraint__, the user must have the **USE_CATALOG** privilege on the referenced parent\n        table's catalog, the **USE_SCHEMA** privilege on the referenced parent table's schema, and be the\n        owner of the referenced parent table.\n\n        :param full_name_arg: str\n          The full name of the table referenced by the constraint.\n        :param constraint: :class:`TableConstraint`\n          A table constraint, as defined by *one* of the following fields being set:\n          __primary_key_constraint__, __foreign_key_constraint__, __named_table_constraint__.\n\n        :returns: :class:`TableConstraint`\n        \n```\n\n----------------------------------------\n\nTITLE: List Job Policy Compliance using Databricks SDK (Python)\nDESCRIPTION: Returns the policy compliance status of all jobs that use a given policy. It supports pagination using page_size and page_token. Jobs could be out of compliance if a cluster policy they use was updated after the job was last edited and its job clusters no longer comply with the updated policy. Returns an iterator over JobCompliance objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/policy_compliance_for_jobs.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: list_compliance(policy_id: str [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[JobCompliance]\n\n    List job policy compliance.\n\n    Returns the policy compliance status of all jobs that use a given policy. Jobs could be out of\n    compliance if a cluster policy they use was updated after the job was last edited and its job clusters\n    no longer comply with the updated policy.\n\n    :param policy_id: str\n      Canonical unique identifier for the cluster policy.\n    :param page_size: int (optional)\n      Use this field to specify the maximum number of results to be returned by the server. The server may\n      further constrain the maximum number of results returned in a single page.\n    :param page_token: str (optional)\n      A page token that can be used to navigate to the next page or previous page as returned by\n      `next_page_token` or `prev_page_token`.\n\n    :returns: Iterator over :class:`JobCompliance`\n```\n\n----------------------------------------\n\nTITLE: Getting Dashboard Schedule\nDESCRIPTION: Retrieves a specific schedule of a Lakeview dashboard using the LakeviewAPI. Requires dashboard ID and schedule ID. It returns the Schedule object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: get_schedule(dashboard_id: str, schedule_id: str) -> Schedule\n\n    Get dashboard schedule.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard to which the schedule belongs.\n    :param schedule_id: str\n      UUID identifying the schedule.\n\n    :returns: :class:`Schedule`\n```\n\n----------------------------------------\n\nTITLE: Defining UpdateJob Class in Python\nDESCRIPTION: Defines the UpdateJob class and its members, excluding documentation for undocumented members. This likely represents the structure used to update an existing Databricks job.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: UpdateJob\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Deleting Schedule Subscription\nDESCRIPTION: Deletes a schedule subscription using the LakeviewAPI. Requires dashboard ID, schedule ID, and subscription ID. An optional etag can be provided for conditional deletion.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: delete_subscription(dashboard_id: str, schedule_id: str, subscription_id: str [, etag: Optional[str]])\n\n    Delete schedule subscription.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard which the subscription belongs.\n    :param schedule_id: str\n      UUID identifying the schedule which the subscription belongs.\n    :param subscription_id: str\n      UUID identifying the subscription.\n    :param etag: str (optional)\n      The etag for the subscription. Can be optionally provided to ensure that the subscription has not\n      been modified since the last read.\n```\n\n----------------------------------------\n\nTITLE: Listing Functions in Databricks\nDESCRIPTION: Lists functions within a specified catalog and schema.  Metastore admins see all functions. Other users require USE_CATALOG and USE_SCHEMA privileges, and will only see functions where they have EXECUTE privilege or are the owner. catalog_name and schema_name are required. Other parameters include include_browse, max_results, and page_token for pagination.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/functions.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list(catalog_name: str, schema_name: str [, include_browse: Optional[bool], max_results: Optional[int], page_token: Optional[str]]) -> Iterator[FunctionInfo]\n\n    List functions.\n\n    List functions within the specified parent catalog and schema. If the user is a metastore admin, all\n    functions are returned in the output list. Otherwise, the user must have the **USE_CATALOG** privilege\n    on the catalog and the **USE_SCHEMA** privilege on the schema, and the output list contains only\n    functions for which either the user has the **EXECUTE** privilege or the user is the owner. There is\n    no guarantee of a specific ordering of the elements in the array.\n\n    :param catalog_name: str\n      Name of parent catalog for functions of interest.\n    :param schema_name: str\n      Parent schema of functions.\n    :param include_browse: bool (optional)\n      Whether to include functions in the response for which the principal can only access selective\n      metadata for\n    :param max_results: int (optional)\n      Maximum number of functions to return. If not set, all the functions are returned (not recommended).\n      - when set to a value greater than 0, the page length is the minimum of this value and a server\n      configured value; - when set to 0, the page length is set to a server configured value\n      (recommended); - when set to a value less than 0, an invalid parameter error is returned;\n    :param page_token: str (optional)\n      Opaque pagination token to go to next page based on previous query.\n\n    :returns: Iterator over :class:`FunctionInfo`\n```\n\n----------------------------------------\n\nTITLE: Listing Model Versions in Databricks with Python\nDESCRIPTION: Lists model versions under a specific schema or all model versions in the current metastore, subject to the calling user's privileges.  The returned models are filtered based on the privileges of the calling user.  Pagination is supported via `max_results` and `page_token`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/model_versions.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nw.model_versions.list(full_name: str [, include_browse: Optional[bool], max_results: Optional[int], page_token: Optional[str]]) -> Iterator[ModelVersionInfo]\n```\n\n----------------------------------------\n\nTITLE: Defining DateRangeValueDynamicDateRange Enum (Python)\nDESCRIPTION: Defines an enumeration representing dynamic date ranges. Options include LAST_12_MONTHS, LAST_14_DAYS, LAST_24_HOURS, LAST_30_DAYS, LAST_60_DAYS, LAST_7_DAYS, LAST_8_HOURS, LAST_90_DAYS, LAST_HOUR, LAST_MONTH, LAST_WEEK, LAST_YEAR, THIS_MONTH, THIS_WEEK, THIS_YEAR, TODAY, and YESTERDAY. These are used for defining time-based filters or parameters in SQL queries or alerts.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: DateRangeValueDynamicDateRange\n\n   .. py:attribute:: LAST_12_MONTHS\n      :value: \"LAST_12_MONTHS\"\n\n   .. py:attribute:: LAST_14_DAYS\n      :value: \"LAST_14_DAYS\"\n\n   .. py:attribute:: LAST_24_HOURS\n      :value: \"LAST_24_HOURS\"\n\n   .. py:attribute:: LAST_30_DAYS\n      :value: \"LAST_30_DAYS\"\n\n   .. py:attribute:: LAST_60_DAYS\n      :value: \"LAST_60_DAYS\"\n\n   .. py:attribute:: LAST_7_DAYS\n      :value: \"LAST_7_DAYS\"\n\n   .. py:attribute:: LAST_8_HOURS\n      :value: \"LAST_8_HOURS\"\n\n   .. py:attribute:: LAST_90_DAYS\n      :value: \"LAST_90_DAYS\"\n\n   .. py:attribute:: LAST_HOUR\n      :value: \"LAST_HOUR\"\n\n   .. py:attribute:: LAST_MONTH\n      :value: \"LAST_MONTH\"\n\n   .. py:attribute:: LAST_WEEK\n      :value: \"LAST_WEEK\"\n\n   .. py:attribute:: LAST_YEAR\n      :value: \"LAST_YEAR\"\n\n   .. py:attribute:: THIS_MONTH\n      :value: \"THIS_MONTH\"\n\n   .. py:attribute:: THIS_WEEK\n      :value: \"THIS_WEEK\"\n\n   .. py:attribute:: THIS_YEAR\n      :value: \"THIS_YEAR\"\n\n   .. py:attribute:: TODAY\n      :value: \"TODAY\"\n\n   .. py:attribute:: YESTERDAY\n      :value: \"YESTERDAY\"\n```\n\n----------------------------------------\n\nTITLE: Get Account IP Access Toggle Setting (Python)\nDESCRIPTION: Retrieves the value of the account IP access toggle setting. Accepts an optional etag for versioning and optimistic concurrency control.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/enable_ip_access_lists.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\na.settings.enable_ip_access_lists.get(etag: Optional[str]) -> AccountIpAccessEnable\n```\n\n----------------------------------------\n\nTITLE: Defining USER attribute for WorkspacePermission in Python\nDESCRIPTION: Defines the USER attribute for the WorkspacePermission class. It indicates the user has standard user permissions.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: WorkspacePermission\n\n   .. py:attribute:: USER\n      :value: \"USER\"\n```\n\n----------------------------------------\n\nTITLE: Defining URN for ListResponseSchema in Python\nDESCRIPTION: Defines the URN_IETF_PARAMS_SCIM_API_MESSAGES_2_0_LIST_RESPONSE attribute for the ListResponseSchema class. It represents the SCIM schema for a list response with a URN string value.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ListResponseSchema\n\n   .. py:attribute:: URN_IETF_PARAMS_SCIM_API_MESSAGES_2_0_LIST_RESPONSE\n      :value: \"URN_IETF_PARAMS_SCIM_API_MESSAGES_2_0_LIST_RESPONSE\"\n```\n\n----------------------------------------\n\nTITLE: UpdateStateInfoState Enum Definition in Python\nDESCRIPTION: Defines an enumeration for the state of an update, mirroring the UpdateInfoState enum. It provides possible states such as CANCELED, COMPLETED, CREATED, FAILED, INITIALIZING, RUNNING, and WAITING_FOR_RESOURCES, used to track the status of the update process.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: UpdateStateInfoState\n\n   .. py:attribute:: CANCELED\n      :value: \"CANCELED\"\n\n   .. py:attribute:: COMPLETED\n      :value: \"COMPLETED\"\n\n   .. py:attribute:: CREATED\n      :value: \"CREATED\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: INITIALIZING\n      :value: \"INITIALIZING\"\n\n   .. py:attribute:: QUEUED\n      :value: \"QUEUED\"\n\n   .. py:attribute:: RESETTING\n      :value: \"RESETTING\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n\n   .. py:attribute:: SETTING_UP_TABLES\n      :value: \"SETTING_UP_TABLES\"\n\n   .. py:attribute:: STOPPING\n      :value: \"STOPPING\"\n\n   .. py:attribute:: WAITING_FOR_RESOURCES\n      :value: \"WAITING_FOR_RESOURCES\"\n```\n\n----------------------------------------\n\nTITLE: Defining ObjectType Enum\nDESCRIPTION: Defines the types of objects that can exist in a Databricks workspace. These include DASHBOARD, DIRECTORY, FILE, LIBRARY, NOTEBOOK, and REPO. This enum is used for specifying object types in workspace operations.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/workspace.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ObjectType\n\n   The type of the object in workspace.\n\n   .. py:attribute:: DASHBOARD\n      :value: \"DASHBOARD\"\n\n   .. py:attribute:: DIRECTORY\n      :value: \"DIRECTORY\"\n\n   .. py:attribute:: FILE\n      :value: \"FILE\"\n\n   .. py:attribute:: LIBRARY\n      :value: \"LIBRARY\"\n\n   .. py:attribute:: NOTEBOOK\n      :value: \"NOTEBOOK\"\n\n   .. py:attribute:: REPO\n      :value: \"REPO\"\n```\n\n----------------------------------------\n\nTITLE: Defining ServedModelInputWorkloadType Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `ServedModelInputWorkloadType` with possible values CPU, GPU_LARGE, GPU_MEDIUM, GPU_SMALL and MULTIGPU_MEDIUM. It indicates the type of workload for a served model.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ServedModelInputWorkloadType\n\n   Please keep this in sync with with workload types in InferenceEndpointEntities.scala\n\n   .. py:attribute:: CPU\n      :value: \"CPU\"\n\n   .. py:attribute:: GPU_LARGE\n      :value: \"GPU_LARGE\"\n\n   .. py:attribute:: GPU_MEDIUM\n      :value: \"GPU_MEDIUM\"\n\n   .. py:attribute:: GPU_SMALL\n      :value: \"GPU_SMALL\"\n\n   .. py:attribute:: MULTIGPU_MEDIUM\n      :value: \"MULTIGPU_MEDIUM\"\n```\n\n----------------------------------------\n\nTITLE: Generate Temporary Table Credentials in Python\nDESCRIPTION: This method generates temporary credentials for accessing table data on cloud storage. It requires the metastore to have the `external_access_enabled` flag set to true, and the caller must have the `EXTERNAL_USE_SCHEMA` privilege on the parent schema. The generated credentials can be configured for read or read-write access based on the `operation` parameter. The function returns a `GenerateTemporaryTableCredentialResponse` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/temporary_table_credentials.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: generate_temporary_table_credentials( [, operation: Optional[TableOperation], table_id: Optional[str]]) -> GenerateTemporaryTableCredentialResponse\n\n    Generate a temporary table credential.\n\n    Get a short-lived credential for directly accessing the table data on cloud storage. The metastore\n    must have external_access_enabled flag set to true (default false). The caller must have\n    EXTERNAL_USE_SCHEMA privilege on the parent schema and this privilege can only be granted by catalog\n    owners.\n\n    :param operation: :class:`TableOperation` (optional)\n      The operation performed against the table data, either READ or READ_WRITE. If READ_WRITE is\n      specified, the credentials returned will have write permissions, otherwise, it will be read only.\n    :param table_id: str (optional)\n      UUID of the table to read or write.\n\n    :returns: :class:`GenerateTemporaryTableCredentialResponse`\n```\n\n----------------------------------------\n\nTITLE: Deleting an Account IP Access List in Python\nDESCRIPTION: Deletes an IP access list, specified by its list ID. The operation requires the `ip_access_list_id` as input, which identifies the list to be deleted.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/ip_access_lists.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(ip_access_list_id: str)\n\n    Delete access list.\n\n    Deletes an IP access list, specified by its list ID.\n\n    :param ip_access_list_id: str\n      The ID for the corresponding IP access list\n```\n\n----------------------------------------\n\nTITLE: Update Enable Results Downloading Setting - Python\nDESCRIPTION: This method updates the 'Enable Results Downloading' setting in Databricks. It requires the `allow_missing` parameter (which should always be true for Settings API), the `setting` object containing the desired state, and a `field_mask` specifying which fields to update. The model follows eventual consistency.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/enable_results_downloading.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: patch_enable_results_downloading(allow_missing: bool, setting: EnableResultsDownloading, field_mask: str) -> EnableResultsDownloading\n\n    Update the Enable Results Downloading setting.\n\n    Updates the Enable Results Downloading setting. The model follows eventual consistency, which means\n    the get after the update operation might receive stale values for some time.\n\n    :param allow_missing: bool\n      This should always be set to true for Settings API. Added for AIP compliance.\n    :param setting: :class:`EnableResultsDownloading`\n    :param field_mask: str\n      The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n      field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n      `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n      the entire collection field can be specified. Field names must exactly match the resource field\n      names.\n\n      A field mask of `*` indicates full replacement. It’s recommended to always explicitly list the\n      fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n      changes in the future.\n\n    :returns: :class:`EnableResultsDownloading`\n```\n\n----------------------------------------\n\nTITLE: Defining TerminationTypeType Enum in Python\nDESCRIPTION: Defines the `TerminationTypeType` enumeration, representing general categories for why a Databricks run was terminated. It includes values for `SUCCESS`, `INTERNAL_ERROR`, `CLIENT_ERROR`, and `CLOUD_FAILURE`. These high-level categories help classify termination causes.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n.. py:attribute:: CLIENT_ERROR\n      :value: \"CLIENT_ERROR\"\n\n   .. py:attribute:: CLOUD_FAILURE\n      :value: \"CLOUD_FAILURE\"\n\n   .. py:attribute:: INTERNAL_ERROR\n      :value: \"INTERNAL_ERROR\"\n\n   .. py:attribute:: SUCCESS\n      :value: \"SUCCESS\"\n```\n\n----------------------------------------\n\nTITLE: Getting an App Environment in Databricks\nDESCRIPTION: Retrieves the app environment and returns the AppEnvironment object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nw.apps.get_environment(name: str) -> AppEnvironment\n```\n\n----------------------------------------\n\nTITLE: Defining Cluster Termination Reason Codes in Python\nDESCRIPTION: This snippet defines various termination reason codes for Databricks clusters. These codes provide detailed explanations for why a cluster was terminated, ranging from user requests and cloud provider failures to internal errors and resource limitations. Each code is represented as a string constant.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n   .. py:attribute:: ABUSE_DETECTED\n      :value: \"ABUSE_DETECTED\"\n\n   .. py:attribute:: ATTACH_PROJECT_FAILURE\n      :value: \"ATTACH_PROJECT_FAILURE\"\n\n   .. py:attribute:: AWS_AUTHORIZATION_FAILURE\n      :value: \"AWS_AUTHORIZATION_FAILURE\"\n\n   .. py:attribute:: AWS_INSUFFICIENT_FREE_ADDRESSES_IN_SUBNET_FAILURE\n      :value: \"AWS_INSUFFICIENT_FREE_ADDRESSES_IN_SUBNET_FAILURE\"\n\n   .. py:attribute:: AWS_INSUFFICIENT_INSTANCE_CAPACITY_FAILURE\n      :value: \"AWS_INSUFFICIENT_INSTANCE_CAPACITY_FAILURE\"\n\n   .. py:attribute:: AWS_MAX_SPOT_INSTANCE_COUNT_EXCEEDED_FAILURE\n      :value: \"AWS_MAX_SPOT_INSTANCE_COUNT_EXCEEDED_FAILURE\"\n\n   .. py:attribute:: AWS_REQUEST_LIMIT_EXCEEDED\n      :value: \"AWS_REQUEST_LIMIT_EXCEEDED\"\n\n   .. py:attribute:: AWS_UNSUPPORTED_FAILURE\n      :value: \"AWS_UNSUPPORTED_FAILURE\"\n\n   .. py:attribute:: AZURE_BYOK_KEY_PERMISSION_FAILURE\n      :value: \"AZURE_BYOK_KEY_PERMISSION_FAILURE\"\n\n   .. py:attribute:: AZURE_EPHEMERAL_DISK_FAILURE\n      :value: \"AZURE_EPHEMERAL_DISK_FAILURE\"\n\n   .. py:attribute:: AZURE_INVALID_DEPLOYMENT_TEMPLATE\n      :value: \"AZURE_INVALID_DEPLOYMENT_TEMPLATE\"\n\n   .. py:attribute:: AZURE_OPERATION_NOT_ALLOWED_EXCEPTION\n      :value: \"AZURE_OPERATION_NOT_ALLOWED_EXCEPTION\"\n\n   .. py:attribute:: AZURE_QUOTA_EXCEEDED_EXCEPTION\n      :value: \"AZURE_QUOTA_EXCEEDED_EXCEPTION\"\n\n   .. py:attribute:: AZURE_RESOURCE_MANAGER_THROTTLING\n      :value: \"AZURE_RESOURCE_MANAGER_THROTTLING\"\n\n   .. py:attribute:: AZURE_RESOURCE_PROVIDER_THROTTLING\n      :value: \"AZURE_RESOURCE_PROVIDER_THROTTLING\"\n\n   .. py:attribute:: AZURE_UNEXPECTED_DEPLOYMENT_TEMPLATE_FAILURE\n      :value: \"AZURE_UNEXPECTED_DEPLOYMENT_TEMPLATE_FAILURE\"\n\n   .. py:attribute:: AZURE_VM_EXTENSION_FAILURE\n      :value: \"AZURE_VM_EXTENSION_FAILURE\"\n\n   .. py:attribute:: AZURE_VNET_CONFIGURATION_FAILURE\n      :value: \"AZURE_VNET_CONFIGURATION_FAILURE\"\n\n   .. py:attribute:: BOOTSTRAP_TIMEOUT\n      :value: \"BOOTSTRAP_TIMEOUT\"\n\n   .. py:attribute:: BOOTSTRAP_TIMEOUT_CLOUD_PROVIDER_EXCEPTION\n      :value: \"BOOTSTRAP_TIMEOUT_CLOUD_PROVIDER_EXCEPTION\"\n\n   .. py:attribute:: CLOUD_PROVIDER_DISK_SETUP_FAILURE\n      :value: \"CLOUD_PROVIDER_DISK_SETUP_FAILURE\"\n\n   .. py:attribute:: CLOUD_PROVIDER_LAUNCH_FAILURE\n      :value: \"CLOUD_PROVIDER_LAUNCH_FAILURE\"\n\n   .. py:attribute:: CLOUD_PROVIDER_RESOURCE_STOCKOUT\n      :value: \"CLOUD_PROVIDER_RESOURCE_STOCKOUT\"\n\n   .. py:attribute:: CLOUD_PROVIDER_SHUTDOWN\n      :value: \"CLOUD_PROVIDER_SHUTDOWN\"\n\n   .. py:attribute:: COMMUNICATION_LOST\n      :value: \"COMMUNICATION_LOST\"\n\n   .. py:attribute:: CONTAINER_LAUNCH_FAILURE\n      :value: \"CONTAINER_LAUNCH_FAILURE\"\n\n   .. py:attribute:: CONTROL_PLANE_REQUEST_FAILURE\n      :value: \"CONTROL_PLANE_REQUEST_FAILURE\"\n\n   .. py:attribute:: DATABASE_CONNECTION_FAILURE\n      :value: \"DATABASE_CONNECTION_FAILURE\"\n\n   .. py:attribute:: DBFS_COMPONENT_UNHEALTHY\n      :value: \"DBFS_COMPONENT_UNHEALTHY\"\n\n   .. py:attribute:: DOCKER_IMAGE_PULL_FAILURE\n      :value: \"DOCKER_IMAGE_PULL_FAILURE\"\n\n   .. py:attribute:: DRIVER_UNREACHABLE\n      :value: \"DRIVER_UNREACHABLE\"\n\n   .. py:attribute:: DRIVER_UNRESPONSIVE\n      :value: \"DRIVER_UNRESPONSIVE\"\n\n   .. py:attribute:: EXECUTION_COMPONENT_UNHEALTHY\n      :value: \"EXECUTION_COMPONENT_UNHEALTHY\"\n\n   .. py:attribute:: GCP_QUOTA_EXCEEDED\n      :value: \"GCP_QUOTA_EXCEEDED\"\n\n   .. py:attribute:: GCP_SERVICE_ACCOUNT_DELETED\n      :value: \"GCP_SERVICE_ACCOUNT_DELETED\"\n\n   .. py:attribute:: GLOBAL_INIT_SCRIPT_FAILURE\n      :value: \"GLOBAL_INIT_SCRIPT_FAILURE\"\n\n   .. py:attribute:: HIVE_METASTORE_PROVISIONING_FAILURE\n      :value: \"HIVE_METASTORE_PROVISIONING_FAILURE\"\n\n   .. py:attribute:: IMAGE_PULL_PERMISSION_DENIED\n      :value: \"IMAGE_PULL_PERMISSION_DENIED\"\n\n   .. py:attribute:: INACTIVITY\n      :value: \"INACTIVITY\"\n\n   .. py:attribute:: INIT_SCRIPT_FAILURE\n      :value: \"INIT_SCRIPT_FAILURE\"\n\n   .. py:attribute:: INSTANCE_POOL_CLUSTER_FAILURE\n      :value: \"INSTANCE_POOL_CLUSTER_FAILURE\"\n\n   .. py:attribute:: INSTANCE_UNREACHABLE\n      :value: \"INSTANCE_UNREACHABLE\"\n\n   .. py:attribute:: INTERNAL_ERROR\n      :value: \"INTERNAL_ERROR\"\n\n   .. py:attribute:: INVALID_ARGUMENT\n      :value: \"INVALID_ARGUMENT\"\n\n   .. py:attribute:: INVALID_SPARK_IMAGE\n      :value: \"INVALID_SPARK_IMAGE\"\n\n   .. py:attribute:: IP_EXHAUSTION_FAILURE\n      :value: \"IP_EXHAUSTION_FAILURE\"\n\n   .. py:attribute:: JOB_FINISHED\n      :value: \"JOB_FINISHED\"\n\n   .. py:attribute:: K8S_AUTOSCALING_FAILURE\n      :value: \"K8S_AUTOSCALING_FAILURE\"\n\n   .. py:attribute:: K8S_DBR_CLUSTER_LAUNCH_TIMEOUT\n      :value: \"K8S_DBR_CLUSTER_LAUNCH_TIMEOUT\"\n\n   .. py:attribute:: METASTORE_COMPONENT_UNHEALTHY\n      :value: \"METASTORE_COMPONENT_UNHEALTHY\"\n\n   .. py:attribute:: NEPHOS_RESOURCE_MANAGEMENT\n      :value: \"NEPHOS_RESOURCE_MANAGEMENT\"\n\n   .. py:attribute:: NETWORK_CONFIGURATION_FAILURE\n      :value: \"NETWORK_CONFIGURATION_FAILURE\"\n\n   .. py:attribute:: NFS_MOUNT_FAILURE\n      :value: \"NFS_MOUNT_FAILURE\"\n\n   .. py:attribute:: NPIP_TUNNEL_SETUP_FAILURE\n      :value: \"NPIP_TUNNEL_SETUP_FAILURE\"\n\n   .. py:attribute:: NPIP_TUNNEL_TOKEN_FAILURE\n      :value: \"NPIP_TUNNEL_TOKEN_FAILURE\"\n\n   .. py:attribute:: REQUEST_REJECTED\n      :value: \"REQUEST_REJECTED\"\n\n   .. py:attribute:: REQUEST_THROTTLED\n      :value: \"REQUEST_THROTTLED\"\n\n   .. py:attribute:: SECRET_RESOLUTION_ERROR\n      :value: \"SECRET_RESOLUTION_ERROR\"\n\n   .. py:attribute:: SECURITY_DAEMON_REGISTRATION_EXCEPTION\n      :value: \"SECURITY_DAEMON_REGISTRATION_EXCEPTION\"\n\n   .. py:attribute:: SELF_BOOTSTRAP_FAILURE\n      :value: \"SELF_BOOTSTRAP_FAILURE\"\n\n   .. py:attribute:: SKIPPED_SLOW_NODES\n      :value: \"SKIPPED_SLOW_NODES\"\n\n   .. py:attribute:: SLOW_IMAGE_DOWNLOAD\n      :value: \"SLOW_IMAGE_DOWNLOAD\"\n\n   .. py:attribute:: SPARK_ERROR\n      :value: \"SPARK_ERROR\"\n\n   .. py:attribute:: SPARK_IMAGE_DOWNLOAD_FAILURE\n      :value: \"SPARK_IMAGE_DOWNLOAD_FAILURE\"\n\n   .. py:attribute:: SPARK_STARTUP_FAILURE\n      :value: \"SPARK_STARTUP_FAILURE\"\n\n   .. py:attribute:: SPOT_INSTANCE_TERMINATION\n      :value: \"SPOT_INSTANCE_TERMINATION\"\n\n   .. py:attribute:: STORAGE_DOWNLOAD_FAILURE\n      :value: \"STORAGE_DOWNLOAD_FAILURE\"\n\n   .. py:attribute:: STS_CLIENT_SETUP_FAILURE\n      :value: \"STS_CLIENT_SETUP_FAILURE\"\n\n   .. py:attribute:: SUBNET_EXHAUSTED_FAILURE\n      :value: \"SUBNET_EXHAUSTED_FAILURE\"\n\n   .. py:attribute:: TEMPORARILY_UNAVAILABLE\n      :value: \"TEMPORARILY_UNAVAILABLE\"\n\n   .. py:attribute:: TRIAL_EXPIRED\n      :value: \"TRIAL_EXPIRED\"\n\n   .. py:attribute:: UNEXPECTED_LAUNCH_FAILURE\n      :value: \"UNEXPECTED_LAUNCH_FAILURE\"\n\n   .. py:attribute:: UNKNOWN\n      :value: \"UNKNOWN\"\n\n   .. py:attribute:: UNSUPPORTED_INSTANCE_TYPE\n      :value: \"UNSUPPORTED_INSTANCE_TYPE\"\n\n   .. py:attribute:: UPDATE_INSTANCE_PROFILE_FAILURE\n      :value: \"UPDATE_INSTANCE_PROFILE_FAILURE\"\n\n   .. py:attribute:: USER_REQUEST\n      :value: \"USER_REQUEST\"\n\n   .. py:attribute:: WORKER_SETUP_FAILURE\n      :value: \"WORKER_SETUP_FAILURE\"\n\n   .. py:attribute:: WORKSPACE_CANCELLED_ERROR\n      :value: \"WORKSPACE_CANCELLED_ERROR\"\n\n   .. py:attribute:: WORKSPACE_CONFIGURATION_ERROR\n      :value: \"WORKSPACE_CONFIGURATION_ERROR\"\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_READ attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_READ attribute for the PermissionLevel class. It represents a permission level that allows reading resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_READ\n      :value: \"CAN_READ\"\n```\n\n----------------------------------------\n\nTITLE: Regenerating a Dashboard - Databricks SDK (Python)\nDESCRIPTION: This method regenerates the monitoring dashboard for a specified table.  The caller needs specific permissions, including ownership or USE privileges on the catalog and schema.  The call must be made from the workspace where the monitor was originally created, and the dashboard will be regenerated in the assets directory specified during monitor creation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/quality_monitors.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: regenerate_dashboard(table_name: str [, warehouse_id: Optional[str]]) -> RegenerateDashboardResponse\n\n        Regenerate a monitoring dashboard.\n\n        Regenerates the monitoring dashboard for the specified table.\n\n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n        - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent schema - be an\n        owner of the table\n\n        The call must be made from the workspace where the monitor was created. The dashboard will be\n        regenerated in the assets directory that was specified when the monitor was created.\n\n        :param table_name: str\n          Full name of the table.\n        :param warehouse_id: str (optional)\n          Optional argument to specify the warehouse for dashboard regeneration. If not specified, the first\n          running warehouse will be used.\n\n        :returns: :class:`RegenerateDashboardResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining ExecuteStatementRequestOnWaitTimeout Enum (Python)\nDESCRIPTION: Defines an enumeration representing the behavior when `wait_timeout` is exceeded during statement execution. The options are CANCEL and CONTINUE, indicating whether the execution should be canceled or continue asynchronously.  If CONTINUE is selected, a statement ID is returned which can be polled.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ExecuteStatementRequestOnWaitTimeout\n\n   When `wait_timeout > 0s`, the call will block up to the specified time. If the statement execution doesn't finish within this time, `on_wait_timeout` determines whether the execution should continue or be canceled. When set to `CONTINUE`, the statement execution continues asynchronously and the call returns a statement ID which can be used for polling with :method:statementexecution/getStatement. When set to `CANCEL`, the statement execution is canceled and the call returns with a `CANCELED` state.\n\n   .. py:attribute:: CANCEL\n      :value: \"CANCEL\"\n\n   .. py:attribute:: CONTINUE\n      :value: \"CONTINUE\"\n```\n\n----------------------------------------\n\nTITLE: Defining DESCENDING attribute for ListSortOrder in Python\nDESCRIPTION: Defines the DESCENDING attribute for the ListSortOrder class. It represents the descending sort order with a string value.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ListSortOrder\n\n   .. py:attribute:: DESCENDING\n      :value: \"DESCENDING\"\n```\n\n----------------------------------------\n\nTITLE: Listing Clean Room Assets with Python\nDESCRIPTION: Lists all assets within a specified clean room, allowing for pagination through the results using an optional page token.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/cleanrooms/clean_room_assets.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list(clean_room_name: str [, page_token: Optional[str]]) -> Iterator[CleanRoomAsset]\n\n        List assets.\n\n        :param clean_room_name: str\n          Name of the clean room.\n        :param page_token: str (optional)\n          Opaque pagination token to go to next page based on previous query.\n\n        :returns: Iterator over :class:`CleanRoomAsset`\n```\n\n----------------------------------------\n\nTITLE: Defining ColumnInfoTypeName Enum (Python)\nDESCRIPTION: Defines an enumeration for the base data types of columns in Databricks SQL. It includes types like ARRAY, BINARY, BOOLEAN, BYTE, CHAR, DATE, DECIMAL, DOUBLE, FLOAT, INT, INTERVAL, LONG, MAP, NULL, SHORT, STRING, STRUCT, TIMESTAMP and USER_DEFINED_TYPE.  This is used when describing the schema of query results.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ColumnInfoTypeName\n\n   The name of the base data type. This doesn't include details for complex types such as STRUCT, MAP or ARRAY.\n\n   .. py:attribute:: ARRAY\n      :value: \"ARRAY\"\n\n   .. py:attribute:: BINARY\n      :value: \"BINARY\"\n\n   .. py:attribute:: BOOLEAN\n      :value: \"BOOLEAN\"\n\n   .. py:attribute:: BYTE\n      :value: \"BYTE\"\n\n   .. py:attribute:: CHAR\n      :value: \"CHAR\"\n\n   .. py:attribute:: DATE\n      :value: \"DATE\"\n\n   .. py:attribute:: DECIMAL\n      :value: \"DECIMAL\"\n\n   .. py:attribute:: DOUBLE\n      :value: \"DOUBLE\"\n\n   .. py:attribute:: FLOAT\n      :value: \"FLOAT\"\n\n   .. py:attribute:: INT\n      :value: \"INT\"\n\n   .. py:attribute:: INTERVAL\n      :value: \"INTERVAL\"\n\n   .. py:attribute:: LONG\n      :value: \"LONG\"\n\n   .. py:attribute:: MAP\n      :value: \"MAP\"\n\n   .. py:attribute:: NULL\n      :value: \"NULL\"\n\n   .. py:attribute:: SHORT\n      :value: \"SHORT\"\n\n   .. py:attribute:: STRING\n      :value: \"STRING\"\n\n   .. py:attribute:: STRUCT\n      :value: \"STRUCT\"\n\n   .. py:attribute:: TIMESTAMP\n      :value: \"TIMESTAMP\"\n\n   .. py:attribute:: USER_DEFINED_TYPE\n      :value: \"USER_DEFINED_TYPE\"\n```\n\n----------------------------------------\n\nTITLE: Defining AzureAvailability Enum in Python\nDESCRIPTION: This code defines an enumeration (`AzureAvailability`) for specifying the availability type of Azure nodes in a Databricks cluster. It offers options for ON_DEMAND_AZURE, SPOT_AZURE, and SPOT_WITH_FALLBACK_AZURE instances.  The availability type is used for subsequent nodes past the `first_on_demand` nodes. If `first_on_demand` is zero, this availability type will be used for the entire cluster.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: AzureAvailability\n\n   Availability type used for all subsequent nodes past the `first_on_demand` ones. Note: If `first_on_demand` is zero, this availability type will be used for the entire cluster.\n\n   .. py:attribute:: ON_DEMAND_AZURE\n      :value: \"ON_DEMAND_AZURE\"\n\n   .. py:attribute:: SPOT_AZURE\n      :value: \"SPOT_AZURE\"\n\n   .. py:attribute:: SPOT_WITH_FALLBACK_AZURE\n      :value: \"SPOT_WITH_FALLBACK_AZURE\"\n```\n\n----------------------------------------\n\nTITLE: Getting an App in Databricks using Python\nDESCRIPTION: This code snippet demonstrates how to retrieve information about an app using the `get` method.  It requires the name of the app as a string. The method returns an `App` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(name: str) -> App\n\n        Get an app.\n\n        Retrieves information for the app with the supplied name.\n\n        :param name: str\n          The name of the app.\n\n        :returns: :class:`App`\n```\n\n----------------------------------------\n\nTITLE: ListingShareType Enum Definition in Python\nDESCRIPTION: Defines the types of sharing available for listings on the Databricks Marketplace. The options are 'FULL' and 'SAMPLE', which indicate whether the entire listing or a sample of it is shared.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ListingShareType\n\n   .. py:attribute:: FULL\n      :value: \"FULL\"\n\n   .. py:attribute:: SAMPLE\n      :value: \"SAMPLE\"\n```\n\n----------------------------------------\n\nTITLE: ListingStatus Enum Definition in Python\nDESCRIPTION: Defines the possible statuses for a listing on the Databricks Marketplace. The possible values include 'DRAFT', 'PENDING', 'PUBLISHED', and 'SUSPENDED', indicating the stage of the listing in the publication process.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ListingStatus\n\n   Enums\n\n   .. py:attribute:: DRAFT\n      :value: \"DRAFT\"\n\n   .. py:attribute:: PENDING\n      :value: \"PENDING\"\n\n   .. py:attribute:: PUBLISHED\n      :value: \"PUBLISHED\"\n\n   .. py:attribute:: SUSPENDED\n      :value: \"SUSPENDED\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Default Namespace Setting - Python\nDESCRIPTION: Deletes the default namespace setting for the workspace.  A fresh etag needs to be provided. The etag can be retrieved by making a GET request before the DELETE request. If the setting is updated/deleted concurrently, DELETE fails with 409 and the request must be retried by using the fresh etag in the 409 response.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/default_namespace.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n    .. py:method:: delete( [, etag: Optional[str]]) -> DeleteDefaultNamespaceSettingResponse\n\n        Delete the default namespace setting.\n\n        Deletes the default namespace setting for the workspace. A fresh etag needs to be provided in `DELETE`\n        requests (as a query parameter). The etag can be retrieved by making a `GET` request before the\n        `DELETE` request. If the setting is updated/deleted concurrently, `DELETE` fails with 409 and the\n        request must be retried by using the fresh etag in the 409 response.\n\n        :param etag: str (optional)\n          etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n          optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n          each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n          to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n          request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n        :returns: :class:`DeleteDefaultNamespaceSettingResponse`\n```\n\n----------------------------------------\n\nTITLE: Creating a Comment in Model Registry with Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to create a comment on a specific model version using the Databricks SDK. It first creates a model and a model version, then posts a comment to that version. Finally, it cleans up by deleting the created comment.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/model_registry.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nmodel = w.model_registry.create_model(name=f\"sdk-{time.time_ns()}\")\n\nmv = w.model_registry.create_model_version(name=model.registered_model.name, source=\"dbfs:/tmp\")\n\ncreated = w.model_registry.create_comment(\n    comment=f\"sdk-{time.time_ns()}\",\n    name=mv.model_version.name,\n    version=mv.model_version.version,\n)\n\n# cleanup\nw.model_registry.delete_comment(id=created.comment.id)\n```\n\n----------------------------------------\n\nTITLE: Defining UpdateResponse Class in Python\nDESCRIPTION: Defines the UpdateResponse class and its members, excluding documentation for undocumented members. This likely represents the response structure received after updating a Databricks job.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: UpdateResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining UNKNOWN attribute for WorkspacePermission in Python\nDESCRIPTION: Defines the UNKNOWN attribute for the WorkspacePermission class. It indicates an unknown permission level.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: WorkspacePermission\n\n   .. py:attribute:: UNKNOWN\n      :value: \"UNKNOWN\"\n```\n\n----------------------------------------\n\nTITLE: Updating Webhook Example in Model Registry - Python\nDESCRIPTION: This code snippet demonstrates how to update a webhook in the Databricks Model Registry using the Databricks SDK for Python. It initializes a WorkspaceClient, creates a webhook, and then updates its description using the 'update_webhook' method, followed by cleaning up by deleting the webhook.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/model_registry.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import ml\n\nw = WorkspaceClient()\n\ncreated = w.model_registry.create_webhook(\n    description=f\"sdk-{time.time_ns()}\",\n    events=[ml.RegistryWebhookEvent.MODEL_VERSION_CREATED],\n    http_url_spec=ml.HttpUrlSpec(url=w.config.host),\n)\n\nw.model_registry.update_webhook(id=created.webhook.id, description=f\"sdk-{time.time_ns()}\")\n\n# cleanup\nw.model_registry.delete_webhook(id=created.webhook.id)\n```\n\n----------------------------------------\n\nTITLE: Defining DatePrecision Enum (Python)\nDESCRIPTION: Defines an enumeration representing the precision of a date value. Options include DAY_PRECISION, MINUTE_PRECISION, and SECOND_PRECISION.  This allows specifying how precise date/time values should be treated.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: DatePrecision\n\n   .. py:attribute:: DAY_PRECISION\n      :value: \"DAY_PRECISION\"\n\n   .. py:attribute:: MINUTE_PRECISION\n      :value: \"MINUTE_PRECISION\"\n\n   .. py:attribute:: SECOND_PRECISION\n      :value: \"SECOND_PRECISION\"\n```\n\n----------------------------------------\n\nTITLE: Get Token Information by ID with Databricks SDK in Python\nDESCRIPTION: This snippet retrieves token information by its ID using the Databricks SDK. It starts by creating a service principal and an OBO token. Then, it retrieves token information using the token's ID. Finally, it cleans up by deleting the service principal and the token.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/token_management.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import iam\n\nw = WorkspaceClient()\n\ngroups = w.groups.group_display_name_to_id_map(iam.ListGroupsRequest())\n\nspn = w.service_principals.create(\n    display_name=f\"sdk-{time.time_ns()}\",\n    groups=[iam.ComplexValue(value=groups[\"admins\"])],\n)\n\nobo = w.token_management.create_obo_token(application_id=spn.application_id, lifetime_seconds=60)\n\nby_id = w.token_management.get(token_id=obo.token_info.token_id)\n\n# cleanup\nw.service_principals.delete(id=spn.id)\nw.token_management.delete(token_id=obo.token_info.token_id)\n```\n\n----------------------------------------\n\nTITLE: Defining CreateFunctionParameterStyle Enum in Python\nDESCRIPTION: This code defines an enumeration `CreateFunctionParameterStyle` with a single member, `S`. This enumeration indicates the parameter style for the function, and 'S' likely refers to SQL parameter style.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CreateFunctionParameterStyle\n\n   Function parameter style. **S** is the value for SQL.\n\n   .. py:attribute:: S\n      :value: \"S\"\n```\n\n----------------------------------------\n\nTITLE: Defining AppResourceSecretSecretPermission Enum (Python)\nDESCRIPTION: This Python code defines an enumeration `AppResourceSecretSecretPermission` with values representing the permission level that can be granted on a Secret resource associated with an App, such as `MANAGE`, `READ`, and `WRITE`. These permissions control access and actions allowed on the secret.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/apps.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: AppResourceSecretSecretPermission\n\n   Permission to grant on the secret scope. Supported permissions are: \"READ\", \"WRITE\", \"MANAGE\".\n\n   .. py:attribute:: MANAGE\n      :value: \"MANAGE\"\n\n   .. py:attribute:: READ\n      :value: \"READ\"\n\n   .. py:attribute:: WRITE\n      :value: \"WRITE\"\n```\n\n----------------------------------------\n\nTITLE: Defining WarehouseTypePairWarehouseType in Python\nDESCRIPTION: This snippet defines the `WarehouseTypePairWarehouseType` class with its attributes representing different warehouse types. The attributes include `CLASSIC`, `PRO`, and `TYPE_UNSPECIFIED`. It specifies whether the warehouse is of type PRO or CLASSIC.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nclass WarehouseTypePairWarehouseType:\n   \"\"\"Warehouse type: `PRO` or `CLASSIC`.\"\"\"\n\n   CLASSIC = \"CLASSIC\"\n   PRO = \"PRO\"\n   TYPE_UNSPECIFIED = \"TYPE_UNSPECIFIED\"\n```\n\n----------------------------------------\n\nTITLE: Defining Instance Pool Permission Levels in Python\nDESCRIPTION: This code defines the permission levels for an instance pool. It includes CAN_ATTACH_TO and CAN_MANAGE as possible permission levels.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: InstancePoolPermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_ATTACH_TO\n      :value: \"CAN_ATTACH_TO\"\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n```\n\n----------------------------------------\n\nTITLE: Defining CreateFunctionRoutineBody Enum in Python\nDESCRIPTION: This code defines an enumeration `CreateFunctionRoutineBody` with members EXTERNAL and SQL, representing the language of the function routine body. EXTERNAL indicates an external language, while SQL signifies a SQL routine body.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CreateFunctionRoutineBody\n\n   Function language. When **EXTERNAL** is used, the language of the routine function should be specified in the __external_language__ field, and the __return_params__ of the function cannot be used (as **TABLE** return type is not supported), and the __sql_data_access__ field must be **NO_SQL**.\n\n   .. py:attribute:: EXTERNAL\n      :value: \"EXTERNAL\"\n\n   .. py:attribute:: SQL\n      :value: \"SQL\"\n```\n\n----------------------------------------\n\nTITLE: Updating Approved Domains (Python)\nDESCRIPTION: Updates the list of domains approved to host embedded AI/BI dashboards. It requires 'allow_missing' to be true, a 'setting' object, and a 'field_mask' string. Fails if the workspace access policy is not ALLOW_APPROVED_DOMAINS.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/aibi_dashboard_embedding_approved_domains.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(allow_missing: bool, setting: AibiDashboardEmbeddingApprovedDomainsSetting, field_mask: str) -> AibiDashboardEmbeddingApprovedDomainsSetting\n\n    Update the list of domains approved to host embedded AI/BI dashboards.\n\n    Updates the list of domains approved to host embedded AI/BI dashboards. This update will fail if the\n    current workspace access policy is not ALLOW_APPROVED_DOMAINS.\n\n    :param allow_missing: bool\n      This should always be set to true for Settings API. Added for AIP compliance.\n    :param setting: :class:`AibiDashboardEmbeddingApprovedDomainsSetting`\n    :param field_mask: str\n      The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n      field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n      `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n      the entire collection field can be specified. Field names must exactly match the resource field\n      names.\n\n      A field mask of `*` indicates full replacement. It’s recommended to always explicitly list the\n      fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n      changes in the future.\n\n    :returns: :class:`AibiDashboardEmbeddingApprovedDomainsSetting`\n```\n\n----------------------------------------\n\nTITLE: Deleting a Dashboard Widget in Databricks\nDESCRIPTION: This method deletes an existing widget from a Databricks dashboard. It requires the ID of the widget to be removed.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/dashboard_widgets.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(id: str)\n\n    Remove widget.\n\n    :param id: str\n      Widget ID returned by :method:dashboardwidgets/create\n```\n\n----------------------------------------\n\nTITLE: Listing Network Connectivity Configurations in Databricks\nDESCRIPTION: This method lists network connectivity configurations. It supports pagination using an optional page token. It returns an iterator over NetworkConnectivityConfiguration objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/network_connectivity.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nNetworkConnectivityAPI.list_network_connectivity_configurations( [, page_token: Optional[str]]) -> Iterator[NetworkConnectivityConfiguration]\n```\n\n----------------------------------------\n\nTITLE: Defining EditWarehouseRequestWarehouseType Enum (Python)\nDESCRIPTION: Defines an enumeration representing warehouse types for editing a Databricks SQL warehouse. The types are CLASSIC, PRO, and TYPE_UNSPECIFIED. The documentation emphasizes setting the type to `PRO` and `enable_serverless_compute` to `true` when using serverless compute.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EditWarehouseRequestWarehouseType\n\n   Warehouse type: `PRO` or `CLASSIC`. If you want to use serverless compute, you must set to `PRO` and also set the field `enable_serverless_compute` to `true`.\n\n   .. py:attribute:: CLASSIC\n      :value: \"CLASSIC\"\n\n   .. py:attribute:: PRO\n      :value: \"PRO\"\n\n   .. py:attribute:: TYPE_UNSPECIFIED\n      :value: \"TYPE_UNSPECIFIED\"\n```\n\n----------------------------------------\n\nTITLE: Restarting Python Kernel in Databricks Notebook\nDESCRIPTION: This snippet demonstrates how to restart the Python kernel within a Databricks notebook after upgrading the Databricks SDK. It uses the `dbutils.library.restartPython()` function.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndbutils.library.restartPython()\n```\n\n----------------------------------------\n\nTITLE: Marketplace DataRefresh Enum Definition in Python\nDESCRIPTION: Defines the frequency at which data in a listing is refreshed on the Databricks Marketplace. The possible values range from 'NONE' to 'SECOND', 'MINUTE', 'HOURLY', 'DAILY', 'WEEKLY', 'MONTHLY', 'QUARTERLY', and 'YEARLY'.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: DataRefresh\n\n   .. py:attribute:: DAILY\n      :value: \"DAILY\"\n\n   .. py:attribute:: HOURLY\n      :value: \"HOURLY\"\n\n   .. py:attribute:: MINUTE\n      :value: \"MINUTE\"\n\n   .. py:attribute:: MONTHLY\n      :value: \"MONTHLY\"\n\n   .. py:attribute:: NONE\n      :value: \"NONE\"\n\n   .. py:attribute:: QUARTERLY\n      :value: \"QUARTERLY\"\n\n   .. py:attribute:: SECOND\n      :value: \"SECOND\"\n\n   .. py:attribute:: WEEKLY\n      :value: \"WEEKLY\"\n\n   .. py:attribute:: YEARLY\n      :value: \"YEARLY\"\n```\n\n----------------------------------------\n\nTITLE: Getting a Message Attachment Query Result in Genie using Python\nDESCRIPTION: This method retrieves the result of a SQL query associated with a specific attachment of a message in a Genie conversation.  It's only available if the message has a query attachment and its status is 'EXECUTING_QUERY' or 'COMPLETED'. The method returns a GenieGetMessageQueryResultResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/genie.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: get_message_attachment_query_result(space_id: str, conversation_id: str, message_id: str, attachment_id: str) -> GenieGetMessageQueryResultResponse\n\n        Get message attachment SQL query result.\n\n        Get the result of SQL query if the message has a query attachment. This is only available if a message\n        has a query attachment and the message status is `EXECUTING_QUERY` OR `COMPLETED`.\n\n        :param space_id: str\n          Genie space ID\n        :param conversation_id: str\n          Conversation ID\n        :param message_id: str\n          Message ID\n        :param attachment_id: str\n          Attachment ID\n\n        :returns: :class:`GenieGetMessageQueryResultResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining RestrictWorkspaceAdminsMessageStatus Enum in Python\nDESCRIPTION: This code defines an enumeration `RestrictWorkspaceAdminsMessageStatus`. The possible values are `ALLOW_ALL` and `RESTRICT_TOKENS_AND_JOB_RUN_AS`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: RestrictWorkspaceAdminsMessageStatus\n\n   .. py:attribute:: ALLOW_ALL\n      :value: \"ALLOW_ALL\"\n\n   .. py:attribute:: RESTRICT_TOKENS_AND_JOB_RUN_AS\n      :value: \"RESTRICT_TOKENS_AND_JOB_RUN_AS\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Dashboard Schedule\nDESCRIPTION: Deletes a specific schedule from a Lakeview dashboard using the LakeviewAPI. The schedule and dashboard are identified by their UUIDs. An optional etag can be provided for conditional deletion.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: delete_schedule(dashboard_id: str, schedule_id: str [, etag: Optional[str]])\n\n    Delete dashboard schedule.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard to which the schedule belongs.\n    :param schedule_id: str\n      UUID identifying the schedule.\n    :param etag: str (optional)\n      The etag for the schedule. Optionally, it can be provided to verify that the schedule has not been\n      modified from its last retrieval.\n```\n\n----------------------------------------\n\nTITLE: Defining Column Type Names for Delta Sharing in Python\nDESCRIPTION: This code defines an enumeration of supported column types for Delta Sharing. It includes common data types like ARRAY, BINARY, BOOLEAN, STRING, and TIMESTAMP.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sharing.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ColumnTypeName:\n    \"\"\"UC supported column types Copied from https://src.dev.databricks.com/databricks/universe@23a85902bb58695ab9293adc9f327b0714b55e72/-/blob/managed-catalog/api/messages/table.proto?L68\"\"\"\n\n    ARRAY = \"ARRAY\"\n    BINARY = \"BINARY\"\n    BOOLEAN = \"BOOLEAN\"\n    BYTE = \"BYTE\"\n    CHAR = \"CHAR\"\n    DATE = \"DATE\"\n    DECIMAL = \"DECIMAL\"\n    DOUBLE = \"DOUBLE\"\n    FLOAT = \"FLOAT\"\n    INT = \"INT\"\n    INTERVAL = \"INTERVAL\"\n    LONG = \"LONG\"\n    MAP = \"MAP\"\n    NULL = \"NULL\"\n    SHORT = \"SHORT\"\n    STRING = \"STRING\"\n    STRUCT = \"STRUCT\"\n    TABLE_TYPE = \"TABLE_TYPE\"\n    TIMESTAMP = \"TIMESTAMP\"\n    TIMESTAMP_NTZ = \"TIMESTAMP_NTZ\"\n    USER_DEFINED_TYPE = \"USER_DEFINED_TYPE\"\n    VARIANT = \"VARIANT\"\n```\n\n----------------------------------------\n\nTITLE: CleanRoomAsset Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomAsset dataclass, representing an asset within a Databricks Clean Room. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomAsset\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Install libraries - Python\nDESCRIPTION: Installs libraries on a Databricks cluster. The `cluster_id` parameter identifies the cluster, and `libraries` is a list of `Library` objects to be installed. The installation process is asynchronous and occurs in the background.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/libraries.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: install(cluster_id: str, libraries: List[Library])\n\n    Add a library.\n\n    Add libraries to install on a cluster. The installation is asynchronous; it happens in the background\n    after the completion of this request.\n\n    :param cluster_id: str\n      Unique identifier for the cluster on which to install these libraries.\n    :param libraries: List[:class:`Library`]\n      The libraries to install.\n```\n\n----------------------------------------\n\nTITLE: Defining NccAzurePrivateEndpointRuleGroupId Enum in Python\nDESCRIPTION: This code defines an enumeration `NccAzurePrivateEndpointRuleGroupId` representing the sub-resource type (group ID) of the target resource. Values include `BLOB`, `DFS`, `MYSQL_SERVER`, and `SQL_SERVER`. Connecting to workspace root storage requires two endpoints, one for `blob` and one for `dfs`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: NccAzurePrivateEndpointRuleGroupId\n\n   The sub-resource type (group ID) of the target resource. Note that to connect to workspace root storage (root DBFS), you need two endpoints, one for `blob` and one for `dfs`.\n\n   .. py:attribute:: BLOB\n      :value: \"BLOB\"\n\n   .. py:attribute:: DFS\n      :value: \"DFS\"\n\n   .. py:attribute:: MYSQL_SERVER\n      :value: \"MYSQL_SERVER\"\n\n   .. py:attribute:: SQL_SERVER\n      :value: \"SQL_SERVER\"\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_MANAGE_RUN attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_MANAGE_RUN attribute for the PermissionLevel class. It represents a permission level that allows managing runs of resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_MANAGE_RUN\n      :value: \"CAN_MANAGE_RUN\"\n```\n\n----------------------------------------\n\nTITLE: Getting Share Permissions in Python\nDESCRIPTION: This code snippet demonstrates how to retrieve share permissions for a recipient using the `share_permissions` method after creating the recipient using the `create` method. It utilizes `WorkspaceClient` from `databricks.sdk` to interact with the Databricks workspace. The recipient's name is generated dynamically using a timestamp to ensure uniqueness. It imports the `time` module to generate a unique name.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/recipients.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.recipients.create(name=f\"sdk-{time.time_ns()}\")\n\nshare_permissions = w.recipients.share_permissions(name=created.name)\n\n# cleanup\nw.recipients.delete(name=created.name)\n```\n\n----------------------------------------\n\nTITLE: FulfillmentType Enum Definition in Python\nDESCRIPTION: Defines the types of fulfillment options for listings on the Databricks Marketplace. These can be 'INSTALL' or 'REQUEST_ACCESS', indicating whether the listing can be directly installed or requires a request for access.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: FulfillmentType\n\n   .. py:attribute:: INSTALL\n      :value: \"INSTALL\"\n\n   .. py:attribute:: REQUEST_ACCESS\n      :value: \"REQUEST_ACCESS\"\n```\n\n----------------------------------------\n\nTITLE: Defining Metastore Delta Sharing Scopes in Python\nDESCRIPTION: This code defines the UpdateMetastoreDeltaSharingScope class, which specifies the scope of delta sharing enabled for the metastore. Each possible scope is defined as a class attribute with a string value. The scope can be INTERNAL.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nclass UpdateMetastoreDeltaSharingScope:\n   \"\"\"The scope of Delta Sharing enabled for the metastore.\"\"\"\n\n   INTERNAL = \"INTERNAL\"\n```\n\n----------------------------------------\n\nTITLE: Defining Cluster State Enum (Python)\nDESCRIPTION: Defines an enumeration `State` representing the possible states of a Databricks cluster. It specifies allowable state transitions and includes attributes like `PENDING`, `RUNNING`, `TERMINATING`, etc.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: State\n\n   The state of a Cluster. The current allowable state transitions are as follows:\n   - `PENDING` -> `RUNNING` - `PENDING` -> `TERMINATING` - `RUNNING` -> `RESIZING` - `RUNNING` -> `RESTARTING` - `RUNNING` -> `TERMINATING` - `RESTARTING` -> `RUNNING` - `RESTARTING` -> `TERMINATING` - `RESIZING` -> `RUNNING` - `RESIZING` -> `TERMINATING` - `TERMINATING` -> `TERMINATED`\n\n   .. py:attribute:: ERROR\n      :value: \"ERROR\"\n\n   .. py:attribute:: PENDING\n      :value: \"PENDING\"\n\n   .. py:attribute:: RESIZING\n      :value: \"RESIZING\"\n\n   .. py:attribute:: RESTARTING\n      :value: \"RESTARTING\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n\n   .. py:attribute:: TERMINATED\n      :value: \"TERMINATED\"\n\n   .. py:attribute:: TERMINATING\n      :value: \"TERMINATING\"\n\n   .. py:attribute:: UNKNOWN\n      :value: \"UNKNOWN\"\n```\n\n----------------------------------------\n\nTITLE: Defining Online Table States in Python\nDESCRIPTION: This code defines the OnlineTableState class, which represents the possible states of an online table. Each state is represented as a class attribute with a string value. The states include OFFLINE, OFFLINE_FAILED, ONLINE, ONLINE_CONTINUOUS_UPDATE, ONLINE_NO_PENDING_UPDATE, ONLINE_PIPELINE_FAILED, ONLINE_TRIGGERED_UPDATE, ONLINE_UPDATING_PIPELINE_RESOURCES, PROVISIONING, PROVISIONING_INITIAL_SNAPSHOT and PROVISIONING_PIPELINE_RESOURCES.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass OnlineTableState:\n   \"\"\"The state of an online table.\"\"\"\n\n   OFFLINE = \"OFFLINE\"\n\n   OFFLINE_FAILED = \"OFFLINE_FAILED\"\n\n   ONLINE = \"ONLINE\"\n\n   ONLINE_CONTINUOUS_UPDATE = \"ONLINE_CONTINUOUS_UPDATE\"\n\n   ONLINE_NO_PENDING_UPDATE = \"ONLINE_NO_PENDING_UPDATE\"\n\n   ONLINE_PIPELINE_FAILED = \"ONLINE_PIPELINE_FAILED\"\n\n   ONLINE_TRIGGERED_UPDATE = \"ONLINE_TRIGGERED_UPDATE\"\n\n   ONLINE_UPDATING_PIPELINE_RESOURCES = \"ONLINE_UPDATING_PIPELINE_RESOURCES\"\n\n   PROVISIONING = \"PROVISIONING\"\n\n   PROVISIONING_INITIAL_SNAPSHOT = \"PROVISIONING_INITIAL_SNAPSHOT\"\n\n   PROVISIONING_PIPELINE_RESOURCES = \"PROVISIONING_PIPELINE_RESOURCES\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Databricks Job with Python SDK\nDESCRIPTION: This code snippet demonstrates the creation of a new Databricks job using the Python SDK. It initializes a WorkspaceClient, defines a notebook task, and creates a job with that task. Essential parameters include the job name, task configuration (existing_cluster_id, notebook_task, task_key, timeout_seconds), and dependencies on databricks.sdk and databricks.sdk.service.jobs.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\ncreated_job = w.jobs.create(\n    name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.Task(\n            description=\"test\",\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=\"test\",\n            timeout_seconds=0,\n        )\n    ],\n)\n\n# cleanup\nw.jobs.delete(job_id=created_job.job_id)\n```\n\n----------------------------------------\n\nTITLE: ListingTagType Enum Definition in Python\nDESCRIPTION: Defines the types of tags that can be applied to a listing on the Databricks Marketplace. These tag types are 'LISTING_TAG_TYPE_LANGUAGE' and 'LISTING_TAG_TYPE_TASK', used to categorize listings based on programming language or task.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ListingTagType\n\n   .. py:attribute:: LISTING_TAG_TYPE_LANGUAGE\n      :value: \"LISTING_TAG_TYPE_LANGUAGE\"\n\n   .. py:attribute:: LISTING_TAG_TYPE_TASK\n      :value: \"LISTING_TAG_TYPE_TASK\"\n```\n\n----------------------------------------\n\nTITLE: GkeConfigConnectivityType Enum (Python)\nDESCRIPTION: This enumeration defines the network connectivity types for GKE nodes and the GKE master network. It is used to specify if the GKE cluster is private or public in the `databricks.sdk.service.provisioning` module. Values include `PRIVATE_NODE_PUBLIC_MASTER` and `PUBLIC_NODE_PUBLIC_MASTER`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/provisioning.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: GkeConfigConnectivityType\n\n   Specifies the network connectivity types for the GKE nodes and the GKE master network.\n   Set to `PRIVATE_NODE_PUBLIC_MASTER` for a private GKE cluster for the workspace. The GKE nodes will not have public IPs.\n   Set to `PUBLIC_NODE_PUBLIC_MASTER` for a public GKE cluster. The nodes of a public GKE cluster have public IP addresses.\n\n   .. py:attribute:: PRIVATE_NODE_PUBLIC_MASTER\n      :value: \"PRIVATE_NODE_PUBLIC_MASTER\"\n\n   .. py:attribute:: PUBLIC_NODE_PUBLIC_MASTER\n      :value: \"PUBLIC_NODE_PUBLIC_MASTER\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Network Connectivity Configuration in Databricks\nDESCRIPTION: This method deletes a network connectivity configuration. It requires the network connectivity configuration ID as a parameter to identify the configuration to be deleted. No specific return value is indicated.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/network_connectivity.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNetworkConnectivityAPI.delete_network_connectivity_configuration(network_connectivity_config_id: str)\n```\n\n----------------------------------------\n\nTITLE: Create a Share in Databricks\nDESCRIPTION: This snippet demonstrates how to create a new share in Databricks using the SharesAPI. It imports the necessary modules, initializes the WorkspaceClient, creates a share with a unique name, and then cleans up by deleting the share. The name parameter is required.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/shares.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated_share = w.shares.create(name=f\"sdk-{time.time_ns()}\")\n\n# cleanup\nw.shares.delete(name=created_share.name)\n```\n\n----------------------------------------\n\nTITLE: Getting an App Deployment in Databricks\nDESCRIPTION: Retrieves information for the app deployment with the supplied name and deployment id and returns the AppDeployment object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nw.apps.get_deployment(app_name: str, deployment_id: str) -> AppDeployment\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_QUERY attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_QUERY attribute for the PermissionLevel class.  It represents a permission level that allows querying resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_QUERY\n      :value: \"CAN_QUERY\"\n```\n\n----------------------------------------\n\nTITLE: List Federation Policies (Python)\nDESCRIPTION: Lists all account federation policies. Supports pagination through `page_size` and `page_token`. Returns an iterator over `FederationPolicy` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/federation_policy.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list( [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[FederationPolicy]\n\n    List account federation policies.\n\n    :param page_size: int (optional)\n    :param page_token: str (optional)\n\n    :returns: Iterator over :class:`FederationPolicy`\n```\n\n----------------------------------------\n\nTITLE: Defining DiskTypeAzureDiskVolumeType Enum in Python\nDESCRIPTION: This code defines an enumeration (`DiskTypeAzureDiskVolumeType`) for the supported Azure Disk types in Databricks. It lists PREMIUM_LRS and STANDARD_LRS as possible values. These values correspond to different performance tiers of Azure managed disks.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: DiskTypeAzureDiskVolumeType\n\n   All Azure Disk types that Databricks supports. See https://docs.microsoft.com/en-us/azure/storage/storage-about-disks-and-vhds-linux#types-of-disks\n\n   .. py:attribute:: PREMIUM_LRS\n      :value: \"PREMIUM_LRS\"\n\n   .. py:attribute:: STANDARD_LRS\n      :value: \"STANDARD_LRS\"\n```\n\n----------------------------------------\n\nTITLE: DeleteResponse Class Definition (Python)\nDESCRIPTION: Defines the DeleteResponse dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: DeleteResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Settings API Property: enhanced_security_monitoring\nDESCRIPTION: This property controls whether enhanced security monitoring is enabled for the current workspace. If the compliance security profile is enabled, this is automatically enabled.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/settings.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n.. py:property:: enhanced_security_monitoring\n    :type: EnhancedSecurityMonitoringAPI\n\n    Controls whether enhanced security monitoring is enabled for the current workspace. If the compliance\n    security profile is enabled, this is automatically enabled. By default, it is disabled. However, if the\n    compliance security profile is enabled, this is automatically enabled.\n\n    If the compliance security profile is disabled, you can enable or disable this setting and it is not\n    permanent.\n```\n\n----------------------------------------\n\nTITLE: Defining GCP Availability Options in Python\nDESCRIPTION: This code defines the availability options for GCP instances in an instance pool. It includes ON_DEMAND_GCP, PREEMPTIBLE_GCP, and PREEMPTIBLE_WITH_FALLBACK_GCP as possible choices.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: GcpAvailability\n\n   This field determines whether the instance pool will contain preemptible VMs, on-demand VMs, or preemptible VMs with a fallback to on-demand VMs if the former is unavailable.\n\n   .. py:attribute:: ON_DEMAND_GCP\n      :value: \"ON_DEMAND_GCP\"\n\n   .. py:attribute:: PREEMPTIBLE_GCP\n      :value: \"PREEMPTIBLE_GCP\"\n\n   .. py:attribute:: PREEMPTIBLE_WITH_FALLBACK_GCP\n      :value: \"PREEMPTIBLE_WITH_FALLBACK_GCP\"\n```\n\n----------------------------------------\n\nTITLE: PipelineState Enum Definition in Python\nDESCRIPTION: Defines an enumeration representing the possible states of a Delta Live Tables pipeline. It includes states such as DELETED, DEPLOYING, FAILED, IDLE, RUNNING, STARTING, and STOPPING, providing insight into the current stage of the pipeline's lifecycle.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: PipelineState\n\n   The pipeline state.\n\n   .. py:attribute:: DELETED\n      :value: \"DELETED\"\n\n   .. py:attribute:: DEPLOYING\n      :value: \"DEPLOYING\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: IDLE\n      :value: \"IDLE\"\n\n   .. py:attribute:: RECOVERING\n      :value: \"RECOVERING\"\n\n   .. py:attribute:: RESETTING\n      :value: \"RESETTING\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n\n   .. py:attribute:: STARTING\n      :value: \"STARTING\"\n\n   .. py:attribute:: STOPPING\n      :value: \"STOPPING\"\n```\n\n----------------------------------------\n\nTITLE: Getting a Clean Room - Python\nDESCRIPTION: Retrieves a data object clean room from the metastore. The caller must be a metastore admin or the owner of the clean room. Requires the name of the clean room as a parameter.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/clean_rooms.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(name: str [, include_remote_details: Optional[bool]]) -> CleanRoomInfo\n\n    Get a clean room.\n    \n    Gets a data object clean room from the metastore. The caller must be a metastore admin or the owner of\n    the clean room.\n    \n    :param name: str\n      The name of the clean room.\n    :param include_remote_details: bool (optional)\n      Whether to include remote details (central) on the clean room.\n    \n    :returns: :class:`CleanRoomInfo`\n```\n\n----------------------------------------\n\nTITLE: Defining View Types in Python\nDESCRIPTION: Defines the different view types available for Databricks items.  Currently includes Notebook and Dashboard views. These constants specify the type of view being referenced, for example, when exporting or displaying a Databricks asset.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n.. py:attribute:: DASHBOARD\n   :value: \"DASHBOARD\"\n\n.. py:attribute:: NOTEBOOK\n   :value: \"NOTEBOOK\"\n```\n\n----------------------------------------\n\nTITLE: Defining Shared Securable Kind for Delta Sharing in Python\nDESCRIPTION: This code defines an enumeration for securable kinds of delta-shared objects. It includes options for FUNCTION_FEATURE_SPEC, FUNCTION_REGISTERED_MODEL, and FUNCTION_STANDARD.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sharing.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass SharedSecurableKind:\n    \"\"\"The SecurableKind of a delta-shared object.\"\"\"\n\n    FUNCTION_FEATURE_SPEC = \"FUNCTION_FEATURE_SPEC\"\n    FUNCTION_REGISTERED_MODEL = \"FUNCTION_REGISTERED_MODEL\"\n    FUNCTION_STANDARD = \"FUNCTION_STANDARD\"\n```\n\n----------------------------------------\n\nTITLE: Defining ChannelName Enum (Python)\nDESCRIPTION: Defines an enumeration representing different channel names for Databricks SQL. It includes CHANNEL_NAME_CURRENT, CHANNEL_NAME_CUSTOM, CHANNEL_NAME_PREVIEW, and CHANNEL_NAME_PREVIOUS, which likely refer to different release channels or versions of the SQL service.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ChannelName\n\n   .. py:attribute:: CHANNEL_NAME_CURRENT\n      :value: \"CHANNEL_NAME_CURRENT\"\n\n   .. py:attribute:: CHANNEL_NAME_CUSTOM\n      :value: \"CHANNEL_NAME_CUSTOM\"\n\n   .. py:attribute:: CHANNEL_NAME_PREVIEW\n      :value: \"CHANNEL_NAME_PREVIEW\"\n\n   .. py:attribute:: CHANNEL_NAME_PREVIOUS\n      :value: \"CHANNEL_NAME_PREVIOUS\"\n```\n\n----------------------------------------\n\nTITLE: Refreshing Table Monitor Python\nDESCRIPTION: The `refresh` method triggers a manual refresh of the table monitor's metric tables. It requires specific permissions on the parent catalog and schema, as well as ownership of the table. The method returns information about the refresh process.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/quality_monitors.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: refresh(table_name: str) -> MonitorRefreshInfo\n\n        Refresh a table monitor.\n\n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n        - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent schema - be an\n        owner of the table\n\n        Additionally, the call must be made from the workspace where the monitor was created.\n\n        :param table_name: str\n          Full name of the table.\n\n        :returns: :class:`MonitorRefreshInfo`\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_MANAGE_STAGING_VERSIONS attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_MANAGE_STAGING_VERSIONS attribute for the PermissionLevel class. It represents a permission level that allows managing staging versions of resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_MANAGE_STAGING_VERSIONS\n      :value: \"CAN_MANAGE_STAGING_VERSIONS\"\n```\n\n----------------------------------------\n\nTITLE: Update EnableNotebookTableClipboard Setting (Python)\nDESCRIPTION: Updates the Enable Notebook Table Clipboard setting.  This method takes an `EnableNotebookTableClipboard` object with the desired settings, a field mask indicating which fields to update, and the `allow_missing` parameter. The update follows eventual consistency.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/enable_notebook_table_clipboard.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nw.settings.patch_enable_notebook_table_clipboard(allow_missing: bool, setting: EnableNotebookTableClipboard, field_mask: str) -> EnableNotebookTableClipboard\n```\n\n----------------------------------------\n\nTITLE: ExchangeFilterType Enum Definition in Python\nDESCRIPTION: Defines the types of filters that can be applied to exchanges in the Databricks Marketplace. Currently, the only supported type is 'GLOBAL_METASTORE_ID', which filters exchanges based on the global metastore ID.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ExchangeFilterType\n\n   .. py:attribute:: GLOBAL_METASTORE_ID\n      :value: \"GLOBAL_METASTORE_ID\"\n```\n\n----------------------------------------\n\nTITLE: PersonalizationRequestStatus Enum in Python\nDESCRIPTION: Defines statuses for a personalization request, which include NEW, REQUEST_PENDING, FULFILLED, and DENIED. This allows tracking of whether requests are fulfilled or denied, or are in pending stages.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PersonalizationRequestStatus\n\n   .. py:attribute:: DENIED\n      :value: \"DENIED\"\n\n   .. py:attribute:: FULFILLED\n      :value: \"FULFILLED\"\n\n   .. py:attribute:: NEW\n      :value: \"NEW\"\n\n   .. py:attribute:: REQUEST_PENDING\n      :value: \"REQUEST_PENDING\"\n```\n\n----------------------------------------\n\nTITLE: Updating Default Namespace Setting - Python\nDESCRIPTION: Updates the default namespace setting for the workspace. Requires an etag and field_mask. If the setting is updated concurrently, PATCH fails with 409 and the request must be retried by using the fresh etag in the 409 response.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/default_namespace.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n    .. py:method:: update(allow_missing: bool, setting: DefaultNamespaceSetting, field_mask: str) -> DefaultNamespaceSetting\n\n        Update the default namespace setting.\n\n        Updates the default namespace setting for the workspace. A fresh etag needs to be provided in `PATCH`\n        requests (as part of the setting field). The etag can be retrieved by making a `GET` request before\n        the `PATCH` request. Note that if the setting does not exist, `GET` returns a NOT_FOUND error and the\n        etag is present in the error response, which should be set in the `PATCH` request. If the setting is\n        updated concurrently, `PATCH` fails with 409 and the request must be retried by using the fresh etag\n        in the 409 response.\n\n        :param allow_missing: bool\n          This should always be set to true for Settings API. Added for AIP compliance.\n        :param setting: :class:`DefaultNamespaceSetting`\n          This represents the setting configuration for the default namespace in the Databricks workspace.\n          Setting the default catalog for the workspace determines the catalog that is used when queries do\n          not reference a fully qualified 3 level name. For example, if the default catalog is set to\n          'retail_prod' then a query 'SELECT * FROM myTable' would reference the object\n          'retail_prod.default.myTable' (the schema 'default' is always assumed). This setting requires a\n          restart of clusters and SQL warehouses to take effect. Additionally, the default namespace only\n          applies when using Unity Catalog-enabled compute.\n        :param field_mask: str\n          The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n          field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n          `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n          the entire collection field can be specified. Field names must exactly match the resource field\n          names.\n\n          A field mask of `*` indicates full replacement. It’s recommended to always explicitly list the\n          fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n          changes in the future.\n\n        :returns: :class:`DefaultNamespaceSetting`\n```\n\n----------------------------------------\n\nTITLE: ListingType Enum Definition in Python\nDESCRIPTION: Defines the types of listings available on the Databricks Marketplace. These types are 'PERSONALIZED' and 'STANDARD', indicating whether the listing is tailored for a specific user or is a general listing.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ListingType\n\n   .. py:attribute:: PERSONALIZED\n      :value: \"PERSONALIZED\"\n\n   .. py:attribute:: STANDARD\n      :value: \"STANDARD\"\n```\n\n----------------------------------------\n\nTITLE: Defining DateValueDynamicDate Enum (Python)\nDESCRIPTION: Defines an enumeration representing dynamic dates. Options include NOW and YESTERDAY. These can be used when specifying date parameters in SQL queries or alerts that need to refer to the current date or the previous day.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: DateValueDynamicDate\n\n   .. py:attribute:: NOW\n      :value: \"NOW\"\n\n   .. py:attribute:: YESTERDAY\n      :value: \"YESTERDAY\"\n```\n\n----------------------------------------\n\nTITLE: Updating Restrict Workspace Admins Setting in Python\nDESCRIPTION: Updates the restrict workspace admins setting. Requires an etag to be included in the setting field.  `allow_missing` must be set to true. `field_mask` determines which fields are being updated. A fresh etag needs to be provided in `PATCH` requests as part of the setting field.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/restrict_workspace_admins.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsettings.restrict_workspace_admins.update(allow_missing: bool, setting: RestrictWorkspaceAdminsSetting, field_mask: str) -> RestrictWorkspaceAdminsSetting\n```\n\n----------------------------------------\n\nTITLE: ModelVersionStatus Constants Definition in Python\nDESCRIPTION: Defines constants for the status of a model version. These states (`FAILED_REGISTRATION`, `PENDING_REGISTRATION`, `READY`) are used to indicate the current status of a registered model during the registration and deployment process.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ModelVersionStatus\n\n   Current status of `model_version`\n\n   .. py:attribute:: FAILED_REGISTRATION\n      :value: \"FAILED_REGISTRATION\"\n\n   .. py:attribute:: PENDING_REGISTRATION\n      :value: \"PENDING_REGISTRATION\"\n\n   .. py:attribute:: READY\n      :value: \"READY\"\n```\n\n----------------------------------------\n\nTITLE: Listing Group Details in Databricks (Python)\nDESCRIPTION: Lists details of groups associated with the Databricks account. The attributes, count, excluded_attributes, filter, sort_by, sort_order, and start_index parameters are available for filtering and pagination. Returns an iterator over Group objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/groups.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list( [, attributes: Optional[str], count: Optional[int], excluded_attributes: Optional[str], filter: Optional[str], sort_by: Optional[str], sort_order: Optional[ListSortOrder], start_index: Optional[int]]) -> Iterator[Group]\n\n        List group details.\n\n        Gets all details of the groups associated with the Databricks account.\n\n        :param attributes: str (optional)\n          Comma-separated list of attributes to return in response.\n        :param count: int (optional)\n          Desired number of results per page. Default is 10000.\n        :param excluded_attributes: str (optional)\n          Comma-separated list of attributes to exclude in response.\n        :param filter: str (optional)\n          Query by which the results have to be filtered. Supported operators are equals(`eq`),\n          contains(`co`), starts with(`sw`) and not equals(`ne`). Additionally, simple expressions can be\n          formed using logical operators - `and` and `or`. The [SCIM RFC] has more details but we currently\n          only support simple expressions.\n\n          [SCIM RFC]: https://tools.ietf.org/html/rfc7644#section-3.4.2.2\n        :param sort_by: str (optional)\n          Attribute to sort the results.\n        :param sort_order: :class:`ListSortOrder` (optional)\n          The order to sort the results.\n        :param start_index: int (optional)\n          Specifies the index of the first result. First item is number 1.\n\n        :returns: Iterator over :class:`Group`\n```\n\n----------------------------------------\n\nTITLE: Creating a Secret Scope in Databricks using Python SDK\nDESCRIPTION: This code snippet demonstrates how to create a new secret scope in Databricks using the Python SDK. It initializes a WorkspaceClient, generates a unique scope name, creates the scope using `w.secrets.create_scope()`, and cleans up by deleting a dummy secret and the scope itself.  The scope name must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/secrets.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nkey_name = f\"sdk-{time.time_ns()}\"\n\nscope_name = f\"sdk-{time.time_ns()}\"\n\nw.secrets.create_scope(scope=scope_name)\n\n# cleanup\nw.secrets.delete_secret(scope=scope_name, key=key_name)\nw.secrets.delete_scope(scope=scope_name)\n```\n\n----------------------------------------\n\nTITLE: Updating Metastore Assignment in Python\nDESCRIPTION: This method allows updating a metastore assignment. It can update the metastore_id or default_catalog_name for a given workspace. The user needs to be an account admin to update the metastore_id, or a workspace admin otherwise.  It takes workspace_id as a required parameter and metastore_id and default_catalog_name as optional parameters.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/metastores.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef update_assignment(workspace_id: int [, default_catalog_name: Optional[str], metastore_id: Optional[str]]):\n    \"\"\"Update an assignment.\n\n    Updates a metastore assignment. This operation can be used to update __metastore_id__ or\n    __default_catalog_name__ for a specified Workspace, if the Workspace is already assigned a metastore.\n    The caller must be an account admin to update __metastore_id__; otherwise, the caller can be a\n    Workspace admin.\n\n    :param workspace_id: int\n      A workspace ID.\n    :param default_catalog_name: str (optional)\n      The name of the default catalog in the metastore. This field is depracted. Please use \"Default\n      Namespace API\" to configure the default catalog for a Databricks workspace.\n    :param metastore_id: str (optional)\n      The unique ID of the metastore.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Search Listings - Python\nDESCRIPTION: Searches for listings in the Databricks Marketplace based on a query string. Supports fuzzy matching and filtering based on asset types, categories, free listings, private exchange listings, and provider IDs. Returns an iterator over `Listing` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_listings.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: search(query: str [, assets: Optional[List[AssetType]], categories: Optional[List[Category]], is_free: Optional[bool], is_private_exchange: Optional[bool], page_size: Optional[int], page_token: Optional[str], provider_ids: Optional[List[str]]]) -> Iterator[Listing]\n\n    Search listings.\n\n    Search published listings in the Databricks Marketplace that the consumer has access to. This query\n    supports a variety of different search parameters and performs fuzzy matching.\n\n    :param query: str\n      Fuzzy matches query\n    :param assets: List[:class:`AssetType`] (optional)\n      Matches any of the following asset types\n    :param categories: List[:class:`Category`] (optional)\n      Matches any of the following categories\n    :param is_free: bool (optional)\n    :param is_private_exchange: bool (optional)\n    :param page_size: int (optional)\n    :param page_token: str (optional)\n    :param provider_ids: List[str] (optional)\n      Matches any of the following provider ids\n\n    :returns: Iterator over :class:`Listing`\n```\n\n----------------------------------------\n\nTITLE: Create Provider - Python\nDESCRIPTION: Creates a new provider in the Databricks Marketplace. It requires a ProviderInfo object as input, which contains the necessary details for creating the provider. The method returns a CreateProviderResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_providers.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(provider: ProviderInfo) -> CreateProviderResponse\n\n    Create a provider.\n\n    Create a provider\n\n    :param provider: :class:`ProviderInfo`\n\n    :returns: :class:`CreateProviderResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining STREAMING_BACKLOG_BYTES Constant in Python\nDESCRIPTION: This snippet defines a constant STREAMING_BACKLOG_BYTES, representing an estimate of the maximum bytes of data waiting to be consumed across all streams. This metric is in Public Preview and useful for monitoring streaming job backlog.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nSTREAMING_BACKLOG_BYTES = \"STREAMING_BACKLOG_BYTES\"\n```\n\n----------------------------------------\n\nTITLE: Get a Share in Databricks\nDESCRIPTION: This snippet shows how to retrieve a share's information using the SharesAPI. It creates a share, retrieves it using the get method, and then deletes the share for cleanup. The name parameter is required. The include_shared_data parameter is optional and controls whether data to include in the share.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/shares.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated_share = w.shares.create(name=f\"sdk-{time.time_ns()}\")\n\n_ = w.shares.get(name=created_share.name)\n\n# cleanup\nw.shares.delete(name=created_share.name)\n```\n\n----------------------------------------\n\nTITLE: Creating a Message in Genie using Python\nDESCRIPTION: This method creates a new message in an existing Genie conversation. It requires the space ID, conversation ID, and the content of the user's message.  The AI response will use all previously created messages in the conversation to formulate a response. It returns a long-running operation waiter for GenieMessage.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/genie.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: create_message(space_id: str, conversation_id: str, content: str) -> Wait[GenieMessage]\n\n        Create conversation message.\n\n        Create new message in a [conversation](:method:genie/startconversation). The AI response uses all\n        previously created messages in the conversation to respond.\n\n        :param space_id: str\n          The ID associated with the Genie space where the conversation is started.\n        :param conversation_id: str\n          The ID associated with the conversation.\n        :param content: str\n          User message content.\n\n        :returns:\n          Long-running operation waiter for :class:`GenieMessage`.\n          See :method:wait_get_message_genie_completed for more details.\n```\n\n----------------------------------------\n\nTITLE: ComplianceSecurityProfile Class Definition (Python)\nDESCRIPTION: Defines the ComplianceSecurityProfile dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: ComplianceSecurityProfile\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Listing Budget Policies (Python)\nDESCRIPTION: Lists all budget policies using the `list` method of the `BudgetPolicyAPI`. Supports filtering, pagination, and sorting using optional parameters `filter_by`, `page_size`, `page_token`, and `sort_spec`. Returns an iterator over `BudgetPolicy` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/budget_policy.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nBudgetPolicyAPI.list( [, filter_by: Optional[Filter], page_size: Optional[int], page_token: Optional[str], sort_spec: Optional[SortSpec]]) -> Iterator[BudgetPolicy]\n```\n\n----------------------------------------\n\nTITLE: Autogenerating Documentation for Subscriber Class in Python\nDESCRIPTION: This code snippet uses the 'autoclass' directive to automatically generate documentation for the Subscriber class. The ':members:' option includes all public members in the documentation, while ':undoc-members:' excludes undocumented members. This is part of the Databricks SDK documentation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: Subscriber\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Deleting Approved Domains (Python)\nDESCRIPTION: Deletes the list of domains approved to host embedded AI/BI dashboards, reverting back to the default empty list. It takes an optional 'etag' parameter for optimistic concurrency control.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/aibi_dashboard_embedding_approved_domains.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete( [, etag: Optional[str]]) -> DeleteAibiDashboardEmbeddingApprovedDomainsSettingResponse\n\n    Delete AI/BI dashboard embedding approved domains.\n\n    Delete the list of domains approved to host embedded AI/BI dashboards, reverting back to the default\n    empty list.\n\n    :param etag: str (optional)\n      etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n      optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n      each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n      to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n      request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n    :returns: :class:`DeleteAibiDashboardEmbeddingApprovedDomainsSettingResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining AppDeploymentState Enum (Python)\nDESCRIPTION: This Python code defines an enumeration `AppDeploymentState` with values representing the various states an app deployment can be in, such as `CANCELLED`, `FAILED`, `IN_PROGRESS`, and `SUCCEEDED`. These states indicate the current status of the deployment process.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/apps.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: AppDeploymentState\n\n   .. py:attribute:: CANCELLED\n      :value: \"CANCELLED\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: IN_PROGRESS\n      :value: \"IN_PROGRESS\"\n\n   .. py:attribute:: SUCCEEDED\n      :value: \"SUCCEEDED\"\n```\n\n----------------------------------------\n\nTITLE: Defining GREATER_THAN Constant for JobsHealthOperator in Python\nDESCRIPTION: This snippet defines a constant GREATER_THAN representing the operator used to compare the health metric value with a specified threshold for JobsHealthOperator.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nGREATER_THAN = \"GREATER_THAN\"\n```\n\n----------------------------------------\n\nTITLE: Defining Enum: ClusterAutoRestartMessageMaintenanceWindowWeekDayFrequency - Python\nDESCRIPTION: This Python enum defines the frequency options for scheduling weekly maintenance windows. The frequency can be set to every week, first and third of the month, first of the month, fourth of the month, second and fourth of the month, second of the month or third of the month.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ClusterAutoRestartMessageMaintenanceWindowWeekDayFrequency\n\n   .. py:attribute:: EVERY_WEEK\n      :value: \"EVERY_WEEK\"\n\n   .. py:attribute:: FIRST_AND_THIRD_OF_MONTH\n      :value: \"FIRST_AND_THIRD_OF_MONTH\"\n\n   .. py:attribute:: FIRST_OF_MONTH\n      :value: \"FIRST_OF_MONTH\"\n\n   .. py:attribute:: FOURTH_OF_MONTH\n      :value: \"FOURTH_OF_MONTH\"\n\n   .. py:attribute:: SECOND_AND_FOURTH_OF_MONTH\n      :value: \"SECOND_AND_FOURTH_OF_MONTH\"\n\n   .. py:attribute:: SECOND_OF_MONTH\n      :value: \"SECOND_OF_MONTH\"\n\n   .. py:attribute:: THIRD_OF_MONTH\n      :value: \"THIRD_OF_MONTH\"\n```\n\n----------------------------------------\n\nTITLE: Retrieve Cluster Events\nDESCRIPTION: This snippet demonstrates how to retrieve cluster events using the `events` method from the Databricks SDK. It first creates a cluster, retrieves its events, and then deletes the cluster.  It depends on the `WorkspaceClient` module from the `databricks.sdk` and uses `os` and `time` for environment variables and unique cluster names, respectively.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nlatest = w.clusters.select_spark_version(latest=True, long_term_support=True)\n\ncluster_name = f\"sdk-{time.time_ns()}\"\n\nclstr = w.clusters.create(\n    cluster_name=cluster_name,\n    spark_version=latest,\n    instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n    autotermination_minutes=15,\n    num_workers=1,\n).result()\n\nevents = w.clusters.events(cluster_id=clstr.cluster_id)\n\n# cleanup\nw.clusters.permanent_delete(cluster_id=clstr.cluster_id)\n```\n\n----------------------------------------\n\nTITLE: Listing ACLs for a Secret Scope in Databricks using Python SDK\nDESCRIPTION: This code snippet showcases how to list the Access Control Lists (ACLs) for a given secret scope in Databricks using the Python SDK. It initializes a WorkspaceClient, creates a secret scope, lists the ACLs using `w.secrets.list_acls()`, and then cleans up by deleting a secret and the scope. The user must have `MANAGE` permission to list ACLs.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/secrets.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nkey_name = f\"sdk-{time.time_ns()}\"\n\nscope_name = f\"sdk-{time.time_ns()}\"\n\nw.secrets.create_scope(scope=scope_name)\n\nacls = w.secrets.list_acls(scope=scope_name)\n\n# cleanup\nw.secrets.delete_secret(scope=scope_name, key=key_name)\nw.secrets.delete_scope(scope=scope_name)\n```\n\n----------------------------------------\n\nTITLE: Getting a Model Version By Alias in Databricks with Python\nDESCRIPTION: Retrieves a model version using an alias. Requires appropriate privileges on the registered model.  Parameters include the full name of the registered model and the alias name. The include_aliases parameter is optional.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/model_versions.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nw.model_versions.get_by_alias(full_name: str, alias: str [, include_aliases: Optional[bool]]) -> ModelVersionInfo\n```\n\n----------------------------------------\n\nTITLE: Updating a Budget Policy (Python)\nDESCRIPTION: Updates an existing budget policy using the `update` method of the `BudgetPolicyAPI`. Requires the `policy_id` of the policy to be updated as input, along with optional `limit_config` and `policy` parameters.  Returns the updated `BudgetPolicy` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/budget_policy.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nBudgetPolicyAPI.update(policy_id: str [, limit_config: Optional[LimitConfig], policy: Optional[BudgetPolicy]]) -> BudgetPolicy\n```\n\n----------------------------------------\n\nTITLE: Defining Disposition Enum (Python)\nDESCRIPTION: Defines an enumeration representing disposition types. Options include EXTERNAL_LINKS and INLINE, defining how the result is displayed.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: Disposition\n\n   .. py:attribute:: EXTERNAL_LINKS\n      :value: \"EXTERNAL_LINKS\"\n\n   .. py:attribute:: INLINE\n      :value: \"INLINE\"\n```\n\n----------------------------------------\n\nTITLE: Defining AlertOptionsEmptyResultState Enum (Python)\nDESCRIPTION: Defines an enumeration for the possible states an alert can evaluate to when the query result is empty. The states are OK, TRIGGERED, and UNKNOWN. This configuration allows specification of the alert behavior when no data is returned by the associated query.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: AlertOptionsEmptyResultState\n\n   State that alert evaluates to when query result is empty.\n\n   .. py:attribute:: OK\n      :value: \"OK\"\n\n   .. py:attribute:: TRIGGERED\n      :value: \"TRIGGERED\"\n\n   .. py:attribute:: UNKNOWN\n      :value: \"UNKNOWN\"\n```\n\n----------------------------------------\n\nTITLE: Defining ClusterPolicyPermissionLevel Enum in Python\nDESCRIPTION: This code defines an enumeration (`ClusterPolicyPermissionLevel`) specifying the permission level for cluster policies in Databricks. Currently, the only defined option is CAN_USE, indicating that a user has permission to use a particular cluster policy when creating a cluster.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ClusterPolicyPermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_USE\n      :value: \"CAN_USE\"\n```\n\n----------------------------------------\n\nTITLE: List System Schemas in Databricks\nDESCRIPTION: Lists system schemas within a specified metastore. Requires account admin or metastore admin permissions. Supports pagination with optional parameters for maximum results per page and a page token for subsequent requests. Returns an iterator over SystemSchemaInfo objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/system_schemas.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nw.system_schemas.list(metastore_id: str [, max_results: Optional[int], page_token: Optional[str]]) -> Iterator[SystemSchemaInfo]\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_BIND attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_BIND attribute for the PermissionLevel class. It represents a permission level that allows binding to resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_BIND\n      :value: \"CAN_BIND\"\n```\n\n----------------------------------------\n\nTITLE: Defining AWS Instance Pool Availability in Python\nDESCRIPTION: This code defines the availability types for AWS instances in an instance pool. It includes ON_DEMAND and SPOT as possible choices for instance provisioning.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: InstancePoolAwsAttributesAvailability\n\n   The set of AWS availability types supported when setting up nodes for a cluster.\n\n   .. py:attribute:: ON_DEMAND\n      :value: \"ON_DEMAND\"\n\n   .. py:attribute:: SPOT\n      :value: \"SPOT\"\n```\n\n----------------------------------------\n\nTITLE: WorkspaceStatus Enum Definition (Python)\nDESCRIPTION: Defines an enumeration that represents the status of a Databricks workspace in the `databricks.sdk.service.provisioning` module. Possible values include `BANNED`, `CANCELLING`, `FAILED`, `NOT_PROVISIONED`, `PROVISIONING`, and `RUNNING`. This is used to track the lifecycle of a Databricks workspace, especially during creation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/provisioning.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: WorkspaceStatus\n\n   The status of the workspace. For workspace creation, usually it is set to `PROVISIONING` initially. Continue to check the status until the status is `RUNNING`.\n\n   .. py:attribute:: BANNED\n      :value: \"BANNED\"\n\n   .. py:attribute:: CANCELLING\n      :value: \"CANCELLING\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: NOT_PROVISIONED\n      :value: \"NOT_PROVISIONED\"\n\n   .. py:attribute:: PROVISIONING\n      :value: \"PROVISIONING\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n```\n\n----------------------------------------\n\nTITLE: Defining UpsertDataStatus Enum in Python\nDESCRIPTION: Defines an enumeration `UpsertDataStatus` representing the status of an upsert operation within the Databricks Vector Search service. It can have values of FAILURE, PARTIAL_SUCCESS, or SUCCESS.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/vectorsearch.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: UpsertDataStatus\n\n   Status of the upsert operation.\n\n   .. py:attribute:: FAILURE\n      :value: \"FAILURE\"\n\n   .. py:attribute:: PARTIAL_SUCCESS\n      :value: \"PARTIAL_SUCCESS\"\n\n   .. py:attribute:: SUCCESS\n      :value: \"SUCCESS\"\n```\n\n----------------------------------------\n\nTITLE: Listing Cluster Policy Compliance with Python\nDESCRIPTION: This method returns the policy compliance status of all clusters that use a specific policy. It supports pagination to handle large numbers of clusters and allows filtering by policy ID.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/policy_compliance_for_clusters.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef list_compliance(policy_id: str [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[ClusterCompliance]\n```\n\n----------------------------------------\n\nTITLE: Listing Apps in Databricks\nDESCRIPTION: Lists all apps in the workspace. Supports pagination through optional page_size and page_token parameters. Returns an iterator over App objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nw.apps.list( [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[App]\n```\n\n----------------------------------------\n\nTITLE: Starting a Conversation in Genie using Python\nDESCRIPTION: This method starts a new conversation in a Genie space. It requires the space ID and the initial message content. It returns a long-running operation waiter for GenieMessage.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/genie.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: start_conversation(space_id: str, content: str) -> Wait[GenieMessage]\n\n        Start conversation.\n\n        Start a new conversation.\n\n        :param space_id: str\n          The ID associated with the Genie space where you want to start a conversation.\n        :param content: str\n          The text of the message that starts the conversation.\n\n        :returns:\n          Long-running operation waiter for :class:`GenieMessage`.\n          See :method:wait_get_message_genie_completed for more details.\n```\n\n----------------------------------------\n\nTITLE: Cancelling All Job Runs with Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to cancel all runs of a specific job using the Databricks SDK for Python. It requires a configured WorkspaceClient and the job_id of the target job. The cancel_all_runs method is used to asynchronously cancel all active runs of the specified job.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/jobs/jobs.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import jobs\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\ncluster_id = (\n    w.clusters.ensure_cluster_is_running(os.environ[\"DATABRICKS_CLUSTER_ID\"]) and os.environ[\"DATABRICKS_CLUSTER_ID\"]\n)\n\ncreated_job = w.jobs.create(\n    name=f\"sdk-{time.time_ns()}\",\n    tasks=[\n        jobs.Task(\n            description=\"test\",\n            existing_cluster_id=cluster_id,\n            notebook_task=jobs.NotebookTask(notebook_path=notebook_path),\n            task_key=\"test\",\n            timeout_seconds=0,\n        )\n    ],\n)\n\nw.jobs.cancel_all_runs(job_id=created_job.job_id)\n\n# cleanup\nw.jobs.delete(job_id=created_job.job_id)\n```\n\n----------------------------------------\n\nTITLE: Canceling a Monitor Refresh - Databricks SDK (Python)\nDESCRIPTION: This method cancels an active monitor refresh given a table name and refresh ID. The caller needs specific permissions including ownership or USE privileges on the catalog and schema. It must be invoked from the workspace where the monitor was created.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/quality_monitors.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: cancel_refresh(table_name: str, refresh_id: str)\n\n        Cancel refresh.\n\n        Cancel an active monitor refresh for the given refresh ID.\n\n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n        - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent schema - be an\n        owner of the table\n\n        Additionally, the call must be made from the workspace where the monitor was created.\n\n        :param table_name: str\n          Full name of the table.\n        :param refresh_id: str\n          ID of the refresh.\n```\n\n----------------------------------------\n\nTITLE: Update Legacy Access Disablement Status (Python)\nDESCRIPTION: Updates the legacy access disablement status with the provided settings. The allow_missing parameter should always be set to true for Settings API. The field_mask parameter specifies which fields to update.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/disable_legacy_access.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(allow_missing: bool, setting: DisableLegacyAccess, field_mask: str) -> DisableLegacyAccess\n\n    Update Legacy Access Disablement Status.\n\n    Updates legacy access disablement status.\n\n    :param allow_missing: bool\n      This should always be set to true for Settings API. Added for AIP compliance.\n    :param setting: :class:`DisableLegacyAccess`\n    :param field_mask: str\n      The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n      field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n      `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n      the entire collection field can be specified. Field names must exactly match the resource field\n      names.\n\n      A field mask of `*` indicates full replacement. It’s recommended to always explicitly list the\n      fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n      changes in the future.\n\n    :returns: :class:`DisableLegacyAccess`\n```\n\n----------------------------------------\n\nTITLE: Auto-generated class: UnpinClusterResponse in Python\nDESCRIPTION: This section documents the `UnpinClusterResponse` class within the Databricks SDK for Python. Both documented and undocumented members are included in the documentation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_40\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: UnpinClusterResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining JobSourceDirtyState Enum Python\nDESCRIPTION: This code snippet defines an enumeration called `JobSourceDirtyState` representing the dirty state of a job source. Possible values include `DISCONNECTED` and `NOT_SYNCED`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: JobSourceDirtyState\n\n   Dirty state indicates the job is not fully synced with the job specification in the remote repository.\n   Possible values are: * `NOT_SYNCED`: The job is not yet synced with the remote job specification. Import the remote job specification from UI to make the job fully synced. * `DISCONNECTED`: The job is temporary disconnected from the remote job specification and is allowed for live edit. Import the remote job specification again from UI to make the job fully synced.\n\n   .. py:attribute:: DISCONNECTED\n      :value: \"DISCONNECTED\"\n\n   .. py:attribute:: NOT_SYNCED\n      :value: \"NOT_SYNCED\"\n```\n\n----------------------------------------\n\nTITLE: Updating Table Monitor Python\nDESCRIPTION: The `update` method modifies the configuration of a table monitor. It requires specific permissions on the parent catalog and schema, ownership of the table, and the caller must be the original creator of the monitor in the workspace where it was created. Certain fields, like output asset identifiers, cannot be updated.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/quality_monitors.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(table_name: str, output_schema_name: str [, baseline_table_name: Optional[str], custom_metrics: Optional[List[MonitorMetric]], dashboard_id: Optional[str], data_classification_config: Optional[MonitorDataClassificationConfig], inference_log: Optional[MonitorInferenceLog], notifications: Optional[MonitorNotifications], schedule: Optional[MonitorCronSchedule], slicing_exprs: Optional[List[str]], snapshot: Optional[MonitorSnapshot], time_series: Optional[MonitorTimeSeries]]) -> MonitorInfo\n\n        Update a table monitor.\n\n        Updates a monitor for the specified table.\n\n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n        - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent schema - be an\n        owner of the table.\n\n        Additionally, the call must be made from the workspace where the monitor was created, and the caller\n        must be the original creator of the monitor.\n\n        Certain configuration fields, such as output asset identifiers, cannot be updated.\n\n        :param table_name: str\n          Full name of the table.\n        :param output_schema_name: str\n          Schema where output metric tables are created.\n        :param baseline_table_name: str (optional)\n          Name of the baseline table from which drift metrics are computed from. Columns in the monitored\n          table should also be present in the baseline table.\n        :param custom_metrics: List[:class:`MonitorMetric`] (optional)\n          Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics\n          (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).\n        :param dashboard_id: str (optional)\n          Id of dashboard that visualizes the computed metrics. This can be empty if the monitor is in PENDING\n          state.\n        :param data_classification_config: :class:`MonitorDataClassificationConfig` (optional)\n          The data classification config for the monitor.\n        :param inference_log: :class:`MonitorInferenceLog` (optional)\n          Configuration for monitoring inference logs.\n        :param notifications: :class:`MonitorNotifications` (optional)\n          The notification settings for the monitor.\n        :param schedule: :class:`MonitorCronSchedule` (optional)\n          The schedule for automatically updating and refreshing metric tables.\n        :param slicing_exprs: List[str] (optional)\n          List of column expressions to slice data with for targeted analysis. The data is grouped by each\n          expression independently, resulting in a separate slice for each predicate and its complements. For\n          high-cardinality columns, only the top 100 unique values by frequency will generate slices.\n        :param snapshot: :class:`MonitorSnapshot` (optional)\n          Configuration for monitoring snapshot tables.\n        :param time_series: :class:`MonitorTimeSeries` (optional)\n          Configuration for monitoring time series tables.\n\n        :returns: :class:`MonitorInfo`\n```\n\n----------------------------------------\n\nTITLE: Defining Condition Enum Python\nDESCRIPTION: This code snippet defines an enumeration called `Condition` with possible values `ALL_UPDATED` and `ANY_UPDATED`, representing different conditions related to job updates.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: Condition\n\n   .. py:attribute:: ALL_UPDATED\n      :value: \"ALL_UPDATED\"\n\n   .. py:attribute:: ANY_UPDATED\n      :value: \"ANY_UPDATED\"\n```\n\n----------------------------------------\n\nTITLE: List Webhooks Usage Example (Python)\nDESCRIPTION: This example demonstrates how to list all registry webhooks using the Databricks SDK. It initializes a `WorkspaceClient` and then uses the `list_webhooks` method to retrieve a list of webhooks. The example requires the `databricks-sdk` library and utilizes the `ml.ListWebhooksRequest` class.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/model_registry.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import ml\n\nw = WorkspaceClient()\n\nall = w.model_registry.list_webhooks(ml.ListWebhooksRequest())\n```\n\n----------------------------------------\n\nTITLE: Defining Enum: AibiDashboardEmbeddingAccessPolicyAccessPolicyType - Python\nDESCRIPTION: This Python enum defines the possible access policy types for A/B embedding dashboards. It includes options for allowing all domains, allowing approved domains, and denying all domains. These values are used to control access to embedded A/B dashboards.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: AibiDashboardEmbeddingAccessPolicyAccessPolicyType\n\n   .. py:attribute:: ALLOW_ALL_DOMAINS\n      :value: \"ALLOW_ALL_DOMAINS\"\n\n   .. py:attribute:: ALLOW_APPROVED_DOMAINS\n      :value: \"ALLOW_APPROVED_DOMAINS\"\n\n   .. py:attribute:: DENY_ALL_DOMAINS\n      :value: \"DENY_ALL_DOMAINS\"\n```\n\n----------------------------------------\n\nTITLE: Defining EndpointType Enum in Python\nDESCRIPTION: Defines an enumeration `EndpointType` representing the type of an endpoint, which can be STANDARD.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/vectorsearch.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EndpointType\n\n   Type of endpoint.\n\n   .. py:attribute:: STANDARD\n      :value: \"STANDARD\"\n```\n\n----------------------------------------\n\nTITLE: Auto-generated class: UnpinCluster in Python\nDESCRIPTION: This section documents the `UnpinCluster` class within the Databricks SDK for Python. Members and undocumented members are included in the documentation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_39\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: UnpinCluster\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Getting an Alert by ID with Databricks SDK\nDESCRIPTION: This snippet demonstrates how to retrieve a Databricks SQL alert by its ID using the Databricks SDK for Python. It creates a query and an alert, then retrieves the alert using its ID. Finally, it cleans up by deleting the query and the alert.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/alerts.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import sql\n\nw = WorkspaceClient()\n\nsrcs = w.data_sources.list()\n\nquery = w.queries.create(\n    query=sql.CreateQueryRequestQuery(\n        display_name=f\"sdk-{time.time_ns()}\",\n        warehouse_id=srcs[0].warehouse_id,\n        description=\"test query from Go SDK\",\n        query_text=\"SELECT 1\",\n    )\n)\n\nalert = w.alerts.create(\n    alert=sql.CreateAlertRequestAlert(\n        condition=sql.AlertCondition(\n            operand=sql.AlertConditionOperand(column=sql.AlertOperandColumn(name=\"1\")),\n            op=sql.AlertOperator.EQUAL,\n            threshold=sql.AlertConditionThreshold(value=sql.AlertOperandValue(double_value=1)),\n        ),\n        display_name=f\"sdk-{time.time_ns()}\",\n        query_id=query.id,\n    )\n)\n\nby_id = w.alerts.get(id=alert.id)\n\n# cleanup\nw.queries.delete(id=query.id)\nw.alerts.delete(id=alert.id)\n```\n\n----------------------------------------\n\nTITLE: DayOfWeek Enum Definition in Python\nDESCRIPTION: Defines an enumeration of days of the week, used for specifying the days when a pipeline restart is allowed. The restart can happen within a five-hour window starting at `start_hour`. If no days are specified, all days of the week are used.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: DayOfWeek\n\n   Days of week in which the restart is allowed to happen (within a five-hour window starting at start_hour). If not specified all days of the week will be used.\n\n   .. py:attribute:: FRIDAY\n      :value: \"FRIDAY\"\n\n   .. py:attribute:: MONDAY\n      :value: \"MONDAY\"\n\n   .. py:attribute:: SATURDAY\n      :value: \"SATURDAY\"\n\n   .. py:attribute:: SUNDAY\n      :value: \"SUNDAY\"\n\n   .. py:attribute:: THURSDAY\n      :value: \"THURSDAY\"\n\n   .. py:attribute:: TUESDAY\n      :value: \"TUESDAY\"\n\n   .. py:attribute:: WEDNESDAY\n      :value: \"WEDNESDAY\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Provider with Databricks SDK in Python\nDESCRIPTION: This code snippet demonstrates how to create a new data provider using the Databricks SDK in Python. It initializes a WorkspaceClient, defines a recipient profile string, creates a provider with a unique name using the current timestamp, and then cleans up by deleting the created provider. It shows the basic steps to use the create and delete methods.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/providers.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\npublic_share_recipient = \"\"\"{\n        \"shareCredentialsVersion\":1,\n        \"bearerToken\":\"dapiabcdefghijklmonpqrstuvwxyz\",\n        \"endpoint\":\"https://sharing.delta.io/delta-sharing/\"\n    }\n\"\"\"\n\ncreated = w.providers.create(name=f\"sdk-{time.time_ns()}\", recipient_profile_str=public_share_recipient)\n\n# cleanup\nw.providers.delete(name=created.name)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom OAuth Application\nDESCRIPTION: This Python code demonstrates how to create a custom OAuth application using the `account_client.custom_app_integration.create` API in the Databricks SDK for Python. It initializes an `AccountClient`, prompts the user for account ID, username and password, and creates a new custom app with specified redirect URLs. It requires the Databricks SDK library.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport logging, getpass\nfrom databricks.sdk import AccountClient\naccount_client = AccountClient(host='https://accounts.cloud.databricks.com',\n                               account_id=input('Databricks Account ID: '),\n                               username=input('Username: '),\n                               password=getpass.getpass('Password: '))\ncustom_app = account_client.custom_app_integration.create(\n    name='awesome-app',\n    redirect_urls=[f'https://host.domain/path/to/callback'],\n    confidential=True)\nlogging.info(f'Created new custom app: '\n             f'--client_id {custom_app.client_id} '\n             f'--client_secret {custom_app.client_secret}')\n```\n\n----------------------------------------\n\nTITLE: Configuring Private OAuth Application (Terraform)\nDESCRIPTION: This Terraform configuration creates an Azure AD application configured as a private client for OAuth. It defines a web application with a redirect URI, creates an application password, and outputs both the application's client ID and secret.\nDependencies: AzureAD and Time providers\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_5\n\nLANGUAGE: hcl\nCODE:\n```\nresource \"azuread_application\" \"private_client\" {\n  display_name     = \"sample-oauth-app-private-client\"\n  owners           = [data.azuread_client_config.current.object_id]\n  sign_in_audience = \"AzureADMyOrg\"\n  web {\n    redirect_uris = [\"http://localhost:8080/\"]\n  }\n}\n\nresource \"time_rotating\" \"weekly\" {\n  rotation_days = 7\n}\n\nresource \"azuread_application_password\" \"private_client\" {\n  application_object_id = azuread_application.private_client.object_id\n  rotate_when_changed = {\n    rotation = time_rotating.weekly.id\n  }\n}\n\noutput \"private_client_id\" {\n  value = azuread_application.private_client.application_id\n}\n\noutput \"private_client_secret\" {\n  value = azuread_application_password.private_client.value\n  sensitive = true\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting a Storage Credential (Python)\nDESCRIPTION: This method deletes a storage credential from a metastore. The caller must be the owner of the storage credential. A force option is available to delete the credential even if it is not empty.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/catalog/storage_credentials.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmetastore_id: str, storage_credential_name: str [, force: Optional[bool]]\n```\n\n----------------------------------------\n\nTITLE: Enabling Experimental Async Token Refresh in Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to enable the experimental async token refresh feature in the Databricks SDK for Python. This feature can improve performance in certain scenarios. Note that this setting is experimental and might be removed in future releases.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport DATABRICKS_ENABLE_EXPERIMENTAL_ASYNC_TOKEN_REFRESH=1.\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_VIEW_METADATA attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_VIEW_METADATA attribute for the PermissionLevel class. It represents a permission level that allows viewing metadata of resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_VIEW_METADATA\n      :value: \"CAN_VIEW_METADATA\"\n```\n\n----------------------------------------\n\nTITLE: Get ESM Enablement Setting (Python)\nDESCRIPTION: Retrieves the enhanced security monitoring setting using the get method of the EsmEnablementAPI. It accepts an optional etag parameter for versioning and returns an EsmEnablementSetting object. The etag is crucial for managing concurrent updates.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/esm_enablement.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get( [, etag: Optional[str]]) -> EsmEnablementSetting\n\n    Get the enhanced security monitoring setting.\n        \n    Gets the enhanced security monitoring setting.\n        \n    :param etag: str (optional)\n      etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n      optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n      each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n      to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n      request, and pass it with the DELETE request to identify the rule set version you are deleting.\n        \n    :returns: :class:`EsmEnablementSetting`\n```\n\n----------------------------------------\n\nTITLE: Settings API Property: restrict_workspace_admins\nDESCRIPTION: This property controls the capabilities of workspace admins, specifically regarding service principal personal access tokens and job ownership/run_as settings.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/settings.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n.. py:property:: restrict_workspace_admins\n    :type: RestrictWorkspaceAdminsAPI\n\n    The Restrict Workspace Admins setting lets you control the capabilities of workspace admins. With the\n    setting status set to ALLOW_ALL, workspace admins can create service principal personal access tokens on\n    behalf of any service principal in their workspace. Workspace admins can also change a job owner to any\n    user in their workspace. And they can change the job run_as setting to any user in their workspace or to a\n    service principal on which they have the Service Principal User role. With the setting status set to\n    RESTRICT_TOKENS_AND_JOB_RUN_AS, workspace admins can only create personal access tokens on behalf of\n    service principals they have the Service Principal User role on. They can also only change a job owner to\n    themselves. And they can change the job run_as setting to themselves or to a service principal on which\n    they have the Service Principal User role.\n```\n\n----------------------------------------\n\nTITLE: Defining ServedModelStateDeployment Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `ServedModelStateDeployment` with possible values ABORTED, CREATING, FAILED, READY, and RECOVERING. It represents the deployment state of a served model.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ServedModelStateDeployment\n\n   .. py:attribute:: ABORTED\n      :value: \"ABORTED\"\n\n   .. py:attribute:: CREATING\n      :value: \"CREATING\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: READY\n      :value: \"READY\"\n\n   .. py:attribute:: RECOVERING\n      :value: \"RECOVERING\"\n```\n\n----------------------------------------\n\nTITLE: Updating Personalization Request Status - Python\nDESCRIPTION: This method updates the status of a specific personalization request. It requires the `listing_id`, `request_id`, and the new `status`. Optionally, a `reason` and `share` can be provided. It returns an `UpdatePersonalizationRequestResponse` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_personalization_requests.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(listing_id: str, request_id: str, status: PersonalizationRequestStatus [, reason: Optional[str], share: Optional[ShareInfo]]) -> UpdatePersonalizationRequestResponse\n\n    Update personalization request status.\n\n    Update personalization request. This method only permits updating the status of the request.\n\n    :param listing_id: str\n    :param request_id: str\n    :param status: :class:`PersonalizationRequestStatus`\n    :param reason: str (optional)\n    :param share: :class:`ShareInfo` (optional)\n\n    :returns: :class:`UpdatePersonalizationRequestResponse`\n```\n\n----------------------------------------\n\nTITLE: Deleting Restrict Workspace Admins Setting in Python\nDESCRIPTION: Deletes the restrict workspace admins setting. Requires a fresh etag obtained from a GET request to handle concurrent updates. The etag should be provided as a query parameter.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/restrict_workspace_admins.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsettings.restrict_workspace_admins.delete(etag: Optional[str]) -> DeleteRestrictWorkspaceAdminsSettingResponse\n```\n\n----------------------------------------\n\nTITLE: Defining Enum: EgressNetworkPolicyInternetAccessPolicyInternetDestinationInternetDestinationType - Python\nDESCRIPTION: This Python enum defines the destination type for internet access policies, specifying if the destination is a Fully Qualified Domain Name (FQDN). This enum is used when configuring egress network policies to filter internet access based on destination type.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EgressNetworkPolicyInternetAccessPolicyInternetDestinationInternetDestinationType\n\n   .. py:attribute:: FQDN\n      :value: \"FQDN\"\n```\n\n----------------------------------------\n\nTITLE: Defining AppResourceSqlWarehouseSqlWarehousePermission Enum (Python)\nDESCRIPTION: This Python code defines an enumeration `AppResourceSqlWarehouseSqlWarehousePermission` with values representing the permission level that can be granted on a SQL Warehouse resource associated with an App, such as `CAN_MANAGE`, `CAN_USE`, and `IS_OWNER`. These permissions control access and actions allowed on the SQL warehouse.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/apps.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: AppResourceSqlWarehouseSqlWarehousePermission\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_USE\n      :value: \"CAN_USE\"\n\n   .. py:attribute:: IS_OWNER\n      :value: \"IS_OWNER\"\n```\n\n----------------------------------------\n\nTITLE: Defining DataSecurityMode Enum in Python\nDESCRIPTION: This code defines an enumeration (`DataSecurityMode`) for specifying the data security mode of a Databricks cluster. It details modes like DATA_SECURITY_MODE_AUTO, DATA_SECURITY_MODE_STANDARD, DATA_SECURITY_MODE_DEDICATED, NONE, SINGLE_USER, and USER_ISOLATION, as well as deprecated modes for legacy compatibility. These modes determine the level of data governance and isolation within the cluster.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: DataSecurityMode\n\n   Data security mode decides what data governance model to use when accessing data from a cluster.\n   The following modes can only be used when `kind = CLASSIC_PREVIEW`. * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration. * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`. * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.\n   The following modes can be used regardless of `kind`. * `NONE`: No security isolation for multiple users sharing the cluster. Data governance features are not available in this mode. * `SINGLE_USER`: A secure cluster that can only be exclusively used by a single user specified in `single_user_name`. Most programming languages, cluster features and data governance features are available in this mode. * `USER_ISOLATION`: A secure cluster that can be shared by multiple users. Cluster users are fully isolated so that they cannot see each other's data and credentials. Most data governance features are supported in this mode. But programming languages and cluster features might be limited.\n   The following modes are deprecated starting with Databricks Runtime 15.0 and will be removed for future Databricks Runtime versions:\n   * `LEGACY_TABLE_ACL`: This mode is for users migrating from legacy Table ACL clusters. * `LEGACY_PASSTHROUGH`: This mode is for users migrating from legacy Passthrough on high concurrency clusters. * `LEGACY_SINGLE_USER`: This mode is for users migrating from legacy Passthrough on standard clusters. * `LEGACY_SINGLE_USER_STANDARD`: This mode provides a way that doesn’t have UC nor passthrough enabled.\n\n   .. py:attribute:: DATA_SECURITY_MODE_AUTO\n      :value: \"DATA_SECURITY_MODE_AUTO\"\n\n   .. py:attribute:: DATA_SECURITY_MODE_DEDICATED\n      :value: \"DATA_SECURITY_MODE_DEDICATED\"\n\n   .. py:attribute:: DATA_SECURITY_MODE_STANDARD\n      :value: \"DATA_SECURITY_MODE_STANDARD\"\n\n   .. py:attribute:: LEGACY_PASSTHROUGH\n      :value: \"LEGACY_PASSTHROUGH\"\n\n   .. py:attribute:: LEGACY_SINGLE_USER\n      :value: \"LEGACY_SINGLE_USER\"\n\n   .. py:attribute:: LEGACY_SINGLE_USER_STANDARD\n      :value: \"LEGACY_SINGLE_USER_STANDARD\"\n\n   .. py:attribute:: LEGACY_TABLE_ACL\n      :value: \"LEGACY_TABLE_ACL\"\n\n   .. py:attribute:: NONE\n      :value: \"NONE\"\n\n   .. py:attribute:: SINGLE_USER\n      :value: \"SINGLE_USER\"\n\n   .. py:attribute:: USER_ISOLATION\n      :value: \"USER_ISOLATION\"\n```\n\n----------------------------------------\n\nTITLE: Defining Cluster Termination Reason Constants in Python\nDESCRIPTION: This code snippet defines a series of constants in Python representing possible cluster termination reasons. These constants are used to categorize and handle different failure scenarios within the Databricks environment. The values are string literals describing specific reasons such as client errors, cloud failures, or service faults.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n   .. py:attribute:: CONTROL_PLANE_REQUEST_FAILURE\n      :value: \"CONTROL_PLANE_REQUEST_FAILURE\"\n\n   .. py:attribute:: CONTROL_PLANE_REQUEST_FAILURE_DUE_TO_MISCONFIG\n      :value: \"CONTROL_PLANE_REQUEST_FAILURE_DUE_TO_MISCONFIG\"\n\n   .. py:attribute:: DATABASE_CONNECTION_FAILURE\n      :value: \"DATABASE_CONNECTION_FAILURE\"\n\n   .. py:attribute:: DATA_ACCESS_CONFIG_CHANGED\n      :value: \"DATA_ACCESS_CONFIG_CHANGED\"\n\n   .. py:attribute:: DBFS_COMPONENT_UNHEALTHY\n      :value: \"DBFS_COMPONENT_UNHEALTHY\"\n\n   .. py:attribute:: DISASTER_RECOVERY_REPLICATION\n      :value: \"DISASTER_RECOVERY_REPLICATION\"\n\n   .. py:attribute:: DOCKER_CONTAINER_CREATION_EXCEPTION\n      :value: \"DOCKER_CONTAINER_CREATION_EXCEPTION\"\n\n   .. py:attribute:: DOCKER_IMAGE_PULL_FAILURE\n      :value: \"DOCKER_IMAGE_PULL_FAILURE\"\n\n   .. py:attribute:: DOCKER_IMAGE_TOO_LARGE_FOR_INSTANCE_EXCEPTION\n      :value: \"DOCKER_IMAGE_TOO_LARGE_FOR_INSTANCE_EXCEPTION\"\n\n   .. py:attribute:: DOCKER_INVALID_OS_EXCEPTION\n      :value: \"DOCKER_INVALID_OS_EXCEPTION\"\n\n   .. py:attribute:: DRIVER_EVICTION\n      :value: \"DRIVER_EVICTION\"\n\n   .. py:attribute:: DRIVER_LAUNCH_TIMEOUT\n      :value: \"DRIVER_LAUNCH_TIMEOUT\"\n\n   .. py:attribute:: DRIVER_NODE_UNREACHABLE\n      :value: \"DRIVER_NODE_UNREACHABLE\"\n\n   .. py:attribute:: DRIVER_OUT_OF_DISK\n      :value: \"DRIVER_OUT_OF_DISK\"\n\n   .. py:attribute:: DRIVER_OUT_OF_MEMORY\n      :value: \"DRIVER_OUT_OF_MEMORY\"\n\n   .. py:attribute:: DRIVER_POD_CREATION_FAILURE\n      :value: \"DRIVER_POD_CREATION_FAILURE\"\n\n   .. py:attribute:: DRIVER_UNEXPECTED_FAILURE\n      :value: \"DRIVER_UNEXPECTED_FAILURE\"\n\n   .. py:attribute:: DRIVER_UNREACHABLE\n      :value: \"DRIVER_UNREACHABLE\"\n\n   .. py:attribute:: DRIVER_UNRESPONSIVE\n      :value: \"DRIVER_UNRESPONSIVE\"\n\n   .. py:attribute:: DYNAMIC_SPARK_CONF_SIZE_EXCEEDED\n      :value: \"DYNAMIC_SPARK_CONF_SIZE_EXCEEDED\"\n\n   .. py:attribute:: EOS_SPARK_IMAGE\n      :value: \"EOS_SPARK_IMAGE\"\n\n   .. py:attribute:: EXECUTION_COMPONENT_UNHEALTHY\n      :value: \"EXECUTION_COMPONENT_UNHEALTHY\"\n\n   .. py:attribute:: EXECUTOR_POD_UNSCHEDULED\n      :value: \"EXECUTOR_POD_UNSCHEDULED\"\n\n   .. py:attribute:: GCP_API_RATE_QUOTA_EXCEEDED\n      :value: \"GCP_API_RATE_QUOTA_EXCEEDED\"\n\n   .. py:attribute:: GCP_FORBIDDEN\n      :value: \"GCP_FORBIDDEN\"\n\n   .. py:attribute:: GCP_IAM_TIMEOUT\n      :value: \"GCP_IAM_TIMEOUT\"\n\n   .. py:attribute:: GCP_INACCESSIBLE_KMS_KEY_FAILURE\n      :value: \"GCP_INACCESSIBLE_KMS_KEY_FAILURE\"\n\n   .. py:attribute:: GCP_INSUFFICIENT_CAPACITY\n      :value: \"GCP_INSUFFICIENT_CAPACITY\"\n\n   .. py:attribute:: GCP_IP_SPACE_EXHAUSTED\n      :value: \"GCP_IP_SPACE_EXHAUSTED\"\n\n   .. py:attribute:: GCP_KMS_KEY_PERMISSION_DENIED\n      :value: \"GCP_KMS_KEY_PERMISSION_DENIED\"\n\n   .. py:attribute:: GCP_NOT_FOUND\n      :value: \"GCP_NOT_FOUND\"\n\n   .. py:attribute:: GCP_QUOTA_EXCEEDED\n      :value: \"GCP_QUOTA_EXCEEDED\"\n\n   .. py:attribute:: GCP_RESOURCE_QUOTA_EXCEEDED\n      :value: \"GCP_RESOURCE_QUOTA_EXCEEDED\"\n\n   .. py:attribute:: GCP_SERVICE_ACCOUNT_ACCESS_DENIED\n      :value: \"GCP_SERVICE_ACCOUNT_ACCESS_DENIED\"\n\n   .. py:attribute:: GCP_SERVICE_ACCOUNT_DELETED\n      :value: \"GCP_SERVICE_ACCOUNT_DELETED\"\n\n   .. py:attribute:: GCP_SERVICE_ACCOUNT_NOT_FOUND\n      :value: \"GCP_SERVICE_ACCOUNT_NOT_FOUND\"\n\n   .. py:attribute:: GCP_SUBNET_NOT_READY\n      :value: \"GCP_SUBNET_NOT_READY\"\n\n   .. py:attribute:: GCP_TRUSTED_IMAGE_PROJECTS_VIOLATED\n      :value: \"GCP_TRUSTED_IMAGE_PROJECTS_VIOLATED\"\n\n   .. py:attribute:: GKE_BASED_CLUSTER_TERMINATION\n      :value: \"GKE_BASED_CLUSTER_TERMINATION\"\n\n   .. py:attribute:: GLOBAL_INIT_SCRIPT_FAILURE\n      :value: \"GLOBAL_INIT_SCRIPT_FAILURE\"\n\n   .. py:attribute:: HIVE_METASTORE_PROVISIONING_FAILURE\n      :value: \"HIVE_METASTORE_PROVISIONING_FAILURE\"\n\n   .. py:attribute:: IMAGE_PULL_PERMISSION_DENIED\n      :value: \"IMAGE_PULL_PERMISSION_DENIED\"\n\n   .. py:attribute:: INACTIVITY\n      :value: \"INACTIVITY\"\n\n   .. py:attribute:: INIT_CONTAINER_NOT_FINISHED\n      :value: \"INIT_CONTAINER_NOT_FINISHED\"\n\n   .. py:attribute:: INIT_SCRIPT_FAILURE\n      :value: \"INIT_SCRIPT_FAILURE\"\n\n   .. py:attribute:: INSTANCE_POOL_CLUSTER_FAILURE\n      :value: \"INSTANCE_POOL_CLUSTER_FAILURE\"\n\n   .. py:attribute:: INSTANCE_POOL_MAX_CAPACITY_REACHED\n      :value: \"INSTANCE_POOL_MAX_CAPACITY_REACHED\"\n\n   .. py:attribute:: INSTANCE_POOL_NOT_FOUND\n      :value: \"INSTANCE_POOL_NOT_FOUND\"\n\n   .. py:attribute:: INSTANCE_UNREACHABLE\n      :value: \"INSTANCE_UNREACHABLE\"\n\n   .. py:attribute:: INSTANCE_UNREACHABLE_DUE_TO_MISCONFIG\n      :value: \"INSTANCE_UNREACHABLE_DUE_TO_MISCONFIG\"\n\n   .. py:attribute:: INTERNAL_CAPACITY_FAILURE\n      :value: \"INTERNAL_CAPACITY_FAILURE\"\n\n   .. py:attribute:: INTERNAL_ERROR\n      :value: \"INTERNAL_ERROR\"\n\n   .. py:attribute:: INVALID_ARGUMENT\n      :value: \"INVALID_ARGUMENT\"\n\n   .. py:attribute:: INVALID_AWS_PARAMETER\n      :value: \"INVALID_AWS_PARAMETER\"\n\n   .. py:attribute:: INVALID_INSTANCE_PLACEMENT_PROTOCOL\n      :value: \"INVALID_INSTANCE_PLACEMENT_PROTOCOL\"\n\n   .. py:attribute:: INVALID_SPARK_IMAGE\n      :value: \"INVALID_SPARK_IMAGE\"\n\n   .. py:attribute:: INVALID_WORKER_IMAGE_FAILURE\n      :value: \"INVALID_WORKER_IMAGE_FAILURE\"\n\n   .. py:attribute:: IN_PENALTY_BOX\n      :value: \"IN_PENALTY_BOX\"\n\n   .. py:attribute:: IP_EXHAUSTION_FAILURE\n      :value: \"IP_EXHAUSTION_FAILURE\"\n\n   .. py:attribute:: JOB_FINISHED\n      :value: \"JOB_FINISHED\"\n\n   .. py:attribute:: K8S_AUTOSCALING_FAILURE\n      :value: \"K8S_AUTOSCALING_FAILURE\"\n\n   .. py:attribute:: K8S_DBR_CLUSTER_LAUNCH_TIMEOUT\n      :value: \"K8S_DBR_CLUSTER_LAUNCH_TIMEOUT\"\n\n   .. py:attribute:: LAZY_ALLOCATION_TIMEOUT\n      :value: \"LAZY_ALLOCATION_TIMEOUT\"\n\n   .. py:attribute:: MAINTENANCE_MODE\n      :value: \"MAINTENANCE_MODE\"\n\n   .. py:attribute:: METASTORE_COMPONENT_UNHEALTHY\n      :value: \"METASTORE_COMPONENT_UNHEALTHY\"\n\n   .. py:attribute:: NEPHOS_RESOURCE_MANAGEMENT\n      :value: \"NEPHOS_RESOURCE_MANAGEMENT\"\n\n   .. py:attribute:: NETVISOR_SETUP_TIMEOUT\n      :value: \"NETVISOR_SETUP_TIMEOUT\"\n\n   .. py:attribute:: NETWORK_CONFIGURATION_FAILURE\n      :value: \"NETWORK_CONFIGURATION_FAILURE\"\n\n   .. py:attribute:: NFS_MOUNT_FAILURE\n      :value: \"NFS_MOUNT_FAILURE\"\n\n   .. py:attribute:: NO_MATCHED_K8S\n      :value: \"NO_MATCHED_K8S\"\n\n   .. py:attribute:: NO_MATCHED_K8S_TESTING_TAG\n      :value: \"NO_MATCHED_K8S_TESTING_TAG\"\n\n   .. py:attribute:: NPIP_TUNNEL_SETUP_FAILURE\n      :value: \"NPIP_TUNNEL_SETUP_FAILURE\"\n\n   .. py:attribute:: NPIP_TUNNEL_TOKEN_FAILURE\n      :value: \"NPIP_TUNNEL_TOKEN_FAILURE\"\n\n   .. py:attribute:: POD_ASSIGNMENT_FAILURE\n      :value: \"POD_ASSIGNMENT_FAILURE\"\n\n   .. py:attribute:: POD_SCHEDULING_FAILURE\n      :value: \"POD_SCHEDULING_FAILURE\"\n\n   .. py:attribute:: REQUEST_REJECTED\n      :value: \"REQUEST_REJECTED\"\n\n   .. py:attribute:: REQUEST_THROTTLED\n      :value: \"REQUEST_THROTTLED\"\n\n   .. py:attribute:: RESOURCE_USAGE_BLOCKED\n      :value: \"RESOURCE_USAGE_BLOCKED\"\n\n   .. py:attribute:: SECRET_CREATION_FAILURE\n      :value: \"SECRET_CREATION_FAILURE\"\n\n   .. py:attribute:: SECRET_RESOLUTION_ERROR\n      :value: \"SECRET_RESOLUTION_ERROR\"\n\n   .. py:attribute:: SECURITY_DAEMON_REGISTRATION_EXCEPTION\n      :value: \"SECURITY_DAEMON_REGISTRATION_EXCEPTION\"\n\n   .. py:attribute:: SELF_BOOTSTRAP_FAILURE\n      :value: \"SELF_BOOTSTRAP_FAILURE\"\n\n   .. py:attribute:: SERVERLESS_LONG_RUNNING_TERMINATED\n      :value: \"SERVERLESS_LONG_RUNNING_TERMINATED\"\n\n   .. py:attribute:: SKIPPED_SLOW_NODES\n      :value: \"SKIPPED_SLOW_NODES\"\n\n   .. py:attribute:: SLOW_IMAGE_DOWNLOAD\n      :value: \"SLOW_IMAGE_DOWNLOAD\"\n\n   .. py:attribute:: SPARK_ERROR\n      :value: \"SPARK_ERROR\"\n\n   .. py:attribute:: SPARK_IMAGE_DOWNLOAD_FAILURE\n      :value: \"SPARK_IMAGE_DOWNLOAD_FAILURE\"\n\n   .. py:attribute:: SPARK_IMAGE_DOWNLOAD_THROTTLED\n      :value: \"SPARK_IMAGE_DOWNLOAD_THROTTLED\"\n\n   .. py:attribute:: SPARK_IMAGE_NOT_FOUND\n      :value: \"SPARK_IMAGE_NOT_FOUND\"\n\n   .. py:attribute:: SPARK_STARTUP_FAILURE\n      :value: \"SPARK_STARTUP_FAILURE\"\n\n   .. py:attribute:: SPOT_INSTANCE_TERMINATION\n      :value: \"SPOT_INSTANCE_TERMINATION\"\n\n   .. py:attribute:: SSH_BOOTSTRAP_FAILURE\n      :value: \"SSH_BOOTSTRAP_FAILURE\"\n\n   .. py:attribute:: STORAGE_DOWNLOAD_FAILURE\n      :value: \"STORAGE_DOWNLOAD_FAILURE\"\n\n   .. py:attribute:: STORAGE_DOWNLOAD_FAILURE_DUE_TO_MISCONFIG\n      :value: \"STORAGE_DOWNLOAD_FAILURE_DUE_TO_MISCONFIG\"\n\n   .. py:attribute:: STORAGE_DOWNLOAD_FAILURE_SLOW\n      :value: \"STORAGE_DOWNLOAD_FAILURE_SLOW\"\n\n   .. py:attribute:: STORAGE_DOWNLOAD_FAILURE_THROTTLED\n      :value: \"STORAGE_DOWNLOAD_FAILURE_THROTTLED\"\n\n   .. py:attribute:: STS_CLIENT_SETUP_FAILURE\n      :value: \"STS_CLIENT_SETUP_FAILURE\"\n\n   .. py:attribute:: SUBNET_EXHAUSTED_FAILURE\n      :value: \"SUBNET_EXHAUSTED_FAILURE\"\n\n   .. py:attribute:: TEMPORARILY_UNAVAILABLE\n      :value: \"TEMPORARILY_UNAVAILABLE\"\n\n   .. py:attribute:: TRIAL_EXPIRED\n      :value: \"TRIAL_EXPIRED\"\n\n   .. py:attribute:: UNEXPECTED_LAUNCH_FAILURE\n      :value: \"UNEXPECTED_LAUNCH_FAILURE\"\n\n   .. py:attribute:: UNEXPECTED_POD_RECREATION\n      :value: \"UNEXPECTED_POD_RECREATION\"\n\n   .. py:attribute:: UNKNOWN\n      :value: \"UNKNOWN\"\n\n   .. py:attribute:: UNSUPPORTED_INSTANCE_TYPE\n      :value: \"UNSUPPORTED_INSTANCE_TYPE\"\n\n   .. py:attribute:: UPDATE_INSTANCE_PROFILE_FAILURE\n      :value: \"UPDATE_INSTANCE_PROFILE_FAILURE\"\n\n   .. py:attribute:: USER_INITIATED_VM_TERMINATION\n      :value: \"USER_INITIATED_VM_TERMINATION\"\n\n   .. py:attribute:: USER_REQUEST\n      :value: \"USER_REQUEST\"\n\n   .. py:attribute:: WORKER_SETUP_FAILURE\n      :value: \"WORKER_SETUP_FAILURE\"\n\n   .. py:attribute:: WORKSPACE_CANCELLED_ERROR\n      :value: \"WORKSPACE_CANCELLED_ERROR\"\n\n   .. py:attribute:: WORKSPACE_CONFIGURATION_ERROR\n      :value: \"WORKSPACE_CONFIGURATION_ERROR\"\n\n   .. py:attribute:: WORKSPACE_UPDATE\n      :value: \"WORKSPACE_UPDATE\"\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_RESTART attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_RESTART attribute for the PermissionLevel class. It represents a permission level that allows restarting resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_RESTART\n      :value: \"CAN_RESTART\"\n```\n\n----------------------------------------\n\nTITLE: Get Network Configuration by ID with AccountClient in Python\nDESCRIPTION: This code snippet demonstrates how to retrieve a Databricks network configuration by its ID using the AccountClient. It assumes a network configuration has already been created (as shown in the previous snippet). It initializes the AccountClient, creates a network configuration, and then calls the networks.get method with the network_id to retrieve the network configuration.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/networks.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nnetw = a.networks.create(\n    network_name=f\"sdk-{time.time_ns()}\",\n    vpc_id=hex(time.time_ns())[2:],\n    subnet_ids=[hex(time.time_ns())[2:], hex(time.time_ns())[2:]],\n    security_group_ids=[hex(time.time_ns())[2:]],\n)\n\nby_id = a.networks.get(network_id=netw.network_id)\n```\n\n----------------------------------------\n\nTITLE: Update a Metastore in Python\nDESCRIPTION: This snippet demonstrates how to update an existing metastore using the Databricks SDK.  It first creates a metastore, then updates its name, and finally cleans up by deleting the created metastore.  The new name is created to be unique using the current timestamp. It relies on the WorkspaceClient from the databricks.sdk.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/metastores.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.metastores.create(\n    name=f\"sdk-{time.time_ns()}\",\n    storage_root=\"s3://%s/%s\" % (os.environ[\"TEST_BUCKET\"], f\"sdk-{time.time_ns()}\"),\n)\n\n_ = w.metastores.update(id=created.metastore_id, new_name=f\"sdk-{time.time_ns()}\")\n\n# cleanup\nw.metastores.delete(id=created.metastore_id, force=True)\n```\n\n----------------------------------------\n\nTITLE: Cancelling Monitor Refresh in Python\nDESCRIPTION: Cancels an active monitor refresh given the table name and refresh ID.  Requires specific permissions on the table's parent catalog and schema, and must be called from the workspace where the monitor was created. It takes table_name and refresh_id as input.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/lakehouse_monitors.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: cancel_refresh(table_name: str, refresh_id: str)\n\n        Cancel refresh.\n        \n        Cancel an active monitor refresh for the given refresh ID.\n        \n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n        - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent schema - be an\n        owner of the table\n        \n        Additionally, the call must be made from the workspace where the monitor was created.\n        \n        :param table_name: str\n          Full name of the table.\n        :param refresh_id: str\n          ID of the refresh.\n```\n\n----------------------------------------\n\nTITLE: Getting a Message Query Result by Attachment in Genie using Python\nDESCRIPTION: This deprecated method retrieves the result of a SQL query associated with a specific attachment of a message in a Genie conversation.  It's only available if the message has a query attachment and its status is 'EXECUTING_QUERY' or 'COMPLETED'. The method returns a GenieGetMessageQueryResultResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/genie.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: get_message_query_result_by_attachment(space_id: str, conversation_id: str, message_id: str, attachment_id: str) -> GenieGetMessageQueryResultResponse\n\n        [Deprecated] Get conversation message SQL query result.\n\n        Get the result of SQL query if the message has a query attachment. This is only available if a message\n        has a query attachment and the message status is `EXECUTING_QUERY` OR `COMPLETED`.\n\n        :param space_id: str\n          Genie space ID\n        :param conversation_id: str\n          Conversation ID\n        :param message_id: str\n          Message ID\n        :param attachment_id: str\n          Attachment ID\n\n        :returns: :class:`GenieGetMessageQueryResultResponse`\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Logging with Databricks SDK\nDESCRIPTION: This snippet shows how to enable debug logging for the Databricks SDK for Python. It configures the logging module to output log messages at the DEBUG level and above to standard error. It then initializes a `WorkspaceClient` with debug options to truncate log messages and include headers. It logs the name of each cluster found when listing clusters.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport logging, sys\nlogging.basicConfig(stream=sys.stderr,\n                    level=logging.INFO,\n                    format='%(asctime)s [%(name)s][%(levelname)s] %(message)s')\nlogging.getLogger('databricks.sdk').setLevel(logging.DEBUG)\n\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(debug_truncate_bytes=1024, debug_headers=False)\nfor cluster in w.clusters.list():\n    logging.info(f'Found cluster: {cluster.cluster_name}')\n```\n\n----------------------------------------\n\nTITLE: Creating a Schema in Databricks\nDESCRIPTION: This code snippet demonstrates how to create a new schema within a specified catalog using the Databricks SDK for Python. It initializes the WorkspaceClient, creates a catalog, then creates a schema in that catalog. Finally, it cleans up by deleting both the schema and the catalog.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/schemas.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\ncreated_schema = w.schemas.create(name=f\"sdk-{time.time_ns()}\", catalog_name=created_catalog.name)\n\n# cleanup\nw.catalogs.delete(name=created_catalog.name, force=True)\nw.schemas.delete(full_name=created_schema.full_name)\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_MANAGE_PRODUCTION_VERSIONS attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_MANAGE_PRODUCTION_VERSIONS attribute for the PermissionLevel class. It represents a permission level that allows managing production versions of resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_MANAGE_PRODUCTION_VERSIONS\n      :value: \"CAN_MANAGE_PRODUCTION_VERSIONS\"\n```\n\n----------------------------------------\n\nTITLE: Updating App Permissions in Databricks using Python\nDESCRIPTION: This code snippet demonstrates how to update the permissions on an app using the `update_permissions` method. It accepts the app name and an optional access control list. The method returns the updated `AppPermissions` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update_permissions(app_name: str [, access_control_list: Optional[List[AppAccessControlRequest]]]) -> AppPermissions\n\n        Update app permissions.\n\n        Updates the permissions on an app. Apps can inherit permissions from their root object.\n\n        :param app_name: str\n          The app for which to get or manage permissions.\n        :param access_control_list: List[:class:`AppAccessControlRequest`] (optional)\n\n        :returns: :class:`AppPermissions`\n```\n\n----------------------------------------\n\nTITLE: Get OAuth Custom App Integration (Python)\nDESCRIPTION: Retrieves a Custom OAuth App Integration based on its integration ID. The method returns a GetCustomAppIntegrationOutput object containing the integration's details.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/custom_app_integration.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(integration_id: str) -> GetCustomAppIntegrationOutput\n\n    Get OAuth Custom App Integration.\n\n    Gets the Custom OAuth App Integration for the given integration id.\n\n    :param integration_id: str\n      The OAuth app integration ID.\n\n    :returns: :class:`GetCustomAppIntegrationOutput`\n```\n\n----------------------------------------\n\nTITLE: Defining EndpointStateReady Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `EndpointStateReady` with possible values NOT_READY and READY. It indicates whether an endpoint is ready to serve requests.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EndpointStateReady\n\n   .. py:attribute:: NOT_READY\n      :value: \"NOT_READY\"\n\n   .. py:attribute:: READY\n      :value: \"READY\"\n```\n\n----------------------------------------\n\nTITLE: Getting Notification Destination in Python\nDESCRIPTION: Retrieves a notification destination by its ID. Returns a NotificationDestination object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/notification_destinations.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(id: str) -> NotificationDestination\n\n    Get a notification destination.\n\n    Gets a notification destination.\n\n    :param id: str\n\n    :returns: :class:`NotificationDestination`\n```\n\n----------------------------------------\n\nTITLE: Defining REQUEST_AUTHZ_IDENTITY_SERVICE_IDENTITY attribute for RequestAuthzIdentity in Python\nDESCRIPTION: Defines the REQUEST_AUTHZ_IDENTITY_SERVICE_IDENTITY attribute for the RequestAuthzIdentity class. It represents the service identity to be used for authorization of the request on the server side.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: RequestAuthzIdentity\n\n   Defines the identity to be used for authZ of the request on the server side. See one pager for for more information: http://go/acl/service-identity\n\n   .. py:attribute:: REQUEST_AUTHZ_IDENTITY_SERVICE_IDENTITY\n      :value: \"REQUEST_AUTHZ_IDENTITY_SERVICE_IDENTITY\"\n```\n\n----------------------------------------\n\nTITLE: MarketplaceFileType Enum Definition in Python\nDESCRIPTION: Defines the file types that can be associated with Databricks Marketplace. It specifies APP, EMBEDDED_NOTEBOOK, and PROVIDER_ICON, representing the file formats for applications, embedded notebooks, and provider icons respectively.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: MarketplaceFileType\n\n   .. py:attribute:: APP\n      :value: \"APP\"\n\n   .. py:attribute:: EMBEDDED_NOTEBOOK\n      :value: \"EMBEDDED_NOTEBOOK\"\n\n   .. py:attribute:: PROVIDER_ICON\n      :value: \"PROVIDER_ICON\"\n```\n\n----------------------------------------\n\nTITLE: CleanRoomAssetForeignTable Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomAssetForeignTable dataclass, representing a foreign table asset within a Clean Room. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomAssetForeignTable\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining AlertOperator Enum (Python)\nDESCRIPTION: Defines an enumeration representing different alert operators such as EQUAL, GREATER_THAN, LESS_THAN, etc. These operators are used to define the conditions that trigger an alert based on query results. The class provides string values for each attribute.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: AlertOperator\n\n   .. py:attribute:: EQUAL\n      :value: \"EQUAL\"\n\n   .. py:attribute:: GREATER_THAN\n      :value: \"GREATER_THAN\"\n\n   .. py:attribute:: GREATER_THAN_OR_EQUAL\n      :value: \"GREATER_THAN_OR_EQUAL\"\n\n   .. py:attribute:: IS_NULL\n      :value: \"IS_NULL\"\n\n   .. py:attribute:: LESS_THAN\n      :value: \"LESS_THAN\"\n\n   .. py:attribute:: LESS_THAN_OR_EQUAL\n      :value: \"LESS_THAN_OR_EQUAL\"\n\n   .. py:attribute:: NOT_EQUAL\n      :value: \"NOT_EQUAL\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Marketplace File in Python\nDESCRIPTION: This Python method creates a new file within the Databricks Marketplace, associating it with a parent entity. It supports file types like provider icons and attached notebooks, requiring parameters such as the parent entity, file type, MIME type, and an optional display name. It returns a CreateFileResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_files.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(file_parent: FileParent, marketplace_file_type: MarketplaceFileType, mime_type: str [, display_name: Optional[str]]) -> CreateFileResponse\n\n        Create a file.\n\n        Create a file. Currently, only provider icons and attached notebooks are supported.\n\n        :param file_parent: :class:`FileParent`\n        :param marketplace_file_type: :class:`MarketplaceFileType`\n        :param mime_type: str\n        :param display_name: str (optional)\n\n        :returns: :class:`CreateFileResponse`\n```\n\n----------------------------------------\n\nTITLE: Exchange Token using CredentialsManagerAPI in Databricks SDK\nDESCRIPTION: Exchanges tokens with an Identity Provider to get a new access token. It requires specifying the partition ID, a list of token types being requested, and an array of scopes for the token request. The method returns an ExchangeTokenResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/credentials_manager.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef exchange_token(partition_id: PartitionId, token_type: List[TokenType], scopes: List[str]) -> ExchangeTokenResponse:\n    \"\"\"Exchange token.\n\n    Exchange tokens with an Identity Provider to get a new access token. It allows specifying scopes to\n    determine token permissions.\n\n    :param partition_id: :class:`PartitionId`\n      The partition of Credentials store\n    :param token_type: List[:class:`TokenType`]\n      A list of token types being requested\n    :param scopes: List[str]\n      Array of scopes for the token request.\n\n    :returns: :class:`ExchangeTokenResponse`\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating an App in Databricks using Python\nDESCRIPTION: This code snippet demonstrates how to create a new app within the Databricks environment using the `create` method of the `AppsAPI`.  It requires the `App` object to define the app and an optional boolean to specify if the app should be started after creation.  The method returns a long-running operation waiter for the app.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create( [, app: Optional[App], no_compute: Optional[bool]]) -> Wait[App]\n\n        Create an app.\n\n        Creates a new app.\n\n        :param app: :class:`App` (optional)\n        :param no_compute: bool (optional)\n          If true, the app will not be started after creation.\n\n        :returns:\n          Long-running operation waiter for :class:`App`.\n          See :method:wait_get_app_active for more details.\n```\n\n----------------------------------------\n\nTITLE: Listing All Fulfillments with ConsumerFulfillments.list() in Python\nDESCRIPTION: This method retrieves all listings fulfillments associated with a specified listing ID. A fulfillment represents a potential installation. The return is an iterator of ListingFulfillment objects. Optional parameters include page_size and page_token for pagination.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_fulfillments.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef list(listing_id: str [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[ListingFulfillment]:\n    \"\"\"List all listing fulfillments.\n\n    Get all listings fulfillments associated with a listing. A _fulfillment_ is a potential installation.\n    Standard installations contain metadata about the attached share or git repo. Only one of these fields\n    will be present. Personalized installations contain metadata about the attached share or git repo, as\n    well as the Delta Sharing recipient type.\n\n    :param listing_id: str\n    :param page_size: int (optional)\n    :param page_token: str (optional)\n\n    :returns: Iterator over :class:`ListingFulfillment`\n    \"\"\n```\n\n----------------------------------------\n\nTITLE: List Resource Quotas - Python\nDESCRIPTION: This method lists all resource quotas under a specified metastore in Unity Catalog. It returns an iterator over QuotaInfo objects. There are no service level agreements on the freshness of the counts returned, and the API does not trigger a refresh of quota counts.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/resource_quotas.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list_quotas( [, max_results: Optional[int], page_token: Optional[str]]) -> Iterator[QuotaInfo]\n\n    List all resource quotas under a metastore.\n\n    ListQuotas returns all quota values under the metastore. There are no SLAs on the freshness of the\n    counts returned. This API does not trigger a refresh of quota counts.\n\n    :param max_results: int (optional)\n      The number of quotas to return.\n    :param page_token: str (optional)\n      Opaque token for the next page of results.\n\n    :returns: Iterator over :class:`QuotaInfo`\n```\n\n----------------------------------------\n\nTITLE: Defining Job Run Trigger Types in Python\nDESCRIPTION: Defines the possible trigger types for Databricks job runs.  These include periodic schedules, one-time triggers, retry runs, runs triggered by Run Job tasks, file arrival triggers, table update triggers, and continuous restarts. These constants are used to specify how a job run was initiated.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n.. py:attribute:: FILE_ARRIVAL\n   :value: \"FILE_ARRIVAL\"\n\n.. py:attribute:: ONE_TIME\n   :value: \"ONE_TIME\"\n\n.. py:attribute:: PERIODIC\n   :value: \"PERIODIC\"\n\n.. py:attribute:: RETRY\n   :value: \"RETRY\"\n\n.. py:attribute:: RUN_JOB_TASK\n   :value: \"RUN_JOB_TASK\"\n\n.. py:attribute:: TABLE\n   :value: \"TABLE\"\n```\n\n----------------------------------------\n\nTITLE: Updating a Dashboard Widget in Databricks\nDESCRIPTION: This method updates an existing widget on a Databricks dashboard. It requires the widget ID, dashboard ID, widget options, and width. Optionally, you can specify text for textbox widgets or a visualization ID for widgets linked to a query visualization.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/dashboard_widgets.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(id: str, dashboard_id: str, options: WidgetOptions, width: int [, text: Optional[str], visualization_id: Optional[str]]) -> Widget\n\n    Update existing widget.\n\n    :param id: str\n      Widget ID returned by :method:dashboardwidgets/create\n    :param dashboard_id: str\n      Dashboard ID returned by :method:dashboards/create.\n    :param options: :class:`WidgetOptions`\n    :param width: int\n      Width of a widget\n    :param text: str (optional)\n      If this is a textbox widget, the application displays this text. This field is ignored if the widget\n      contains a visualization in the `visualization` field.\n    :param visualization_id: str (optional)\n      Query Vizualization ID returned by :method:queryvisualizations/create.\n\n    :returns: :class:`Widget`\n```\n\n----------------------------------------\n\nTITLE: Update Provider - Python\nDESCRIPTION: Updates an existing provider's profile in the Databricks Marketplace. It requires the provider's ID (string) and a ProviderInfo object containing the updated details. The method returns an UpdateProviderResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_providers.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(id: str, provider: ProviderInfo) -> UpdateProviderResponse\n\n    Update provider.\n\n    Update provider profile\n\n    :param id: str\n    :param provider: :class:`ProviderInfo`\n\n    :returns: :class:`UpdateProviderResponse`\n```\n\n----------------------------------------\n\nTITLE: Getting Instance Pool Information by ID\nDESCRIPTION: This code snippet demonstrates how to retrieve instance pool information using its ID. It creates an instance pool, fetches the details using the `get` method, and then cleans up by deleting the instance pool. This example illustrates how to use the API to retrieve details about a specific instance pool.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/instance_pools.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nsmallest = w.clusters.select_node_type(local_disk=True)\n\ncreated = w.instance_pools.create(instance_pool_name=f\"sdk-{time.time_ns()}\", node_type_id=smallest)\n\nby_id = w.instance_pools.get(instance_pool_id=created.instance_pool_id)\n\n# cleanup\nw.instance_pools.delete(instance_pool_id=created.instance_pool_id)\n```\n\n----------------------------------------\n\nTITLE: Defining URN for UserSchema in Python\nDESCRIPTION: Defines the URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER attribute for the UserSchema class. It represents the SCIM schema for a user with a URN string value.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: UserSchema\n\n   .. py:attribute:: URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER\n      :value: \"URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER\"\n```\n\n----------------------------------------\n\nTITLE: Defining ValidateCredentialResult enum in Python\nDESCRIPTION: Defines a Python enum called `ValidateCredentialResult` with possible values FAIL, PASS and SKIP. This enum is used for representing the result of a credential validation process.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ValidateCredentialResult\n\n   A enum represents the result of the file operation\n\n   .. py:attribute:: FAIL\n      :value: \"FAIL\"\n\n   .. py:attribute:: PASS\n      :value: \"PASS\"\n\n   .. py:attribute:: SKIP\n      :value: \"SKIP\"\n```\n\n----------------------------------------\n\nTITLE: Defining ValidationResultOperation enum in Python\nDESCRIPTION: Defines a Python enum called `ValidationResultOperation` with possible values DELETE, LIST, PATH_EXISTS, READ and WRITE. This enum is used to denote the type of file operation performed during validation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ValidationResultOperation\n\n   The operation tested.\n\n   .. py:attribute:: DELETE\n      :value: \"DELETE\"\n\n   .. py:attribute:: LIST\n      :value: \"LIST\"\n\n   .. py:attribute:: PATH_EXISTS\n      :value: \"PATH_EXISTS\"\n\n   .. py:attribute:: READ\n      :value: \"READ\"\n\n   .. py:attribute:: WRITE\n      :value: \"WRITE\"\n```\n\n----------------------------------------\n\nTITLE: Defining RunIf Constants in Python\nDESCRIPTION: This snippet defines constants representing conditions under which a task should be run once its dependencies are complete. Possible values include ALL_SUCCESS, AT_LEAST_ONE_SUCCESS, NONE_FAILED, ALL_DONE, AT_LEAST_ONE_FAILED, and ALL_FAILED.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nALL_DONE = \"ALL_DONE\"\n```\n\nLANGUAGE: python\nCODE:\n```\nALL_FAILED = \"ALL_FAILED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nALL_SUCCESS = \"ALL_SUCCESS\"\n```\n\nLANGUAGE: python\nCODE:\n```\nAT_LEAST_ONE_FAILED = \"AT_LEAST_ONE_FAILED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nAT_LEAST_ONE_SUCCESS = \"AT_LEAST_ONE_SUCCESS\"\n```\n\nLANGUAGE: python\nCODE:\n```\nNONE_FAILED = \"NONE_FAILED\"\n```\n\n----------------------------------------\n\nTITLE: Batch Get Providers in Databricks Marketplace (Python)\nDESCRIPTION: This method retrieves a batch of providers from the Databricks Marketplace, allowing for up to 50 provider IDs to be specified in a single request. It utilizes the BatchGetProvidersResponse object to return the results. The 'ids' parameter is a list of provider IDs to retrieve.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_providers.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: batch_get( [, ids: Optional[List[str]]]) -> BatchGetProvidersResponse\n\n    Get one batch of providers. One may specify up to 50 IDs per request.\n\n    Batch get a provider in the Databricks Marketplace with at least one visible listing.\n\n    :param ids: List[str] (optional)\n\n    :returns: :class:`BatchGetProvidersResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining ExportFormat Enum\nDESCRIPTION: Defines the supported export formats for workspace objects. Available options include AUTO, DBC, HTML, JUPYTER, RAW, R_MARKDOWN, and SOURCE. This enum is used when exporting workspace objects using the Databricks API.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/workspace.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ExportFormat\n\n   The format for workspace import and export.\n\n   .. py:attribute:: AUTO\n      :value: \"AUTO\"\n\n   .. py:attribute:: DBC\n      :value: \"DBC\"\n\n   .. py:attribute:: HTML\n      :value: \"HTML\"\n\n   .. py:attribute:: JUPYTER\n      :value: \"JUPYTER\"\n\n   .. py:attribute:: RAW\n      :value: \"RAW\"\n\n   .. py:attribute:: R_MARKDOWN\n      :value: \"R_MARKDOWN\"\n\n   .. py:attribute:: SOURCE\n      :value: \"SOURCE\"\n```\n\n----------------------------------------\n\nTITLE: Defining ServedModelInputWorkloadSize Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `ServedModelInputWorkloadSize` with possible values LARGE, MEDIUM, and SMALL. It indicates the size of the workload for a served model.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ServedModelInputWorkloadSize\n\n   .. py:attribute:: LARGE\n      :value: \"LARGE\"\n\n   .. py:attribute:: MEDIUM\n      :value: \"MEDIUM\"\n\n   .. py:attribute:: SMALL\n      :value: \"SMALL\"\n```\n\n----------------------------------------\n\nTITLE: Getting CspEnablementSetting Python\nDESCRIPTION: Retrieves the current Compliance Security Profile setting. Includes an optional `etag` parameter for optimistic concurrency control. Returns a `CspEnablementSetting` object representing the current state.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/csp_enablement.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get( [, etag: Optional[str]]) -> CspEnablementSetting\n\n    Get the compliance security profile setting.\n        \n    Gets the compliance security profile setting.\n        \n    :param etag: str (optional)\n      etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n      optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n      each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n      to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n      request, and pass it with the DELETE request to identify the rule set version you are deleting.\n        \n    :returns: :class:`CspEnablementSetting`\n```\n\n----------------------------------------\n\nTITLE: Defining ConnectionType Enum in Python\nDESCRIPTION: This code defines an enumeration `ConnectionType` with members for different types of connections. These types include BIGQUERY, DATABRICKS, GLUE, HIVE_METASTORE, HTTP, MYSQL, ORACLE, POSTGRESQL, REDSHIFT, SNOWFLAKE, SQLDW, SQLSERVER, and TERADATA. These represent the different external data sources that Databricks can connect to.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ConnectionType\n\n   The type of connection.\n\n   .. py:attribute:: BIGQUERY\n      :value: \"BIGQUERY\"\n\n   .. py:attribute:: DATABRICKS\n      :value: \"DATABRICKS\"\n\n   .. py:attribute:: GLUE\n      :value: \"GLUE\"\n\n   .. py:attribute:: HIVE_METASTORE\n      :value: \"HIVE_METASTORE\"\n\n   .. py:attribute:: HTTP\n      :value: \"HTTP\"\n\n   .. py:attribute:: MYSQL\n      :value: \"MYSQL\"\n\n   .. py:attribute:: ORACLE\n      :value: \"ORACLE\"\n\n   .. py:attribute:: POSTGRESQL\n      :value: \"POSTGRESQL\"\n\n   .. py:attribute:: REDSHIFT\n      :value: \"REDSHIFT\"\n\n   .. py:attribute:: SNOWFLAKE\n      :value: \"SNOWFLAKE\"\n\n   .. py:attribute:: SQLDW\n      :value: \"SQLDW\"\n\n   .. py:attribute:: SQLSERVER\n      :value: \"SQLSERVER\"\n\n   .. py:attribute:: TERADATA\n      :value: \"TERADATA\"\n```\n\n----------------------------------------\n\nTITLE: CleanRoomOutputCatalog Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomOutputCatalog dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomOutputCatalog\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Creating and Deleting a Recipient in Python\nDESCRIPTION: This code snippet demonstrates how to create a recipient using the `create` method and then delete it using the `delete` method. It utilizes the `WorkspaceClient` from the `databricks.sdk` to interact with the Databricks workspace. The recipient's name is generated dynamically using a timestamp to ensure uniqueness. It imports the `time` module to generate a unique name.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/recipients.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.recipients.create(name=f\"sdk-{time.time_ns()}\")\n\n# cleanup\nw.recipients.delete(name=created.name)\n```\n\n----------------------------------------\n\nTITLE: Updating Comment Example in Model Registry - Python\nDESCRIPTION: This code snippet demonstrates how to update a comment in the Databricks Model Registry using the Databricks SDK for Python. It initializes a WorkspaceClient, creates a model, a model version, and a comment, then updates the comment using the 'update_comment' method, and finally cleans up by deleting the comment.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/model_registry.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nmodel = w.model_registry.create_model(name=f\"sdk-{time.time_ns()}\")\n\nmv = w.model_registry.create_model_version(name=model.registered_model.name, source=\"dbfs:/tmp\")\n\ncreated = w.model_registry.create_comment(\n    comment=f\"sdk-{time.time_ns()}\",\n    name=mv.model_version.name,\n    version=mv.model_version.version,\n)\n\n_ = w.model_registry.update_comment(comment=f\"sdk-{time.time_ns()}\", id=created.comment.id)\n\n# cleanup\nw.model_registry.delete_comment(id=created.comment.id)\n```\n\n----------------------------------------\n\nTITLE: Updating a Registered Model with databricks-sdk-py\nDESCRIPTION: Updates the specified registered model. The caller must be a metastore admin or an owner of the registered model and have the USE_CATALOG privilege on the parent catalog and the USE_SCHEMA privilege on the parent schema. Only the name, owner, and comment can be updated.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/registered_models.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(full_name: str [, comment: Optional[str], new_name: Optional[str], owner: Optional[str]]) -> RegisteredModelInfo\n\n    Update a Registered Model.\n\n    Updates the specified registered model.\n\n    The caller must be a metastore admin or an owner of the registered model. For the latter case, the\n    caller must also be the owner or have the **USE_CATALOG** privilege on the parent catalog and the\n    **USE_SCHEMA** privilege on the parent schema.\n\n    Currently only the name, the owner or the comment of the registered model can be updated.\n\n    :param full_name: str\n      The three-level (fully qualified) name of the registered model\n    :param comment: str (optional)\n      The comment attached to the registered model\n    :param new_name: str (optional)\n```\n\n----------------------------------------\n\nTITLE: Creating a Credential Configuration with AccountClient\nDESCRIPTION: This code snippet demonstrates how to create a new credential configuration using the AccountClient. It uses the 'create' method, providing a credential name and AWS credentials. The AWS credentials include the ARN of the STS role that Databricks will assume. The created credential is then deleted.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/credentials.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import provisioning\n\na = AccountClient()\n\nrole = a.credentials.create(\n    credentials_name=f\"sdk-{time.time_ns()}\",\n    aws_credentials=provisioning.CreateCredentialAwsCredentials(\n        sts_role=provisioning.CreateCredentialStsRole(role_arn=os.environ[\"TEST_CROSSACCOUNT_ARN\"])\n    ),\n)\n\n# cleanup\na.credentials.delete(credentials_id=role.credentials_id)\n```\n\n----------------------------------------\n\nTITLE: Defining Stage for Model Version in Python\nDESCRIPTION: These attributes define the different stages that a model version can be in, from the initial 'None' stage to 'Staging', 'Production', and 'Archived'. These stages represent the model's lifecycle from development to deployment and eventual archiving.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: Stage\n\n   Stage of the model version. Valid values are:\n   * `None`: The initial stage of a model version.\n   * `Staging`: Staging or pre-production stage.\n   * `Production`: Production stage.\n   * `Archived`: Archived stage.\n\n   .. py:attribute:: ARCHIVED\n      :value: \"ARCHIVED\"\n\n   .. py:attribute:: NONE\n      :value: \"NONE\"\n\n   .. py:attribute:: PRODUCTION\n      :value: \"PRODUCTION\"\n\n   .. py:attribute:: STAGING\n      :value: \"STAGING\"\n```\n\n----------------------------------------\n\nTITLE: Generating Temporary Service Credential (Python)\nDESCRIPTION: This method generates temporary credentials using a specified service credential. The caller must be a metastore admin or have the 'ACCESS' privilege on the service credential. Cloud-specific options can be provided to customize the temporary credential.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/credentials.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: generate_temporary_service_credential(credential_name: str [, azure_options: Optional[GenerateTemporaryServiceCredentialAzureOptions], gcp_options: Optional[GenerateTemporaryServiceCredentialGcpOptions]]) -> TemporaryCredentials\n\n        Generate a temporary service credential.\n\n        Returns a set of temporary credentials generated using the specified service credential. The caller\n        must be a metastore admin or have the metastore privilege **ACCESS** on the service credential.\n\n        :param credential_name: str\n          The name of the service credential used to generate a temporary credential\n        :param azure_options: :class:`GenerateTemporaryServiceCredentialAzureOptions` (optional)\n          The Azure cloud options to customize the requested temporary credential\n        :param gcp_options: :class:`GenerateTemporaryServiceCredentialGcpOptions` (optional)\n          The GCP cloud options to customize the requested temporary credential\n\n        :returns: :class:`TemporaryCredentials`\n```\n\n----------------------------------------\n\nTITLE: Creating Clean Room Assets with Python\nDESCRIPTION: Creates a new clean room asset, such as a notebook or table, sharing it into the specified clean room. The clean room owner needs sufficient privileges to consume the asset, typically maintained through a group.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/cleanrooms/clean_room_assets.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(clean_room_name: str [, asset: Optional[CleanRoomAsset]]) -> CleanRoomAsset\n\n        Create an asset.\n\n        Create a clean room asset —share an asset like a notebook or table into the clean room. For each UC\n        asset that is added through this method, the clean room owner must also have enough privilege on the\n        asset to consume it. The privilege must be maintained indefinitely for the clean room to be able to\n        access the asset. Typically, you should use a group as the clean room owner.\n\n        :param clean_room_name: str\n          Name of the clean room.\n        :param asset: :class:`CleanRoomAsset` (optional)\n          Metadata of the clean room asset\n\n        :returns: :class:`CleanRoomAsset`\n```\n\n----------------------------------------\n\nTITLE: Defining URN for GroupSchema in Python\nDESCRIPTION: Defines the URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP attribute for the GroupSchema class. It represents the SCIM schema for a group with a URN string value.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: GroupSchema\n\n   .. py:attribute:: URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP\n      :value: \"URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP\"\n```\n\n----------------------------------------\n\nTITLE: Listing Schemas in a Catalog in Databricks\nDESCRIPTION: This code snippet shows how to list all schemas within a specific catalog using the Databricks SDK for Python. It first creates a new catalog, then retrieves all schemas associated with that catalog using `w.schemas.list`. Finally, it deletes the created catalog for cleanup. It requires WorkspaceClient to interact with Databricks.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/schemas.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nnew_catalog = w.catalogs.create(name=f\"sdk-{time.time_ns()}\")\n\nall = w.schemas.list(catalog_name=new_catalog.name)\n\n# cleanup\nw.catalogs.delete(name=new_catalog.name, force=True)\n```\n\n----------------------------------------\n\nTITLE: List Providers in Databricks Marketplace (Python)\nDESCRIPTION: This method lists all providers in the Databricks Marketplace, offering optional filtering and pagination.  It returns an iterator over ProviderInfo objects.  'is_featured' filters by featured status, 'page_size' sets the number of results per page, and 'page_token' is used for pagination.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_providers.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: list( [, is_featured: Optional[bool], page_size: Optional[int], page_token: Optional[str]]) -> Iterator[ProviderInfo]\n\n    List providers.\n\n    List all providers in the Databricks Marketplace with at least one visible listing.\n\n    :param is_featured: bool (optional)\n    :param page_size: int (optional)\n    :param page_token: str (optional)\n\n    :returns: Iterator over :class:`ProviderInfo`\n```\n\n----------------------------------------\n\nTITLE: CreateCleanRoomOutputCatalogResponse Class Definition (Python)\nDESCRIPTION: Defines the CreateCleanRoomOutputCatalogResponse dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CreateCleanRoomOutputCatalogResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Listing OAuth Published Apps in Python\nDESCRIPTION: This method retrieves a list of available published OAuth applications within Databricks. It supports pagination through the `page_size` and `page_token` parameters. The `page_size` determines the maximum number of applications returned per page, and the `page_token` is used to fetch subsequent pages of results. The method returns an iterator over `PublishedAppOutput` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/o_auth_published_apps.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\na.o_auth_published_apps.list(page_size: Optional[int], page_token: Optional[str]) -> Iterator[PublishedAppOutput]\n```\n\n----------------------------------------\n\nTITLE: ListCleanRoomNotebookTaskRunsResponse Class Definition (Python)\nDESCRIPTION: Defines the ListCleanRoomNotebookTaskRunsResponse dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: ListCleanRoomNotebookTaskRunsResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining RateLimitRenewalPeriod Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `RateLimitRenewalPeriod` with possible value MINUTE. It specifies the time unit used for rate limiting renewal.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: RateLimitRenewalPeriod\n\n   .. py:attribute:: MINUTE\n      :value: \"MINUTE\"\n```\n\n----------------------------------------\n\nTITLE: Generating OAuth Tokens with Databricks SDK (Python)\nDESCRIPTION: This change introduces a method to generate OAuth tokens. Note that `@credentials_provider`/`CredentialsProvider` has been renamed to `@credentials_strategy`/`CredentialsStrategy`. This is a breaking change affecting how credentials are handled.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Create and Wait Forecasting Experiment using Databricks SDK\nDESCRIPTION: This snippet demonstrates how to create and wait for a forecasting experiment to complete using the `create_experiment_and_wait` method of the `ForecastingAPI` class. It takes similar parameters as `create_experiment` and includes a timeout parameter. It returns a `ForecastingExperiment` object upon completion.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/forecasting.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nw.forecasting.create_experiment_and_wait(train_data_path: str, target_column: str, time_column: str, forecast_granularity: str, forecast_horizon: int [, custom_weights_column: Optional[str], experiment_path: Optional[str], holiday_regions: Optional[List[str]], include_features: Optional[List[str]], max_runtime: Optional[int], prediction_data_path: Optional[str], primary_metric: Optional[str], register_to: Optional[str], split_column: Optional[str], timeseries_identifier_columns: Optional[List[str]], training_frameworks: Optional[List[str]], timeout: datetime.timedelta = 2:00:00]) -> ForecastingExperiment\n```\n\n----------------------------------------\n\nTITLE: Importing a workspace object in Python\nDESCRIPTION: This code snippet demonstrates how to import a workspace object (notebook) into a Databricks workspace using the WorkspaceClient. It creates a temporary notebook path, encodes the notebook content, and then imports the object using the `import_` method. This example requires base64 encoding and the Databricks SDK.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/workspace.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import workspace\n\nw = WorkspaceClient()\n\nnotebook_path = f\"/Users/{w.current_user.me().user_name}/sdk-{time.time_ns()}\"\n\nw.workspace.import_(\n    path=notebook_path,\n    overwrite=True,\n    format=workspace.ImportFormat.SOURCE,\n    language=workspace.Language.PYTHON,\n    content=base64.b64encode(\n        (\n            \"\"\"print(1)\n\"\"\"\n        ).encode()\n    ).decode(),\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dashboard Widget in Databricks\nDESCRIPTION: This method creates a new widget on a Databricks dashboard. It requires the dashboard ID, widget options, and width. Optionally, you can specify text for textbox widgets or a visualization ID for widgets linked to a query visualization.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/dashboard_widgets.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create(dashboard_id: str, options: WidgetOptions, width: int [, text: Optional[str], visualization_id: Optional[str]]) -> Widget\n\n    Add widget to a dashboard.\n\n    :param dashboard_id: str\n      Dashboard ID returned by :method:dashboards/create.\n    :param options: :class:`WidgetOptions`\n    :param width: int\n      Width of a widget\n    :param text: str (optional)\n      If this is a textbox widget, the application displays this text. This field is ignored if the widget\n      contains a visualization in the `visualization` field.\n    :param visualization_id: str (optional)\n      Query Vizualization ID returned by :method:queryvisualizations/create.\n\n    :returns: :class:`Widget`\n```\n\n----------------------------------------\n\nTITLE: Defining Enum: EgressNetworkPolicyInternetAccessPolicyStorageDestinationStorageDestinationType - Python\nDESCRIPTION: This Python enum defines the supported storage destination types for internet access policies.  It includes AWS_S3, AZURE_STORAGE, CLOUDFLARE_R2, and GOOGLE_CLOUD_STORAGE, allowing users to specify which cloud storage services are permitted under egress network policies.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EgressNetworkPolicyInternetAccessPolicyStorageDestinationStorageDestinationType\n\n   .. py:attribute:: AWS_S3\n      :value: \"AWS_S3\"\n\n   .. py:attribute:: AZURE_STORAGE\n      :value: \"AZURE_STORAGE\"\n\n   .. py:attribute:: CLOUDFLARE_R2\n      :value: \"CLOUDFLARE_R2\"\n\n   .. py:attribute:: GOOGLE_CLOUD_STORAGE\n      :value: \"GOOGLE_CLOUD_STORAGE\"\n```\n\n----------------------------------------\n\nTITLE: Creating and Waiting for a Cluster with Timeout in Databricks SDK\nDESCRIPTION: This example demonstrates how to create a Databricks cluster and wait for it to reach the 'RUNNING' state using the `create_and_wait` method. The `timeout` parameter is used to specify the maximum time to wait for the cluster to become ready. It imports the necessary modules and initializes the Databricks WorkspaceClient.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\nimport logging\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\ninfo = w.clusters.create_and_wait(cluster_name='Created cluster',\n                                  spark_version='12.0.x-scala2.12',\n                                  node_type_id='m5d.large',\n                                  autotermination_minutes=10,\n                                  num_workers=1,\n                                  timeout=datetime.timedelta(minutes=10))\nlogging.info(f'Created: {info}')\n```\n\n----------------------------------------\n\nTITLE: Defining CatalogType Enum in Python\nDESCRIPTION: This code defines an enumeration `CatalogType` with members DELTASHARING_CATALOG, FOREIGN_CATALOG, MANAGED_CATALOG, and SYSTEM_CATALOG. These values describe the different types of catalogs that can exist within the Unity Catalog environment, defining their origin and management.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CatalogType\n\n   The type of the catalog.\n\n   .. py:attribute:: DELTASHARING_CATALOG\n      :value: \"DELTASHARING_CATALOG\"\n\n   .. py:attribute:: FOREIGN_CATALOG\n      :value: \"FOREIGN_CATALOG\"\n\n   .. py:attribute:: MANAGED_CATALOG\n      :value: \"MANAGED_CATALOG\"\n\n   .. py:attribute:: SYSTEM_CATALOG\n      :value: \"SYSTEM_CATALOG\"\n```\n\n----------------------------------------\n\nTITLE: PipelineClusterAutoscaleMode Enum Definition in Python\nDESCRIPTION: Defines an enumeration for the autoscaling mode of a pipeline cluster. It distinguishes between ENHANCED (Databricks Enhanced Autoscaling) and LEGACY autoscaling. Enhanced autoscaling optimizes cluster utilization based on workload volume, while legacy uses the older autoscaling method.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: PipelineClusterAutoscaleMode\n\n   Databricks Enhanced Autoscaling optimizes cluster utilization by automatically allocating cluster resources based on workload volume, with minimal impact to the data processing latency of your pipelines. Enhanced Autoscaling is available for `updates` clusters only. The legacy autoscaling feature is used for `maintenance` clusters.\n\n   .. py:attribute:: ENHANCED\n      :value: \"ENHANCED\"\n\n   .. py:attribute:: LEGACY\n      :value: \"LEGACY\"\n```\n\n----------------------------------------\n\nTITLE: Defining Enum: EgressNetworkPolicyInternetAccessPolicyRestrictionMode - Python\nDESCRIPTION: This Python enum defines the restriction modes for internet access in egress network policies. It provides modes such as FULL_ACCESS, PRIVATE_ACCESS_ONLY, and RESTRICTED_ACCESS, which determine the level of internet access granted to Databricks.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EgressNetworkPolicyInternetAccessPolicyRestrictionMode\n\n   At which level can Databricks and Databricks managed compute access Internet. FULL_ACCESS: Databricks can access Internet. No blocking rules will apply. RESTRICTED_ACCESS: Databricks can only access explicitly allowed internet and storage destinations, as well as UC connections and external locations. PRIVATE_ACCESS_ONLY (not used): Databricks can only access destinations via private link.\n\n   .. py:attribute:: FULL_ACCESS\n      :value: \"FULL_ACCESS\"\n\n   .. py:attribute:: PRIVATE_ACCESS_ONLY\n      :value: \"PRIVATE_ACCESS_ONLY\"\n\n   .. py:attribute:: RESTRICTED_ACCESS\n      :value: \"RESTRICTED_ACCESS\"\n```\n\n----------------------------------------\n\nTITLE: Deleting a Legacy Query in Databricks SQL\nDESCRIPTION: This method deletes a query by moving it to the trash in Databricks SQL.  Trashed queries are immediately removed from searches and list views and are permanently deleted after 30 days.  The method takes the query_id as a parameter.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/queries_legacy.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(query_id: str)\n```\n\n----------------------------------------\n\nTITLE: CleanRoomCollaborator Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomCollaborator dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomCollaborator\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining ASCENDING attribute for ListSortOrder in Python\nDESCRIPTION: Defines the ASCENDING attribute for the ListSortOrder class. It represents the ascending sort order with a string value.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ListSortOrder\n\n   .. py:attribute:: ASCENDING\n      :value: \"ASCENDING\"\n```\n\n----------------------------------------\n\nTITLE: CleanRoomOutputCatalogOutputCatalogStatus Enum (Python)\nDESCRIPTION: Defines the CleanRoomOutputCatalogOutputCatalogStatus enumeration. Includes constants like CREATED, NOT_CREATED, and NOT_ELIGIBLE.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: CleanRoomOutputCatalogOutputCatalogStatus\n\n   .. py:attribute:: CREATED\n      :value: \"CREATED\"\n\n   .. py:attribute:: NOT_CREATED\n      :value: \"NOT_CREATED\"\n\n   .. py:attribute:: NOT_ELIGIBLE\n      :value: \"NOT_ELIGIBLE\"\n```\n\n----------------------------------------\n\nTITLE: Defining Languages in Python\nDESCRIPTION: This code defines the possible languages supported by Databricks. It includes PYTHON, SCALA, and SQL as valid languages.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: Language\n\n   .. py:attribute:: PYTHON\n      :value: \"PYTHON\"\n\n   .. py:attribute:: SCALA\n      :value: \"SCALA\"\n\n   .. py:attribute:: SQL\n      :value: \"SQL\"\n```\n\n----------------------------------------\n\nTITLE: Delete User with AccountClient in Python\nDESCRIPTION: This code snippet shows how to delete a user from a Databricks account using the AccountClient and the `delete` method. It depends on having a valid user ID to delete. It requires the `databricks.sdk` library.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/users.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nuser = a.users.create(\n    display_name=f\"sdk-{time.time_ns()}\",\n    user_name=f\"sdk-{time.time_ns()}@example.com\",\n)\n\na.users.delete(id=user.id)\n```\n\n----------------------------------------\n\nTITLE: DeleteCleanRoomAssetResponse Class Definition (Python)\nDESCRIPTION: Defines the DeleteCleanRoomAssetResponse dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: DeleteCleanRoomAssetResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Autogenerating Documentation for Subscription Class in Python\nDESCRIPTION: This code snippet uses the 'autoclass' directive to automatically generate documentation for the Subscription class. The ':members:' option includes all public members in the documentation, while ':undoc-members:' excludes undocumented members. This is part of the Databricks SDK documentation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: Subscription\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining Enum: DestinationType - Python\nDESCRIPTION: This Python enum defines the supported destination types for notifications. These types include EMAIL, MICROSOFT_TEAMS, PAGERDUTY, SLACK, and WEBHOOK. This enumeration allows specifying the destination type when creating or configuring notification settings.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: DestinationType\n\n   .. py:attribute:: EMAIL\n      :value: \"EMAIL\"\n\n   .. py:attribute:: MICROSOFT_TEAMS\n      :value: \"MICROSOFT_TEAMS\"\n\n   .. py:attribute:: PAGERDUTY\n      :value: \"PAGERDUTY\"\n\n   .. py:attribute:: SLACK\n      :value: \"SLACK\"\n\n   .. py:attribute:: WEBHOOK\n      :value: \"WEBHOOK\"\n```\n\n----------------------------------------\n\nTITLE: Defining ContextStatus Enum in Python\nDESCRIPTION: This code defines an enumeration (`ContextStatus`) describing the status of a Databricks notebook context. Possible values include ERROR, PENDING, and RUNNING, indicating the context's current state.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ContextStatus\n\n   .. py:attribute:: ERROR\n      :value: \"ERROR\"\n\n   .. py:attribute:: PENDING\n      :value: \"PENDING\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n```\n\n----------------------------------------\n\nTITLE: Deleting a User using Databricks SDK in Python\nDESCRIPTION: Deletes a user from the Databricks workspace using the WorkspaceClient. The user is first created, then deleted. It depends on the `databricks.sdk` library.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/users.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nother_owner = w.users.create(user_name=f\"sdk-{time.time_ns()}@example.com\")\n\nw.users.delete(id=other_owner.id)\n```\n\n----------------------------------------\n\nTITLE: Defining ImportFormat Enum\nDESCRIPTION: Defines the supported import formats for workspace objects. Available options include AUTO, DBC, HTML, JUPYTER, RAW, R_MARKDOWN, and SOURCE. This enum is used when importing workspace objects using the Databricks API.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/workspace.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ImportFormat\n\n   The format for workspace import and export.\n\n   .. py:attribute:: AUTO\n      :value: \"AUTO\"\n\n   .. py:attribute:: DBC\n      :value: \"DBC\"\n\n   .. py:attribute:: HTML\n      :value: \"HTML\"\n\n   .. py:attribute:: JUPYTER\n      :value: \"JUPYTER\"\n\n   .. py:attribute:: RAW\n      :value: \"RAW\"\n\n   .. py:attribute:: R_MARKDOWN\n      :value: \"R_MARKDOWN\"\n\n   .. py:attribute:: SOURCE\n      :value: \"SOURCE\"\n```\n\n----------------------------------------\n\nTITLE: Listing Shares by Provider with Databricks SDK in Python\nDESCRIPTION: This example demonstrates how to list shares associated with a specific data provider. It creates a provider, then lists the shares for that provider using the list_shares method, and finally cleans up by deleting the provider. The example shows how to retrieve shares associated with a specific provider.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/providers.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\npublic_share_recipient = \"\"\"{\n        \"shareCredentialsVersion\":1,\n        \"bearerToken\":\"dapiabcdefghijklmonpqrstuvwxyz\",\n        \"endpoint\":\"https://sharing.delta.io/delta-sharing/\"\n    }\n\"\"\"\n\ncreated = w.providers.create(name=f\"sdk-{time.time_ns()}\", recipient_profile_str=public_share_recipient)\n\nshares = w.providers.list_shares(name=created.name)\n\n# cleanup\nw.providers.delete(name=created.name)\n```\n\n----------------------------------------\n\nTITLE: Defining Enum: ClusterAutoRestartMessageMaintenanceWindowDayOfWeek - Python\nDESCRIPTION: This Python enum specifies the days of the week for scheduling cluster auto-restart maintenance windows. It includes all days of the week, from Monday to Sunday. This enumeration allows scheduling maintenance windows on specific days.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ClusterAutoRestartMessageMaintenanceWindowDayOfWeek\n\n   .. py:attribute:: FRIDAY\n      :value: \"FRIDAY\"\n\n   .. py:attribute:: MONDAY\n      :value: \"MONDAY\"\n\n   .. py:attribute:: SATURDAY\n      :value: \"SATURDAY\"\n\n   .. py:attribute:: SUNDAY\n      :value: \"SUNDAY\"\n\n   .. py:attribute:: THURSDAY\n      :value: \"THURSDAY\"\n\n   .. py:attribute:: TUESDAY\n      :value: \"TUESDAY\"\n\n   .. py:attribute:: WEDNESDAY\n      :value: \"WEDNESDAY\"\n```\n\n----------------------------------------\n\nTITLE: Defining Table Types in Python\nDESCRIPTION: This code defines the TableType class, enumerating the possible types of tables. Each table type is represented as a class attribute with a string value. Table types include EXTERNAL, EXTERNAL_SHALLOW_CLONE, FOREIGN, MANAGED, MANAGED_SHALLOW_CLONE, MATERIALIZED_VIEW, STREAMING_TABLE and VIEW.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass TableType:\n   EXTERNAL = \"EXTERNAL\"\n\n   EXTERNAL_SHALLOW_CLONE = \"EXTERNAL_SHALLOW_CLONE\"\n\n   FOREIGN = \"FOREIGN\"\n\n   MANAGED = \"MANAGED\"\n\n   MANAGED_SHALLOW_CLONE = \"MANAGED_SHALLOW_CLONE\"\n\n   MATERIALIZED_VIEW = \"MATERIALIZED_VIEW\"\n\n   STREAMING_TABLE = \"STREAMING_TABLE\"\n\n   VIEW = \"VIEW\"\n```\n\n----------------------------------------\n\nTITLE: Defining Workspace URN for UserSchema in Python\nDESCRIPTION: Defines the URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER attribute for the UserSchema class. It represents the SCIM workspace schema extension for a user with a URN string value.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: UserSchema\n\n   .. py:attribute:: URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER\n      :value: \"URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER\"\n```\n\n----------------------------------------\n\nTITLE: Volume Deletion and Catalog Cleanup\nDESCRIPTION: This snippet demonstrates deleting a created volume and a catalog with the `force=True` option. It relies on the `w` object which is assumed to be an initialized `WorkspaceClient` from the Databricks SDK.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/volumes.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nw.catalogs.delete(name=created_catalog.name, force=True)\nw.volumes.delete(name=created_volume.full_name)\n```\n\n----------------------------------------\n\nTITLE: Patching Group Details in Databricks (Python)\nDESCRIPTION: Partially updates details of a group. The id parameter specifies the unique ID in the Databricks workspace. The operations parameter is a list of Patch operations. The schemas parameter is the schema of the patch request.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/groups.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: patch(id: str [, operations: Optional[List[Patch]], schemas: Optional[List[PatchSchema]]])\n\n        Update group details.\n\n        Partially updates the details of a group.\n\n        :param id: str\n          Unique ID in the Databricks workspace.\n        :param operations: List[:class:`Patch`] (optional)\n        :param schemas: List[:class:`PatchSchema`] (optional)\n          The schema of the patch request. Must be [\"urn:ietf:params:scim:api:messages:2.0:PatchOp\"].\n```\n\n----------------------------------------\n\nTITLE: CleanRoomNotebookTaskRun Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomNotebookTaskRun dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomNotebookTaskRun\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Poll Published Query Status - Python\nDESCRIPTION: This method polls the results for a query associated with a published, embedded dashboard. It requires the dashboard name and revision ID. An optional list of tokens can be provided for authorization.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/query_execution.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef poll_published_query_status(dashboard_name: str, dashboard_revision_id: str [, tokens: Optional[List[str]]]) -> PollQueryStatusResponse:\n    \"\"\"Poll the results for the a query for a published, embedded dashboard.\n\n    :param dashboard_name: str\n    :param dashboard_revision_id: str\n    :param tokens: List[str] (optional)\n      Example: EC0A..ChAB7WCEn_4Qo4vkLqEbXsxxEgh3Y2pbWw45WhoQXgZSQo9aS5q2ZvFcbvbx9CgA-PAEAQ\n\n    :returns: :class:`PollQueryStatusResponse`\n    \"\"\n```\n\n----------------------------------------\n\nTITLE: Autogenerating Documentation for TextAttachment Class in Python\nDESCRIPTION: This code snippet uses the 'autoclass' directive to automatically generate documentation for the TextAttachment class. The ':members:' option includes all public members in the documentation, while ':undoc-members:' excludes undocumented members. This is part of the Databricks SDK documentation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: TextAttachment\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Retrieving AI/BI Dashboard Embedding Access Policy in Python\nDESCRIPTION: Retrieves the AI/BI dashboard embedding access policy. The default setting is ALLOW_APPROVED_DOMAINS, permitting AI/BI dashboards to be embedded on approved domains. An optional etag can be provided for optimistic concurrency control. The method returns an AibiDashboardEmbeddingAccessPolicySetting object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/aibi_dashboard_embedding_access_policy.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n    .. py:method:: get( [, etag: Optional[str]]) -> AibiDashboardEmbeddingAccessPolicySetting\n\n        Retrieve the AI/BI dashboard embedding access policy.\n\n        Retrieves the AI/BI dashboard embedding access policy. The default setting is ALLOW_APPROVED_DOMAINS,\n        permitting AI/BI dashboards to be embedded on approved domains.\n\n        :param etag: str (optional)\n          etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n          optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n          each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n          to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n          request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n        :returns: :class:`AibiDashboardEmbeddingAccessPolicySetting`\n```\n\n----------------------------------------\n\nTITLE: Defining ClusterPermissionLevel Enum in Python\nDESCRIPTION: This code defines an enumeration (`ClusterPermissionLevel`) specifying the permission levels a user can have on a Databricks cluster. The options include CAN_ATTACH_TO, CAN_MANAGE, and CAN_RESTART, representing the ability to attach to a cluster, manage its settings, and restart it respectively.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ClusterPermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_ATTACH_TO\n      :value: \"CAN_ATTACH_TO\"\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_RESTART\n      :value: \"CAN_RESTART\"\n```\n\n----------------------------------------\n\nTITLE: Update User in Databricks Workspace Python\nDESCRIPTION: This snippet demonstrates how to update a user's information in a Databricks workspace using the `update` method. It requires the `id` of the user and can optionally update properties like `user_name` and `active` status. This example also utilizes the `create` method to create a test user before updating it.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/users.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nuser = w.users.create(\n    display_name=f\"sdk-{time.time_ns()}\",\n    user_name=f\"sdk-{time.time_ns()}@example.com\",\n)\n\nw.users.update(id=user.id, user_name=user.user_name, active=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Termination Reason Types in Python\nDESCRIPTION: This code snippet defines termination reason types, categorized into Client Error, Cloud Failure and Service Fault. These enums provide a high level classification for different termination reasons within the Databricks environment.  These values are string literals.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: TerminationReasonType\n\n   type of the termination\n\n   .. py:attribute:: CLIENT_ERROR\n      :value: \"CLIENT_ERROR\"\n\n   .. py:attribute:: CLOUD_FAILURE\n      :value: \"CLOUD_FAILURE\"\n\n   .. py:attribute:: SERVICE_FAULT\n```\n\n----------------------------------------\n\nTITLE: Get Global Init Script by ID in Databricks\nDESCRIPTION: This code snippet shows how to retrieve a global init script by its ID using the Databricks SDK. It first creates an init script, then retrieves it using the `get` method, and finally deletes it. The base64 encoding is used for the script content.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/global_init_scripts.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.global_init_scripts.create(\n    name=f\"sdk-{time.time_ns()}\",\n    script=base64.b64encode((\"echo 1\").encode()).decode(),\n    enabled=True,\n    position=10,\n)\n\nby_id = w.global_init_scripts.get(script_id=created.script_id)\n\n# cleanup\nw.global_init_scripts.delete(script_id=created.script_id)\n```\n\n----------------------------------------\n\nTITLE: Uninstall libraries - Python\nDESCRIPTION: Uninstalls libraries from a Databricks cluster. The `cluster_id` specifies the cluster, and `libraries` is a list of `Library` objects to be uninstalled. The actual uninstallation occurs when the cluster is restarted. Uninstalling a library that is not currently installed is ignored.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/libraries.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: uninstall(cluster_id: str, libraries: List[Library])\n\n    Uninstall libraries.\n\n    Set libraries to uninstall from a cluster. The libraries won't be uninstalled until the cluster is\n    restarted. A request to uninstall a library that is not currently installed is ignored.\n\n    :param cluster_id: str\n      Unique identifier for the cluster on which to uninstall these libraries.\n    :param libraries: List[:class:`Library`]\n      The libraries to uninstall.\n```\n\n----------------------------------------\n\nTITLE: Defining SUCCESS constant in Python\nDESCRIPTION: This code snippet defines a constant named SUCCESS and assigns it the string value \"SUCCESS\". This constant probably represents a successful operation status within the Databricks SDK.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\n:value: \"SUCCESS\"\n```\n\n----------------------------------------\n\nTITLE: Create Federation Policy (Python)\nDESCRIPTION: Creates a new account federation policy.  The `policy_id` is optional; if not specified, Databricks will assign one. The policy itself is passed as a `FederationPolicy` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/federation_policy.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create( [, policy: Optional[FederationPolicy], policy_id: Optional[str]]) -> FederationPolicy\n\n    Create account federation policy.\n\n    :param policy: :class:`FederationPolicy` (optional)\n    :param policy_id: str (optional)\n      The identifier for the federation policy. The identifier must contain only lowercase alphanumeric\n      characters, numbers, hyphens, and slashes. If unspecified, the id will be assigned by Databricks.\n\n    :returns: :class:`FederationPolicy`\n```\n\n----------------------------------------\n\nTITLE: Listing Clean Rooms - Python\nDESCRIPTION: Retrieves an array of data object clean rooms from the metastore. The caller must be a metastore admin or the owner of the clean room. Supports pagination with optional parameters for max_results and page_token.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/clean_rooms.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list( [, max_results: Optional[int], page_token: Optional[str]]) -> Iterator[CleanRoomInfo]\n\n    List clean rooms.\n    \n    Gets an array of data object clean rooms from the metastore. The caller must be a metastore admin or\n    the owner of the clean room. There is no guarantee of a specific ordering of the elements in the\n    array.\n    \n    :param max_results: int (optional)\n      Maximum number of clean rooms to return. If not set, all the clean rooms are returned (not\n      recommended). - when set to a value greater than 0, the page length is the minimum of this value and\n      a server configured value; - when set to 0, the page length is set to a server configured value\n      (recommended); - when set to a value less than 0, an invalid parameter error is returned;\n    :param page_token: str (optional)\n      Opaque pagination token to go to next page based on previous query.\n    \n    :returns: Iterator over :class:`CleanRoomInfo`\n```\n\n----------------------------------------\n\nTITLE: Deleting Personal Compute Setting with ETag - Python\nDESCRIPTION: This method deletes the Personal Compute setting, reverting it to the default value (ON). It uses an optional etag for optimistic concurrency control to prevent simultaneous writes from overwriting each other. The etag should be obtained from a prior GET request.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/personal_compute.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsettings.personal_compute.delete([, etag: Optional[str]]) -> DeletePersonalComputeSettingResponse\n```\n\n----------------------------------------\n\nTITLE: Autogenerating Documentation for SubscriptionSubscriberDestination Class in Python\nDESCRIPTION: This code snippet uses the 'autoclass' directive to automatically generate documentation for the SubscriptionSubscriberDestination class. The ':members:' option includes all public members in the documentation, while ':undoc-members:' excludes undocumented members. This is part of the Databricks SDK documentation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: SubscriptionSubscriberDestination\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining ClusterSource Enum in Python\nDESCRIPTION: This code defines an enumeration (`ClusterSource`) that determines how a Databricks cluster was created. Possible values include API, JOB, MODELS, PIPELINE, PIPELINE_MAINTENANCE, SQL, and UI, each indicating a different creation method like through the API, a Databricks Job, or the Databricks UI.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ClusterSource\n\n   Determines whether the cluster was created by a user through the UI, created by the Databricks Jobs Scheduler, or through an API request. This is the same as cluster_creator, but read only.\n\n   .. py:attribute:: API\n      :value: \"API\"\n\n   .. py:attribute:: JOB\n      :value: \"JOB\"\n\n   .. py:attribute:: MODELS\n      :value: \"MODELS\"\n\n   .. py:attribute:: PIPELINE\n      :value: \"PIPELINE\"\n\n   .. py:attribute:: PIPELINE_MAINTENANCE\n      :value: \"PIPELINE_MAINTENANCE\"\n\n   .. py:attribute:: SQL\n      :value: \"SQL\"\n\n   .. py:attribute:: UI\n      :value: \"UI\"\n```\n\n----------------------------------------\n\nTITLE: Defining RegisteredModelPermissionLevel in Python\nDESCRIPTION: These attributes define the different permission levels that a user can have on a registered model, ranging from read-only access to full management capabilities. Each attribute represents a specific level of access control.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: RegisteredModelPermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_EDIT\n      :value: \"CAN_EDIT\"\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_MANAGE_PRODUCTION_VERSIONS\n      :value: \"CAN_MANAGE_PRODUCTION_VERSIONS\"\n\n   .. py:attribute:: CAN_MANAGE_STAGING_VERSIONS\n      :value: \"CAN_MANAGE_STAGING_VERSIONS\"\n\n   .. py:attribute:: CAN_READ\n      :value: \"CAN_READ\"\n```\n\n----------------------------------------\n\nTITLE: Downloading a File from DBFS with Databricks SDK in Python\nDESCRIPTION: This code snippet demonstrates how to download a file from DBFS using the Databricks SDK in Python. It initializes a WorkspaceClient, uploads a small file to DBFS, and then downloads it using the `download` method. The downloaded content is asserted to match the original content.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/files/dbfs.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport io\nimport pathlib\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nroot = pathlib.Path(f\"/tmp/{time.time_ns()}\")\n\nf = io.BytesIO(b\"some text data\")\nw.dbfs.upload(f\"{root}/01\", f)\n\nwith w.dbfs.download(f\"{root}/01\") as f:\n    assert f.read() == b\"some text data\"\n```\n\n----------------------------------------\n\nTITLE: Updating Workspace Assignments with AccountClient in Python\nDESCRIPTION: This code snippet shows how to create or update workspace permissions for a service principal using the AccountClient. It first creates a service principal, then assigns it the 'USER' permission for a specific workspace retrieved from an environment variable.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/workspace_assignment.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import iam\n\na = AccountClient()\n\nspn = a.service_principals.create(display_name=f\"sdk-{time.time_ns()}\")\n\nspn_id = spn.id\n\nworkspace_id = os.environ[\"TEST_WORKSPACE_ID\"]\n\na.workspace_assignment.update(\n    workspace_id=workspace_id,\n    principal_id=spn_id,\n    permissions=[iam.WorkspacePermission.USER],\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Update Binding Securable Types in Python\nDESCRIPTION: This code defines the UpdateBindingsSecurableType class, which enumerates the types of securable objects that can be used when updating bindings. Each type is represented as a class attribute with a string value. Types include CATALOG, CREDENTIAL, EXTERNAL_LOCATION and STORAGE_CREDENTIAL.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass UpdateBindingsSecurableType:\n   CATALOG = \"CATALOG\"\n\n   CREDENTIAL = \"CREDENTIAL\"\n\n   EXTERNAL_LOCATION = \"EXTERNAL_LOCATION\"\n\n   STORAGE_CREDENTIAL = \"STORAGE_CREDENTIAL\"\n```\n\n----------------------------------------\n\nTITLE: Setting a Registered Model Alias with databricks-sdk-py\nDESCRIPTION: Sets an alias on the specified registered model. The caller must be a metastore admin or an owner of the registered model and have the USE_CATALOG privilege on the parent catalog and the USE_SCHEMA privilege on the parent schema.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/registered_models.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: set_alias(full_name: str, alias: str, version_num: int) -> RegisteredModelAlias\n\n    Set a Registered Model Alias.\n\n    Set an alias on the specified registered model.\n\n    The caller must be a metastore admin or an owner of the registered model. For the latter case, the\n    caller must also be the owner or have the **USE_CATALOG** privilege on the parent catalog and the\n    **USE_SCHEMA** privilege on the parent schema.\n\n    :param full_name: str\n      Full name of the registered model\n    :param alias: str\n      The name of the alias\n    :param version_num: int\n      The version number of the model version to which the alias points\n\n    :returns: :class:`RegisteredModelAlias`\n```\n\n----------------------------------------\n\nTITLE: Setting Permissions on Databricks Serving Endpoint\nDESCRIPTION: This method allows setting permissions on a Databricks serving endpoint, replacing any existing permissions. If no permissions are specified, all direct permissions are deleted, and the endpoint can inherit permissions from its root object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: set_permissions(serving_endpoint_id: str [, access_control_list: Optional[List[ServingEndpointAccessControlRequest]]]) -> ServingEndpointPermissions\n\n        Set serving endpoint permissions.\n\n        Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct\\n        permissions if none are specified. Objects can inherit permissions from their root object.\n\n        :param serving_endpoint_id: str\n          The serving endpoint for which to get or manage permissions.\n        :param access_control_list: List[:class:`ServingEndpointAccessControlRequest`] (optional)\n\n        :returns: :class:`ServingEndpointPermissions`\n        \n```\n\n----------------------------------------\n\nTITLE: Create Global Init Script in Databricks\nDESCRIPTION: This code snippet demonstrates how to create a global init script using the Databricks SDK. It encodes a simple echo command to base64 and then creates the script with a specified name, content, enabled status, and position. The script is then deleted to clean up.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/global_init_scripts.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.global_init_scripts.create(\n    name=f\"sdk-{time.time_ns()}\",\n    script=base64.b64encode((\"echo 1\").encode()).decode(),\n    enabled=True,\n    position=10,\n)\n\n# cleanup\nw.global_init_scripts.delete(script_id=created.script_id)\n```\n\n----------------------------------------\n\nTITLE: Running a Monitor Refresh - Databricks SDK (Python)\nDESCRIPTION: This method queues a metric refresh on the monitor for the specified table. The refresh will execute in the background.  The caller needs specific permissions, including ownership or USE privileges on the catalog and schema.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/quality_monitors.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: run_refresh(table_name: str) -> MonitorRefreshInfo\n\n        Queue a metric refresh for a monitor.\n\n        Queues a metric refresh on the monitor for the specified table. The refresh will execute in the\n        background.\n\n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n```\n\n----------------------------------------\n\nTITLE: Batch Get Listings - Python\nDESCRIPTION: Retrieves a batch of listings from the Databricks Marketplace by their IDs. Allows specifying up to 50 listing IDs per request. Returns a `BatchGetListingsResponse` object containing the retrieved listings.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_listings.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: batch_get( [, ids: Optional[List[str]]]) -> BatchGetListingsResponse\n\n    Get one batch of listings. One may specify up to 50 IDs per request.\n\n    Batch get a published listing in the Databricks Marketplace that the consumer has access to.\n\n    :param ids: List[str] (optional)\n\n    :returns: :class:`BatchGetListingsResponse`\n```\n\n----------------------------------------\n\nTITLE: Getting a Genie Space in Genie using Python\nDESCRIPTION: This method retrieves details of a specific Genie space. It requires the space ID. The method returns a GenieSpace object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/genie.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: get_space(space_id: str) -> GenieSpace\n\n        Get Genie Space.\n\n        Get details of a Genie Space.\n\n        :param space_id: str\n          The ID associated with the Genie space\n\n        :returns: :class:`GenieSpace`\n```\n\n----------------------------------------\n\nTITLE: Getting a Conversation Message in Genie using Python\nDESCRIPTION: This method retrieves a specific message from a Genie conversation. It requires the space ID, conversation ID, and the message ID. The method returns a GenieMessage object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/genie.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: get_message(space_id: str, conversation_id: str, message_id: str) -> GenieMessage\n\n        Get conversation message.\n\n        Get message from conversation.\n\n        :param space_id: str\n          The ID associated with the Genie space where the target conversation is located.\n        :param conversation_id: str\n          The ID associated with the target conversation.\n        :param message_id: str\n          The ID associated with the target message from the identified conversation.\n\n        :returns: :class:`GenieMessage`\n```\n\n----------------------------------------\n\nTITLE: Auto-generated class: UninstallLibrariesResponse in Python\nDESCRIPTION: This section documents the auto-generated class `UninstallLibrariesResponse` within the Databricks SDK for Python. The members and undocumented members of this class are included.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: UninstallLibrariesResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Listing Apps in Databricks using Python\nDESCRIPTION: This code snippet explains how to list all apps in the workspace using the `list` method.  It takes optional parameters for pagination, including `page_size` and `page_token`. The method returns an iterator over `App` objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list( [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[App]\n\n        List apps.\n\n        Lists all apps in the workspace.\n\n        :param page_size: int (optional)\n          Upper bound for items returned.\n        :param page_token: str (optional)\n          Pagination token to go to the next page of apps. Requests first page if absent.\n\n        :returns: Iterator over :class:`App`\n```\n\n----------------------------------------\n\nTITLE: Listing Lakeview Dashboards\nDESCRIPTION: Lists Lakeview dashboards using the LakeviewAPI.  Supports pagination and filtering of trashed dashboards. Returns an iterator over Dashboard objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: list( [, page_size: Optional[int], page_token: Optional[str], show_trashed: Optional[bool], view: Optional[DashboardView]]) -> Iterator[Dashboard]\n\n    List dashboards.\n\n    :param page_size: int (optional)\n      The number of dashboards to return per page.\n    :param page_token: str (optional)\n      A page token, received from a previous `ListDashboards` call. This token can be used to retrieve the\n      subsequent page.\n    :param show_trashed: bool (optional)\n      The flag to include dashboards located in the trash. If unspecified, only active dashboards will be\n      returned.\n    :param view: :class:`DashboardView` (optional)\n      `DASHBOARD_VIEW_BASIC`only includes summary metadata from the dashboard.\n\n    :returns: Iterator over :class:`Dashboard`\n```\n\n----------------------------------------\n\nTITLE: WarningType Enum Definition (Python)\nDESCRIPTION: Defines an enumeration representing the AWS resource associated with a warning in the `databricks.sdk.service.provisioning` module. The possible values are `SECURITY_GROUP` and `SUBNET`. It identifies the type of AWS resource that has generated a warning during workspace provisioning.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/provisioning.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: WarningType\n\n   The AWS resource associated with this warning: a subnet or a security group.\n\n   .. py:attribute:: SECURITY_GROUP\n      :value: \"SECURITY_GROUP\"\n\n   .. py:attribute:: SUBNET\n      :value: \"SUBNET\"\n```\n\n----------------------------------------\n\nTITLE: Defining RunInfoStatus in Python\nDESCRIPTION: These attributes define the possible statuses for a run, including FAILED, FINISHED, KILLED, RUNNING, and SCHEDULED. These statuses describe the lifecycle of a run during model training and evaluation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: RunInfoStatus\n\n   Status of a run.\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: FINISHED\n      :value: \"FINISHED\"\n\n   .. py:attribute:: KILLED\n      :value: \"KILLED\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n\n   .. py:attribute:: SCHEDULED\n      :value: \"SCHEDULED\"\n```\n\n----------------------------------------\n\nTITLE: Updating Model Version Example in Model Registry - Python\nDESCRIPTION: This code snippet demonstrates how to update a model version's description in the Databricks Model Registry using the Databricks SDK for Python. It initializes a WorkspaceClient, creates a model and a model version, and then updates the model version's description using the 'update_model_version' method.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/model_registry.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nmodel = w.model_registry.create_model(name=f\"sdk-{time.time_ns()}\")\n\ncreated = w.model_registry.create_model_version(name=model.registered_model.name, source=\"dbfs:/tmp\")\n\nw.model_registry.update_model_version(\n    description=f\"sdk-{time.time_ns()}\",\n    name=created.model_version.name,\n    version=created.model_version.version,\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug HTTP Headers in Python\nDESCRIPTION: This code shows how to enable debug HTTP headers for the Databricks SDK. It requires the `databricks.sdk` package and initializes the `WorkspaceClient` with `debug_headers=True`. This will output HTTP headers in the debug logs, which may contain sensitive information like access tokens.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(debug_headers=True)\n# Now call the Databricks workspace APIs as desired...\n```\n\n----------------------------------------\n\nTITLE: Defining DataSourceFormat Enum in Python\nDESCRIPTION: This code defines an enumeration `DataSourceFormat` with members for various data source formats. These include AVRO, BIGQUERY_FORMAT, CSV, DATABRICKS_FORMAT, DELTA, DELTASHARING, HIVE_CUSTOM, HIVE_SERDE, JSON, MYSQL_FORMAT, NETSUITE_FORMAT, ORC, PARQUET, POSTGRESQL_FORMAT, REDSHIFT_FORMAT, SALESFORCE_FORMAT, SNOWFLAKE_FORMAT, SQLDW_FORMAT, SQLSERVER_FORMAT, TEXT, UNITY_CATALOG, VECTOR_INDEX_FORMAT and WORKDAY_RAAS_FORMAT. These formats define the different types of data sources that can be ingested and processed by Databricks.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: DataSourceFormat\n\n   Data source format\n\n   .. py:attribute:: AVRO\n      :value: \"AVRO\"\n\n   .. py:attribute:: BIGQUERY_FORMAT\n      :value: \"BIGQUERY_FORMAT\"\n\n   .. py:attribute:: CSV\n      :value: \"CSV\"\n\n   .. py:attribute:: DATABRICKS_FORMAT\n      :value: \"DATABRICKS_FORMAT\"\n\n   .. py:attribute:: DELTA\n      :value: \"DELTA\"\n\n   .. py:attribute:: DELTASHARING\n      :value: \"DELTASHARING\"\n\n   .. py:attribute:: HIVE_CUSTOM\n      :value: \"HIVE_CUSTOM\"\n\n   .. py:attribute:: HIVE_SERDE\n      :value: \"HIVE_SERDE\"\n\n   .. py:attribute:: JSON\n      :value: \"JSON\"\n\n   .. py:attribute:: MYSQL_FORMAT\n      :value: \"MYSQL_FORMAT\"\n\n   .. py:attribute:: NETSUITE_FORMAT\n      :value: \"NETSUITE_FORMAT\"\n\n   .. py:attribute:: ORC\n      :value: \"ORC\"\n\n   .. py:attribute:: PARQUET\n      :value: \"PARQUET\"\n\n   .. py:attribute:: POSTGRESQL_FORMAT\n      :value: \"POSTGRESQL_FORMAT\"\n\n   .. py:attribute:: REDSHIFT_FORMAT\n      :value: \"REDSHIFT_FORMAT\"\n\n   .. py:attribute:: SALESFORCE_FORMAT\n      :value: \"SALESFORCE_FORMAT\"\n\n   .. py:attribute:: SNOWFLAKE_FORMAT\n      :value: \"SNOWFLAKE_FORMAT\"\n\n   .. py:attribute:: SQLDW_FORMAT\n      :value: \"SQLDW_FORMAT\"\n\n   .. py:attribute:: SQLSERVER_FORMAT\n      :value: \"SQLSERVER_FORMAT\"\n\n   .. py:attribute:: TEXT\n      :value: \"TEXT\"\n\n   .. py:attribute:: UNITY_CATALOG\n      :value: \"UNITY_CATALOG\"\n\n   .. py:attribute:: VECTOR_INDEX_FORMAT\n      :value: \"VECTOR_INDEX_FORMAT\"\n\n   .. py:attribute:: WORKDAY_RAAS_FORMAT\n      :value: \"WORKDAY_RAAS_FORMAT\"\n```\n\n----------------------------------------\n\nTITLE: Defining Language Enum\nDESCRIPTION: Defines the supported languages for notebooks. Available options include PYTHON, R, SCALA, and SQL. This enum is used when creating or managing notebooks in the Databricks workspace.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/workspace.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: Language\n\n   The language of notebook.\n\n   .. py:attribute:: PYTHON\n      :value: \"PYTHON\"\n\n   .. py:attribute:: R\n      :value: \"R\"\n\n   .. py:attribute:: SCALA\n      :value: \"SCALA\"\n\n   .. py:attribute:: SQL\n      :value: \"SQL\"\n```\n\n----------------------------------------\n\nTITLE: Defining CatalogIsolationMode Enum in Python\nDESCRIPTION: This code defines an enumeration `CatalogIsolationMode` with members ISOLATED and OPEN. This enumeration describes the accessibility of a securable, whether it's restricted to specific workspaces or accessible from all workspaces.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CatalogIsolationMode\n\n   Whether the current securable is accessible from all workspaces or a specific set of workspaces.\n\n   .. py:attribute:: ISOLATED\n      :value: \"ISOLATED\"\n\n   .. py:attribute:: OPEN\n      :value: \"OPEN\"\n```\n\n----------------------------------------\n\nTITLE: Defining Queuing Reason Constants in QueueDetailsCodeCode in Python\nDESCRIPTION: This snippet defines constants representing reasons for queuing a job run.  These reasons include reaching active run limits, maximum concurrent run limits, and active run job tasks limits.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nACTIVE_RUNS_LIMIT_REACHED = \"ACTIVE_RUNS_LIMIT_REACHED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nACTIVE_RUN_JOB_TASKS_LIMIT_REACHED = \"ACTIVE_RUN_JOB_TASKS_LIMIT_REACHED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nMAX_CONCURRENT_RUNS_REACHED = \"MAX_CONCURRENT_RUNS_REACHED\"\n```\n\n----------------------------------------\n\nTITLE: Defining WarehousePermissionLevel in Python\nDESCRIPTION: This snippet defines the `WarehousePermissionLevel` class with its attributes representing different permission levels for a warehouse. The attributes include `CAN_MANAGE`, `CAN_MONITOR`, `CAN_USE`, `CAN_VIEW`, and `IS_OWNER`.  These define the actions a user is allowed to do on a specific Databricks SQL Warehouse.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass WarehousePermissionLevel:\n   \"\"\"Permission level\"\"\"\n\n   CAN_MANAGE = \"CAN_MANAGE\"\n   CAN_MONITOR = \"CAN_MONITOR\"\n   CAN_USE = \"CAN_USE\"\n   CAN_VIEW = \"CAN_VIEW\"\n   IS_OWNER = \"IS_OWNER\"\n```\n\n----------------------------------------\n\nTITLE: Create a Databricks Cluster using SDK\nDESCRIPTION: This snippet shows how to create a new Databricks cluster using the Databricks SDK for Python. It utilizes the WorkspaceClient to interact with the Clusters API and creates a cluster with specified configurations, cleaning up the cluster after creation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/clusters.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nlatest = w.clusters.select_spark_version(latest=True, long_term_support=True)\n\ncluster_name = f\"sdk-{time.time_ns()}\"\n\nclstr = w.clusters.create(\n    cluster_name=cluster_name,\n    spark_version=latest,\n    instance_pool_id=os.environ[\"TEST_INSTANCE_POOL_ID\"],\n    autotermination_minutes=15,\n    num_workers=1,\n).result()\n\n# cleanup\nw.clusters.permanent_delete(cluster_id=clstr.cluster_id)\n```\n\n----------------------------------------\n\nTITLE: Defining SqlDashboardWidgetOutputStatus Enum in Python\nDESCRIPTION: Defines the `SqlDashboardWidgetOutputStatus` enumeration, representing the status of a SQL dashboard widget. It includes values like `CANCELLED`, `FAILED`, `PENDING`, `RUNNING`, and `SUCCESS`. Used to track the status of individual widgets within a SQL dashboard.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n.. py:attribute:: CANCELLED\n      :value: \"CANCELLED\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: PENDING\n      :value: \"PENDING\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n\n   .. py:attribute:: SUCCESS\n      :value: \"SUCCESS\"\n```\n\n----------------------------------------\n\nTITLE: Listing Repositories with Pagination in Databricks SDK\nDESCRIPTION: This example demonstrates how to list all repositories in a Databricks workspace using the `repos.list()` method. The SDK handles pagination internally, allowing you to iterate through all repositories without needing to manage offsets or cursors. For each repo, the path is logged using the `logging` module.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient()\nfor repo in w.repos.list():\n    logging.info(f'Found repo: {repo.path}')\n```\n\n----------------------------------------\n\nTITLE: Deleting a Credential in Databricks (Python)\nDESCRIPTION: This method deletes a credential from the Databricks metastore. The caller must be the owner of the credential. The 'force' parameter can be used to delete the credential even if there are dependent services or external locations/tables.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/credentials.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete_credential(name_arg: str [, force: Optional[bool]])\n\n        Delete a credential.\n\n        Deletes a service or storage credential from the metastore. The caller must be an owner of the\n        credential.\n\n        :param name_arg: str\n          Name of the credential.\n        :param force: bool (optional)\n          Force an update even if there are dependent services (when purpose is **SERVICE**) or dependent\n          external locations and external tables (when purpose is **STORAGE**).\n```\n\n----------------------------------------\n\nTITLE: Defining AmazonBedrockConfigBedrockProvider Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `AmazonBedrockConfigBedrockProvider` with possible values AI21LABS, AMAZON, ANTHROPIC, and COHERE. It specifies the provider used in Amazon Bedrock configuration.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: AmazonBedrockConfigBedrockProvider\n\n   .. py:attribute:: AI21LABS\n      :value: \"AI21LABS\"\n\n   .. py:attribute:: AMAZON\n      :value: \"AMAZON\"\n\n   .. py:attribute:: ANTHROPIC\n      :value: \"ANTHROPIC\"\n\n   .. py:attribute:: COHERE\n      :value: \"COHERE\"\n```\n\n----------------------------------------\n\nTITLE: Defining Instance Pool States in Python\nDESCRIPTION: This code defines the possible states for an instance pool. It includes ACTIVE, DELETED, and STOPPED as valid states, along with the allowable state transitions.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: InstancePoolState\n\n   The state of a Cluster. The current allowable state transitions are as follows:\n   - ``ACTIVE`` -> ``STOPPED`` - ``ACTIVE`` -> ``DELETED`` - ``STOPPED`` -> ``ACTIVE`` - ``STOPPED`` -> ``DELETED``\n\n   .. py:attribute:: ACTIVE\n      :value: \"ACTIVE\"\n\n   .. py:attribute:: DELETED\n      :value: \"DELETED\"\n\n   .. py:attribute:: STOPPED\n      :value: \"STOPPED\"\n```\n\n----------------------------------------\n\nTITLE: ExperimentPermissionLevel Constants Definition in Python\nDESCRIPTION: Defines constants that represent different permission levels for an experiment. These constants specify the degree of access and control a user has over an experiment, such as `CAN_EDIT`, `CAN_MANAGE`, or `CAN_READ`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ExperimentPermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_EDIT\n      :value: \"CAN_EDIT\"\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_READ\n      :value: \"CAN_READ\"\n```\n\n----------------------------------------\n\nTITLE: Listing Recent Job Runs with Databricks SDK\nDESCRIPTION: This snippet shows how to retrieve and summarize recent job runs in a Databricks workspace. It fetches all jobs, iterates through their runs, and calculates the average duration and latest state for each job. The output is a sorted list of jobs with their last status, finish time, and average duration. It requires the databricks-sdk library.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/pagination.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom collections import defaultdict\nfrom datetime import datetime, timezone\nfrom databricks.sdk import WorkspaceClient\nlatest_state = {}\nall_jobs = {}\ndurations = defaultdict(list)\nw = WorkspaceClient()\nfor job in w.jobs.list():\n    all_jobs[job.job_id] = job\n    for run in w.jobs.list_runs(job_id=job.job_id, expand_tasks=False):\n        durations[job.job_id].append(run.run_duration)\n        if job.job_id not in latest_state:\n            latest_state[job.job_id] = run\n            continue\n        if run.end_time < latest_state[job.job_id].end_time:\n            continue\n        latest_state[job.job_id] = run\nsummary = []\nfor job_id, run in latest_state.items():\n    summary.append({\n        'job_name': all_jobs[job_id].settings.name,\n        'last_status': run.state.result_state,\n        'last_finished': datetime.fromtimestamp(run.end_time/1000, timezone.utc),\n        'average_duration': sum(durations[job_id]) / len(durations[job_id])\n    })\nfor line in sorted(summary, key=lambda s: s['last_finished'], reverse=True):\n    logging.info(f'Latest: {line}')\n```\n\n----------------------------------------\n\nTITLE: Waiting for Serving Endpoint Not Updating in Databricks\nDESCRIPTION: This method waits for a serving endpoint to no longer be in an updating state. It accepts the endpoint name, a timeout duration, and an optional callback function to be executed during the wait.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: wait_get_serving_endpoint_not_updating(name: str, timeout: datetime.timedelta = 0:20:00, callback: Optional[Callable[[ServingEndpointDetailed], None]]) -> ServingEndpointDetailed\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Public Client OAuth Application (Terraform)\nDESCRIPTION: This Terraform configuration creates an Azure AD application configured as a public client for OAuth. It defines a public client with a redirect URI and outputs the application's client ID.\nDependencies: AzureAD provider\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/oauth.md#_snippet_3\n\nLANGUAGE: hcl\nCODE:\n```\nresource \"azuread_application\" \"public_client\" {\n  display_name     = \"sample-oauth-app-public-client\"\n  owners           = [data.azuread_client_config.current.object_id]\n  sign_in_audience = \"AzureADMyOrg\"\n  public_client {\n    redirect_uris = [\"http://localhost:8080/\"]\n  }\n}\n\noutput \"public_client_id\" {\n  value = azuread_application.public_client.application_id\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Online Table\nDESCRIPTION: This method deletes an existing online table. It requires the full three-part name (catalog, schema, table) of the table to be deleted. Deleting an online table results in the permanent loss of data if the source Delta table has been deleted or modified.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/online_tables.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(name: str)\n\n    Delete an Online Table.\n\n    Delete an online table. Warning: This will delete all the data in the online table. If the source\n    Delta table was deleted or modified since this Online Table was created, this will lose the data\n    forever!\n\n    :param name: str\n      Full three-part (catalog, schema, table) name of the table.\n```\n\n----------------------------------------\n\nTITLE: Defining Visibility Class with Attributes in Python\nDESCRIPTION: This code snippet defines a `Visibility` class using the `py:class` directive. It includes two class attributes, `PRIVATE` and `PUBLIC`, each assigned a string value.  This class likely represents the visibility settings for a data sharing entity.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: Visibility\n\n   .. py:attribute:: PRIVATE\n      :value: \"PRIVATE\"\n\n   .. py:attribute:: PUBLIC\n      :value: \"PUBLIC\"\n```\n\n----------------------------------------\n\nTITLE: Defining Event Details Cause in Python\nDESCRIPTION: This code defines the possible causes for a change in target size within an event detail. It includes AUTORECOVERY, AUTOSCALE, REPLACE_BAD_NODES, and USER_REQUEST as potential causes.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: EventDetailsCause\n\n   The cause of a change in target size.\n\n   .. py:attribute:: AUTORECOVERY\n      :value: \"AUTORECOVERY\"\n\n   .. py:attribute:: AUTOSCALE\n      :value: \"AUTOSCALE\"\n\n   .. py:attribute:: REPLACE_BAD_NODES\n      :value: \"REPLACE_BAD_NODES\"\n\n   .. py:attribute:: USER_REQUEST\n      :value: \"USER_REQUEST\"\n```\n\n----------------------------------------\n\nTITLE: EventLevel Enum Definition in Python\nDESCRIPTION: Defines an enumeration representing the severity level of a pipeline event. It includes levels such as ERROR, INFO, METRICS, and WARN, allowing for categorization of events based on their severity.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EventLevel\n\n   The severity level of the event.\n\n   .. py:attribute:: ERROR\n      :value: \"ERROR\"\n\n   .. py:attribute:: INFO\n      :value: \"INFO\"\n\n   .. py:attribute:: METRICS\n      :value: \"METRICS\"\n\n   .. py:attribute:: WARN\n      :value: \"WARN\"\n```\n\n----------------------------------------\n\nTITLE: Defining WorkspaceObjectPermissionLevel Enum\nDESCRIPTION: Defines permission levels for workspace objects. Includes CAN_EDIT, CAN_MANAGE, CAN_READ, and CAN_RUN permissions. These values are used when setting access control for objects within the Databricks workspace.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/workspace.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: WorkspaceObjectPermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_EDIT\n      :value: \"CAN_EDIT\"\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_READ\n      :value: \"CAN_READ\"\n\n   .. py:attribute:: CAN_RUN\n      :value: \"CAN_RUN\"\n```\n\n----------------------------------------\n\nTITLE: Getting a Workspace by ID with Databricks AccountClient in Python\nDESCRIPTION: This snippet demonstrates how to retrieve a Databricks workspace by its ID using the `AccountClient` from the `databricks.sdk`. It initializes the client, and then fetches the workspace using `a.workspaces.get(workspace_id=created.workspace_id)`. The `workspace_id` is expected to be an integer representing the unique identifier of the workspace.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/workspaces.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\ncreated = a.waiter.get()\n\nby_id = a.workspaces.get(workspace_id=created.workspace_id)\n```\n\n----------------------------------------\n\nTITLE: Update Share Recipient Databricks SDK (Python)\nDESCRIPTION: This snippet demonstrates updating a share recipient's comment using the Databricks SDK for Python. It utilizes the `w.recipients.update()` method to modify the recipient's `name` and `comment` attributes.  `w` likely represents a client object for interacting with Databricks.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/recipients.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nw.recipients.update(name=created.name, comment=f\"sdk-{time.time_ns()}\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Legacy Alert in Databricks SQL\nDESCRIPTION: This method creates a new alert in Databricks SQL. An alert periodically runs a query, evaluates a condition, and notifies users or notification destinations if the condition is met. It takes parameters like name, alert options, and query ID to configure the alert.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/alerts_legacy.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nw.alerts_legacy.create(name: str, options: AlertOptions, query_id: str [, parent: Optional[str], rearm: Optional[int]]) -> LegacyAlert\n```\n\n----------------------------------------\n\nTITLE: Listing Metastore Assignments with AccountClient in Python\nDESCRIPTION: This code snippet demonstrates how to list all workspaces assigned to a given metastore using the `AccountClient` and the `metastore_assignments.list` method. It requires the `databricks.sdk` and `os` modules.  The `TEST_METASTORE_ID` environment variable must be set.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/catalog/metastore_assignments.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nws = a.metastore_assignments.list(metastore_id=os.environ[\"TEST_METASTORE_ID\"])\n```\n\n----------------------------------------\n\nTITLE: Adding Token Management Permissions Methods - Python\nDESCRIPTION: This snippet introduces methods for managing token permissions within the Databricks workspace via the `w.token_management` service. The added methods are: `get_token_permission_levels()`, `get_token_permissions()`, `set_token_permissions()`, and `update_token_permissions()`. These methods provide capabilities to control access to tokens.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nw.token_management.get_token_permission_levels()\nw.token_management.get_token_permissions()\nw.token_management.set_token_permissions()\nw.token_management.update_token_permissions()\n```\n\n----------------------------------------\n\nTITLE: Defining SSE Encryption Algorithms in Python\nDESCRIPTION: This code defines the SseEncryptionDetailsAlgorithm class, enumerating the supported server-side encryption (SSE) algorithms. Each algorithm is represented as a class attribute with a string value.  Algorithms include AWS_SSE_KMS and AWS_SSE_S3. This affects headers from s3 client.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass SseEncryptionDetailsAlgorithm:\n   \"\"\"The type of key encryption to use (affects headers from s3 client).\"\"\"\n\n   AWS_SSE_KMS = \"AWS_SSE_KMS\"\n\n   AWS_SSE_S3 = \"AWS_SSE_S3\"\n```\n\n----------------------------------------\n\nTITLE: Listing all Secret Scopes in Databricks using Python SDK\nDESCRIPTION: This code snippet demonstrates how to list all secret scopes available in the Databricks workspace using the Python SDK. It initializes a WorkspaceClient and retrieves the list of scopes using `w.secrets.list_scopes()`.  The code assumes that the user has the necessary permissions to list scopes.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/secrets.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nscopes = w.secrets.list_scopes()\n```\n\n----------------------------------------\n\nTITLE: Defining Function Parameter Type for Delta Sharing in Python\nDESCRIPTION: This code defines an enumeration for function parameter types, specifying whether a parameter is a COLUMN or a PARAM.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sharing.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass FunctionParameterType:\n    COLUMN = \"COLUMN\"\n    PARAM = \"PARAM\"\n```\n\n----------------------------------------\n\nTITLE: Defining Source Enum in Python\nDESCRIPTION: Defines the `Source` enumeration to specify the source of a SQL file for a Databricks SQL task. It can be either `WORKSPACE` (Databricks workspace) or `GIT` (cloud Git provider). This is used when defining a SQL task, clarifying where the SQL file is located.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n.. py:attribute:: GIT\n      :value: \"GIT\"\n\n   .. py:attribute:: WORKSPACE\n      :value: \"WORKSPACE\"\n```\n\n----------------------------------------\n\nTITLE: Settings API Property: enable_results_downloading\nDESCRIPTION: This property controls whether users can download notebook results. By default, this setting is enabled.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/settings.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. py:property:: enable_results_downloading\n    :type: EnableResultsDownloadingAPI\n\n    Controls whether users can download notebook results. By default, this setting is enabled.\n```\n\n----------------------------------------\n\nTITLE: Defining AppResourceServingEndpointServingEndpointPermission Enum (Python)\nDESCRIPTION: This Python code defines an enumeration `AppResourceServingEndpointServingEndpointPermission` with values representing the permission level that can be granted on a Serving Endpoint resource associated with an App, such as `CAN_MANAGE`, `CAN_QUERY`, and `CAN_VIEW`. These permissions control access and actions allowed on the serving endpoint.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/apps.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: AppResourceServingEndpointServingEndpointPermission\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_QUERY\n      :value: \"CAN_QUERY\"\n\n   .. py:attribute:: CAN_VIEW\n      :value: \"CAN_VIEW\"\n```\n\n----------------------------------------\n\nTITLE: Defining AuthenticationMethod Enum Python\nDESCRIPTION: This code snippet defines an enumeration called `AuthenticationMethod` with possible values `OAUTH` and `PAT`, representing different authentication methods for Databricks jobs.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: AuthenticationMethod\n\n   .. py:attribute:: OAUTH\n      :value: \"OAUTH\"\n\n   .. py:attribute:: PAT\n      :value: \"PAT\"\n```\n\n----------------------------------------\n\nTITLE: Defining EndpointStateConfigUpdate Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `EndpointStateConfigUpdate` with possible values IN_PROGRESS, NOT_UPDATING, UPDATE_CANCELED, and UPDATE_FAILED. It indicates the state of an endpoint configuration update.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EndpointStateConfigUpdate\n\n   .. py:attribute:: IN_PROGRESS\n      :value: \"IN_PROGRESS\"\n\n   .. py:attribute:: NOT_UPDATING\n      :value: \"NOT_UPDATING\"\n\n   .. py:attribute:: UPDATE_CANCELED\n      :value: \"UPDATE_CANCELED\"\n\n   .. py:attribute:: UPDATE_FAILED\n      :value: \"UPDATE_FAILED\"\n```\n\n----------------------------------------\n\nTITLE: Creating Git Credentials with Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to create Git credentials using the Databricks SDK. It initializes a WorkspaceClient, creates a credential, and then cleans up by deleting the created credential. It showcases the `create` and `delete` methods of the `git_credentials` API.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/git_credentials.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncr = w.git_credentials.create(git_provider=\"gitHub\", git_username=\"test\", personal_access_token=\"test\")\n\n# cleanup\nw.git_credentials.delete(credential_id=cr.credential_id)\n```\n\n----------------------------------------\n\nTITLE: Executing a Message Query in Genie using Python\nDESCRIPTION: This deprecated method executes the SQL query associated with a message in a Genie conversation. It requires the space ID, conversation ID, and message ID. The method returns a GenieGetMessageQueryResultResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/genie.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: execute_message_query(space_id: str, conversation_id: str, message_id: str) -> GenieGetMessageQueryResultResponse\n\n        [Deprecated] Execute SQL query in a conversation message.\n\n        Execute the SQL query in the message.\n\n        :param space_id: str\n          Genie space ID\n        :param conversation_id: str\n          Conversation ID\n        :param message_id: str\n          Message ID\n\n        :returns: :class:`GenieGetMessageQueryResultResponse`\n```\n\n----------------------------------------\n\nTITLE: Getting a Databricks Repo by ID in Python\nDESCRIPTION: This snippet shows how to retrieve a Databricks Repo by its ID using the WorkspaceClient and the repos.get method. It first creates a repo, then retrieves it using its ID, and finally cleans up by deleting the repo.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/repos.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nroot = f\"sdk-{time.time_ns()}\"\n\nri = w.repos.create(\n    path=root,\n    url=\"https://github.com/shreyas-goenka/empty-repo.git\",\n    provider=\"github\",\n)\n\nby_id = w.repos.get(repo_id=ri.id)\n\n# cleanup\nw.repos.delete(repo_id=ri.id)\n```\n\n----------------------------------------\n\nTITLE: Waiting for Forecasting Experiment Success using Databricks SDK\nDESCRIPTION: This snippet demonstrates how to wait for a forecasting experiment to succeed using the `wait_get_experiment_forecasting_succeeded` method of the `ForecastingAPI` class. It requires the experiment ID and allows specifying a timeout and an optional callback function. The method returns a `ForecastingExperiment` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/forecasting.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nw.forecasting.wait_get_experiment_forecasting_succeeded(experiment_id: str, timeout: datetime.timedelta = 2:00:00, callback: Optional[Callable[[ForecastingExperiment], None]]) -> ForecastingExperiment\n```\n\n----------------------------------------\n\nTITLE: Defining ResultType Enum (Python)\nDESCRIPTION: Defines an enumeration `ResultType` that represents the type of result returned from a Databricks operation. Attributes include `ERROR`, `IMAGE`, `IMAGES`, `TABLE`, and `TEXT`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ResultType\n\n   .. py:attribute:: ERROR\n      :value: \"ERROR\"\n\n   .. py:attribute:: IMAGE\n      :value: \"IMAGE\"\n\n   .. py:attribute:: IMAGES\n      :value: \"IMAGES\"\n\n   .. py:attribute:: TABLE\n      :value: \"TABLE\"\n\n   .. py:attribute:: TEXT\n      :value: \"TEXT\"\n```\n\n----------------------------------------\n\nTITLE: Update Serving Endpoint Rate Limits - Python\nDESCRIPTION: Updates the rate limits of a serving endpoint.  This method is deprecated and suggests using AI Gateway for rate limit management. Requires the name of the serving endpoint and a list of endpoint rate limits.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: put(name: str [, rate_limits: Optional[List[RateLimit]]]) -> PutResponse\n\n    Update rate limits of a serving endpoint.\n\n    Deprecated: Please use AI Gateway to manage rate limits instead.\n\n    :param name: str\n      The name of the serving endpoint whose rate limits are being updated. This field is required.\n    :param rate_limits: List[:class:`RateLimit`] (optional)\n      The list of endpoint rate limits.\n\n    :returns: :class:`PutResponse`\n```\n\n----------------------------------------\n\nTITLE: Getting a Dashboard using Databricks SDK\nDESCRIPTION: This code snippet demonstrates how to retrieve a dashboard definition using the Databricks SDK. It creates a dashboard, then retrieves it using the `get` method, and finally cleans up by deleting the created dashboard.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/dashboards.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\ncreated = w.dashboards.create(name=f\"sdk-{time.time_ns()}\")\n\nby_id = w.dashboards.get(dashboard_id=created.id)\n\n# cleanup\nw.dashboards.delete(dashboard_id=created.id)\n```\n\n----------------------------------------\n\nTITLE: Defining AiGatewayGuardrailPiiBehaviorBehavior Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `AiGatewayGuardrailPiiBehaviorBehavior` with possible values BLOCK and NONE. It is part of the AI Gateway guardrail configuration for controlling Personally Identifiable Information (PII) behavior.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: AiGatewayGuardrailPiiBehaviorBehavior\n\n   .. py:attribute:: BLOCK\n      :value: \"BLOCK\"\n\n   .. py:attribute:: NONE\n      :value: \"NONE\"\n```\n\n----------------------------------------\n\nTITLE: Defining CredentialType Enum in Python\nDESCRIPTION: This code defines an enumeration `CredentialType` with members BEARER_TOKEN and USERNAME_PASSWORD. These represent the type of authentication credential used, allowing specification between using a bearer token or a username/password combination.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CredentialType\n\n   The type of credential.\n\n   .. py:attribute:: BEARER_TOKEN\n      :value: \"BEARER_TOKEN\"\n\n   .. py:attribute:: USERNAME_PASSWORD\n      :value: \"USERNAME_PASSWORD\"\n```\n\n----------------------------------------\n\nTITLE: Retrieve Legacy Access Disablement Status (Python)\nDESCRIPTION: Retrieves the current legacy access disablement status. The etag parameter is used for optimistic concurrency control.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/disable_legacy_access.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get( [, etag: Optional[str]]) -> DisableLegacyAccess\n\n    Retrieve Legacy Access Disablement Status.\n\n    Retrieves legacy access disablement Status.\n\n    :param etag: str (optional)\n      etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n      optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n      each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n      to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n      request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n    :returns: :class:`DisableLegacyAccess`\n```\n\n----------------------------------------\n\nTITLE: Scanning a Vector Search Index\nDESCRIPTION: This method scans a specified vector search index and returns the first `num_results` entries after the exclusive `primary_key`. It requires the index name and optionally accepts the last primary key and the number of results.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_indexes.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: scan_index(index_name: str [, last_primary_key: Optional[str], num_results: Optional[int]]) -> ScanVectorIndexResponse\n\n        Scan an index.\n\n        Scan the specified vector index and return the first `num_results` entries after the exclusive\n        `primary_key`.\n\n        :param index_name: str\n          Name of the vector index to scan.\n        :param last_primary_key: str (optional)\n          Primary key of the last entry returned in the previous scan.\n        :param num_results: int (optional)\n          Number of results to return. Defaults to 10.\n\n        :returns: :class:`ScanVectorIndexResponse`\n```\n\n----------------------------------------\n\nTITLE: KeyUseCase Enum Definition (Python)\nDESCRIPTION: This enumeration specifies the possible uses of a key within Databricks in the `databricks.sdk.service.provisioning` module.  It defines whether a key is used for encrypting notebook and secret data in the control plane (`MANAGED_SERVICES`) or for encrypting the workspace's root S3 bucket and optionally cluster EBS volumes (`STORAGE`).\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/provisioning.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: KeyUseCase\n\n   Possible values are: * `MANAGED_SERVICES`: Encrypts notebook and secret data in the control plane * `STORAGE`: Encrypts the workspace's root S3 bucket (root DBFS and system data) and, optionally, cluster EBS volumes.\n\n   .. py:attribute:: MANAGED_SERVICES\n      :value: \"MANAGED_SERVICES\"\n\n   .. py:attribute:: STORAGE\n      :value: \"STORAGE\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Provider Analytics Dashboard with Databricks SDK\nDESCRIPTION: Retrieves a provider analytics dashboard using the `get()` method. This method retrieves provider analytics dashboard. The return type is a `ListProviderAnalyticsDashboardResponse` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_provider_analytics_dashboards.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nw.provider_provider_analytics_dashboards.get() -> ListProviderAnalyticsDashboardResponse\n```\n\n----------------------------------------\n\nTITLE: Get Enhanced Security Monitoring Setting\nDESCRIPTION: Retrieves the enhanced security monitoring setting for the current workspace. It supports optimistic concurrency control using an etag to prevent simultaneous write conflicts.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/enhanced_security_monitoring.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get( [, etag: Optional[str]]) -> EnhancedSecurityMonitoringSetting\n\n    Get the enhanced security monitoring setting.\n\n    Gets the enhanced security monitoring setting.\n\n    :param etag: str (optional)\n      etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n      optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n      each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n      to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n      request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n    :returns: :class:`EnhancedSecurityMonitoringSetting`\n```\n\n----------------------------------------\n\nTITLE: Autogenerating Documentation for SubscriptionSubscriberUser Class in Python\nDESCRIPTION: This code snippet uses the 'autoclass' directive to automatically generate documentation for the SubscriptionSubscriberUser class. The ':members:' option includes all public members in the documentation, while ':undoc-members:' excludes undocumented members. This is part of the Databricks SDK documentation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: SubscriptionSubscriberUser\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Getting a Model Version in Databricks with Python\nDESCRIPTION: Retrieves a specific model version. The caller must have appropriate privileges on the parent registered model. Parameters include the full name of the model version and its version number. Optionally, aliases and browse permissions can be included in the response.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/model_versions.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nw.model_versions.get(full_name: str, version: int [, include_aliases: Optional[bool], include_browse: Optional[bool]]) -> ModelVersionInfo\n```\n\n----------------------------------------\n\nTITLE: Getting an App Deployment in Databricks using Python\nDESCRIPTION: This code snippet shows how to retrieve information about an app deployment using the `get_deployment` method. It requires the app name and the deployment ID as strings. The method returns an `AppDeployment` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/apps/apps.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_deployment(app_name: str, deployment_id: str) -> AppDeployment\n\n        Get an app deployment.\n\n        Retrieves information for the app deployment with the supplied name and deployment id.\n\n        :param app_name: str\n          The name of the app.\n        :param deployment_id: str\n          The unique id of the deployment.\n\n        :returns: :class:`AppDeployment`\n```\n\n----------------------------------------\n\nTITLE: Defining JobPermissionLevel Enum Python\nDESCRIPTION: This code snippet defines an enumeration called `JobPermissionLevel` representing different permission levels for Databricks jobs. It includes `CAN_MANAGE`, `CAN_MANAGE_RUN`, `CAN_VIEW`, and `IS_OWNER`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: JobPermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_MANAGE_RUN\n      :value: \"CAN_MANAGE_RUN\"\n\n   .. py:attribute:: CAN_VIEW\n      :value: \"CAN_VIEW\"\n\n   .. py:attribute:: IS_OWNER\n      :value: \"IS_OWNER\"\n```\n\n----------------------------------------\n\nTITLE: Listing Dashboard Schedules\nDESCRIPTION: Lists schedules for a given Lakeview dashboard using the LakeviewAPI. Requires dashboard ID. Supports pagination. Returns an iterator over Schedule objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: list_schedules(dashboard_id: str [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[Schedule]\n\n    List dashboard schedules.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard to which the schedules belongs.\n    :param page_size: int (optional)\n      The number of schedules to return per page.\n    :param page_token: str (optional)\n      A page token, received from a previous `ListSchedules` call. Use this to retrieve the subsequent\n      page.\n\n    :returns: Iterator over :class:`Schedule`\n```\n\n----------------------------------------\n\nTITLE: Get Artifact Allowlist - Python\nDESCRIPTION: Retrieves the artifact allowlist for a specified artifact type.  Requires metastore admin privileges or the MANAGE ALLOWLIST privilege. Takes the artifact type as input and returns ArtifactAllowlistInfo.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/artifact_allowlists.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(artifact_type: ArtifactType) -> ArtifactAllowlistInfo\n\n    Get an artifact allowlist.\n\n    Get the artifact allowlist of a certain artifact type. The caller must be a metastore admin or have\n    the **MANAGE ALLOWLIST** privilege on the metastore.\n\n    :param artifact_type: :class:`ArtifactType`\n      The artifact type of the allowlist.\n\n    :returns: :class:`ArtifactAllowlistInfo`\n```\n\n----------------------------------------\n\nTITLE: DeleteTransitionRequestStage Constants Definition in Python\nDESCRIPTION: Defines constants representing the stage of a model version when deleting a transition request. These stages like `ARCHIVED`, `NONE`, `PRODUCTION` and `STAGING` denote the environment of the model the request concerns.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: DeleteTransitionRequestStage\n\n   .. py:attribute:: ARCHIVED\n      :value: \"ARCHIVED\"\n\n   .. py:attribute:: NONE\n      :value: \"NONE\"\n\n   .. py:attribute:: PRODUCTION\n      :value: \"PRODUCTION\"\n\n   .. py:attribute:: STAGING\n      :value: \"STAGING\"\n```\n\n----------------------------------------\n\nTITLE: Defining REMOVE attribute for PatchOp in Python\nDESCRIPTION: Defines the REMOVE attribute for the PatchOp class, representing the 'remove' operation type in a patch request.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PatchOp\n\n   Type of patch operation.\n\n   .. py:attribute:: REMOVE\n      :value: \"REMOVE\"\n```\n\n----------------------------------------\n\nTITLE: Getting Online Table Information\nDESCRIPTION: This method retrieves information about an existing online table. It requires the full three-part name (catalog, schema, table) of the table. The method returns an OnlineTable object containing the table's metadata and status.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/online_tables.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(name: str) -> OnlineTable\n\n    Get an Online Table.\n\n    Get information about an existing online table and its status.\n\n    :param name: str\n      Full three-part (catalog, schema, table) name of the table.\n\n    :returns: :class:`OnlineTable`\n```\n\n----------------------------------------\n\nTITLE: Defining Partition Value Operation for Delta Sharing in Python\nDESCRIPTION: This code defines an enumeration for partition value operations, including EQUAL and LIKE.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sharing.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass PartitionValueOp:\n    EQUAL = \"EQUAL\"\n    LIKE = \"LIKE\"\n```\n\n----------------------------------------\n\nTITLE: Defining DiskTypeEbsVolumeType Enum in Python\nDESCRIPTION: This code defines an enumeration (`DiskTypeEbsVolumeType`) for the supported EBS (Elastic Block Storage) volume types in Databricks. It includes GENERAL_PURPOSE_SSD as a possible value, representing a general-purpose SSD volume suitable for a variety of workloads.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: DiskTypeEbsVolumeType\n\n   All EBS volume types that Databricks supports. See https://aws.amazon.com/ebs/details/ for details.\n\n   .. py:attribute:: GENERAL_PURPOSE_SSD\n      :value: \"GENERAL_PURPOSE_SSD\"\n```\n\n----------------------------------------\n\nTITLE: Defining AlertState Enum (Python)\nDESCRIPTION: Defines an enumeration representing the possible states of an alert. The states are OK, TRIGGERED, and UNKNOWN, indicating the current status of the alert based on its defined conditions and the query results.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: AlertState\n\n   .. py:attribute:: OK\n      :value: \"OK\"\n\n   .. py:attribute:: TRIGGERED\n      :value: \"TRIGGERED\"\n\n   .. py:attribute:: UNKNOWN\n      :value: \"UNKNOWN\"\n```\n\n----------------------------------------\n\nTITLE: CleanRoomAssetAssetType Enum (Python)\nDESCRIPTION: Defines the CleanRoomAssetAssetType enumeration, representing the possible asset types within a Clean Room, such as FOREIGN_TABLE, NOTEBOOK_FILE, TABLE, VIEW, and VOLUME.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: CleanRoomAssetAssetType\n\n   .. py:attribute:: FOREIGN_TABLE\n      :value: \"FOREIGN_TABLE\"\n\n   .. py:attribute:: NOTEBOOK_FILE\n      :value: \"NOTEBOOK_FILE\"\n\n   .. py:attribute:: TABLE\n      :value: \"TABLE\"\n\n   .. py:attribute:: VIEW\n      :value: \"VIEW\"\n\n   .. py:attribute:: VOLUME\n      :value: \"VOLUME\"\n```\n\n----------------------------------------\n\nTITLE: Deploying an App in Databricks\nDESCRIPTION: Creates a new app deployment for the app with the supplied name, using source code from the specified workspace file system path, and sets the deployment mode. Returns a long-running operation waiter for the AppDeployment object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nw.apps.deploy(app_name: str, source_code_path: str, mode: AppDeploymentMode) -> Wait[AppDeployment]\n```\n\n----------------------------------------\n\nTITLE: Defining ServingEndpointDetailedPermissionLevel Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `ServingEndpointDetailedPermissionLevel` with possible values CAN_MANAGE, CAN_QUERY, and CAN_VIEW. It represents the permission level for a serving endpoint.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ServingEndpointDetailedPermissionLevel\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_QUERY\n      :value: \"CAN_QUERY\"\n\n   .. py:attribute:: CAN_VIEW\n      :value: \"CAN_VIEW\"\n```\n\n----------------------------------------\n\nTITLE: Defining ServingEndpointPermissionLevel Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `ServingEndpointPermissionLevel` with possible values CAN_MANAGE, CAN_QUERY, and CAN_VIEW. It represents the permission level for a serving endpoint.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ServingEndpointPermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_QUERY\n      :value: \"CAN_QUERY\"\n\n   .. py:attribute:: CAN_VIEW\n      :value: \"CAN_VIEW\"\n```\n\n----------------------------------------\n\nTITLE: Creating an Experiment with the MLflow Experiments API in Python\nDESCRIPTION: This code snippet demonstrates how to create an MLflow experiment using the Databricks SDK. It initializes a WorkspaceClient, creates an experiment with a unique name based on the current timestamp, and then cleans up by deleting the experiment.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/ml/experiments.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nexperiment = w.experiments.create_experiment(name=f\"sdk-{time.time_ns()}\")\n\n# cleanup\nw.experiments.delete_experiment(experiment_id=experiment.experiment_id)\n```\n\n----------------------------------------\n\nTITLE: Get Metastore Summary in Python\nDESCRIPTION: This snippet demonstrates how to retrieve a summary of the metastore using the Databricks SDK. It initializes a `WorkspaceClient` and then retrieves the summary information for the metastore by calling the `summary()` method.  This relies on the WorkspaceClient from the databricks.sdk.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/metastores.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nsummary = w.metastores.summary()\n```\n\n----------------------------------------\n\nTITLE: Create a directory (Python)\nDESCRIPTION: Creates an empty directory at the specified path. This method is idempotent and will also create parent directories if needed. It returns a success response if the directory already exists.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/files/files.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create_directory(directory_path: str)\n\n    Create a directory.\n\n    Creates an empty directory. If necessary, also creates any parent directories of the new, empty\n    directory (like the shell command `mkdir -p`). If called on an existing directory, returns a success\n    response; this method is idempotent (it will succeed if the directory already exists).\n\n    :param directory_path: str\n      The absolute path of a directory.\n```\n\n----------------------------------------\n\nTITLE: Google ID Authentication in Python\nDESCRIPTION: This code illustrates how to authenticate with Databricks using Google ID authentication. It requires the `databricks.sdk` package and prompts the user for the Databricks Workspace URL and Google Service Account email.  The `WorkspaceClient` is initialized with these credentials.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(host=input('Databricks Workspace URL: '),\n                    google_service_account=input('Google Service Account: '))\n```\n\n----------------------------------------\n\nTITLE: Listing Listing Installations using ConsumerInstallationsAPI in Python\nDESCRIPTION: Lists all installations for a particular listing. The `list_listing_installations` method requires the listing ID and accepts optional parameters for pagination, such as page size and page token.  It returns an iterator over InstallationDetail objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_installations.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nw.consumer_installations.list_listing_installations(listing_id: str [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[InstallationDetail]\n```\n\n----------------------------------------\n\nTITLE: Update Enhanced Security Monitoring Setting\nDESCRIPTION: Updates the enhanced security monitoring setting for the workspace. It requires a fresh etag for PATCH requests and handles concurrent updates by returning a 409 error. A field mask is also used to specify which fields to update.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/enhanced_security_monitoring.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(allow_missing: bool, setting: EnhancedSecurityMonitoringSetting, field_mask: str) -> EnhancedSecurityMonitoringSetting\n\n    Update the enhanced security monitoring setting.\n\n    Updates the enhanced security monitoring setting for the workspace. A fresh etag needs to be provided\n    in `PATCH` requests (as part of the setting field). The etag can be retrieved by making a `GET`\n    request before the `PATCH` request. If the setting is updated concurrently, `PATCH` fails with 409 and\n    the request must be retried by using the fresh etag in the 409 response.\n\n    :param allow_missing: bool\n      This should always be set to true for Settings API. Added for AIP compliance.\n    :param setting: :class:`EnhancedSecurityMonitoringSetting`\n    :param field_mask: str\n      The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n      field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n      `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n      the entire collection field can be specified. Field names must exactly match the resource field\n      names.\n\n      A field mask of `*` indicates full replacement. It’s recommended to always explicitly list the\n      fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n      changes in the future.\n\n    :returns: :class:`EnhancedSecurityMonitoringSetting`\n```\n\n----------------------------------------\n\nTITLE: Creating a Visualization in Databricks (Python)\nDESCRIPTION: This method adds a visualization to a specific query within Databricks. It requires the `CreateVisualizationRequestVisualization` object as input, which defines the properties of the new visualization. The method returns a `Visualization` object representing the newly created visualization.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/query_visualizations.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create( [, visualization: Optional[CreateVisualizationRequestVisualization]]) -> Visualization\n\n    Add a visualization to a query.\n\n    Adds a visualization to a query.\n\n    :param visualization: :class:`CreateVisualizationRequestVisualization` (optional)\n\n    :returns: :class:`Visualization`\n```\n\n----------------------------------------\n\nTITLE: Adding Repo Permissions Methods - Python\nDESCRIPTION: This snippet introduces methods for managing repo permissions within the Databricks workspace via the `w.repos` service. The added methods are: `get_repo_permission_levels()` and `get_repo_permissions()`. These methods provide functionalities to manage permissions associated with Repos.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nw.repos.get_repo_permission_levels()\nw.repos.get_repo_permissions()\n```\n\n----------------------------------------\n\nTITLE: Settings API Property: compliance_security_profile\nDESCRIPTION: This property controls whether to enable the compliance security profile for the current workspace. Enabling it on a workspace is permanent and cannot be disabled once enabled. By default, it is turned off.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/settings.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:property:: compliance_security_profile\n    :type: ComplianceSecurityProfileAPI\n\n    Controls whether to enable the compliance security profile for the current workspace. Enabling it on a\n    workspace is permanent. By default, it is turned off.\n\n    This settings can NOT be disabled once it is enabled.\n```\n\n----------------------------------------\n\nTITLE: Getting a Marketplace File in Python\nDESCRIPTION: This Python method retrieves details of a specific file from the Databricks Marketplace using its file ID. The method takes the file_id as a string parameter and returns a GetFileResponse object containing the file's metadata.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_files.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(file_id: str) -> GetFileResponse\n\n        Get a file.\n\n        Get a file\n\n        :param file_id: str\n\n        :returns: :class:`GetFileResponse`\n```\n\n----------------------------------------\n\nTITLE: Getting Usage Dashboard with Databricks SDK (Python)\nDESCRIPTION: This code snippet illustrates the 'get' method of the UsageDashboardsAPI. It retrieves an existing usage dashboard based on optional parameters like 'dashboard_type' and 'workspace_id'. The method returns a GetBillingUsageDashboardResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/usage_dashboards.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\na.usage_dashboards.get(dashboard_type: Optional[UsageDashboardType], workspace_id: Optional[int]) -> GetBillingUsageDashboardResponse\n```\n\n----------------------------------------\n\nTITLE: List Service Principal Federation Policies (Python)\nDESCRIPTION: Lists service principal federation policies for a given service principal ID. Supports pagination using `page_size` and `page_token`. Returns an iterator over FederationPolicy objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/service_principal_federation_policy.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list(service_principal_id: int [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[FederationPolicy]\n\n    List service principal federation policies.\n\n    :param service_principal_id: int\n      The service principal id for the federation policy.\n    :param page_size: int (optional)\n    :param page_token: str (optional)\n\n    :returns: Iterator over :class:`FederationPolicy`\n```\n\n----------------------------------------\n\nTITLE: Initializing WorkspaceClient for Default Authentication\nDESCRIPTION: This snippet demonstrates how to initialize the `WorkspaceClient` which uses the [default authentication flow]. This snippet is the most simple way to authenticate with Databricks if you have configuration profiles or environment variables set.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient()\nw. # press <TAB> for autocompletion\n```\n\n----------------------------------------\n\nTITLE: Getting a Credential in Databricks (Python)\nDESCRIPTION: This method retrieves a service or storage credential from the metastore. The caller must be a metastore admin, the owner of the credential, or have any permission on the credential.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/credentials.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_credential(name_arg: str) -> CredentialInfo\n\n        Get a credential.\n\n        Gets a service or storage credential from the metastore. The caller must be a metastore admin, the\n        owner of the credential, or have any permission on the credential.\n\n        :param name_arg: str\n          Name of the credential.\n\n        :returns: :class:`CredentialInfo`\n```\n\n----------------------------------------\n\nTITLE: Defining Dashboard View Constant in Python\nDESCRIPTION: This code defines a class `DashboardView` with a constant `DASHBOARD_VIEW_BASIC`. This constant is a string with the value \"DASHBOARD_VIEW_BASIC\". It is likely used to specify a basic view type when interacting with the Dashboards API.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: DashboardView\n\n   .. py:attribute:: DASHBOARD_VIEW_BASIC\n      :value: \"DASHBOARD_VIEW_BASIC\"\n```\n\n----------------------------------------\n\nTITLE: GetPipelineResponseHealth Enum Definition in Python\nDESCRIPTION: Defines an enumeration indicating the overall health status of a Delta Live Tables pipeline. It can be either HEALTHY or UNHEALTHY, providing a simple status indicator for the pipeline's operational state.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: GetPipelineResponseHealth\n\n   The health of a pipeline.\n\n   .. py:attribute:: HEALTHY\n      :value: \"HEALTHY\"\n\n   .. py:attribute:: UNHEALTHY\n      :value: \"UNHEALTHY\"\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_USE attribute for PasswordPermissionLevel in Python\nDESCRIPTION: Defines the CAN_USE attribute for the PasswordPermissionLevel class. This represents the permission level indicating that the user can use the password.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PasswordPermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_USE\n      :value: \"CAN_USE\"\n```\n\n----------------------------------------\n\nTITLE: Getting a Storage Credential (Python)\nDESCRIPTION: This method retrieves a specific storage credential from a metastore. The caller must be a metastore admin, the owner of the credential, or have appropriate privileges on the credential.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/catalog/storage_credentials.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmetastore_id: str, storage_credential_name: str\n```\n\n----------------------------------------\n\nTITLE: PrivateAccessLevel Enum (Python)\nDESCRIPTION: Defines an enumeration specifying the private access level for a Databricks workspace in the `databricks.sdk.service.provisioning` module.  It controls which VPC endpoints can connect to the workspace's UI or API.  Values include `ACCOUNT` and `ENDPOINT`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/provisioning.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PrivateAccessLevel\n\n   The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. * `ACCOUNT` level access (the default) allows only VPC endpoints that are registered in your Databricks account connect to your workspace. * `ENDPOINT` level access allows only specified VPC endpoints connect to your workspace. For details, see `allowed_vpc_endpoint_ids`.\n\n   .. py:attribute:: ACCOUNT\n      :value: \"ACCOUNT\"\n\n   .. py:attribute:: ENDPOINT\n      :value: \"ENDPOINT\"\n```\n\n----------------------------------------\n\nTITLE: Defining URN for ServicePrincipalSchema in Python\nDESCRIPTION: Defines the URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_SERVICE_PRINCIPAL attribute for the ServicePrincipalSchema class. It represents the SCIM schema for a service principal with a URN string value.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ServicePrincipalSchema\n\n   .. py:attribute:: URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_SERVICE_PRINCIPAL\n      :value: \"URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_SERVICE_PRINCIPAL\"\n```\n\n----------------------------------------\n\nTITLE: Deleting a Model Version in Databricks with Python\nDESCRIPTION: Deletes a model version from the specified registered model. The caller must have appropriate privileges.  Any aliases assigned to the model version will also be deleted.  The full name of the model version and the version number are required.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/model_versions.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nw.model_versions.delete(full_name: str, version: int)\n```\n\n----------------------------------------\n\nTITLE: Defining AiGatewayRateLimitRenewalPeriod Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `AiGatewayRateLimitRenewalPeriod` with possible value MINUTE.  It specifies the time unit used for rate limiting renewal in the AI Gateway.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: AiGatewayRateLimitRenewalPeriod\n\n   .. py:attribute:: MINUTE\n      :value: \"MINUTE\"\n```\n\n----------------------------------------\n\nTITLE: Listing Installations using ConsumerInstallationsAPI in Python\nDESCRIPTION: Lists all installations across all listings. The `list` method accepts optional parameters for pagination, such as page size and page token. It returns an iterator over InstallationDetail objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_installations.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nw.consumer_installations.list( [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[InstallationDetail]\n```\n\n----------------------------------------\n\nTITLE: Delete Metastore - Python\nDESCRIPTION: Deletes a Unity Catalog metastore for an account using the provided metastore ID. The `force` parameter allows deletion even if the metastore is not empty. Requires a metastore ID.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/catalog/metastores.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(metastore_id: str [, force: Optional[bool]])\n\n    Delete a metastore.\n\n    Deletes a Unity Catalog metastore for an account, both specified by ID.\n\n    :param metastore_id: str\n      Unity Catalog metastore ID\n    :param force: bool (optional)\n      Force deletion even if the metastore is not empty. Default is false.\n```\n\n----------------------------------------\n\nTITLE: Update Artifact Allowlist - Python\nDESCRIPTION: Sets the artifact allowlist for a specified artifact type, replacing the existing allowlist. Requires metastore admin privileges or the MANAGE ALLOWLIST privilege. Takes the artifact type and a list of artifact matchers as input, and returns ArtifactAllowlistInfo.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/artifact_allowlists.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: update(artifact_type: ArtifactType, artifact_matchers: List[ArtifactMatcher]) -> ArtifactAllowlistInfo\n\n    Set an artifact allowlist.\n\n    Set the artifact allowlist of a certain artifact type. The whole artifact allowlist is replaced with\n    the new allowlist. The caller must be a metastore admin or have the **MANAGE ALLOWLIST** privilege on\n    the metastore.\n\n    :param artifact_type: :class:`ArtifactType`\n      The artifact type of the allowlist.\n    :param artifact_matchers: List[:class:`ArtifactMatcher`]\n      A list of allowed artifact match patterns.\n\n    :returns: :class:`ArtifactAllowlistInfo`\n```\n\n----------------------------------------\n\nTITLE: Update Enhanced Security Monitoring Setting - Python\nDESCRIPTION: Updates the enhanced security monitoring setting for new workspaces. It requires allow_missing (always true for Settings API), the setting to update, and a field_mask indicating which fields to update. Returns the updated EsmEnablementAccountSetting object.  The field_mask parameter is used to specify which fields of the setting object should be updated; a comma-separated list of field names.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/esm_enablement_account.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: update(allow_missing: bool, setting: EsmEnablementAccountSetting, field_mask: str) -> EsmEnablementAccountSetting\n\n        Update the enhanced security monitoring setting for new workspaces.\n\n        Updates the value of the enhanced security monitoring setting for new workspaces.\n\n        :param allow_missing: bool\n          This should always be set to true for Settings API. Added for AIP compliance.\n        :param setting: :class:`EsmEnablementAccountSetting`\n        :param field_mask: str\n          The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n          field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n          `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n          the entire collection field can be specified. Field names must exactly match the resource field\n          names.\n\n          A field mask of `*` indicates full replacement. It’s recommended to always explicitly list the\n          fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n          changes in the future.\n\n        :returns: :class:`EsmEnablementAccountSetting`\n```\n\n----------------------------------------\n\nTITLE: Creating Schedule Subscription\nDESCRIPTION: Creates a subscription for a specific schedule of a Lakeview dashboard using the LakeviewAPI.  Requires dashboard ID and schedule ID. Returns the created Subscription object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: create_subscription(dashboard_id: str, schedule_id: str [, subscription: Optional[Subscription]]) -> Subscription\n\n    Create schedule subscription.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard to which the subscription belongs.\n    :param schedule_id: str\n      UUID identifying the schedule to which the subscription belongs.\n    :param subscription: :class:`Subscription` (optional)\n\n    :returns: :class:`Subscription`\n```\n\n----------------------------------------\n\nTITLE: Defining Format Enum (Python)\nDESCRIPTION: Defines an enumeration representing different data formats. The options are ARROW_STREAM, CSV, and JSON_ARRAY. Used for specifying the format of data returned from SQL queries.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: Format\n\n   .. py:attribute:: ARROW_STREAM\n      :value: \"ARROW_STREAM\"\n\n   .. py:attribute:: CSV\n      :value: \"CSV\"\n\n   .. py:attribute:: JSON_ARRAY\n      :value: \"JSON_ARRAY\"\n```\n\n----------------------------------------\n\nTITLE: Getting Schedule Subscription\nDESCRIPTION: Retrieves a specific subscription of a dashboard schedule using the LakeviewAPI. Requires dashboard ID, schedule ID, and subscription ID. Returns the Subscription object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: get_subscription(dashboard_id: str, schedule_id: str, subscription_id: str) -> Subscription\n\n    Get schedule subscription.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard which the subscription belongs.\n    :param schedule_id: str\n      UUID identifying the schedule which the subscription belongs.\n    :param subscription_id: str\n      UUID identifying the subscription.\n\n    :returns: :class:`Subscription`\n```\n\n----------------------------------------\n\nTITLE: Creating Provider Analytics Dashboard with Databricks SDK\nDESCRIPTION: Creates a provider analytics dashboard using the `create()` method. This method returns a Marketplace specific ID, which is different from the Lakeview dashboard ID. The return type is a `ProviderAnalyticsDashboard` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_provider_analytics_dashboards.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nw.provider_provider_analytics_dashboards.create() -> ProviderAnalyticsDashboard\n```\n\n----------------------------------------\n\nTITLE: Defining Table Operations in Python\nDESCRIPTION: This code defines the TableOperation class, enumerating the possible operations on a table. Each operation is represented as a class attribute with a string value. Operations include READ and READ_WRITE.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass TableOperation:\n   READ = \"READ\"\n\n   READ_WRITE = \"READ_WRITE\"\n```\n\n----------------------------------------\n\nTITLE: Create Custom OAuth App Integration (Python)\nDESCRIPTION: Creates a new Custom OAuth App Integration. This method allows specifying various parameters such as whether a client secret is required, the name of the app, redirect URLs, OAuth scopes, token access policy, and user-authorized scopes. It returns a CreateCustomAppIntegrationOutput object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/custom_app_integration.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create( [, confidential: Optional[bool], name: Optional[str], redirect_urls: Optional[List[str]], scopes: Optional[List[str]], token_access_policy: Optional[TokenAccessPolicy], user_authorized_scopes: Optional[List[str]]]) -> CreateCustomAppIntegrationOutput\n\n    Create Custom OAuth App Integration.\n\n    Create Custom OAuth App Integration.\n\n    You can retrieve the custom OAuth app integration via :method:CustomAppIntegration/get.\n\n    :param confidential: bool (optional)\n      This field indicates whether an OAuth client secret is required to authenticate this client.\n    :param name: str (optional)\n      Name of the custom OAuth app\n    :param redirect_urls: List[str] (optional)\n      List of OAuth redirect urls\n    :param scopes: List[str] (optional)\n      OAuth scopes granted to the application. Supported scopes: all-apis, sql, offline_access, openid,\n      profile, email.\n    :param token_access_policy: :class:`TokenAccessPolicy` (optional)\n      Token access policy\n    :param user_authorized_scopes: List[str] (optional)\n      Scopes that will need to be consented by end user to mint the access token. If the user does not\n      authorize the access token will not be minted. Must be a subset of scopes.\n\n    :returns: :class:`CreateCustomAppIntegrationOutput`\n```\n\n----------------------------------------\n\nTITLE: DeploymentKind Enum Definition in Python\nDESCRIPTION: Defines an enumeration specifying the deployment method used to manage the Delta Live Tables pipeline. Currently, it supports the 'BUNDLE' deployment kind, which indicates that the pipeline is managed by a Databricks Asset Bundle.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: DeploymentKind\n\n   The deployment method that manages the pipeline: - BUNDLE: The pipeline is managed by a Databricks Asset Bundle.\n\n   .. py:attribute:: BUNDLE\n      :value: \"BUNDLE\"\n```\n\n----------------------------------------\n\nTITLE: PricingTier Enum Definition (Python)\nDESCRIPTION: Defines an enumeration representing the pricing tier of a Databricks workspace in the `databricks.sdk.service.provisioning` module.  It lists possible tiers such as `COMMUNITY_EDITION`, `DEDICATED`, `ENTERPRISE`, `PREMIUM`, `STANDARD`, and `UNKNOWN`.  This is used during workspace creation and management.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/provisioning.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PricingTier\n\n   The pricing tier of the workspace. For pricing tier information, see [AWS Pricing].\n   [AWS Pricing]: https://databricks.com/product/aws-pricing\n\n   .. py:attribute:: COMMUNITY_EDITION\n      :value: \"COMMUNITY_EDITION\"\n\n   .. py:attribute:: DEDICATED\n      :value: \"DEDICATED\"\n\n   .. py:attribute:: ENTERPRISE\n      :value: \"ENTERPRISE\"\n\n   .. py:attribute:: PREMIUM\n      :value: \"PREMIUM\"\n\n   .. py:attribute:: STANDARD\n      :value: \"STANDARD\"\n\n   .. py:attribute:: UNKNOWN\n      :value: \"UNKNOWN\"\n```\n\n----------------------------------------\n\nTITLE: Defining RunResultState Constants in Python\nDESCRIPTION: This snippet defines constants representing the result state of a job run, such as SUCCESS, FAILED, TIMEDOUT, CANCELED, MAXIMUM_CONCURRENT_RUNS_REACHED, EXCLUDED, SUCCESS_WITH_FAILURES, UPSTREAM_FAILED, UPSTREAM_CANCELED and DISABLED.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nCANCELED = \"CANCELED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nDISABLED = \"DISABLED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nEXCLUDED = \"EXCLUDED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nFAILED = \"FAILED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nMAXIMUM_CONCURRENT_RUNS_REACHED = \"MAXIMUM_CONCURRENT_RUNS_REACHED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nSUCCESS = \"SUCCESS\"\n```\n\nLANGUAGE: python\nCODE:\n```\nSUCCESS_WITH_FAILURES = \"SUCCESS_WITH_FAILURES\"\n```\n\nLANGUAGE: python\nCODE:\n```\nTIMEDOUT = \"TIMEDOUT\"\n```\n\nLANGUAGE: python\nCODE:\n```\nUPSTREAM_CANCELED = \"UPSTREAM_CANCELED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nUPSTREAM_FAILED = \"UPSTREAM_FAILED\"\n```\n\n----------------------------------------\n\nTITLE: Defining ListSortOrder Enum (Python)\nDESCRIPTION: Defines an enumeration `ListSortOrder` to specify the order to use when sorting policies. Includes `ASC` and `DESC` attributes.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ListSortOrder\n\n   .. py:attribute:: ASC\n      :value: \"ASC\"\n\n   .. py:attribute:: DESC\n      :value: \"DESC\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Redash V2 Configuration (Python)\nDESCRIPTION: This code snippet documents the `get_config()` method of the `RedashConfigAPI` class. The `get_config()` method retrieves the workspace configuration specifically for Redash V2. It returns a `ClientConfig` object containing the configuration details.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/redash_config.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n .. py:method:: get_config() -> ClientConfig\n\n        Read workspace configuration for Redash-v2.\n\n        :returns: :class:`ClientConfig`\n```\n\n----------------------------------------\n\nTITLE: Defining EmbeddingsV1ResponseEmbeddingElementObject Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `EmbeddingsV1ResponseEmbeddingElementObject` with value EMBEDDING. This specifies the type of object that will always be embedding.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EmbeddingsV1ResponseEmbeddingElementObject\n\n   This will always be 'embedding'.\n\n   .. py:attribute:: EMBEDDING\n      :value: \"EMBEDDING\"\n```\n\n----------------------------------------\n\nTITLE: Auto-generated class: UninstallLibraries in Python\nDESCRIPTION: This section documents the auto-generated class `UninstallLibraries` within the Databricks SDK for Python. It indicates that members and undocumented members (`undoc-members`) are included in the documentation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: UninstallLibraries\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Getting User Details using Databricks SDK in Python\nDESCRIPTION: Retrieves user details from the Databricks workspace using the WorkspaceClient. First a user is created, then it retrieves the details of that user. It depends on the `databricks.sdk` library.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/users.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\nuser = w.users.create(\n    display_name=f\"sdk-{time.time_ns()}\",\n    user_name=f\"sdk-{time.time_ns()}@example.com\",\n)\n\nfetch = w.users.get(id=user.id)\n```\n\n----------------------------------------\n\nTITLE: Defining STREAMING_BACKLOG_SECONDS Constant in Python\nDESCRIPTION: This snippet defines a constant STREAMING_BACKLOG_SECONDS, representing an estimate of the maximum consumer delay across all streams. This metric is in Public Preview and helps in monitoring consumer delay in streaming jobs.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nSTREAMING_BACKLOG_SECONDS = \"STREAMING_BACKLOG_SECONDS\"\n```\n\n----------------------------------------\n\nTITLE: Defining ArtifactType Enum in Python\nDESCRIPTION: This code defines an enumeration `ArtifactType` with members for different artifact types, such as INIT_SCRIPT, LIBRARY_JAR, and LIBRARY_MAVEN. These types represent different kinds of deployable artifacts that can be managed within the Databricks environment.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ArtifactType\n\n   The artifact type\n\n   .. py:attribute:: INIT_SCRIPT\n      :value: \"INIT_SCRIPT\"\n\n   .. py:attribute:: LIBRARY_JAR\n      :value: \"LIBRARY_JAR\"\n\n   .. py:attribute:: LIBRARY_MAVEN\n      :value: \"LIBRARY_MAVEN\"\n```\n\n----------------------------------------\n\nTITLE: Getting a Monitor Refresh - Databricks SDK (Python)\nDESCRIPTION: This method retrieves information about a specific monitor refresh using a given refresh ID.  The caller needs specific permissions, including ownership or USE privileges on the catalog and schema, as well as SELECT privilege on the table. It must be called from the workspace where the monitor was created.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/quality_monitors.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_refresh(table_name: str, refresh_id: str) -> MonitorRefreshInfo\n\n        Get refresh.\n\n        Gets info about a specific monitor refresh using the given refresh ID.\n\n        The caller must either: 1. be an owner of the table's parent catalog 2. have **USE_CATALOG** on the\n        table's parent catalog and be an owner of the table's parent schema 3. have the following permissions:\n        - **USE_CATALOG** on the table's parent catalog - **USE_SCHEMA** on the table's parent schema -\n        **SELECT** privilege on the table.\n\n        Additionally, the call must be made from the workspace where the monitor was created.\n\n        :param table_name: str\n          Full name of the table.\n        :param refresh_id: str\n          ID of the refresh.\n\n        :returns: :class:`MonitorRefreshInfo`\n```\n\n----------------------------------------\n\nTITLE: List Encryption Key Configurations in Databricks\nDESCRIPTION: This snippet demonstrates how to list all encryption key configurations using the AccountClient in Databricks.  It initializes an AccountClient and then calls the list method on the encryption_keys API to retrieve all key configurations.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/encryption_keys.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nall = a.encryption_keys.list()\n```\n\n----------------------------------------\n\nTITLE: Defining URN for PatchSchema in Python\nDESCRIPTION: Defines the URN_IETF_PARAMS_SCIM_API_MESSAGES_2_0_PATCH_OP attribute for the PatchSchema class. It represents the SCIM schema for a patch operation with a URN string value.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PatchSchema\n\n   .. py:attribute:: URN_IETF_PARAMS_SCIM_API_MESSAGES_2_0_PATCH_OP\n      :value: \"URN_IETF_PARAMS_SCIM_API_MESSAGES_2_0_PATCH_OP\"\n```\n\n----------------------------------------\n\nTITLE: Updating a Legacy Alert in Databricks SQL\nDESCRIPTION: This method updates an existing alert in Databricks SQL. It takes the alert ID, name, alert options, and query ID as parameters to modify the alert's configuration.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/alerts_legacy.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nw.alerts_legacy.update(alert_id: str, name: str, options: AlertOptions, query_id: str [, rearm: Optional[int]])\n```\n\n----------------------------------------\n\nTITLE: Listing Databricks Repos in Python\nDESCRIPTION: This snippet demonstrates how to list all Databricks Repos using the WorkspaceClient and the repos.list method. It retrieves all repos that the calling user has Manage permissions on.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/workspace/repos.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import workspace\n\nw = WorkspaceClient()\n\nall = w.repos.list(workspace.ListReposRequest())\n```\n\n----------------------------------------\n\nTITLE: Deleting a Marketplace File in Python\nDESCRIPTION: This Python method deletes a file from the Databricks Marketplace based on its unique file ID.  The method takes the file_id as a string parameter and removes the corresponding file from the system.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_files.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(file_id: str)\n\n        Delete a file.\n\n        Delete a file\n\n        :param file_id: str\n```\n\n----------------------------------------\n\nTITLE: Initializing WorkspaceClient with Token Authentication - Python\nDESCRIPTION: This code snippet demonstrates how to initialize the Databricks WorkspaceClient using Databricks token authentication. It requires the user to input the Databricks Workspace URL and a personal access token.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/authentication.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(host=input('Databricks Workspace URL: '), token=input('Token: '))\n```\n\n----------------------------------------\n\nTITLE: Creating an Endpoint in Databricks\nDESCRIPTION: This code snippet describes the method for creating an Endpoint object using the Databricks SDK. It takes an optional Endpoint object as input and returns the created Endpoint.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/endpoints.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: create( [, endpoint: Optional[Endpoint]]) -> Endpoint\n\n    Create an Endpoint.\n        \n    :param endpoint: :class:`Endpoint` (optional)\n        Endpoint\n        \n    :returns: :class:`Endpoint`\n```\n\n----------------------------------------\n\nTITLE: Defining RateLimitKey Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `RateLimitKey` with possible values ENDPOINT and USER. It specifies the key used for rate limiting.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: RateLimitKey\n\n   .. py:attribute:: ENDPOINT\n      :value: \"ENDPOINT\"\n\n   .. py:attribute:: USER\n      :value: \"USER\"\n```\n\n----------------------------------------\n\nTITLE: Defining ComputeState Enum (Python)\nDESCRIPTION: This Python code defines an enumeration `ComputeState` with values representing the possible states of a compute resource, such as `ACTIVE`, `DELETING`, `ERROR`, `STARTING`, `STOPPED`, `STOPPING`, and `UPDATING`. This provides the status of the compute resource.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/apps.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ComputeState\n\n   .. py:attribute:: ACTIVE\n      :value: \"ACTIVE\"\n\n   .. py:attribute:: DELETING\n      :value: \"DELETING\"\n\n   .. py:attribute:: ERROR\n      :value: \"ERROR\"\n\n   .. py:attribute:: STARTING\n      :value: \"STARTING\"\n\n   .. py:attribute:: STOPPED\n      :value: \"STOPPED\"\n\n   .. py:attribute:: STOPPING\n      :value: \"STOPPING\"\n\n   .. py:attribute:: UPDATING\n      :value: \"UPDATING\"\n```\n\n----------------------------------------\n\nTITLE: Getting an Account IP Access List in Python\nDESCRIPTION: Gets an IP access list, specified by its list ID. This function requires the `ip_access_list_id` to identify the specific list to retrieve.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/ip_access_lists.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(ip_access_list_id: str) -> GetIpAccessListResponse\n\n    Get IP access list.\n\n    Gets an IP access list, specified by its list ID.\n\n    :param ip_access_list_id: str\n      The ID for the corresponding IP access list\n\n    :returns: :class:`GetIpAccessListResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining Function Parameter Mode for Delta Sharing in Python\nDESCRIPTION: This code defines an enumeration for function parameter modes, indicating whether a parameter is an input (IN), output (OUT), or both (INOUT).\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sharing.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass FunctionParameterMode:\n    IN = \"IN\"\n    INOUT = \"INOUT\"\n    OUT = \"OUT\"\n```\n\n----------------------------------------\n\nTITLE: Defining Webhook Class in Python\nDESCRIPTION: Defines the Webhook class and its members, excluding documentation for undocumented members. This class likely represents a webhook configuration within the Databricks environment.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: Webhook\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining EndpointStatusState Enum in Python\nDESCRIPTION: Defines an enumeration `EndpointStatusState` representing the current state of an endpoint. It can be OFFLINE, ONLINE, or PROVISIONING.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/vectorsearch.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EndpointStatusState\n\n   Current state of the endpoint\n\n   .. py:attribute:: OFFLINE\n      :value: \"OFFLINE\"\n\n   .. py:attribute:: ONLINE\n      :value: \"ONLINE\"\n\n   .. py:attribute:: PROVISIONING\n      :value: \"PROVISIONING\"\n```\n\n----------------------------------------\n\nTITLE: Restoring a Legacy Query in Databricks SQL\nDESCRIPTION: This method restores a query that has been moved to the trash in Databricks SQL. Restored queries reappear in list views and searches. It takes the query_id as a parameter.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/queries_legacy.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: restore(query_id: str)\n```\n\n----------------------------------------\n\nTITLE: Listing Personalization Requests - Python\nDESCRIPTION: Lists all personalization requests associated with a consumer across all marketplace listings. Supports pagination using page_size and page_token parameters. Returns an iterator over PersonalizationRequest objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/consumer_personalization_requests.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nw.consumer_personalization_requests.list( [, page_size: Optional[int], page_token: Optional[str]]) -> Iterator[PersonalizationRequest]\n```\n\n----------------------------------------\n\nTITLE: Listing Budget Configurations with Databricks SDK\nDESCRIPTION: This code snippet shows how to list all budget configurations associated with an account using the `list` method of the `BudgetsAPI` class.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/billing/budgets.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import billing\n\na = AccountClient()\n\nall = a.budgets.list(billing.ListBudgetConfigurationsRequest())\n```\n\n----------------------------------------\n\nTITLE: Defining RUN_DURATION_SECONDS Constant in Python\nDESCRIPTION: This snippet defines a constant RUN_DURATION_SECONDS, representing the expected total time for a job run in seconds. This metric helps in monitoring the performance and duration of jobs.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nRUN_DURATION_SECONDS = \"RUN_DURATION_SECONDS\"\n```\n\n----------------------------------------\n\nTITLE: Disable System Schema in Databricks\nDESCRIPTION: Disables a system schema within a specified metastore. Requires account admin or metastore admin permissions. Takes the metastore ID and schema name as parameters and removes the schema from the system catalog.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/system_schemas.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nw.system_schemas.disable(metastore_id: str, schema_name: str)\n```\n\n----------------------------------------\n\nTITLE: Settings API Property: enable_notebook_table_clipboard\nDESCRIPTION: This property controls whether users can copy tabular data to the clipboard via the UI. By default, this setting is enabled.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/settings.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. py:property:: enable_notebook_table_clipboard\n    :type: EnableNotebookTableClipboardAPI\n\n    Controls whether users can copy tabular data to the clipboard via the UI. By default, this setting is\n    enabled.\n```\n\n----------------------------------------\n\nTITLE: Defining WebhookNotifications Class in Python\nDESCRIPTION: Defines the WebhookNotifications class and its members, excluding documentation for undocumented members. This class likely handles notifications associated with a webhook in Databricks.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: WebhookNotifications\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Get Serving Endpoint Permission Levels - Python\nDESCRIPTION: Retrieves the permission levels that a user can have on a serving endpoint. Requires the serving endpoint ID.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get_permission_levels(serving_endpoint_id: str) -> GetServingEndpointPermissionLevelsResponse\n\n    Get serving endpoint permission levels.\n\n    Gets the permission levels that a user can have on an object.\n\n    :param serving_endpoint_id: str\n      The serving endpoint for which to get or manage permissions.\n\n    :returns: :class:`GetServingEndpointPermissionLevelsResponse`\n```\n\n----------------------------------------\n\nTITLE: Defining WidgetErrorDetail Class in Python\nDESCRIPTION: Defines the WidgetErrorDetail class and its members, excluding documentation for undocumented members. This class likely represents error details associated with a Databricks widget.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: WidgetErrorDetail\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining RunType Enum in Python\nDESCRIPTION: Defines the `RunType` enumeration, indicating the type of a Databricks run. It includes values for `JOB_RUN`, `WORKFLOW_RUN`, and `SUBMIT_RUN`. It specifies the different ways a run can be initiated in Databricks, either through the Jobs API or the dbutils.notebook.run command.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n.. py:attribute:: JOB_RUN\n   :value: \"JOB_RUN\"\n\n   .. py:attribute:: SUBMIT_RUN\n      :value: \"SUBMIT_RUN\"\n\n   .. py:attribute:: WORKFLOW_RUN\n      :value: \"WORKFLOW_RUN\"\n```\n\n----------------------------------------\n\nTITLE: Defining JobDeploymentKind Enum Python\nDESCRIPTION: This code snippet defines an enumeration called `JobDeploymentKind` with possible value `BUNDLE`, indicating that the job is managed by Databricks Asset Bundle.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: JobDeploymentKind\n\n   * `BUNDLE`: The job is managed by Databricks Asset Bundle.\n\n   .. py:attribute:: BUNDLE\n      :value: \"BUNDLE\"\n```\n\n----------------------------------------\n\nTITLE: Listing All Credential Configurations\nDESCRIPTION: This snippet demonstrates how to retrieve a list of all credential configurations associated with the account. It uses the 'list' method to obtain an iterator over the credential objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/credentials.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nconfigs = a.credentials.list()\n```\n\n----------------------------------------\n\nTITLE: Defining ViewItem Class in Python\nDESCRIPTION: Defines the ViewItem class and its members, excluding documentation for undocumented members. This probably represents a generic viewable item in the Databricks UI.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: ViewItem\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining Shared Data Object History Data Sharing Status in Python\nDESCRIPTION: This code defines an enumeration for the data sharing status of shared data object history. It includes options for ENABLED and DISABLED.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sharing.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass SharedDataObjectHistoryDataSharingStatus:\n    DISABLED = \"DISABLED\"\n    ENABLED = \"ENABLED\"\n```\n\n----------------------------------------\n\nTITLE: Auto-generated class: UpdateClusterResource in Python\nDESCRIPTION: This section documents the `UpdateClusterResource` class within the Databricks SDK. Documentation includes both member functions and undocumented member functions.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: UpdateClusterResource\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Getting Published Dashboard\nDESCRIPTION: Retrieves a published Lakeview dashboard using the LakeviewAPI. The dashboard is identified by its UUID. It returns the PublishedDashboard object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: get_published(dashboard_id: str) -> PublishedDashboard\n\n    Get published dashboard.\n\n    Get the current published dashboard.\n\n    :param dashboard_id: str\n      UUID identifying the published dashboard.\n\n    :returns: :class:`PublishedDashboard`\n```\n\n----------------------------------------\n\nTITLE: Defining RuntimeEngine Enum (Python)\nDESCRIPTION: Defines an enumeration `RuntimeEngine` to represent the runtime engine used by a Databricks cluster. It includes attributes `NULL`, `PHOTON`, and `STANDARD`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: RuntimeEngine\n\n   .. py:attribute:: NULL\n      :value: \"NULL\"\n\n   .. py:attribute:: PHOTON\n      :value: \"PHOTON\"\n\n   .. py:attribute:: STANDARD\n      :value: \"STANDARD\"\n```\n\n----------------------------------------\n\nTITLE: Defining Message Status Constants in Python\nDESCRIPTION: This code defines a class `MessageStatus` with several constants representing different states of a message. These constants provide status indicators for various stages of message processing, such as fetching metadata, executing queries, and handling errors. They are used to track the progress and outcome of message-related operations within the Dashboards API.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: MessageStatus\n\n   MessageStatus. The possible values are: * `FETCHING_METADATA`: Fetching metadata from the data sources. * `FILTERING_CONTEXT`: Running smart context step to determine relevant context. * `ASKING_AI`: Waiting for the LLM to respond to the user's question. * `PENDING_WAREHOUSE`: Waiting for warehouse before the SQL query can start executing. * `EXECUTING_QUERY`: Executing a generated SQL query. Get the SQL query result by calling [getMessageAttachmentQueryResult](:method:genie/getMessageAttachmentQueryResult) API. * `FAILED`: The response generation or query execution failed. See `error` field. * `COMPLETED`: Message processing is completed. Results are in the `attachments` field. Get the SQL query result by calling [getMessageAttachmentQueryResult](:method:genie/getMessageAttachmentQueryResult) API. * `SUBMITTED`: Message has been submitted. * `QUERY_RESULT_EXPIRED`: SQL result is not available anymore. The user needs to rerun the query. Rerun the SQL query result by calling [executeMessageAttachmentQuery](:method:genie/executeMessageAttachmentQuery) API. * `CANCELLED`: Message has been cancelled.\n\n   .. py:attribute:: ASKING_AI\n      :value: \"ASKING_AI\"\n\n   .. py:attribute:: CANCELLED\n      :value: \"CANCELLED\"\n\n   .. py:attribute:: COMPLETED\n      :value: \"COMPLETED\"\n\n   .. py:attribute:: EXECUTING_QUERY\n      :value: \"EXECUTING_QUERY\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: FETCHING_METADATA\n      :value: \"FETCHING_METADATA\"\n\n   .. py:attribute:: FILTERING_CONTEXT\n      :value: \"FILTERING_CONTEXT\"\n\n   .. py:attribute:: PENDING_WAREHOUSE\n      :value: \"PENDING_WAREHOUSE\"\n\n   .. py:attribute:: QUERY_RESULT_EXPIRED\n      :value: \"QUERY_RESULT_EXPIRED\"\n\n   .. py:attribute:: SUBMITTED\n      :value: \"SUBMITTED\"\n```\n\n----------------------------------------\n\nTITLE: Get cluster status - Python\nDESCRIPTION: Retrieves the status of libraries on a specific Databricks cluster. The `cluster_id` parameter specifies the target cluster. The method returns an iterator over `LibraryFullStatus` objects, detailing the installed libraries and their statuses.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/compute/libraries.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: cluster_status(cluster_id: str) -> Iterator[LibraryFullStatus]\n\n    Get status.\n\n    Get the status of libraries on a cluster. A status is returned for all libraries installed on this\n    cluster via the API or the libraries UI. The order of returned libraries is as follows: 1. Libraries\n    set to be installed on this cluster, in the order that the libraries were added to the cluster, are\n    returned first. 2. Libraries that were previously requested to be installed on this cluster or, but\n    are now marked for removal, in no particular order, are returned last.\n\n    :param cluster_id: str\n      Unique identifier of the cluster whose status should be retrieved.\n\n    :returns: Iterator over :class:`LibraryFullStatus`\n```\n\n----------------------------------------\n\nTITLE: Defining PERFORMANCE_OPTIMIZED and STANDARD Constants for PerformanceTarget in Python\nDESCRIPTION: This snippet defines constants PERFORMANCE_OPTIMIZED and STANDARD for the PerformanceTarget class. These constants indicate the performance level of a job run on serverless compute.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nPERFORMANCE_OPTIMIZED = \"PERFORMANCE_OPTIMIZED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nSTANDARD = \"STANDARD\"\n```\n\n----------------------------------------\n\nTITLE: Defining Status for Model Version in Python\nDESCRIPTION: These attributes define the status of a model version. Valid statuses are PENDING_REGISTRATION, FAILED_REGISTRATION and READY. These statuses represent the model version registration process.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: Status\n\n   The status of the model version. Valid values are: * `PENDING_REGISTRATION`: Request to register a new model version is pending as server performs background tasks.\n   * `FAILED_REGISTRATION`: Request to register a new model version has failed.\n   * `READY`: Model version is ready for use.\n\n   .. py:attribute:: FAILED_REGISTRATION\n      :value: \"FAILED_REGISTRATION\"\n\n   .. py:attribute:: PENDING_REGISTRATION\n      :value: \"PENDING_REGISTRATION\"\n\n   .. py:attribute:: READY\n      :value: \"READY\"\n```\n\n----------------------------------------\n\nTITLE: Get EnableNotebookTableClipboard Setting (Python)\nDESCRIPTION: Retrieves the current Enable Notebook Table Clipboard setting.  This method returns an `EnableNotebookTableClipboard` object representing the setting's current state. No parameters are required.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/enable_notebook_table_clipboard.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nw.settings.get_enable_notebook_table_clipboard() -> EnableNotebookTableClipboard\n```\n\n----------------------------------------\n\nTITLE: Deleting a Clean Room - Python\nDESCRIPTION: Deletes a data object clean room from the metastore. The caller must be an owner of the clean room. Requires the name of the clean room as a parameter.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sharing/clean_rooms.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(name: str)\n\n    Delete a clean room.\n    \n    Deletes a data object clean room from the metastore. The caller must be an owner of the clean room.\n    \n    :param name: str\n      The name of the clean room.\n```\n\n----------------------------------------\n\nTITLE: Defining TokenPermissionLevel Enum in Python\nDESCRIPTION: This code defines an enumeration `TokenPermissionLevel` representing the permission level for tokens. The only defined value is `CAN_USE`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: TokenPermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_USE\n      :value: \"CAN_USE\"\n```\n\n----------------------------------------\n\nTITLE: Settings API Property: automatic_cluster_update\nDESCRIPTION: This property controls whether automatic cluster update is enabled for the current workspace. By default, it is turned off.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/settings.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:property:: automatic_cluster_update\n    :type: AutomaticClusterUpdateAPI\n\n    Controls whether automatic cluster update is enabled for the current workspace. By default, it is turned\n    off.\n```\n\n----------------------------------------\n\nTITLE: Deleting a Group in Databricks (Python)\nDESCRIPTION: Deletes a group from the Databricks account. The id parameter specifies the unique ID for a group in the Databricks account that should be removed.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/iam/groups.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(id: str)\n\n        Delete a group.\n\n        Deletes a group from the Databricks account.\n\n        :param id: str\n          Unique ID for a group in the Databricks account.\n```\n\n----------------------------------------\n\nTITLE: Waiting for App Deployment to Succeed in Databricks\nDESCRIPTION: Waits for the specified app deployment to succeed. Includes an optional timeout and callback function.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/apps.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nw.apps.wait_get_deployment_app_succeeded(app_name: str, deployment_id: str, timeout: datetime.timedelta = 0:20:00, callback: Optional[Callable[[AppDeployment], None]]) -> AppDeployment\n```\n\n----------------------------------------\n\nTITLE: Auto-generated class: UpdateResponse in Python\nDESCRIPTION: This section documents the auto-generated class `UpdateResponse` within the Databricks SDK for Python. It documents members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_44\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: UpdateResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_MANAGE attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_MANAGE attribute for the PermissionLevel class. It represents a permission level that allows managing resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n```\n\n----------------------------------------\n\nTITLE: Defining Securable Types in Python\nDESCRIPTION: This code defines the SecurableType class, enumerating the different types of securable objects in Unity Catalog. Each securable type is represented as a class attribute with a string value, such as CATALOG, CLEAN_ROOM, CONNECTION, CREDENTIAL, EXTERNAL_LOCATION, FUNCTION, METASTORE, PIPELINE, PROVIDER, RECIPIENT, SCHEMA, SHARE, STORAGE_CREDENTIAL, TABLE, and VOLUME.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass SecurableType:\n   \"\"\"The type of Unity Catalog securable\"\"\"\n\n   CATALOG = \"CATALOG\"\n\n   CLEAN_ROOM = \"CLEAN_ROOM\"\n\n   CONNECTION = \"CONNECTION\"\n\n   CREDENTIAL = \"CREDENTIAL\"\n\n   EXTERNAL_LOCATION = \"EXTERNAL_LOCATION\"\n\n   FUNCTION = \"FUNCTION\"\n\n   METASTORE = \"METASTORE\"\n\n   PIPELINE = \"PIPELINE\"\n\n   PROVIDER = \"PROVIDER\"\n\n   RECIPIENT = \"RECIPIENT\"\n\n   SCHEMA = \"SCHEMA\"\n\n   SHARE = \"SHARE\"\n\n   STORAGE_CREDENTIAL = \"STORAGE_CREDENTIAL\"\n\n   TABLE = \"TABLE\"\n\n   VOLUME = \"VOLUME\"\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_RUN attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_RUN attribute for the PermissionLevel class. It represents a permission level that allows running resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_RUN\n      :value: \"CAN_RUN\"\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_VIEW attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_VIEW attribute for the PermissionLevel class. It represents a permission level that allows viewing resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_VIEW\n      :value: \"CAN_VIEW\"\n```\n\n----------------------------------------\n\nTITLE: Deleting an Endpoint in Databricks\nDESCRIPTION: This code snippet describes the method for deleting an Endpoint object using the Databricks SDK. It takes the name of the Endpoint as a string parameter.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/catalog/endpoints.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(name: str)\n\n    Delete an Endpoint.\n        \n    :param name: str\n```\n\n----------------------------------------\n\nTITLE: Settings API Property: disable_legacy_dbfs\nDESCRIPTION: This property controls access to DBFS root and DBFS mounts. When the setting is on, access to DBFS root and DBFS mounts is disallowed (as well as creation of new mounts).\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/settings.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:property:: disable_legacy_dbfs\n    :type: DisableLegacyDbfsAPI\n\n    When this setting is on, access to DBFS root and DBFS mounts is disallowed (as well as creation of new\n    mounts). When the setting is off, all DBFS functionality is enabled\n```\n\n----------------------------------------\n\nTITLE: Defining Shared Data Object Update Action for Delta Sharing in Python\nDESCRIPTION: This code defines an enumeration for shared data object update actions, including ADD, REMOVE, and UPDATE.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sharing.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass SharedDataObjectUpdateAction:\n    ADD = \"ADD\"\n    REMOVE = \"REMOVE\"\n    UPDATE = \"UPDATE\"\n```\n\n----------------------------------------\n\nTITLE: Defining STREAMING_BACKLOG_FILES Constant in Python\nDESCRIPTION: This snippet defines a constant STREAMING_BACKLOG_FILES, representing an estimate of the maximum number of outstanding files across all streams. This metric is in Public Preview and helps in tracking file backlog in streaming jobs.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nSTREAMING_BACKLOG_FILES = \"STREAMING_BACKLOG_FILES\"\n```\n\n----------------------------------------\n\nTITLE: Defining ScopeBackendType Enum\nDESCRIPTION: Defines the backend types for secret scopes. Supported types are AZURE_KEYVAULT and DATABRICKS. Specifies the storage mechanism used for storing secrets.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/workspace.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ScopeBackendType\n\n   .. py:attribute:: AZURE_KEYVAULT\n      :value: \"AZURE_KEYVAULT\"\n\n   .. py:attribute:: DATABRICKS\n      :value: \"DATABRICKS\"\n```\n\n----------------------------------------\n\nTITLE: CleanRoom Class Definition (Python)\nDESCRIPTION: Defines the CleanRoom dataclass, representing a Databricks Clean Room. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoom\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining ADD attribute for PatchOp in Python\nDESCRIPTION: Defines the ADD attribute for the PatchOp class, representing the 'add' operation type in a patch request.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PatchOp\n\n   Type of patch operation.\n\n   .. py:attribute:: ADD\n      :value: \"ADD\"\n```\n\n----------------------------------------\n\nTITLE: Listing Storage Credentials (Python)\nDESCRIPTION: This method retrieves a list of all storage credentials associated with a specified metastore. It returns an iterator over StorageCredentialInfo objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/catalog/storage_credentials.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmetastore_id: str\n```\n\n----------------------------------------\n\nTITLE: Defining EndpointInfoWarehouseType Enum (Python)\nDESCRIPTION: Defines an enumeration representing warehouse types for an endpoint. The types are CLASSIC, PRO, and TYPE_UNSPECIFIED. The documentation emphasizes setting the type to `PRO` and `enable_serverless_compute` to `true` when using serverless compute.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EndpointInfoWarehouseType\n\n   Warehouse type: `PRO` or `CLASSIC`. If you want to use serverless compute, you must set to `PRO` and also set the field `enable_serverless_compute` to `true`.\n\n   .. py:attribute:: CLASSIC\n      :value: \"CLASSIC\"\n\n   .. py:attribute:: PRO\n      :value: \"PRO\"\n\n   .. py:attribute:: TYPE_UNSPECIFIED\n```\n\n----------------------------------------\n\nTITLE: Defining Shared Data Object Type for Delta Sharing in Python\nDESCRIPTION: This code defines an enumeration of shared data object types. It includes options for FEATURE_SPEC, FUNCTION, MATERIALIZED_VIEW, MODEL, NOTEBOOK_FILE, SCHEMA, STREAMING_TABLE, TABLE, and VIEW.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sharing.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass SharedDataObjectDataObjectType:\n    FEATURE_SPEC = \"FEATURE_SPEC\"\n    FUNCTION = \"FUNCTION\"\n    MATERIALIZED_VIEW = \"MATERIALIZED_VIEW\"\n    MODEL = \"MODEL\"\n    NOTEBOOK_FILE = \"NOTEBOOK_FILE\"\n    SCHEMA = \"SCHEMA\"\n    STREAMING_TABLE = \"STREAMING_TABLE\"\n    TABLE = \"TABLE\"\n    VIEW = \"VIEW\"\n```\n\n----------------------------------------\n\nTITLE: Upgrading Databricks SDK\nDESCRIPTION: This snippet shows how to upgrade the Databricks SDK for Python within a Databricks notebook. It uses the `%pip` magic command for package management.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%pip install --upgrade databricks-sdk\n```\n\n----------------------------------------\n\nTITLE: Getting Network Connectivity Configuration in Databricks\nDESCRIPTION: This method retrieves a network connectivity configuration. It requires the network connectivity configuration ID as a parameter. It returns the NetworkConnectivityConfiguration object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/network_connectivity.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nNetworkConnectivityAPI.get_network_connectivity_configuration(network_connectivity_config_id: str) -> NetworkConnectivityConfiguration\n```\n\n----------------------------------------\n\nTITLE: CleanRoomAssetTableLocalDetails Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomAssetTableLocalDetails dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomAssetTableLocalDetails\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: CleanRoomAssetForeignTableLocalDetails Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomAssetForeignTableLocalDetails dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomAssetForeignTableLocalDetails\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: CleanRoomAssetVolumeLocalDetails Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomAssetVolumeLocalDetails dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomAssetVolumeLocalDetails\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: ListCleanRoomAssetsResponse Class Definition (Python)\nDESCRIPTION: Defines the ListCleanRoomAssetsResponse dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: ListCleanRoomAssetsResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: CollaboratorJobRunInfo Class Definition (Python)\nDESCRIPTION: Defines the CollaboratorJobRunInfo dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CollaboratorJobRunInfo\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: InstallationStatus Enum Definition in Python\nDESCRIPTION: Defines the statuses of an installation on the Databricks Marketplace. These statuses include 'FAILED' and 'INSTALLED', indicating whether the installation process was successful or not.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: InstallationStatus\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: INSTALLED\n      :value: \"INSTALLED\"\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_EDIT_METADATA attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_EDIT_METADATA attribute for the PermissionLevel class. It represents a permission level that allows editing metadata of resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_EDIT_METADATA\n      :value: \"CAN_EDIT_METADATA\"\n```\n\n----------------------------------------\n\nTITLE: Defining RunState Enum in Python\nDESCRIPTION: Defines the `RunState` enumeration with possible values representing the current state of a Databricks run. States include `PENDING`, `RUNNING`, `TERMINATING`, `TERMINATED`, and `SKIPPED`.  It is used to represent the state of a run in Databricks. The `UPSTREAM_FAILED` state is explicitly assigned a value as well.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n.. py:attribute:: UPSTREAM_FAILED\n   :value: \"UPSTREAM_FAILED\"\n```\n\n----------------------------------------\n\nTITLE: Defining CreateFunctionSqlDataAccess Enum in Python\nDESCRIPTION: This code defines an enumeration `CreateFunctionSqlDataAccess` with members CONTAINS_SQL, NO_SQL, and READS_SQL_DATA. These represent the SQL data access capabilities of a function, specifying if it contains SQL, has no SQL access, or reads SQL data.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CreateFunctionSqlDataAccess\n\n   Function SQL data access.\n\n   .. py:attribute:: CONTAINS_SQL\n      :value: \"CONTAINS_SQL\"\n\n   .. py:attribute:: NO_SQL\n      :value: \"NO_SQL\"\n\n   .. py:attribute:: READS_SQL_DATA\n      :value: \"READS_SQL_DATA\"\n```\n\n----------------------------------------\n\nTITLE: CleanRoomAccessRestricted Enum (Python)\nDESCRIPTION: Defines the CleanRoomAccessRestricted enumeration, representing possible access restrictions for a Clean Room. It includes constants like CSP_MISMATCH and NO_RESTRICTION.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: CleanRoomAccessRestricted\n\n   .. py:attribute:: CSP_MISMATCH\n      :value: \"CSP_MISMATCH\"\n\n   .. py:attribute:: NO_RESTRICTION\n      :value: \"NO_RESTRICTION\"\n```\n\n----------------------------------------\n\nTITLE: ListCleanRoomsResponse Class Definition (Python)\nDESCRIPTION: Defines the ListCleanRoomsResponse dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: ListCleanRoomsResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: CleanRoomRemoteDetail Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomRemoteDetail dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomRemoteDetail\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: CleanRoomAssetTable Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomAssetTable dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomAssetTable\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: CleanRoomAssetView Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomAssetView dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomAssetView\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Installing Databricks SDK for Python\nDESCRIPTION: This snippet shows how to install the Databricks SDK for Python using pip. It is a prerequisite for using the SDK and interacting with Databricks services.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install databricks-sdk\n```\n\n----------------------------------------\n\nTITLE: Defining ADMIN attribute for WorkspacePermission in Python\nDESCRIPTION: Defines the ADMIN attribute for the WorkspacePermission class.  It indicates the user has admin permissions.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: WorkspacePermission\n\n   .. py:attribute:: ADMIN\n      :value: \"ADMIN\"\n```\n\n----------------------------------------\n\nTITLE: Defining Authentication Types for Delta Sharing in Python\nDESCRIPTION: This code defines an enumeration for Delta Sharing authentication types. It includes options for Databricks authentication, OAuth client credentials, and token-based authentication.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sharing.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass AuthenticationType:\n    \"\"\"The delta sharing authentication type.\"\"\"\n\n    DATABRICKS = \"DATABRICKS\"\n    OAUTH_CLIENT_CREDENTIALS = \"OAUTH_CLIENT_CREDENTIALS\"\n    TOKEN = \"TOKEN\"\n```\n\n----------------------------------------\n\nTITLE: Defining Event Types in Python\nDESCRIPTION: This code defines the possible event types for Databricks clusters. It includes events such as CREATING, RUNNING, TERMINATING, and various error states, providing comprehensive coverage of cluster lifecycle events.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: EventType\n\n   .. py:attribute:: ADD_NODES_FAILED\n      :value: \"ADD_NODES_FAILED\"\n\n   .. py:attribute:: AUTOMATIC_CLUSTER_UPDATE\n      :value: \"AUTOMATIC_CLUSTER_UPDATE\"\n\n   .. py:attribute:: AUTOSCALING_BACKOFF\n      :value: \"AUTOSCALING_BACKOFF\"\n\n   .. py:attribute:: AUTOSCALING_FAILED\n      :value: \"AUTOSCALING_FAILED\"\n\n   .. py:attribute:: AUTOSCALING_STATS_REPORT\n      :value: \"AUTOSCALING_STATS_REPORT\"\n\n   .. py:attribute:: CREATING\n      :value: \"CREATING\"\n\n   .. py:attribute:: DBFS_DOWN\n      :value: \"DBFS_DOWN\"\n\n   .. py:attribute:: DID_NOT_EXPAND_DISK\n      :value: \"DID_NOT_EXPAND_DISK\"\n\n   .. py:attribute:: DRIVER_HEALTHY\n      :value: \"DRIVER_HEALTHY\"\n\n   .. py:attribute:: DRIVER_NOT_RESPONDING\n      :value: \"DRIVER_NOT_RESPONDING\"\n\n   .. py:attribute:: DRIVER_UNAVAILABLE\n      :value: \"DRIVER_UNAVAILABLE\"\n\n   .. py:attribute:: EDITED\n      :value: \"EDITED\"\n\n   .. py:attribute:: EXPANDED_DISK\n      :value: \"EXPANDED_DISK\"\n\n   .. py:attribute:: FAILED_TO_EXPAND_DISK\n      :value: \"FAILED_TO_EXPAND_DISK\"\n\n   .. py:attribute:: INIT_SCRIPTS_FINISHED\n      :value: \"INIT_SCRIPTS_FINISHED\"\n\n   .. py:attribute:: INIT_SCRIPTS_STARTED\n      :value: \"INIT_SCRIPTS_STARTED\"\n\n   .. py:attribute:: METASTORE_DOWN\n      :value: \"METASTORE_DOWN\"\n\n   .. py:attribute:: NODES_LOST\n      :value: \"NODES_LOST\"\n\n   .. py:attribute:: NODE_BLACKLISTED\n      :value: \"NODE_BLACKLISTED\"\n\n   .. py:attribute:: NODE_EXCLUDED_DECOMMISSIONED\n      :value: \"NODE_EXCLUDED_DECOMMISSIONED\"\n\n   .. py:attribute:: PINNED\n      :value: \"PINNED\"\n\n   .. py:attribute:: RESIZING\n      :value: \"RESIZING\"\n\n   .. py:attribute:: RESTARTING\n      :value: \"RESTARTING\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n\n   .. py:attribute:: SPARK_EXCEPTION\n      :value: \"SPARK_EXCEPTION\"\n\n   .. py:attribute:: STARTING\n      :value: \"STARTING\"\n\n   .. py:attribute:: TERMINATING\n      :value: \"TERMINATING\"\n\n   .. py:attribute:: UNPINNED\n      :value: \"UNPINNED\"\n\n   .. py:attribute:: UPSIZE_COMPLETED\n      :value: \"UPSIZE_COMPLETED\"\n```\n\n----------------------------------------\n\nTITLE: List All Serving Endpoints - Python\nDESCRIPTION: Retrieves all serving endpoints. Returns an iterator over ServingEndpoint objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/serving/serving_endpoints.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: list() -> Iterator[ServingEndpoint]\n\n    Get all serving endpoints.\n\n    :returns: Iterator over :class:`ServingEndpoint`\n```\n\n----------------------------------------\n\nTITLE: Deleting Exchange Filter using ProviderExchangeFiltersAPI in Python\nDESCRIPTION: Deletes an existing exchange filter using the ``delete`` method. It requires the ``id`` of the filter to be deleted as a string.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_exchange_filters.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nw.provider_exchange_filters.delete(id: str)\n```\n\n----------------------------------------\n\nTITLE: Defining PAUSED and UNPAUSED Constants for PauseStatus in Python\nDESCRIPTION: This snippet defines constants PAUSED and UNPAUSED representing the status of a paused job. These constants are used in the PauseStatus class.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nPAUSED = \"PAUSED\"\n```\n\nLANGUAGE: python\nCODE:\n```\nUNPAUSED = \"UNPAUSED\"\n```\n\n----------------------------------------\n\nTITLE: Get Service Principal Federation Policy (Python)\nDESCRIPTION: Retrieves a service principal federation policy based on the service principal ID and the policy ID.  The returned FederationPolicy object contains the policy details.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/service_principal_federation_policy.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get(service_principal_id: int, policy_id: str) -> FederationPolicy\n\n    Get service principal federation policy.\n\n    :param service_principal_id: int\n      The service principal id for the federation policy.\n    :param policy_id: str\n      The identifier for the federation policy.\n\n    :returns: :class:`FederationPolicy`\n```\n\n----------------------------------------\n\nTITLE: TableSpecificConfigScdType Enum Definition in Python\nDESCRIPTION: Defines an enumeration for the Slowly Changing Dimension (SCD) type to use when ingesting a table. It includes SCD_TYPE_1 and SCD_TYPE_2, specifying the method for handling changes to the table's data.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: TableSpecificConfigScdType\n\n   The SCD type to use to ingest the table.\n\n   .. py:attribute:: SCD_TYPE_1\n      :value: \"SCD_TYPE_1\"\n\n   .. py:attribute:: SCD_TYPE_2\n      :value: \"SCD_TYPE_2\"\n```\n\n----------------------------------------\n\nTITLE: Defining ListSortColumn Enum (Python)\nDESCRIPTION: Defines an enumeration `ListSortColumn` to specify the column to use when sorting policies. Includes `POLICY_CREATION_TIME` and `POLICY_NAME` attributes.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ListSortColumn\n\n   .. py:attribute:: POLICY_CREATION_TIME\n      :value: \"POLICY_CREATION_TIME\"\n\n   .. py:attribute:: POLICY_NAME\n      :value: \"POLICY_NAME\"\n```\n\n----------------------------------------\n\nTITLE: Defining CreateFunctionSecurityType Enum in Python\nDESCRIPTION: This code defines an enumeration `CreateFunctionSecurityType` with a single member DEFINER. This specifies that the function's security context is that of the definer (creator) of the function.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CreateFunctionSecurityType\n\n   The security type of the function.\n\n   .. py:attribute:: DEFINER\n      :value: \"DEFINER\"\n```\n\n----------------------------------------\n\nTITLE: Defining ListType Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `ListType` that represents the type of IP access list, either `ALLOW` or `BLOCK`. The `ALLOW` type includes the IP or range, while the `BLOCK` type excludes it. IP addresses in the block list are excluded even if they are in the allow list.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ListType\n\n   Type of IP access list. Valid values are as follows and are case-sensitive:\n   * `ALLOW`: An allow list. Include this IP or range. * `BLOCK`: A block list. Exclude this IP or range. IP addresses in the block list are excluded even if they are included in an allow list.\n\n   .. py:attribute:: ALLOW\n      :value: \"ALLOW\"\n\n   .. py:attribute:: BLOCK\n      :value: \"BLOCK\"\n```\n\n----------------------------------------\n\nTITLE: Defining Get Events Order in Python\nDESCRIPTION: This code defines the possible ordering options for retrieving events. It includes ASC (ascending) and DESC (descending) as valid order options.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: GetEventsOrder\n\n   .. py:attribute:: ASC\n      :value: \"ASC\"\n\n   .. py:attribute:: DESC\n      :value: \"DESC\"\n```\n\n----------------------------------------\n\nTITLE: Defining PipelineType Enum in Python\nDESCRIPTION: Defines an enumeration `PipelineType` representing the execution mode of a pipeline. The pipeline can be either TRIGGERED (processes updates only once after a refresh) or CONTINUOUS (processes new data as it arrives).\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/vectorsearch.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: PipelineType\n\n   Pipeline execution mode.\n   - `TRIGGERED`: If the pipeline uses the triggered execution mode, the system stops processing after successfully refreshing the source table in the pipeline once, ensuring the table is updated based on the data available when the update started. - `CONTINUOUS`: If the pipeline uses continuous execution, the pipeline processes new data as it arrives in the source table to keep vector index fresh.\n\n   .. py:attribute:: CONTINUOUS\n      :value: \"CONTINUOUS\"\n\n   .. py:attribute:: TRIGGERED\n      :value: \"TRIGGERED\"\n```\n\n----------------------------------------\n\nTITLE: Defining CloudProviderNodeStatus Enum in Python\nDESCRIPTION: This code defines an enumeration (`CloudProviderNodeStatus`) for the status of a cloud provider node. It includes statuses such as NOT_AVAILABLE_IN_REGION and NOT_ENABLED_ON_SUBSCRIPTION, which are relevant for troubleshooting cluster creation or node availability issues.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: CloudProviderNodeStatus\n\n   .. py:attribute:: NOT_AVAILABLE_IN_REGION\n      :value: \"NOT_AVAILABLE_IN_REGION\"\n\n   .. py:attribute:: NOT_ENABLED_ON_SUBSCRIPTION\n      :value: \"NOT_ENABLED_ON_SUBSCRIPTION\"\n```\n\n----------------------------------------\n\nTITLE: Delete Federation Policy (Python)\nDESCRIPTION: Deletes an existing account federation policy, identified by its `policy_id`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/oauth2/federation_policy.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: delete(policy_id: str)\n\n    Delete account federation policy.\n\n    :param policy_id: str\n      The identifier for the federation policy.\n```\n\n----------------------------------------\n\nTITLE: Defining ListClustersSortByField Enum (Python)\nDESCRIPTION: Defines an enumeration `ListClustersSortByField` to specify the field by which clusters are sorted. It includes `CLUSTER_NAME` and `DEFAULT` attributes, indicating sorting by cluster name or a default sorting method.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ListClustersSortByField\n\n   .. py:attribute:: CLUSTER_NAME\n      :value: \"CLUSTER_NAME\"\n\n   .. py:attribute:: DEFAULT\n      :value: \"DEFAULT\"\n```\n\n----------------------------------------\n\nTITLE: CleanRoomAssetStatusEnum Enum (Python)\nDESCRIPTION: Defines the CleanRoomAssetStatusEnum enumeration with values such as ACTIVE, PENDING, and PERMISSION_DENIED.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: CleanRoomAssetStatusEnum\n\n   .. py:attribute:: ACTIVE\n      :value: \"ACTIVE\"\n\n   .. py:attribute:: PENDING\n      :value: \"PENDING\"\n\n   .. py:attribute:: PERMISSION_DENIED\n      :value: \"PERMISSION_DENIED\"\n```\n\n----------------------------------------\n\nTITLE: Restarting Python in Notebook\nDESCRIPTION: This command restarts the Python interpreter within a Databricks notebook. It's necessary after upgrading packages to ensure the new versions are loaded. It utilizes the `dbutils.library.restartPython()` function, which is specific to Databricks notebooks.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/getting-started.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndbutils.library.restartPython()\n```\n\n----------------------------------------\n\nTITLE: ForecastingExperimentState Constants Definition in Python\nDESCRIPTION: Defines constants for the possible states of a forecasting experiment. These states such as `CANCELLED`, `FAILED`, `RUNNING`, and `SUCCEEDED` are used to track the lifecycle and status of automated forecasting tasks within the Databricks ML platform.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ForecastingExperimentState\n\n   .. py:attribute:: CANCELLED\n      :value: \"CANCELLED\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: PENDING\n      :value: \"PENDING\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n\n   .. py:attribute:: SUCCEEDED\n      :value: \"SUCCEEDED\"\n```\n\n----------------------------------------\n\nTITLE: Defining Registry Webhook Status in Python\nDESCRIPTION: These attributes define the possible statuses for a registry webhook, including active, disabled, and test mode. The status determines whether the webhook is triggered by events or only through the test endpoint.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: RegistryWebhookStatus\n\n   Enable or disable triggering the webhook, or put the webhook into test mode. The default is `ACTIVE`: * `ACTIVE`: Webhook is triggered when an associated event happens.\n   * `DISABLED`: Webhook is not triggered.\n   * `TEST_MODE`: Webhook can be triggered through the test endpoint, but is not triggered on a real event.\n\n   .. py:attribute:: ACTIVE\n      :value: \"ACTIVE\"\n\n   .. py:attribute:: DISABLED\n      :value: \"DISABLED\"\n\n   .. py:attribute:: TEST_MODE\n      :value: \"TEST_MODE\"\n```\n\n----------------------------------------\n\nTITLE: Defining Registry Webhook Events in Python\nDESCRIPTION: These attributes define the different events that can trigger a registry webhook, such as model version creation, stage transitions, and comment creation. Each attribute represents a specific event type that the webhook can listen for.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: RegistryWebhookEvent\n\n   .. py:attribute:: COMMENT_CREATED\n      :value: \"COMMENT_CREATED\"\n\n   .. py:attribute:: MODEL_VERSION_CREATED\n      :value: \"MODEL_VERSION_CREATED\"\n\n   .. py:attribute:: MODEL_VERSION_TAG_SET\n      :value: \"MODEL_VERSION_TAG_SET\"\n\n   .. py:attribute:: MODEL_VERSION_TRANSITIONED_STAGE\n      :value: \"MODEL_VERSION_TRANSITIONED_STAGE\"\n\n   .. py:attribute:: MODEL_VERSION_TRANSITIONED_TO_ARCHIVED\n      :value: \"MODEL_VERSION_TRANSITIONED_TO_ARCHIVED\"\n\n   .. py:attribute:: MODEL_VERSION_TRANSITIONED_TO_PRODUCTION\n      :value: \"MODEL_VERSION_TRANSITIONED_TO_PRODUCTION\"\n\n   .. py:attribute:: MODEL_VERSION_TRANSITIONED_TO_STAGING\n      :value: \"MODEL_VERSION_TRANSITIONED_TO_STAGING\"\n\n   .. py:attribute:: REGISTERED_MODEL_CREATED\n      :value: \"REGISTERED_MODEL_CREATED\"\n\n   .. py:attribute:: TRANSITION_REQUEST_CREATED\n      :value: \"TRANSITION_REQUEST_CREATED\"\n\n   .. py:attribute:: TRANSITION_REQUEST_TO_ARCHIVED_CREATED\n      :value: \"TRANSITION_REQUEST_TO_ARCHIVED_CREATED\"\n\n   .. py:attribute:: TRANSITION_REQUEST_TO_PRODUCTION_CREATED\n      :value: \"TRANSITION_REQUEST_TO_PRODUCTION_CREATED\"\n\n   .. py:attribute:: TRANSITION_REQUEST_TO_STAGING_CREATED\n      :value: \"TRANSITION_REQUEST_TO_STAGING_CREATED\"\n```\n\n----------------------------------------\n\nTITLE: UpdateCleanRoomRequest Class Definition (Python)\nDESCRIPTION: Defines the UpdateCleanRoomRequest dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: UpdateCleanRoomRequest\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining Compute Kind in Python\nDESCRIPTION: This code defines the 'kind' of compute environment, specifying if it's CLASSIC_PREVIEW or another type. This affects available configurations and validations.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: Kind\n\n   The kind of compute described by this compute specification.\n   Depending on `kind`, different validations and default values will be applied.\n   Clusters with `kind = CLASSIC_PREVIEW` support the following fields, whereas clusters with no specified `kind` do not. * [is_single_node](/api/workspace/clusters/create#is_single_node) * [use_ml_runtime](/api/workspace/clusters/create#use_ml_runtime) * [data_security_mode](/api/workspace/clusters/create#data_security_mode) set to `DATA_SECURITY_MODE_AUTO`, `DATA_SECURITY_MODE_DEDICATED`, or `DATA_SECURITY_MODE_STANDARD`\n   By using the [simple form], your clusters are automatically using `kind = CLASSIC_PREVIEW`.\n   [simple form]: https://docs.databricks.com/compute/simple-form.html\n\n   .. py:attribute:: CLASSIC_PREVIEW\n      :value: \"CLASSIC_PREVIEW\"\n```\n\n----------------------------------------\n\nTITLE: Getting a Credential Configuration by ID\nDESCRIPTION: This example shows how to retrieve a specific credential configuration by its ID. It first creates a credential configuration, then retrieves it using the 'get' method with the credential ID obtained from the creation step. Finally, it deletes the created credential.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/credentials.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service import provisioning\n\na = AccountClient()\n\nrole = a.credentials.create(\n    credentials_name=f\"sdk-{time.time_ns()}\",\n    aws_credentials=provisioning.CreateCredentialAwsCredentials(\n        sts_role=provisioning.CreateCredentialStsRole(role_arn=os.environ[\"TEST_CROSSACCOUNT_ARN\"])\n    ),\n)\n\nby_id = a.credentials.get(credentials_id=role.credentials_id)\n\n# cleanup\na.credentials.delete(credentials_id=role.credentials_id)\n```\n\n----------------------------------------\n\nTITLE: Autogenerating Documentation for TrashDashboardResponse Class in Python\nDESCRIPTION: This code snippet uses the 'autoclass' directive to automatically generate documentation for the TrashDashboardResponse class. The ':members:' option includes all public members in the documentation, while ':undoc-members:' excludes undocumented members. This is part of the Databricks SDK documentation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: TrashDashboardResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining Table Internal Attributes Shared Table Type in Python\nDESCRIPTION: This code defines an enumeration for shared table types, including DIRECTORY_BASED_TABLE, FILE_BASED_TABLE, FOREIGN_TABLE, MATERIALIZED_VIEW, and STREAMING_TABLE.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sharing.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass TableInternalAttributesSharedTableType:\n    DIRECTORY_BASED_TABLE = \"DIRECTORY_BASED_TABLE\"\n    FILE_BASED_TABLE = \"FILE_BASED_TABLE\"\n    FOREIGN_TABLE = \"FOREIGN_TABLE\"\n    MATERIALIZED_VIEW = \"MATERIALIZED_VIEW\"\n    STREAMING_TABLE = \"STREAMING_TABLE\"\n```\n\n----------------------------------------\n\nTITLE: Defining CredentialPurpose Enum in Python\nDESCRIPTION: This code defines an enumeration `CredentialPurpose` with members SERVICE and STORAGE. These represent the purpose of a credential, specifying whether it's for a service or for storage access.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CredentialPurpose\n\n   .. py:attribute:: SERVICE\n      :value: \"SERVICE\"\n\n   .. py:attribute:: STORAGE\n      :value: \"STORAGE\"\n```\n\n----------------------------------------\n\nTITLE: Defining ViewType in Python\nDESCRIPTION: These attributes define the possible view types for querying data, including ACTIVE_ONLY, ALL, and DELETED_ONLY. These options control which records are returned based on their active or deleted status.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ViewType\n\n   Qualifier for the view type.\n\n   .. py:attribute:: ACTIVE_ONLY\n      :value: \"ACTIVE_ONLY\"\n\n   .. py:attribute:: ALL\n      :value: \"ALL\"\n\n   .. py:attribute:: DELETED_ONLY\n      :value: \"DELETED_ONLY\"\n```\n\n----------------------------------------\n\nTITLE: Settings API Property: aibi_dashboard_embedding_approved_domains\nDESCRIPTION: This property controls the list of domains approved to host the embedded AI/BI dashboards. The approved domains list can't be mutated when the current access policy is not set to ALLOW_APPROVED_DOMAINS.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/settings.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. py:property:: aibi_dashboard_embedding_approved_domains\n    :type: AibiDashboardEmbeddingApprovedDomainsAPI\n\n    Controls the list of domains approved to host the embedded AI/BI dashboards. The approved domains list\n    can't be mutated when the current access policy is not set to ALLOW_APPROVED_DOMAINS.\n```\n\n----------------------------------------\n\nTITLE: Defining Library Install Status in Python\nDESCRIPTION: This code defines the possible statuses for a library installation on a cluster. It includes statuses such as FAILED, INSTALLED, INSTALLING, PENDING, and SKIPPED to track the installation progress.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: LibraryInstallStatus\n\n   The status of a library on a specific cluster.\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: INSTALLED\n      :value: \"INSTALLED\"\n\n   .. py:attribute:: INSTALLING\n      :value: \"INSTALLING\"\n\n   .. py:attribute:: PENDING\n      :value: \"PENDING\"\n\n   .. py:attribute:: RESOLVING\n      :value: \"RESOLVING\"\n\n   .. py:attribute:: RESTORED\n      :value: \"RESTORED\"\n\n   .. py:attribute:: SKIPPED\n      :value: \"SKIPPED\"\n\n   .. py:attribute:: UNINSTALL_ON_RESTART\n      :value: \"UNINSTALL_ON_RESTART\"\n```\n\n----------------------------------------\n\nTITLE: Auto-generated class: WorkloadType in Python\nDESCRIPTION: This section documents the WorkloadType class, a part of the Databricks SDK in Python. Includes documentation for both documented and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_46\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: WorkloadType\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Getting a Message Query Result in Genie using Python\nDESCRIPTION: This deprecated method retrieves the result of a SQL query associated with a message in a Genie conversation. It's only available if the message has a query attachment and its status is 'EXECUTING_QUERY'. The method returns a GenieGetMessageQueryResultResponse object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/genie.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: get_message_query_result(space_id: str, conversation_id: str, message_id: str) -> GenieGetMessageQueryResultResponse\n\n        [Deprecated] Get conversation message SQL query result.\n\n        Get the result of SQL query if the message has a query attachment. This is only available if a message\n        has a query attachment and the message status is `EXECUTING_QUERY`.\n\n        :param space_id: str\n          Genie space ID\n        :param conversation_id: str\n          Conversation ID\n        :param message_id: str\n          Message ID\n\n        :returns: :class:`GenieGetMessageQueryResultResponse`\n```\n\n----------------------------------------\n\nTITLE: FileParentType Enum Definition in Python\nDESCRIPTION: Defines the parent types for files within the Databricks Marketplace. This can be a 'LISTING', 'LISTING_RESOURCE', or 'PROVIDER', indicating the file's association with a specific listing or provider.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/marketplace.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: FileParentType\n\n   .. py:attribute:: LISTING\n      :value: \"LISTING\"\n\n   .. py:attribute:: LISTING_RESOURCE\n      :value: \"LISTING_RESOURCE\"\n\n   .. py:attribute:: PROVIDER\n      :value: \"PROVIDER\"\n```\n\n----------------------------------------\n\nTITLE: Defining DataPlaneEventDetailsEventType Enum in Python\nDESCRIPTION: This code defines an enumeration (`DataPlaneEventDetailsEventType`) for the different types of data plane events in Databricks. It includes NODE_BLACKLISTED and NODE_EXCLUDED_DECOMMISSIONED, representing events related to node exclusion due to blacklisting or decommissioning.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: DataPlaneEventDetailsEventType\n\n   .. py:attribute:: NODE_BLACKLISTED\n      :value: \"NODE_BLACKLISTED\"\n\n   .. py:attribute:: NODE_EXCLUDED_DECOMMISSIONED\n      :value: \"NODE_EXCLUDED_DECOMMISSIONED\"\n```\n\n----------------------------------------\n\nTITLE: Defining Warehouse Health Status in Python\nDESCRIPTION: This snippet defines the possible health statuses of a Databricks warehouse, including DEGRADED, FAILED, HEALTHY, and STATUS_UNSPECIFIED. These states represent the operational health of the warehouse. Each status is represented as a string constant.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n   .. py:attribute:: DEGRADED\n      :value: \"DEGRADED\"\n\n   .. py:attribute:: FAILED\n      :value: \"FAILED\"\n\n   .. py:attribute:: HEALTHY\n      :value: \"HEALTHY\"\n\n   .. py:attribute:: STATUS_UNSPECIFIED\n      :value: \"STATUS_UNSPECIFIED\"\n```\n\n----------------------------------------\n\nTITLE: Defining RepoPermissionLevel Enum\nDESCRIPTION: Defines permission levels for Repos. Includes CAN_EDIT, CAN_MANAGE, CAN_READ, and CAN_RUN permissions. These values are used when setting access control for Repos.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/workspace.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: RepoPermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_EDIT\n      :value: \"CAN_EDIT\"\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_READ\n      :value: \"CAN_READ\"\n\n   .. py:attribute:: CAN_RUN\n      :value: \"CAN_RUN\"\n```\n\n----------------------------------------\n\nTITLE: Autogenerating Documentation for UnpublishDashboardResponse Class in Python\nDESCRIPTION: This code snippet uses the 'autoclass' directive to automatically generate documentation for the UnpublishDashboardResponse class. The ':members:' option includes all public members in the documentation, while ':undoc-members:' excludes undocumented members. This is part of the Databricks SDK documentation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: UnpublishDashboardResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining Schedule Pause Status Constants in Python\nDESCRIPTION: This code snippet defines a Python class SchedulePauseStatus with two attributes: PAUSED and UNPAUSED. These attributes represent the possible states of a schedule's pause status and are defined as string constants. They are used to indicate whether a schedule is currently paused or unpaused.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: SchedulePauseStatus\n\n   .. py:attribute:: PAUSED\n      :value: \"PAUSED\"\n\n   .. py:attribute:: UNPAUSED\n      :value: \"UNPAUSED\"\n```\n\n----------------------------------------\n\nTITLE: Auto-generated class: WorkspaceStorageInfo in Python\nDESCRIPTION: This section describes the WorkspaceStorageInfo class, a component of the Databricks SDK for Python. It provides documentation for members and undocumented members of the class.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: WorkspaceStorageInfo\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: CleanRoomAssetNotebook Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomAssetNotebook dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomAssetNotebook\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Listing Private Access Settings with AccountClient in Python\nDESCRIPTION: This code snippet shows how to retrieve a list of all private access settings objects for an account using the AccountClient in the Databricks SDK for Python. It retrieves all private access settings objects associated with the account. The returned value is an iterator.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/provisioning/private_access.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom databricks.sdk import AccountClient\n\na = AccountClient()\n\nall = a.private_access.list()\n```\n\n----------------------------------------\n\nTITLE: Getting Latest Version of Provider Analytics Dashboard with Databricks SDK\nDESCRIPTION: Retrieves the latest version of a provider analytics dashboard using the `get_latest_version()` method. The return type is a `GetLatestVersionProviderAnalyticsDashboardResponse` object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_provider_analytics_dashboards.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nw.provider_provider_analytics_dashboards.get_latest_version() -> GetLatestVersionProviderAnalyticsDashboardResponse\n```\n\n----------------------------------------\n\nTITLE: Listing Legacy Alerts in Databricks SQL\nDESCRIPTION: This method retrieves a list of alerts from Databricks SQL. It returns an iterator over LegacyAlert objects, allowing you to iterate through all available alerts.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/alerts_legacy.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nw.alerts_legacy.list() -> Iterator[LegacyAlert]\n```\n\n----------------------------------------\n\nTITLE: CommentActivityAction Constants Definition in Python\nDESCRIPTION: Defines constants for actions related to comments. The actions include editing and deleting comments and are used to indicate the specific action taken or allowed on a comment object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CommentActivityAction\n\n   An action that a user (with sufficient permissions) could take on a comment. Valid values are: * `EDIT_COMMENT`: Edit the comment\n   * `DELETE_COMMENT`: Delete the comment\n\n   .. py:attribute:: DELETE_COMMENT\n      :value: \"DELETE_COMMENT\"\n\n   .. py:attribute:: EDIT_COMMENT\n      :value: \"EDIT_COMMENT\"\n```\n\n----------------------------------------\n\nTITLE: Defining Enum: EgressNetworkPolicyInternetAccessPolicyLogOnlyModeWorkloadType - Python\nDESCRIPTION: This Python enum defines the workload types used in networkconfig.proto for log-only mode in internet access policies. It includes DBSQL and ML_SERVING, allowing for specific workload-based logging configurations.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EgressNetworkPolicyInternetAccessPolicyLogOnlyModeWorkloadType\n\n   The values should match the list of workloads used in networkconfig.proto\n\n   .. py:attribute:: DBSQL\n      :value: \"DBSQL\"\n\n   .. py:attribute:: ML_SERVING\n      :value: \"ML_SERVING\"\n```\n\n----------------------------------------\n\nTITLE: Defining Lifecycle State Constants in Python\nDESCRIPTION: This code defines a class `LifecycleState` with two constants: `ACTIVE` and `TRASHED`. These constants are strings representing the possible states of a dashboard's lifecycle. They are used to indicate whether a dashboard is currently active or has been moved to the trash.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/dashboards.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: LifecycleState\n\n   .. py:attribute:: ACTIVE\n      :value: \"ACTIVE\"\n\n   .. py:attribute:: TRASHED\n      :value: \"TRASHED\"\n```\n\n----------------------------------------\n\nTITLE: Settings API Property: disable_legacy_access\nDESCRIPTION: This property disables direct access to Hive Metastores, fallback mode on external location access, and Databricks Runtime versions prior to 13.3LTS.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/settings.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. py:property:: disable_legacy_access\n    :type: DisableLegacyAccessAPI\n\n    'Disabling legacy access' has the following impacts:\n\n    1. Disables direct access to Hive Metastores from the workspace. However, you can still access a Hive\n    Metastore through Hive Metastore federation. 2. Disables fallback mode on external location access from\n    the workspace. 3. Disables Databricks Runtime versions prior to 13.3LTS.\n```\n\n----------------------------------------\n\nTITLE: Defining ValidationResultResult enum in Python\nDESCRIPTION: Defines a Python enum called `ValidationResultResult` with possible values FAIL, PASS and SKIP. This enum represents the result (pass, fail, skip) of an operation during a validation process.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: ValidationResultResult\n\n   The results of the tested operation.\n\n   .. py:attribute:: FAIL\n      :value: \"FAIL\"\n\n   .. py:attribute:: PASS\n      :value: \"PASS\"\n\n   .. py:attribute:: SKIP\n      :value: \"SKIP\"\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_USE attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_USE attribute for the PermissionLevel class. It represents a permission level that allows using resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_USE\n      :value: \"CAN_USE\"\n```\n\n----------------------------------------\n\nTITLE: Defining STREAMING_BACKLOG_RECORDS Constant in Python\nDESCRIPTION: This snippet defines a constant STREAMING_BACKLOG_RECORDS, representing an estimate of the maximum offset lag across all streams. This metric is in Public Preview and used for monitoring record backlog in streaming jobs.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nSTREAMING_BACKLOG_RECORDS = \"STREAMING_BACKLOG_RECORDS\"\n```\n\n----------------------------------------\n\nTITLE: ActivityType Constants Definition in Python\nDESCRIPTION: Defines constants representing different types of activities. These activity types, like `APPLIED_TRANSITION`, `REQUESTED_TRANSITION`, and `SYSTEM_TRANSITION`, are used to categorize actions and events related to machine learning models and their deployment stages.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ActivityType\n\n   Type of activity. Valid values are: * `APPLIED_TRANSITION`: User applied the corresponding stage transition.\n   * `REQUESTED_TRANSITION`: User requested the corresponding stage transition.\n   * `CANCELLED_REQUEST`: User cancelled an existing transition request.\n   * `APPROVED_REQUEST`: User approved the corresponding stage transition.\n   * `REJECTED_REQUEST`: User rejected the coressponding stage transition.\n   * `SYSTEM_TRANSITION`: For events performed as a side effect, such as archiving existing model versions in a stage.\n\n   .. py:attribute:: APPLIED_TRANSITION\n      :value: \"APPLIED_TRANSITION\"\n\n   .. py:attribute:: APPROVED_REQUEST\n      :value: \"APPROVED_REQUEST\"\n\n   .. py:attribute:: CANCELLED_REQUEST\n      :value: \"CANCELLED_REQUEST\"\n\n   .. py:attribute:: NEW_COMMENT\n      :value: \"NEW_COMMENT\"\n\n   .. py:attribute:: REJECTED_REQUEST\n      :value: \"REJECTED_REQUEST\"\n\n   .. py:attribute:: REQUESTED_TRANSITION\n      :value: \"REQUESTED_TRANSITION\"\n\n   .. py:attribute:: SYSTEM_TRANSITION\n      :value: \"SYSTEM_TRANSITION\"\n```\n\n----------------------------------------\n\nTITLE: CleanRoomAssetViewLocalDetails Class Definition (Python)\nDESCRIPTION: Defines the CleanRoomAssetViewLocalDetails dataclass. Includes members and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/cleanrooms.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: CleanRoomAssetViewLocalDetails\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining AppPermissionLevel Enum (Python)\nDESCRIPTION: This Python code defines an enumeration `AppPermissionLevel` with values representing the permission level that can be granted on an App, such as `CAN_MANAGE` and `CAN_USE`. These define the capabilities a user or group has on the app.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/apps.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: AppPermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_MANAGE\n      :value: \"CAN_MANAGE\"\n\n   .. py:attribute:: CAN_USE\n      :value: \"CAN_USE\"\n```\n\n----------------------------------------\n\nTITLE: Auto-generated class: VolumesStorageInfo in Python\nDESCRIPTION: This section describes the VolumesStorageInfo class in the Databricks SDK for Python. Both member functions and undocumented member functions are documented.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_45\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: VolumesStorageInfo\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining DeleteDataStatus Enum in Python\nDESCRIPTION: Defines an enumeration `DeleteDataStatus` representing the status of a delete operation within the Databricks Vector Search service. It can have values of FAILURE, PARTIAL_SUCCESS, or SUCCESS.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/vectorsearch.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: DeleteDataStatus\n\n   Status of the delete operation.\n\n   .. py:attribute:: FAILURE\n      :value: \"FAILURE\"\n\n   .. py:attribute:: PARTIAL_SUCCESS\n      :value: \"PARTIAL_SUCCESS\"\n\n   .. py:attribute:: SUCCESS\n      :value: \"SUCCESS\"\n```\n\n----------------------------------------\n\nTITLE: Using dbutils from databricks.sdk.runtime\nDESCRIPTION: This snippet demonstrates how to use `dbutils` directly from the `databricks.sdk.runtime` module.  It lists all secret scopes and the secrets within those scopes and prints the names. Requires authentication to be configured using environment variables.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk.runtime import dbutils\n\nfor secret_scope in dbutils.secrets.listScopes():\n    for secret_metadata in dbutils.secrets.list(secret_scope.name):\n        print(f'found {secret_metadata.key} secret in {secret_scope.name} scope')\n```\n\n----------------------------------------\n\nTITLE: Defining SqlAlertState Enum in Python\nDESCRIPTION: Defines the `SqlAlertState` enumeration, representing the state of a SQL alert in Databricks. It includes values for `UNKNOWN`, `OK`, and `TRIGGERED`. It's used to indicate whether a SQL alert has been evaluated and if its trigger conditions have been met.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n.. py:attribute:: OK\n      :value: \"OK\"\n\n   .. py:attribute:: TRIGGERED\n      :value: \"TRIGGERED\"\n\n   .. py:attribute:: UNKNOWN\n      :value: \"UNKNOWN\"\n```\n\n----------------------------------------\n\nTITLE: Defining Shared Data Object Status for Delta Sharing in Python\nDESCRIPTION: This code defines an enumeration for shared data object statuses, including ACTIVE and PERMISSION_DENIED.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sharing.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass SharedDataObjectStatus:\n    ACTIVE = \"ACTIVE\"\n    PERMISSION_DENIED = \"PERMISSION_DENIED\"\n```\n\n----------------------------------------\n\nTITLE: Defining ASCENDING attribute for GetSortOrder in Python\nDESCRIPTION: Defines the ASCENDING attribute for the GetSortOrder class. It represents the ascending sort order with a string value.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: GetSortOrder\n\n   .. py:attribute:: ASCENDING\n      :value: \"ASCENDING\"\n```\n\n----------------------------------------\n\nTITLE: ActivityAction Constants Definition in Python\nDESCRIPTION: Defines constants for actions that a user can take on an activity, such as approving, rejecting, or canceling a transition request. These constants are used to specify the type of action in API calls related to activity management.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/ml.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ActivityAction\n\n   An action that a user (with sufficient permissions) could take on an activity. Valid values are: * `APPROVE_TRANSITION_REQUEST`: Approve a transition request\n   * `REJECT_TRANSITION_REQUEST`: Reject a transition request\n   * `CANCEL_TRANSITION_REQUEST`: Cancel (delete) a transition request\n\n   .. py:attribute:: APPROVE_TRANSITION_REQUEST\n      :value: \"APPROVE_TRANSITION_REQUEST\"\n\n   .. py:attribute:: CANCEL_TRANSITION_REQUEST\n      :value: \"CANCEL_TRANSITION_REQUEST\"\n\n   .. py:attribute:: REJECT_TRANSITION_REQUEST\n      :value: \"REJECT_TRANSITION_REQUEST\"\n```\n\n----------------------------------------\n\nTITLE: Defining AclPermission Enum\nDESCRIPTION: Defines an enumeration of possible ACL permissions. It includes the permissions MANAGE, READ, and WRITE. These permissions can be used to control access to workspace objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/workspace.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: AclPermission\n\n   .. py:attribute:: MANAGE\n      :value: \"MANAGE\"\n\n   .. py:attribute:: READ\n      :value: \"READ\"\n\n   .. py:attribute:: WRITE\n      :value: \"WRITE\"\n```\n\n----------------------------------------\n\nTITLE: Defining NccAzurePrivateEndpointRuleConnectionState Enum in Python\nDESCRIPTION: This snippet defines an enumeration `NccAzurePrivateEndpointRuleConnectionState` representing the current status of a private endpoint connection. The possible values are `INIT`, `PENDING`, `ESTABLISHED`, `REJECTED`, and `DISCONNECTED`. The private endpoint rules are effective only if the connection state is `ESTABLISHED`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: NccAzurePrivateEndpointRuleConnectionState\n\n   The current status of this private endpoint. The private endpoint rules are effective only if the connection state is `ESTABLISHED`. Remember that you must approve new endpoints on your resources in the Azure portal before they take effect.\n   The possible values are: - INIT: (deprecated) The endpoint has been created and pending approval. - PENDING: The endpoint has been created and pending approval. - ESTABLISHED: The endpoint has been approved and is ready to use in your serverless compute resources. - REJECTED: Connection was rejected by the private link resource owner. - DISCONNECTED: Connection was removed by the private link resource owner, the private endpoint becomes informative and should be deleted for clean-up.\n\n   .. py:attribute:: DISCONNECTED\n      :value: \"DISCONNECTED\"\n\n   .. py:attribute:: ESTABLISHED\n      :value: \"ESTABLISHED\"\n\n   .. py:attribute:: INIT\n      :value: \"INIT\"\n\n   .. py:attribute:: PENDING\n      :value: \"PENDING\"\n\n   .. py:attribute:: REJECTED\n      :value: \"REJECTED\"\n```\n\n----------------------------------------\n\nTITLE: List Endpoints in Databricks\nDESCRIPTION: Lists all vector search endpoints. Accepts an optional page token for pagination. Returns an iterator over EndpointInfo objects.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/vectorsearch/vector_search_endpoints.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nw.vector_search_endpoints.list_endpoints( [, page_token: Optional[str]]) -> Iterator[EndpointInfo]\n```\n\n----------------------------------------\n\nTITLE: Defining CAN_MONITOR attribute for PermissionLevel in Python\nDESCRIPTION: Defines the CAN_MONITOR attribute for the PermissionLevel class. It represents a permission level that allows monitoring resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: CAN_MONITOR\n      :value: \"CAN_MONITOR\"\n```\n\n----------------------------------------\n\nTITLE: Defining SERVICE_FAULT constant in Python\nDESCRIPTION: This code snippet defines a constant named SERVICE_FAULT and assigns it the string value \"SERVICE_FAULT\". This is likely used as a status or error code within the Databricks SDK.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\n:value: \"SERVICE_FAULT\"\n```\n\n----------------------------------------\n\nTITLE: Defining StorageMode Enum in Python\nDESCRIPTION: Defines the `StorageMode` enumeration, representing the storage mode used for data access in Databricks SQL Analytics.  It includes values for `DIRECT_QUERY`, `DUAL`, and `IMPORT`. It is used to configure how data is accessed and stored for SQL queries and dashboards.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n.. py:attribute:: DIRECT_QUERY\n      :value: \"DIRECT_QUERY\"\n\n   .. py:attribute:: DUAL\n      :value: \"DUAL\"\n\n   .. py:attribute:: IMPORT\n      :value: \"IMPORT\"\n```\n\n----------------------------------------\n\nTITLE: Auto-generated class: UpdateCluster in Python\nDESCRIPTION: This section documents the `UpdateCluster` class in the Databricks SDK for Python. This includes documentation for both public and undocumented members of the class.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_41\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: UpdateCluster\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Get Enhanced Security Monitoring Setting - Python\nDESCRIPTION: Retrieves the enhanced security monitoring setting for new workspaces. It accepts an optional etag for versioning and returns an EsmEnablementAccountSetting object. The etag is used for optimistic concurrency control.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/account/settings/esm_enablement_account.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    .. py:method:: get( [, etag: Optional[str]]) -> EsmEnablementAccountSetting\n\n        Get the enhanced security monitoring setting for new workspaces.\n\n        Gets the enhanced security monitoring setting for new workspaces.\n\n        :param etag: str (optional)\n          etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n          optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n          each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n          to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n          request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n        :returns: :class:`EsmEnablementAccountSetting`\n```\n\n----------------------------------------\n\nTITLE: Deleting a Visualization in Databricks SQL (Legacy)\nDESCRIPTION: Deletes an existing visualization. This method takes the visualization ID as input. It is recommended to use the new `queryvisualizations/delete` endpoint instead of this legacy API.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/sql/query_visualizations_legacy.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef delete(id: str):\n    \"\"\"Remove visualization.\n\n    Removes a visualization from the query.\n\n    **Note**: A new version of the Databricks SQL API is now available. Please use\n    :method:queryvisualizations/delete instead. [Learn more]\n\n    [Learn more]: https://docs.databricks.com/en/sql/dbsql-api-latest.html\n\n    :param id: str\n      Widget ID returned by :method:queryvizualisations/create\n    \"\"\n```\n\n----------------------------------------\n\nTITLE: Settings API Property: enable_export_notebook\nDESCRIPTION: This property controls whether users can export notebooks and files from the Workspace. By default, this setting is enabled.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/settings.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. py:property:: enable_export_notebook\n    :type: EnableExportNotebookAPI\n\n    Controls whether users can export notebooks and files from the Workspace. By default, this setting is\n    enabled.\n```\n\n----------------------------------------\n\nTITLE: Defining REQUEST_AUTHZ_IDENTITY_USER_CONTEXT attribute for RequestAuthzIdentity in Python\nDESCRIPTION: Defines the REQUEST_AUTHZ_IDENTITY_USER_CONTEXT attribute for the RequestAuthzIdentity class. It represents the user context to be used for authorization of the request on the server side.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: RequestAuthzIdentity\n\n   Defines the identity to be used for authZ of the request on the server side. See one pager for for more information: http://go/acl/service-identity\n\n   .. py:attribute:: REQUEST_AUTHZ_IDENTITY_USER_CONTEXT\n      :value: \"REQUEST_AUTHZ_IDENTITY_USER_CONTEXT\"\n```\n\n----------------------------------------\n\nTITLE: Auto-generated class: UpdateClusterResponse in Python\nDESCRIPTION: This section documents the auto-generated `UpdateClusterResponse` class. This is part of the Databricks SDK and the documentation includes both public and undocumented members.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/compute.rst#_snippet_43\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: UpdateClusterResponse\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Settings API Property: default_namespace\nDESCRIPTION: The default namespace setting API allows users to configure the default namespace for a Databricks workspace. This setting requires a restart of clusters and SQL warehouses to take effect and only applies when using Unity Catalog-enabled compute.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/settings.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. py:property:: default_namespace\n    :type: DefaultNamespaceAPI\n\n    The default namespace setting API allows users to configure the default namespace for a Databricks\n    workspace.\n\n    Through this API, users can retrieve, set, or modify the default namespace used when queries do not\n    reference a fully qualified three-level name. For example, if you use the API to set 'retail_prod' as the\n    default catalog, then a query 'SELECT * FROM myTable' would reference the object\n    'retail_prod.default.myTable' (the schema 'default' is always assumed).\n\n    This setting requires a restart of clusters and SQL warehouses to take effect. Additionally, the default\n    namespace only applies when using Unity Catalog-enabled compute.\n```\n\n----------------------------------------\n\nTITLE: Defining ChatMessageRole Enum in Python\nDESCRIPTION: This code snippet defines an enumeration `ChatMessageRole` with possible values ASSISTANT, SYSTEM, and USER. It specifies the role of a message in a chat conversation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/serving.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: ChatMessageRole\n\n   The role of the message. One of [system, user, assistant].\n\n   .. py:attribute:: ASSISTANT\n      :value: \"ASSISTANT\"\n\n   .. py:attribute:: SYSTEM\n      :value: \"SYSTEM\"\n\n   .. py:attribute:: USER\n      :value: \"USER\"\n```\n\n----------------------------------------\n\nTITLE: Defining Cluster Termination Reason Types in Python\nDESCRIPTION: This snippet defines the types of reasons for cluster termination, including CLIENT_ERROR, CLOUD_FAILURE, SERVICE_FAULT, and SUCCESS. These types provide a high-level categorization of the termination reason. Each type is represented as a string constant.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/sql.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n   .. py:attribute:: CLIENT_ERROR\n      :value: \"CLIENT_ERROR\"\n\n   .. py:attribute:: CLOUD_FAILURE\n      :value: \"CLOUD_FAILURE\"\n\n   .. py:attribute:: SERVICE_FAULT\n      :value: \"SERVICE_FAULT\"\n\n   .. py:attribute:: SUCCESS\n      :value: \"SUCCESS\"\n```\n\n----------------------------------------\n\nTITLE: MaturityLevel Enum Definition in Python\nDESCRIPTION: Defines an enumeration for the maturity level of event details in Delta Live Tables. It encompasses levels such as DEPRECATED, EVOLVING, and STABLE, reflecting the stage of development and stability of the event details.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/pipelines.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: MaturityLevel\n\n   Maturity level for EventDetails.\n\n   .. py:attribute:: DEPRECATED\n      :value: \"DEPRECATED\"\n\n   .. py:attribute:: EVOLVING\n      :value: \"EVOLVING\"\n\n   .. py:attribute:: STABLE\n      :value: \"STABLE\"\n```\n\n----------------------------------------\n\nTITLE: Defining IS_OWNER attribute for PermissionLevel in Python\nDESCRIPTION: Defines the IS_OWNER attribute for the PermissionLevel class. It represents a permission level indicating ownership of resources.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PermissionLevel\n\n   Permission level\n\n   .. py:attribute:: IS_OWNER\n      :value: \"IS_OWNER\"\n```\n\n----------------------------------------\n\nTITLE: Defining Enum: CreatePrivateEndpointRuleRequestGroupId - Python\nDESCRIPTION: This Python enum defines the sub-resource type (group ID) of the target resource for creating private endpoint rules. It includes BLOB and DFS for workspace root storage (root DBFS), and MYSQL_SERVER and SQL_SERVER. It's used to specify the type of the target resource for private endpoint creation.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CreatePrivateEndpointRuleRequestGroupId\n\n   The sub-resource type (group ID) of the target resource. Note that to connect to workspace root storage (root DBFS), you need two endpoints, one for `blob` and one for `dfs`.\n\n   .. py:attribute:: BLOB\n      :value: \"BLOB\"\n\n   .. py:attribute:: DFS\n      :value: \"DFS\"\n\n   .. py:attribute:: MYSQL_SERVER\n      :value: \"MYSQL_SERVER\"\n\n   .. py:attribute:: SQL_SERVER\n      :value: \"SQL_SERVER\"\n```\n\n----------------------------------------\n\nTITLE: Defining TokenType Enum in Python\nDESCRIPTION: This code defines an enumeration `TokenType` representing the type of token request.  Supported values include various Arclight and Azure Active Directory token types.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: TokenType\n\n   The type of token request. As of now, only `AZURE_ACTIVE_DIRECTORY_TOKEN` is supported.\n\n   .. py:attribute:: ARCLIGHT_AZURE_EXCHANGE_TOKEN\n      :value: \"ARCLIGHT_AZURE_EXCHANGE_TOKEN\"\n\n   .. py:attribute:: ARCLIGHT_AZURE_EXCHANGE_TOKEN_WITH_USER_DELEGATION_KEY\n      :value: \"ARCLIGHT_AZURE_EXCHANGE_TOKEN_WITH_USER_DELEGATION_KEY\"\n\n   .. py:attribute:: ARCLIGHT_MULTI_TENANT_AZURE_EXCHANGE_TOKEN\n      :value: \"ARCLIGHT_MULTI_TENANT_AZURE_EXCHANGE_TOKEN\"\n\n   .. py:attribute:: ARCLIGHT_MULTI_TENANT_AZURE_EXCHANGE_TOKEN_WITH_USER_DELEGATION_KEY\n      :value: \"ARCLIGHT_MULTI_TENANT_AZURE_EXCHANGE_TOKEN_WITH_USER_DELEGATION_KEY\"\n\n   .. py:attribute:: AZURE_ACTIVE_DIRECTORY_TOKEN\n      :value: \"AZURE_ACTIVE_DIRECTORY_TOKEN\"\n```\n\n----------------------------------------\n\nTITLE: AccessControlAPI Class Definition - Python\nDESCRIPTION: Defines the AccessControlAPI class, which provides methods for managing and checking access control policies for Databricks resources. This class is part of the databricks.sdk.service.iam module.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/iam/access_control.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: AccessControlAPI\n\n    Rule based Access Control for Databricks Resources.\n```\n\n----------------------------------------\n\nTITLE: Defining REPLACE attribute for PatchOp in Python\nDESCRIPTION: Defines the REPLACE attribute for the PatchOp class, representing the 'replace' operation type in a patch request.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/iam.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: PatchOp\n\n   Type of patch operation.\n\n   .. py:attribute:: REPLACE\n      :value: \"REPLACE\"\n```\n\n----------------------------------------\n\nTITLE: Migrating Classic SQL Dashboard\nDESCRIPTION: Migrates a classic SQL dashboard to a Lakeview dashboard using the LakeviewAPI. Requires the source dashboard ID. It allows specifying display name, parent path and parameter syntax update. It returns the migrated Dashboard object.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: migrate(source_dashboard_id: str [, display_name: Optional[str], parent_path: Optional[str], update_parameter_syntax: Optional[bool]]) -> Dashboard\n\n    Migrate dashboard.\n\n    Migrates a classic SQL dashboard to Lakeview.\n\n    :param source_dashboard_id: str\n      UUID of the dashboard to be migrated.\n    :param display_name: str (optional)\n      Display name for the new Lakeview dashboard.\n    :param parent_path: str (optional)\n      The workspace path of the folder to contain the migrated Lakeview dashboard.\n    :param update_parameter_syntax: bool (optional)\n      Flag to indicate if mustache parameter syntax ({{ param }}) should be auto-updated to named syntax\n      (:param) when converting datasets in the dashboard.\n\n    :returns: :class:`Dashboard`\n```\n\n----------------------------------------\n\nTITLE: Defining VolumeType enum in Python\nDESCRIPTION: Defines a Python enum `VolumeType` with values EXTERNAL and MANAGED. This represents the type of volume, whether external or managed, within the Databricks environment.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n.. py:class:: VolumeType\n\n   The type of the volume. An external volume is located in the specified external location. A managed volume is located in the default location which is specified by the parent schema, or the parent catalog, or the Metastore. [Learn more]\n   [Learn more]: https://docs.databricks.com/aws/en/volumes/managed-vs-external\n\n   .. py:attribute:: EXTERNAL\n      :value: \"EXTERNAL\"\n\n   .. py:attribute:: MANAGED\n      :value: \"MANAGED\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Asynchronous Token Refresh - Configuration Object\nDESCRIPTION: This snippet illustrates how to disable asynchronous token refresh using the configuration object within the Databricks SDK for Python. It involves setting the `disable_async_token_refresh` option to `true` in the configuration. This is useful for environments where asynchronous token refresh might not be desired.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/NEXT_CHANGELOG.md#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Defining INTERNAL_AND_EXTERNAL attribute in Python\nDESCRIPTION: Defines a constant string attribute `INTERNAL_AND_EXTERNAL`. This attribute likely represents a scope or visibility setting used within the Databricks environment.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/catalog.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n.. py:attribute:: INTERNAL_AND_EXTERNAL\n :value: \"INTERNAL_AND_EXTERNAL\"\n```\n\n----------------------------------------\n\nTITLE: Trashing Lakeview Dashboard\nDESCRIPTION: Moves a Lakeview dashboard to the trash using the LakeviewAPI. Requires the dashboard ID.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/dashboards/lakeview.rst#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n.. py:method:: trash(dashboard_id: str)\n\n    Trash dashboard.\n\n    Trash a dashboard.\n\n    :param dashboard_id: str\n      UUID identifying the dashboard.\n```\n\n----------------------------------------\n\nTITLE: Defining CleanRoomTaskRunLifeCycleState Enum Python\nDESCRIPTION: This code snippet defines an enumeration called `CleanRoomTaskRunLifeCycleState` representing the lifecycle state of a clean room task run. Possible values include `BLOCKED`, `INTERNAL_ERROR`, `PENDING`, `QUEUED`, `RUNNING`, `RUN_LIFE_CYCLE_STATE_UNSPECIFIED`, `SKIPPED`, `TERMINATED`, `TERMINATING`, and `WAITING_FOR_RETRY`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/jobs.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: CleanRoomTaskRunLifeCycleState\n\n   Copied from elastic-spark-common/api/messages/runs.proto. Using the original definition to remove coupling with jobs API definition\n\n   .. py:attribute:: BLOCKED\n      :value: \"BLOCKED\"\n\n   .. py:attribute:: INTERNAL_ERROR\n      :value: \"INTERNAL_ERROR\"\n\n   .. py:attribute:: PENDING\n      :value: \"PENDING\"\n\n   .. py:attribute:: QUEUED\n      :value: \"QUEUED\"\n\n   .. py:attribute:: RUNNING\n      :value: \"RUNNING\"\n\n   .. py:attribute:: RUN_LIFE_CYCLE_STATE_UNSPECIFIED\n      :value: \"RUN_LIFE_CYCLE_STATE_UNSPECIFIED\"\n\n   .. py:attribute:: SKIPPED\n      :value: \"SKIPPED\"\n\n   .. py:attribute:: TERMINATED\n      :value: \"TERMINATED\"\n\n   .. py:attribute:: TERMINATING\n      :value: \"TERMINATING\"\n\n   .. py:attribute:: WAITING_FOR_RETRY\n      :value: \"WAITING_FOR_RETRY\"\n```\n\n----------------------------------------\n\nTITLE: Defining Product User Agent with Databricks SDK\nDESCRIPTION: This snippet shows how to use the `with_product()` function from the `databricks.sdk.useragent` module to add the product's name and version to the User-Agent header. It specifies 'databricks-example-product' as the product name and '1.2.0' as the version, which should conform to SemVer standards.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import useragent\nuseragent.with_product(\"databricks-example-product\", \"1.2.0\")\n```\n\n----------------------------------------\n\nTITLE: Creating Exchange Filter using ProviderExchangeFiltersAPI in Python\nDESCRIPTION: Creates a new exchange filter using the ``create`` method. It takes an ``ExchangeFilter`` object as input and returns a ``CreateExchangeFilterResponse`` object. The ExchangeFilter parameter defines the properties of the new filter.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/marketplace/provider_exchange_filters.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nw.provider_exchange_filters.create(filter: ExchangeFilter) -> CreateExchangeFilterResponse\n```\n\n----------------------------------------\n\nTITLE: Defining Enum: EgressNetworkPolicyInternetAccessPolicyInternetDestinationInternetDestinationFilteringProtocol - Python\nDESCRIPTION: This Python enum defines the filtering protocol used by the DP for Internet destinations in egress network policies. It specifies the protocol used for filtering traffic to internet destinations. Currently, only TCP is supported.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/dbdataclasses/settings.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n.. py:class:: EgressNetworkPolicyInternetAccessPolicyInternetDestinationInternetDestinationFilteringProtocol\n\n   The filtering protocol used by the DP. For private and public preview, SEG will only support TCP filtering (i.e. DNS based filtering, filtering by destination IP address), so protocol will be set to TCP by default and hidden from the user. In the future, users may be able to select HTTP filtering (i.e. SNI based filtering, filtering by FQDN).\n\n   .. py:attribute:: TCP\n      :value: \"TCP\"\n```\n\n----------------------------------------\n\nTITLE: Handling Errors with Databricks SDK\nDESCRIPTION: This snippet demonstrates how to handle errors when using the Databricks SDK for Python. It attempts to retrieve a cluster with a specific ID and catches the `ResourceDoesNotExist` exception if the cluster is not found. It prints an error message if the cluster is not found. Requires `WorkspaceClient` and `ResourceDoesNotExist` from `databricks.sdk`.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/README.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.errors import ResourceDoesNotExist\n\nw = WorkspaceClient()\ntry:\n    w.clusters.get(cluster_id='1234-5678-9012')\nexcept ResourceDoesNotExist as e:\n    print(f'Cluster not found: {e}')\n```\n\n----------------------------------------\n\nTITLE: Get Automatic Cluster Update Setting - Python\nDESCRIPTION: Retrieves the automatic cluster update setting for the workspace. Accepts an optional etag for optimistic concurrency control.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/docs/workspace/settings/automatic_cluster_update.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. py:method:: get( [, etag: Optional[str]]) -> AutomaticClusterUpdateSetting\n\n    Get the automatic cluster update setting.\n\n    Gets the automatic cluster update setting.\n\n    :param etag: str (optional)\n      etag used for versioning. The response is at least as fresh as the eTag provided. This is used for\n      optimistic concurrency control as a way to help prevent simultaneous writes of a setting overwriting\n      each other. It is strongly suggested that systems make use of the etag in the read -> delete pattern\n      to perform setting deletions in order to avoid race conditions. That is, get an etag from a GET\n      request, and pass it with the DELETE request to identify the rule set version you are deleting.\n\n    :returns: :class:`AutomaticClusterUpdateSetting`\n```\n\n----------------------------------------\n\nTITLE: Handling InvalidParameterValue Exception in Python (Deprecated)\nDESCRIPTION: This code snippet illustrates the original (deprecated) way of handling the InvalidParameterValue exception when fetching a job by ID using the Databricks SDK for Python. It is included to show the before-and-after state of the error handling modification.\nSOURCE: https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntry:\n    w.jobs.get_by_id(\"123\")\nexcept e as InvalidParameterValue:\n    ...\n```"
  }
]
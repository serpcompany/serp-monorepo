[
  {
    "owner": "huggingface",
    "repo": "trl",
    "content": "TITLE: Initializing and Training DPOTrainer\nDESCRIPTION: This Python code snippet demonstrates how to use the `DPOTrainer` class from the `trl` library for Direct Preference Optimization. It loads a model and tokenizer, loads a dataset, configures the trainer with training arguments, model, dataset, and tokenizer, and then starts the training process. It depends on `trl`, `datasets` and `transformers` libraries.\nSOURCE: https://github.com/huggingface/trl/blob/main/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import DPOConfig, DPOTrainer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\ndataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\ntraining_args = DPOConfig(output_dir=\"Qwen2.5-0.5B-DPO\")\ntrainer = DPOTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    processing_class=tokenizer\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Training LoRA Adapters with SFTTrainer and PEFT\nDESCRIPTION: This snippet demonstrates how to train LoRA adapters using the `SFTTrainer` and the `peft` library. It loads a dataset, defines a `LoraConfig`, and passes it to the `SFTTrainer` for training. The `modules_to_save` parameter is crucial when the chat template contains special tokens.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\nfrom peft import LoraConfig\n\ndataset = load_dataset(\"trl-lib/Capybara\", split=\"train\")\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=\"all-linear\",\n    modules_to_save=[\"lm_head\", \"embed_token\"],\n    task_type=\"CAUSAL_LM\",\n)\n\ntrainer = SFTTrainer(\n    \"Qwen/Qwen2.5-0.5B\",\n    train_dataset=dataset,\n    args=SFTConfig(output_dir=\"Qwen2.5-0.5B-SFT\"),\n    peft_config=peft_config\n)\n\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training RewardTrainer\nDESCRIPTION: This Python code snippet demonstrates how to use the `RewardTrainer` class from the `trl` library for training a reward model.  It loads a model and tokenizer, loads a dataset, configures the trainer with training arguments, model, dataset, and tokenizer, and then starts the training process. It depends on `trl`, `datasets` and `transformers` libraries.\nSOURCE: https://github.com/huggingface/trl/blob/main/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import RewardConfig, RewardTrainer\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"Qwen/Qwen2.5-0.5B-Instruct\", num_labels=1\n)\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\ndataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\n\ntraining_args = RewardConfig(output_dir=\"Qwen2.5-0.5B-Reward\", per_device_train_batch_size=2)\ntrainer = RewardTrainer(\n    args=training_args,\n    model=model,\n    processing_class=tokenizer,\n    train_dataset=dataset,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Supervised Fine-tuning with SFTTrainer using an initialized model - Python\nDESCRIPTION: This code snippet demonstrates how to fine-tune a pre-initialized language model using the `SFTTrainer` class from the TRL library. It loads the `imdb` dataset, initializes the `facebook/opt-350m` model using `AutoModelForCausalLM`, configures the training arguments using `SFTConfig`, and then trains the model.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\n\ndataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n\ntraining_args = SFTConfig(output_dir=\"/tmp\")\n\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    args=training_args,\n)\n\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Supervised Fine-tuning with SFTTrainer using a dataset from the Hub - Python\nDESCRIPTION: This code snippet demonstrates how to fine-tune a language model using the `SFTTrainer` class from the TRL library with a dataset hosted on the Hugging Face Hub. It loads the `imdb` dataset, configures the training arguments using `SFTConfig`, and then trains the `facebook/opt-350m` model.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\n\ndataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n\ntraining_args = SFTConfig(\n    max_length=512,\n    output_dir=\"/tmp\",\n)\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    args=training_args,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Online DPO Training Script with Qwen2 Model in Python\nDESCRIPTION: This script demonstrates how to train a model using the online DPO method with the Qwen2-0.5B-Instruct model and the UltraFeedback dataset. It initializes the model, tokenizer, judge, dataset, training configuration, and trainer, then starts the training process.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/online_dpo_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train_online_dpo.py\nfrom datasets import load_dataset\nfrom trl import OnlineDPOConfig, OnlineDPOTrainer, PairRMJudge\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\njudge = PairRMJudge()\ntrain_dataset = load_dataset(\"trl-lib/ultrafeedback-prompt\", split=\"train\")\n\ntraining_args = OnlineDPOConfig(output_dir=\"Qwen2-0.5B-OnlineDPO\", logging_steps=10)\ntrainer = OnlineDPOTrainer(\n    model=model, judge=judge, args=training_args, processing_class=tokenizer, train_dataset=train_dataset\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Adding Margin to Reward Model Loss in Python\nDESCRIPTION: This code snippet illustrates how to add a margin to the reward model loss by creating a 'margin' column in the dataset. It defines a function `add_margin` that calculates the margin based on 'score_chosen' and 'score_rejected' columns and then maps this function to the dataset using the `map` method. This margin is used during loss computation.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reward_trainer.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef add_margin(row):\n    # Assume you have a score_chosen and score_rejected columns that you want to use to compute the margin\n    return {'margin': row['score_chosen'] - row['score_rejected']}\n\ndataset = dataset.map(add_margin)\n\n```\n\n----------------------------------------\n\nTITLE: DPO Training Script in Python\nDESCRIPTION: This script demonstrates how to train a language model using the DPO method with the TRL library. It loads a pre-trained model and tokenizer, loads a preference dataset, configures training arguments, and then trains the model using the DPOTrainer. It requires the `datasets`, `trl`, and `transformers` libraries.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/dpo_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train_dpo.py\nfrom datasets import load_dataset\nfrom trl import DPOConfig, DPOTrainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\ntrain_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\n\ntraining_args = DPOConfig(output_dir=\"Qwen2-0.5B-DPO\", logging_steps=10)\ntrainer = DPOTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Loading Model with 4-bit Precision and PEFT Configuration in Python\nDESCRIPTION: This Python snippet demonstrates how to load a pre-trained language model with 4-bit precision and a PEFT (LoRA) configuration. It uses the `AutoModelForCausalLMWithValueHead` to load the model with 4-bit quantization and applies the LoRA configuration for parameter-efficient fine-tuning.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/peft_integration.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npretrained_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n    config.model_name, \n    peft_config=lora_config,\n    load_in_4bit=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Replacing Judge with Reward Model in Online DPO Training\nDESCRIPTION: This code snippet shows how to replace the `PairRMJudge` with a reward model (in this case, `trl-lib/Qwen2-0.5B-Reward`) in the `OnlineDPOTrainer`.  It requires loading the reward model and its tokenizer from the transformers library. Note that the code uses a diff format.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/online_dpo_trainer.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n- from trl import PairRMJudge\n+ from transformers import AutoModelForSequenceClassification\n\n- judge = PairRMJudge()\n+ reward_model = AutoModelForSequenceClassification.from_pretrained(\"trl-lib/Qwen2-0.5B-Reward\", num_labels=1)\n+ reward_tokenizer = AutoTokenizer.from_pretrained(\"trl-lib/Qwen2-0.5B-Reward\")\n\n  trainer = OnlineDPOTrainer(\n      ...\n-     judge=judge,\n+     reward_model=reward_model,\n+     reward_processing_class=reward_tokenizer,\n      ...\n```\n\n----------------------------------------\n\nTITLE: Loading and using a jsonl dataset with SFTTrainer - Python\nDESCRIPTION: This code snippet shows how to load a dataset from a local JSONL file or the Hugging Face Hub and use it with the `SFTTrainer`. It also shows how to configure packing.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\n\n...\n\n# load jsonl dataset\ndataset = load_dataset(\"json\", data_files=\"path/to/dataset.jsonl\", split=\"train\")\n# load dataset from the HuggingFace Hub\ndataset = load_dataset(\"philschmid/dolly-15k-oai-style\", split=\"train\")\n\n...\n\ntraining_args = SFTConfig(packing=True)\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    args=training_args,\n    train_dataset=dataset,\n)\n```\n\n----------------------------------------\n\nTITLE: ORPO Training Script with Transformers and TRL\nDESCRIPTION: This Python script demonstrates how to train a model using the ORPO method with the Transformers and TRL libraries. It loads a pre-trained language model and tokenizer, loads a preference dataset, configures the ORPO trainer, and starts the training process. The model is Qwen2-0.5B-Instruct and the dataset is ultrafeedback_binarized.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/orpo_trainer.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# train_orpo.py\nfrom datasets import load_dataset\nfrom trl import ORPOConfig, ORPOTrainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\ntrain_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\n\ntraining_args = ORPOConfig(output_dir=\"Qwen2-0.5B-ORPO\", logging_steps=10)\ntrainer = ORPOTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: SFT Configuration with Packing\nDESCRIPTION: This code snippet shows how to enable sequence packing in the `SFTConfig` from the `trl` library by setting `packing=True`. Packing concatenates multiple sequences into a single sequence up to `max_length`, reducing padding and improving memory efficiency. Setting `max_length` determines the maximum length of the packed sequence.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reducing_memory_usage.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import SFTConfig\n\ntraining_args = SFTConfig(..., packing=True, max_length=512)\n```\n\n----------------------------------------\n\nTITLE: Train a model using KTO\nDESCRIPTION: This script demonstrates how to train a language model using the Kahneman-Tversky Optimization (KTO) method from the TRL library. It loads a pre-trained model and tokenizer, sets up the KTO configuration, and initiates the training process using the KTOTrainer. Requires transformers, datasets, and trl libraries to be installed.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/kto_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train_kto.py\nfrom datasets import load_dataset\nfrom trl import KTOConfig, KTOTrainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\ntrain_dataset = load_dataset(\"trl-lib/kto-mix-14k\", split=\"train\")\n\ntraining_args = KTOConfig(output_dir=\"Qwen2-0.5B-KTO\", logging_steps=10)\ntrainer = KTOTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Computing Toxicity Rewards with RoBERTa in Python\nDESCRIPTION: This code snippet demonstrates how to compute toxicity rewards using a RoBERTa model. It passes toxicity inputs through the `toxicity_model` and extracts the raw logits of the 'neutral' label, which are then converted to a list to be used as rewards.  This reward is intended to guide the PPO algorithm during training.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/detoxifying_a_lm.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlogits = toxicity_model(**toxicity_inputs).logits.float()\nrewards = (logits[:, 0]).tolist()\n```\n\n----------------------------------------\n\nTITLE: Loading Adapter Twice for DPO Training in Python\nDESCRIPTION: This Python code snippet demonstrates how to load a fine-tuned PEFT adapter into a model twice with different names, one for training and one as a reference model, to be used with the DPOTrainer. It initializes the base model with BitsAndBytesConfig for 4-bit quantization and loads the PEFT adapter twice using `PeftModel.from_pretrained` and `model.load_adapter`, assigning different names ('train' and 'reference'). The trainer is initialized with `DPOConfig` specifying the adapter names.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/dpo_trainer.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Load the base model.\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    llm_int8_threshold=6.0,\n    llm_int8_has_fp16_weight=False,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/mixtral-8x7b-v0.1\",\n    load_in_4bit=True,\n    quantization_config=bnb_config,\n    attn_implementation=\"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nmodel.config.use_cache = False\n\n# Load the adapter.\nmodel = PeftModel.from_pretrained(\n    model,\n    \"/path/to/peft\",\n    is_trainable=True,\n    adapter_name=\"train\",\n)\n# Load the adapter a second time, with a different name, which will be our reference model.\nmodel.load_adapter(\"/path/to/peft\", adapter_name=\"reference\")\n\n# Initialize the trainer, without a ref_model param.\ntraining_args = DPOConfig(\n    model_adapter_name=\"train\",\n    ref_adapter_name=\"reference\",\n)\ndpo_trainer = DPOTrainer(\n    model,\n    args=training_args,\n    ...\n)\n```\n\n----------------------------------------\n\nTITLE: Packing dataset with SFTTrainer - Python\nDESCRIPTION: This code snippet demonstrates how to use example packing with the `SFTTrainer` to increase training efficiency. It sets `packing=True` in the `SFTConfig`.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n...\ntraining_args = SFTConfig(packing=True)\n\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    args=training_args\n)\n\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Conversational Dataset Format - JSON\nDESCRIPTION: This JSON snippet shows the expected format for conversational datasets used with the `SFTTrainer`. It includes an array of messages, where each message has a `role` (system, user, assistant) and `content`.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training GRPOTrainer\nDESCRIPTION: This Python code snippet demonstrates how to use the `GRPOTrainer` class from the `trl` library for Group Relative Policy Optimization. It defines a custom reward function, loads a dataset, configures the trainer with a model, reward function, and training dataset, and then starts the training process. It depends on `trl` and `datasets` libraries.\nSOURCE: https://github.com/huggingface/trl/blob/main/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom trl import GRPOTrainer\n\ndataset = load_dataset(\"trl-lib/tldr\", split=\"train\")\n\n# Dummy reward function: count the number of unique characters in the completions\ndef reward_num_unique_chars(completions, **kwargs):\n    return [len(set(c)) for c in completions]\n\ntrainer = GRPOTrainer(\n    model=\"Qwen/Qwen2-0.5B-Instruct\",\n    reward_funcs=reward_num_unique_chars,\n    train_dataset=dataset,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Training Reward Model with PEFT using RewardTrainer in Python\nDESCRIPTION: This code snippet demonstrates how to train a reward model using the RewardTrainer class from the TRL library, leveraging PEFT (Parameter-Efficient Fine-Tuning) with LoRA (Low-Rank Adaptation). It initializes a sequence classification model, configures LoRA with specific parameters, and then sets up and trains the RewardTrainer with the model, training arguments, tokenizer, dataset, and PEFT configuration. The `peft_config` argument is passed to `RewardTrainer`.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reward_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom peft import LoraConfig, TaskType\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom trl import RewardTrainer, RewardConfig\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"gpt2\")\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n)\n\n...\n\ntrainer = RewardTrainer(\n    model=model,\n    args=training_args,\n    processing_class=tokenizer,\n    train_dataset=dataset,\n    peft_config=peft_config,\n)\n\ntrainer.train()\n\n```\n\n----------------------------------------\n\nTITLE: Running 2.8B Online DPO Experiment\nDESCRIPTION: This shell command launches an online DPO experiment using the Pythia 2.8B model. It utilizes the `accelerate` tool with a DeepSpeed configuration file and sets various hyperparameters, including learning rate, batch size, and number of training epochs.  The experiment uses bf16 mixed precision and is configured to push the results to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/online_dpo_trainer.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\naccelerate launch --config_file examples/accelerate_configs/deepspeed_zero2.yaml \\\n    examples/scripts/dpo_online.py \\\n    --model_name_or_path trl-lib/pythia-2.8b-deduped-tldr-sft  \\\n    --reward_model_path trl-lib/pythia-2.8b-deduped-tldr-rm \\\n    --dataset_name trl-lib/tldr \\\n    --learning_rate 5.0e-7 \\\n    --output_dir pythia-2.8b-deduped-tldr-online-dpo \\\n    --beta 0.1 \\\n    --per_device_train_batch_size 8 \\\n    --gradient_accumulation_steps 2 \\\n    --num_train_epochs 3 \\\n    --max_new_tokens 53 \\\n    --warmup_ratio 0.1 \\\n    --missing_eos_penalty 1.0 \\\n    --bf16 \\\n    --logging_steps 20 \\\n    --save_steps 0.1 \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Executing ORPO Training Script with Accelerate\nDESCRIPTION: This bash command executes the ORPO training script using the accelerate library, which enables distributed training across multiple GPUs. This is essential for efficient training of large language models. It assumes that the `train_orpo.py` script is in the current directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/orpo_trainer.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\naccelerate launch train_orpo.py\n```\n\n----------------------------------------\n\nTITLE: Loading Model with Value Head\nDESCRIPTION: This snippet showcases how to load a fine-tuned model with the value head using `AutoModelForCausalLMWithValueHead` from the TRL library. This is useful when you want to continue training or utilize the value head for specific tasks. It assumes the model has been previously saved with the value head.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/quickstart.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom trl.model import AutoModelForCausalLMWithValueHead\n\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"my-fine-tuned-model-ppo\")\n```\n\n----------------------------------------\n\nTITLE: PRM Model Training with Qwen2-0.5B\nDESCRIPTION: This code snippet demonstrates how to train a model using the PRM method with the Qwen2-0.5B model and the Math Shepherd dataset. It loads the dataset, initializes the model and tokenizer, configures the training arguments, and then starts the training process. The script uses the `datasets`, `trl`, and `transformers` libraries. The output is a trained PRM model.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/prm_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train_prm.py\nfrom datasets import load_dataset\nfrom trl import PRMConfig, PRMTrainer\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"Qwen/Qwen2-0.5B\", num_labels=2)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\ntrain_dataset = load_dataset(\"trl-lib/math_shepherd\", split=\"train[:10%]\")\n\ntraining_args = PRMConfig(output_dir=\"Qwen2-0.5B-Reward-Math-Sheperd\", logging_steps=10)\ntrainer = PRMTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Enabling Flash Attention 2 with AutoModelForCausalLM\nDESCRIPTION: This code snippet shows how to enable Flash Attention 2 when loading an AutoModelForCausalLM using the `attn_implementation` parameter.  It's crucial to have the `flash-attn` package installed.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    load_in_4bit=True,\n    attn_implementation=\"flash_attention_2\"\n)\n```\n\n----------------------------------------\n\nTITLE: GRPO Training Script with Qwen Model - Python\nDESCRIPTION: This script demonstrates how to train a language model using the GRPO (Group Relative Policy Optimization) method. It loads the TLDR dataset, defines a reward function based on completion length, configures the GRPOTrainer, and starts the training process using a Qwen 0.5B Instruct model.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/grpo_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train_grpo.py\nfrom datasets import load_dataset\nfrom trl import GRPOConfig, GRPOTrainer\n\ndataset = load_dataset(\"trl-lib/tldr\", split=\"train\")\n\n# Define the reward function, which rewards completions that are close to 20 characters\ndef reward_len(completions, **kwargs):\n    return [-abs(20 - len(completion)) for completion in completions]\n\ntraining_args = GRPOConfig(output_dir=\"Qwen2-0.5B-GRPO\", logging_steps=10)\ntrainer = GRPOTrainer(\n    model=\"Qwen/Qwen2-0.5B-Instruct\",\n    reward_funcs=reward_len,\n    args=training_args,\n    train_dataset=dataset,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Loading a Model in bfloat16 Precision using Transformers\nDESCRIPTION: This code snippet shows how to load a pre-trained language model in `bfloat16` precision to reduce memory usage. Loading the model with `torch_dtype=torch.bfloat16` reduces the model size by half. This is crucial for training large models on GPUs with limited memory.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/detoxifying_a_lm.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.bfloat16)\n```\n\n----------------------------------------\n\nTITLE: Filtering Dataset Based on Toxicity Score in Python\nDESCRIPTION: This code snippet filters a dataset of prompts based on their toxicity score. It loads the 'allenai/real-toxicity-prompts' dataset and filters it to include only prompts with a toxicity score greater than 0.3.  It utilizes the `load_dataset` and `filter` functions from the `datasets` library.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/detoxifying_a_lm.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrain_dataset = load_dataset(\"allenai/real-toxicity-prompts\", split=\"train\")\n\ndef filter_fn(sample):\n    toxicity = sample[\"prompt\"][\"toxicity\"]\n    return toxicity is not None and toxicity > 0.3\n\ntrain_dataset = train_dataset.filter(filter_fn, batched=False)\n```\n\n----------------------------------------\n\nTITLE: Running DPO Script\nDESCRIPTION: This command executes the DPO training script (`trl/scripts/dpo.py`) using the `accelerate` launcher, fine-tuning the Qwen2 0.5B model on the UltraFeedback dataset. It sets the model name, dataset name, number of training epochs, logging steps, and output directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/dpo_trainer.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch trl/scripts/dpo.py \\\n    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n    --dataset_name trl-lib/ultrafeedback_binarized \\\n    --num_train_epochs 1 \\\n    --logging_steps 25 \\\n    --output_dir Qwen2-0.5B-DPO\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training SFTTrainer\nDESCRIPTION: This Python code snippet demonstrates how to use the `SFTTrainer` class from the `trl` library for supervised fine-tuning. It loads a dataset and configures the trainer with a specified model and training dataset, then starts the training process.  It depends on `trl` and `datasets` libraries.\nSOURCE: https://github.com/huggingface/trl/blob/main/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"trl-lib/Capybara\", split=\"train\")\n\ntrainer = SFTTrainer(\n    model=\"Qwen/Qwen2.5-0.5B\",\n    train_dataset=dataset,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Sentiment Analysis of Query/Response Pairs\nDESCRIPTION: Performs sentiment analysis on query/response pairs before and after a certain action. It uses a sentiment pipeline (`sentiment_pipe`) to get positive sentiment scores and stores them in the `game_data` dictionary under 'rewards (before)' and 'rewards (after)' keys. The `sent_kwargs` are passed to the sentiment pipeline.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntexts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\npipe_outputs = sentiment_pipe(texts, **sent_kwargs)\npositive_scores = [\n    item[\"score\"]\n    for output in pipe_outputs\n    for item in output\n    if item[\"label\"] == \"POSITIVE\"\n]\ngame_data[\"rewards (before)\"] = positive_scores\n\ntexts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\npipe_outputs = sentiment_pipe(texts, **sent_kwargs)\npositive_scores = [\n    item[\"score\"]\n    for output in pipe_outputs\n    for item in output\n    if item[\"label\"] == \"POSITIVE\"\n]\ngame_data[\"rewards (after)\"] = positive_scores\n```\n\n----------------------------------------\n\nTITLE: Computing Reward Score in PPO Training Loop in Python\nDESCRIPTION: This snippet shows how to access the `compute_reward_score` method from the `PPOTrainer` to calculate rewards within a PPO training loop. It uses the `model` attribute of the trainer instance and passes the necessary inputs.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/multi_adapter_rl.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nrewards = trainer.model.compute_reward_score(**inputs)\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Transformers Pipeline (Full Fine-tuning)\nDESCRIPTION: This snippet shows how to generate text using the transformers pipeline with a fully fine-tuned model. It initializes the pipeline for text generation and then uses it to generate text from a given input. This requires the `transformers` library.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/use_model.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline\n\nmodel_name_or_path = \"kashif/stack-llama-2\" #path/to/your/model/or/name/on/hub\npipe = pipeline(\"text-generation\", model=model_name_or_path)\nprint(pipe(\"This movie was really\")[0][\"generated_text\"])\n```\n\n----------------------------------------\n\nTITLE: SFT Configuration with Padding-Free Batching\nDESCRIPTION: This code snippet shows how to enable padding-free batching in the `SFTConfig` from the `trl` library by setting `padding_free=True`.  It also configures the model to use `flash_attention_2` to avoid batch contamination. Padding-free batching improves memory efficiency by eliminating padding from the batches during training.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reducing_memory_usage.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import SFTConfig\n\ntraining_args = SFTConfig(..., padding_free=True, model_init_kwargs={\"attn_implementation\": \"flash_attention_2\"})\n```\n\n----------------------------------------\n\nTITLE: Using the Finetuned Stable Diffusion Model\nDESCRIPTION: This code snippet demonstrates how to load and use a finetuned Stable Diffusion model. It loads the pipeline, sets LoRA weights, generates images from prompts, and saves the resulting images.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/alignprop_trainer.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\npipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\npipeline.to(\"cuda\")\n\npipeline.load_lora_weights('mihirpd/alignprop-trl-aesthetics')\n\nprompts = [\"squirrel\", \"crab\", \"starfish\", \"whale\",\"sponge\", \"plankton\"]\nresults = pipeline(prompts)\n\nfor prompt, image in zip(prompts,results.images):\n    image.save(f\"dump/{prompt}.png\")\n```\n\n----------------------------------------\n\nTITLE: Supervised Fine-Tuning (SFT)\nDESCRIPTION: This command launches the SFT training script using `accelerate`. It fine-tunes the base llama-v2-7b model to create llama-v2-7b-se. Key parameters include the output directory, number of training steps, logging and saving frequencies, batch sizes, learning rate, and optimization settings. The training progress is reported to WandB.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama_2/scripts/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch examples/research_projects/stack_llama_2/scripts/sft_llama2.py \\\n    --output_dir=\"./sft\" \\\n    --max_steps=500 \\\n    --logging_steps=10 \\\n    --save_steps=10 \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=1 \\\n    --gradient_accumulation_steps=2 \\\n    --gradient_checkpointing=False \\\n    --group_by_length=False \\\n    --learning_rate=1e-4 \\\n    --lr_scheduler_type=\"cosine\" \\\n    --warmup_steps=100 \\\n    --weight_decay=0.05 \\\n    --optim=\"paged_adamw_32bit\" \\\n    --bf16=True \\\n    --remove_unused_columns=False \\\n    --run_name=\"sft_llama2\" \\\n    --report_to=\"wandb\"\n```\n\n----------------------------------------\n\nTITLE: Customizing Sample Size in BestOfNSampler\nDESCRIPTION: This snippet demonstrates how to customize the sample size for the `BestOfNSampler` at the time of instance initialization. The `sample_size` parameter controls the number of samples generated for each query.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/best_of_n.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nbest_of_n = BestOfNSampler(model, tokenizer, queries_to_scores, length_sampler=output_length_sampler, sample_size=8)\n```\n\n----------------------------------------\n\nTITLE: Import Modules\nDESCRIPTION: Imports specific modules from the installed libraries, including torch, tqdm, pandas, transformers, datasets, and trl.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom tqdm import tqdm\nimport pandas as pd\n\ntqdm.pandas()\n\nfrom transformers import pipeline, AutoTokenizer\nfrom datasets import load_dataset\n\nfrom trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\nfrom trl.core import LengthSampler\n```\n\n----------------------------------------\n\nTITLE: Configure QLoRA for fine-tuning\nDESCRIPTION: This snippet configures QLoRA (Quantization-aware Low-Rank Adaptation) for fine-tuning a language model. It defines parameters like `lora_alpha`, `lora_dropout`, `r` (rank), `bias`, `target_modules`, `task_type`, and `modules_to_save`. The `LoraConfig` object is then used in the training process to enable efficient fine-tuning with reduced memory footprint.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/training_vlm_sft.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.05,\n    r=16,\n    bias=\"none\",\n    target_modules=\"all-linear\",\n    task_type=\"CAUSAL_LM\",\n    modules_to_save=[\n        \"lm_head\",\n        \"embed_tokens\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Centering Rewards in Reward Model Training with Python\nDESCRIPTION: This code snippet demonstrates how to center rewards during reward model training by setting the `center_rewards_coefficient` in the `RewardConfig`.  It initializes `RewardConfig` with a non-None `center_rewards_coefficient` which activates the auxiliary loss designed to learn a centered reward model, encouraging the model to produce mean-zero outputs. The auxiliary loss is combined with the main loss function, weighted by the `center_rewards_coefficient` parameter.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reward_trainer.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntraining_args = RewardConfig(\n    center_rewards_coefficient=0.01,\n    ...\n)\n\n```\n\n----------------------------------------\n\nTITLE: Training Model with Nash-MD Method (Python)\nDESCRIPTION: This script demonstrates how to train a model using the Nash-MD method. It loads the Qwen 0.5B model and the UltraFeedback dataset, configures the training arguments and initializes the NashMDTrainer.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/nash_md_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train_nash_md.py\nfrom datasets import load_dataset\nfrom trl import NashMDConfig, NashMDTrainer, PairRMJudge\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\njudge = PairRMJudge()\ntrain_dataset = load_dataset(\"trl-lib/ultrafeedback-prompt\", split=\"train\")\n\ntraining_args = NashMDConfig(output_dir=\"Qwen2-0.5B-NashMD\", logging_steps=10)\ntrainer = NashMDTrainer(\n    model=model, judge=judge, args=training_args, processing_class=tokenizer, train_dataset=train_dataset\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: CPO Trainer Initialization and Training in Python\nDESCRIPTION: This Python snippet demonstrates how to initialize and train a model using the Contrastive Preference Optimization (CPO) method from the trl library. It loads a pre-trained model and tokenizer, sets up the CPO configuration, and trains the model on a preference dataset.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/cpo_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train_cpo.py\nfrom datasets import load_dataset\nfrom trl import CPOConfig, CPOTrainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\ntrain_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\n\ntraining_args = CPOConfig(output_dir=\"Qwen2-0.5B-CPO\", logging_steps=10)\ntrainer = CPOTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoModel with Specified Precision in TRL\nDESCRIPTION: This code snippet demonstrates how to initialize an AutoModelForCausalLM with a specific precision (bfloat16 in this case) using the `model_init_kwargs` parameter in the `SFTConfig`. This is done to load the model in a different precision similar to the `from_pretrained()` method.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.bfloat16)\n\n...\n\ntraining_args = SFTConfig(\n    model_init_kwargs={\n        \"torch_dtype\": \"bfloat16\",\n    },\n    output_dir=\"/tmp\",\n)\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    args=training_args,\n)\n\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: RLHF Training Loop with PPO\nDESCRIPTION: Implements the core RLHF training loop using PPO, generating responses, computing sentiment scores as rewards, and performing a PPO step to update the policy. It also includes logging of statistics to Wandb for monitoring training progress.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/using_llama_models.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfor epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n    question_tensors = batch[\"input_ids\"]\n\n\t# sample from the policy and to generate responses\n    response_tensors = ppo_trainer.generate(\n        question_tensors,\n        return_prompt=False,\n        length_sampler=output_length_sampler,\n        **generation_kwargs,\n    )\n    batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n\n    # Compute sentiment score\n    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n    rewards = [torch.tensor(output[0][\"score\"] - script_args.reward_baseline) for output in pipe_outputs]\n\n    # Run PPO step\n    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n\t# Log stats to Wandb\n    ppo_trainer.log_stats(stats, batch, rewards)\n```\n\n----------------------------------------\n\nTITLE: Setting up Chat Format with special tokens - Python\nDESCRIPTION: This code snippet demonstrates how to set up a model and tokenizer for conversational AI tasks using the `setup_chat_format` function from the TRL library.  It adds special tokens to the tokenizer, resizes the model's embedding layer, and sets the `chat_template` of the tokenizer.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import setup_chat_format\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n\n# Set up the chat format with default 'chatml' format\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n```\n\n----------------------------------------\n\nTITLE: Loading Model and Processor (python)\nDESCRIPTION: Loads the `google/gemma-3-4b-it` model and its corresponding processor using `AutoModelForImageTextToText` and `AutoProcessor` from the `transformers` library. It configures `BitsAndBytesConfig` for 4-bit quantization to optimize memory usage. The `attn_implementation=\"eager\"` argument is important to avoid errors related to attention mechanisms.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/training_vlm_sft.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig\n\nmodel_id = \"google/gemma-3-4b-it\"\n\n# BitsAndBytesConfig int-4 config\bnbb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_storage=torch.bfloat16,\n)\n\n# Load model and tokenizer\nmodel = AutoModelForImageTextToText.from_pretrained(\n    model_id, \n    device_map=\"auto\", \n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"eager\", # Important (Ref: https://github.com/huggingface/transformers/blob/c15a7adb283fa984a40558c7fe7bed30ae975cdd/src/transformers/models/gemma3/modeling_gemma3.py#L934)\n    quantization_config=bnb_config\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\nprocessor.tokenizer.padding_side = \"right\"\n```\n\n----------------------------------------\n\nTITLE: Customize prompts with packed dataset with SFTTrainer - Python\nDESCRIPTION: This code snippet demonstrates how to customize prompts using a packed dataset with the `SFTTrainer`. It defines a `formatting_func` to combine dataset fields and uses it during training.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef formatting_func(example):\n    text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n    return text\n\ntraining_args = SFTConfig(packing=True)\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    args=training_args,\n    formatting_func=formatting_func\n)\n\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Accelerating DPO fine-tuning with Unsloth\nDESCRIPTION: This code snippet demonstrates how to use the Unsloth library to accelerate DPO fine-tuning, replacing standard Hugging Face AutoModelForCausalLM with FastLanguageModel, which provides faster training and reduced memory usage.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/dpo_trainer.md#_snippet_4\n\nLANGUAGE: diff\nCODE:\n```\n  from datasets import load_dataset\n  from trl import DPOConfig, DPOTrainer\n- from transformers import AutoModelForCausalLM, AutoTokenizer\n+ from unsloth import FastLanguageModel\n\n- model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n- tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n+ model, tokenizer = FastLanguageModel.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n+ model = FastLanguageModel.get_peft_model(model)\n  train_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\n\n- training_args = DPOConfig(output_dir=\"Qwen2-0.5B-DPO\", logging_steps=10)\n+ training_args = DPOConfig(output_dir=\"Qwen2-0.5B-DPO\", logging_steps=10, bf16=True)\n  trainer = DPOTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)\n  trainer.train()\n```\n\n----------------------------------------\n\nTITLE: Initialize PPOTrainer\nDESCRIPTION: Initializes the `PPOTrainer` from the `trl` library, which handles the device placement and optimization during the PPO training process. It uses the configured PPO configuration, the GPT2 model, the reference model, the tokenizer, the training dataset, and the data collator.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nppo_trainer = PPOTrainer(\n    config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring SFTTrainer for Vision-Language Model in Python\nDESCRIPTION: This code shows how to configure the `SFTTrainer` to train a vision-language model with multi-modal data. It sets `remove_unused_columns` to `False` and `skip_prepare_dataset` to `True` in the `training_args` to avoid default text-only processing. The trainer is then initialized with the model, training arguments, data collator, training dataset, and processing class.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ntraining_args.remove_unused_columns = False\ntraining_args.dataset_kwargs = {\"skip_prepare_dataset\": True}\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=train_dataset,\n    processing_class=processor.tokenizer,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding a Learning Rate Scheduler to DPOTrainer in TRL\nDESCRIPTION: This code snippet illustrates how to incorporate a learning rate scheduler (StepLR) into the training process with DPOTrainer in TRL. It initializes both the optimizer (AdamW) and the learning rate scheduler, passing them to the DPOTrainer. The scheduler adjusts the learning rate during training. The required dependencies are datasets, transformers, torch, and trl.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/customization.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom torch import optim\nfrom trl import DPOConfig, DPOTrainer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\ndataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\ntraining_args = DPOConfig(output_dir=\"Qwen2.5-0.5B-DPO\")\n\noptimizer = optim.AdamW(model.parameters(), lr=training_args.learning_rate)\nlr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n\ntrainer = DPOTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n    optimizers=(optimizer, lr_scheduler),\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Initializing IterativeSFTTrainer in Python\nDESCRIPTION: This snippet demonstrates how to initialize an IterativeSFTTrainer with a pre-trained causal language model and tokenizer. It also sets the pad token if it is not already defined. The model_name variable should be defined before this snippet is run.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/iterative_sft_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ntrainer = IterativeSFTTrainer(\n    model,\n    tokenizer\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Model with 8-bit Precision and PEFT Configuration in Python\nDESCRIPTION: This Python snippet demonstrates how to load a pre-trained language model with 8-bit precision and a PEFT (LoRA) configuration.  It utilizes the `AutoModelForCausalLMWithValueHead` class to load the model, enabling 8-bit quantization and applying the specified LoRA configuration.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/peft_integration.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npretrained_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n    config.model_name, \n    load_in_8bit=True,\n    peft_config=lora_config,\n)\n```\n\n----------------------------------------\n\nTITLE: Load GPT2 model and tokenizer\nDESCRIPTION: This snippet loads the pre-trained GPT2 model with a value head, creates a reference model, and loads the tokenizer.  The pad token is set to the end-of-sentence token for proper handling during generation.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ngpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\ngpt2_ref_model = create_reference_model(gpt2_model)\ngpt2_tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n\ngpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n```\n\n----------------------------------------\n\nTITLE: Training a VLM with SFTTrainer\nDESCRIPTION: This code snippet demonstrates how to train a Vision-Language Model (VLM) using the `SFTTrainer` from the `trl` library. It initializes the trainer with the model, training arguments, data collator, training dataset, processor, and QLoRA configuration. After initializing the trainer, the `train()` method is called to start the fine-tuning process, and the final fine-tuned model is saved.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/training_vlm_sft.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Training\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=dataset[\"train\"], # multi-image -> train_dataset=dataset[\"test\"],\n    processing_class=processor,\n    peft_config=peft_config,\n)\n\ntrainer.train()\n\n# Save the final model\ntrainer.save_model()\n```\n\n----------------------------------------\n\nTITLE: Enable vLLM in Online DPO Config\nDESCRIPTION: This code snippet shows how to enable vLLM within the OnlineDPOConfig in TRL by setting the `use_vllm` parameter to `True`. This instructs the trainer to use vLLM for faster generation during training.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/speeding_up_training.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import OnlineDPOConfig\n\ntraining_args = OnlineDPOConfig(..., use_vllm=True)\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning VLM with DPO (Diff)\nDESCRIPTION: This code snippet illustrates the changes needed when fine-tuning a vision-language model (VLM) using the DPO Trainer.  Instead of `AutoModelForCausalLM` and `AutoTokenizer`, you need to use `AutoModelForVision2Seq` and `AutoProcessor` respectively. The `processing_class` parameter in `DPOTrainer` is also changed from `tokenizer` to `processor`.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/dpo_trainer.md#_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n- model = AutoModelForCausalLM.from_pretrained(model_id)\n+ model = AutoModelForVision2Seq.from_pretrained(model_id)\n\n- tokenizer = AutoTokenizer.from_pretrained(model_id)\n+ processor = AutoProcessor.from_pretrained(model_id)\n\n  trainer = DPOTrainer(\n      model,\n      args=training_args,\n      train_dataset=train_dataset,\n-     processing_class=tokenizer,\n+     processing_class=processor,\n)\n```\n\n----------------------------------------\n\nTITLE: Online DPO Configuration: Disable Model Gathering for Generation\nDESCRIPTION: This code snippet shows how to disable model gathering for generation in the `OnlineDPOConfig` from the `trl` library. Setting `ds3_gather_for_generation=False` avoids OOM errors in DeepSpeed ZeRO-3 environments by preventing the gathering of model weights on a single GPU during generation, potentially sacrificing generation speed for memory efficiency.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reducing_memory_usage.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import OnlineDPOConfig\n\ntraining_args = OnlineDPOConfig(..., ds3_gather_for_generation=False)\n```\n\n----------------------------------------\n\nTITLE: PRM Model Evaluation with Pipeline\nDESCRIPTION: This code snippet demonstrates how to use a trained PRM model for token classification using the `pipeline` from the `transformers` library. It loads a dataset, defines an example input, and then uses the pipeline to predict labels for each step in the example. The output is printed to the console, showing the predicted label and the ground truth label for each step.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/prm_trainer.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom transformers import pipeline\n\npipe = pipeline(\"token-classification\", model=\"trl-lib/Qwen2-0.5B-Reward-Math-Sheperd\")\ndataset = load_dataset(\"trl-lib/math_shepherd\")\nexample = {\n    \"prompt\": \"Musa is the class teacher of a class of 45 students. He wants to split them into three groups by age. If a third of the class is under 11 years, and two-fifths are above 11 but under 13, how many students will be in the third group (13 years and above)?\",\n    \"completions\": [\n        \"Step 1: A third of the class is under 11 years because 11 - 1/3 = <<11-1/3=7>>7.\",\n        \"Step 2: Two-fifths of the class are above 11 but under 13 because 2/5 * 11 = <<2/5*11=8>>8.\",\n        \"Step 3: There are 45 students, so the third group will have 45 - 7 - 8 = <<45-7-8=20>>20 students. The answer is: 20\",\n    ],\n    \"labels\": [True, False, False],\n}\n\n\nseparator = \"\\n\"  # It's important to use the same separator as the one used during training\n\nfor idx in range(1, len(example[\"completions\"]) + 1):\n    steps = example[\"completions\"][0:idx]\n    text = separator.join((example[\"prompt\"], *steps)) + separator  # Add a separator between the prompt and each steps\n    pred_entity = pipe(text)[-1][\"entity\"]\n    pred = {\"LABEL_0\": False, \"LABEL_1\": True}[pred_entity]\n    label = example[\"labels\"][idx - 1]\n    print(f\"Step {idx}\\tPredicted: {pred} \\tLabel: {label}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing GKDTrainer for Knowledge Distillation with Qwen Models (Python)\nDESCRIPTION: This code snippet demonstrates how to initialize and use the `GKDTrainer` class from the `trl` library for Generalized Knowledge Distillation (GKD). It loads a student and a teacher model (Qwen), creates dummy datasets, defines training configurations using `GKDConfig`, and then initializes and trains the `GKDTrainer`.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/gkd_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import Dataset\nfrom trl import GKDConfig, GKDTrainer\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n)\n\nNUM_DUMMY_SAMPLES = 100\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n# The model to optimise\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n# The teacher model to calculate the KL divergence against\nteacher_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-1.5B-Instruct\")\n\ntrain_dataset = Dataset.from_dict(\n    {\n        \"messages\": [\n            [\n                {\"role\": \"user\", \"content\": \"Hi, how are you?\"},\n                {\"role\": \"assistant\", \"content\": \"I'm great thanks\"},\n            ]\n        ]\n        * NUM_DUMMY_SAMPLES\n    }\n)\neval_dataset = Dataset.from_dict(\n    {\n        \"messages\": [\n            [\n                {\"role\": \"user\", \"content\": \"What colour is the sky?\"},\n                {\"role\": \"assistant\", \"content\": \"The sky is blue\"},\n            ]\n        ]\n        * NUM_DUMMY_SAMPLES\n    }\n)\n\ntraining_args = GKDConfig(output_dir=\"gkd-model\", per_device_train_batch_size=1)\ntrainer = GKDTrainer(\n    model=model,\n    teacher_model=teacher_model,\n    args=training_args,\n    processing_class=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: GRPOTrainer Example with Custom Reward\nDESCRIPTION: This code showcases how to use the `GRPOTrainer` with a custom reward function and vLLM. It defines a dummy reward function based on unique character count and sets up the trainer with necessary configurations.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/grpo_trainer.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\n\nfrom datasets import load_dataset\nfrom trl import GRPOTrainer, GRPOConfig\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--vllm_server_host\", type=str, default=\"\", help=\"The server IP\")\n    args = parser.parse_args()\n\n    # Example dataset from TLDR\n    dataset = load_dataset(\"trl-lib/tldr\", split=\"train\")\n\n    # Dummy reward function: count the number of unique characters in the completions\n    def reward_num_unique_chars(completions, **kwargs):\n        return [len(set(c)) for c in completions]\n\n    training_args = GRPOConfig(\n        output_dir=\"Qwen2.5-72B-GRPO\",\n        per_device_train_batch_size=4,\n        bf16=True,\n        gradient_checkpointing=True,\n        logging_steps=10,\n        use_vllm=True,\n        vllm_server_host=args.vllm_server_host.replace(\"ip-\", \"\").replace(\"-\", \".\"),  # from ip-X-X-X-X to X.X.X.X\n    )\n\n    trainer = GRPOTrainer(model=\"Qwen/Qwen2.5-72B\", args=training_args, reward_funcs=reward_num_unique_chars, train_dataset=dataset)\n    trainer.train()\n\nif __name__==\"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: DPO Configuration with Truncation\nDESCRIPTION: This snippet demonstrates how to configure the `DPOConfig` class from the `trl` library to apply truncation to both the prompt and completion sequences. It sets the `max_prompt_length` and `max_length` parameters to control the maximum lengths of the prompt and the overall sequence, respectively, mitigating memory usage by limiting sequence lengths.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reducing_memory_usage.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import DPOConfig\n\ntraining_args = DPOConfig(..., max_prompt_length=..., max_length=...)\n```\n\n----------------------------------------\n\nTITLE: System Information Output (Text)\nDESCRIPTION: This is an example of the output generated by the `trl env` command. It includes details about the platform, Python version, PyTorch version, CUDA device, Transformers version, Accelerate version, Datasets version, HF Hub version, TRL version, bitsandbytes version, DeepSpeed version, Diffusers version, Liger-Kernel version, LLM-Blender version, OpenAI version, and PEFT version.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/clis.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nCopy-paste the following information when reporting an issue:\n\n- Platform: Linux-5.15.0-1048-aws-x86_64-with-glibc2.31\n- Python version: 3.11.9\n- PyTorch version: 2.4.1\n- CUDA device: NVIDIA H100 80GB HBM3\n- Transformers version: 4.45.0.dev0\n- Accelerate version: 0.34.2\n- Accelerate config: \n  - compute_environment: LOCAL_MACHINE\n  - distributed_type: DEEPSPEED\n  - mixed_precision: no\n  - use_cpu: False\n  - debug: False\n  - num_processes: 4\n  - machine_rank: 0\n  - num_machines: 1\n  - rdzv_backend: static\n  - same_network: True\n  - main_training_function: main\n  - enable_cpu_affinity: False\n  - deepspeed_config: {'gradient_accumulation_steps': 4, 'offload_optimizer_device': 'none', 'offload_param_device': 'none', 'zero3_init_flag': False, 'zero_stage': 2}\n  - downcast_bf16: no\n  - tpu_use_cluster: False\n  - tpu_use_sudo: False\n  - tpu_env: []\n- Datasets version: 3.0.0\n- HF Hub version: 0.24.7\n- TRL version: 0.12.0.dev0+acb4d70\n- bitsandbytes version: 0.41.1\n- DeepSpeed version: 0.15.1\n- Diffusers version: 0.30.3\n- Liger-Kernel version: 0.3.0\n- LLM-Blender version: 0.0.2\n- OpenAI version: 1.46.0\n- PEFT version: 0.12.0\n```\n\n----------------------------------------\n\nTITLE: Sharing Layers for Memory-Efficient Fine-tuning with DPOTrainer\nDESCRIPTION: This code snippet demonstrates how to share layers between the main model and the reference model to reduce memory consumption during fine-tuning with DPOTrainer. It uses the `create_reference_model` function to create a reference model with a specified number of shared layers. The required dependencies are datasets, transformers, and trl.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/customization.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import create_reference_model, DPOConfig, DPOTrainer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\nref_model = create_reference_model(model, num_shared_layers=6)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\ndataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train[:1%]\")\ntraining_args = DPOConfig(output_dir=\"Qwen2.5-0.5B-DPO\")\n\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=ref_model,\n    args=training_args,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Using a Loaded Tool\nDESCRIPTION: This snippet shows how to use a tool that has been loaded from the `transformers` library.  The tools accept a text query as input and return a string as output. This example uses the calculator tool to evaluate a fraction.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/text_environments.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncalc_tool(\"1/2\")\n>>> \"0.5\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Model with Reward Adapter and LoRA for PPO in Python\nDESCRIPTION: This snippet demonstrates how to initialize a causal language model with a value head using `AutoModelForCausalLMWithValueHead` from the `trl` library. It configures a LoRA adapter for PPO training and loads a reward adapter from the Hugging Face Hub. It also shows how to instantiate the PPOTrainer.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/multi_adapter_rl.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"huggyllama/llama-7b\"\nrm_adapter_id = \"trl-lib/llama-7b-hh-rm-adapter\"\n\n# PPO adapter\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\n    model_name,\n    peft_config=lora_config,\n    reward_adapter=rm_adapter_id,\n)\n\n...\ntrainer = PPOTrainer(\n    model=model,\n    ...\n)\n\n...\n```\n\n----------------------------------------\n\nTITLE: Loading DPO-trained LoRA Adapters\nDESCRIPTION: This Python code snippet demonstrates how to load the DPO-trained LoRA adapters using `AutoPeftModelForCausalLM` from the `peft` library. It loads the model in 4-bit precision for memory efficiency and specifies the data type as float16.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama_2/scripts/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom peft import AutoPeftModelForCausalLM\n\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    \"dpo/final_checkpoint\",\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n)\n\nmodel.generate(...)\n```\n\n----------------------------------------\n\nTITLE: Enabling CUDA Cache Optimization in DPOTrainer\nDESCRIPTION: This code snippet demonstrates how to enable CUDA cache optimization within the DPOTrainer by setting `optimize_cuda_cache=True` in the `DPOConfig`. This helps manage CUDA memory more effectively during training, especially with large models.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/customization.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntraining_args = DPOConfig(..., optimize_cuda_cache=True)\n```\n\n----------------------------------------\n\nTITLE: Using HfPairwiseJudge for Completion Comparison (Python)\nDESCRIPTION: This snippet demonstrates how to use the `HfPairwiseJudge` from the `trl` library to compare two completions for given prompts. It initializes the judge and then calls the `judge` method with lists of prompts and corresponding completions, producing a list of scores indicating the preferred completion for each prompt.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/judges.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import HfPairwiseJudge\n\njudge = HfPairwiseJudge()\njudge.judge(\n    prompts=[\"What is the capital of France?\", \"What is the biggest planet in the solar system?\"],\n    completions=[[\"Paris\", \"Lyon\"], [\"Saturn\", \"Jupiter\"]],\n)  # Outputs: [0, 1]\n```\n\n----------------------------------------\n\nTITLE: Setting up UDM Embedding Function\nDESCRIPTION: This code defines an embedding function used for Underlying Distribution Matching (UDM). It uses a pre-trained model to embed prompts and then calculates the mean of the last hidden state. It uses `AutoModel` and `AutoTokenizer` from `transformers` library.  The function can be customized for different embedding models.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/bco_trainer.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nembedding_model = AutoModel.from_pretrained(your_model_id)\nembedding_tokenizer = AutoTokenizer.from_pretrained(your_model_id)\n\n# customize this function depending on your embedding model\ndef embed_prompt(input_ids, attention_mask, model):\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    return outputs.last_hidden_state.mean(dim=1)\n\nembedding_model = Accelerator().prepare_model(self.embedding_model)\nembedding_func = partial(embed_prompt, model=embedding_model)\n```\n\n----------------------------------------\n\nTITLE: Custom Collate Function for VLMs\nDESCRIPTION: This code defines a custom collate function, `collate_fn`, to prepare batches of data for training a Vision-Language Model (VLM). It processes input texts, images (both single and multi-image examples), and generates labels with masked tokens (padding and image tokens) for loss computation.  The function utilizes a processor to tokenize the text and process images into tensors, making it suitable for training with image and text data.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/training_vlm_sft.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\n\n# For multi-image cases\ndef process_vision_info(messages: list[dict]) -> list[Image.Image]:\n    image_inputs = []\n    for msg in messages:\n        content = msg.get(\"content\", [])\n        if not isinstance(content, list):\n            content = [content]\n\n        for element in content:\n            if isinstance(element, dict) and (\"image\" in element or element.get(\"type\") == \"image\"):\n                if \"image\" in element:\n                    image = element[\"image\"]\n                else:\n                    image = element\n                if image is not None:\n                    image = Image.open(io.BytesIO(image[\"bytes\"]))\n                    image_inputs.append(image.convert(\"RGB\"))\n    return image_inputs\n\ndef collate_fn(examples):\n    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=False).strip() for example in examples]\n    if \"images\" in examples[0]:  # single-image\n        images = [\n            [img.convert(\"RGB\") for img in example[\"images\"]]\n            for example in examples\n        ]\n    else:  # multi-image\n        images = [process_vision_info(example[\"messages\"]) for example in examples]\n\n    # Tokenize the texts and process the images\n    batch = processor(\n        text=texts, images=images, return_tensors=\"pt\", padding=True\n    )  # Encode texts and images into tensors\n\n    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n    # Mask image tokens\n    image_token_id = [\n        processor.tokenizer.convert_tokens_to_ids(processor.tokenizer.special_tokens_map[\"boi_token\"])\n    ]\n    # Mask tokens for not being used in the loss computation\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n    labels[labels == image_token_id] = -100\n    labels[labels == 262144] = -100\n\n    batch[\"labels\"] = labels\n    return batch  # Return the prepared batch\n```\n\n----------------------------------------\n\nTITLE: Initializing BestOfNSampler with Model and Tokenizer\nDESCRIPTION: This code snippet demonstrates how to initialize an instance of the `BestOfNSampler` class with a model, tokenizer, and a reward pipeline. It imports necessary modules from `transformers` and `trl` and defines a function `queries_to_scores` that returns reward scores for input queries. The tokenizer's pad token is set to the end-of-sequence token.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/best_of_n.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline, AutoTokenizer\nfrom trl import AutoModelForCausalLMWithValueHead\nfrom trl.core import LengthSampler\nfrom trl.extras import BestOfNSampler\n\nref_model = AutoModelForCausalLMWithValueHead.from_pretrained(ref_model_name)\nreward_pipe = pipeline(\"sentiment-analysis\", model=reward_model, device=device)\ntokenizer = AutoTokenizer.from_pretrained(ref_model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\n# callable that takes a list of raw text and returns a list of corresponding reward scores\ndef queries_to_scores(list_of_strings):\n  return [output[\"score\"] for output in reward_pipe(list_of_strings)]\n\nbest_of_n = BestOfNSampler(model, tokenizer, queries_to_scores, length_sampler=output_length_sampler)\n```\n\n----------------------------------------\n\nTITLE: RLOO Advantage Calculation\nDESCRIPTION: This Python code snippet demonstrates a vectorized implementation of advantage calculation for the RLOO (Reward Learning with Online Optimization) algorithm. It calculates the advantage by subtracting the average reward of other responses from the current response's reward. It uses `torch` for tensor operations and includes assertions for verifying correctness.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/rloo_trainer.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef test_rloo_reward():\n    local_batch_size = 3\n    rloo_k = 4\n    rlhf_reward = torch.tensor([\n        1, 2, 3, # first rlhf reward for three prompts\n        2, 3, 4, # second rlhf reward for three prompts\n        5, 6, 7, # third rlhf reward for three prompts\n        8, 9, 10, # fourth rlhf reward for three prompts\n    ]).float() # here we have 3 prompts which have 4 completions each\n\n    baseline = (rlhf_reward.sum(0) - rlhf_reward) / (rloo_k - 1)\n    advantages = torch.zeros_like(rlhf_reward)\n    for i in range(0, len(advantages), local_batch_size):\n        other_response_rlhf_rewards = []\n        for j in range(0, len(advantages), local_batch_size):\n            if i != j:\n                other_response_rlhf_rewards.append(rlhf_reward[j : j + local_batch_size])\n        advantages[i : i + local_batch_size] = rlhf_reward[i : i + local_batch_size] - torch.stack(other_response_rlhf_rewards).mean(0)\n    \n    assert (1 - (2 + 5 + 8) / 3 - advantages[0].item()) < 1e-6  # First rlhf reward for the first prompt\n    assert (6 - (3 + 2 + 9) / 3 - advantages[7].item()) < 1e-6  # Third rlhf reward for the second prompt\n\n    # Vectorized implementation\n    rlhf_reward = rlhf_reward.reshape(rloo_k, local_batch_size)\n    baseline = (rlhf_reward.sum(0) - rlhf_reward) / (rloo_k - 1)\n    vec_advantages = rlhf_reward - baseline\n    torch.testing.assert_close(vec_advantages.flatten(), advantages)\n```\n\n----------------------------------------\n\nTITLE: Initializing PPO with Logging Configuration (Python)\nDESCRIPTION: This code snippet demonstrates how to initialize the PPO trainer with logging enabled using the `PPOConfig`. It allows users to specify either `wandb` or `tensorboard` as the reporting platform. The `project_kwargs` can be added to specify the logging directory when using tensorboard. No dependencies outside of the TRL library are explicitly listed, but it's assumed that `wandb` or `tensorboard` is installed.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/logging.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntraining_args = PPOConfig(..., report_to=\"wandb\")  # or \"tensorboard\"\n```\n\n----------------------------------------\n\nTITLE: Replacing Judge with Reward Model\nDESCRIPTION: This code snippet demonstrates how to replace the default `PairRMJudge` with a reward model during XPO training. It imports `AutoModelForSequenceClassification` from the `transformers` library, instantiates the reward model, and then passes it to the `XPOTrainer`. This ensures that the training process uses the specified reward model for evaluating the generated sequences. It is crucial that the SFT and Reward model use the same tokenizer and chat template.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/xpo_trainer.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForSequenceClassification\n\nreward_model = AutoModelForSequenceClassification.from_pretrained(\"trl-lib/Qwen2-0.5B-Reward\", num_labels=1)\n\ntrainer = XPOTrainer(\n      ...\n     reward_model=reward_model,\n  )\n```\n\n----------------------------------------\n\nTITLE: Initializing BCOTrainer with UDM\nDESCRIPTION: This snippet initializes the BCOTrainer with Underlying Distribution Matching (UDM) enabled.  It sets the `prompt_sample_size` and passes the `embedding_func` and `embedding_tokenizer` to the trainer. Requires `embedding_func` and `embedding_tokenizer` to be defined.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/bco_trainer.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntraining_args = BCOConfig(\n    beta=0.1,\n    prompt_sample_size=512,\n)\n\nbco_trainer = BCOTrainer(\n    model,\n    model_ref,\n    args=training_args,\n    train_dataset=train_dataset,\n    processing_class=tokenizer,\n    embedding_func=embedding_func,\n    embedding_tokenizer=self.embedding_tokenizer,\n)\n\nbco_trainer.train()\n```\n\n----------------------------------------\n\nTITLE: Load Pre-trained GPT2 Models\nDESCRIPTION: Loads the pre-trained GPT2 language model with a value head, and the tokenizer. A reference model is also loaded for KL-divergence calculation during PPO training. Requires the `transformers` and `trl` libraries.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\nref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\ntokenizer = AutoTokenizer.from_pretrained(config.model_name)\n\ntokenizer.pad_token = tokenizer.eos_token\n```\n\n----------------------------------------\n\nTITLE: Using Different Optimizers with DPOTrainer in TRL\nDESCRIPTION: This code snippet demonstrates how to use a different optimizer (SGD) instead of the default AdamW optimizer with the DPOTrainer in the TRL library. It initializes the optimizer and passes it to the DPOTrainer during initialization. The required dependencies are datasets, transformers, torch, and trl.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/customization.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom torch import optim\nfrom trl import DPOConfig, DPOTrainer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\ndataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\ntraining_args = DPOConfig(output_dir=\"Qwen2.5-0.5B-DPO\")\n\noptimizer = optim.SGD(model.parameters(), lr=training_args.learning_rate)\n\ntrainer = DPOTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n    optimizers=(optimizer, None),\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: TRL SFT CLI Example (Bash)\nDESCRIPTION: This command demonstrates the basic usage of the `trl sft` command for supervised fine-tuning. It specifies the model name, dataset name, and output directory.  The SFT CLI is based on the `trl/scripts/sft.py` script.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/clis.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrl sft --model_name_or_path facebook/opt-125m --dataset_name stanfordnlp/imdb --output_dir opt-sft-imdb\n```\n\n----------------------------------------\n\nTITLE: Custom Reward Function Example\nDESCRIPTION: Defines a simple custom reward function that rewards longer completions in the standard data format. It calculates the length of each completion and returns a list of float rewards.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/grpo_trainer.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef reward_func(completions, **kwargs):\n    \"\"\"Reward function that gives higher scores to longer completions.\"\"\"\n    return [float(len(completion)) for completion in completions]\n```\n\n----------------------------------------\n\nTITLE: Loading a Simple Calculator Tool in Python\nDESCRIPTION: This snippet demonstrates how to load a pre-existing calculator tool from Hugging Face Spaces using the `transformers` library.  It defines a function to use the tool and round the result to two decimal places. Requires the `transformers` library to be installed.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/learning_tools.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer, load_tool\ntool = load_tool(\"ybelkada/simple-calculator\")\ntool_fn = lambda text: str(round(float(tool(text)), 2))  # rounding to 2 decimal places\n```\n\n----------------------------------------\n\nTITLE: Initialize PPOTrainer\nDESCRIPTION: This snippet initializes the PPOTrainer with the configuration, models, tokenizer, dataset, and data collator.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nppo_trainer = PPOTrainer(\n    config, gpt2_model, gpt2_ref_model, gpt2_tokenizer, dataset, data_collator=collator\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Model with LoRA\nDESCRIPTION: This Python code snippet demonstrates how to load a pre-trained model in 8-bit precision, prepare it for k-bit training, and add LoRA (Low-Rank Adaptation) layers for parameter-efficient fine-tuning. It utilizes the `transformers`, `accelerate`, and `peft` libraries. The `LoraConfig` defines the LoRA parameters, and `get_peft_model` applies the LoRA configuration to the model.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/using_llama_models.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# load model in 8bit\nmodel = AutoModelForCausalLM.from_pretrained(\n        args.model_path,\n        load_in_8bit=True,\n        device_map={\"\": Accelerator().local_process_index}\n    )\nmodel = prepare_model_for_kbit_training(model)\n\n# add LoRA to model\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)\n```\n\n----------------------------------------\n\nTITLE: Creating a TextEnvironment\nDESCRIPTION: This snippet shows how to create a `TextEnvironment` object. It includes arguments such as the model, tokenizer, tools, reward function, prompt, and various settings for controlling the interaction between the model and the environment. It leverages `load_tool` to include a simple calculator.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/text_environments.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nprompt = \"\"\"\\\nWhat is 13-3?\n<request><SimpleCalculatorTool>13-3<call>10.0<response>\nResult=10<submit>\n\"\"\"\n\ndef reward_fn(result, answer):\n    \"\"\"Simplified reward function returning 1 if result matches answer and 0 otherwise.\"\"\"\n    result_parsed = result.split(\"=\")[1].split(\"<\")[0]\n    return int(result_parsed==answer)\n\ntext_env = TextEnvironemnt(\n    model=model, \n    tokenizer=tokenizer,\n    tools= {\"SimpleCalculatorTool\": load_tool(\"ybelkada/simple-calculator\")},\n    reward_fn=exact_match_reward,\n    prompt=prompt, \n    max_turns=1\n    max_tool_response=100\n    generation_kwargs={\"do_sample\": \"true\"}\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Generating Text with Transformers (Full Fine-tuning)\nDESCRIPTION: This snippet demonstrates how to load a fully fine-tuned language model (without PEFT) using transformers and generate text. It initializes the tokenizer and model, encodes the input text, generates output, and decodes the generated text. It requires the `transformers` library.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/use_model.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name_or_path = \"kashif/stack-llama-2\" #path/to/your/model/or/name/on/hub\ndevice = \"cpu\" # or \"cuda\" if you have a GPU\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\ninputs = tokenizer.encode(\"This movie was really\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n\n----------------------------------------\n\nTITLE: Merging PEFT Adapters into Base Model\nDESCRIPTION: This snippet shows how to merge the PEFT adapters into the base model, creating a single model checkpoint. It loads the base model and adapters, merges them using `merge_and_unload()`, and then saves the merged model. This requires the `peft` and `transformers` libraries.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/use_model.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name)\nmodel = PeftModel.from_pretrained(model, adapter_model_name)\n\nmodel = model.merge_and_unload()\nmodel.save_pretrained(\"merged_adapters\")\n```\n\n----------------------------------------\n\nTITLE: Logging Completions with Callback in Online DPO\nDESCRIPTION: This example shows how to use the `LogCompletionsCallback` to log sample model completions to Weights & Biases during training. This helps monitor and understand the model's behavior by visualizing its generated text.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/online_dpo_trainer.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrainer = OnlineDPOTrainer(..., eval_dataset=eval_dataset)\ncompletions_callback = LogCompletionsCallback(trainer, num_prompts=8)\ntrainer.add_callback(completions_callback)\n```\n\n----------------------------------------\n\nTITLE: Set Generation Parameters\nDESCRIPTION: Defines the generation settings for the GPT2 model. Includes disabling top-k and nucleus sampling, enabling sampling, and setting the pad token ID.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ngen_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n}\n```\n\n----------------------------------------\n\nTITLE: PPO Training Loop in Python\nDESCRIPTION: This code snippet outlines the PPO training loop, including generating responses from a policy, calculating rewards using sentiment analysis, and optimizing the policy.  It uses the Hugging Face Transformers library and PyTorch for model interaction and training. The loop iterates over epochs and batches, prepending control tokens to the queries.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfor epoch in range(2):\n    for batch in tqdm(ppo_trainer.dataloader):\n        (\n            logs,\n            game_data,\n        ) = (\n            dict(),\n            dict(),\n        )\n\n        #### prepend a random control token\n        task_list = choices(ctrl_str, k=config.batch_size)\n        game_data[\"query\"] = [t + q for t, q in zip(task_list, batch[\"query\"])]\n        query_tensors = [\n            torch.cat((ctrl_tokens[t], input_ids))\n            for t, input_ids in zip(task_list, batch[\"input_ids\"])\n        ]\n\n        #### get response from gpt2\n        response_tensors = []\n        for query in query_tensors:\n            response = ppo_trainer.generate(query, **generation_kwargs)\n            response_tensors.append(response.squeeze()[-txt_out_len:])\n        game_data[\"response\"] = [\n            gpt2_tokenizer.decode(r.squeeze()) for r in response_tensors\n        ]\n\n        #### sentiment analysis\n        texts = [q + r for q, r in zip(batch[\"query\"], game_data[\"response\"])]\n        logits = extract_pipe_output(sentiment_pipe(texts, **sentiment_pipe_kwargs))\n        rewards = pos_logit_to_reward(logits, task_list)\n\n        #### Run PPO training\n        t = time.time()\n        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n\n        for cs in ctrl_str:\n            key = \"env/reward_\" + cs.strip(\"[]\")\n            stats[key] = np.mean(\n                [r.cpu().numpy() for r, t in zip(rewards, task_list) if t == cs]\n            )\n        ppo_trainer.log_stats(stats, game_data, rewards)\n```\n\n----------------------------------------\n\nTITLE: SFT Configuration with Truncation\nDESCRIPTION: This snippet demonstrates how to configure `SFTConfig` from the `trl` library to truncate the input sequence to a specified `max_length`. This parameter controls the maximum length of the input sequence, reducing memory footprint by limiting the size of the input data.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reducing_memory_usage.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import SFTConfig\n\ntraining_args = SFTConfig(..., max_length=...)\n```\n\n----------------------------------------\n\nTITLE: Launching Training with Accelerate in Bash\nDESCRIPTION: This bash script shows how to configure and launch a training script using `accelerate`, a library for distributed training. It first configures the training environment using `accelerate config`, then launches the `ppo.py` script with the `--use_peft` flag to enable PEFT during training.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/peft_integration.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config # will prompt you to define the training configuration\naccelerate launch examples/scripts/ppo.py --use_peft # launch`es training\n```\n\n----------------------------------------\n\nTITLE: Score Responses\nDESCRIPTION: Scores the generated responses using the reward model. It iterates through the responses generated by each model and calculates a sentiment score for each response. The scores are stored in lists. For the best-of-n sampling approach, it calculates a score for each response and stores them in a nested list.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/best_of_n.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nscores_ref = [\n    output[0][\"score\"] for output in reward_pipe(response_tensors_ref, **sent_kwargs)\n]\nscores = [output[0][\"score\"] for output in reward_pipe(response_tensors, **sent_kwargs)]\nscores_best_of = []\nfor i, response in enumerate(response_tensors_best_of):\n    # base_score = scores_ref[i]\n    scores_best_of.append(\n        torch.tensor(\n            [output[0][\"score\"] for output in reward_pipe(response, **sent_kwargs)]\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Launching Training with DeepSpeed and Accelerate\nDESCRIPTION: This command launches a training script using Accelerate and DeepSpeed, enabling efficient sharding of model states for large-scale training. It requires specifying a DeepSpeed configuration file.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/example_overview.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\naccelerate launch --config_file=examples/accelerate_configs/deepspeed_zero{1,2,3}.yaml --num_processes {NUM_GPUS} path_to_script.py --all_arguments_of_the_script\n```\n\n----------------------------------------\n\nTITLE: DPO Configuration with Padding-Free Batching\nDESCRIPTION: This snippet enables padding-free batching in the `DPOConfig` from the `trl` library by setting `padding_free=True`.  It also initializes the model with `flash_attention_2` to avoid batch contamination issues. Padding-free batching avoids padding by flattening the batch into a single sequence, which optimizes memory usage.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reducing_memory_usage.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import DPOConfig\n\ntraining_args = DPOConfig(..., padding_free=True, model_init_kwargs={\"attn_implementation\": \"flash_attention_2\"})\n```\n\n----------------------------------------\n\nTITLE: Configuring EOS Token Penalty in Online DPO\nDESCRIPTION: This snippet demonstrates how to configure a penalty for not generating an end-of-sequence (EOS) token within the maximum allowed length during training. This encourages the model to produce completions of appropriate length and to signal completion explicitly.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/online_dpo_trainer.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntraining_args = OnlineDPOConfig(..., max_new_tokens=128, missing_eos_penalty=1.0)\n```\n\n----------------------------------------\n\nTITLE: Running CPO Example Script with Qwen2 and UltraFeedback Dataset\nDESCRIPTION: This bash command runs the CPO example script (`examples/scripts/cpo.py`) with specific configurations for the Qwen2 0.5B model and UltraFeedback dataset. It sets parameters like the model path, dataset name, number of training epochs, logging frequency, and output directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/cpo_trainer.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch examples/scripts/cpo.py \\\n    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n    --dataset_name trl-lib/ultrafeedback_binarized \\\n    --num_train_epochs 1 \\\n    --logging_steps 25 \\\n    --output_dir Qwen2-0.5B-CPO\n```\n\n----------------------------------------\n\nTITLE: PPO Training Loop\nDESCRIPTION: Implements the PPO training loop to optimize the GPT2 model. It generates responses, computes sentiment scores using BERT, and updates the policy using the `PPOTrainer`. Requires the `trl` library and a pre-trained language model and sentiment classifier.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\noutput_min_length = 4\noutput_max_length = 16\noutput_length_sampler = LengthSampler(output_min_length, output_max_length)\n\n\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n}\n\n\nfor epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):\n    query_tensors = batch[\"input_ids\"]\n\n    #### Get response from gpt2\n    response_tensors = []\n    for query in query_tensors:\n        gen_len = output_length_sampler()\n        generation_kwargs[\"max_new_tokens\"] = gen_len\n        query_response = ppo_trainer.generate(query, **generation_kwargs).squeeze()\n        response_len = len(query_response) - len(query)\n        response_tensors.append(query_response[-response_len:])\n    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n\n    #### Compute sentiment score\n    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n    positive_scores = [\n        item[\"score\"]\n        for output in pipe_outputs\n        for item in output\n        if item[\"label\"] == \"POSITIVE\"\n    ]\n    rewards = [torch.tensor(score) for score in positive_scores]\n\n    #### Run PPO step\n    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n    ppo_trainer.log_stats(stats, batch, rewards)\n```\n\n----------------------------------------\n\nTITLE: Using Tensors as Input in IterativeSFTTrainer in Python\nDESCRIPTION: This snippet shows how to use a dictionary of tensors (input_ids and attention_mask) as input to the step function of the IterativeSFTTrainer.  This assumes that `input_ids` and `attention_mask` tensors are already defined. These tensors are used to train the language model.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/iterative_sft_trainer.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninputs = {\n    \"input_ids\": input_ids,\n    \"attention_mask\": attention_mask\n}\n\ntrainer.step(**inputs)\n```\n\n----------------------------------------\n\nTITLE: Running XPO Training Script\nDESCRIPTION: This bash command demonstrates how to run the XPO training script (`xpo.py`) with specified configurations, including the model name, dataset name, learning rate, logging steps, output directory, warmup ratio, and whether to push the model to the Hugging Face Hub. The script trains the Qwen2.5 0.5B model on the UltraFeedback dataset using the PairRM judge.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/xpo_trainer.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython examples/scripts/xpo.py \\\n    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \\\n    --judge pair_rm \\\n    --dataset_name trl-lib/ultrafeedback-prompt \\\n    --learning_rate 5.0e-7 \\\n    --logging_steps 25 \\\n    --output_dir Qwen2.5-0.5B-XPO-PairRM \\\n    --warmup_ratio 0.1 \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Merging PEFT Adapter\nDESCRIPTION: This command uses the `merge_peft_adapter.py` script to merge the trained LoRA adapter into the base Llama 2 model. `--base_model_name` specifies the base model, `--adapter_model_name` specifies the adapter's location, and `--output_name` determines the name of the merged model, which will also be pushed to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama_2/scripts/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython examples/research_projects/stack_llama/scripts/merge_peft_adapter.py --base_model_name=\"meta-llama/Llama-2-7b-hf\" --adapter_model_name=\"dpo/final_checkpoint/\" --output_name=\"stack-llama-2\"\n```\n\n----------------------------------------\n\nTITLE: Example ORPO Training Command\nDESCRIPTION: This command shows how to run the `orpo.py` script located in the `examples/scripts` directory using `accelerate launch`.  It configures the training process by specifying the model name, dataset, number of training epochs, logging frequency, and output directory. It uses the Qwen2-0.5B-Instruct model and the ultrafeedback_binarized dataset.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/orpo_trainer.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\naccelerate launch examples/scripts/orpo.py \\\n    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n    --dataset_name trl-lib/ultrafeedback_binarized \\\n    --num_train_epochs 1 \\\n    --logging_steps 25 \\\n    --output_dir Qwen2-0.5B-ORPO\n```\n\n----------------------------------------\n\nTITLE: Build Training Dataset\nDESCRIPTION: Builds the dataset for training the GPT2 model using the `load_dataset` function from the `datasets` library. It loads the IMDB dataset, filters reviews based on length, and tokenizes the text data. Requires transformers and datasets libraries.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef build_dataset(\n    config,\n    dataset_name=\"stanfordnlp/imdb\",\n    input_min_text_length=2,\n    input_max_text_length=8,\n):\n    \"\"\"\n    Build dataset for training. This builds the dataset from `load_dataset`, one should\n    customize this function to train the model on its own dataset.\n\n    Args:\n        dataset_name (`str`):\n            The name of the dataset to be loaded.\n\n    Returns:\n        dataloader (`torch.utils.data.DataLoader`):\n            The dataloader for the dataset.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    # load imdb with datasets\n    ds = load_dataset(dataset_name, split=\"train\")\n    ds = ds.rename_columns({\"text\": \"review\"})\n    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n\n    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n\n    def tokenize(sample):\n        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n        return sample\n\n    ds = ds.map(tokenize, batched=False)\n    ds.set_format(type=\"torch\")\n    return ds\n```\n\n----------------------------------------\n\nTITLE: Example Online DPO Training Script Execution\nDESCRIPTION: This command provides an example of how to execute the `dpo_online.py` script with specific arguments to train a model using online DPO. It specifies parameters such as the model name, judge type, dataset, learning rate, and output directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/online_dpo_trainer.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython examples/scripts/dpo_online.py \\\n    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \\\n    --judge pair_rm \\\n    --dataset_name trl-lib/ultrafeedback-prompt \\\n    --learning_rate 5.0e-7 \\\n    --logging_steps 25 \\\n    --output_dir Qwen2.5-0.5B-Online-DPO-PairRM \\\n    --warmup_ratio 0.1 \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Enable Liger-Kernel in SFTConfig\nDESCRIPTION: This snippet shows how to enable Liger-Kernel within the `SFTConfig` by setting the `use_liger_kernel` parameter to `True`. No other changes are needed to integrate Liger-Kernel into the `SFTTrainer` workflow.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ntraining_args = SFTConfig(\n  use_liger_kernel=True\n)\n```\n\n----------------------------------------\n\nTITLE: RLOO Training Command with Accelerate\nDESCRIPTION: This command launches an RLOO training run using the `accelerate` library, utilizing a DeepSpeed configuration for efficient training. It specifies dataset, model paths, training parameters, and hyperparameters for the RLOO training process. It sets up training parameters such as batch size, learning rate, number of epochs, and KL coefficient.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/rloo_trainer.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --config_file examples/accelerate_configs/deepspeed_zero2.yaml \\\n    --output_dir models/minimal/rloo_tldr \\\n    --dataset_name trl-internal-testing/tldr-preference-sft-trl-style \\\n    --dataset_test_split validation \\\n    --num_ppo_epochs 2 \\\n    --num_mini_batches 2 \\\n    --learning_rate 3e-6 \\\n    --per_device_train_batch_size 16 \\\n    --gradient_accumulation_steps 16 \\\n    --total_episodes 1000000 \\\n    --model_name_or_path EleutherAI/pythia-1b-deduped \\\n    --sft_model_path cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr \\\n    --reward_model_path cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr \\\n    --local_rollout_forward_batch_size 16 \\\n    --missing_eos_penalty 1.0 \\\n    --stop_token eos \\\n    --kl_coef 0.03\n```\n\n----------------------------------------\n\nTITLE: TRL SFT CLI Usage with Config (Bash)\nDESCRIPTION: This command demonstrates how to use the `trl sft` command with a YAML configuration file. It also shows how to override arguments from the config file by explicitly passing them to the CLI. The `output_dir` and `lr_scheduler_type` arguments are passed directly to the CLI, overriding the values from the config.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/clis.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntrl sft --config examples/cli_configs/example_config.yaml --output_dir test-trl-cli --lr_scheduler_type cosine_with_restarts\n```\n\n----------------------------------------\n\nTITLE: Replacing Judge with Reward Model (Diff)\nDESCRIPTION: This code snippet demonstrates how to replace a judge with a reward model in the Nash-MD training process. It showcases the necessary imports and modifications to the trainer initialization.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/nash_md_trainer.md#_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n- from trl import PairRMJudge\n+ from transformers import AutoModelForSequenceClassification\n\n- judge = PairRMJudge()\n+ reward_model = AutoModelForSequenceClassification.from_pretrained(\"trl-lib/Qwen2-0.5B-Reward\", num_labels=1)\n\n  trainer = NashMDTrainer(\n      ...\n-     judge=judge,\n+     reward_model=reward_model,\n  )\n```\n\n----------------------------------------\n\nTITLE: Install DeepSpeed\nDESCRIPTION: Installs the DeepSpeed library, which provides advanced training optimization techniques such as optimizer state partitioning and gradient partitioning. This is a prerequisite for using DeepSpeed with TRL.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/deepspeed_integration.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install deepspeed\n```\n\n----------------------------------------\n\nTITLE: Training LoRA Adapters with Base 8-bit Models\nDESCRIPTION: This snippet shows how to train LoRA adapters with a base 8-bit model using the `SFTTrainer`. The model is first loaded outside the Trainer with `load_in_8bit=True` and then passed to the Trainer along with a `PeftConfig`.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n...\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"EleutherAI/gpt-neo-125m\",\n    load_in_8bit=True,\n    device_map=\"auto\",\n)\n\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    args=SFTConfig(),\n    peft_config=peft_config,\n)\n\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: DPO Training\nDESCRIPTION: This command launches the DPO training script using `accelerate`. It trains the model saved from the SFT step using the DPO objective. The `--model_name_or_path` parameter specifies the location of the SFT-trained model, and `--output_dir` defines where the DPO-trained model is saved.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama_2/scripts/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch examples/research_projects/stack_llama_2/scripts/dpo_llama2.py \\\n    --model_name_or_path=\"sft/final_checkpoint\" \\\n    --output_dir=\"dpo\"\n```\n\n----------------------------------------\n\nTITLE: Supervised Fine-tuning Command (StackLLaMa)\nDESCRIPTION: This command performs supervised fine-tuning of a base LLaMA 7B model to create llama-7b-se. It uses `torchrun` for distributed training and specifies parameters such as model path, learning rate, maximum steps, and output directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ntorchrun --nnodes 1  --nproc_per_node 8 examples/research_projects/stack_llama/scripts/supervised_finetuning.py --model_path=<LLAMA_MODEL_PATH> --streaming --learning_rate 1e-5 --max_steps 5000 --output_dir ./llama-se\n```\n\n----------------------------------------\n\nTITLE: Running RLOO Training Script\nDESCRIPTION: This bash command executes the RLOO training script. It specifies the dataset, learning rate, output directory, batch size, gradient accumulation steps, total episodes, model name, reward model path, and a missing EOS penalty.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/rloo_trainer.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython examples/scripts/rloo/rloo.py \\\n    --dataset_name trl-internal-testing/descriptiveness-sentiment-trl-style \\\n    --dataset_train_split descriptiveness \\\n    --learning_rate 3e-6 \\\n    --output_dir models/minimal/rloo \\\n    --per_device_train_batch_size 64 \\\n    --gradient_accumulation_steps 1 \\\n    --total_episodes 10000 \\\n    --model_name_or_path EleutherAI/pythia-14m \\\n    --reward_model_path EleutherAI/pythia-14m \\\n    --missing_eos_penalty 1.0\n```\n\n----------------------------------------\n\nTITLE: Installing Flash Attention 2\nDESCRIPTION: This bash command installs the `flash-attn` package, which is necessary for using Flash Attention 2. It ensures that you have the required dependencies for the `attn_implementation=\"flash_attention_2\"` setting.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npip install -U flash-attn\n```\n\n----------------------------------------\n\nTITLE: Launching Distributed Training with Accelerate\nDESCRIPTION: This command launches the training script in a distributed environment using the configuration previously created with `accelerate config`.  It distributes the workload across all available GPUs or nodes according to the configuration.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/distributing_training.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train.py\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Llama-2 with SFTTrainer in Bash\nDESCRIPTION: This bash script demonstrates how to fine-tune the Llama-2 7b model using the `SFTTrainer` script in the TRL library.  It specifies various parameters such as the output directory, model name, dataset, quantization settings (4-bit), PEFT usage, batch size, and gradient accumulation steps.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/peft_integration.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython trl/scripts/sft.py --output_dir sft_openassistant-guanaco  --model_name meta-llama/Llama-2-7b-hf --dataset_name timdettmers/openassistant-guanaco --load_in_4bit --use_peft --per_device_train_batch_size 4 --gradient_accumulation_steps 2\n```\n\n----------------------------------------\n\nTITLE: Training the BCOTrainer\nDESCRIPTION: This snippet shows how to start the training process after the BCOTrainer has been initialized.  It calls the `train()` method on the trainer object to begin fine-tuning the model.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/bco_trainer.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbco_trainer.train()\n```\n\n----------------------------------------\n\nTITLE: Loading 8-bit Base Model with Reward Adapter in Python\nDESCRIPTION: This snippet illustrates how to load a base model in 8-bit mode using `bitsandbytes` for memory-efficient fine-tuning. It passes the `load_in_8bit=True` argument to `AutoModelForCausalLMWithValueHead.from_pretrained` while loading the reward adapter and configuring LoRA for PPO.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/multi_adapter_rl.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"llama-7b\"\nrm_adapter_id = \"trl-lib/llama-7b-hh-rm-adapter\"\n\n# PPO adapter\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\n    model_name,\n    peft_config=lora_config,\n    reward_adapter=rm_adapter_id,\n    load_in_8bit=True,\n)\n\n...\ntrainer = PPOTrainer(\n    model=model,\n    ...\n)\n...\n```\n\n----------------------------------------\n\nTITLE: Configure PPO Training\nDESCRIPTION: Configures the PPO training process with parameters such as model name, learning rate, and logging options using the `PPOConfig` class from the `trl` library.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconfig = PPOConfig(\n    model_name=\"lvwerra/gpt2-imdb\",\n    learning_rate=1.41e-5,\n    log_with=\"wandb\",\n)\n\nsent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\": 16}\n```\n\n----------------------------------------\n\nTITLE: Configure PPO training\nDESCRIPTION: This snippet defines the PPO configuration, including the model name, training steps, learning rate, and logging settings. It also sets the input and output lengths for the text generation and a random seed for reproducibility.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsentiment_pipe_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\"}\n\nconfig = PPOConfig(\n    model_name=\"lvwerra/gpt2-imdb\",\n    steps=51200,\n    learning_rate=1.41e-5,\n    remove_unused_columns=False,\n    log_with=\"wandb\",\n)\n\ntxt_in_len = 5\ntxt_out_len = 20\nseed = 1\n```\n\n----------------------------------------\n\nTITLE: Executing the CPO Training Script using Accelerate\nDESCRIPTION: This bash command executes the CPO training script (`train_cpo.py`) using `accelerate launch`. This command is used to distribute the training process across multiple GPUs or machines for faster training.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/cpo_trainer.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_cpo.py\n```\n\n----------------------------------------\n\nTITLE: Launch KTO training script\nDESCRIPTION: This command executes the KTO training script using the accelerate launcher for distributed training. It assumes that the `train_kto.py` script is in the current directory and configures the training to run on multiple GPUs or TPUs based on the accelerate configuration. Requires accelerate to be installed.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/kto_trainer.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_kto.py\n```\n\n----------------------------------------\n\nTITLE: Define SFT Training Arguments\nDESCRIPTION: This code snippet defines the training arguments for Supervised Fine-Tuning (SFT) using the `SFTConfig` class from the `trl` library. It configures parameters such as the output directory, number of training epochs, batch size, gradient accumulation steps, optimizer, learning rate, and other training-related settings. The configuration enables features such as gradient checkpointing, bfloat16 precision, and automatic push to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/training_vlm_sft.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import SFTConfig\n\ntraining_args = SFTConfig(\n    output_dir=\"gemma-3-4b-it-trl-sft-llava-instruct-mix-vsft\",     # Directory to save the model and push to the Hub. Use a specific repository id (e.g., gemma-3-4b-it-trl-sft-MMIU-Benchmark for multi-image datasets).\n    num_train_epochs=1,                                             # Set the number of epochs to train the model.\n    per_device_train_batch_size=8,                                  # Batch size for each device (e.g., GPU) during training. multi-image -> per_device_train_batch_size=1\n    gradient_accumulation_steps=4,                                  # Number of steps before performing a backward/update pass to accumulate gradients. multi-image -> gradient_accumulation_steps=1\n    gradient_checkpointing=True,                                    # Enable gradient checkpointing to reduce memory usage during training.\n    optim=\"adamw_torch_fused\",                                      # Use the fused AdamW optimizer for better performance.\n    logging_steps=10,                                               # Frequency of logging training progress (log every 10 steps).\n    save_strategy=\"epoch\",                                          # Save checkpoints at the end of each epoch.\n    learning_rate=2e-05,                                            # Learning rate for training.\n    bf16=True,                                                      # Enable bfloat16 precision for training to save memory and speed up computations.\n    push_to_hub=True,                                               # Automatically push the fine-tuned model to Hugging Face Hub after training.\n    report_to=\"tensorboard\",                                        # Automatically report metrics to tensorboard.\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},         # Set gradient checkpointing to non-reentrant to avoid issues.\n    dataset_kwargs={\"skip_prepare_dataset\": True},                  # Skip dataset preparation to handle preprocessing manually.\n    remove_unused_columns=False,                                    # Ensure unused columns are not removed in the collator (important for batch processing).\n)\n```\n\n----------------------------------------\n\nTITLE: Accelerate fine-tuning with Unsloth\nDESCRIPTION: This snippet demonstrates how to use the `unsloth` library to accelerate fine-tuning of a language model with `SFTTrainer`. It involves loading a `FastLanguageModel` instead of `AutoModelForCausalLM`, patching the model with LoRA weights using `FastLanguageModel.get_peft_model`, and then training the model using the `SFTTrainer`.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom trl import SFTConfig, SFTTrainer\nfrom unsloth import FastLanguageModel\n\nmax_length = 2048 # Supports automatic RoPE Scaling, so choose any number\n\n# Load model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/mistral-7b\",\n    max_seq_length=max_length,\n    dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n    load_in_4bit=True,  # Use 4bit quantization to reduce memory usage. Can be False\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\n# Do model patching and add fast LoRA weights\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha=16,\n    lora_dropout=0,  # Dropout = 0 is currently optimized\n    bias=\"none\",  # Bias = \"none\" is currently optimized\n    use_gradient_checkpointing=True,\n    random_state=3407,\n)\n\ntraining_args = SFTConfig(output_dir=\"./output\", max_length=max_length)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Accelerate Configuration\nDESCRIPTION: This command initializes the accelerate configuration, prompting the user to define the training configuration. This is a prerequisite for running the examples.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/example_overview.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\naccelerate config # will prompt you to define the training configuration\n```\n\n----------------------------------------\n\nTITLE: Using the finetuned Stable Diffusion model\nDESCRIPTION: This Python snippet demonstrates how to load and use a finetuned Stable Diffusion model using the `DefaultDDPOStableDiffusionPipeline`. It loads the model, moves it to the GPU, and generates images from prompts.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/ddpo_trainer.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nfrom trl import DefaultDDPOStableDiffusionPipeline\n\npipeline = DefaultDDPOStableDiffusionPipeline(\"metric-space/ddpo-finetuned-sd-model\")\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# memory optimization\npipeline.vae.to(device, torch.float16)\npipeline.text_encoder.to(device, torch.float16)\npipeline.unet.to(device, torch.float16)\n\nprompts = [\"squirrel\", \"crab\", \"starfish\", \"whale\",\"sponge\", \"plankton\"]\nresults = pipeline(prompts)\n\nfor prompt, image in zip(prompts,results.images):\n    image.save(f\"{prompt}.png\")\n\n```\n\n----------------------------------------\n\nTITLE: Specifying Adapter Name for Reward Computation in Python\nDESCRIPTION: This snippet demonstrates how to control the adapter name used when computing the reward score. It passes the `ppo_adapter_name` argument to the `compute_reward_score` method, allowing the selection of a specific policy adapter.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/multi_adapter_rl.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nadapter_name_policy_1 = \"policy_1\"\nrewards = trainer.model.compute_reward_score(**inputs, ppo_adapter_name=adapter_name_policy_1)\n...\n```\n\n----------------------------------------\n\nTITLE: Training with PPO\nDESCRIPTION: This snippet demonstrates how to use the results from the `TextEnvironment` to train a model using Proximal Policy Optimization (PPO). It shows how to forward the queries, responses, rewards, and masks returned by the `run` method to the `step` method of a `ppo_trainer`.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/text_environments.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ntrain_stats = ppo_trainer.step(queries, responses, rewards, masks)\n```\n\n----------------------------------------\n\nTITLE: Running 6.9B Online DPO Experiment\nDESCRIPTION: This shell command launches an online DPO experiment using the Pythia 6.9B model. It utilizes the `accelerate` tool with a DeepSpeed configuration file and sets various hyperparameters, including learning rate, batch size, and number of training epochs. The experiment includes bf16 mixed precision, gradient checkpointing, and is configured to push the results to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/online_dpo_trainer.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\naccelerate launch --config_file examples/accelerate_configs/deepspeed_zero2.yaml \\\n    examples/scripts/dpo_online.py \\\n    --model_name_or_path trl-lib/pythia-6.9b-deduped-tldr-sft  \\\n    --reward_model_path trl-lib/pythia-6.9b-deduped-tldr-rm \\\n    --dataset_name trl-lib/tldr \\\n    --learning_rate 5.0e-7 \\\n    --output_dir pythia-6.9b-deduped-tldr-online-dpo \\\n    --beta 0.1 \\\n    --per_device_train_batch_size 4 \\\n    --gradient_accumulation_steps 4 \\\n    --num_train_epochs 3 \\\n    --max_new_tokens 53 \\\n    --warmup_ratio 0.1 \\\n    --missing_eos_penalty 1.0 \\\n    --bf16 \\\n    --gradient_checkpointing \\\n    --logging_steps 20 \\\n    --save_steps 0.1 \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Prepare Batch Data\nDESCRIPTION: Prepares a batch of data for generating responses. Samples a batch of data from the dataset as a Pandas DataFrame, extracts the queries and input IDs, and converts them to lists. Initializes empty lists to store the generated responses from the reference model, fine-tuned model, and best-of-n sampled model.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/best_of_n.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\noutput_min_length = 4\noutput_max_length = 16\noutput_length_sampler = LengthSampler(output_min_length, output_max_length)\n\n#### get a batch from the dataset\nbs = 16\noutput_data = dict()\ndataset.set_format(\"pandas\")\ndf_batch = dataset[:].sample(bs)\noutput_data[\"query\"] = df_batch[\"query\"].tolist()\nquery_tensors = df_batch[\"input_ids\"].tolist()\n\n# :: [Resp]\nresponse_tensors_ref, response_tensors = [], []\n# :: [[Resp]]\nresponse_tensors_best_of = []\n```\n\n----------------------------------------\n\nTITLE: Enabling NEFTune in SFTTrainer\nDESCRIPTION: This code snippet shows how to enable NEFTune in the `SFTTrainer` by setting the `neftune_noise_alpha` parameter in the `SFTConfig`. NEFTune adds noise to the embedding vectors during training to boost performance.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\n\ndataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n\ntraining_args = SFTConfig(\n    neftune_noise_alpha=5,\n)\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    args=training_args,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Preparing Multi-Image Dataset (python)\nDESCRIPTION: Prepares the multi-image dataset by downloading and extracting ZIP files containing the images, then mapping the `format_data` function across the dataset.  This function utilizes `hf_hub_download` to fetch the zip files and requires `zipfile` and `os` libraries.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/training_vlm_sft.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# For multi-image example\ndef prepare_dataset(dataset: DatasetDict, dataset_name: str, dataset_train_split: str) -> DatasetDict:\n    all_files = list_repo_files(dataset_name, repo_type=\"dataset\")\n    zip_files = [f for f in all_files if f.endswith(\".zip\")]\n\n    for zip_filename in zip_files:\n        zip_path = hf_hub_download(repo_id=dataset_name, filename=zip_filename, repo_type=\"dataset\")\n        extract_folder = zip_filename.replace(\".zip\", \"\")\n        os.makedirs(extract_folder, exist_ok=True)\n\n        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n            zip_ref.extractall(extract_folder)\n\n    dataset = dataset.map(format_data, batched=True, batch_size=4, num_proc=16)\n    return dataset\n\ndataset = prepare_dataset(dataset, dataset_name, dataset_train_split)\n```\n\n----------------------------------------\n\nTITLE: Installing TRL with Quantization Support\nDESCRIPTION: This command installs or upgrades the TRL library with quantization support, enabling training with 4-bit or 8-bit models.  This is required before using quantization features.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/example_overview.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade trl[quantization]\n```\n\n----------------------------------------\n\nTITLE: Executing Online DPO Training Script in Bash\nDESCRIPTION: This command executes the online DPO training script using the `accelerate` launcher, distributing the training process across available GPUs. This allows for faster training times by leveraging parallel processing capabilities.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/online_dpo_trainer.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_online_dpo.py\n```\n\n----------------------------------------\n\nTITLE: Saving Model and Tokenizer in Python\nDESCRIPTION: This code saves the trained GPT-2 model and tokenizer to disk. It uses the `save_pretrained` method from the Hugging Face Transformers library.  The model and tokenizer are saved to the specified directory, 'gpt2-imdb-ctrl'.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ngpt2_model.save_pretrained(\"gpt2-imdb-ctrl\")\ngpt2_tokenizer.save_pretrained(\"gpt2-imdb-ctrl\")\n```\n\n----------------------------------------\n\nTITLE: Running 1B Online DPO Experiment\nDESCRIPTION: This shell command launches an online DPO experiment using the Pythia 1B model. It utilizes the `accelerate` tool with a specified configuration file and sets various hyperparameters, including learning rate, batch size, and number of training epochs. The experiment is configured to push the results to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/online_dpo_trainer.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\naccelerate launch --config_file examples/accelerate_configs/multi_gpu.yaml \\\n    examples/scripts/dpo_online.py \\\n    --model_name_or_path trl-lib/pythia-1b-deduped-tldr-sft  \\\n    --reward_model_path trl-lib/pythia-1b-deduped-tldr-rm \\\n    --dataset_name trl-lib/tldr \\\n    --learning_rate 5.0e-7 \\\n    --output_dir pythia-1b-deduped-tldr-online-dpo \\\n    --beta 0.1 \\\n    --per_device_train_batch_size 8 \\\n    --gradient_accumulation_steps 2 \\\n    --num_train_epochs 3 \\\n    --max_new_tokens 53 \\\n    --warmup_ratio 0.1 \\\n    --missing_eos_penalty 1.0 \\\n    --logging_steps 20 \\\n    --save_steps 0.1 \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: TRL DPO CLI Example (Bash)\nDESCRIPTION: This command demonstrates the basic usage of the `trl dpo` command for Direct Policy Optimization. It specifies the model name, output directory, and dataset name.  The dataset should be in the TRL format with `prompt`, `chosen`, and `rejected` columns. The DPO CLI is based on the `trl/scripts/dpo.py` script.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/clis.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntrl dpo --model_name_or_path facebook/opt-125m --output_dir trl-hh-rlhf --dataset_name trl-internal-testing/hh-rlhf-helpful-base-trl-style\n```\n\n----------------------------------------\n\nTITLE: Loading Predefined Tools from Transformers\nDESCRIPTION: This snippet demonstrates how to load pre-defined tools from the `transformers` library. These tools, such as a calculator, a Python interpreter, and a Wikipedia search index, can be loaded from the Hugging Face Hub or a local folder and used within a text environment.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/text_environments.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom transformers import load_tool\n\n# simple calculator tool that runs +-/* operations\ncalc_tool = load_tool(\"ybelkada/simple-calculator\")\n\n# python interpreter that executes program and returns outputs\npy_tool = load_tool(\"lvwerra/python-interpreter\")\n\n# wikipedia search index that returns best search match\nwiki_tool = load_tool(\"vwxyzjn/pyserini-wikipedia-kilt-doc\")\n```\n\n----------------------------------------\n\nTITLE: Create Dataset and Data Collator\nDESCRIPTION: Creates the training dataset using the `build_dataset` function and defines a data collator function to prepare data batches for training. The collator converts lists of dictionaries into a dictionary of lists.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset = build_dataset(config)\n\n\ndef collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])\n```\n\n----------------------------------------\n\nTITLE: Executing PRM training using example script\nDESCRIPTION: This bash command shows how to execute the `prm.py` training script using the `accelerate` library, for a given dataset and model. It also specifies number of epochs, the logging steps and the output directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/prm_trainer.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch examples/scripts/prm.py \\\n    --model_name_or_path Qwen/Qwen2-0.5B \\\n    --dataset_name trl-lib/math_shepherd \\\n    --num_train_epochs 1 \\\n    --logging_steps 25 \\\n    --output_dir Qwen2-0.5B-Reward-Math-Sheperd\n```\n\n----------------------------------------\n\nTITLE: Using the Custom Pairwise Judge (Python)\nDESCRIPTION: This code snippet shows how to use the custom `PrefersShorterJudge` defined previously. It initializes the judge and then calls the `judge` method with lists of prompts and corresponding completions.  The output shows that the judge prefers the shorter completion for each prompt.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/judges.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\njudge = PrefersShorterJudge()\njudge.judge(\n    prompts=[\"What is the capital of France?\", \"What is the biggest planet in the solar system?\"],\n    completions=[\n        [\"Paris\", \"The capital of France is Paris.\"],\n        [\"Jupiter is the biggest planet in the solar system.\", \"Jupiter\"],\n    ],\n)  # Outputs: [0, 1]\n```\n\n----------------------------------------\n\nTITLE: Start vLLM Server\nDESCRIPTION: This command starts a vLLM server using the TRL CLI.  The `--model <model_name>` argument specifies the model to be loaded by the vLLM server for fast generation.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/speeding_up_training.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntrl vllm-serve --model <model_name>\n```\n\n----------------------------------------\n\nTITLE: Initializing BCOTrainer with BCOConfig (basic)\nDESCRIPTION: This snippet initializes the BCOTrainer with a model, a reference model, training arguments defined by BCOConfig, a training dataset, and a tokenizer.  The `beta` parameter in the `BCOConfig` controls the hyperparameter of the implicit reward.  The models should have the same architecture.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/bco_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntraining_args = BCOConfig(\n    beta=0.1,\n)\n\nbco_trainer = BCOTrainer(\n    model,\n    model_ref,\n    args=training_args,\n    train_dataset=train_dataset,\n    processing_class=tokenizer,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This command installs the required Python packages specified in the `requirements.txt` file. It uses `pip` to manage the packages and the `-U` flag ensures that packages are upgraded to the newest compatible version.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama_2/scripts/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Creating a TextEnvironment for Calculator Tool in Python\nDESCRIPTION: This snippet creates a `trl.TextEnvironment` to facilitate interaction between the LLM and the simple calculator tool.  It takes the model, tokenizer, the tool function (`tool_fn`), a reward function, and a prompt as input. It also uses the `generation_kwargs` for controlling the text generation process. Relies on the `trl` library and the `TextEnvironment` API.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/learning_tools.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nenv = TextEnvironment(\n        model,\n        tokenizer,\n        {\"SimpleCalculatorTool\": tool_fn},\n        reward_fn,\n        prompt,\n        generation_kwargs=generation_kwargs,\n    )\n```\n\n----------------------------------------\n\nTITLE: Encouraging EOS Token Generation (Python)\nDESCRIPTION: This snippet shows how to encourage the model to generate an EOS token before reaching the maximum length by using the `missing_eos_penalty` argument in `NashMDConfig`. It penalizes the model for not generating the EOS token before the maximum length.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/nash_md_trainer.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntraining_args = NashMDConfig(..., max_new_tokens=128, missing_eos_penalty=1.0)\n```\n\n----------------------------------------\n\nTITLE: Using Strings as Input in IterativeSFTTrainer in Python\nDESCRIPTION: This snippet shows how to use a dictionary containing a list of strings (\"texts\") as input to the step function of the IterativeSFTTrainer.  This assumes that `texts` variable containing the list of strings is already defined.  The texts are used to train the language model.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/iterative_sft_trainer.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninputs = {\n    \"texts\": texts\n}\n\ntrainer.step(**inputs)\n```\n\n----------------------------------------\n\nTITLE: Searching Wikipedia with Pyserini in Python\nDESCRIPTION: This code snippet shows how to use `pyserini` to search a pre-built index of the KILT Wikipedia dump. It initializes a `LuceneSearcher` and defines a `search` function that takes a query and returns the contents of the top search result.  Requires `pyserini` and `json` libraries.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/learning_tools.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pyserini.search.lucene import LuceneSearcher\nimport json\nsearcher = LuceneSearcher.from_prebuilt_index('wikipedia-kilt-doc')\ndef search(query):\n    hits = searcher.search(query, k=1)\n    hit = hits[0]\n    contents = json.loads(hit.raw)['contents']\n    return contents\nprint(search(\"tennis racket\"))\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(search(\"tennis racket\"))\n```\n\n----------------------------------------\n\nTITLE: RL Fine-tuning Command (StackLLaMa)\nDESCRIPTION: This command performs RL fine-tuning of the llama-7b-se model using the llama-7b-se-rm reward model. It employs `accelerate launch` for distributed training and sets various parameters, including logging, model names, learning rate, batch size, and output directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\naccelerate launch --multi_gpu --num_machines 1  --num_processes 8 examples/research_projects/stack_llama/scripts/rl_training.py --log_with=wandb --model_name=<LLAMA_SE_MODEL> --reward_model_name=<LLAMA_SE_RM_MODEL> --adafactor=False --tokenizer_name=<LLAMA_TOKENIZER> --save_freq=100 --output_max_length=128 --batch_size=8 --gradient_accumulation_steps=8 --batched_gen=True --ppo_epochs=4 --seed=0 --learning_rate=1.4e-5 --early_stopping=True --output_dir=llama-se-rl-finetune-128-8-8-1.4e-5_adam\n```\n\n----------------------------------------\n\nTITLE: LoRA Configuration for Reward Modeling\nDESCRIPTION: This Python code snippet defines the LoRA configuration for reward modeling. It sets the task type to sequence classification (`TaskType.SEQ_CLS`), disables inference mode, and specifies the rank (`r`), alpha (`lora_alpha`), and dropout (`lora_dropout`) parameters for the LoRA adapter.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/using_llama_models.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n)\n```\n\n----------------------------------------\n\nTITLE: Enable vLLM in GRPO Config\nDESCRIPTION: This code snippet demonstrates how to enable vLLM within the GRPOConfig in TRL by setting `use_vllm` to `True`. This configuration ensures GRPO utilizes vLLM for quicker generation during the training process.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/speeding_up_training.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import GRPOConfig\n\ntraining_args = GRPOConfig(..., use_vllm=True)\n```\n\n----------------------------------------\n\nTITLE: Test BERT Sentiment Classifier - Negative\nDESCRIPTION: Tests the loaded BERT sentiment classifier with a negative example text to verify its functionality. The `sent_kwargs` are used to configure the output.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntext = \"this movie was really bad!!\"\nsentiment_pipe(text, **sent_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Image logging hook function\nDESCRIPTION: This Python snippet showcases how to implement an image logging hook function for DDPO training, using Wandb to log generated images, prompts, and rewards during the training process.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/ddpo_trainer.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# for logging these images to wandb\n\ndef image_outputs_hook(image_data, global_step, accelerate_logger):\n    # For the sake of this example, we only care about the last batch\n    # hence we extract the last element of the list\n    result = {}\n    images, prompts, _, rewards, _ = image_data[-1]\n    for i, image in enumerate(images):\n        pil = Image.fromarray(\n            (image.cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)\n        )\n        pil = pil.resize((256, 256))\n        result[f\"{prompts[i]:.25} | {rewards[i]:.2f}\"] = [pil]\n    accelerate_logger.log_images(\n        result,\n        step=global_step,\n    )\n```\n\n----------------------------------------\n\nTITLE: Define Data Collator\nDESCRIPTION: This snippet defines a data collator function to batch data for training.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])\n```\n\n----------------------------------------\n\nTITLE: Run LayerSkip SFT Training\nDESCRIPTION: This script initiates the LayerSkip supervised fine-tuning (SFT) training process. It requires the 'scripts' directory to be the current working directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/layer_skip/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncd scripts\npython layer_skip_sft.py\n```\n\n----------------------------------------\n\nTITLE: Load Models and Tokenizers\nDESCRIPTION: Loads the pre-trained models and tokenizers using the `transformers` library. The code loads the base model, the RLHF fine-tuned model and the sentiment analysis model. It also sets the padding token for the tokenizer. The models are then moved to the appropriate device (GPU if available, otherwise CPU).\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/best_of_n.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n\nref_model = AutoModelForCausalLMWithValueHead.from_pretrained(ref_model_name)\n\nreward_pipe = pipeline(\"sentiment-analysis\", model=reward_model, device=device)\n\ntokenizer = AutoTokenizer.from_pretrained(ref_model_name)\n\ntokenizer.pad_token = tokenizer.eos_token\n\n# cuda-ize models\nmodel.to(device)\nref_model.to(device)\n```\n\n----------------------------------------\n\nTITLE: PPO Fine-tuning Example with TRL\nDESCRIPTION: This example demonstrates the basic steps for fine-tuning a GPT-2 model using Proximal Policy Optimization (PPO) with the TRL library. It showcases loading the model and tokenizer, initializing the PPOTrainer, generating a response, defining a reward, and performing a training step. The code assumes you have the `trl` and `transformers` libraries installed.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/quickstart.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# 0. imports\nimport torch\nfrom transformers import GPT2Tokenizer\n\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n\n\n# 1. load a pretrained model\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\nref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\n# 2. initialize trainer\nppo_config = {\"mini_batch_size\": 1, \"batch_size\": 1}\nconfig = PPOConfig(**ppo_config)\nppo_trainer = PPOTrainer(config, model, ref_model, tokenizer)\n\n# 3. encode a query\nquery_txt = \"This morning I went to the \"\nquery_tensor = tokenizer.encode(query_txt, return_tensors=\"pt\").to(model.pretrained_model.device)\n\n# 4. generate model response\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n    \"max_new_tokens\": 20,\n}\nresponse_tensor = ppo_trainer.generate([item for item in query_tensor], return_prompt=False, **generation_kwargs)\nresponse_txt = tokenizer.decode(response_tensor[0])\n\n# 5. define a reward for response\n# (this could be any reward such as human feedback or output from another model)\nreward = [torch.tensor(1.0, device=model.pretrained_model.device)]\n\n# 6. train model with ppo\ntrain_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM for TRL\nDESCRIPTION: Installs the vLLM (Very Large Language Model) package to accelerate generation during training within the TRL library. vLLM is a library that enables fast generation of text.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/grpo_trainer.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install trl[vllm]\n```\n\n----------------------------------------\n\nTITLE: Install TRL Judges Dependencies\nDESCRIPTION: This command installs the necessary dependencies for using the judges API in TRL. It utilizes pip, the Python package installer, to install the trl package with the judges extra.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/judges.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install trl[judges]\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging and Batch Size for PPO Script\nDESCRIPTION: This command configures the `ppo.py` script to log training metrics with WandB, and sets the mini-batch size to 1 and gradient accumulation steps to 16. This is useful for controlling the training process and monitoring performance.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sentiment_tuning.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython examples/scripts/ppo.py --log_with wandb --mini_batch_size 1 --gradient_accumulation_steps 16\n```\n\n----------------------------------------\n\nTITLE: Accelerate Configuration (Bash)\nDESCRIPTION: This command is essential before using the `sft` or `dpo` commands. It configures the training setup for the environment, specifying single/multi-GPU usage, DeepSpeed configurations, and other relevant settings. Ensure all steps of `accelerate config` are completed before running any CLI command.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/clis.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Using GenerationConfig with BestOfNSampler\nDESCRIPTION: This code snippet demonstrates how to use a `GenerationConfig` object from the `transformers` library to configure generation settings for the `BestOfNSampler`.  Settings like `temperature` and `pad_token_id` can be specified in the `GenerationConfig` and passed during initialization, so that they do not need to be passed during a `generate` call.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/best_of_n.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import GenerationConfig\n\ngeneration_config = GenerationConfig(min_length= -1, top_k=0.0, top_p= 1.0, do_sample= True, pad_token_id=tokenizer.eos_token_id)\n\nbest_of_n = BestOfNSampler(model, tokenizer, queries_to_scores, length_sampler=output_length_sampler, generation_config=generation_config)\n\nbest_of_n.generate(query_tensors, device=device)\n```\n\n----------------------------------------\n\nTITLE: Starting vLLM Server\nDESCRIPTION: Starts the vLLM (Very Large Language Model) server with the specified model. This server is used to speed up text generation during training.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/grpo_trainer.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntrl vllm-serve --model <model_name>\n```\n\n----------------------------------------\n\nTITLE: GRPO Configuration: Disable Model Gathering for Generation\nDESCRIPTION: This snippet demonstrates how to disable model gathering for generation in the `GRPOConfig` from the `trl` library by setting `ds3_gather_for_generation=False`. This is crucial when using DeepSpeed ZeRO-3 to prevent out-of-memory errors by avoiding the temporary gathering of model weights on a single GPU during the generation step.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reducing_memory_usage.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import GRPOConfig\n\ntraining_args = GRPOConfig(..., ds3_gather_for_generation=False)\n```\n\n----------------------------------------\n\nTITLE: Executing GRPO Training Script - Bash\nDESCRIPTION: This command executes the GRPO training script using the accelerate launcher, enabling distributed training across multiple GPUs.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/grpo_trainer.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_grpo.py\n```\n\n----------------------------------------\n\nTITLE: Loading Model with PEFT Configuration in Python\nDESCRIPTION: This Python snippet demonstrates how to load a pre-trained language model with a PEFT configuration using the `AutoModelForCausalLMWithValueHead` class from the TRL library and `LoraConfig` from the PEFT library. It initializes a LoRA configuration with specific rank, alpha, and dropout values, then loads the model with this configuration.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/peft_integration.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom peft import LoraConfig\nfrom trl import AutoModelForCausalLMWithValueHead\n\nmodel_id = \"edbeeching/gpt-neo-125M-imdb\"\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\n    model_id, \n    peft_config=lora_config,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Reference Model with Shared Layers in Python\nDESCRIPTION: This code snippet demonstrates how to create a reference model with shared layers in the PPOTrainer.  Sharing layers between the active and reference models reduces the memory footprint, enabling the training of larger models.  The `num_shared_layers` parameter specifies the number of layers to share.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/detoxifying_a_lm.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nref_model = create_reference_model(model, num_shared_layers=6)\ntrainer = PPOTrainer(..., ref_model=ref_model)\n```\n\n----------------------------------------\n\nTITLE: Define Control Tokens\nDESCRIPTION: This snippet defines control tokens for negative, neutral, and positive sentiments and encodes them using the GPT2 tokenizer.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nctrl_str = [\"[negative]\", \"[neutral]\", \"[positive]\"]\ndevice = torch.device(\n    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n)  # this should be handled by accelerate\nctrl_tokens = dict(\n    (s, gpt2_tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device))\n    for s in ctrl_str\n)\n```\n\n----------------------------------------\n\nTITLE: Store Results in DataFrame\nDESCRIPTION: Organizes the generated responses and scores into a Pandas DataFrame for easier analysis. The DataFrame includes the query, responses from each model, and the corresponding scores. For the best-of-n sampling approach, it selects the response with the highest score.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/best_of_n.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\noutput_data[\"response (ref)\"] = response_tensors_ref\noutput_data[\"scores (ref)\"] = scores_ref\noutput_data[\"response (RLHF)\"] = response_tensors\noutput_data[\"scores (RLHF)\"] = scores\noutput_data[\"response (best_of)\"] = [\n    response_tensors_best_of[i][a.argmax().item()] for i, a in enumerate(scores_best_of)\n]\noutput_data[\"scores (best_of)\"] = [a.max().item() for a in scores_best_of]\n\n\n# store results in a dataframe\ndf_results = pd.DataFrame(output_data)\ndf_results\n```\n\n----------------------------------------\n\nTITLE: Defining a System Prompt for Wiki Tool in Python\nDESCRIPTION: This snippet defines a system prompt for teaching the LLM how to use a wiki tool for question answering. It provides example questions, answers, and the structure for tool requests, calls, responses, and submissions. The prompt guides the LLM in formulating queries and interpreting search results to answer questions.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/learning_tools.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"\"\"\\\nAnswer the following question:\n\nQ: In which branch of the arts is Patricia Neary famous?\nA: Ballets\nA2: <request><Wiki>Patricia Neary<call>Patricia Neary (born October 27, 1942) is an American ballerina, choreographer and ballet director, who has been particularly active in Switzerland. She has also been a highly successful ambassador for the Balanchine Trust, bringing George Balanchine's ballets to 60 cities around the globe.<response>\nResult=Ballets<submit>\n\nQ: Who won Super Bowl XX?\nA: Chicago Bears\nA2: <request><Wiki>Super Bowl XX<call>Super Bowl XX was an American football game between the National Football Conference (NFC) champion Chicago Bears and the American Football Conference (AFC) champion New England Patriots to decide the National Football League (NFL) champion for the 1985 season. The Bears defeated the Patriots by the score of 46–10, capturing their first NFL championship (and Chicago's first overall sports victory) since 1963, three years prior to the birth of the Super Bowl. Super Bowl XX was played on January 26, 1986 at the Louisiana Superdome in New Orleans.<response>\nResult=Chicago Bears<submit>\n\nQ: \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Format Reward Function for Conversational Data\nDESCRIPTION: Implements a custom reward function designed for conversational data format, rewarding completions that adhere to a specific format (e.g., `<think>...</think><answer>...</answer>`). It uses regular expressions to match the required format within the completion's content.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/grpo_trainer.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\ndef format_reward_func(completions, **kwargs):\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n    completion_contents = [completion[0][\"content\"] for completion in completions]\n    matches = [re.match(pattern, content) for content in completion_contents]\n    return [1.0 if match else 0.0 for match in matches]\n```\n\n----------------------------------------\n\nTITLE: Install vLLM with TRL extras\nDESCRIPTION: This command installs vLLM along with extra dependencies specific to TRL using pip. This ensures compatibility and includes necessary components for using vLLM with TRL.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/speeding_up_training.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"trl[vllm]\"\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Candidates in BestOfNSampler\nDESCRIPTION: This snippet illustrates how to set the number of top scored candidates to return using the `n_candidates` argument during `BestOfNSampler` initialization. This determines how many of the best scoring outputs are chosen.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/best_of_n.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbest_of_n = BestOfNSampler(model, tokenizer, queries_to_scores, length_sampler=output_length_sampler, n_candidates=2)\n```\n\n----------------------------------------\n\nTITLE: Loading Model with PEFT Adapters\nDESCRIPTION: This snippet demonstrates how to load a language model with PEFT (Parameter-Efficient Fine-Tuning) adapters. It loads the base model and then loads the adapter weights on top of it using the `PeftModel` class. This requires the `peft` and `transformers` libraries.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/use_model.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom peft import PeftConfig, PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nbase_model_name = \"kashif/stack-llama-2\" #path/to/your/model/or/name/on/hub\"\nadapter_model_name = \"path/to/my/adapter\"\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name)\nmodel = PeftModel.from_pretrained(model, adapter_model_name)\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n```\n\n----------------------------------------\n\nTITLE: Executing PRM Training Script\nDESCRIPTION: This bash command shows how to execute the PRM training script using the `accelerate` library, enabling distributed training across multiple GPUs. This assumes that the `train_prm.py` script is in the current directory and that the accelerate environment has been configured.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/prm_trainer.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_prm.py\n```\n\n----------------------------------------\n\nTITLE: Install Liger-Kernel\nDESCRIPTION: This command installs the Liger-Kernel package using pip. Liger-Kernel is a collection of Triton kernels designed specifically for LLM training that improves training throughput and memory usage.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\npip install liger-kernel\n```\n\n----------------------------------------\n\nTITLE: Formatting Multi-Image + Text Data (python)\nDESCRIPTION: Formats the loaded data into a conversational structure suitable for the model. The `format_data` function processes each sample, extracts images, and structures the data into a list of messages with 'system', 'user', and 'assistant' roles.  It requires the PIL library.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/training_vlm_sft.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport zipfile\nimport io\nfrom datasets import DatasetDict\nfrom huggingface_hub import hf_hub_download, list_repo_files\nfrom PIL import Image\n\ndataset_train_split = \"test\"\n\ndef format_data(samples: dict[str, any]) -> dict[str, list]:\n    formatted_samples = {\"messages\": []}\n    for cont in range(len(samples[\"question\"])):\n        images = []\n        for img_path in samples[\"input_image_path\"][cont]:\n            try:\n                with open(img_path, \"rb\") as f:\n                    img_bytes = f.read()\n                image = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n                images.append({\"type\": \"image\", \"image\": image})\n            except Exception as e:\n                print(f\"Error processing image {img_path}: {e}\")\n                continue\n\n        formatted_samples[\"messages\"].append(\n            [\n                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": samples[\"context\"][cont]}]},\n                {\"role\": \"user\", \"content\": images + [{\"type\": \"text\", \"text\": samples[\"question\"][cont]}]},\n                {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": samples[\"output\"][cont]}]}],\n        )\n    return formatted_samples\n```\n\n----------------------------------------\n\nTITLE: Generating Output with BestOfNSampler\nDESCRIPTION: This snippet shows how to use the `generate` method of the `BestOfNSampler` class to generate improved output. It takes a list/tensor of tokenized queries as input, along with device information and generation keyword arguments.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/best_of_n.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbest_of_n.generate(query_tensors, device=device, **gen_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Using 8-bit Reference Models with DPOTrainer for Memory Efficiency\nDESCRIPTION: This code snippet shows how to use an 8-bit reference model with the DPOTrainer for memory-efficient fine-tuning. It utilizes `BitsAndBytesConfig` and `load_in_8bit=True` to load the reference model in 8-bit precision.  Dependencies include datasets, transformers, trl and BitsAndBytesConfig from transformers.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/customization.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom trl import DPOConfig, DPOTrainer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nref_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", quantization_config= quantization_config)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\ndataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\ntraining_args = DPOConfig(output_dir=\"Qwen2.5-0.5B-DPO\")\n\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=ref_model,\n    args=training_args,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Define Generation Settings\nDESCRIPTION: This snippet defines generation settings for the GPT2 model, including minimum length, top-k, top-p, sampling, pad token ID, maximum new tokens, and end-of-sentence token ID.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": gpt2_tokenizer.eos_token_id,\n    \"max_new_tokens\": txt_out_len,\n    \"eos_token_id\": -1,\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dev Tools using Pip\nDESCRIPTION: This command installs the TRL library in editable mode along with development dependencies. The -e flag installs the library in editable mode, allowing changes to the source code to be reflected immediately without reinstalling. The `.[dev]` specifies the 'dev' extra, which includes development-related dependencies such as testing and linting tools.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .[dev]\n```\n\n----------------------------------------\n\nTITLE: Saving/Loading Trained Model to Hub/Locally\nDESCRIPTION: This code snippet demonstrates how to save a trained `AutoModelForCausalLMWithValueHead` model either to the Hugging Face Model Hub or locally. It then shows how to load the saved model using `AutoModelForCausalLM` for inference purposes. The code assumes you have a trained model named `model`.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/quickstart.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# .. Let's assume we have a trained model using `PPOTrainer` and `AutoModelForCausalLMWithValueHead`\n\n# push the model on the Hub\nmodel.push_to_hub(\"my-fine-tuned-model-ppo\")\n\n# or save it locally\nmodel.save_pretrained(\"my-fine-tuned-model-ppo\")\n\n# load the model from the Hub\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"my-fine-tuned-model-ppo\")\n```\n\n----------------------------------------\n\nTITLE: Accelerate Configuration\nDESCRIPTION: This command configures the `accelerate` library, which is used for distributed training. Running this command prompts the user to set up configurations for the training environment, such as hardware setup and distributed strategies.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama_2/scripts/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ accelerate config\n```\n\n----------------------------------------\n\nTITLE: DPO Configuration with Completion Truncation\nDESCRIPTION: This code snippet shows how to use the `max_completion_length` parameter within `DPOConfig` from the `trl` library. This parameter truncates the completion sequence to a specified maximum length, which can be useful for managing memory usage, although it is generally less preferred to avoid losing information from the completion.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reducing_memory_usage.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import DPOConfig\n\ntraining_args = DPOConfig(..., max_completion_length=...)\n```\n\n----------------------------------------\n\nTITLE: Launching Training with Accelerate\nDESCRIPTION: This command launches a training script using the Accelerate library with a specified configuration file and number of GPUs.  It facilitates distributed training across multiple GPUs.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/example_overview.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\naccelerate launch --config_file=examples/accelerate_configs/multi_gpu.yaml --num_processes {NUM_GPUS} path_to_script.py --all_arguments_of_the_script\n```\n\n----------------------------------------\n\nTITLE: Custom Preference Dataset Creation (Bash)\nDESCRIPTION: This command shows how to format a dataset into TRL format using a provided example script and push it to the Hugging Face Hub. The `examples/datasets/anthropic_hh.py` script can be adapted for custom datasets.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/clis.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython examples/datasets/anthropic_hh.py --push_to_hub --hf_entity your-hf-org\n```\n\n----------------------------------------\n\nTITLE: Applying Style Corrections with precommit\nDESCRIPTION: Runs pre-commit hooks to automatically fix style issues and run code verification checks on the files modified by the pull request. This ensures code consistency and formatting.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ make precommit\n```\n\n----------------------------------------\n\nTITLE: Running PPO Script Directly\nDESCRIPTION: This command demonstrates how to execute the `ppo.py` script directly from the command line. It serves as a basic way to start the Proximal Policy Optimization (PPO) training process.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sentiment_tuning.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython examples/scripts/ppo.py\n```\n\n----------------------------------------\n\nTITLE: Import Dependencies\nDESCRIPTION: This snippet imports necessary libraries for the experiment, including torch, wandb, datasets, transformers, and trl.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Loading Single Image + Text Dataset (python)\nDESCRIPTION: Loads the `HuggingFaceH4/llava-instruct-mix-vsft` dataset using the `load_dataset` function from the `datasets` library. This dataset contains single images paired with text prompts formatted for supervised fine-tuning.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/training_vlm_sft.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset_name = \"HuggingFaceH4/llava-instruct-mix-vsft\"\n\n# Load Dataset\ndataset = load_dataset(dataset_name)\n```\n\n----------------------------------------\n\nTITLE: Merging PEFT Adapters (StackLLaMa)\nDESCRIPTION: This python script merges PEFT adapter layers with a base model. It takes the adapter model name, base model name, and output name as arguments. This script requires `peft>=0.3.0`.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython examples/research_projects/stack_llama/scripts/merge_peft_adapter.py --adapter_model_name=XXX --base_model_name=YYY --output_name=ZZZ\n```\n\n----------------------------------------\n\nTITLE: Run Sentiment Analysis\nDESCRIPTION: This snippet runs sentiment analysis on the provided text with the loaded sentiment analysis pipeline.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntext = \"this movie was a documentary\"\noutput = sentiment_pipe(text, **sentiment_pipe_kwargs)\noutput\n```\n\n----------------------------------------\n\nTITLE: RLOO Configuration: Disable Model Gathering for Generation\nDESCRIPTION: This snippet demonstrates how to disable model gathering for generation in the `RLOOConfig` from the `trl` library. Configuring `ds3_gather_for_generation=False` is useful in DeepSpeed ZeRO-3 setups to prevent out-of-memory (OOM) errors that may arise from gathering model weights onto a single GPU during the generation process; however, this may impact the speed of generation.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reducing_memory_usage.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import RLOOConfig\n\ntraining_args = RLOOConfig(..., ds3_gather_for_generation=False)\n```\n\n----------------------------------------\n\nTITLE: Test BERT Sentiment Classifier - Positive\nDESCRIPTION: Tests the loaded BERT sentiment classifier with a positive example text to verify its functionality. The `sent_kwargs` are used to configure the output.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntext = \"this movie was really good!!\"\nsentiment_pipe(text, **sent_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Running AlignProp Script\nDESCRIPTION: This code snippet shows how to run the `alignprop.py` script, which fine-tunes a Stable Diffusion model. It requires a Hugging Face user access token to upload the model after fine-tuning.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/alignprop_trainer.md#_snippet_0\n\nLANGUAGE: batch\nCODE:\n```\npython alignprop.py --hf_user_access_token <token>\n```\n\n----------------------------------------\n\nTITLE: Run Training with ZeRO Stage 2\nDESCRIPTION: Executes a training script using 🤗 Accelerate, configured with a DeepSpeed ZeRO Stage 2 configuration file.  ZeRO Stage 2 optimizes memory usage by partitioning optimizer states across devices.  The provided example uses a pre-configured YAML file from the `examples/accelerate_configs` directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/deepspeed_integration.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --config_file examples/accelerate_configs/deepspeed_zero2.yaml train.py\n```\n\n----------------------------------------\n\nTITLE: Run Sentiment Analysis\nDESCRIPTION: This snippet runs sentiment analysis on the provided text with the loaded sentiment analysis pipeline.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntext = \"this movie was really bad!!\"\noutput = sentiment_pipe(text, **sentiment_pipe_kwargs)\noutput\n```\n\n----------------------------------------\n\nTITLE: Running the Nash-MD example script (Bash)\nDESCRIPTION: This command shows how to run the `nash_md.py` example script. It uses the Qwen2.5 0.5B model on the UltraFeedback dataset, with specific learning rate, logging steps, and output directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/nash_md_trainer.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython examples/scripts/nash_md.py \\\n    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \\\n    --judge pair_rm \\\n    --dataset_name trl-lib/ultrafeedback-prompt \\\n    --learning_rate 5.0e-7 \\\n    --logging_steps 25 \\\n    --output_dir Qwen2.5-0.5B-NashMD-PairRM \\\n    --warmup_ratio 0.1 \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Define Generation Keyword Arguments\nDESCRIPTION: Defines keyword arguments used for text generation. Includes parameters such as `min_length`, `top_k`, `top_p`, `do_sample`, and `pad_token_id`. These parameters control the generation process, allowing for customization of the generated text.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/best_of_n.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ngen_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n}\n```\n\n----------------------------------------\n\nTITLE: Formatting prompts with SFTTrainer - Python\nDESCRIPTION: This code snippet demonstrates how to use a formatting function with the `SFTTrainer` to customize the input prompts based on the dataset fields.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n...\ndef formatting_prompts_func(example):\n    return f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n\n\ntrainer = SFTTrainer(\n    model,\n    args=training_args,\n    train_dataset=dataset,\n    formatting_func=formatting_prompts_func,\n)\n\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Launching Distributed Training with Config File\nDESCRIPTION: This command launches the training script in a distributed environment using a specific configuration file. The `--config_file` argument specifies the path to the configuration file, allowing for reproducible distributed training setups.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/distributing_training.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\naccelerate launch --config_file examples/accelerate_configs/multi_gpu.yaml train.py <SCRIPT_ARGS>\n```\n\n----------------------------------------\n\nTITLE: Access Input IDs\nDESCRIPTION: This snippet accesses the input IDs for the 3rd element in the dataset.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndataset[3][\"input_ids\"]\n```\n\n----------------------------------------\n\nTITLE: TRL vLLM Server CLI Usage\nDESCRIPTION: This snippet shows the usage of the `trl vllm-serve` command-line interface, including available options such as specifying the model, revision, tensor parallel size, host, port, GPU memory utilization, data type, maximum model length, and enabling prefix caching. It shows the help message to guide users on the available options for configuring the vLLM server.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/vllm_integration.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n$ trl vllm-serve --help\nusage: trl vllm-serve [-h] --model MODEL [--revision REVISION] [--tensor_parallel_size TENSOR_PARALLEL_SIZE] [--host HOST]\n                      [--port PORT] [--gpu_memory_utilization GPU_MEMORY_UTILIZATION] [--dtype DTYPE]\n                      [--max_model_len MAX_MODEL_LEN] [--enable_prefix_caching ENABLE_PREFIX_CACHING]\n\noptions:\n  -h, --help            Show this help message and exit\n  --model MODEL         Model name or path to load the model from. (default: None)\n  --revision REVISION   Revision to use for the model. If not specified, the default branch will be used. (default: None)\n  --tensor_parallel_size TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE\n                        Number of tensor parallel workers to use. (default: 1)\n  --host HOST           Host address to run the server on. (default: 0.0.0.0)\n  --port PORT           Port to run the server on. (default: 8000)\n  --gpu_memory_utilization GPU_MEMORY_UTILIZATION, --gpu-memory-utilization GPU_MEMORY_UTILIZATION\n                        Ratio (between 0 and 1) of GPU memory to reserve for the model weights, activations, and KV cache on the device\n                        dedicated to generation powered by vLLM. Higher values will increase the KV cache size and thus improve the\n                        model's throughput. However, if the value is too high, it may cause out-of-memory (OOM) errors during\n                        initialization. (default: 0.9)\n  --dtype DTYPE         Data type to use for vLLM generation. If set to 'auto', the data type will be automatically determined based on\n                        the model configuration. Find the supported values in the vLLM documentation. (default: auto)\n  --max_model_len MAX_MODEL_LEN, --max-model-len MAX_MODEL_LEN\n                        If set, the `max_model_len` to use for vLLM. This can be useful when running with reduced\n                        `vllm_gpu_memory_utilization`, leading to a reduced KV cache size. If not set, vLLM will use the model context\n                        size, which might be much larger than the KV cache, leading to inefficiencies. (default: None)\n  --enable_prefix_caching ENABLE_PREFIX_CACHING, --enable-prefix-caching ENABLE_PREFIX_CACHING\n                        Whether to enable prefix caching in vLLM. If set to `True`, ensure that the model and the hardware support this\n                        feature. (default: None)\n```\n\n----------------------------------------\n\nTITLE: Tokenize IMDB Reviews\nDESCRIPTION: This snippet tokenizes the IMDB reviews, extracts the first `txt_in_len` tokens, and decodes them back into text to be used as input queries.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset = dataset.map(\n    lambda x: {\n        \"input_ids\": gpt2_tokenizer.encode(\" \" + x[\"review\"], return_tensors=\"pt\")[\n            0, :txt_in_len\n        ]\n    },\n    batched=False,\n)\ndataset = dataset.map(\n    lambda x: {\"query\": gpt2_tokenizer.decode(x[\"input_ids\"])}, batched=False\n)\ndataset = dataset[:20480]\n\nfrom datasets import Dataset\n\ndataset = Dataset.from_dict(dataset)\ndataset.set_format(\"pytorch\")\n```\n\n----------------------------------------\n\nTITLE: Import dependencies\nDESCRIPTION: Imports necessary libraries for the project, including transformers, trl, and wandb for model training and evaluation.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Set GPU Visibility for vLLM Server\nDESCRIPTION: This command sets the `CUDA_VISIBLE_DEVICES` environment variable to specify which GPUs the vLLM server should use. In this example, GPUs 0-3 are assigned to the vLLM server. This is crucial for separating resources and preventing conflicts between training and generation.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/speeding_up_training.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 trl vllm-serve --model <model_name>\n```\n\n----------------------------------------\n\nTITLE: Run KTO script with parameters\nDESCRIPTION: This command executes the KTO training script with specific parameters, including the model name, dataset name, number of training epochs, logging steps, and output directory.  It leverages the accelerate launch command to run the script in a distributed training environment. Requires accelerate to be installed.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/kto_trainer.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch trl/scripts/kto.py \\\n    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n    --dataset_name trl-lib/kto-mix-14k \\\n    --num_train_epochs 1 \\\n    --logging_steps 25 \\\n    --output_dir Qwen2-0.5B-KTO\n```\n\n----------------------------------------\n\nTITLE: Access Sentiment Score\nDESCRIPTION: This snippet accesses the sentiment score for the 1st result from the output list.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\noutput[1][\"score\"]\n```\n\n----------------------------------------\n\nTITLE: Instruction Dataset Format - JSON\nDESCRIPTION: This JSON snippet shows the expected format for instruction datasets used with the `SFTTrainer`. It includes `prompt` and `completion` fields.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n```\n\n----------------------------------------\n\nTITLE: Plotting RLHF Scaling Chart\nDESCRIPTION: This Python script generates a plot of the RLHF scaling chart using `matplotlib`. It defines the results for SFT, online-dpo and offline-dpo methods across different model sizes and plots them against the win rate. It also includes a horizontal line representing the human reference summary performance.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/online_dpo_trainer.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nresults = {\n    \"SFT\": {1.0e9: 0.21, 2.8e9: 0.27, 6.9e9: 0.316},\n    \"online-dpo\": {1.0e9: 0.542, 2.8e9: 0.746, 6.9e9: 0.796},\n    \"offline-dpo\": {1.0e9: 0.422, 2.8e9: 0.517, 6.9e9: 0.701},\n}\n\n\nplt.plot(results[\"SFT\"].keys(), results[\"SFT\"].values(), label=\"SFT\", marker=\"o\")\nplt.plot(results[\"online-dpo\"].keys(), results[\"online-dpo\"].values(), label=\"Online-dpo with RM judge\", marker=\"o\")\nplt.plot(results[\"offline-dpo\"].keys(), results[\"offline-dpo\"].values(), label=\"Offline-dpo\", marker=\"o\")\nplt.axhline(y=0.5, color=\"black\", linestyle=\"-.\", label=\"Human reference summary\")\nplt.xscale(\"log\")\nplt.xlabel(\"Model size\")\nplt.ylabel(\"Win rate against reference summaries\\n(according to GPT-4-0613)\")\nplt.title(\"DPO scaling by model size\")\nplt.legend()\nplt.xlim(5e8, 1.2e10)\nplt.xticks([1e9, 3e9, 1e10], [\"1B\", \"3B\", \"10B\"])\nplt.grid(True, which=\"both\", ls=\"--\", c=\"0.7\")\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Pairwise Judge (Python)\nDESCRIPTION: This code defines a custom pairwise judge, `PrefersShorterJudge`, which inherits from `BasePairwiseJudge`. The judge's `judge` method compares the lengths of the two completions for each prompt and returns 0 if the first completion is longer than the second, and 1 otherwise. `shuffle_order` parameter is ignored.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/judges.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import BasePairwiseJudge\n\nclass PrefersShorterJudge(BasePairwiseJudge):\n    def judge(self, prompts, completions, shuffle_order=False):\n        return [0 if len(completion[0]) > len(completion[1]) else 1 for completion in completions]\n```\n\n----------------------------------------\n\nTITLE: Python Prompt Example\nDESCRIPTION: This code snippet demonstrates a prompt example for training an agent to solve math questions using a Python interpreter. The prompt provides an example question, a Python code block to solve the question, and the expected result. The agent is expected to generate similar code and results for new questions.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/learning_tools.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"\"\"\\\nExample of using a Python API to solve math questions.\n\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n\n<request><PythonInterpreter>\ndef solution():\n    money_initial = 23\n    bagels = 5\n    bagel_cost = 3\n    money_spent = bagels * bagel_cost\n    money_left = money_initial - money_spent\n    result = money_left\n    return result\nprint(solution())\n<call>8<response>\n\nResult = 8 <submit>\n\nQ: \"\"\"\n```\n\n----------------------------------------\n\nTITLE: TRL Development Installation\nDESCRIPTION: This command set allows you to install TRL for development.  This includes cloning the repository, navigating into the directory, and installing the required dependencies for development and testing using pip.\nSOURCE: https://github.com/huggingface/trl/blob/main/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/trl.git\ncd trl/\npip install -e .[dev]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Development Environment\nDESCRIPTION: Installs the TRL library in editable mode with development dependencies using pip.  This allows for making changes to the library and testing them without reinstalling.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -e .[dev]\n```\n\n----------------------------------------\n\nTITLE: Defining a System Prompt for Calculator Tool in Python\nDESCRIPTION: This snippet defines a system prompt that guides the LLM on how to interact with the simple calculator tool. The prompt includes examples of questions and the expected format for tool requests, calls, responses, and submissions. The prompt is a crucial part of teaching the LLM how to effectively use the tool.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/learning_tools.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"\"\"\\\n    What is 13.1-3?\n\n    <request><SimpleCalculatorTool>13.1-3<call>10.1<response>\n\n    Result=10.1<submit>\n\n    What is 4*3?\n\n    <request><SimpleCalculatorTool>4*3<call>12<response>\n\n    Result=12<submit>\n\n    What is 12.1+1?\n\n    <request><SimpleCalculatorTool>12.1+1<call>13.1<response>\n\n    Result=13.1<submit>\n\n    What is 12.1-20?\n\n    <request><SimpleCalculatorTool>12.1-20<call>-7.9<response>\n\n    Result=-7.9<submit>\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Evaluating TL;DR Summaries with Judge Model\nDESCRIPTION: This shell script evaluates the generated TL;DR summaries using a judge model (GPT-4o mini). It runs the `judge_tldr.py` script with specified model paths and the number of examples to evaluate. The script calculates and prints the model win rate.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/online_dpo_trainer.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\n$ python examples/scripts/evals/judge_tldr.py --model_name_or_path trl-lib/pythia-1b-deduped-tldr-sft --judge_model gpt-4o-mini --num_examples 1000\nModel win rate: 33.00%\npython examples/scripts/evals/judge_tldr.py --model_name_or_path trl-lib/pythia-6.9b-deduped-tldr-sft --judge_model gpt-4o-mini --num_examples 1000\nModel win rate: 41.50%\npython examples/scripts/evals/judge_tldr.py --model_name_or_path trl-lib/pythia-1b-deduped-tldr-online-dpo --judge_model gpt-4o-mini --num_examples 1000\nModel win rate: 62.60%\npython examples/scripts/evals/judge_tldr.py --model_name_or_path trl-lib/pythia-6.9b-deduped-tldr-online-dpo --judge_model gpt-4o-mini --num_examples 1000\nModel win rate: 74.20%\n```\n\n----------------------------------------\n\nTITLE: Installing Transformers from Source\nDESCRIPTION: This bash command installs the `transformers` library directly from its GitHub repository. This ensures that you have the latest features and bug fixes, which are required for using Flash Attention.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip install -U git+https://github.com/huggingface/transformers.git\n```\n\n----------------------------------------\n\nTITLE: Install dependencies\nDESCRIPTION: Installs the required Python packages: transformers, trl, and wandb.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%pip install transformers trl wandb\n```\n\n----------------------------------------\n\nTITLE: Run LayerSkip Benchmark\nDESCRIPTION: This script runs the benchmarking process for the LayerSkip implementation. It requires the 'scripts' directory to be the current working directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/layer_skip/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncd scripts\npython benchmark_layer_skip.py\n```\n\n----------------------------------------\n\nTITLE: Running DDPO training script\nDESCRIPTION: Executes the DDPO training script with a Hugging Face user access token. This token is required to upload the finetuned model to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/ddpo_trainer.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npython ddpo.py --hf_user_access_token <token>\n```\n\n----------------------------------------\n\nTITLE: Enabling Flash Attention 1 with torch.backends.cuda.sdp_kernel\nDESCRIPTION: This code snippet shows how to enable Flash Attention 1 using the `torch.backends.cuda.sdp_kernel` context manager.  This context manager is used to wrap the `trainer.train()` call, enabling Flash Attention kernels.  It requires packing=True to avoid issues with padding tokens.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n    trainer.train()\n```\n\n----------------------------------------\n\nTITLE: Accelerate Configuration\nDESCRIPTION: This command initializes the 🤗 Accelerate configuration, prompting the user to answer questions about their multi-GPU/multi-node setup. The responses are stored in a configuration file used by the `accelerate launch` command.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/distributing_training.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Import Libraries\nDESCRIPTION: Imports necessary Python libraries for the script. Includes `torch` for tensor operations, `pandas` for data manipulation, `transformers` for using pre-trained models and tokenizers, `datasets` for loading datasets, and `trl` for reinforcement learning tools.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/best_of_n.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport pandas as pd\n\nfrom transformers import pipeline, AutoTokenizer\nfrom datasets import load_dataset\n\nfrom trl import AutoModelForCausalLMWithValueHead\nfrom trl.core import LengthSampler\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n```\n\n----------------------------------------\n\nTITLE: Load BERT Sentiment Classifier\nDESCRIPTION: Loads a pre-trained BERT sentiment classifier fine-tuned on the IMDB dataset using the `pipeline` function from the `transformers` library. It determines the device to run the classifier on based on GPU availability.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndevice = ppo_trainer.accelerator.device\nif ppo_trainer.accelerator.num_processes == 1:\n    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\nsentiment_pipe = pipeline(\n    \"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device\n)\n```\n\n----------------------------------------\n\nTITLE: Example Call Syntax\nDESCRIPTION: This code block showcases the call syntax used within text environments to request a tool's usage and integrate its response.  It highlights the specific tokens used to delineate requests, tool names, queries, and responses, facilitating the model's interaction with available tools.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/text_environments.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n\"<request><Calculator>1/2<call>0.5<response>\"\n```\n\n----------------------------------------\n\nTITLE: Calculate Reward Statistics\nDESCRIPTION: Calculates and displays the mean and median of the 'rewards (before)' and 'rewards (after)' columns in the `df_results` DataFrame.  This allows for comparison of the reward distributions.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nprint(\"mean:\")\ndisplay(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\nprint()\nprint(\"median:\")\ndisplay(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())\n```\n\n----------------------------------------\n\nTITLE: Installing TRL with PEFT and Dependencies\nDESCRIPTION: This bash script installs the TRL library with PEFT support, along with necessary dependencies such as bitsandbytes, loralib, and the latest version of transformers from its GitHub repository. It also includes an optional installation of wandb for experiment tracking.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/peft_integration.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install trl[peft]\npip install bitsandbytes loralib\npip install git+https://github.com/huggingface/transformers.git@main\n#optional: wandb\npip install wandb\n```\n\n----------------------------------------\n\nTITLE: SLURM Script for Distributed GRPO Training\nDESCRIPTION: A SLURM script for training a large language model (70B+) with GRPO (Generative Ranking and Policy Optimization) across multiple nodes. The script allocates nodes for training and vLLM-powered generation, using DeepSpeed ZeRO Stage 3 and Accelerate for efficient distributed training.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/grpo_trainer.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n#!/bin/bash\n#SBATCH --nodes=5\n#SBATCH --gres=gpu:8\n\n# Get the list of allocated nodes\nNODELIST=($(scontrol show hostnames $SLURM_JOB_NODELIST))\n\n# Assign the first 4 nodes for training and the 5th node for vLLM\nTRAIN_NODES=\"${NODELIST[@]:0:4}\"  # Nodes 0, 1, 2, 3 for training\nVLLM_NODE=\"${NODELIST[4]}\"  # Node 4 for vLLM\n\n# Run training on the first 4 nodes (Group 1)\nsrun --nodes=4 --ntasks=4 --nodelist=\"${NODELIST[@]:0:4}\" accelerate launch \\\n     --config_file examples/accelerate_configs/deepspeed_zero3.yaml \\\n     --num_processes 32 \\\n     --num_machines 4 \\\n     --main_process_ip ${NODELIST[0]} \\\n     --machine_rank $SLURM_PROCID \\\n     --rdzv_backend c10d \\\n     train_grpo.py \\\n     --server_ip $VLLM_NODE &\n\n# Run vLLM server on the 5th node (Group 2)\nsrun --nodes=1 --ntasks=1 --nodelist=\"${NODELIST[4]}\" trl vllm-serve --model Qwen/Qwen2.5-72B --tensor_parallel_size 8 &\n\nwait\n```\n\n----------------------------------------\n\nTITLE: OpenRLBenchmark Metric Tracking\nDESCRIPTION: This command uses the `openrlbenchmark` tool to track and visualize various training metrics. It specifies filters to select relevant metrics, model paths, and output configurations for generating benchmark plots. It requires `openrlbenchmark` to be installed.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/rloo_trainer.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# pip install openrlbenchmark==0.2.1a5\n# see https://github.com/openrlbenchmark/openrlbenchmark#get-started for documentation\n# to use it, change `?we=huggingface&wpn=trl` to your own project and `?tag=pr-1540` to your own tag\npython -m openrlbenchmark.rlops_multi_metrics \\\n    --filters '?we=huggingface&wpn=trl&xaxis=train/episode&ceik=output_dir&cen=sft_model_path&metrics=train/objective/rlhf_reward&metrics=train/objective/scores&metrics=train/objective/kl&metrics=train/objective/non_score_reward&metrics=train/objective/entropy&metrics=train/policy/approxkl_avg&metrics=train/policy/clipfrac_avg&metrics=train/loss/policy_avg&metrics=train/policy/entropy_avg&metrics=train/val/ratio&metrics=train/val/ratio_var&metrics=train/val/num_eos_tokens&metrics=train/lr&metrics=train/eps' \\\n        \"cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr?tag=pr-1540\" \\\n    --env-ids models/minimal/rloo_tldr \\\n    --pc.ncols 4 \\\n    --pc.ncols-legend 1 \\\n    --pc.xlabel \"Episode\" \\\n    --output-filename benchmark/trl/pr-1540/rloo \\\n    --scan-history\n```\n\n----------------------------------------\n\nTITLE: Reward Distribution Plotting in Python\nDESCRIPTION: This snippet generates a histogram of the reward distribution for different control strings. It uses matplotlib to plot the distribution and includes labels and a title.  The code assumes that `logs`, `task_list`, and `ctrl_str` are defined in the surrounding context.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfor ctrl_s in ctrl_str:\n    plt.hist(\n        [r for r, t in zip(logs[\"env/reward_dist\"], task_list) if t == ctrl_s],\n        density=True,\n        alpha=0.5,\n        label=ctrl_s,\n    )\nplt.legend(loc=\"best\")\nplt.title(\"reward distribution\")\nplt.grid(True)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: PPO Configuration: Disable Model Gathering for Generation\nDESCRIPTION: This snippet illustrates how to disable model gathering for generation in the `PPOConfig` from the `trl` library. Setting `ds3_gather_for_generation=False` is important when training with DeepSpeed ZeRO-3 to avoid out-of-memory (OOM) errors that can occur when model weights are gathered on a single GPU during generation, although this might reduce generation speed.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/reducing_memory_usage.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import PPOConfig\n\ntraining_args = PPOConfig(..., ds3_gather_for_generation=False)\n```\n\n----------------------------------------\n\nTITLE: Query and Response Prompt Template\nDESCRIPTION: Defines the template structure for queries and responses used during the SFT, RM, and RLHF stages. The template combines a question and an answer using the Question: and Answer: prefixes to format the input for the model.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/using_llama_models.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nQuestion: <Query>\n\nAnswer: <Response>\n```\n\n----------------------------------------\n\nTITLE: Loading Multi-Image + Text Dataset (python)\nDESCRIPTION: Loads the `FanqingM/MMIU-Benchmark` dataset using the `load_dataset` function. This dataset contains multiple images paired with text, requiring preprocessing before training.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/training_vlm_sft.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset_name = \"FanqingM/MMIU-Benchmark\"\n\n# Load Dataset\ndataset = load_dataset(dataset_name)\n```\n\n----------------------------------------\n\nTITLE: Set GPU Visibility for Training\nDESCRIPTION: This command sets the `CUDA_VISIBLE_DEVICES` environment variable to specify which GPUs training should use.  In this example, GPUs 4-7 are assigned for training using `accelerate launch train.py`. This ensures the training process uses distinct GPUs from the vLLM server, preventing resource contention.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/speeding_up_training.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nCUDA_VISIBLE_DEVICES=4,5,6,7 accelerate launch train.py\n```\n\n----------------------------------------\n\nTITLE: Install TRL using uv\nDESCRIPTION: This command installs the TRL library using the uv package manager. uv is a fast, Rust-based alternative to pip. It assumes uv is already installed.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/installation.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install trl\n```\n\n----------------------------------------\n\nTITLE: Creating a New Branch\nDESCRIPTION: Creates a new branch for development changes, branching off from the synchronized `main` branch. This isolates changes and makes it easier to manage contributions.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ git checkout -b a-descriptive-name-for-my-changes\n```\n\n----------------------------------------\n\nTITLE: Verifying the Custom Collator in Python\nDESCRIPTION: This code snippet demonstrates how to load a dataset and verify that the custom collator `collate_fn` is working as expected. It loads a sample dataset, collates a small batch of examples, and prints the keys of the collated data to check for the expected output format.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"HuggingFaceH4/llava-instruct-mix-vsft\", split=\"train\")\nexamples = [dataset[0], dataset[1]]  # Just two examples for the sake of the example\ncollated_data = collate_fn(examples)\nprint(collated_data.keys())  # dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'labels'])\n```\n\n----------------------------------------\n\nTITLE: XPO Training Script with Qwen Model\nDESCRIPTION: This script demonstrates how to train a language model using the XPO method with the Qwen2-0.5B-Instruct model. It loads the model and tokenizer, initializes a judge, loads the training dataset, configures the XPOTrainer with specified training arguments, and starts the training process. The script utilizes the `trl` library for XPO training and the `datasets` library for loading the dataset.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/xpo_trainer.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train_xpo.py\nfrom datasets import load_dataset\nfrom trl import PairRMJudge, XPOConfig, XPOTrainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\njudge = PairRMJudge()\ntrain_dataset = load_dataset(\"trl-lib/ultrafeedback-prompt\", split=\"train\")\n\ntraining_args = XPOConfig(output_dir=\"Qwen2-0.5B-XPO\", logging_steps=10)\ntrainer = XPOTrainer(\n    model=model, judge=judge, args=training_args, processing_class=tokenizer, train_dataset=train_dataset\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face Hub (bash)\nDESCRIPTION: Logs into the Hugging Face Hub using the `huggingface-cli` command. This is necessary to access gated models like Gemma 3, which requires requesting access permissions.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/training_vlm_sft.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: TRL CLI for DPO\nDESCRIPTION: This command demonstrates how to use the TRL command-line interface (CLI) to perform Direct Preference Optimization (DPO) on a language model. It specifies the model name, dataset name, and output directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntrl dpo --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \\\n    --dataset_name argilla/Capybara-Preferences \\\n    --output_dir Qwen2.5-0.5B-DPO \n```\n\n----------------------------------------\n\nTITLE: Calculate Reward (Zero Logits)\nDESCRIPTION: This snippet calculates the reward for zero logits using the `pos_logit_to_reward` function.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npos_logit_to_reward(torch.Tensor([0, 0, 0]), ctrl_str)\n```\n\n----------------------------------------\n\nTITLE: TRL CLI for SFT\nDESCRIPTION: This command demonstrates how to use the TRL command-line interface (CLI) to perform supervised fine-tuning (SFT) on a language model. It specifies the model name, dataset name, and output directory.\nSOURCE: https://github.com/huggingface/trl/blob/main/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntrl sft --model_name_or_path Qwen/Qwen2.5-0.5B \\\n    --dataset_name trl-lib/Capybara \\\n    --output_dir Qwen2.5-0.5B-SFT\n```\n\n----------------------------------------\n\nTITLE: Calculate Reward (Positive Logits)\nDESCRIPTION: This snippet calculates the reward for positive logits using the `pos_logit_to_reward` function.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\npos_logit_to_reward(torch.Tensor([4, 4, 4]), ctrl_str)\n```\n\n----------------------------------------\n\nTITLE: Inspect Model Responses\nDESCRIPTION: Inspects the responses generated by the tuned model compared to the reference model. It fetches a batch of data, generates responses using both models, and decodes the responses using the tokenizer for comparison.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#### get a batch from the dataset\nbs = 16\ngame_data = dict()\ndataset.set_format(\"pandas\")\ndf_batch = dataset[:].sample(bs)\ngame_data[\"query\"] = df_batch[\"query\"].tolist()\nquery_tensors = df_batch[\"input_ids\"].tolist()\n\nresponse_tensors_ref, response_tensors = [], []\n\n#### get response from gpt2 and gpt2_ref\nfor i in range(bs):\n    query = torch.tensor(query_tensors[i]).to(device)\n\n    gen_len = output_length_sampler()\n    query_response = ref_model.generate(\n        query.unsqueeze(0), max_new_tokens=gen_len, **gen_kwargs\n    ).squeeze()\n    response_len = len(query_response) - len(query)\n    response_tensors_ref.append(query_response[-response_len:])\n\n    query_response = model.generate(\n        query.unsqueeze(0), max_new_tokens=gen_len, **gen_kwargs\n    ).squeeze()\n    response_len = len(query_response) - len(query)\n    response_tensors.append(query_response[-response_len:])\n\n#### decode responses\ngame_data[\"response (before)\"] = [\n    tokenizer.decode(response_tensors_ref[i]) for i in range(bs)\n]\ngame_data[\"response (after)\"] = [\n    tokenizer.decode(response_tensors[i]) for i in range(bs)\n]\n```\n\n----------------------------------------\n\nTITLE: Launching XPO Training\nDESCRIPTION: This command shows how to launch the XPO training script using accelerate. It assumes that the `train_xpo.py` script is already created and configured. It distributes the training process across multiple GPUs for faster training.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/xpo_trainer.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_xpo.py\n```\n\n----------------------------------------\n\nTITLE: Calculate Reward (Negative Logits)\nDESCRIPTION: This snippet calculates the reward for negative logits using the `pos_logit_to_reward` function.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npos_logit_to_reward(torch.Tensor([-4, -4, -4]), ctrl_str)\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum for Flash Attention 1\nDESCRIPTION: This bash command installs the `optimum` library.  Optimum provides tools to accelerate training and inference of transformers models, including integrations for Flash Attention 1.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npip install -U optimum\n```\n\n----------------------------------------\n\nTITLE: Image Logging Hook Function Example\nDESCRIPTION: This Python code defines an example image logging hook function for use with `wandb`. It processes batched image data including images, prompts and rewards, resizes images, and logs them.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/alignprop_trainer.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# for logging these images to wandb\n\ndef image_outputs_hook(image_data, global_step, accelerate_logger):\n    # For the sake of this example, we only care about the last batch\n    # hence we extract the last element of the list\n    result = {}\n    images, prompts, rewards = [image_data['images'],image_data['prompts'],image_data['rewards']]\n    for i, image in enumerate(images):\n        pil = Image.fromarray(\n            (image.cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)\n        )\n        pil = pil.resize((256, 256))\n        result[f\"{prompts[i]:.25} | {rewards[i]:.2f}\"] = [pil]\n    accelerate_logger.log_images(\n        result,\n        step=global_step,\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring GRPO with vLLM\nDESCRIPTION: Configures the GRPO (Generative Ranking and Policy Optimization) training arguments to use vLLM (Very Large Language Model) for faster generation. The `use_vllm` flag is set to `True` within the training arguments.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/grpo_trainer.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import GRPOConfig\n\ntraining_args = GRPOConfig(..., use_vllm=True)\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Main Branch with Upstream\nDESCRIPTION: Synchronizes the local `main` branch with the `upstream/main` branch to keep it up to date with the latest changes in the original repository.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ git checkout main\n$ git fetch upstream\n$ git merge upstream/main\n```\n\n----------------------------------------\n\nTITLE: Extract Positive Logits\nDESCRIPTION: This snippet extracts the positive sentiment logits from the pipeline output.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef extract_pipe_output(outputs):\n    positive_logits = []\n    for out in outputs:\n        for element in out:\n            if element[\"label\"] == \"POSITIVE\":\n                positive_logits.append(torch.tensor(element[\"score\"]))\n    return positive_logits\n```\n\n----------------------------------------\n\nTITLE: Adding and Committing Changes\nDESCRIPTION: Stages the modified files and commits the changes with a descriptive commit message. This records the changes locally.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ git add modified_file.py\n$ git commit\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries (bash)\nDESCRIPTION: Installs the necessary Python libraries for fine-tuning the multimodal model, including `trl`, `bitsandbytes`, `peft`, `hf_xet`, and `tensorboard`.  This sets up the environment with the correct dependencies. Further details are available at https://huggingface.co/docs/trl/installation.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/training_vlm_sft.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the required libraries. Futher details: https://huggingface.co/docs/trl/installation \npip install -U -q trl bitsandbytes peft hf_xet tensorboard\n```\n\n----------------------------------------\n\nTITLE: Run GPT-J-6B Toxicity Script with Accelerate\nDESCRIPTION: This command executes the `gpt-j-6b-toxicity.py` script using the `accelerate` library, which facilitates distributed training and execution. It sets the `ACCELERATE_LOG_LEVEL` environment variable to `info` for detailed logging and uses a configuration file specified by `{CONFIG}`. The `--log_with wandb` flag enables logging to Weights & Biases (wandb) for tracking and visualization.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/toxicity/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nACCELERATE_LOG_LEVEL=info accelerate launch --config_file {CONFIG} examples/research_projects/toxicity/scripts/gpt-j-6b-toxicity.py --log_with wandb\n```\n\n----------------------------------------\n\nTITLE: Display Control Strings\nDESCRIPTION: This snippet prints the control strings list\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nprint(ctrl_str)\n```\n\n----------------------------------------\n\nTITLE: Logging Completions with Callback (Python)\nDESCRIPTION: This code adds a callback to log sample completions during training using the `LogCompletionsCallback`. It initializes the callback with the trainer and an evaluation dataset and then adds it to the trainer.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/nash_md_trainer.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrainer = NashMDTrainer(..., eval_dataset=eval_dataset)\ncompletions_callback = LogCompletionsCallback(trainer, num_prompts=8)\ntrainer.add_callback(completions_callback)\n```\n\n----------------------------------------\n\nTITLE: Reward Modeling Command (StackLLaMa)\nDESCRIPTION: This command trains a reward model using dialog pairs from the Stack Exchange dataset, based on the llama-7b-se model.  It utilizes `torchrun` for distributed training and requires specifying the model name of the supervised fine-tuned model.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ntorchrun --nnodes 1  --nproc_per_node 8 examples/research_projects/stack_llama/scripts/reward_modeling.py --model_name=<LLAMA_SE_MODEL>\n```\n\n----------------------------------------\n\nTITLE: Load IMDB Dataset\nDESCRIPTION: This snippet loads the IMDB dataset, renames the columns, filters for reviews longer than 500 characters, and truncates reviews to 1000 characters.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# create the dataset\n#\ndataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\ndataset = dataset.rename_columns({\"text\": \"review\", \"label\": \"sentiment\"})\n# make sure the comments are are at least 500 and trim to 1000\ndataset = dataset.filter(lambda x: len(x[\"review\"]) > 500, batched=False)\ndataset = dataset.map(lambda x: {\"review\": x[\"review\"][:1000]}, batched=False)\n\ndataset\n```\n\n----------------------------------------\n\nTITLE: Display Control Tokens\nDESCRIPTION: This snippet displays the defined control tokens.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nctrl_tokens\n```\n\n----------------------------------------\n\nTITLE: Build Dataset\nDESCRIPTION: Defines a function to build the dataset. It loads the IMDb dataset using the `datasets` library, filters reviews longer than 200 characters, and tokenizes them. The `LengthSampler` is used to determine the length of input sequences. The function returns a PyTorch dataset.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/best_of_n.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef build_dataset(\n    tokenizer,\n    dataset_name=\"stanfordnlp/imdb\",\n    input_min_text_length=2,\n    input_max_text_length=8,\n):\n    # load imdb with datasets\n    ds = load_dataset(dataset_name, split=\"train\")\n    ds = ds.rename_columns({\"text\": \"review\"})\n    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n\n    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n\n    def tokenize(sample):\n        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n        return sample\n\n    ds = ds.map(tokenize, batched=False)\n    ds.set_format(type=\"torch\")\n    return ds\n\n\ndataset = build_dataset(tokenizer)\n```\n\n----------------------------------------\n\nTITLE: Configuring Model with ModelConfig Utility\nDESCRIPTION: This code snippet demonstrates how to use the `ModelConfig` utility to configure the model for training.  It shows how to set various parameters such as `attn_implementation`, `torch_dtype`, and quantization configurations.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom trl import ModelConfig, SFTTrainer, get_kbit_device_map, get_peft_config, get_quantization_config\nmodel_args = ModelConfig(\n    model_name_or_path=\"facebook/opt-350m\"\n    attn_implementation=None, # or \"flash_attention_2\"\n)\ntorch_dtype = (\n    model_args.torch_dtype\n    if model_args.torch_dtype in [\"auto\", None]\n    else getattr(torch, model_args.torch_dtype)\n)\nquantization_config = get_quantization_config(model_args)\nmodel_kwargs = dict(\n    revision=model_args.model_revision,\n    trust_remote_code=model_args.trust_remote_code,\n    attn_implementation=model_args.attn_implementation,\n    torch_dtype=torch_dtype,\n    use_cache=False if training_args.gradient_checkpointing else True,\n    device_map=get_kbit_device_map() if quantization_config is not None else None,\n    quantization_config=quantization_config,\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs)\ntrainer = SFTTrainer(\n    ...,\n    model=model_args.model_name_or_path,\n    peft_config=get_peft_config(model_args),\n)\n```\n\n----------------------------------------\n\nTITLE: Install TRL from source\nDESCRIPTION: This set of commands clones the TRL repository from GitHub, navigates into the cloned directory, and installs the package in editable mode. This allows for local modifications to the source code to be reflected immediately.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/installation.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/trl.git\ncd trl/\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Generate Responses\nDESCRIPTION: Generates responses using the reference model, the RLHF fine-tuned model, and the best-of-n sampling approach. Iterates through the batch of queries, generates responses using each model, and stores the generated responses in lists. For the best-of-n sampling approach, it generates multiple responses for each query and stores them in a nested list.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/best_of_n.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfor i in range(bs):\n    gen_len = output_length_sampler()\n\n    query = torch.tensor(query_tensors[i])\n\n    output = ref_model.generate(\n        query.unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n    ).squeeze()\n    response_tensors_ref.append(tokenizer.decode(output))\n\n    output = model.generate(\n        query.unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n    ).squeeze()\n    response_tensors.append(tokenizer.decode(output))\n\n    # generating copies of the same query for the Best-of-n sampling\n    queries = query.repeat((N_BEST_OF, 1))\n    output = ref_model.generate(\n        queries.to(device), max_new_tokens=gen_len, **gen_kwargs\n    ).squeeze()\n    response_tensors_best_of.append(tokenizer.batch_decode(output))\n```\n\n----------------------------------------\n\nTITLE: Custom Reward Trainer with Loss Computation\nDESCRIPTION: This Python code snippet defines a custom `RewardTrainer` class that inherits from the `transformers.Trainer`. It overrides the `compute_loss` method to calculate the reward modeling loss based on the difference between rewards for chosen and rejected samples. The loss is computed using the log-sigmoid function, and the method returns the loss and optional reward outputs.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/using_llama_models.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass RewardTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        rewards_j = model(input_ids=inputs[\"input_ids_j\"],  attention_mask=inputs[\"attention_mask_j\"])[0]\n        rewards_k = model(input_ids=inputs[\"input_ids_k\"], attention_mask=inputs[\"attention_mask_k\"])[0]\n        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n        if return_outputs:\n            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n        return loss\n```\n\n----------------------------------------\n\nTITLE: Logging Completions with Callback\nDESCRIPTION: This snippet demonstrates how to log model completions during training using the `LogCompletionsCallback`. It instantiates the callback with the trainer and the number of prompts to log, and then adds the callback to the trainer. This allows for monitoring and understanding the model's behavior during training by logging sample completions to Weights & Biases.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/xpo_trainer.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrainer = XPOTrainer(..., eval_dataset=eval_dataset)\ncompletions_callback = LogCompletionsCallback(trainer, num_prompts=8)\ntrainer.add_callback(completions_callback)\n```\n\n----------------------------------------\n\nTITLE: Custom Collator for Multi-Modal Data in Python\nDESCRIPTION: This code defines a custom collator function `collate_fn` to process multi-modal data (text and images). It applies a chat template to the text, tokenizes the texts and images, and prepares labels by masking padding tokens. This collator is designed to work with vision-language models.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndef collate_fn(examples):\n    # Get the texts and images, and apply the chat template\n    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) for example in examples]\n    images = [example[\"images\"][0] for example in examples]\n\n    # Tokenize the texts and process the images\n    batch = processor(texts, images, return_tensors=\"pt\", padding=True)\n\n    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n    labels = batch[\"input_ids\"].clone()\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n    batch[\"labels\"] = labels\n\n    return batch\n```\n\n----------------------------------------\n\nTITLE: Text Generation Configuration for PPO Training (trl)\nDESCRIPTION: This code snippet provides the recommended generation settings for Proximal Policy Optimization (PPO) training to avoid issues with negative KL divergence. Key settings include disabling top-k and nucleus sampling, enabling sampling, using the EOS token as the padding token, and setting a maximum number of new tokens.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/how_to_train.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ngeneration_kwargs = {\n    \"min_length\": -1, # don't ignore the EOS token (see above)\n    \"top_k\": 0.0, # no top-k sampling\n    \"top_p\": 1.0, # no nucleus sampling\n    \"do_sample\": True, # yes, we want to sample\n    \"pad_token_id\": tokenizer.eos_token_id, # most decoder models don't have a padding token - use EOS token instead\n    \"max_new_tokens\": 32, # specify how many tokens you want to generate at most\n}\n```\n\n----------------------------------------\n\nTITLE: Install vLLM using pip\nDESCRIPTION: This command installs the vLLM library using pip, which is required for fast generation in online methods like GRPO or Online DPO.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/speeding_up_training.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install vllm\n```\n\n----------------------------------------\n\nTITLE: Running an Episode in a TextEnvironment\nDESCRIPTION: This snippet shows how to run a query through the text environment using the `run` method. It returns the queries, responses, masks, rewards, and histories of the interaction. Extra keyword arguments passed to run are passed to the reward function.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/text_environments.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nqueries = [\"What is 1/2?\"]\nanswers = [\"0.5\"]\n\nqueries, responses, masks, rewards, histories = text_env.run(queries, answers=answers)\n```\n\n----------------------------------------\n\nTITLE: Launching Training Script with Accelerate/Torchrun\nDESCRIPTION: This code snippet demonstrates how to launch a training script using either `accelerate` or `torchrun` for data parallelism across multiple GPUs. The `accelerate launch` command distributes the training process across 8 GPUs on a single machine, while `torchrun` achieves the same result with similar parameters.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/using_llama_models.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --multi_gpu --num_machines 1  --num_processes 8 my_accelerate_script.py\ntorchrun --nnodes 1  --nproc_per_node 8 my_torch_script.py\n```\n\n----------------------------------------\n\nTITLE: Vision Language Model Data Format\nDESCRIPTION: This snippet showcases the data format for training a vision language model (VLM). The data includes both text and images, formatted in a conversational manner with \"user\" and \"assistant\" roles. The content for each role includes both text and image types.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimages = [\"obama.png\"]\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Who is this?\"},\n            {\"type\": \"image\"}\n        ]\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Barack Obama\"}\n        ]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What is he famous for?\"}\n        ]\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"He is the 44th President of the United States.\"}\n        ]\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Cloning TRL Repository\nDESCRIPTION: This command clones the TRL repository from GitHub, which is useful for accessing example scripts and contributing to the library.  You'll need git installed.\nSOURCE: https://github.com/huggingface/trl/blob/main/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/trl.git\n```\n\n----------------------------------------\n\nTITLE: Data keys expected by image_outputs_hook\nDESCRIPTION: This code provides the structure of the dictionary with keys expected by image logging hook function.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/alignprop_trainer.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n['image', 'prompt', 'prompt_metadata', 'rewards']\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes to Remote Repository\nDESCRIPTION: Pushes the local branch with the changes to the forked repository on GitHub. The `-u` option sets up tracking, so subsequent pushes can be done with `git push`.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ git push -u origin a-descriptive-name-for-my-changes\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies\nDESCRIPTION: Installs the required Python packages: `transformers` and `trl`. These packages are necessary for using pre-trained models, tokenizers, and training utilities from the Hugging Face ecosystem.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/best_of_n.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install transformers trl\n```\n\n----------------------------------------\n\nTITLE: Install TRL using pip\nDESCRIPTION: This command installs the TRL library using pip, the standard Python package installer. It downloads and installs the latest version of the TRL package from PyPI.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/installation.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install trl\n```\n\n----------------------------------------\n\nTITLE: Set Random Seed\nDESCRIPTION: This snippet sets the random seed using numpy for reproducibility of the experiment.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(seed)\n```\n\n----------------------------------------\n\nTITLE: Chatting with the Trained Model (Bash)\nDESCRIPTION: This command demonstrates how to use the Transformers Chat CLI to interact with the trained model.  It specifies the model's path and launches an interactive chat session, allowing users to generate responses from the model.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/nash_md_trainer.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntransformers-cli chat --model_name_or_path trl-lib/Qwen2-0.5B-NashMD\n```\n\n----------------------------------------\n\nTITLE: TRL Environment Information (Bash)\nDESCRIPTION: This command retrieves and prints system information, including GPU details, CUDA version, PyTorch version, Transformers version, and TRL version, along with optional dependencies. This information is helpful for debugging and reporting issues.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/clis.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntrl env\n```\n\n----------------------------------------\n\nTITLE: Running PPO Script with Accelerate\nDESCRIPTION: This set of commands shows how to run the `ppo.py` script using the `accelerate` library, which enables features like multi-GPU support and DeepSpeed integration. The `accelerate config` command initializes the training configuration, and `accelerate launch` starts the training process.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sentiment_tuning.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config # will prompt you to define the training configuration\naccelerate launch examples/scripts/ppo.py # launches training\n```\n\n----------------------------------------\n\nTITLE: TL;DR Judge Evaluation Script\nDESCRIPTION: This bash script evaluates the generated TL;DR summaries against reference TL;DRs using a judge model (GPT-4o mini). It runs the `judge_tldr.py` script with specified model paths and evaluation parameters to determine the model's win rate.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/rloo_trainer.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ python examples/scripts/evals/judge_tldr.py --model_name_or_path cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr --judge_model gpt-4o-mini --num_examples 1000\nModel win rate: 33.00%\n$ python examples/scripts/evals/judge_tldr.py --model_name_or_path vwxyzjn/rloo_tldr --judge_model gpt-4o-mini --num_examples 1000\nModel win rate: 51.20%\n```\n\n----------------------------------------\n\nTITLE: Save Model and Tokenizer to Hugging Face Hub\nDESCRIPTION: Saves the trained model and tokenizer to the Hugging Face Hub using the `save_pretrained` method.  The model and tokenizer are saved with the name \"gpt2-imdb-pos-v2\" and pushed to the hub for later use. It requires the `model` and `tokenizer` objects to be defined and initialized.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmodel.save_pretrained(\"gpt2-imdb-pos-v2\", push_to_hub=True)\ntokenizer.save_pretrained(\"gpt2-imdb-pos-v2\", push_to_hub=True)\n```\n\n----------------------------------------\n\nTITLE: Define Reward Function\nDESCRIPTION: This snippet defines a reward function that scales the positive sentiment logit based on the target sentiment (negative, neutral, positive).\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef pos_logit_to_reward(logit, task):\n    \"\"\"\n    Take the positive sentiment logit and scale it for the task.\n        task [negative]: reward = -logit\n        task [neutral]: reward = -2*abs(logit)+4\n        task [positive]: reward = logit\n    \"\"\"\n    for i in range(len(logit)):\n        if task[i] == \"[negative]\":\n            logit[i] = -logit[i]\n        elif task[i] == \"[neutral]\":\n            logit[i] = -2 * torch.abs(logit[i]) + 4\n        elif task[i] == \"[positive]\":\n            pass\n        else:\n            raise ValueError(\"task has to be in [0, 1, 2]!\")\n    return logit\n```\n\n----------------------------------------\n\nTITLE: Run Sentiment Analysis\nDESCRIPTION: This snippet runs sentiment analysis on the provided text with the loaded sentiment analysis pipeline.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntext = \"this movie was really good!!\"\noutput = sentiment_pipe(text, **sentiment_pipe_kwargs)\noutput\n```\n\n----------------------------------------\n\nTITLE: Define Sentiment Analysis Keyword Arguments\nDESCRIPTION: Defines keyword arguments for sentiment analysis. These parameters control how sentiment scores are calculated and returned. `top_k` is set to `None` to return all scores, and `function_to_apply` is set to `none` to return raw scores, without sigmoid or softmax.  The `batch_size` controls how many sequences are processed by the sentiment model at once.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/best_of_n.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\": 16}\n```\n\n----------------------------------------\n\nTITLE: Example YAML Configuration File\nDESCRIPTION: This YAML file provides an example configuration for training models with the `trl sft` command. It defines parameters such as the model name, dataset name, learning rate, and learning rate scheduler type. Arguments in this file can be overwritten using the CLI.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/clis.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_name_or_path:\n  Qwen/Qwen2.5-0.5B\ndataset_name:\n  stanfordnlp/imdb\nreport_to:\n  none\nlearning_rate:\n  0.0001\nlr_scheduler_type:\n  cosine\n```\n\n----------------------------------------\n\nTITLE: Import Dependencies\nDESCRIPTION: This snippet imports necessary libraries for the experiment, including torch, wandb, datasets, transformers, and trl.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport torch\nimport wandb\nimport time\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom random import choices\nimport matplotlib.pyplot as plt\n\ntqdm.pandas()\n\nfrom datasets import load_dataset\n\nfrom transformers import AutoTokenizer, pipeline\n\nfrom trl import (\n    PPOTrainer,\n    PPOConfig,\n    AutoModelForCausalLMWithValueHead,\n    create_reference_model,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Tool\nDESCRIPTION: This snippet demonstrates how to define a custom tool as a Python function. The tool must accept a string as input and return a string as output. This example defines a simple addition tool.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/text_environments.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef add(text):\n    int_1, int_2 = text.split(\"+\")\n    result = int(int_1) + int(int_2)\n    return str(result)\n\nprint(add(\"1+1\"))\n>>> \"2\"\n```\n\n----------------------------------------\n\nTITLE: XPO Configuration for EOS Penalty\nDESCRIPTION: This snippet shows how to configure the `XPOConfig` to encourage EOS token generation by adding a penalty for not generating the EOS token before reaching the maximum length during training. It sets `max_new_tokens` to specify the maximum length and `missing_eos_penalty` to penalize the model for not generating the EOS token.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/xpo_trainer.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntraining_args = XPOConfig(..., max_new_tokens=128, missing_eos_penalty=1.0)\n```\n\n----------------------------------------\n\nTITLE: Executing the DPO Training Script\nDESCRIPTION: This command executes the DPO training script using the `accelerate` launcher.  It assumes the script `train_dpo.py` is in the current directory and that the `accelerate` library is configured for distributed training. It facilitates distributed training across multiple GPUs.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/dpo_trainer.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_dpo.py\n```\n\n----------------------------------------\n\nTITLE: Define Constants\nDESCRIPTION: Defines constant variables used throughout the script, including model names for the reference model, fine-tuned model, and reward model, as well as the number of samples to generate for the best-of-n sampling approach. These constants allow for easy modification and configuration of the experiment.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/best_of_n.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nref_model_name = \"lvwerra/gpt2-imdb\"\nmodel_name = \"lvwerra/gpt2-imdb-pos-v2\"\nreward_model = \"lvwerra/distilbert-imdb\"\n\nN_BEST_OF = 4\n```\n\n----------------------------------------\n\nTITLE: Executing Training Script (Bash)\nDESCRIPTION: This command executes the training script for the Nash-MD method.  It utilizes the accelerate library to launch the training process, enabling distributed training across multiple GPUs. The training script is expected to be named 'train_nash_md.py'.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/nash_md_trainer.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_nash_md.py\n```\n\n----------------------------------------\n\nTITLE: Initialize Weights & Biases\nDESCRIPTION: Initializes the Weights & Biases (wandb) logging system to track and visualize the training progress. Requires the wandb library to be installed.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport wandb\n\nwandb.init()\n```\n\n----------------------------------------\n\nTITLE: Run Training Script with DeepSpeed\nDESCRIPTION: Launches a training script using 🤗 Accelerate with a specified DeepSpeed configuration file. The configuration file dictates the DeepSpeed optimization settings used during training, such as ZeRO stage.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/deepspeed_integration.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --config_file <ACCELERATE_WITH_DEEPSPEED_CONFIG_FILE.yaml> train.py\n```\n\n----------------------------------------\n\nTITLE: Getting Help Text for PPO Script\nDESCRIPTION: This command displays the help text and documentation for the `ppo.py` script. It provides information on available command-line arguments and usage instructions.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sentiment_tuning.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython examples/scripts/ppo.py --help\n```\n\n----------------------------------------\n\nTITLE: Installing TRL from Source\nDESCRIPTION: This command installs the TRL library directly from the GitHub repository. This allows you to use the latest, unreleased features and contributions.\nSOURCE: https://github.com/huggingface/trl/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/huggingface/trl.git\n```\n\n----------------------------------------\n\nTITLE: Installing TRL Python Package\nDESCRIPTION: This command installs the TRL (Transformer Reinforcement Learning) library using pip. This is the standard method for installing the released version of the library.\nSOURCE: https://github.com/huggingface/trl/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install trl\n```\n\n----------------------------------------\n\nTITLE: Load BERT Sentiment Classifier\nDESCRIPTION: This snippet loads a pre-trained BERT sentiment classifier for reward calculation.\nSOURCE: https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nif ppo_trainer.accelerator.num_processes == 1:\n    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\nelse:\n    device = ppo_trainer.accelerator.device\nsentiment_pipe = pipeline(\n    \"sentiment-analysis\", \"lvwerra/distilbert-imdb\", device=device\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Custom Reward Function\nDESCRIPTION: Demonstrates how to test the custom reward function with example prompts and completions, showing the expected output.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/grpo_trainer.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> prompts = [\"The sky is\", \"The sun is\"]\n>>> completions = [\" blue.\", \" in the sky.\"]\n>>> print(reward_func(prompts=prompts, completions=completions))\n[6.0, 12.0]\n```\n\n----------------------------------------\n\nTITLE: Testing Format Reward Function\nDESCRIPTION: Demonstrates testing the format reward function with conversational prompts and completions, showcasing the expected output based on format matching.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/grpo_trainer.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> prompts = [\n...     [{\"role\": \"assistant\", \"content\": \"What is the result of (1 + 2) * 4?\"}],\n...     [{\"role\": \"assistant\", \"content\": \"What is the result of (3 + 1) * 2?\"}],\n... ]\n>>> completions = [\n...     [{\"role\": \"assistant\", \"content\": \"<think>The sum of 1 and 2 is 3, which we multiply by 4 to get 12.</think><answer>(1 + 2) * 4 = 12</answer>\"}],\n...     [{\"role\": \"assistant\", \"content\": \"The sum of 3 and 1 is 4, which we multiply by 2 to get 8. So (3 + 1) * 2 = 8.\"}],\n... ]\n>>> format_reward_func(prompts=prompts, completions=completions)\n[1.0, 0.0]\n```\n\n----------------------------------------\n\nTITLE: Process VLM data using LLaVA processor\nDESCRIPTION: This snippet demonstrates how to process the VLM data format using the LLaVA model's processor. It shows how to apply the chat template to the messages and print the formatted output.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\nprint(processor.apply_chat_template(messages, tokenize=False))\n```\n\n----------------------------------------\n\nTITLE: Running Specific Tests\nDESCRIPTION: Runs a specific test file using pytest.  This is useful for testing changes made to a particular feature or component.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ pytest tests/<TEST_TO_RUN>.py\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Upstream Repository\nDESCRIPTION: Clones the forked repository to the local machine and adds the original repository as an upstream remote. This allows for synchronizing changes from the main repository into the fork.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone git@github.com:<your Github handle>/trl.git\n$ cd trl\n$ git remote add upstream https://github.com/huggingface/trl.git\n```\n\n----------------------------------------\n\nTITLE: Install TRL in development mode\nDESCRIPTION: This command installs TRL in editable mode with development dependencies. This is useful for contributors and developers who want to work on the TRL library itself and need the development tools and testing frameworks.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/installation.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Running TRL Environment Information\nDESCRIPTION: This command executes the `trl env` script to gather and display environment information relevant to the TRL library. It helps in debugging and issue reporting by providing details about the operating system, Python version, PyTorch version, TRL version, and Transformers version.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntrl env\n```\n\n----------------------------------------\n\nTITLE: Running the Full Test Suite\nDESCRIPTION: Runs the entire test suite using `make`. This ensures that all tests pass before submitting a pull request.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ make test\n```\n\n----------------------------------------\n\nTITLE: Device Placement for Multi-GPU training\nDESCRIPTION: This snippet shows how to ensure the model is placed on the correct device when using multi-GPU training with `accelerate`. It uses `PartialState` to determine the process index and map the model to the correct device.\nSOURCE: https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import PartialState\ndevice_string = PartialState().process_index\nmodel = AutoModelForCausalLM.from_pretrained(\n     ...\n    device_map={'':device_string}\n)\n```\n\n----------------------------------------\n\nTITLE: Running pytest\nDESCRIPTION: Runs the pytest test suite for the library.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m pytest -sv ./tests\n```\n\n----------------------------------------\n\nTITLE: Rebasing with Upstream\nDESCRIPTION: Syncs the local branch with the upstream repository and rebases the changes on top of the latest main branch. This resolves conflicts and keeps the branch up to date.\nSOURCE: https://github.com/huggingface/trl/blob/main/CONTRIBUTING.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ git fetch upstream\n$ git rebase upstream/main\n```"
  }
]
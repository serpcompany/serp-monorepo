[
  {
    "owner": "beam-cloud",
    "repo": "examples",
    "content": "TITLE: Configuring Beam Function with Secret\nDESCRIPTION: Python decorator for a Beam function, specifying the use of the Hugging Face token secret. This configuration allows the function to securely access the Mixtral 7B model using the stored token.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/language_models/mixtral_7b/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@endpoint(secrets=[\"HF_TOKEN\"])\n```\n\n----------------------------------------\n\nTITLE: Adding Secret to Beam Function Decorator\nDESCRIPTION: This Python code snippet shows how to add the Hugging Face token secret to a Beam function decorator, enabling the function to access the token during execution.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/language_models/llama3_8b/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@endpoint(secrets=[\"HF_TOKEN\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring Beam endpoint for Gemma inference with GPU acceleration\nDESCRIPTION: This decorator sets up a Beam endpoint for serving the fine-tuned Gemma model. It configures the endpoint to load the model only once at startup, mount the volume containing the model weights, and utilize a T4 GPU with auto-scaling capabilities.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/gemma/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@endpoint(\n    name=\"gemma-inference\",\n    on_start=load_finetuned_model,\n    volumes=[Volume(name=\"gemma-ft\", mount_path=MOUNT_PATH)],\n    cpu=1,\n    memory=\"16Gi\",\n    gpu=\"T4\",\n    image=Image(\n        python_version=\"python3.9\",\n        python_packages=[\"transformers==4.42.0\", \"torch\", \"peft\"],\n    ),\n    autoscaler=QueueDepthAutoscaler(max_containers=5, tasks_per_container=1),\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying Huggingface Model Endpoint\nDESCRIPTION: Command to deploy a persistent web endpoint for Huggingface inference in production.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/huggingface_inference/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy app.py:predict\n```\n\n----------------------------------------\n\nTITLE: Configuring Beam Cloud Endpoint for LLaMA Inference\nDESCRIPTION: Sets up the Beam Cloud endpoint configuration with necessary compute resources, dependencies, and autoscaling settings for model inference.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/llama/README.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@endpoint(\n    name=\"llama-inference\",\n    on_start=load_finetuned_model,\n    volumes=[Volume(name=\"llama-ft\", mount_path=MOUNT_PATH)],\n    cpu=1,\n    memory=\"16Gi\",\n    # We can switch to a smaller, more cost-effective GPU for inference rather than fine-tuning\n    gpu=\"T4\",\n    image=Image(\n        python_version=\"python3.9\",\n        python_packages=[\"transformers==4.42.0\", \"torch\", \"peft\"],\n    ),\n    # This autoscaler spawns new containers (up to 5) if the queue depth for tasks exceeds 1\n    autoscaler=QueueDepthAutoscaler(max_containers=5, tasks_per_container=1),\n)\ndef predict(**inputs):\n    # This function is unchanged\n\nif __name__ == \"__main__\":\n    predict.remote()\n```\n\n----------------------------------------\n\nTITLE: Deploying OpenAI-compatible APIs for Multiple LLM Models\nDESCRIPTION: These commands deploy OpenAI-compatible APIs for four different LLM models (internvl, yicoder_chat, mistral_instruct, and deepseek_r1) using the Beam SDK's VLLM wrapper class.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/vllm/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy models.py:internvl\nbeam deploy models.py:yicoder_chat\nbeam deploy models.py:mistral_instruct\nbeam deploy models.py:deepseek_r1\n```\n\n----------------------------------------\n\nTITLE: Deploying Whisper App on Beam\nDESCRIPTION: Command to deploy the transcription function from app.py to Beam cloud platform.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/whisper_stt/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy app.py:transcribe\n```\n\n----------------------------------------\n\nTITLE: Deploying WhisperX Transcription Endpoint with Beam\nDESCRIPTION: This command deploys the 'transcribe_audio' function from the 'app.py' file as a Beam endpoint for audio transcription.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/whisperx_stt/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy app.py:transcribe_audio\n```\n\n----------------------------------------\n\nTITLE: Deploying Zonos TTS API with Beam Cloud\nDESCRIPTION: Command to deploy the Text-to-Speech application to Beam Cloud. This single command deploys the 'generate' function from the app.py file to make it accessible as an API endpoint.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/zonos/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy app.py:generate\n```\n\n----------------------------------------\n\nTITLE: Deploying Parler TTS API Service with Beam\nDESCRIPTION: Command to deploy the text-to-speech service using Beam's deployment system. Deploys the generate_speech function from app.py.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/parler-tts/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy app.py:generate_speech\n```\n\n----------------------------------------\n\nTITLE: Serving Faster Whisper Transcription Application on Beam\nDESCRIPTION: Command to serve the Whisper transcription application on Beam Cloud. This makes the transcribe function from app.py available as an API endpoint.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/faster_whisper/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbeam serve app.py:transcribe\n```\n\n----------------------------------------\n\nTITLE: Deploying Mochi-1 Text-to-Video Service with Beam\nDESCRIPTION: Command to deploy the video generation service using Beam Cloud.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/video_models/mochi1/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy app.py:generate_video\n```\n\n----------------------------------------\n\nTITLE: Serving the Object Detection Endpoint with Beam\nDESCRIPTION: Command to serve the object detection endpoint using Beam. This makes the API accessible for testing.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/object_detection/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbeam serve app.py:predict\n```\n\n----------------------------------------\n\nTITLE: Deploying Flux Application to Beam\nDESCRIPTION: This command deploys the Flux application to Beam Cloud. It specifies the Python file 'app.py' and the 'generate' function as the entry point for the serverless application.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/flux/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy app.py:generate\n```\n\n----------------------------------------\n\nTITLE: Deploying a Prediction API with Beam CLI\nDESCRIPTION: Command for deploying the app.py file as a serverless prediction API on Beam Cloud. This command targets the 'predict' function within the app.py file for deployment.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/language_models/outlines-ai/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy app.py:predict\n```\n\n----------------------------------------\n\nTITLE: Deploying vLLM API Server with Beam\nDESCRIPTION: Command to deploy the vLLM API server using Beam. This deploys the 'generate' function from app.py as a service.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/vllm/vision_models/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy app.py:generate\n```\n\n----------------------------------------\n\nTITLE: Deploying Beam Application via CLI\nDESCRIPTION: Shell command for deploying a Beam application to create a web API endpoint. Deploys the 'main' function from app.py to the cloud.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/bioinformatics/dnabert/README.md#2025-04-07_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nbeam deploy app.py:main\n```\n\n----------------------------------------\n\nTITLE: Deploying Gemma inference endpoint using Beam CLI\nDESCRIPTION: This bash command deploys the inference endpoint to Beam using the CLI. It shows the expected output of the deployment process, including the curl command that can be used to invoke the endpoint.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/gemma/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy inference.py:predict --name gemma-ft\n```\n\n----------------------------------------\n\nTITLE: Beam Cloud Deployment Command\nDESCRIPTION: Command line instruction for deploying the endpoint to Beam Cloud with corresponding API invocation details.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/llama/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST 'https://app.beam.cloud/endpoint/llama-ft/v2' \\\n-H 'Connection: keep-alive' \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer {YOUR_AUTH_TOKEN}' \\\n-d '{}'\n```\n\n----------------------------------------\n\nTITLE: Serving Huggingface Model Preview\nDESCRIPTION: Command to create a preview environment for testing Huggingface inference endpoint.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/huggingface_inference/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbeam serve app.py:predict\n```\n\n----------------------------------------\n\nTITLE: Deploying Backend Service\nDESCRIPTION: Command to deploy the backend application to Beam cloud infrastructure\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/sdxl_turbo/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd backend && beam deploy app.py:generate\n```\n\n----------------------------------------\n\nTITLE: Deploying Backend and Starting Frontend\nDESCRIPTION: Commands to deploy the backend service using Beam and start the Reflex frontend server.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/sdxl/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd backend && beam deploy app.py:generate\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd frontend && reflex run\n```\n\n----------------------------------------\n\nTITLE: Backend Deployment Command\nDESCRIPTION: Command to deploy the backend application to Beam.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/sdxl_turbo_streaming/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd backend && beam deploy app.py:generate\n```\n\n----------------------------------------\n\nTITLE: Deploying Beam Cloud Web Endpoints using CLI\nDESCRIPTION: Shows the general syntax for deploying a Python function as a web endpoint using the Beam CLI. The command requires specifying the file, function name, and an endpoint name.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/endpoints/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy [file.py]:[function] --name [name]\n```\n\n----------------------------------------\n\nTITLE: Deploying Beam Functions CLI Command Template\nDESCRIPTION: Generic CLI command template for deploying functions as task queues or endpoints on Beam\nSOURCE: https://github.com/beam-cloud/examples/blob/main/task_queues/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nbeam deploy [file.py]:[function] --name [name]\n```\n\n----------------------------------------\n\nTITLE: Deploying Specific Beam Function Example\nDESCRIPTION: Example CLI command showing how to deploy a specific multiply function from app.py as a task queue\nSOURCE: https://github.com/beam-cloud/examples/blob/main/task_queues/README.md#2025-04-07_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nbeam deploy app.py:multiply --name my-app\n```\n\n----------------------------------------\n\nTITLE: Example of Deploying a Python Function as a Web Endpoint\nDESCRIPTION: A practical example showing how to deploy a specific function 'multiply' from an app.py file as a web endpoint named 'my-app' using the Beam CLI.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/endpoints/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy app.py:multiply --name my-app\n```\n\n----------------------------------------\n\nTITLE: Setting up Beam function for Gemma fine-tuning with GPU resources\nDESCRIPTION: This decorator configures a Beam function for fine-tuning the Gemma model. It specifies required volumes for storing model weights, necessary Python packages, and computational resources including an A100-40 GPU and 4 CPU cores.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/gemma/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@function(\n    volumes=[Volume(name=\"gemma-ft\", mount_path=MOUNT_PATH)],\n    image=Image(\n        python_packages=[\"transformers\", \"torch\", \"datasets\", \"peft\", \"bitsandbytes\"]\n    ),\n    gpu=\"A100-40\",\n    cpu=4,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Inference\nDESCRIPTION: Python function for performing inference with the fine-tuned model, including prompt formatting and token handling\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/llama/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# inference.py\ndef predict(**inputs):\n    global model, tokenizer, stop_token_ids  # These will have the latest values\n\n    prompt = inputs.get(\"prompt\", None)\n    if not prompt:\n        return {\"error\": \"Please provide a prompt.\"}\n\n    # Now we will format the user provided prompt so that it is of the format that\n    # the fine tuning dataset established.\n    prompt = f\"<|im_start|>user\\n{prompt}\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n\n    # We set the end of sequence token to the last token from <|im_end|>\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n    output = model.generate(\n        inputs,\n        max_length=100,\n        num_return_sequences=1,\n        use_cache=False,\n        eos_token_id=stop_token_ids[-1],\n        pad_token_id=tokenizer.eos_token_id,\n    )\n    # Here we are trimming the input length from the output so that only the newly generated text is returned.\n    text = tokenizer.decode(output[0][len(inputs[0]) :])\n    print(text)\n\n    return {\"text\": text}\n```\n\n----------------------------------------\n\nTITLE: Decorating Functions for Cloud Execution in Python\nDESCRIPTION: This snippet shows how to use the 'function()' decorator to prepare a Python function for cloud execution. It demonstrates the syntax for wrapping any code block to be run on the cloud.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/functions/README.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n@function()\n```\n\n----------------------------------------\n\nTITLE: Using on_start Lifecycle Hook for Model Preloading in Python\nDESCRIPTION: This code demonstrates how to use Beam's on_start hook to download model weights once when a container first starts. The download_models function is attached to the on_start hook, ensuring it executes exactly once at container initialization.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/preload_models/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef download_models():\n    # Implementation would go here\n    pass\n\non_start(download_models)\n```\n\n----------------------------------------\n\nTITLE: Loading Local Model Weights in Beam Function\nDESCRIPTION: Example of loading local model weights file into a Beam remote function with CPU resources allocation.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/jupyter_notebooks/beam-notebook.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom beam import function, Image\n\n# Load local model weights\nWEIGHTS_PATH = \"./weights.pth\"\n\n@function(cpu=2, memory=\"1Gi\", image=Image(python_packages=[\"torch\"]))\ndef handler():\n    import torch\n    # Load model weights from a local file\n    model = torch.load(WEIGHTS_PATH)\n    model.eval()\n    \n    return {\"success\": \"true\"} \n```\n\n----------------------------------------\n\nTITLE: GPU-Accelerated Beam Function with A100\nDESCRIPTION: Implementation of a Beam function utilizing A100 GPU acceleration with increased CPU and memory resources.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/jupyter_notebooks/beam-notebook.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom beam import function, Image\n\n\n# Runs on an A100-40 GPU in the cloud\n@function(gpu=\"A100-40\", cpu=4, memory=\"32Gi\", image=Image(python_packages=[\"torch\"]))\ndef handler():\n    import subprocess\n    \n    # Print the available GPU drivers \n    print(subprocess.check_output([\"nvidia-smi\"], shell=True))\n\n    return {\"gpu\":\"true\"}\n```\n\n----------------------------------------\n\nTITLE: Initializing GPU Function with Biopython in Beam\nDESCRIPTION: Demonstrates how to create a GPU-enabled function using Beam's decorator pattern. Configures an A100-40 GPU and includes the Biopython package as a dependency.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/bioinformatics/dnabert/README.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom beam import function, Image\n\n@function(gpu=\"A100-40\", image=Image(python_packages=[\"biopython\"]))\ndef download_files():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Keep Warm Duration in Beam Cloud Functions\nDESCRIPTION: This code snippet demonstrates how to add the keep_warm_seconds argument to Beam Cloud functions. This argument controls the duration for which the container stays active before shutting down, allowing for better performance and resource management.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/keep_warm/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Configure Keep Warm\n\nYou can add a `keep_warm_seconds` argument to your functions to control\nhow long the container should stay up before shutting down.\n```\n\n----------------------------------------\n\nTITLE: Loading Fine-Tuned LLaMA Model with PEFT\nDESCRIPTION: Initializes the fine-tuned LLaMA model using HuggingFace Transformers and PEFT library. Loads the base model and applies LORA fine-tuning weights.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/llama/README.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif env.is_remote():\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    from peft import PeftModel\n\ndef load_finetuned_model():\n    global model, tokenizer, stop_token_ids\n    print(\"Loading latest...\")\n\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_PATH, attn_implementation=\"eager\", device_map=\"auto\", is_decoder=True\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\n    # using our LORA result via the PEFT library\n    model = PeftModel.from_pretrained(model, FINETUNE_PATH)\n    print(model.config)\n\n    stop_token = \"<|im_end|>\"\n    stop_token_ids = tokenizer.encode(stop_token, add_special_tokens=False)\n```\n\n----------------------------------------\n\nTITLE: Implementing LoRA Fine-tuning Configuration\nDESCRIPTION: Python implementation of LoRA fine-tuning configuration using transformers and PEFT libraries. Sets up model, tokenizer, and training arguments for Llama3 fine-tuning.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/llama/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# finetune.py\ndef llama_fine_tune():\n    import os\n    import torch\n    from datasets import load_dataset\n    from transformers import (\n        AutoTokenizer,\n        AutoModelForCausalLM,\n        TrainingArguments,\n        Trainer,\n        DataCollatorForLanguageModeling,\n    )\n    from peft import LoraConfig, get_peft_model, TaskType\n\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n    if not torch.cuda.is_available():\n        return \"CUDA is not available\"\n\n    torch.set_float32_matmul_precision(\"high\")\n\n    # Load the Llama3 model and tokenizer\n    model = AutoModelForCausalLM.from_pretrained(\n        WEIGHT_PATH, device_map=\"auto\", attn_implementation=\"eager\", use_cache=False\n    )\n    tokenizer = AutoTokenizer.from_pretrained(WEIGHT_PATH, use_fast=False)\n    \n    # Set the pad_token to eos_token\n    tokenizer.pad_token = tokenizer.eos_token\n\n\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=TaskType.CAUSAL_LM,\n    )\n\n    model = get_peft_model(model, lora_config)\n\n    # Load the Yelp Reviews dataset from Hugging Face\n    dataset = load_dataset(DATASET_PATH)\n\n    def prepare_dataset(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n    tokenized_dataset = dataset.map(prepare_dataset, batched=True)\n\n    training_args = TrainingArguments(\n        # This output directory is on our mounted volume\n        output_dir=\"./llama-ft/llama-finetuned\",\n        num_train_epochs=1,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        learning_rate=2e-4,\n        weight_decay=0.01,\n        logging_steps=10,\n        save_steps=100,\n        save_total_limit=3,\n        fp16=True,\n        gradient_checkpointing=False,\n        remove_unused_columns=False,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n    )\n\n    trainer.train()\n\n    # Saving the LORA model and tokenizer to our mounted volume so that our inference endpoint can access it.\n    model.save_pretrained(\"./llama-ft/llama-finetuned\")\n    tokenizer.save_pretrained(\"./llama-ft/llama-finetuned\")\n```\n\n----------------------------------------\n\nTITLE: Defining Cloud Function for GenBank Data Download (Python)\nDESCRIPTION: This snippet defines a Beam function to download plasmid data from GenBank. It uses the Biopython library and stores the downloaded data in a Beam Volume.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/bioinformatics/dnabert/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom beam import function, Image, Volume, env\nimport os\n\nif env.is_remote():\n    from Bio import Entrez, SeqIO\n\nimage = Image(python_packages=[\"biopython\"])\nBEAM_VOLUME_PATH = \"./seq\"\n\n@function(volumes=[Volume(name=\"seq\", mount_path=BEAM_VOLUME_PATH)], image=image)\ndef download(accession_number):\n    Entrez.email = \"your.email@example.com\"\n\n    with Entrez.efetch(\n        db=\"nucleotide\", id=accession_number, rettype=\"gb\", retmode=\"text\"\n    ) as handle:\n        record = SeqIO.read(handle, \"genbank\")\n\n    file_path = os.path.join(BEAM_VOLUME_PATH, f\"{accession_number}.gb\")\n    SeqIO.write(record, open(file_path, \"w\"), \"genbank\")\n\n    print(\n        f\"Sequence ID: {record.id}\\nLength: {len(record.seq)}\\nDescription: {record.description}\"\n    )\n\nif __name__ == \"__main__\":\n    download.remote(\"CM004190.1\")\n```\n\n----------------------------------------\n\nTITLE: Defining Cloud Functions for ML Embedding Generation (Python)\nDESCRIPTION: This snippet defines Beam functions for generating ML embeddings from plasmid sequences. It includes functions for reading DNA sequences, generating embeddings, and chunking sequences for parallel processing.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/bioinformatics/dnabert/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom beam import function, Image, Volume, env, Output\n\nif env.is_remote():\n    import torch\n    from transformers import AutoModel, AutoTokenizer\n    from Bio import SeqIO\n\nCHECKPOINT = \"RaphaelMourad/Mistral-DNA-v1-422M-hg38\"\nBEAM_VOLUME_PATH = \"./cached_models\"\nDNA_FILE_PATH = \"./seq/AE017046.1.gb\"\nCHUNK_SIZE = 3000\n\ndef read_dna_sequence(file_path):\n    records = list(SeqIO.parse(file_path, \"genbank\"))\n    return \"\".join([str(record.seq) for record in records if record.seq])\n\n@function(\n    secrets=[\"HF_TOKEN\"],\n    name=\"dnabert\",\n    cpu=4,\n    memory=\"32Gi\",\n    image=Image(\n        python_version=\"python3.11\",\n        python_packages=[\n            \"transformers\",\n            \"sentencepiece==0.1.99\",\n            \"accelerate==0.23.0\",\n            \"torch\",\n            \"biopython\",\n            \"einops\",\n        ],\n    ),\n    volumes=[\n        Volume(name=\"cached_models\", mount_path=BEAM_VOLUME_PATH),\n        Volume(name=\"seq\", mount_path=\"./seq\"),\n    ],\n)\ndef generate_embeddings(data):\n    dna_chunk = data[\"chunk\"]\n    chunk_index = data[\"index\"]\n\n    model = AutoModel.from_pretrained(\n        CHECKPOINT, cache_dir=BEAM_VOLUME_PATH, trust_remote_code=True\n    )\n    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT, trust_remote_code=True)\n\n    inputs = tokenizer(dna_chunk, return_tensors=\"pt\")[\"input_ids\"]\n    hidden_states = model(inputs)[0]\n    embedding_max = torch.max(hidden_states[0], dim=0)[0]\n\n    file_path = f\"/tmp/embedding_chunk_{chunk_index}.txt\"\n    with open(file_path, \"w\") as f:\n        f.write(\"\\n\".join(map(str, embedding_max.tolist())))\n\n    output_file = Output(path=file_path)\n    output_file.save()\n\n    return {\"output_url\": output_file.public_url()}\n\ndef chunk_sequence(sequence, chunk_size):\n    return [sequence[i : i + chunk_size] for i in range(0, len(sequence), chunk_size)]\n\n@function(\n    image=Image(\n        python_version=\"python3.11\",\n        python_packages=[\n            \"transformers\",\n            \"sentencepiece==0.1.99\",\n            \"accelerate==0.23.0\",\n            \"torch\",\n            \"biopython\",\n            \"einops\",\n        ],\n    ),\n    volumes=[Volume(name=\"seq\", mount_path=\"./seq\")],\n)\ndef main():\n    dna_sequence = read_dna_sequence(DNA_FILE_PATH)\n    if dna_sequence:\n        dna_chunks = chunk_sequence(dna_sequence, CHUNK_SIZE)\n        chunk_data = [\n            {\"chunk\": chunk, \"index\": index} for index, chunk in enumerate(dna_chunks)\n        ]\n        results = generate_embeddings.map(chunk_data)\n        for result in results:\n            print(f\"Embedding saved to: {result}\")\n\nif __name__ == \"__main__\":\n    main.remote()\n```\n\n----------------------------------------\n\nTITLE: Setting up Beam Inference Endpoint\nDESCRIPTION: Configuration for deploying the inference endpoint on Beam, including path definitions and environment setup\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/llama/README.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# inference.py\nfrom beam import Image, endpoint, env, Volume, QueueDepthAutoscaler, experimental\n\nMOUNT_PATH = \"./llama-ft\"\nFINETUNE_PATH = \"./llama-ft/llama-finetuned\"\nMODEL_PATH = \"./llama-ft/weights\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Secrets in a Beam Function\nDESCRIPTION: Python example showing how to access secrets as environment variables within a Beam function. The secret is specified in the function decorator using the 'secrets' parameter and can then be retrieved using os.environ.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/secrets/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom beam import function\n\nos.environ[\"FOO\"] = \"bar\"\n\n\n@function(secrets=[\"FOO\"])\ndef handler():\n    import os\n\n    my_secret = os.environ[\"FOO\"]\n    return f\"secret {my_secret}\"\n```\n\n----------------------------------------\n\nTITLE: Executing Cloud Functions Remotely in Python\nDESCRIPTION: This snippet demonstrates how to invoke a cloud function remotely using the '.remote()' method. This allows the function to be executed in the cloud environment.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/functions/README.md#2025-04-07_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.remote()\n```\n\n----------------------------------------\n\nTITLE: Parallel Processing with Cloud Functions in Python\nDESCRIPTION: This example shows how to use the '.map()' method to distribute workloads across multiple containers in the cloud. It can be used to scale processing from a few to hundreds or thousands of containers.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/functions/README.md#2025-04-07_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.map()\n```\n\n----------------------------------------\n\nTITLE: Using Queue() for State Sharing in Beam Cloud\nDESCRIPTION: Demonstrates the Queue() abstraction usage for sharing state between tasks. The Queue() provides a thread-safe distributed queue that can be accessed both locally and from remote containers.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/sharing_state/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nQueue()\n```\n\n----------------------------------------\n\nTITLE: Invoking Cloud Functions Locally in Python\nDESCRIPTION: This example illustrates how to call a cloud function locally using the '.local()' method. This allows for testing and debugging of cloud functions in a local environment.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/functions/README.md#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.local()\n```\n\n----------------------------------------\n\nTITLE: Running Batched Web Scraping Example in Python\nDESCRIPTION: This command executes the batched web scraping example script. It uses the 'map' method from the 'function' decorator to launch remote function calls for each element of the input list.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/web_scraping/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython batch_crawl.py\n```\n\n----------------------------------------\n\nTITLE: Running Continuous Web Scraping Example in Python\nDESCRIPTION: This command runs the continuous web scraping example script. It utilizes a thread pool to launch remote function calls in parallel, initiating new calls as soon as slots become available.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/web_scraping/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython continuous_crawl.py\n```\n\n----------------------------------------\n\nTITLE: Executing Remote Beam Function\nDESCRIPTION: Simple example of calling a Beam function remotely.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/jupyter_notebooks/beam-notebook.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Run the cell on remotely on Beam\nhandler.remote()\n```\n\n----------------------------------------\n\nTITLE: Creating a Secret in Beam CLI\nDESCRIPTION: Command to create a secret in Beam CLI for storing the Hugging Face token. This step is necessary for securely accessing the Mixtral 7B model.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/language_models/mixtral_7b/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n$ beam secret create [SECRET]\n```\n\n----------------------------------------\n\nTITLE: Creating Hugging Face Token Secret in Beam\nDESCRIPTION: This command creates a secret in Beam to store the Hugging Face token, which is required for accessing the Meta Llama 3 8B Instruct model.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/language_models/llama3_8b/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n$ beam secret create HF_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Creating Huggingface API Token Secret in Beam\nDESCRIPTION: This command creates a secret in Beam to store the Huggingface API token. It's a prerequisite for using Flux in the serverless application.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/flux/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbeam secret create HF_TOKEN [YOUR-HF-TOKEN]\n```\n\n----------------------------------------\n\nTITLE: Creating Secrets using Beam CLI\nDESCRIPTION: Command line syntax for creating secrets in Beam Cloud using the CLI. This command creates a secret with the specified key-value pair that can later be accessed in applications.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/secrets/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nbeam secret create [KEY] [VALUE]\n```\n\n----------------------------------------\n\nTITLE: Uploading Model Data to Beam Volume\nDESCRIPTION: CLI commands to create a Beam volume and upload model weights and fine-tuning dataset\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/llama/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n// using weights as the Volume name\n$ beam volume create llama-ft\n\n// assuming your weights are saved locally to local_weights\n$ beam cp local_weights llama-ft/weights\n\n// assuming your fine-tuning dataset is saved locally as local_dataset\n$ beam cp local_dataset llama-ft/data\n```\n\n----------------------------------------\n\nTITLE: Viewing fine-tuned model files on Beam volume\nDESCRIPTION: This bash command and its output demonstrate how to use the Beam CLI to list the files generated during fine-tuning, showing the adapter model, checkpoints, and tokenizer files stored in the volume.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/gemma/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n❯ beam ls gemma-ft/gemma-2b-finetuned\n\n  Name                                                Size   Modified Time   IsDir\n ──────────────────────────────────────────────────────────────────────────────────\n  gemma-2b-finetuned/README.md                    4.97 KiB   Aug 10 2024     No\n  gemma-2b-finetuned/adapter_config.json          644.00 B   Aug 10 2024     No\n  gemma-2b-finetuned/adapter_model.safetensors   12.20 MiB   Aug 10 2024     No\n  gemma-2b-finetuned/checkpoint-700              36.70 MiB   Aug 01 2024     Yes\n  gemma-2b-finetuned/checkpoint-800              36.70 MiB   Aug 01 2024     Yes\n  gemma-2b-finetuned/checkpoint-809              36.70 MiB   Aug 01 2024     Yes\n  gemma-2b-finetuned/special_tokens_map.json      555.00 B   Aug 10 2024     No\n  gemma-2b-finetuned/tokenizer.json              16.71 MiB   Aug 10 2024     No\n  gemma-2b-finetuned/tokenizer_config.json       45.21 KiB   Aug 10 2024     No\n\n  9 items | 139.06 MiB used\n```\n\n----------------------------------------\n\nTITLE: Configuring Beam Deployment\nDESCRIPTION: Configuration setup for deploying the fine-tuning script on Beam's cloud infrastructure with GPU specifications and volume mounting\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/llama/README.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# finetune.py\n# Deploy to beam by running `$ python finetune.py` in the terminal\nfrom beam import Volume, Image, function, env\n\n# The mount path is the location on the beam volume that we will access. \nMOUNT_PATH = \"./llama-ft\"\nWEIGHT_PATH = \"./llama-ft/weights\"\nDATASET_PATH = \"./llama-ft/data\"\n\n@function(\n    secrets=[\"HF_TOKEN\"],\n    volumes=[Volume(name=\"llama-ft\", mount_path=MOUNT_PATH)],\n    image=Image(\n        python_packages=[\"transformers\", \"torch\", \"datasets\", \"peft\", \"bitsandbytes\"]\n    ),\n    gpu=\"A100-40\",\n    cpu=4,\n)\ndef llama_fine_tune():\n   # this is unchanged\n\nif __name__ == \"__main__\":\\\n    llama_fine_tune.remote()\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Inference on Beam\nDESCRIPTION: This command runs the inference.py script to perform one-off inference using the Yi-Coder-9B-Chat model on the Beam platform.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/vllm/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython inference.py\n```\n\n----------------------------------------\n\nTITLE: Running Chat Client for Interacting with Deployed Models\nDESCRIPTION: This command runs the chat.py script, which provides a simple chat client for interacting with the deployed LLM models through their OpenAI-compatible APIs.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/vllm/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython chat.py\n```\n\n----------------------------------------\n\nTITLE: Running Cloud Function for GenBank Data Download (Shell)\nDESCRIPTION: This snippet shows how to execute the Beam function for downloading GenBank data using a simple Python command. The function runs on the cloud instead of the local machine.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/bioinformatics/dnabert/README.md#2025-04-07_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n$ python download-dna.py\n\n=> Running function: <download-dna:download>\nSequence ID: CM004190.1\nLength: 33445071\nDescription: Pan troglodytes isolate Yerkes chimp pedigree #C0471 (Clint) chromosome 21, whole genome shotgun\nsequence\n=> Function complete <8bb1e80b-533c-4f91-8eba-e8a6f899ed7c>\n```\n\n----------------------------------------\n\nTITLE: Running Cloud Functions for ML Embedding Generation (Shell)\nDESCRIPTION: This snippet demonstrates how to execute the Beam functions for generating ML embeddings. It shows the parallel execution of multiple containers for processing different chunks of the plasmid sequence.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/bioinformatics/dnabert/README.md#2025-04-07_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n$ python app.py\n\n=> Running function: <app:main>\n=> Building image\n=> Using cached image\n=> Syncing files\n=> Files synced\n=> Running function: <app:generate_embeddings>\n=> Running function: <app:generate_embeddings>\n=> Running function: <app:generate_embeddings>\n=> Running function: <app:generate_embeddings>\n=> Function complete <4c4ea387-f8cf-499e-9470-aba852f9a6c3>\nEmbedding saved to: {'output_url': 'https://app.beam.cloud/output/id/53f03dfe-6564-4f89-8657-888dd01ceb62'}\n=> Function complete <b38ca7f5-462d-4b96-a244-fc2ffb6ef2ad>\nEmbedding saved to: {'output_url': 'https://app.beam.cloud/output/id/9a74a538-004d-4203-a09f-21dd9c488b67'}\n=> Function complete <f962cbe9-966f-42e3-9f6b-e609996984b4>\nEmbedding saved to: {'output_url': 'https://app.beam.cloud/output/id/1c62ebce-6a65-429e-b32d-07bc6fe6b2bd'}\n=> Function complete <3a988ff8-1fa0-4804-898a-1e4dc3d30c46>\nEmbedding saved to: {'output_url': 'https://app.beam.cloud/output/id/881afdc9-e13a-4aa7-8599-b41be5f60f80'}\n=> Function complete <436eadd4-b5d4-45af-8f33-a00c8f3e3b77>\n```\n\n----------------------------------------\n\nTITLE: Installing and Configuring Beam SDK\nDESCRIPTION: Initial setup for Beam SDK in a Jupyter notebook including installation via pip and API token configuration.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/jupyter_notebooks/beam-notebook.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install beam-client\n!pip install beam-client\n\n# Import the Beam client\nimport beam\n\n# Add your Beam API key\n!beam configure default --token [YOUR-BEAM-TOKEN]\n\n!beam config select default\n```\n\n----------------------------------------\n\nTITLE: Executing File Output Example in Python\nDESCRIPTION: Simple command to run the file output example application using Python\nSOURCE: https://github.com/beam-cloud/examples/blob/main/outputs/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython app.py\n```\n\n----------------------------------------\n\nTITLE: Running Python Script for Cloud Function Execution\nDESCRIPTION: This command demonstrates how to execute the Python script containing cloud functions. It runs the script in the local shell, which then interacts with the cloud environment.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/functions/README.md#2025-04-07_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\npython [app.py]\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment and Installing Dependencies\nDESCRIPTION: Commands to set up a Python virtual environment and install the required packages (Reflex and Beam client).\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/sdxl/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m virtualenv .venv && source .venv/bin/activate\n```\n\nLANGUAGE: bash\nCODE:\n```\npip3 install reflex beam-client\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment and Installing Dependencies\nDESCRIPTION: Commands to set up a Python virtual environment and install required packages Reflex and Beam client\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/sdxl_turbo/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m virtualenv .venv && source .venv/bin/activate\n```\n\nLANGUAGE: bash\nCODE:\n```\npip3 install reflex beam-client\n```\n\n----------------------------------------\n\nTITLE: Virtual Environment Setup Commands\nDESCRIPTION: Commands to create and activate a Python virtual environment and install the Beam client.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/sdxl_turbo_streaming/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m virtualenv .venv && source .venv/bin/activate\n```\n\nLANGUAGE: bash\nCODE:\n```\npip3 install beam-client\n```\n\n----------------------------------------\n\nTITLE: Downloading Flux Example from Beam\nDESCRIPTION: This command downloads the Flux example from Beam and changes the current directory to the downloaded example folder.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/flux/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbeam example download image_generation/flux && cd image_generation/flux\n```\n\n----------------------------------------\n\nTITLE: Beam CLI Task Management Commands\nDESCRIPTION: Common Beam CLI commands for managing tasks and containers.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/jupyter_notebooks/beam-notebook.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# List all tasks you ran\nbeam task list\n\n# List all containers running \nbeam container list\n```\n\n----------------------------------------\n\nTITLE: Executing Beam Task List Command\nDESCRIPTION: Example of running Beam task list command in Jupyter notebook.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/jupyter_notebooks/beam-notebook.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# List all tasks you ran\n!beam task list\n```\n\n----------------------------------------\n\nTITLE: Starting Frontend Application\nDESCRIPTION: Command to start the Reflex frontend application locally\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/sdxl_turbo/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd frontend && reflex run\n```\n\n----------------------------------------\n\nTITLE: Frontend Setup and Run Commands\nDESCRIPTION: Series of commands to set up and run the Next.js frontend application, including dependency installation and development server startup.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/sdxl_turbo_streaming/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd frontend\nnpm install\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Terminal output during Gemma fine-tuning process\nDESCRIPTION: This terminal output shows the progress of the fine-tuning process, including loading model checkpoints, generating training data, and the training loop statistics including loss and learning rate information.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/gemma/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n=> Building image \n=> Syncing files \n...\n=> Running function: <finetune:gemma_fine_tune> \nLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n...\nGenerating train split: 12947 examples [00:00, 114393.80 examples/s]\n...\nMap:  93%|#########2| 12000/12947 [00:13<00:01, 921.12 examples/s]\n...\n  1%|          | 6/809 [00:08<16:35,  1.24s/it]\n...\n{'loss': 1.617, 'grad_norm': 0.4805833399295807, 'learning_rate': 0.00019752781211372064, 'epoch': 0.01}\n...\n```\n\n----------------------------------------\n\nTITLE: Terminal output during Beam endpoint deployment\nDESCRIPTION: This terminal output shows the process of building, syncing, and deploying the Gemma inference endpoint on Beam, including the generated curl command template for invoking the endpoint.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/gemma/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n=> Building image\n=> Syncing files \n=> Deploying \n=> Deployed 🎉 \n=> Invocation details \ncurl -X POST 'https://app.beam.cloud/endpoint/gemma-ft/v2' \\\n-H 'Connection: keep-alive' \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer {YOUR_AUTH_TOKEN}' \\\n-d '{}'\n```\n\n----------------------------------------\n\nTITLE: Making API Call to Deployed Endpoint\nDESCRIPTION: This curl command demonstrates how to make a POST request to the deployed Beam endpoint. It includes the necessary headers and a JSON payload with system and user messages for the Meta Llama 3 8B Instruct model.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/language_models/llama3_8b/README.md#2025-04-07_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncurl -X POST 'https://app.beam.cloud/endpoint/id/[ENDPOINT-ID]' \\\n-H 'Connection: keep-alive' \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer [AUTH-TOKEN]' \\\n-d '{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a yoda chatbot who always responds in yoda speak!\"},\n        {\"role\": \"user\", \"content\": \"Who are you?\"}\n    ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Making API Request to Zonos TTS with curl\nDESCRIPTION: Example curl command to make a POST request to the Zonos TTS API endpoint. The request includes an authorization bearer token and a JSON payload with the text to be converted to speech.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/zonos/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST 'https://ff6e671a-c43d-468e-ab21-df87c8d87afb.app.beam.cloud' \\\n-H 'Connection: keep-alive' \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer dERxHlMCz4TDU9k7cxZte_FrILTCvs0nf3KSVe8oZumoTAOa4OIkpJiyGOq_hS9nyangjUG6GC9VmswWd_Rt4g==' \\\n-d '{ \"text\": \"On Beam run AI workloads anywhere with zero complexity. One line of Python, global GPUs, full control\"}'\n```\n\n----------------------------------------\n\nTITLE: API Request Payload Structure\nDESCRIPTION: JSON structure for the API request payload showing required fields: prompt (text to convert) and description (voice/style specifications).\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/parler-tts/README.md#2025-04-07_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"prompt\": \"Your text to convert to speech\",\n    \"description\": \"Description of the voice/style\"\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking Faster Whisper API with cURL\nDESCRIPTION: Example cURL command to invoke the deployed Faster Whisper API endpoint with an MP3 file URL. The command includes the necessary headers for authentication and content type specification.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/faster_whisper/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST 'https://app.beam.cloud/endpoint/id/[YOUR-ENDPOINT-ID]' \\\n-H 'Connection: keep-alive' \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer [YOUR-AUTH-TOKEN]' \\\n-d '{\"url\":\"http://commondatastorage.googleapis.com/codeskulptor-demos/DDR_assets/Kangaroo_MusiQue_-_The_Neverwritten_Role_Playing_Game.mp3\"}'\n```\n\n----------------------------------------\n\nTITLE: Structure of API Request for Text-to-Video Generation\nDESCRIPTION: The expected JSON format for sending a text prompt to the Mochi-1 API service.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/video_models/mochi1/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{\n    \"prompt\": \"Your prompt for video\",\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring API Request in Python\nDESCRIPTION: Python code snippet for setting up the API request. It includes the URL and headers required for authentication and specifying the content type.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/object_detection/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nurl = 'https://app.beam.cloud/endpoint/id/[ENDPOINT-ID]'\nheaders = {\n    'Connection': 'keep-alive',\n    'Content-Type': 'application/json',\n    'Authorization': 'Bearer [AUTH_TOKEN]'\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring API Request Parameters in Python\nDESCRIPTION: This code snippet sets up the necessary parameters for making a request to the WhisperX transcription API, including the authentication token, deployment URL, and the URL of the audio file to be transcribed.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/whisperx_stt/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nAUTH_TOKEN = \"BEAM_AUTH_TOKEN\"\nBEAM_URL = \"id/8836f704-b521-4e1c-8979-bc74c97dc47b\"\nAUDIO_URL = \"\"\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark Script\nDESCRIPTION: Command to execute the benchmarking script that tests the deployed API's performance including inference and cold boot times.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/whisper_stt/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython request.py\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI-compatible API for Image Analysis with InternVL2\nDESCRIPTION: Python client code to interact with the deployed vLLM API using OpenAI's SDK. This example shows how to analyze an image by sending it to the InternVL2 5-8B model through the API.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/vllm/vision_models/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nimport requests\nfrom openai import OpenAI\n\nopenai_api_key = \"your-beam-token\"\nopenai_api_base = \"https:your-beam-app/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nimage_url = \"https://tinypng.com/static/images/boat.png\"\n\nchat_completion_from_url = client.chat.completions.create(\n        messages=[{\n            \"role\":\n            \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"What's in this image?\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": image_url\n                    },\n                },\n            ],\n        }],\n        model=\"InternVL2_5-8B\"\n    )\nprint(chat_completion_from_url)\n```\n\n----------------------------------------\n\nTITLE: Python API Request Example\nDESCRIPTION: Example of making an API request to the deployed endpoint using Python requests library.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/llama/README.md#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresponse = requests.post(\n    \"https://app.beam.cloud/endpoint/llama-ft/v2\", \n    headers={\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": \"Bearer YOUR_AUTH_TOKEN\"\n    }, \n    json={\n        \"prompt\": \"hi\"\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: JSON Request Format for Zonos TTS API\nDESCRIPTION: Sample JSON payload structure for the Zonos TTS API. The payload should include a 'text' field containing the string to be converted to speech.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/zonos/README.md#2025-04-07_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \"Your text to convert to speech\",\n}\n```\n\n----------------------------------------\n\nTITLE: Example API Request with Detailed Voice Description\nDESCRIPTION: Complete example of an API request with a specific prompt and detailed voice description for high-quality female voice output.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/parler-tts/README.md#2025-04-07_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"prompt\": \"On Beam run AI workloads anywhere with zero complexity. One line of Python, global GPUs, full control\",\n    \"description\": \"A female speaker delivers a slightly expressive and animated speech with a moderate speed and pitch. The recording is of very high quality, with the speaker's voice sounding clear and very close up.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Request for Mochi-1 Video Generation\nDESCRIPTION: A detailed example of a text prompt describing a jungle scene with a Jeep, formatted as a JSON request to the API.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/video_models/mochi1/README.md#2025-04-07_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"prompt\": \"The camera follows behind a rugged green Jeep with a black snorkel as it speeds along a narrow dirt trail cutting through a dense jungle. Thick vines hang from towering trees with sprawling canopies, their leaves forming a vibrant green tunnel above the vehicle. Mud splashes up from the Jeep's tires as it powers through a shallow stream crossing the path. Sunlight filters through gaps in the trees, casting dappled golden light over the scene. The dirt trail twists sharply into the distance, overgrown with wild ferns and tropical plants. The vehicle is seen from the rear, leaning into the curve as it maneuvers through the untamed terrain, emphasizing the adventure of the rugged journey. The surrounding jungle is alive with texture and color, with distant mountains barely visible through the mist and an overcast sky heavy with the promise of rain.\",\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying the Object Detection Endpoint for Production\nDESCRIPTION: Command to deploy the object detection endpoint for production use on Beam. This creates a persistent endpoint.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/object_detection/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbeam deploy app.py:predict\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Response from WhisperX Transcription API\nDESCRIPTION: This JSON object represents a typical response from the WhisperX transcription API. It includes segmented transcriptions with timing information and confidence scores for each word.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/whisperx_stt/README.md#2025-04-07_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"result\":{\"segments\":[{\"start\":0.309,\"end\":3.133,\"text\":\" My thought, I have nobody by a beauty and will as you t'ward.\",\"words\":[{\"word\":\"My\",\"start\":0.309,\"end\":0.45,\"score\":0.93},{\"word\":\"thought,\",\"start\":0.49,\"end\":0.85,\"score\":0.863},{\"word\":\"I\",\"start\":0.91,\"end\":0.97,\"score\":0.999},{\"word\":\"have\",\"start\":1.01,\"end\":1.191,\"score\":0.874},{\"word\":\"nobody\",\"start\":1.251,\"end\":1.571,\"score\":0.91},{\"word\":\"by\",\"start\":1.611,\"end\":1.751,\"score\":0.863},{\"word\":\"a\",\"start\":1.791,\"end\":1.832,\"score\":0.975},{\"word\":\"beauty\",\"start\":1.872,\"end\":2.152,\"score\":0.836},{\"word\":\"and\",\"start\":2.192,\"end\":2.272,\"score\":0.82},{\"word\":\"will\",\"start\":2.292,\"end\":2.472,\"score\":0.853},{\"word\":\"as\",\"start\":2.512,\"end\":2.593,\"score\":0.838},{\"word\":\"you\",\"start\":2.613,\"end\":2.753,\"score\":0.842},{\"word\":\"t'ward.\",\"start\":2.793,\"end\":3.133,\"score\":0.217}]},{\"start\":3.874,\"end\":9.943,\"text\":\"Mr. Rochester is sub, and that so don't find simpus, and devoted abode, to hath might in a\",\"words\":[{\"word\":\"Mr.\",\"start\":3.874,\"end\":4.175,\"score\":0.563},{\"word\":\"Rochester\",\"start\":4.235,\"end\":4.756,\"score\":0.94},{\"word\":\"is\",\"start\":4.836,\"end\":4.916,\"score\":0.816},{\"word\":\"sub,\",\"start\":4.936,\"end\":5.236,\"score\":0.877},{\"word\":\"and\",\"start\":5.276,\"end\":5.356,\"score\":0.802},{\"word\":\"that\",\"start\":5.397,\"end\":5.577,\"score\":0.948},{\"word\":\"so\",\"start\":5.617,\"end\":5.777,\"score\":0.982},{\"word\":\"don't\",\"start\":5.817,\"end\":6.017,\"score\":0.863},{\"word\":\"find\",\"start\":6.057,\"end\":6.358,\"score\":0.873},{\"word\":\"simpus,\",\"start\":6.398,\"end\":6.839,\"score\":0.865},{\"word\":\"and\",\"start\":7.399,\"end\":7.499,\"score\":0.884},{\"word\":\"devoted\",\"start\":7.54,\"end\":7.92,\"score\":0.969},{\"word\":\"abode,\",\"start\":8.0,\"end\":8.461,\"score\":0.635},{\"word\":\"to\",\"start\":9.102,\"end\":9.222,\"score\":0.839},{\"word\":\"hath\",\"start\":9.262,\"end\":9.402,\"score\":0.65},{\"word\":\"might\",\"start\":9.442,\"end\":9.703,\"score\":0.855},{\"word\":\"in\",\"start\":9.783,\"end\":9.883,\"score\":0.8},{\"word\":\"a\",\"start\":9.923,\"end\":9.943,\"score\":0.97}]}],\"word_segments\":[{\"word\":\"My\",\"start\":0.309,\"end\":0.45,\"score\":0.93},{\"word\":\"thought,\",\"start\":0.49,\"end\":0.85,\"score\":0.863},{\"word\":\"I\",\"start\":0.91,\"end\":0.97,\"score\":0.999},{\"word\":\"have\",\"start\":1.01,\"end\":1.191,\"score\":0.874},{\"word\":\"nobody\",\"start\":1.251,\"end\":1.571,\"score\":0.91},{\"word\":\"by\",\"start\":1.611,\"end\":1.751,\"score\":0.863},{\"word\":\"a\",\"start\":1.791,\"end\":1.832,\"score\":0.975},{\"word\":\"beauty\",\"start\":1.872,\"end\":2.152,\"score\":0.836},{\"word\":\"and\",\"start\":2.192,\"end\":2.272,\"score\":0.82},{\"word\":\"will\",\"start\":2.292,\"end\":2.472,\"score\":0.853},{\"word\":\"as\",\"start\":2.512,\"end\":2.593,\"score\":0.838},{\"word\":\"you\",\"start\":2.613,\"end\":2.753,\"score\":0.842},{\"word\":\"t'ward.\",\"start\":2.793,\"end\":3.133,\"score\":0.217},{\"word\":\"Mr.\",\"start\":3.874,\"end\":4.175,\"score\":0.563},{\"word\":\"Rochester\",\"start\":4.235,\"end\":4.756,\"score\":0.94},{\"word\":\"is\",\"start\":4.836,\"end\":4.916,\"score\":0.816},{\"word\":\"sub,\",\"start\":4.936,\"end\":5.236,\"score\":0.877},{\"word\":\"and\",\"start\":5.276,\"end\":5.356,\"score\":0.802},{\"word\":\"that\",\"start\":5.397,\"end\":5.577,\"score\":0.948},{\"word\":\"so\",\"start\":5.617,\"end\":5.777,\"score\":0.982},{\"word\":\"don't\",\"start\":5.817,\"end\":6.017,\"score\":0.863},{\"word\":\"find\",\"start\":6.057,\"end\":6.358,\"score\":0.873},{\"word\":\"simpus,\",\"start\":6.398,\"end\":6.839,\"score\":0.865},{\"word\":\"and\",\"start\":7.399,\"end\":7.499,\"score\":0.884},{\"word\":\"devoted\",\"start\":7.54,\"end\":7.92,\"score\":0.969},{\"word\":\"abode,\",\"start\":8.0,\"end\":8.461,\"score\":0.635},{\"word\":\"to\",\"start\":9.102,\"end\":9.222,\"score\":0.839},{\"word\":\"hath\",\"start\":9.262,\"end\":9.402,\"score\":0.65},{\"word\":\"might\",\"start\":9.442,\"end\":9.703,\"score\":0.855},{\"word\":\"in\",\"start\":9.783,\"end\":9.883,\"score\":0.8},{\"word\":\"a\",\"start\":9.923,\"end\":9.943,\"score\":0.97}]}}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Request for Zonos TTS API\nDESCRIPTION: Concrete example of a JSON request payload for the Zonos TTS API, containing a sample text about running AI workloads on Beam.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/audio_and_transcription/zonos/README.md#2025-04-07_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \"On Beam run AI workloads anywhere with zero complexity. One line of Python, global GPUs, full control\",\n}\n```\n\n----------------------------------------\n\nTITLE: Example response from Gemma inference endpoint\nDESCRIPTION: This JSON output shows the format of the response returned from the deployed Gemma inference endpoint, including the generated text with the stop token marker at the end.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/gemma/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n{\"text\":\"Hello! How can I help you today?<|im_end|>\"}\n```\n\n----------------------------------------\n\nTITLE: Sample Interaction with Deployed LLM Model\nDESCRIPTION: This snippet shows a sample interaction with the deployed InternVL2_5-8B model, demonstrating its ability to analyze and describe an image provided via URL.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/vllm/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nWelcome to the CLI Chat Application!\nType 'quit' to exit the conversation.\nEnter the app URL: https://internvl-instruct-15c4487-v1.app.beam.cloud\nModel OpenGVLab/InternVL2_5-8B is ready\nQuestion: What is in this image?\nImage link (press enter to skip): https://upload.wikimedia.org/wikipedia/commons/8/86/Wood.duck.arp.jpg\nAssistant:  The image captures a vibrant wood duck in mid-flight, its wings spread wide as it soars through a lush field dotted with yellow flowers. The duck's head is adorned with striking red and black markings, while its body is a mix of green, white, and brown feathers. The perspective of the photo is from below, placing the duck in the center and giving a sense of its impressive wingspan. The background is a vivid green, filled with various shades of green and yellow flowers, providing a stark contrast to the duck's colorful plumage. The image is a beautiful representation of wildlife in its natural habitat\n```\n\n----------------------------------------\n\nTITLE: API Response Example\nDESCRIPTION: Example JSON response from the deployed LLaMA model endpoint.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/finetuning/llama/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n{\"text\":\"Hello! How can I help you today?<|im_end|>\"}\n```\n\n----------------------------------------\n\nTITLE: HTML Image Alignment in Markdown\nDESCRIPTION: HTML snippet embedded in markdown for centering and displaying the Beam logo banner image with specific width attributes.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/README.md#2025-04-07_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<p align=\"center\">\n<img alt=\"Logo\" src=\"https://slai-demo-datasets.s3.amazonaws.com/beam-banner.svg\"/ width=\"500\">\n</p>\n```\n\n----------------------------------------\n\nTITLE: Verifying Downloaded GenBank Data (Shell)\nDESCRIPTION: This snippet demonstrates how to use the Beam CLI to verify that the GenBank data was successfully downloaded and stored in the Beam Volume.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/bioinformatics/dnabert/README.md#2025-04-07_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n$ beam ls seq\n\n  Name                           Size   Modified Time   IsDir\n ─────────────────────────────────────────────────────────────\n  CM004190.1.gb              7.92 KiB   5 minutes ago   No\n```\n\n----------------------------------------\n\nTITLE: Specifying Reflex Framework Version Dependency\nDESCRIPTION: This snippet defines the required version of the Reflex framework as 0.5.2. It follows the standard pip requirements.txt format for Python package version pinning.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/sdxl/frontend/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nreflex==0.5.2\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements Definition\nDESCRIPTION: Defines required Python package dependencies for the project. Specifies Reflex version 0.5.2 as a dependency using pip syntax.\nSOURCE: https://github.com/beam-cloud/examples/blob/main/image_generation/sdxl_turbo/frontend/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nreflex==0.5.2\n```"
  }
]
[
  {
    "owner": "dask",
    "repo": "dask",
    "content": "TITLE: Importing Dask Modules in Python\nDESCRIPTION: Demonstrates the standard imports for working with Dask, including NumPy and pandas for data manipulation, along with Dask's DataFrame, Array, and Bag modules.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> import pandas as pd\n\n>>> import dask.dataframe as dd\n>>> import dask.array as da\n>>> import dask.bag as db\n```\n\n----------------------------------------\n\nTITLE: Processing Large Datasets with Dask DataFrames\nDESCRIPTION: Shows how to use Dask DataFrames to process large datasets in parallel using pandas-like syntax.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\n\n# Read large datasets in parallel\ndf = dd.read_parquet(\"s3://mybucket/data.*.parquet\")\ndf = df[df.value < 0]\nresult = df.groupby(df.name).amount.mean()\n\nresult = result.compute()  # Compute to get pandas result\nresult.plot()\n```\n\n----------------------------------------\n\nTITLE: Importing Dask Core Functions in Python\nDESCRIPTION: This snippet shows how to import core Dask functions for computation, persistence, and visualization. These functions work with any scheduler and are essential for general Dask operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/api.rst#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom dask import compute, is_dask_collection, optimize, persist, visualize\n```\n\n----------------------------------------\n\nTITLE: Chaining Methods with Dask DataFrame\nDESCRIPTION: Shows how to chain multiple operations on a Dask DataFrame, creating a complex computation that is only executed when compute() is called.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> result = ddf[\"2021-10-01\": \"2021-10-09 5:00\"].a.cumsum() - 100\n... result\nDask Series Structure:\nnpartitions=1\n2021-10-01 00:00:00.000000000    int64\n2021-10-09 05:00:59.999999999      ...\nName: a, dtype: int64\nDask Name: sub, 16 tasks\n\n>>> result.compute()\n2021-10-01 00:00:00       620\n2021-10-01 01:00:00      1341\n2021-10-01 02:00:00      2063\n2021-10-01 03:00:00      2786\n2021-10-01 04:00:00      3510\n                         ...  \n2021-10-09 01:00:00    158301\n2021-10-09 02:00:00    159215\n2021-10-09 03:00:00    160130\n2021-10-09 04:00:00    161046\n2021-10-09 05:00:00    161963\nFreq: H, Name: a, Length: 198, dtype: int64\n```\n\n----------------------------------------\n\nTITLE: Creating a Dask Array from NumPy Array\nDESCRIPTION: Shows how to create a Dask Array from a NumPy array by specifying chunk sizes, which determines how the data will be partitioned for parallel processing.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport dask.array as da\n\ndata = np.arange(100_000).reshape(200, 500)\na = da.from_array(data, chunks=(100, 100))\na\n```\n\n----------------------------------------\n\nTITLE: Machine Learning with Pandas/XGBoost and Dask DataFrame/XGBoost in Python\nDESCRIPTION: Illustrates how to use XGBoost for machine learning tasks with both pandas and Dask DataFrame. Shows the differences in data preparation and model training between the two approaches.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import pandas as pd\n>>> import xgboost\n>>> from sklearn.cross_validation import train_test_split\n\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...    X, y, test_size=0.2,\n)\n>>> dtrain = xgboost.DMatrix(X_train, label=y_train)\n\n>>> xgboost.train(params, dtrain, 100)\n<xgboost.Booster ...>\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.dataframe as dd\n>>> import xgboost.dask\n>>> from dask_ml.model_selection import train_test_split\n\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...    X, y, test_size=0.2,\n)\n>>> dtrain = xgboost.dask.DaskDMatrix(client, X, y)\n\n>>> xgboost.dask.train(params, dtrain, 100)\n<xgboost.Booster ...>\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Local Dask Cluster with Client in Python\nDESCRIPTION: This code shows how to set up a local Dask cluster on your machine by instantiating a Client with no arguments. It creates a scheduler in the local process with workers and threads based on the number of cores in the machine.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nclient = Client()\n```\n\n----------------------------------------\n\nTITLE: Correct Dask Client Initialization with __main__ Check - Python\nDESCRIPTION: Example showing the proper way to initialize a Dask client by wrapping it in an if __name__ == \"__main__\" block to prevent infinite subprocess creation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduling.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# myscript.py\n\nif __name__ == \"__main__\":  # This avoids infinite subprocess creation\n\n   from dask.distributed import Client\n   client = Client()\n```\n\n----------------------------------------\n\nTITLE: Using Dask High-Level Collections with Arrays, DataFrames and Bags\nDESCRIPTION: Demonstrates basic usage of Dask's high-level interfaces including Arrays for NumPy-like operations, DataFrames for Pandas-like operations, and Bags for handling collections of Python objects.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/user-interfaces.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Arrays\nimport dask.array as da\nrng = da.random.default_rng()\nx = rng.uniform(low=0, high=10, size=(10000, 10000),  # normal numpy code\n                   chunks=(1000, 1000))  # break into chunks of size 1000x1000\n\ny = x + x.T - x.mean(axis=0)  # Use normal syntax for high level algorithms\n\n# DataFrames\nimport dask.dataframe as dd\ndf = dd.read_csv('2018-*-*.csv', parse_dates='timestamp',  # normal Pandas code\n                    blocksize=64000000)  # break text into 64MB chunks\n\ns = df.groupby('name').balance.mean()  # Use normal syntax for high level algorithms\n\n# Bags / lists\nimport dask.bag as db\nb = db.read_text('*.json').map(json.loads)\ntotal = (b.filter(lambda d: d['name'] == 'Alice')\n             .map(lambda d: d['balance'])\n             .sum())\n```\n\n----------------------------------------\n\nTITLE: Computing Results from Dask DataFrame\nDESCRIPTION: Demonstrates how to execute a computation on a Dask DataFrame by calling compute() to convert the lazy representation to actual results.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf[\"2021-10-01\": \"2021-10-09 5:00\"].compute()\n                      a  b\n2021-10-01 00:00:00  720  a\n2021-10-01 01:00:00  721  b\n2021-10-01 02:00:00  722  c\n2021-10-01 03:00:00  723  a\n2021-10-01 04:00:00  724  d\n...                  ... ..\n2021-10-09 01:00:00  913  b\n2021-10-09 02:00:00  914  c\n2021-10-09 03:00:00  915  a\n2021-10-09 04:00:00  916  d\n2021-10-09 05:00:00  917  d\n\n[198 rows x 2 columns]\n```\n\n----------------------------------------\n\nTITLE: Creating and Executing a Custom Dask Graph for Data Pipeline\nDESCRIPTION: This snippet demonstrates how to create a custom Dask graph for a data pipeline involving loading, cleaning, analyzing, and storing data. It shows the graph structure and how to execute it using the Dask threaded scheduler.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/custom-graphs.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef load(filename):\n    ...\n\ndef clean(data):\n    ...\n\ndef analyze(sequence_of_data):\n    ...\n\ndef store(result):\n    with open(..., 'w') as f:\n        f.write(result)\n\ndsk = {'load-1': (load, 'myfile.a.data'),\n       'load-2': (load, 'myfile.b.data'),\n       'load-3': (load, 'myfile.c.data'),\n       'clean-1': (clean, 'load-1'),\n       'clean-2': (clean, 'load-2'),\n       'clean-3': (clean, 'load-3'),\n       'analyze': (analyze, ['clean-%d' % i for i in [1, 2, 3]]),\n       'store': (store, 'analyze')}\n\nfrom dask.threaded import get\nget(dsk, 'store')  # executes in parallel\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet Files with Dask DataFrame\nDESCRIPTION: Shows how to read Parquet files into a Dask DataFrame, supporting both single files and directories of Parquet files.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-create.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> df = dd.read_parquet(\"path/to/mydata.parquet\")\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> df = dd.read_parquet(\"path/to/my/parquet/\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Dask Distributed Client in Python\nDESCRIPTION: This snippet demonstrates how to create a Dask distributed scheduler by importing and instantiating a Client with no arguments. This setup overrides any previously set default.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nclient = Client()\n```\n\n----------------------------------------\n\nTITLE: Basic Dask Delayed Example in Python\nDESCRIPTION: A simple example showing how to use dask.delayed to create a task graph with inc and add functions, demonstrating delayed computation and visualization.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> x = dask.delayed(inc)(1)\n>>> y = dask.delayed(inc)(2)\n>>> z = dask.delayed(add)(x, y)\n>>> z.compute()\n5\n>>> z.visualize()\n```\n\n----------------------------------------\n\nTITLE: Persisting Data in Distributed Memory\nDESCRIPTION: Pattern for efficiently persisting data in memory after initial loading and preprocessing steps for faster subsequent operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-best-practices.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = dd.read_csv('s3://bucket/path/to/*.csv')\ndf = df[df.balance < 0]\ndf = client.persist(df)\n\ndf = df.set_index('timestamp')\ndf = client.persist(df)\n\n>>> df.customer_id.nunique().compute()\n18452844\n\n>>> df.groupby(df.city).size().compute()\n...\n```\n\n----------------------------------------\n\nTITLE: Implementing Adaptive Scaling with KubeCluster in Dask\nDESCRIPTION: Example showing how to set up adaptive scaling using the KubeCluster implementation. The code creates a Kubernetes cluster and configures it to automatically scale between 0 and 100 workers based on computational demand.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/adaptive.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_kubernetes import KubeCluster\n\ncluster = KubeCluster()\ncluster.adapt(minimum=0, maximum=100)  # scale between 0 and 100 workers\n```\n\n----------------------------------------\n\nTITLE: Performing Large to Small Join with Dask DataFrame in Python\nDESCRIPTION: This snippet demonstrates how to perform a large to small join using Dask DataFrame. It creates a large timeseries dataset and a small one, repartitions the small dataset to a single partition, and then performs a left merge on the timestamp column.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-joins.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dask\nlarge = dask.datasets.timeseries(freq=\"10s\", npartitions=10)\nsmall = dask.datasets.timeseries(freq=\"1D\", dtypes={\"z\": int})\n\nsmall = small.repartition(npartitions=1)\nresult = large.merge(small, how=\"left\", on=[\"timestamp\"])\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Function to Dask DataFrame Partitions\nDESCRIPTION: This snippet demonstrates how to use the map_partitions method to apply a custom function across every pandas DataFrame partition in a Dask collection. This allows for easy parallelization of custom operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/best-practices.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf.map_partitions(my_custom_func)\n```\n\n----------------------------------------\n\nTITLE: Reading from Cloud Storage with Dask DataFrame\nDESCRIPTION: Examples of reading data from cloud storage services like S3 and GCS, including authentication options.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-create.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> df = dd.read_csv('s3://bucket/path/to/data-*.csv')\n>>> df = dd.read_parquet('gcs://bucket/path/to/data-*.parq')\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> df = dd.read_csv('s3://bucket-name/my-data-*.csv',\n...                  storage_options={'anon': True})\n>>> df = dd.read_parquet('gs://dask-nyc-taxi/yellowtrip.parquet',\n...                      storage_options={'token': 'anon'})\n```\n\n----------------------------------------\n\nTITLE: Basic Dask Bag Operations Reference\nDESCRIPTION: Example showing the basic operations available in Dask Bag including map, filter, fold and groupby. These operations work on collections of generic Python objects in parallel.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/bag.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmap, filter, fold, groupby\n```\n\n----------------------------------------\n\nTITLE: Data Processing with Pandas and Dask DataFrame in Python\nDESCRIPTION: Shows how to perform data processing operations like filtering, merging, and aggregation using both pandas and Dask DataFrame. Demonstrates the lazy evaluation of Dask and the use of .compute() to get results.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import pandas as pd\n\n>>> df = df[df.value >= 0]\n>>> joined = df.merge(other, on=\"account\")\n>>> result = joined.groupby(\"account\").value.mean()\n\n>>> result\nalice 123\nbob   456\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.dataframe as dd\n\n>>> df = df[df.value >= 0]\n>>> joined = df.merge(other, on=\"account\")\n>>> result = joined.groupby(\"account\").value.mean()\n\n>>> result.compute()\nalice 123\nbob   456\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Computation with Dask Delayed in Python\nDESCRIPTION: Converting a sequential computation to use dask.delayed, creating a task graph that can be executed in parallel without changing the algorithm structure.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dask\n\noutput = []\nfor x in data:\n    a = dask.delayed(inc)(x)\n    b = dask.delayed(double)(x)\n    c = dask.delayed(add)(a, b)\n    output.append(c)\n\ntotal = dask.delayed(sum)(output)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Dask Compute Calls in Python\nDESCRIPTION: This snippet illustrates how to optimize Dask compute calls. It compares an inefficient method of calling compute repeatedly in a loop, with a more efficient approach of collecting all operations and computing them at once.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/best-practices.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfoo = ...\nresults = []\nfor i in range(...):\n     results.append(foo.select(...).compute())\n```\n\nLANGUAGE: python\nCODE:\n```\nfoo = ...\nresults = []\nfor i in range(...):\n     results.append(foo.select(...))  # no compute here\nresults = dask.compute(*results)\n```\n\n----------------------------------------\n\nTITLE: Reading CSV and Parquet files from S3 and GCS using Dask\nDESCRIPTION: Demonstrates how to read CSV files from Amazon S3 and Parquet files from Google Cloud Storage using Dask DataFrame. It also shows how to read JSON files from HDFS using Dask Bag.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/connect-to-remote-data.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\ndf = dd.read_csv('s3://bucket/path/to/data-*.csv')\ndf = dd.read_parquet('gcs://bucket/path/to/data-*.parq')\n\nimport dask.bag as db\nb = db.read_text('hdfs://path/to/*.json').map(json.loads)\n```\n\n----------------------------------------\n\nTITLE: Stacking Dask Arrays with da.stack\nDESCRIPTION: Demonstrates how to use da.stack to combine multiple Dask arrays into a new array with an additional dimension. The example shows stacking along different axes and the resulting shapes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-stack.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.array as da\n\n>>> arr0 = da.from_array(np.zeros((3, 4)), chunks=(1, 2))\n>>> arr1 = da.from_array(np.ones((3, 4)), chunks=(1, 2))\n\n>>> data = [arr0, arr1]\n\n>>> x = da.stack(data, axis=0)\n>>> x.shape\n(2, 3, 4)\n\n>>> da.stack(data, axis=1).shape\n(3, 2, 4)\n\n>>> da.stack(data, axis=-1).shape\n(3, 4, 2)\n```\n\n----------------------------------------\n\nTITLE: Loading DataFrames with Dask in Python\nDESCRIPTION: This snippet demonstrates how to efficiently load data into Dask DataFrames. It contrasts an inefficient method of reading CSV files locally and then passing them to Dask, with a more efficient approach of using Dask to read the files directly.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/best-practices.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nddf = dd.read_parquet(...)\n\npandas_dfs = []\nfor fn in filenames:\n    pandas_dfs(pandas.read_csv(fn))     # Read locally with pandas\nddf = dd.concat([ddf] + pandas_dfs)     # Give to Dask\n```\n\nLANGUAGE: python\nCODE:\n```\nddf = dd.read_parquet(...)\nddf2 = dd.read_csv(filenames)\nddf = dd.concat([ddf, ddf2])\n```\n\n----------------------------------------\n\nTITLE: Setting Dask Configuration in Python\nDESCRIPTION: Shows how to set Dask configuration values directly in Python using the dask.config.set function. It demonstrates both permanent and temporary (context manager) configuration changes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/configuration.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> dask.config.set({'optimization.fuse.ave-width': 4})\n\n>>> with dask.config.set({'optimization.fuse.ave-width': 4}):\n...     arr2, = dask.optimize(arr)\n```\n\n----------------------------------------\n\nTITLE: Using Dask Delayed as a Decorator in Python\nDESCRIPTION: An alternative syntax using dask.delayed as a decorator to define functions that will be executed lazily, creating the same parallel computation as the previous example.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dask\n\n@dask.delayed\ndef inc(x):\n    return x + 1\n\n@dask.delayed\ndef double(x):\n    return x * 2\n\n@dask.delayed\ndef add(x, y):\n    return x + y\n\ndata = [1, 2, 3, 4, 5]\n\noutput = []\nfor x in data:\n    a = inc(x)\n    b = double(x)\n    c = add(a, b)\n    output.append(c)\n\ntotal = dask.delayed(sum)(output)\n```\n\n----------------------------------------\n\nTITLE: Applying Dask Delayed to Functions in Python\nDESCRIPTION: Demonstrates the correct way to apply dask.delayed to functions rather than their results. This ensures lazy evaluation and proper parallelization.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# This makes a delayed function, acting lazily\n\ndask.delayed(f)(x, y)\n```\n\n----------------------------------------\n\nTITLE: Using Map Partitions with Dask DataFrame in Python\nDESCRIPTION: Demonstrates the correct approach to apply functions to Dask collections using mapping methods instead of wrapping them in delayed calls.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\ndf = dd.read_csv('/path/to/*.csv')\ndf.map_partitions(train)\n```\n\n----------------------------------------\n\nTITLE: Writing to Azure Blob Storage with Dask DataFrame\nDESCRIPTION: Demonstrates creating a DataFrame and saving it to Azure Blob Storage in Parquet format with authentication.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-create.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> d = {'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8]}\n>>> df = dd.from_pandas(pd.DataFrame(data=d), npartitions=2)\n>>> dd.to_parquet(df=df,\n...               path='abfs://CONTAINER/FILE.parquet'\n...               storage_options={'account_name': 'ACCOUNT_NAME',\n...                                'account_key': 'ACCOUNT_KEY'})\n```\n\n----------------------------------------\n\nTITLE: Simple HPO with Dask Futures\nDESCRIPTION: Implements basic hyperparameter optimization using Dask Futures for parallel model training and evaluation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/ml.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import LocalCluster\n\ncluster = LocalCluster(processes=False)  # replace this with some scalable cluster\nclient = cluster.get_client()\n\ndef train_and_score(params: dict) -> float:\n    data = load_data()\n    model = make_model(**params)\n    train(model)\n    score = evaluate(model)\n    return score\n\nparams_list = [...]\nfutures = [\n    client.submit(train_and_score, params) for params in params_list\n]\nscores = client.gather(futures)\nbest = max(scores)\n\nbest_params = params_list[scores.index(best)]\n```\n\n----------------------------------------\n\nTITLE: Working with Dask Array Objects in Python\nDESCRIPTION: Demonstration of creating and manipulating Dask Array objects, including reshaping and computing results.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-api.rst#2025-04-19_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport dask.array as da\n\n# Create a Dask array\narr = da.random.random((1000, 1000), chunks=(100, 100))\n\n# Reshape the array\nreshaped = arr.reshape((1000000,))\n\n# Compute the result\nresult = reshaped.compute()\n```\n\n----------------------------------------\n\nTITLE: Loading Arrays with Dask in Python\nDESCRIPTION: This snippet shows how to efficiently load array data into Dask. It compares an inefficient method of reading data into a NumPy array locally before passing it to Dask, with a more efficient approach of using Dask to read the data directly from the file.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/best-practices.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nf = h5py.File(...)\n\nx = np.asarray(f[\"x\"])  # Get data as a NumPy array locally\nx = da.from_array(x)   # Hand NumPy array to Dask\n```\n\nLANGUAGE: python\nCODE:\n```\nf = h5py.File(...)\nx = da.from_array(f[\"x\"])  # Let Dask do the reading\n```\n\n----------------------------------------\n\nTITLE: Complete Dask Setup with Context Managers\nDESCRIPTION: Example demonstrating a complete Dask setup using context managers for Scheduler, Workers, and Client, including computation execution.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python-advanced.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dask.distributed import Scheduler, Worker, Client\n\nasync def f():\n    async with Scheduler() as s:\n        async with Worker(s.address) as w1, Worker(s.address) as w2:\n            async with Client(s.address, asynchronous=True) as client:\n                future = client.submit(lambda x: x + 1, 10)\n                result = await future\n                print(result)\n\nasyncio.get_event_loop().run_until_complete(f())\n```\n\n----------------------------------------\n\nTITLE: Loading Parquet Files with Dask Dataframe\nDESCRIPTION: Examples of loading Parquet files from different sources including local files, directories, and remote storage like S3. These snippets demonstrate various ways to access Parquet data using Dask's read_parquet function.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-parquet.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.dataframe as dd\n\n# Load a single local parquet file\n>>> df = dd.read_parquet(\"path/to/mydata.parquet\")\n\n# Load a directory of local parquet files\n>>> df = dd.read_parquet(\"path/to/my/parquet/\")\n\n# Load a directory of parquet files from S3\n>>> df = dd.read_parquet(\"s3://bucket-name/my/parquet/\")\n```\n\n----------------------------------------\n\nTITLE: Optimizing Parallelism with Dask Compute in Python\nDESCRIPTION: Shows how to collect multiple delayed operations and compute them together for improved parallelism, rather than computing each result individually.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Collect many calls for one compute\n\nresults = []\nfor x in L:\n    y = dask.delayed(f)(x)\n    results.append(y)\n\nresults = dask.compute(*results)\n```\n\n----------------------------------------\n\nTITLE: Using Common DataFrame Operations with Good Performance\nDESCRIPTION: Examples of common DataFrame operations that perform efficiently in Dask without special considerations. These include common groupby reductions, applying functions to grouped data with known divisions, and joining along indexes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf.groupby(columns).known_reduction()            # Fast and common case\n>>> ddf.groupby(columns_with_index).apply(user_fn)    # Fast and common case\n>>> ddf.join(pandas_df, on=column)                    # Fast and common case\n>>> lhs.join(rhs)                                     # Fast and common case\n>>> lhs.merge(rhs, on=columns_with_index)             # Fast and common case\n```\n\n----------------------------------------\n\nTITLE: Using Dask Semaphore for Resource Limitation in Python\nDESCRIPTION: Demonstrates how to use a Dask Semaphore to limit concurrent access to resources like databases across a cluster. The example initializes a semaphore with 2 maximum leases and uses it within a function that's mapped across multiple values.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Semaphore\n\nsem = Semaphore(max_leases=2, name=\"database\")\n\ndef access_limited(val, sem):\n   with sem:\n      # Interact with the DB\n      return\n\nfutures = client.map(access_limited, range(10), sem=sem)\nclient.gather(futures)\nsem.close()\n```\n\n----------------------------------------\n\nTITLE: Using Parquet Format for Data Storage\nDESCRIPTION: Example of reading and writing data using the recommended Parquet format for optimal performance and storage efficiency.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-best-practices.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf.to_parquet('path/to/my-results/')\ndf = dd.read_parquet('path/to/my-results/')\n```\n\n----------------------------------------\n\nTITLE: Loading Data with Pandas and Dask DataFrame in Python\nDESCRIPTION: Demonstrates how to load Parquet data using both pandas and Dask DataFrame, highlighting the similarity in API and the ability of Dask to handle multiple files.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import pandas as pd\n\n>>> df = pd.read_parquet('s3://mybucket/myfile.parquet')\n>>> df.head()\n0  1  a\n1  2  b\n2  3  c\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.dataframe as dd\n\n>>> df = dd.read_parquet('s3://mybucket/myfile.*.parquet')\n>>> df.head()\n0  1  a\n1  2  b\n2  3  c\n```\n\n----------------------------------------\n\nTITLE: Using Statistical Methods with Dask DataFrame\nDESCRIPTION: Demonstrates calculating mean and finding unique values on a Dask DataFrame, showing both the lazy representation and computed results.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf.a.mean()\ndd.Scalar<series-..., dtype=float64>\n\n>>> ddf.a.mean().compute()\n1199.5\n\n>>> ddf.b.unique()\nDask Series Structure:\nnpartitions=1\n   object\n      ...\nName: b, dtype: object\nDask Name: unique-agg, 33 tasks\n\n>>> ddf.b.unique().compute()\n0    a\n1    b\n2    c\n3    d\n4    e\nName: b, dtype: object\n```\n\n----------------------------------------\n\nTITLE: Converting Pandas DataFrame to cuDF with Dask\nDESCRIPTION: Example showing how to convert a Pandas-backed Dask DataFrame to a cuDF-backed DataFrame for GPU acceleration. Requires cuDF installation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/gpu.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cudf\n\ndf = df.map_partitions(cudf.from_pandas)  # convert pandas partitions into cudf partitions\n```\n\n----------------------------------------\n\nTITLE: Initializing Complete Dask Distributed Setup with Python\nDESCRIPTION: Comprehensive example showing how to set up a Scheduler, two Workers, and a Client in the same event loop. Demonstrates running a simple computation and cleanup.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python-advanced.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dask.distributed import Scheduler, Worker, Client\n\nasync def f():\n    async with Scheduler() as s:\n        async with Worker(s.address) as w1, Worker(s.address) as w2:\n            async with Client(s.address, asynchronous=True) as client:\n                future = client.submit(lambda x: x + 1, 10)\n                result = await future\n                print(result)\n\nasyncio.get_event_loop().run_until_complete(f())\n```\n\n----------------------------------------\n\nTITLE: Using Dask Compute and Persist Methods\nDESCRIPTION: Demonstrates different ways to trigger computation in Dask, including computing single results, multiple results, and persisting data in distributed memory.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/user-interfaces.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# compute multiple results at the same time with the compute function\nmin, max = dask.compute(y.min(), y.max())\n\n# Write larger results out to disk rather than store them in memory\nmy_dask_dataframe.to_parquet('myfile.parquet')\nmy_dask_array.to_hdf5('myfile.hdf5')\nmy_dask_bag.to_textfiles('myfile.*.txt')\n```\n\n----------------------------------------\n\nTITLE: Using Dask Delayed for Custom Parallel Operations\nDESCRIPTION: Shows how to use Dask delayed to parallelize existing for-loop code by wrapping individual function calls into a lazily constructed task graph.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/user-interfaces.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dask\n\nlazy_results = []\nfor a in A:\n    for b in B:\n        if a < b:\n            c = dask.delayed(f)(a, b)  # add lazy task\n        else:\n            c = dask.delayed(g)(a, b)  # add lazy task\n        lazy_results.append(c)\n\nresults = dask.compute(*lazy_results)  # compute all in parallel\n```\n\n----------------------------------------\n\nTITLE: Using Dask Array Top-Level Functions in Python\nDESCRIPTION: Example of using some of the top-level functions provided by dask.array, such as creating an array and performing operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-api.rst#2025-04-19_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport dask.array as da\n\n# Create a Dask array\nx = da.arange(100, chunks=10)\n\n# Perform operations\ny = da.sin(x)\nz = da.mean(y)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dask Client with In-Process Workers in Python\nDESCRIPTION: This snippet demonstrates how to create a Dask Client with workers running in the same process. This is useful for avoiding inter-worker communication and is common when primarily using NumPy or Dask Array.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = Client(processes=False)\n```\n\n----------------------------------------\n\nTITLE: Using Mathematical Functions with Dask Array\nDESCRIPTION: Demonstrates applying NumPy functions to Dask Arrays and computing the results, showing how NumPy-like operations work with Dask.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> a.mean()\ndask.array<mean_agg-aggregate, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray>\n\n>>> a.mean().compute()\n49999.5\n\n>>> np.sin(a)\ndask.array<sin, shape=(200, 500), dtype=float64, chunksize=(100, 100), chunktype=numpy.ndarray>\n\n>>> np.sin(a).compute()\narray([[ 0.        ,  0.84147098,  0.90929743, ...,  0.58781939,\n          0.99834363,  0.49099533],\n       [-0.46777181, -0.9964717 , -0.60902011, ..., -0.89796748,\n       -0.85547315, -0.02646075],\n       [ 0.82687954,  0.9199906 ,  0.16726654, ...,  0.99951642,\n          0.51387502, -0.4442207 ],\n       ...,\n       [-0.99720859, -0.47596473,  0.48287891, ..., -0.76284376,\n          0.13191447,  0.90539115],\n       [ 0.84645538,  0.00929244, -0.83641393, ...,  0.37178568,\n       -0.5802765 , -0.99883514],\n       [-0.49906936,  0.45953849,  0.99564877, ...,  0.10563876,\n          0.89383946,  0.86024828]])\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Dask Array Example\nDESCRIPTION: Demonstrates basic Dask array creation and inspection of internal properties including chunk size, name, and graph structure.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-design.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.array as da\n>>> x = da.arange(0, 15, chunks=(5,))\n\n>>> x.name\n'arange-539766a'\n\n>>> x.__dask_graph__()\n<dask.highlevelgraph.HighLevelGraph at 0x7f9f6f686d68>\n\n>>> dict(x.__dask_graph__())  # somewhat simplified\n{('arange-539766a', 0): (np.arange, 0, 5),\n ('arange-539766a', 1): (np.arange, 5, 10),\n ('arange-539766a', 2): (np.arange, 10, 15)}\n\n>>> x.chunks\n((5, 5, 5),)\n\n>>> x.dtype\ndtype('int64')\n```\n\n----------------------------------------\n\nTITLE: Repartitioning for Performance Optimization\nDESCRIPTION: Shows how to repartition a filtered DataFrame to reduce computational overhead by consolidating partitions.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-best-practices.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = dd.read_csv('s3://bucket/path/to/*.csv')\ndf = df[df.name == 'Alice']  # only 1/100th of the data\ndf = df.repartition(npartitions=df.npartitions // 100)\n\ndf = df.persist()  # if on a distributed system\n```\n\n----------------------------------------\n\nTITLE: Using Dask Delayed for Efficient Data Processing in Python\nDESCRIPTION: This snippet demonstrates how to use Dask Delayed for efficient data processing. It contrasts an inefficient method of reading a large CSV file locally before processing, with a more efficient approach of using Delayed to read and process the data.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/best-practices.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dask.delayed\ndef process(a, b):\n    ...\n\ndf = pandas.read_csv(\"some-large-file.csv\")  # Create large object locally\nresults = []\nfor item in L:\n    result = process(item, df)  # include df in every delayed call\n    results.append(result)\n```\n\nLANGUAGE: python\nCODE:\n```\n@dask.delayed\ndef process(a, b):\n   ...\n\ndf = dask.delayed(pandas.read_csv)(\"some-large-file.csv\")  # Let Dask build object\nresults = []\nfor item in L:\n   result = process(item, df)  # include pointer to df in every delayed call\n   results.append(result)\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Tasks with Dask Futures\nDESCRIPTION: Demonstrates using Dask Futures to parallelize for-loop style Python code using LocalCluster for distributed computing.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import LocalCluster\nclient = LocalCluster().get_client()\n\n# Submit work to happen in parallel\nresults = []\nfor filename in filenames:\n    data = client.submit(load, filename)\n    result = client.submit(process, data)\n    results.append(result)\n\n# Gather results back to local computer\nresults = client.gather(results)\n```\n\n----------------------------------------\n\nTITLE: Correct Usage of Delayed Functions in Python\nDESCRIPTION: Illustrates the proper way to structure functions when using dask.delayed, avoiding nested delayed calls for better performance and clarity.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef process_all(L):\n    result = []\n    for x in L:\n        y = dask.delayed(f)(x)\n        result.append(y)\n    return result\n```\n\n----------------------------------------\n\nTITLE: Label-based Indexing with .loc in Dask DataFrame using Python\nDESCRIPTION: Shows how to use .loc for label-based indexing in Dask DataFrames. Examples include selecting specific rows and columns, using boolean conditions, and using lambda functions for row selection.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-indexing.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf.loc[['b', 'c'], ['A']]\nDask DataFrame Structure:\n                     A\nnpartitions=1\nb              int64\nc                ...\nDask Name: loc, 2 tasks\n\n>>> ddf.loc[df[\"A\"] > 1, [\"B\"]]\nDask DataFrame Structure:\n                     B\nnpartitions=1\na              int64\nc                ...\nDask Name: try_loc, 2 tasks\n\n>>> ddf.loc[lambda df: df[\"A\"] > 1, [\"B\"]]\nDask DataFrame Structure:\n                     B\nnpartitions=1\na              int64\nc                ...\nDask Name: try_loc, 2 tasks\n```\n\n----------------------------------------\n\nTITLE: Event-based Client Synchronization in Python\nDESCRIPTION: Shows how to use distributed events for synchronizing multiple clients and coordinating task execution.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Event\n\ndef wait_for_event(x):\n   event = Event(\"my-event\")\n\n   event.wait()\n   # at this point, all function calls\n   # are in sync once the event is set\n\nfutures = client.map(wait_for_event, range(10))\n\nEvent(\"my-event\").set()\nclient.gather(futures)\n```\n\n----------------------------------------\n\nTITLE: Converting NumPy Array to CuPy with Dask\nDESCRIPTION: Example demonstrating how to convert a NumPy-backed Dask Array into a CuPy-backed Array for GPU acceleration. Requires CuPy version 6 or higher and NumPy version 1.17 or higher.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/gpu.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cupy\n\nx = x.map_blocks(cupy.asarray)\n```\n\n----------------------------------------\n\nTITLE: Specifying Shuffle Method in set_index Operation\nDESCRIPTION: Examples showing how to explicitly specify the shuffle method when performing a set_index operation, allowing choice between disk-based, task-based, or peer-to-peer shuffling.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nddf.set_index(column, shuffle_method='disk')\nddf.set_index(column, shuffle_method='tasks')\nddf.set_index(column, shuffle_method='p2p')\n```\n\n----------------------------------------\n\nTITLE: Converting Dask DataFrame to Pandas After Reduction\nDESCRIPTION: Demonstrates the pattern of reducing a large dataset with Dask and then converting to pandas for further processing once the data size is manageable.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-best-practices.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = dd.read_parquet('my-giant-file.parquet')\ndf = df[df.name == 'Alice']              # Select a subsection\nresult = df.groupby('id').value.mean()   # Reduce to a smaller size\nresult = result.compute()                # Convert to pandas dataframe\nresult...                                # Continue working with pandas\n```\n\n----------------------------------------\n\nTITLE: Interacting with NumPy Arrays in Dask Array Operations in Python\nDESCRIPTION: This example demonstrates how Dask array operations automatically convert NumPy arrays into single-chunk Dask arrays and how NumPy and Dask arrays interact in operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n>>> x = da.sum(np.ones(5))\n>>> x.compute()\n5\n\n>>> x = da.ones(10, chunks=(5,))\n>>> y = np.ones(10)\n>>> z = x + y\n>>> z\ndask.array<add, shape=(10,), dtype=float64, chunksize=(5,), chunktype=numpy.ndarray>\n```\n\n----------------------------------------\n\nTITLE: Performing Sorted Join with Dask DataFrame in Python\nDESCRIPTION: This code snippet shows how to perform a sorted join using Dask DataFrame. It creates a timeseries dataset, saves it to Parquet format, reads it back, and then performs left joins with two other timeseries datasets using the index.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-joins.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dask\nimport dask.dataframe as dd\n\nleft = dask.datasets.timeseries(dtypes={\"foo\": int})\n\n# timeseries returns a dataframe indexed by\n# timestamp, we don't need to set_index.\n\n# left.set_index(\"timestamp\")\n\nleft.to_parquet(\"left\", overwrite=True)\nleft = dd.read_parquet(\"left\")\n\nright_one = dask.datasets.timeseries(dtypes={\"bar\": int})\nright_two = dask.datasets.timeseries(dtypes={\"baz\": int})\n\nresult = left.merge(\n    right_one, how=\"left\", left_index=True, right_index=True)\nresult = result.merge(\n    right_two, how=\"left\", left_index=True, right_index=True)\n```\n\n----------------------------------------\n\nTITLE: Breaking Up Computations for Parallelism in Dask\nDESCRIPTION: Illustrates how to break up a large computation into many smaller Dask delayed tasks to achieve better parallelism, as opposed to using a single large task.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Break up into many tasks\n\n@dask.delayed\ndef load(filename):\n    ...\n\n@dask.delayed\ndef process(data):\n    ...\n\n@dask.delayed\ndef save(data):\n    ...\n\ndef f(filenames):\n    results = []\n    for filename in filenames:\n        data = load(filename)\n        data = process(data)\n        result = save(data)\n        results.append(result)\n\n    return results\n\ndask.compute(f(filenames))\n```\n\n----------------------------------------\n\nTITLE: Initializing Dask Client for Futures in Python\nDESCRIPTION: Demonstrates how to start a Dask Client to use the futures interface. The Client tracks state among worker processes or threads.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\n\nclient = Client()  # start local workers as processes\n# or\nclient = Client(processes=False)  # start local workers as threads\n```\n\n----------------------------------------\n\nTITLE: Interacting with Dask Actor Methods and Attributes in Python\nDESCRIPTION: Demonstrates how to interact with Dask Actors by calling methods that return ActorFutures and accessing attributes. Method calls are asynchronous while attribute access is synchronous and blocking.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> future = counter.increment()\n>>> future\n<ActorFuture>\n\n>>> future.result()\n1\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> counter.n\n1\n```\n\n----------------------------------------\n\nTITLE: Examining Dask DataFrame Partitions and Divisions\nDESCRIPTION: Shows how to inspect the division boundaries of a Dask DataFrame and access individual partitions, which is important for understanding how the data is distributed.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> # check the index values covered by each partition\n... ddf.divisions\n(Timestamp('2021-09-01 00:00:00', freq='H'),\nTimestamp('2021-09-11 00:00:00', freq='H'),\nTimestamp('2021-09-21 00:00:00', freq='H'),\nTimestamp('2021-10-01 00:00:00', freq='H'),\nTimestamp('2021-10-11 00:00:00', freq='H'),\nTimestamp('2021-10-21 00:00:00', freq='H'),\nTimestamp('2021-10-31 00:00:00', freq='H'),\nTimestamp('2021-11-10 00:00:00', freq='H'),\nTimestamp('2021-11-20 00:00:00', freq='H'),\nTimestamp('2021-11-30 00:00:00', freq='H'),\nTimestamp('2021-12-09 23:00:00', freq='H'))\n\n>>> # access a particular partition\n... ddf.partitions[1]\nDask DataFrame Structure:\n                   a       b\nnpartitions=1\n2021-09-11     int64  object\n2021-09-21       ...     ...\nDask Name: blocks, 11 tasks\n```\n\n----------------------------------------\n\nTITLE: Converting between Dask Delayed and Dask DataFrame for Custom Data Formats\nDESCRIPTION: This code demonstrates a workflow for processing data in a custom format using Dask. It uses dask.delayed to read custom format files into Pandas DataFrames, converts them to a Dask DataFrame for processing, and then converts back to delayed objects for saving the results in the custom format.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-collections.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\nfrom dask.delayed import delayed\n\nfrom my_custom_library import load, save\n\nfilenames = ...\ndfs = [delayed(load)(fn) for fn in filenames]\n\ndf = dd.from_delayed(dfs)\ndf = ... # do work with dask.dataframe\n\ndfs = df.to_delayed()\nwrites = [delayed(save)(df, fn) for df, fn in zip(dfs, filenames)]\n\ndd.compute(*writes)\n```\n\n----------------------------------------\n\nTITLE: Converting Columns to Categoricals in Dask\nDESCRIPTION: Shows different methods to convert non-categorical columns to categorical type, including astype() for unknown categoricals and categorize() for known categoricals.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-categoricals.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# astype operates lazily, and results in unknown categoricals\nddf = ddf.astype({'mycol': 'category', ...})\n# or\nddf['mycol'] = ddf.mycol.astype('category')\n\n# categorize requires computation, and results in known categoricals\nddf = ddf.categorize(columns=['mycol', ...])\n```\n\n----------------------------------------\n\nTITLE: Performing Statistical Tests with Dask Array\nDESCRIPTION: Shows how to perform a basic statistical test (paired t-test) using Dask Array's stats module. It generates two related datasets and performs a t-test, returning a dask.delayed object wrapping a scipy namedtuple result.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-stats.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> rng = da.random.default_rng()\n>>> a = rng.uniform(size=(50,), chunks=(25,))\n>>> b = a + rng.uniform(low=-0.15, high=0.15, size=(50,), chunks=(25,))\n>>> result = stats.ttest_rel(a, b)\n>>> result.compute()\nTtest_relResult(statistic=-1.5102104380013242, pvalue=0.13741197274874514)\n```\n\n----------------------------------------\n\nTITLE: Explicit Dask LocalCluster and Client Setup in Python\nDESCRIPTION: This code explicitly creates a LocalCluster and passes it to the Client. This is equivalent to the shorthand Client() call but provides more control over cluster configuration.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster()\nclient = Client(cluster)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Extent Aggregation Components\nDESCRIPTION: Functions defining the components of a custom extent (max-min) aggregation. These include the chunk function to process each partition, an aggregation function to combine chunks, and a finalize function to compute the final result.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> def chunk(grouped):\n...     return grouped.max(), grouped.min()\n\n>>> def agg(chunk_maxes, chunk_mins):\n...     return chunk_maxes.max(), chunk_mins.min()\n\n>>> def finalize(maxima, minima):\n...     return maxima - minima\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet files from S3 with custom storage options in Dask\nDESCRIPTION: Demonstrates how to read Parquet files from Amazon S3 using Dask DataFrame with custom storage options, such as anonymous access and disabling SSL.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/connect-to-remote-data.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = dd.read_parquet('s3://bucket/path',\n                        storage_options={'anon': True, 'use_ssl': False})\n```\n\n----------------------------------------\n\nTITLE: Setting up Dask Cluster with Docker\nDESCRIPTION: Demonstrates how to create a dedicated Docker network, start a Dask scheduler and multiple worker containers, and launch a Jupyter notebook server. This allows running a complete Dask distributed computing environment using Docker containers.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-docker.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker network create dask\n\ndocker run --network dask -p 8787:8787 --name scheduler ghcr.io/dask/dask dask-scheduler  # start scheduler\n\ndocker run --network dask ghcr.io/dask/dask dask-worker scheduler:8786 # start worker\ndocker run --network dask ghcr.io/dask/dask dask-worker scheduler:8786 # start worker\ndocker run --network dask ghcr.io/dask/dask dask-worker scheduler:8786 # start worker\n\ndocker run --network dask -p 8888:8888 ghcr.io/dask/dask-notebook  # start Jupyter server\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Files with Dask DataFrame\nDESCRIPTION: Demonstrates loading CSV files into a Dask DataFrame using globstrings for multiple files or blocksize parameter for large single files.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-create.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> df = dd.read_csv('myfiles.*.csv')\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> df = dd.read_csv('largefile.csv', blocksize=25e6)  # 25MB chunks\n```\n\n----------------------------------------\n\nTITLE: Setting Index for Optimized Operations\nDESCRIPTION: Shows how to set an index on a Dask DataFrame to enable faster operations like time-based selections and index-based merges.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-best-practices.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = df.set_index('timestamp')  # set the index to make some operations fast\n\ndf.loc['2001-01-05':'2001-01-12']  # this is very fast if you have an index\ndf.merge(df2, left_index=True, right_index=True)  # this is also very fast\n```\n\n----------------------------------------\n\nTITLE: Examining Dask Array Chunks and Blocks\nDESCRIPTION: Shows how to inspect the chunk structure of a Dask Array and access individual blocks, which helps understand how the array is divided for computation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# inspect the chunks\na.chunks\n```\n\nLANGUAGE: python\nCODE:\n```\n# access a particular block of data\na.blocks[1, 3]\n```\n\n----------------------------------------\n\nTITLE: Optimizing Memory Usage with Dask bind()\nDESCRIPTION: Improved version of the previous example using the bind() function to prevent loading the entire array into memory. This trades increased computation time for reduced memory usage by recomputing array chunks as needed.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/graph_manipulation.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from dask.graph_manipulation import bind\n>>> xb = bind(x, x_mean)\n>>> y = (xb - x_mean).max().compute()\n```\n\n----------------------------------------\n\nTITLE: Calculating Divisions when Reading Parquet\nDESCRIPTION: Example of calculating dataframe divisions from Parquet metadata. This enables better query planning by providing known partition boundaries based on the Parquet file statistics.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-parquet.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> dd.read_parquet(\n...     \"s3://path/to/myparquet/\",\n...     index=\"timestamp\",  # Specify a specific index column\n...     calculate_divisions=True,  # Calculate divisions from metadata\n... )\n```\n\n----------------------------------------\n\nTITLE: Creating a Dask DataFrame from pandas DataFrame\nDESCRIPTION: Shows how to create a Dask DataFrame from a pandas DataFrame with date range index, specifying the number of partitions to divide the data.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> index = pd.date_range(\"2021-09-01\", periods=2400, freq=\"1h\")\n... df = pd.DataFrame({\"a\": np.arange(2400), \"b\": list(\"abcaddbe\" * 300)}, index=index)\n... ddf = dd.from_pandas(df, npartitions=10)\n... ddf\nDask DataFrame Structure:\n                         a       b\nnpartitions=10\n2021-09-01 00:00:00  int64  object\n2021-09-11 00:00:00    ...     ...\n...                    ...     ...\n2021-11-30 00:00:00    ...     ...\n2021-12-09 23:00:00    ...     ...\nDask Name: from_pandas, 10 tasks\n```\n\n----------------------------------------\n\nTITLE: Array Operations with Dask Arrays\nDESCRIPTION: Demonstrates using Dask Arrays for parallel processing of large numerical arrays with NumPy-like operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\n\nx = da.random.random((10000, 10000))\ny = (x + x.T) - x.mean(axis=1)\n\nz = y.var(axis=0).compute()\n```\n\n----------------------------------------\n\nTITLE: Submitting Tasks with Variables in Python\nDESCRIPTION: Example showing how to submit tasks and store results using a distributed Variable object to enable result retrieval across processes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom distributed import Variable\n\nvar = Variable(\"my-result\")\nfut = client.submit(...)\nvar.set(fut)\n```\n\nLANGUAGE: python\nCODE:\n```\nvar = Variable(\"my-result\")\nfut = var.get()\nresult = fut.result()\n```\n\n----------------------------------------\n\nTITLE: Initializing Dask Distributed Client\nDESCRIPTION: Code snippet showing how to create a Dask distributed client which automatically starts a local scheduler and provides access to the dashboard interface.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dashboard.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nclient = Client()  # start distributed scheduler locally.\n```\n\n----------------------------------------\n\nTITLE: Generating Random Dask Array using dask.array.random in Python\nDESCRIPTION: This code snippet shows how to create a Dask array of random data using the dask.array.random module, which implements functions similar to numpy.random.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n>>> import dask.array as da\n>>> rng = da.random.default_rng()\n>>> x = rng.random((10000, 10000), chunks=(1000, 1000))\n```\n\n----------------------------------------\n\nTITLE: Computing Multiple Dask Collections\nDESCRIPTION: Demonstrates how to compute multiple Dask collections simultaneously using the compute function, which allows sharing of intermediate results for better performance.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduler-overview.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> y = (x + 1).sum()\n>>> z = (x + 1).mean()\n>>> da.compute(y, z)    # Compute y and z, sharing intermediate results\n(5050, 50.5)\n```\n\n----------------------------------------\n\nTITLE: Calculating Statistical Measures with Dask Array\nDESCRIPTION: Demonstrates how to calculate kurtosis, skewness, and arbitrary moments using Dask Array's stats module. It uses a beta distribution to generate random data and computes these statistics.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-stats.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from dask.array import stats\n>>> rng = da.random.default_rng()\n>>> x = rng.beta(1, 1, size=(1000,), chunks=10)\n>>> k, s, m = [stats.kurtosis(x), stats.skew(x), stats.moment(x, 5)]\n>>> dask.compute(k, s, m)\n(1.7612340817172787, -0.064073498030693302, -0.00054523780628304799)\n```\n\n----------------------------------------\n\nTITLE: Computing Results from Dask Array\nDESCRIPTION: Shows how to execute a computation on a Dask Array by calling compute() to transform the lazy task graph into actual NumPy array results.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> a[:50, 200].compute()\narray([  200,   700,  1200,  1700,  2200,  2700,  3200,  3700,  4200,\n      4700,  5200,  5700,  6200,  6700,  7200,  7700,  8200,  8700,\n      9200,  9700, 10200, 10700, 11200, 11700, 12200, 12700, 13200,\n      13700, 14200, 14700, 15200, 15700, 16200, 16700, 17200, 17700,\n      18200, 18700, 19200, 19700, 20200, 20700, 21200, 21700, 22200,\n      22700, 23200, 23700, 24200, 24700])\n```\n\n----------------------------------------\n\nTITLE: Submitting Tasks to Dask Client in Python\nDESCRIPTION: Shows how to submit individual tasks and map functions over multiple inputs using Dask Client. The submit method returns a Future object representing the remote result.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef inc(x):\n    return x + 1\n\ndef add(x, y):\n    return x + y\n\na = client.submit(inc, 10)  # calls inc(10) in background thread or process\nb = client.submit(inc, 20)  # calls inc(20) in background thread or process\n\nc = client.submit(add, a, b)  # calls add on the results of a and b\n\nfutures = client.map(inc, range(1000))\n```\n\n----------------------------------------\n\nTITLE: Deploying Dask Cluster with Coiled on Cloud Providers\nDESCRIPTION: Creates a Dask cluster on cloud infrastructure using Coiled. This example demonstrates creating a 100-worker cluster in AWS us-west-2 region with m6i.xlarge VM instances and connecting a client to it.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-cloud.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import coiled\n>>> cluster = coiled.Cluster(\n...     n_workers=100,             # Size of cluster\n...     region=\"us-west-2\",        # Same region as data\n...     vm_type=\"m6i.xlarge\",      # Hardware of your choosing\n... )\n>>> client = cluster.get_client()\n```\n\n----------------------------------------\n\nTITLE: Local Dask Cluster Setup\nDESCRIPTION: Setting up a multi-process Dask cluster on a local machine with full features including diagnostic dashboards.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import LocalCluster\ncluster = LocalCluster()          # Fully-featured local Dask cluster\nclient = cluster.get_client()\n\n# Dask works as normal and leverages the infrastructure defined above\ndf.x.sum().compute()\n```\n\n----------------------------------------\n\nTITLE: Parallel Optimization with Dask and Optuna\nDESCRIPTION: Executes parallel optimization trials using Dask distributed computing and Optuna.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/ml.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import LocalCluster, wait\n\ncluster = LocalCluster(processes=False)  # replace this with some scalable cluster\nclient = cluster.get_client()\n\nfutures = [\n    client.submit(study.optimize, objective, n_trials=1, pure=False) for _ in range(500)\n]\nwait(futures)\n\nprint(study.best_params)\n```\n\n----------------------------------------\n\nTITLE: Batching Tasks with Dask Bag in Python\nDESCRIPTION: Demonstrates how to use dask.bag to automatically batch function applications, reducing the number of tasks and improving efficiency.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport dask.bag as db\nb = db.from_sequence(range(10000000), npartitions=1000)\nb = b.map(f)\n```\n\n----------------------------------------\n\nTITLE: Manual SQL Data Loading with from_map in Dask\nDESCRIPTION: Demonstrates a custom approach to load SQL data into Dask DataFrames using from_map when the standard read_sql functions aren't sufficient. This method is useful for unsupported databases or when needing optimized custom queries.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-sql.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\n\ndef fetch_partition(part):\n    conn = establish_connection()\n    df = fetch_query(base_query.format(part))\n    return df.astype(known_types)\n\nddf = dd.from_map(fetch_partition,\n                  parts,\n                  meta=known_types,\n```\n\n----------------------------------------\n\nTITLE: Efficient Dask Array Creation from Image Files using da.map_blocks in Python\nDESCRIPTION: This code demonstrates a more efficient way to create a Dask array from a stack of images using da.map_blocks instead of da.stack. It provides better performance for large datasets.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport glob\nimport skimage.io\nimport numpy as np\nimport dask.array as da\n\nfilenames = sorted(glob.glob('*.jpg'))\n\ndef read_one_image(block_id, filenames=filenames, axis=0):\n    # a function that reads in one chunk of data\n    path = filenames[block_id[axis]]\n    image = skimage.io.imread(path)\n    return np.expand_dims(image, axis=axis)\n\n# load the first image (assume rest are same shape/dtype)\nsample = skimage.io.imread(filenames[0])\n\nstack = da.map_blocks(\n    read_one_image,\n    dtype=sample.dtype,\n    chunks=((1,) * len(filenames),  *sample.shape)\n)\n```\n\n----------------------------------------\n\nTITLE: Using SQLAlchemy Expressions with Dask's read_sql_query\nDESCRIPTION: Shows how to use SQLAlchemy SQL expressions with Dask's read_sql_query function to perform server-side operations like calculating string length. The example demonstrates selecting columns, applying functions, and proper labeling of operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-sql.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sqlalchemy import sql\nnumber = sql.column(\"number\")\nname = sql.column(\"name\")\ns1 = sql.select([\n        number, name, sql.func.length(name).label(\"lenname\")\n    ]\n    ).select_from(sql.table(\"test\"))\ndata = read_sql_query(\n    s1, db, npartitions=2, index_col=number\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Dask Configuration in Python\nDESCRIPTION: Demonstrates how to access Dask configuration settings using the dask.config.get function. It shows nested access using dot notation and handling of underscores/hyphens.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/configuration.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask\n>>> import dask.distributed  # populate config with distributed defaults\n\n>>> dask.config.get(\"distributed.client\") # use `.` for nested access\n{'heartbeat': '5s', 'scheduler-info-interval': '2s'}\n\n>>> dask.config.get(\"distributed.scheduler.unknown-task-duration\")\n'500ms'\n```\n\n----------------------------------------\n\nTITLE: Avoiding Input Mutation in Dask Delayed Functions\nDESCRIPTION: Illustrates the correct way to handle inputs in Dask delayed functions by returning new values or copies instead of mutating inputs directly.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Return new values or copies\n\n@dask.delayed\ndef f(x):\n    x = x + 1\n    return x\n```\n\n----------------------------------------\n\nTITLE: Creating a Dask Array with Sparse Data in Python\nDESCRIPTION: This snippet demonstrates how to create a Dask array with mostly zero values, which is suitable for sparse representation. It uses Dask's random number generation and array manipulation functions.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-sparse.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nrng = da.random.default_rng()\nx = rng.random((100000, 100000), chunks=(1000, 1000))\nx[x < 0.95] = 0\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dask Task Graph with Ordering\nDESCRIPTION: This code uses dask.visualize to create a visual representation of the task graph with ordering information. It applies the 'evaluate' function to slices of the input arrays and sets visualization parameters.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/order.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask\n>>> n = 125 * 4\n>>> dask.visualize(evaluate(x1[:n], y1[:n], x2[:n], y2[:n]),\n...                optimize_graph=True, color=\"order\",\n...                cmap=\"autumn\", node_attr={\"penwidth\": \"4\"})\n```\n\n----------------------------------------\n\nTITLE: Moving Data with Dask Futures in Python\nDESCRIPTION: Demonstrates how to gather results from futures and scatter data to workers. The gather method can efficiently collect multiple results concurrently.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Gathering results\nc.result()\nresults = client.gather(futures)\n\n# Scattering data\ndf = pd.read_csv('training-data.csv')\nremote_df = client.scatter(df)\nfuture = client.submit(my_function, remote_df)\n```\n\n----------------------------------------\n\nTITLE: Rechunking Dask Arrays\nDESCRIPTION: This snippet shows how to use the rechunk method to change the chunking layout of a Dask array, which can be useful for optimizing certain operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-chunks.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nx = x.rechunk((50, 1000))\n\nx = x.rechunk(1000)\nx = x.rechunk((50, 1000))\nx = x.rechunk({0: 50, 1: 1000})\n```\n\n----------------------------------------\n\nTITLE: Dask Bag Chaining Operations\nDESCRIPTION: Demonstrates chaining multiple Dask Bag operations together using zip and map transformations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n>>> c = db.zip(b, b.map(lambda x: x * 10))\n... c\ndask.bag<zip, npartitions=2>\n\n>>> c.compute()\n[(1, 10), (2, 20), (3, 30), (4, 40), (5, 50), (6, 60), (2, 20), (1, 10)]\n```\n\n----------------------------------------\n\nTITLE: Configuring CuPy Backend for Dask Array\nDESCRIPTION: Demonstrates how to change the default Dask Array backend to CuPy using configuration settings. Shows creation of array with alternate backend and converting existing arrays.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/selecting-the-collection-backend.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask\n>>> import dask.array as da\n>>> with dask.config.set({\"array.backend\": \"cupy\"}):\n...     darr = da.ones(10, chunks=(5,))  # Get cupy-backed collection\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask\n>>> import dask.array as da\n>>> darr = da.ones(10, chunks=(5,))  # Creates numpy-backed collection\n>>> with dask.config.set({\"array.backend\": \"cupy\"}):\n...     darr = darr.to_backend()  # Moves numpy data to cupy\n```\n\n----------------------------------------\n\nTITLE: Creating Dask Array from Dask DataFrame in Python\nDESCRIPTION: This snippet shows different methods to create a Dask array from a Dask DataFrame, including using to_dask_array(), values attribute, and map_partitions with np.asarray.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n>>> df = dask.dataframes.from_pandas(...)\n>>> df.to_dask_array()\ndask.array<values, shape=(nan, 3), dtype=float64, chunksize=(nan, 3), chunktype=numpy.ndarray>\n\n>>> df.values\ndask.array<values, shape=(nan, 3), dtype=float64, chunksize=(nan, 3), chunktype=numpy.ndarray>\n\n>>> df.to_dask_array(lengths=True)\ndask.array<array, shape=(100, 3), dtype=float64, chunksize=(50, 3), chunktype=numpy.ndarray>\n\n>>> df.to_records()\ndask.array<to_records, shape=(nan,), dtype=(numpy.record, [('index', '<i8'), ('x', '<f8'), ('y', '<f8'), ('z', '<f8')]), chunksize=(nan,), chunktype=numpy.ndarray>\n\n>>> df.map_partitions(np.asarray)\ndask.array<asarray, shape=(nan, 3), dtype=float64, chunksize=(nan, 3), chunktype=numpy.ndarray>\n```\n\n----------------------------------------\n\nTITLE: Optimizing Dask Computation Graph\nDESCRIPTION: Example showing how to optimize a Dask computation graph separately from computation using dask.optimize. This allows profiling of the optimization phase independently.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/phases-of-computation.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# x, y = dask.compute(x, y)\nx, y = dask.optimize(x, y)\n```\n\n----------------------------------------\n\nTITLE: Handling Mutable Operations in Dask Delayed Functions\nDESCRIPTION: Demonstrates how to safely perform mutable operations in Dask delayed functions by making a copy of the input first.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dask.delayed\ndef f(x):\n    x = copy(x)\n    x += 1\n    return x\n```\n\n----------------------------------------\n\nTITLE: Creating Dask Array from HDF5 File using NumPy-style Slicing in Python\nDESCRIPTION: This snippet demonstrates how to create a Dask array from an HDF5 file using h5py and NumPy-style slicing. It shows the lazy loading nature of Dask arrays.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> import h5py\n>>> f = h5py.File('myfile.hdf5') # HDF5 file\n>>> d = f['/data/path']          # Pointer on on-disk array\n>>> d.shape                      # d can be very large\n(1000000, 1000000)\n\n>>> x = d[:5, :5]                # We slice to get numpy arrays\n\n>>> import dask.array as da\n>>> x = da.from_array(d, chunks=(1000, 1000))\n```\n\n----------------------------------------\n\nTITLE: Using the 'as_gufunc' Decorator for Python Functions in Dask\nDESCRIPTION: This snippet demonstrates the use of the 'as_gufunc' decorator to convert a Python function into a generalized ufunc. It defines a function to compute the mean along the last axis and applies it to a Dask array.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-gufunc.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nx = da.random.normal(size=(10, 5), chunks=(2, 5))\n\n@da.as_gufunc(signature=\"(i)->()\", output_dtypes=float, vectorize=True)\ndef gufoo(x):\n    return np.mean(x, axis=-1)\n\ny = gufoo(x)\n```\n\n----------------------------------------\n\nTITLE: Avoiding Global State in Dask Delayed Functions\nDESCRIPTION: Shows an example of what to avoid when using Dask delayed functions, specifically referencing global variables which can cause issues in multiprocessing or distributed computing.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nL = []\n\n# This references global variable L\n\n@dask.delayed\ndef f(x):\n    L.append(x)\n```\n\n----------------------------------------\n\nTITLE: DataFrame Join Operations with Performance Implications\nDESCRIPTION: Demonstrates different types of joins between DataFrames and their performance characteristics.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-best-practices.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndd.merge(a, pandas_df)  # fast\ndd.merge(a, b, left_index=True, right_index=True)  # fast\ndd.merge(a, b, left_index=True, right_on='id')  # half-fast, half-slow\ndd.merge(a, b, left_on='id', right_on='id')  # slow\n```\n\n----------------------------------------\n\nTITLE: Implementing a Scheduler Plugin in Python for Dask\nDESCRIPTION: This code snippet demonstrates how to create a custom scheduler plugin for Dask. It includes a class that inherits from SchedulerPlugin and implements custom behavior for adding workers. The script also uses Click to handle command-line arguments.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/customize-initialization.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# scheduler-setup.py\nimport click\n\nfrom distributed.diagnostics.plugin import SchedulerPlugin\n\nclass MyPlugin(SchedulerPlugin):\n    def __init__(self, print_count):\n      self.print_count = print_count\n      super().__init__()\n\n    def add_worker(self, scheduler=None, worker=None, **kwargs):\n        print(\"Added a new worker at:\", worker)\n        if self.print_count and scheduler is not None:\n            print(\"Total workers:\", len(scheduler.workers))\n\n@click.command()\n@click.option(\"--print-count/--no-print-count\", default=False)\ndef dask_setup(scheduler, print_count):\n    plugin = MyPlugin(print_count)\n    scheduler.add_plugin(plugin)\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns in Dask DataFrame using Python\nDESCRIPTION: Demonstrates how to select multiple columns and a single column from a Dask DataFrame. Selecting multiple columns returns a Dask DataFrame, while selecting a single column returns a Dask Series.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-indexing.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf[['B', 'A']]\nDask DataFrame Structure:\n                     B      A\nnpartitions=1\na              int64  int64\nc                ...    ...\nDask Name: getitem, 2 tasks\n\n>>> ddf['A']\nDask Series Structure:\nnpartitions=1\na    int64\nc      ...\nName: A, dtype: int64\nDask Name: getitem, 2 tasks\n```\n\n----------------------------------------\n\nTITLE: Performing FFT Operations with Dask Array in Python\nDESCRIPTION: Example of using Dask Array's FFT module to perform Fourier transforms on large datasets.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-api.rst#2025-04-19_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport dask.array as da\n\n# Create a large array\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\n\n# Perform FFT\nfft_result = da.fft.fft2(x)\n\n# Compute the result\nresult = fft_result.compute()\n```\n\n----------------------------------------\n\nTITLE: Implementing nunique Aggregation for Multiple Columns\nDESCRIPTION: Example of implementing a custom nunique (number of unique values) aggregation that can be applied to multiple columns of a grouped DataFrame. The aggregation handles set operations to count unique values.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> df['c'] = [1, 2, 1, 1, 2]\n>>> ddf = dd.from_pandas(df, 2)\n>>> nunique = dd.Aggregation(\n...     name=\"nunique\",\n...     chunk=lambda s: s.apply(lambda x: list(set(x))),\n...     agg=lambda s0: s0.obj.groupby(level=list(range(s0.obj.index.nlevels))).sum(),\n...     finalize=lambda s1: s1.apply(lambda final: len(set(final))),\n... )\n>>> ddf.groupby('a').agg({'b':nunique, 'c':nunique})\n```\n\n----------------------------------------\n\nTITLE: Creating Memory-Mapped Dask Array from Raw Binary File in Python\nDESCRIPTION: This code defines functions to create a memory-mapped Dask array from a raw binary file. It's particularly efficient for accessing data already in the file system cache and for extracting arbitrary subsets without optimizing the chunking scheme.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport dask\nimport dask.array as da\n\n\ndef mmap_load_chunk(filename, shape, dtype, offset, sl):\n    '''\n    Memory map the given file with overall shape and dtype and return a slice\n    specified by :code:`sl`.\n\n    Parameters\n    ----------\n\n    filename : str\n    shape : tuple\n        Total shape of the data in the file\n    dtype:\n        NumPy dtype of the data in the file\n    offset : int\n        Skip :code:`offset` bytes from the beginning of the file.\n    sl:\n        Object that can be used for indexing or slicing a NumPy array to\n        extract a chunk\n\n    Returns\n    -------\n\n    numpy.memmap or numpy.ndarray\n        View into memory map created by indexing with :code:`sl`,\n        or NumPy ndarray in case no view can be created using :code:`sl`.\n    '''\n    data = np.memmap(filename, mode='r', shape=shape, dtype=dtype, offset=offset)\n    return data[sl]\n\n\ndef mmap_dask_array(filename, shape, dtype, offset=0, blocksize=5):\n    '''\n    Create a Dask array from raw binary data in :code:`filename`\n    by memory mapping.\n\n    This method is particularly effective if the file is already\n    in the file system cache and if arbitrary smaller subsets are\n    to be extracted from the Dask array without optimizing its\n    chunking scheme.\n\n    It may perform poorly on Windows if the file is not in the file\n    system cache. On Linux it performs well under most circumstances.\n\n    Parameters\n    ----------\n\n    filename : str\n    shape : tuple\n        Total shape of the data in the file\n    dtype:\n        NumPy dtype of the data in the file\n    offset : int, optional\n        Skip :code:`offset` bytes from the beginning of the file.\n    blocksize : int, optional\n    '''\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Cache Usage in Dask\nDESCRIPTION: Example showing how the cache affects computation speed and handles multiple operations across different columns.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/caching.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> df.amount.max().compute()  # slow the first time\n1000\n>>> df.amount.min().compute()  # fast because df.amount is in the cache\n-1000\n>>> df.id.nunique().compute()  # starts to push out df.amount from cache\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Aggregations with groupby\nDESCRIPTION: Example of using multiple built-in reduction functions with Dask's aggregate method on grouped data. This allows computing several statistics in a single operation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf.groupby(columns).aggregate(['sum', 'mean', 'max', 'min', list])\n```\n\n----------------------------------------\n\nTITLE: Aligning Dask Array Chunks with HDF5 Storage in Python\nDESCRIPTION: This snippet demonstrates how to create a Dask array from an HDF5 file, aligning the Dask chunks with the storage chunks for optimal performance. It shows how to inspect the HDF5 chunk size and create a Dask array with a multiple of that size.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-best-practices.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import h5py\n>>> storage = h5py.File('myfile.hdf5')['x']\n>>> storage.chunks\n(128, 64)\n\n>>> import dask.array as da\n>>> x = da.from_array(storage, chunks=(1280, 6400))\n```\n\n----------------------------------------\n\nTITLE: Concatenating Dask Arrays with da.concatenate\nDESCRIPTION: Shows how to use da.concatenate to combine Dask arrays along an existing dimension. The example demonstrates concatenation along different axes and the resulting shapes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-stack.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.array as da\n>>> import numpy as np\n\n>>> arr0 = da.from_array(np.zeros((3, 4)), chunks=(1, 2))\n>>> arr1 = da.from_array(np.ones((3, 4)), chunks=(1, 2))\n\n>>> data = [arr0, arr1]\n\n>>> x = da.concatenate(data, axis=0)\n>>> x.shape\n(6, 4)\n\n>>> da.concatenate(data, axis=1).shape\n(3, 8)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from PyArrow Dataset using from_map\nDESCRIPTION: Shows how to convert a PyArrow Dataset into a Dask DataFrame using the from_map function to process fragments.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-create.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow.dataset as ds\n>>> dataset = ds.dataset(\"hive_data_path\", format=\"orc\", partitioning=\"hive\")\n>>> fragments = dataset.get_fragments()\n>>> func = lambda frag: frag.to_table().to_pandas()\n>>> df = dd.from_map(func, fragments)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Complex Custom Aggregation with Intermediates\nDESCRIPTION: Example of implementing a custom mean aggregation that requires tracking multiple intermediate values (count and sum) across the chunk and aggregate phases before finalizing the result.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncustom_mean = dd.Aggregation(\n    'custom_mean',\n    lambda s: (s.count(), s.sum()),\n    lambda count, sum: (count.sum(), sum.sum()),\n    lambda count, sum: sum / count,\n)\nddf.groupby('g').agg(custom_mean)\n```\n\n----------------------------------------\n\nTITLE: Concatenating Dask Arrays from Multiple HDF5 Files in Python\nDESCRIPTION: This example demonstrates how to create Dask arrays from multiple HDF5 files and concatenate them along the first axis.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndask_arrays = []\nfor fn in filenames:\n    f = h5py.File(fn)\n    d = f['/data']\n    array = da.from_array(d, chunks=(1000, 1000))\n    dask_arrays.append(array)\n\nx = da.concatenate(dask_arrays, axis=0)  # concatenate arrays along first axis\n```\n\n----------------------------------------\n\nTITLE: Linear Algebra Operations with Dask Array in Python\nDESCRIPTION: Demonstration of using Dask Array's linear algebra module for operations like matrix inversion and solving linear systems.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-api.rst#2025-04-19_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport dask.array as da\n\n# Create a large matrix\nA = da.random.random((10000, 10000), chunks=(1000, 1000))\n\n# Invert the matrix\nA_inv = da.linalg.inv(A)\n\n# Solve a linear system\nb = da.random.random((10000, 1), chunks=(1000, 1))\nx = da.linalg.solve(A, b)\n```\n\n----------------------------------------\n\nTITLE: Writing Parquet Data with Hive Partitioning in Python\nDESCRIPTION: This code snippet shows how to write a Dask DataFrame to Parquet format using hive partitioning. The 'partition_on' parameter specifies the columns to use for partitioning.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-hive.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> df.to_parquet(\"output-path\", partition_on=[\"year\", \"semester\"])\n\n>>> os.listdir(\"output-path\")\n[\"year=2022\", \"year=2023\"]\n\n>>> os.listdir(\"output-path/year=2022\")\n[\"semester=fall\", \"semester=spring\"]\n\n>>> os.listdir(\"output-path/year=2022/semester=spring\")\n['part.0.parquet', 'part.1.parquet']\n```\n\n----------------------------------------\n\nTITLE: Creating a Dask Cluster with Python KubeCluster\nDESCRIPTION: This code snippet shows how to create a Dask cluster using the KubeCluster class from the dask_kubernetes.operator module. It creates a cluster named 'my-dask-cluster' and scales it to 10 workers.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-kubernetes.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create a cluster in Python\nfrom dask_kubernetes.operator import KubeCluster\ncluster = KubeCluster(name=\"my-dask-cluster\", image='ghcr.io/dask/dask:latest')\ncluster.scale(10)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask Multiprocessing Scheduler in Python\nDESCRIPTION: Sets the Dask scheduler to use multiple processes. This bypasses GIL issues but introduces data transfer overhead between processes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduling.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dask\ndask.config.set(scheduler='processes')  # overwrite default with multiprocessing scheduler\n```\n\n----------------------------------------\n\nTITLE: Connecting to Dask Cluster from Jupyter\nDESCRIPTION: Shows how to establish a connection from a Jupyter notebook to a running Dask cluster. This code initializes a Dask Client object that connects to the scheduler container by hostname.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-docker.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nclient = Client(\"scheduler:8786\")\nclient\n```\n\n----------------------------------------\n\nTITLE: Scaling Dask Cluster Manually and Automatically in Python\nDESCRIPTION: These snippets show how to manually scale a Dask cluster to a specific number of workers and how to set up automatic scaling based on workload. This is useful for managing cluster resources efficiently.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> cluster.scale(10)  # Sets the number of workers to 10\n\n>>> cluster.adapt(minimum=1, maximum=10)  # Allows the cluster to auto scale to 10 when tasks are computed\n```\n\n----------------------------------------\n\nTITLE: Writing Dataframes to Parquet\nDESCRIPTION: Basic examples of writing Dask dataframes to Parquet format. Shows how to write to local directories or remote storage systems like S3.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-parquet.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Write to a local directory\n>>> df.to_parquet(\"path/to/my/parquet/\")\n\n# Write to S3\n>>> df.to_parquet(\"s3://bucket-name/my/parquet/\")\n```\n\n----------------------------------------\n\nTITLE: Thread Pool Configuration in Dask\nDESCRIPTION: Examples of configuring thread pools in Dask, including setting the number of workers and using ThreadPoolExecutor.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduler-overview.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from concurrent.futures import ThreadPoolExecutor\n>>> with dask.config.set(pool=ThreadPoolExecutor(4)):\n...     x.compute()\n```\n\n----------------------------------------\n\nTITLE: Using map_overlap for Custom Rolling Operations in Dask DataFrame\nDESCRIPTION: Adds map_overlap method to Dask DataFrames for performing custom rolling operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_82\n\nLANGUAGE: Python\nCODE:\n```\ndf.map_overlap(func, before, after)\n```\n\n----------------------------------------\n\nTITLE: Broadcasting Assignment Operations in Dask Array\nDESCRIPTION: Demonstrates various broadcasting assignment operations using Dask arrays, including scalar assignment, slice assignment, and array assignment. Shows how NumPy broadcasting rules are applied in Dask.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-assignment.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> x = da.zeros((2, 6))\n>>> x[0] = 1\n>>> x[..., 1] = 2.0\n>>> x[:, 2] = [3, 4]\n>>> x[:, 5:2:-2] = [[6, 5]]\n>>> x.compute()\narray([[1., 2., 3., 5., 1., 6.],\n       [0., 2., 4., 5., 0., 6.]])\n>>> x[1] = -x[0]\n>>> x.compute()\narray([[ 1.,  2.,  3.,  5.,  1.,  6.],\n       [-1., -2., -3., -5., -1., -6.]])\n```\n\n----------------------------------------\n\nTITLE: Partitioning SQL Data with Hex ID Divisions in Dask\nDESCRIPTION: Demonstrates how to load SQL table data with specific divisions for partitioning using hexadecimal ID values as the index column. This approach works with string columns or any data with a natural ordering.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-sql.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = read_sql_table(\"mytable\", divisions=list(\"0123456789abcdefh\"),\n                    index_col=\"hexID\")\n```\n\n----------------------------------------\n\nTITLE: Long-Running Monitor Tasks in Python\nDESCRIPTION: Implementation of long-running monitor tasks that watch resources and submit processing tasks to the cluster.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef monitor(device):\n   client = get_client()\n   while True:\n       data = device.read_data()\n       future = client.submit(process, data)\n       fire_and_forget(future)\n\nfor device in devices:\n    fire_and_forget(client.submit(monitor))\n```\n\n----------------------------------------\n\nTITLE: Creating a Dask Cluster with Dask Gateway\nDESCRIPTION: This Python code demonstrates how to create a Dask cluster using Dask Gateway. It connects to a Gateway server and creates a new cluster.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-kubernetes.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_gateway import Gateway\ngateway = Gateway(\"<gateway service address>\")\ncluster = gateway.new_cluster()\n```\n\n----------------------------------------\n\nTITLE: GCS Authentication Using Session Credentials in Python\nDESCRIPTION: Example showing how to authenticate with Google Cloud Storage by passing GCSFileSystem credentials directly through storage_options. This method is fast but less secure as credentials are passed around the cluster.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/connect-to-remote-data.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ngcs = GCSFileSystem(...)\ndask_function(..., storage_options={'token': gcs.session.credentials})\n```\n\n----------------------------------------\n\nTITLE: Managing Indirect Dependencies with Dask Bind in Python\nDESCRIPTION: Using dask.graph_manipulation.bind to create dependencies between tasks that don't directly share data but depend on side effects, ensuring proper execution order.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dask\nfrom dask.graph_manipulation import bind\n\nDATA = []\n\n@dask.delayed\ndef inc(x):\n    return x + 1\n\n@dask.delayed\ndef add_data(x):\n    DATA.append(x)\n\n@dask.delayed\ndef sum_data(x):\n    return sum(DATA) + x\n\na = inc(1)\nb = add_data(a)\nc = inc(3)\nd = add_data(c)\ne = inc(5)\nf = bind(sum_data, [b, d])(e)\nf.compute()\n```\n\n----------------------------------------\n\nTITLE: Implementing Modern Random Number Generation in Python using Dask\nDESCRIPTION: Demonstrates the recommended modern approach for random number generation using Dask's Generator class. Uses default_rng() to create a Generator instance and generate standard normal distributions.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-random.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Do this (new version)\nimport dask.array as da\nrng = da.random.default_rng()\nvals = rng.standard_normal(10)\nmore_vals = rng.standard_normal(10)\n```\n\n----------------------------------------\n\nTITLE: Creating and Slicing Large Dask Array\nDESCRIPTION: Example showing how to create a trillion-element Dask array and efficiently slice it after an exponential operation. Demonstrates how Dask computes only necessary blocks.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-slicing.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> # Trillion element array of ones, in 1000 by 1000 blocks\n>>> x = da.ones((1000000, 1000000), chunks=(1000, 1000))\n\n>>> da.exp(x)[:1500, :1500]\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Dask Graph Structure in Python\nDESCRIPTION: Example showing the basic structure of a Dask graph using a dictionary to map keys to computations. Demonstrates how tasks and values are represented.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/spec.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n{'x': 1,\n 'y': 2,\n 'z': (add, 'x', 'y'),\n 'w': (sum, ['x', 'y', 'z']),\n 'v': [(sum, ['w', 'z']), 2]}\n```\n\n----------------------------------------\n\nTITLE: Defining Computation on Dask Arrays\nDESCRIPTION: This snippet defines a function 'evaluate' that performs computations on four input Dask arrays. It stacks arrays, performs element-wise operations, and calculates means.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/order.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> def evaluate(x1, y1, x2, y2):\n...     u = da.stack([x1, y1])\n...     v = da.stack([x2, y2])\n...     components = [u, v, u ** 2 + v ** 2]\n...     return [\n...         abs(c[0] - c[1]).mean(axis=-1)\n...         for c in components\n...     ]\n>>> results = evaluate(x1, y1, x2, y2)\n```\n\n----------------------------------------\n\nTITLE: Combining Dask Arrays with da.block\nDESCRIPTION: Illustrates the use of da.block to combine Dask arrays over multiple dimensions simultaneously. This is useful for tiling arrays in a 2-D plane or other multi-dimensional arrangements.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-stack.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.array as da\n>>> import numpy as np\n\n>>> arr0 = da.from_array(np.zeros((3, 4)), chunks=(1, 2))\n>>> arr1 = da.from_array(np.ones((3, 4)), chunks=(1, 2))\n\n>>> data = [\n...     [arr0, arr1],\n...     [arr1, arr0]\n... ]\n\n>>> x = da.block(data)\n>>> x.shape\n(6, 8)\n```\n\n----------------------------------------\n\nTITLE: Dask Array Rechunking with Mixed Strategies\nDESCRIPTION: Example of rechunking a 3D array using different chunk specifications including no chunking (-1) and auto chunking.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-chunks.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nx = x.rechunk({0: -1, 1: 'auto', 2: 'auto'})\n```\n\n----------------------------------------\n\nTITLE: Using Dask's Distributed Print Function in Python\nDESCRIPTION: Demonstrates how to use Dask's distributed print function to forward output to the client-side Python session, making distributed debugging feel more like local debugging.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/debug.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import print\n```\n\n----------------------------------------\n\nTITLE: Launching Dask cluster with PBSCluster in Python\nDESCRIPTION: This snippet demonstrates how to create a Dask cluster using PBSCluster from dask-jobqueue. It specifies cluster parameters such as cores, memory, project, queue, interface, and walltime, then scales the cluster to 100 workers.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-hpc.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_jobqueue import PBSCluster\n\ncluster = PBSCluster(cores=36,\n                        memory=\"100GB\",\n                        project='P48500028',\n                        queue='premium',\n                        interface='ib0',\n                        walltime='02:00:00')\n\ncluster.scale(100)  # Start 100 workers in 100 jobs that match the description above\n\nfrom dask.distributed import Client\nclient = Client(cluster)    # Connect to that cluster\n```\n\n----------------------------------------\n\nTITLE: Basic Dask Scheduler Usage with get Function\nDESCRIPTION: Demonstrates the basic usage of Dask's get function to compute results from a task graph. Shows how to define a simple graph and compute single or multiple results.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduler-overview.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from operator import add\n\n>>> dsk = {'a': 1,\n...        'b': 2,\n...        'c': (add, 'a', 'b'),\n...        'd': (sum, ['a', 'b', 'c'])}\n\n>>> get(dsk, 'c')\n3\n\n>>> get(dsk, 'd')\n6\n\n>>> get(dsk, ['a', 'b', 'c'])\n[1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Memory Mapping Dask Array Example\nDESCRIPTION: Example showing how to create a memory-mapped Dask array with specified filename, shape, and dtype parameters.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nx = mmap_dask_array(\n    filename='testfile-50-50-100-100-float32.raw',\n    shape=(50, 50, 100, 100),\n    dtype=np.float32\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Counter with Dask Actor in Python\nDESCRIPTION: Shows how to create a stateful Counter using Dask's Actor model. The Counter class maintains state and provides an increment method, demonstrating how actors can manage state without coordinating with the central scheduler.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass Counter:\n    n = 0\n\n    def __init__(self):\n        self.n = 0\n\n    def increment(self):\n        self.n += 1\n        return self.n\n\nfrom dask.distributed import Client\n\nif __name__ == '__main__':\n    client = Client()\n\n    future = client.submit(Counter, actor=True)\n    counter = future.result()\n```\n\n----------------------------------------\n\nTITLE: Local Dask Cluster Setup\nDESCRIPTION: Shows how to set up a local Dask cluster for parallel processing within a single machine.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/index.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import LocalCluster\ncluster = LocalCluster()\nclient = cluster.get_client()\n\n# Normal Dask work ...\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Custom Dask Collection Usage in Python\nDESCRIPTION: Example usage of the custom Dask Tuple collection, showing how to create, compute, and persist the collection.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/custom-collections.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from dask_tuple import Tuple\n>>> from operator import add, mul\n\n# Define a dask graph\n>>> dsk = {\"k0\": 1,\n...        (\"x\", \"k1\"): 2,\n...        (\"x\", 1): (add, \"k0\", (\"x\", \"k1\")),\n...        (\"x\", 2): (mul, (\"x\", \"k1\"), 2),\n...        (\"x\", 3): (add, (\"x\", \"k1\"), (\"x\", 1))}\n\n# The output keys for this graph.\n# The first element of each tuple must be the same across the whole collection;\n# the remainder are arbitrary, unique str, bytes, int, or floats\n>>> keys = [(\"x\", \"k1\"), (\"x\", 1), (\"x\", 2), (\"x\", 3)]\n\n>>> x = Tuple(dsk, keys)\n\n# Compute turns Tuple into a tuple\n>>> x.compute()\n(2, 3, 4, 5)\n\n# Persist turns Tuple into a Tuple, with each task already computed\n>>> x2 = x.persist()\n>>> isinstance(x2, Tuple)\nTrue\n>>> x2.__dask_graph__()\n{('x', 'k1'): 2, ('x', 1): 3, ('x', 2): 4, ('x', 3): 5}\n>>> x2.compute()\n(2, 3, 4, 5)\n\n# Run-time typechecking\n>>> from dask.typing import DaskCollection\n>>> isinstance(x, DaskCollection)\nTrue\n```\n\n----------------------------------------\n\nTITLE: Using PipInstall Plugin with Environment Variables in Python\nDESCRIPTION: Example of using the distributed.PipInstall plugin with environment variables to securely install a package from a private repository in Dask.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import PipInstall\nplugin = PipInstall(packages=[\"private_package@git+https://${TOKEN}@github.com/dask/private_package.git])\nclient.register_plugin(plugin)\n```\n\n----------------------------------------\n\nTITLE: Using Performance Report in Dask\nDESCRIPTION: Demonstrates how to use the performance_report context manager to save comprehensive diagnostics dashboards including task streams, worker profiles, and bandwidths.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/diagnostics-distributed.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import performance_report\n\nwith performance_report(filename=\"dask-report.html\"):\n    ## some dask computation\n```\n\n----------------------------------------\n\nTITLE: Creating Overlapping Blocks in Dask Array\nDESCRIPTION: This snippet demonstrates how to create overlapping blocks in a Dask array using the overlap function. It shows how to specify depth and boundary conditions for the overlapping regions.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-overlap.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.array as da\n>>> import numpy as np\n\n>>> x = np.arange(64).reshape((8, 8))\n>>> d = da.from_array(x, chunks=(4, 4))\n>>> d.chunks\n((4, 4), (4, 4))\n\n>>> g = da.overlap.overlap(d, depth={0: 2, 1: 1},\n...                       boundary={0: 100, 1: 'reflect'})\n>>> g.chunks\n((8, 8), (6, 6))\n\n>>> np.array(g)\narray([[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100],\n       [  0,   0,   1,   2,   3,   4,   3,   4,   5,   6,   7,   7],\n       [  8,   8,   9,  10,  11,  12,  11,  12,  13,  14,  15,  15],\n       [ 16,  16,  17,  18,  19,  20,  19,  20,  21,  22,  23,  23],\n       [ 24,  24,  25,  26,  27,  28,  27,  28,  29,  30,  31,  31],\n       [ 32,  32,  33,  34,  35,  36,  35,  36,  37,  38,  39,  39],\n       [ 40,  40,  41,  42,  43,  44,  43,  44,  45,  46,  47,  47],\n       [ 16,  16,  17,  18,  19,  20,  19,  20,  21,  22,  23,  23],\n       [ 24,  24,  25,  26,  27,  28,  27,  28,  29,  30,  31,  31],\n       [ 32,  32,  33,  34,  35,  36,  35,  36,  37,  38,  39,  39],\n       [ 40,  40,  41,  42,  43,  44,  43,  44,  45,  46,  47,  47],\n       [ 48,  48,  49,  50,  51,  52,  51,  52,  53,  54,  55,  55],\n       [ 56,  56,  57,  58,  59,  60,  59,  60,  61,  62,  63,  63],\n       [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]])\n```\n\n----------------------------------------\n\nTITLE: Implementing make_meta Dispatch Methods in Python\nDESCRIPTION: Code for registering custom make_meta dispatch methods to handle custom DataFrame, Series and Index types. These methods return empty versions of non-Dask objects.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-extend.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.dataframe.dispatch import make_meta_dispatch\n\n@make_meta_dispatch.register(MyDataFrame)\ndef make_meta_dataframe(df, index=None):\n    return df.head(0)\n\n\n@make_meta_dispatch.register(MySeries)\ndef make_meta_series(s, index=None):\n    return s.head(0)\n\n\n@make_meta_dispatch.register(MyIndex)\ndef make_meta_index(ind, index=None):\n    return ind[:0]\n```\n\n----------------------------------------\n\nTITLE: Creating a Parameter Server with Dask Actor in Python\nDESCRIPTION: Implements a parameter server using Dask Actors to perform distributed optimization. The example minimizes a simple quadratic function where the parameter server holds the model parameters and clients calculate gradients.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom dask.distributed import Client\nclient = Client(processes=False)\n\nclass ParameterServer:\n    def __init__(self):\n        self.data = dict()\n\n    def put(self, key, value):\n        self.data[key] = value\n\n    def get(self, key):\n        return self.data[key]\n\ndef train(params, lr=0.1):\n    grad = 2 * (params - 1)  # gradient of (params - 1)**2\n    new_params = params - lr * grad\n    return new_params\n\nps_future = client.submit(ParameterServer, actor=True)\nps = ps_future.result()\n\nps.put('parameters', np.random.default_rng().random(1000))\nfor k in range(20):\n    params = ps.get('parameters').result()\n    new_params = train(params)\n    ps.put('parameters', new_params)\n    print(new_params.mean())\n    # k=0: \"0.5988202981316124\"\n    # k=10: \"0.9569236575164062\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Dask Distributed Scheduler Locally in Python\nDESCRIPTION: Sets up the Dask distributed scheduler to run locally. This provides access to asynchronous APIs, a diagnostic dashboard, and more sophisticated data locality handling.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduling.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nclient = Client()\n# or\nclient = Client(processes=False)\n```\n\n----------------------------------------\n\nTITLE: Handling Categorical Data with Parquet I/O\nDESCRIPTION: Shows how to preserve categorical information when writing to and reading from parquet format, including manually setting categories after reading.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-categoricals.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.dataframe as dd\n>>> import pandas as pd\n>>> df = pd.DataFrame(data=list('abcaabbcc'), columns=['col'])\n>>> df.col = df.col.astype('category')\n>>> ddf = dd.from_pandas(df, npartitions=1)\n>>> ddf.col.cat.known\nTrue\n>>> ddf.to_parquet('tmp')\n>>> ddf2 = dd.read_parquet('tmp')\n>>> ddf2.col.cat.known\nFalse\n>>> ddf2.col = ddf2.col.cat.set_categories(ddf2.col.head(1).cat.categories)\n>>> ddf2.col.cat.known\nTrue\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns from Parquet\nDESCRIPTION: Example of reading only specific columns from a Parquet dataset to reduce I/O and memory usage. This is important for optimizing performance when working with large datasets.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-parquet.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> dd.read_parquet(\n...     \"s3://path/to/myparquet/\",\n...     columns=[\"a\", \"b\", \"c\"]  # Only read columns 'a', 'b', and 'c'\n... )\n```\n\n----------------------------------------\n\nTITLE: Rerunning Failed Tasks Locally in Dask Python\nDESCRIPTION: Demonstrates how to rerun failed tasks locally for both single-machine and distributed schedulers in Dask, allowing for easier debugging of exceptions.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/debug.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nx.compute(rerun_exceptions_locally=True)\n```\n\nLANGUAGE: python\nCODE:\n```\nfuture = client.compute(x)\nclient.recreate_error_locally(future)\n```\n\n----------------------------------------\n\nTITLE: Using the Dask Get Function in Python\nDESCRIPTION: Demonstration of using the get function to retrieve computed values from a Dask graph, including single key and multiple key access patterns.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/spec.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from dask.threaded import get\n\n>>> from operator import add\n\n>>> dsk = {'x': 1,\n...        'y': 2,\n...        'z': (add, 'x', 'y'),\n...        'w': (sum, ['x', 'y', 'z'])}\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> get(dsk, 'x')\n1\n\n>>> get(dsk, 'z')\n3\n\n>>> get(dsk, 'w')\n6\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> get(dsk, ['x', 'y', 'z'])\n[1, 2, 3]\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> get(dsk, [['x', 'y'], ['z', 'w']])\n[[1, 2], [3, 6]]\n```\n\n----------------------------------------\n\nTITLE: Submitting Dask jobs using SGE's qsub command in Bash\nDESCRIPTION: This snippet demonstrates how to submit Dask scheduler and worker jobs using SGE's qsub command. It starts a scheduler and 100 worker processes as array jobs.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-hpc.rst#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Start a dask-scheduler somewhere and write the connection information to a file\nqsub -b y /path/to/dask-scheduler --scheduler-file /home/$USER/scheduler.json\n\n# Start 100 dask-worker processes in an array job pointing to the same file\nqsub -b y -t 1-100 /path/to/dask-worker --scheduler-file /home/$USER/scheduler.json\n```\n\n----------------------------------------\n\nTITLE: Reading Hive-Partitioned Parquet Data in Python\nDESCRIPTION: This code snippet shows how to read hive-partitioned Parquet data using Dask. By default, hive-partitioned columns are interpreted as categorical columns.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-hive.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf = dd.read_parquet(\"output-path\", columns=[\"year\", \"semester\"])\n\n>>> ddf\nDask DataFrame Structure:\n                        year         semester\nnpartitions=4                                  \n            category[known]  category[known]\n                        ...              ...\n                        ...              ...\n                        ...              ...\n                        ...              ...\nDask Name: read-parquet, 1 graph layer\n\n>>> ddf.compute()\nyear semester\n0  2022     fall\n1  2022     fall\n2  2022     fall\n3  2022   spring\n4  2022   spring\n5  2022   spring\n6  2023     fall\n7  2023     fall\n```\n\n----------------------------------------\n\nTITLE: Waiting on Dask Futures in Python\nDESCRIPTION: Demonstrates how to wait for futures to complete and iterate over completed futures. The as_completed function allows processing results as they become available.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import wait, as_completed\n\nwait(futures)\n\nfor future in as_completed(futures):\n    y = future.result()\n    if y > best:\n        best = y\n\nfor future, result in as_completed(futures, with_results=True):\n    # Process result\n\nfor batch in as_completed(futures, with_results=True).batches():\n    for future, result in batch:\n        # Process batch of results\n```\n\n----------------------------------------\n\nTITLE: Converting Dask Array Chunks to Sparse COO Format in Python\nDESCRIPTION: This code converts each chunk of the Dask array into a sparse.COO array using the map_blocks function. This transformation changes the internal representation to sparse format while maintaining the same semantic behavior.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-sparse.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport sparse\ns = x.map_blocks(sparse.COO)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask Preloads using YAML\nDESCRIPTION: This YAML configuration snippet shows how to specify preload scripts and their arguments for Dask scheduler, worker, nanny, and client. It demonstrates various ways to define preloads, including Python text, file paths, and module names.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/customize-initialization.rst#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndistributed:\n  scheduler:\n    preload:\n    - \"import os; os.environ['A'] = 'b'\"  # use Python text\n    - /path/to/myfile.py                  # or a filename\n    - my_module                           # or a module name\n    preload-argv:\n    - []                                  # Pass optional keywords\n    - [\"--option\", \"value\"]\n    - []\n  worker:\n    preload: []\n    preload-argv: []\n  nanny:\n    preload: []\n    preload-argv: []\n  client:\n    preload: []\n    preload-argv: []\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Operations with Dask Actors in Python\nDESCRIPTION: Shows how to use asynchronous operations with Dask Actors to avoid blocking. All operations that communicate with remote workers are awaitable, allowing for efficient non-blocking code.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nasync def f():\n    future = client.submit(Counter, actor=True)\n    counter = await future  # gather actor object locally\n\n    counter.increment()  # send off a request asynchronously\n    await counter.increment()  # or wait until it was received\n\n    n = await counter.n  # attribute access also must be awaited\n```\n\nLANGUAGE: python\nCODE:\n```\nawait client.compute(ddf.to_parquet('/tmp/some.parquet', compute=False))\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask Logging in YAML\nDESCRIPTION: Shows how to configure logging verbosity for different Dask components using YAML configuration files, allowing for more detailed debugging information.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/debug.rst#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nlogging:\n  distributed: info\n  distributed.client: warning\n  bokeh: error\n```\n\n----------------------------------------\n\nTITLE: Accessing DataFrame Metadata in Dask\nDESCRIPTION: Demonstrates how metadata is stored and accessed in Dask DataFrames using the _meta and _meta_nonempty attributes. Shows dtype preservation and empty DataFrame structure.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-design.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> df = pd.DataFrame({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n>>> ddf = dd.from_pandas(df, npartitions=2)\n>>> ddf._meta\nEmpty DataFrame\nColumns: [a, b]\nIndex: []\n>>> ddf._meta.dtypes\na     int64\nb    object\ndtype: object\n```\n\n----------------------------------------\n\nTITLE: Batch Prediction with Dask DataFrame\nDESCRIPTION: Shows how to perform batch predictions using Dask DataFrame's map_partitions for distributed processing of tabular data.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/ml.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\n\ndf = dd.read_parquet(\"/path/to/my/data.parquet\")\n\nmodel = load_model(\"/path/to/my/model\")\n\n# pandas code\n# predictions = model.predict(df)\n# predictions.to_parquet(\"/path/to/results.parquet\")\n\n# Dask code\npredictions = df.map_partitions(model.predict)\npredictions.to_parquet(\"/path/to/results.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Custom Extent Aggregation\nDESCRIPTION: Example of creating a custom extent (max-min) aggregation using the previously defined functions and applying it to a grouped Dask DataFrame. The complete workflow from definition to computation is shown.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> extent = dd.Aggregation('extent', chunk, agg, finalize=finalize)\n>>> ddf.groupby('a').agg(extent).compute()\n   b\na\na  2\nb  4\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask Scheduler with Context Manager in Python\nDESCRIPTION: Demonstrates how to set the Dask scheduler using a context manager, allowing for temporary configuration changes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduling.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith dask.config.set(scheduler='threads'):\n    x.compute()\n```\n\n----------------------------------------\n\nTITLE: Array Block Operations\nDESCRIPTION: Shows usage of map_blocks with and without array arguments and block info access.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_54\n\nLANGUAGE: Python\nCODE:\n```\nda.map_blocks(func, block_info=True)\nda.map_blocks(func, None)  # no array arguments\n```\n\n----------------------------------------\n\nTITLE: Specifying Dask Configuration in YAML\nDESCRIPTION: Shows how to specify Dask configuration values in a YAML file. It includes settings for array chunk size and distributed worker memory management.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/configuration.rst#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\narray:\n  chunk-size: 128 MiB\n\ndistributed:\n  worker:\n    memory:\n      spill: 0.85  # default: 0.7\n      target: 0.75  # default: 0.6\n      terminate: 0.98  # default: 0.95\n\n  dashboard:\n    # Locate the dashboard if working on a Jupyter Hub server\n    link: /user/<user>/proxy/8787/status\n```\n\n----------------------------------------\n\nTITLE: Computing Sum of Sparse Dask Array in Python\nDESCRIPTION: This example shows how to perform a reduction operation (sum) on the sparse Dask array along one axis and compute the result. It demonstrates that operations work similarly on sparse arrays as they do on dense arrays.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-sparse.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> s.sum(axis=0)[:100].compute()\n<COO: shape=(100,), dtype=float64, nnz=100>\n\n>>> _.todense()\narray([ 4803.06859272,  4913.94964525,  4877.13266438,  4860.7470773 ,\n        4938.94446802,  4849.51326473,  4858.83977856,  4847.81468485,\n        ... ])\n```\n\n----------------------------------------\n\nTITLE: Loading Default Configuration in Downstream Libraries\nDESCRIPTION: Example code showing how to load and update default configuration from a YAML file in a downstream library's config.py\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/configuration.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# dask_foo/config.py\nimport os\nimport yaml\n\nimport dask.config\n\nfn = os.path.join(os.path.dirname(__file__), 'foo.yaml')\n\nwith open(fn) as f:\n    defaults = yaml.safe_load(f)\n\ndask.config.update_defaults(defaults)\n```\n\n----------------------------------------\n\nTITLE: Applying a Function to Overlapping Blocks in Dask Array\nDESCRIPTION: This snippet shows how to apply a function (in this case, a Gaussian filter) to overlapping blocks in a Dask array using the map_blocks method. It also demonstrates how to handle functions that don't preserve block shape.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-overlap.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from scipy.ndimage import gaussian_filter\n>>> def func(block):\n...    return gaussian_filter(block, sigma=1)\n\n>>> filt = g.map_blocks(func)\n\n>>> g.map_blocks(myfunc, chunks=(5, 5))\n```\n\n----------------------------------------\n\nTITLE: Task Modification Anti-pattern Example\nDESCRIPTION: Demonstrates an anti-pattern showing why in-place modifications of data should be avoided in Dask tasks, using a numpy array example.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/graphs.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nimport numpy as np\n\nclient = Client()\nx = client.submit(np.arange, 10)  # [0, 1, 2, 3, ...]\n\ndef f(arr):\n    arr[arr > 5] = 0  # modifies input directly without making a copy\n    arr += 1          # modifies input directly without making a copy\n    return arr\n\ny = client.submit(f, x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Compute Method for Custom Dask Collections\nDESCRIPTION: Pseudocode showing how the compute method works internally by merging graphs, optimizing, computing with the scheduler, and applying postcompute finalization.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/custom-collections.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef compute(*collections, **kwargs):\n    # 1. Graph Merging & Optimization\n    # -------------------------------\n    if kwargs.pop('optimize_graph', True):\n        # If optimization is turned on, group the collections by\n        # optimization method, and apply each method only once to the merged\n        # sub-graphs.\n        optimization_groups = groupby_optimization_methods(collections)\n        graphs = []\n        for optimize_method, cols in optimization_groups:\n            # Merge the graphs and keys for the subset of collections that\n            # share this optimization method\n            sub_graph = merge_graphs([x.__dask_graph__() for x in cols])\n            sub_keys = [x.__dask_keys__() for x in cols]\n            # kwargs are forwarded to ``__dask_optimize__`` from compute\n            optimized_graph = optimize_method(sub_graph, sub_keys, **kwargs)\n            graphs.append(optimized_graph)\n        graph = merge_graphs(graphs)\n    else:\n        graph = merge_graphs([x.__dask_graph__() for x in collections])\n    # Keys are always the same\n    keys = [x.__dask_keys__() for x in collections]\n\n    # 2. Computation\n    # --------------\n    # Determine appropriate get function based on collections, global\n    # settings, and keyword arguments\n    get = determine_get_function(collections, **kwargs)\n    # Pass the merged graph, keys, and kwargs to ``get``\n    results = get(graph, keys, **kwargs)\n\n    # 3. Postcompute\n    # --------------\n    output = []\n    # Iterate over the results and collections\n    for res, collection in zip(results, collections):\n        finalize, extra_args = collection.__dask_postcompute__()\n        out = finalize(res, **extra_args)\n        output.append(out)\n\n    # `dask.compute` always returns tuples\n    return tuple(output)\n```\n\n----------------------------------------\n\nTITLE: Using NumPy Generalized UFuncs with Dask Arrays\nDESCRIPTION: This snippet demonstrates how to use NumPy's generalized ufuncs with Dask arrays. It creates a 3D Dask array and applies NumPy's eigenvalue decomposition ufunc to it.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-gufunc.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\nimport numpy as np\n\nx = da.random.default_rng().normal(size=(3, 10, 10), chunks=(2, 10, 10))\n\nw, v = np.linalg._umath_linalg.eig(x, output_dtypes=(float, float))\n```\n\n----------------------------------------\n\nTITLE: Implementing Optuna Objective Function with Dask\nDESCRIPTION: Defines an objective function for hyperparameter optimization using Optuna. The function suggests parameter values and returns a model score.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/ml.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef objective(trial):\n    params = {\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10, step=1),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True),\n        ...\n    }\n    model = train_model(train_data, **params)\n    result = score(model, test_data)\n    return result\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Dask CLI Command in Python\nDESCRIPTION: Example showing how to create a custom Dask CLI command using Click decorators. The command accepts a name argument and count option to print greeting messages.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/cli.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# in the file mypackage/cli.py\nimport click\n\n@click.command(name=\"mycommand\")\n@click.argument(\"name\", type=str)\n@click.option(\"-c\", \"--count\", default=1)\ndef main(name, count):\n    for _ in range(count):\n        click.echo(f\"hello {name} from mycommand!\")\n```\n\n----------------------------------------\n\nTITLE: Using 2D Indexing with loc in Dask DataFrame\nDESCRIPTION: Adds support for 2D indexing using loc in Dask DataFrames.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_87\n\nLANGUAGE: Python\nCODE:\n```\ndf.loc[row_indexer, column_indexer]\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask Threaded Scheduler in Python\nDESCRIPTION: Sets the Dask scheduler to use threads. This is lightweight and requires no setup, but only provides parallelism for non-Python code due to the Global Interpreter Lock.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduling.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dask\ndask.config.set(scheduler='threads')  # overwrite default with threaded scheduler\n```\n\n----------------------------------------\n\nTITLE: NumPy-Dask Function Mappings\nDESCRIPTION: Reference table showing the mapping between NumPy array functions and their Dask array counterparts. Functions are marked as either direct ufunc implementations or custom Dask equivalents with additional notes about implementation details and limitations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-numpy-compatibility.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Direct ufunc mappings\nnumpy.invert -> dask.array.invert\nnumpy.isclose -> dask.array.isclose\nnumpy.isfinite -> dask.array.isfinite\nnumpy.isinf -> dask.array.isinf\nnumpy.isnan -> dask.array.isnan\n\n# Dask equivalents with special implementations\nnumpy.mean -> dask.array.mean  # Note: [#1] Special aggregation handling\nnumpy.median -> dask.array.median  # Note: [#16] Special implementation\nnumpy.meshgrid -> dask.array.meshgrid  # Note: [#17] Custom implementation\n```\n\n----------------------------------------\n\nTITLE: Accessing Scheduler Processing State in Dask LocalCluster\nDESCRIPTION: Shows how to inspect the current processing state of tasks on the scheduler. Returns a dictionary mapping worker addresses to sets of task keys currently being processed.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/debug.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> cluster.scheduler.processing\n{'worker-one:59858': {'inc-123', 'add-443'},\n 'worker-two:48248': {'inc-456'}}\n```\n\n----------------------------------------\n\nTITLE: Complete Overlapping Workflow in Dask Array\nDESCRIPTION: This snippet showcases a typical overlapping workflow in Dask Array, combining the overlap, map_blocks, and trim_internal functions. It demonstrates how to create overlapping blocks, apply a function, and trim the excess.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-overlap.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> x = ...\n>>> g = da.overlap.overlap(x, depth={0: 2, 1: 2},\n...                        boundary={0: 'periodic', 1: 'periodic'})\n>>> g2 = g.map_blocks(myfunc)\n>>> result = da.overlap.trim_internal(g2, {0: 2, 1: 2})\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Optimization Function in Python\nDESCRIPTION: Example of defining a custom optimization function that takes a task graph and keys as input and returns a modified task graph. This pattern can be used to create specialized optimization strategies.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/optimize.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef my_optimize_function(dsk, keys):\n    new_dsk = {...}\n    return new_dsk\n```\n\n----------------------------------------\n\nTITLE: Removing Failed Futures Manually in Dask Python\nDESCRIPTION: Illustrates how to manually remove failed futures when working with Dask dataframes, useful for handling partial computation failures in distributed environments.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/debug.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\ndf = ...\ndf = df.persist()\n\nfrom distributed.client import futures_of\nfutures = futures_of(df)\n\nwhile any(f.status == 'pending' for f in futures):\n    sleep(0.1)\n\ngood_futures = [f for f in futures if f.status == 'finished']\ndf = dd.from_delayed(good_futures, meta=df._meta)\n```\n\n----------------------------------------\n\nTITLE: Creating Dask Array from Image Files using dask.delayed in Python\nDESCRIPTION: This snippet shows how to create a Dask array from a stack of images using dask.delayed and skimage.io.imread. It demonstrates lazy evaluation and stacking of multiple small Dask arrays.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport skimage.io\nimport dask.array as da\nimport dask\n\nimread = dask.delayed(skimage.io.imread, pure=True)  # Lazy version of imread\n\nfilenames = sorted(glob.glob('*.jpg'))\n\nlazy_images = [imread(path) for path in filenames]   # Lazily evaluate imread on each path\nsample = lazy_images[0].compute()  # load the first image (assume rest are same shape/dtype)\n\narrays = [da.from_delayed(lazy_image,           # Construct a small Dask array\n                          dtype=sample.dtype,   # for every lazy value\n                          shape=sample.shape)\n          for lazy_image in lazy_images]\n\nstack = da.stack(arrays, axis=0)                # Stack all small Dask arrays into one\n```\n\n----------------------------------------\n\nTITLE: Graph Optimization Pipeline in Python\nDESCRIPTION: Combined optimization function that applies culling, inlining, and fusing to improve task graph performance.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/optimize.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef optimize_and_get(dsk, keys):\n    dsk1, deps = cull(dsk, keys)\n    dsk2 = inline(dsk1, dependencies=deps)\n    dsk3 = inline_functions(dsk2, keys, [len, str.split],\n                            dependencies=deps)\n    dsk4, deps = fuse(dsk3)\n    return get(dsk4, keys)\n```\n\n----------------------------------------\n\nTITLE: Measuring Dask Overhead (IPython)\nDESCRIPTION: IPython code snippet that measures the overhead introduced by Dask by comparing the computation time of a simple sum operation with different chunk sizes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/shared.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nIn [5]: x = da.ones(1000, chunks=(1000,)).sum()\n\nIn [6]: %timeit x.compute()\n1.18 ms  8.64 s per loop (mean  std. dev. of 7 runs, 1000 loops each)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Dask Cluster Logs in Python\nDESCRIPTION: This code demonstrates how to retrieve logs from Dask cluster components using the cluster manager's get_logs() method. It returns a dictionary containing logs from different parts of the cluster.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> cluster.get_logs()\n{'Cluster': '',\n'Scheduler': \"distributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO -   S...\n```\n\n----------------------------------------\n\nTITLE: Setting Distributed Client as Default for Network Shuffling\nDESCRIPTION: Code example showing how to set a distributed client as the default scheduler, which will automatically switch Dask to use network-based shuffling instead of disk-based shuffling.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = Client('scheduler:8786', set_as_default=True)\n```\n\n----------------------------------------\n\nTITLE: Reading Text Files into Dask Bags\nDESCRIPTION: Shows various methods to read text files into Dask bags, including support for multiple files, glob patterns, and compression.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/bag-creation.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> b = db.read_text('myfile.txt')\n>>> b = db.read_text(['myfile.1.txt', 'myfile.2.txt', ...])\n>>> b = db.read_text('myfile.*.txt')\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> b = db.read_text('myfile.*.txt.gz')\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> import json\n>>> b = db.read_text('myfile.*.json').map(json.loads)\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> b = db.read_text('myfile.*.csv').str.strip().str.split(',')\n```\n\n----------------------------------------\n\nTITLE: Ensuring Computation of Dask Delayed Tasks\nDESCRIPTION: Demonstrates the correct way to ensure that Dask delayed tasks are actually computed by calling dask.compute() on the results.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Ensure delayed tasks are computed\n\nx = dask.delayed(f)(1, 2, 3)\n...\ndask.compute(x, ...)\n```\n\n----------------------------------------\n\nTITLE: Deploying Dask Cluster with Dask Cloud Provider on DigitalOcean\nDESCRIPTION: Demonstrates deploying a Dask cluster on DigitalOcean using Dask Cloud Provider. The example configures the API token, creates a DropletCluster with one worker, and shows the VM creation process.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-cloud.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.config\n>>> dask.config.set({\"cloudprovider.digitalocean.token\": \"yourAPItoken\"})\n>>> from dask_cloudprovider.digitalocean import DropletCluster\n>>> cluster = DropletCluster(n_workers=1)\nCreating scheduler instance\nCreated droplet dask-38b817c1-scheduler\nWaiting for scheduler to run\nScheduler is running\nCreating worker instance\nCreated droplet dask-38b817c1-worker-dc95260d\n```\n\n----------------------------------------\n\nTITLE: Implementing Deterministic Hashing for Custom Classes in Python\nDESCRIPTION: Examples of implementing deterministic hashing for custom classes using __dask_tokenize__ method and normalize_token registration.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/custom-collections.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from dask.base import tokenize, normalize_token\n\n# Define a tokenize implementation using a method.\n>>> class Point:\n...     def __init__(self, x, y):\n...         self.x = x\n...         self.y = y\n...\n...     def __dask_tokenize__(self):\n...         # This tuple fully represents self\n...         # Wrap non-trivial objects with normalize_token before returning them\n...         return normalize_token(Point), self.x, self.y\n\n>>> x = Point(1, 2)\n>>> tokenize(x)\n'5988362b6e07087db2bc8e7c1c8cc560'\n>>> tokenize(x) == tokenize(x)  # token is idempotent\nTrue\n>>> tokenize(Point(1, 2)) == tokenize(Point(1, 2))  # token is deterministic\nTrue\n>>> tokenize(Point(1, 2)) == tokenize(Point(2, 1))  # tokens are unique\nFalse\n\n\n# Register an implementation with normalize_token\n>>> class Point3D:\n...     def __init__(self, x, y, z):\n...         self.x = x\n...         self.y = y\n...         self.z = z\n\n>>> @normalize_token.register(Point3D)\n... def normalize_point3d(x):\n...     return normalize_token(Point3D), x.x, x.y, x.z\n\n>>> y = Point3D(1, 2, 3)\n>>> tokenize(y)\n'5a7e9c3645aa44cf13d021c14452152e'\n```\n\n----------------------------------------\n\nTITLE: Configuring Global Shuffling Method\nDESCRIPTION: Example showing how to configure the global shuffling method using the configuration system. This can be done globally or as a context manager for specific operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndask.config.set({\"dataframe.shuffle.method\": \"p2p\"})\n\nddf.groupby(...).apply(...)\n```\n\n----------------------------------------\n\nTITLE: Batch Prediction with Dask Futures\nDESCRIPTION: Implements parallel batch prediction using Dask Futures to process multiple data files concurrently.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/ml.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import LocalCluster\n\ncluster = LocalCluster(processes=False)  # replace this with some scalable cluster\nclient = cluster.get_client()\n\nfilenames = [...]\n\ndef predict(filename, model):\n    data = load(filename)\n    result = model.predict(data)\n    return result\n\nmodel = client.submit(load_model, path_to_model)\npredictions = client.map(predict, filenames, model=model)\nresults = client.gather(predictions)\n```\n\n----------------------------------------\n\nTITLE: Boolean Masking in Dask Array\nDESCRIPTION: Illustrates boolean masking operations with Dask arrays, showing limitations with single broadcastable boolean arrays and correct usage in tuple indexing.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-assignment.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> x = da.arange(12).reshape(2, 6)\n>>> x[x > 7] = np.ma.array(-99, mask=True)\n>>> print(x.compute())\n[[  0   1   2   3   4   5]\n [  6   7 -99 -99 -99 -99]]\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> x = da.arange(12).reshape(2, 6)\n>>> x[1, x[0] > 3] = np.ma.masked\n>>> print(x.compute())\n[[0 1 2 3 4 5]\n [6 7 8 9 -- --]]\n>>> x = da.arange(12).reshape(2, 6)\n>>> print(x.compute())\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]]\n>>> x[(x[:, 2] < 4,)] = np.ma.masked\n>>> print(x.compute())\n[[-- -- -- -- -- --]\n [6 7 8 9 10 11]]\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Metadata in Dask DataFrame Operations\nDESCRIPTION: Example of specifying custom metadata when using map_partitions to ensure correct dtype and column information propagation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-design.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf.map_partitions(foo, meta=pd.DataFrame({'a': [1], 'b': [2]}))\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Fuse Optimization in Python\nDESCRIPTION: Example showing the basic structure of a fuse optimization function that accepts keys parameter, demonstrating how to pass keys through compute call using the fuse_keys keyword argument.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/optimize.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef fuse(dsk, keys=None):\n    ...\n\nx.compute(fuse_keys=['x', 'y', 'z'])\n```\n\n----------------------------------------\n\nTITLE: Simultaneous Min-Max Computation in Dask\nDESCRIPTION: Example showing how to compute multiple aggregations simultaneously to share intermediate results efficiently.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/caching.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> dd.compute(df.amount.max(), df.amount.min())\n(1000, -1000)\n```\n\n----------------------------------------\n\nTITLE: Trimming Excess from Overlapping Blocks in Dask Array\nDESCRIPTION: This snippet demonstrates how to trim the excess borders from overlapping blocks in a Dask array using the trim_internal function. It shows how to specify the depth of trimming for each dimension.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-overlap.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> x.chunks\n((10, 10, 10, 10), (10, 10, 10, 10))\n\n>>> y = da.overlap.trim_internal(x, {0: 2, 1: 1})\n>>> y.chunks\n((6, 6, 6, 6), (8, 8, 8, 8))\n```\n\n----------------------------------------\n\nTITLE: Converting High-Level Dask Objects to Delayed Objects\nDESCRIPTION: Demonstrates how to convert high-level Dask interfaces (array, bag, dataframe) to Delayed objects using the to_delayed() method.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/user-interfaces.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndelayeds = df.to_delayed()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Unknown Chunks in Dask Array Operations\nDESCRIPTION: This snippet shows how unknown chunk sizes can arise from operations on Dask arrays, and how to handle them using compute_chunk_sizes().\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-chunks.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> rng = np.random.default_rng()\n>>> x = da.from_array(rng.standard_normal(100), chunks=20)\n>>> x += 0.1\n>>> y = x[x > 0]  # don't know how many values are greater than 0 ahead of time\n\n>>> y.shape\n(np.nan,)\n>>> y[4]\n...\nValueError: Array chunk sizes unknown\n\nA possible solution: https://docs.dask.org/en/latest/array-chunks.html#unknown-chunks.\nSummary: to compute chunks sizes, use\n\n    x.compute_chunk_sizes()  # for Dask Array\n    ddf.to_dask_array(lengths=True)  # for Dask DataFrame ddf\n\n>>> y.compute_chunk_sizes()\ndask.array<..., chunksize=(19,), ...>\n>>> y.shape\n(44,)\n>>> y[4].compute()\n0.78621774046566\n```\n\n----------------------------------------\n\nTITLE: Implementing make_meta_obj Dispatch in Python\nDESCRIPTION: Example of registering make_meta_obj dispatch method to handle arbitrary object types and convert them to appropriate backend types.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-extend.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.dataframe.dispatch import make_meta_obj\n\n@make_meta_obj.register(MyDataFrame)\ndef make_meta_object(x, index=None):\n    if isinstance(x, dict):\n        return MyDataFrame()\n    elif isinstance(x, int):\n        return MySeries\n    .\n    .\n    .\n```\n\n----------------------------------------\n\nTITLE: Manual Batching of Delayed Tasks in Python\nDESCRIPTION: Shows how to manually batch tasks when applying a function to a large sequence, reducing the number of delayed function calls.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef batch(seq):\n    sub_results = []\n    for x in seq:\n        sub_results.append(f(x))\n    return sub_results\n\nbatches = []\nfor i in range(0, 10000000, 10000):\n    result_batch = dask.delayed(batch)(range(i, i + 10000))\n    batches.append(result_batch)\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost with Dask\nDESCRIPTION: Demonstrates distributed training of XGBoost models using Dask DataFrame and LocalCluster.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/ml.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\nimport xgboost as xgb\nfrom dask.distributed import LocalCluster\n\ndf = dask.datasets.timeseries()  # Randomly generated data\n# df = dd.read_parquet(...)      # In practice, you would probably read data though\n\ntrain, test = df.random_split([0.80, 0.20])\nX_train, y_train, X_test, y_test = ...\n\nwith LocalCluster() as cluster:\n    with cluster.get_client() as client:\n        d_train = xgb.dask.DaskDMatrix(client, X_train, y_train, enable_categorical=True)\n        model = xgb.dask.train(\n            ...\n            d_train,\n        )\n        predictions = xgb.dask.predict(client, model, X_test)\n```\n\n----------------------------------------\n\nTITLE: Installing Single Dask Cluster with Helm\nDESCRIPTION: This command installs a single Dask cluster using Helm. It uses the Dask Helm chart repository and names the release 'my-dask'.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-kubernetes.rst#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --repo https://helm.dask.org my-dask dask\n```\n\n----------------------------------------\n\nTITLE: Converting Futures to Delayed Objects\nDESCRIPTION: Demonstrates how to convert Future objects back to Delayed objects using dask.delayed.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/user-interfaces.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndelayed_value = dask.delayed(future)\n```\n\n----------------------------------------\n\nTITLE: Reshaping Dask Array without Chunk Merging\nDESCRIPTION: Example showing reshaping a Dask array with merge_chunks=False to avoid merging chunks by splitting the input instead.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-chunks.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> a.reshape(6, 4, merge_chunks=False).visualize()\n```\n\n----------------------------------------\n\nTITLE: Converting Unknown to Known Categoricals in Dask\nDESCRIPTION: Demonstrates methods to convert unknown categoricals to known categoricals using either .cat.as_known() for single columns or .categorize() for multiple columns.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-categoricals.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> col_known = ddf.col.cat.as_known()  # use for single column\n>>> col_known.cat.known\nTrue\n>>> ddf_known = ddf.categorize()        # use for multiple columns\n>>> ddf_known.col.cat.known\nTrue\n```\n\n----------------------------------------\n\nTITLE: Installing Dask with conda\nDESCRIPTION: Commands for installing Dask using conda package manager. Includes options for installing all common dependencies or a minimal installation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/install.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda install dask\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda install dask -c conda-forge\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda install dask-core\n```\n\n----------------------------------------\n\nTITLE: Dask Bag Filter Operations\nDESCRIPTION: Shows how to filter and transform collections using Dask Bags with lambda functions and distinct operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> b.filter(lambda x: x % 2)\ndask.bag<filter-lambda, npartitions=2>\n\n>>> b.filter(lambda x: x % 2).compute()\n[1, 3, 5, 1]\n\n>>> b.distinct()\ndask.bag<distinct-aggregate, npartitions=1>\n\n>>> b.distinct().compute()\n[1, 2, 3, 4, 5, 6]\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Data as Unknown Categoricals\nDESCRIPTION: Shows how to read CSV data directly into unknown categorical columns using dd.read_csv with dtype specification.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-categoricals.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf = dd.read_csv(..., dtype={col_name: 'category'})\n```\n\n----------------------------------------\n\nTITLE: Creating Dask Array from Dask DataFrame with Known Chunks\nDESCRIPTION: This snippet demonstrates how to create a Dask array from a Dask DataFrame while ensuring known chunk sizes using the to_dask_array() method.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-chunks.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf = dask.dataframe.from_pandas(...)\n>>> ddf.to_dask_array()\ndask.array<..., shape=(nan, 2), ..., chunksize=(nan, 2)>\n\n>>> ddf.to_dask_array(lengths=True)\ndask.array<..., shape=(100, 2), ..., chunksize=(20, 2)>\n```\n\n----------------------------------------\n\nTITLE: DataFrame Merge and Correlation Operations\nDESCRIPTION: Demonstrates DataFrame merge and correlation functionality including merge_asof and group operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_53\n\nLANGUAGE: Python\nCODE:\n```\ndf1.merge(df2, on='key', how='left')\ndf.groupby('column').corr()\ndf.groupby('column').cov()\n```\n\n----------------------------------------\n\nTITLE: Using Context Managers with Dask Components\nDESCRIPTION: Example showing how to use async context managers for cleaner handling of Dask Scheduler and Worker lifecycle.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python-advanced.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dask.distributed import Scheduler, Worker\n\nasync def f():\n    async with Scheduler() as s:\n        async with Worker(s.address) as w:\n            await w.finished()\n            await s.finished()\n\nasyncio.get_event_loop().run_until_complete(f())\n```\n\n----------------------------------------\n\nTITLE: Transposing a Dask Array\nDESCRIPTION: Shows how to transpose a Dask Array with the T property and compute the result, demonstrating linear algebra operations on distributed arrays.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> a.T\ndask.array<transpose, shape=(500, 200), dtype=int64, chunksize=(100, 100), chunktype=numpy.ndarray>\n\n>>> a.T.compute()\narray([[    0,   500,  1000, ..., 98500, 99000, 99500],\n       [    1,   501,  1001, ..., 98501, 99001, 99501],\n```\n\n----------------------------------------\n\nTITLE: Positional Indexing with .iloc in Dask DataFrame using Python\nDESCRIPTION: Shows the limitations of positional indexing with .iloc in Dask DataFrames. It only supports indexers where the row indexer is slice(None), and attempting to select specific rows will raise an exception.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-indexing.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf.iloc[:, [1, 0]]\nDask DataFrame Structure:\n                     B      A\nnpartitions=1\na              int64  int64\nc                ...    ...\nDask Name: iloc, 2 tasks\n\n>>> ddf.iloc[[0, 2], [1]]\nTraceback (most recent call last)\n  File \"<stdin>\", line 1, in <module>\nValueError: 'DataFrame.iloc' does not support slicing rows. The indexer must be a 2-tuple whose first item is 'slice(None)'.\n```\n\n----------------------------------------\n\nTITLE: Implementing optimize Function for Dask Collections in Python\nDESCRIPTION: Pseudocode implementation of the optimize function for Dask collections. It merges and optimizes the graph, then rebuilds each collection using the optimized graph.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/custom-collections.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef optimize(*collections, **kwargs):\n    # 1. Graph Merging & Optimization\n    # -------------------------------\n    # **Same as in compute**\n    graph = ...\n\n    # 2. Rebuilding\n    # -------------\n    # Rebuild each dask collection using the same large optimized graph\n    output = []\n    for collection in collections:\n        rebuild, extra_args = collection.__dask_postpersist__()\n        out = rebuild(graph, *extra_args)\n        output.append(out)\n\n    # dask.optimize always returns tuples\n    return tuple(output)\n```\n\n----------------------------------------\n\nTITLE: Passing Keyword Arguments in Custom Dask Graphs\nDESCRIPTION: This snippet shows how to pass keyword arguments to functions in a custom Dask graph using the dask.utils.apply function. It demonstrates the structure for creating a task with both positional and keyword arguments.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/custom-graphs.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.utils import apply\n\ntask = (apply, func, args, kwargs)  # equivalent to func(*args, **kwargs)\n\ndsk = {'task-name': task,\n        ...\n       }\n```\n\n----------------------------------------\n\nTITLE: Installing Dask with pip\nDESCRIPTION: Commands for installing Dask using pip. Includes options for full installation, core installation, and specific functionality subsets.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/install.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install \"dask[complete]\"    # Install everything\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install dask                # Install only core parts of dask\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install \"dask[array]\"       # Install requirements for dask array\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install \"dask[dataframe]\"   # Install requirements for dask dataframe\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install \"dask[diagnostics]\" # Install requirements for dask diagnostics\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install \"dask[distributed]\" # Install requirements for distributed dask\n```\n\n----------------------------------------\n\nTITLE: Submitting Tasks from Tasks in Python\nDESCRIPTION: Shows how to submit new tasks from within a running task by obtaining a client instance and performing client operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import get_client\n\ndef my_function(x):\n    ...\n\n    # Get locally created client\n    client = get_client()\n\n    # Do normal client operations, asking cluster for computation\n    a = client.submit(...)\n    b = client.submit(...)\n    a, b = client.gather([a, b])\n\n    return a + b\n```\n\n----------------------------------------\n\nTITLE: Example YAML Configuration for Downstream Libraries\nDESCRIPTION: Shows the recommended YAML configuration structure for downstream Dask libraries, using a fictional 'dask-foo' project as an example\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/configuration.rst#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n# dask_foo/foo.yaml\n\nfoo:\n  color: red\n  admin:\n    a: 1\n    b: 2\n```\n\n----------------------------------------\n\nTITLE: Dask Array Auto Rechunking\nDESCRIPTION: Examples of using automatic chunking with dask arrays, including checking chunk size configuration and creating new arrays.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-chunks.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nx = x.rechunk('auto')\n>>> dask.config.get('array.chunk-size')\n'128MiB'\n>>> dask.array.ones((10000, 10000), chunks=(-1, 'auto'))\ndask.array<wrapped, shape=(10000, 10000), dtype=float64, chunksize=(10000, 1250), chunktype=numpy.ndarray>\n```\n\n----------------------------------------\n\nTITLE: Installing Dask Kubernetes Operator with Helm\nDESCRIPTION: This command installs the Dask Kubernetes Operator using Helm. It creates a new namespace called 'dask-operator' and generates a name for the release.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-kubernetes.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --repo https://helm.dask.org --create-namespace -n dask-operator --generate-name dask-kubernetes-operator\n```\n\n----------------------------------------\n\nTITLE: Importing Client from Dask Distributed\nDESCRIPTION: Examples of initializing Dask clients for local and distributed environments. Shows different methods of client initialization including basic Client() and LocalCluster().\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/software-environments.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nClient(...)\nLocalCluster(...)\n```\n\n----------------------------------------\n\nTITLE: Computing with Single-Threaded Scheduler in Dask Python\nDESCRIPTION: Shows how to use the single-threaded scheduler in Dask for debugging purposes, allowing the use of normal Python debugging tools like pdb and profiling tools.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/debug.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nx.compute(scheduler='single-threaded')\n```\n\n----------------------------------------\n\nTITLE: Registering Custom FileSystem Protocol in Python\nDESCRIPTION: Example demonstrating how to register a custom file system protocol implementation with fsspec. Shows registration of 'myproto' protocol with a custom FileSystem class.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/connect-to-remote-data.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfsspec.registry['myproto'] = MyProtoFileSystem\n```\n\n----------------------------------------\n\nTITLE: Implementing Persist Method for Custom Dask Collections\nDESCRIPTION: Pseudocode showing how the persist method works internally by merging graphs, computing with the scheduler, and rebuilding collections with computed results.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/custom-collections.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef persist(*collections, **kwargs):\n    # 1. Graph Merging & Optimization\n    # -------------------------------\n    # **Same as in compute**\n    graph = ...\n    keys = ...\n\n    # 2. Computation\n    # --------------\n    # **Same as in compute**\n    results = ...\n\n    # 3. Postpersist\n    # --------------\n    output = []\n    # Iterate over the results and collections\n    for res, collection in zip(results, collections):\n        # res has the same structure as keys\n        keys = collection.__dask_keys__()\n        # Get the computed graph for this collection.\n        # Here flatten converts a nested list into a single list\n        subgraph = {k: r for (k, r) in zip(flatten(keys), flatten(res))}\n\n        # Rebuild the output dask collection with the computed graph\n        rebuild, extra_args = collection.__dask_postpersist__()\n        out = rebuild(subgraph, *extra_args)\n\n        output.append(out)\n\n    # dask.persist always returns tuples\n    return tuple(output)\n```\n\n----------------------------------------\n\nTITLE: Creating Identity Matrix with Dask Array\nDESCRIPTION: Implementation of a custom eye function that creates an identity matrix using Dask arrays. Shows how to construct Dask arrays from scratch with proper graph structure and chunking.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-design.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef eye(n, blocksize):\n    chunks = ((blocksize,) * (n // blocksize),\n              (blocksize,) * (n // blocksize))\n\n    name = 'eye' + next(tokens)  # unique identifier\n\n    layer = {(name, i, j): (np.eye, blocksize)\n                           if i == j else\n                           (np.zeros, (blocksize, blocksize))\n             for i in range(n // blocksize)\n             for j in range(n // blocksize)}\n    dsk = dask.highlevelgraph.HighLevelGraph.from_collections(name, layer, dependencies=())\n\n    dtype = np.eye(0).dtype  # take dtype default from numpy\n\n    return dask.array.Array(dsk, name, chunks, dtype)\n```\n\n----------------------------------------\n\nTITLE: Managing Helm Dask Cluster with HelmCluster\nDESCRIPTION: This Python code demonstrates how to manage a Helm-deployed Dask cluster using the HelmCluster class from dask_kubernetes. It creates a cluster object and scales it to 10 workers.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-kubernetes.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_kubernetes import HelmCluster\n\ncluster = HelmCluster(release_name=\"myrelease\")\ncluster.scale(10)\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Storage Options for Parquet\nDESCRIPTION: Example of passing storage-specific configuration options when reading Parquet data from remote storage. This demonstrates how to provide authentication credentials or other fsspec options.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-parquet.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> df = dd.read_parquet(\n...      \"s3://bucket-name/my/parquet/\",\n...      storage_options={\"anon\": True}  # passed to `s3fs.S3FileSystem`\n... )\n```\n\n----------------------------------------\n\nTITLE: Launching Dask Scheduler from Command Line\nDESCRIPTION: Command to start a Dask scheduler process. When executed, the scheduler displays the TCP address where it can be reached by workers.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-cli.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ dask scheduler\nScheduler at:   tcp://192.0.0.100:8786\n```\n\n----------------------------------------\n\nTITLE: Reading Avro Files into Dask Bags\nDESCRIPTION: Explains how to read Avro format files into Dask bags with optional chunking and compression support.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/bag-creation.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> b = db.read_avro('datafile.avro')\n>>> b = db.read_avro('data.*.avro')\n```\n\nLANGUAGE: python\nCODE:\n```\n> b = bd.read_avro('compressed.*.avro.gz', blocksize=None, compression='gzip')\n```\n\n----------------------------------------\n\nTITLE: Multidimensional Indexing with Xarray and Dask in Python\nDESCRIPTION: Creates a Dask-backed Xarray DataArray and demonstrates multidimensional indexing, which now maintains consistent chunk sizes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\narr = xr.DataArray(\n    da.random.random((100, 100, 100), chunks=(5, 5, 50)),\n    dims=['a', \"b\", \"c\"],\n)\n```\n\n----------------------------------------\n\nTITLE: HPC Deployment with Dask-Jobqueue\nDESCRIPTION: Shows how to deploy Dask clusters on HPC systems using job schedulers like PBS.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/index.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_jobqueue import PBSCluster\ncluster = PBSCluster(\n   cores=24,\n   memory=\"100GB\",\n   queue=\"regular\",\n   account=\"my-account\",\n)\ncluster.scale(jobs=100)\nclient = cluster.get_client()\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask Scheduler with Custom Thread Pool in Python\nDESCRIPTION: Demonstrates how to use a custom thread pool with Dask, allowing for fine-grained control over the number of workers.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduling.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom concurrent.futures import ThreadPoolExecutor\nwith dask.config.set(pool=ThreadPoolExecutor(4)):\n    x.compute()\n\nwith dask.config.set(num_workers=4):\n    x.compute()\n```\n\n----------------------------------------\n\nTITLE: Slicing a Dask Array\nDESCRIPTION: Shows how to slice a Dask Array to select specific rows and columns, similar to NumPy array indexing.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\na[:50, 200]\n```\n\n----------------------------------------\n\nTITLE: Defining Valid Dask Keys in Python\nDESCRIPTION: Examples of valid key formats in Dask graphs, showing both simple string keys and tuple-based compound keys.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/spec.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n'x'\n('x', 2, 3)\n```\n\n----------------------------------------\n\nTITLE: Progress Bar Implementation in Dask\nDESCRIPTION: Shows the difference between local and distributed progress bar implementations in Dask, including how to initialize and use progress tracking for distributed computations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/diagnostics-distributed.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Progress bar on a single-machine scheduler\nfrom dask.diagnostics import ProgressBar\n\nwith ProgressBar():\n    x.compute()\n\n# Progress bar with the distributed scheduler\nfrom dask.distributed import Client, progress\n\nclient = Client()  # use dask.distributed by default\n\nx = x.persist()  # start computation in the background\nprogress(x)      # watch progress\n\nx.compute()      # convert to final result when done if desired\n```\n\n----------------------------------------\n\nTITLE: Capturing Task Stream Data in Python with Dask\nDESCRIPTION: Shows how to capture task stream data and profile information using get_task_stream and Client.profile functions. The data can be saved to HTML files for offline analysis.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/diagnostics-distributed.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith get_task_stream(plot='save', filename=\"task-stream.html\") as ts:\n    x.compute()\n\nclient.profile(filename=\"dask-profile.html\")\n\nhistory = ts.data\n```\n\n----------------------------------------\n\nTITLE: Handling Exceptions in Dask Futures in Python\nDESCRIPTION: Shows how Dask raises remote exceptions and tracebacks when trying to get the result of a failed future. Dependent futures also propagate the error.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef div(x, y):\n    return x / y\n\na = client.submit(div, 1, 0)  # 1 / 0 raises a ZeroDivisionError\na.result()\n\nb = client.submit(inc, a)\n```\n\n----------------------------------------\n\nTITLE: Fire and Forget Operations with Dask Futures in Python\nDESCRIPTION: Explains how to use fire_and_forget to compute tasks without keeping active futures, useful for tasks with side effects or when futures may go out of scope.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import fire_and_forget\n\na = client.submit(load, filename)\nb = client.submit(process, a)\nc = client.submit(write, b, out_filename)\nfire_and_forget(c)\n\ndef process(filename):\n    out_filename = 'out-' + filename\n    a = client.submit(load, filename)\n    b = client.submit(process, a)\n    c = client.submit(write, b, out_filename)\n    fire_and_forget(c)\n    return  # here we lose the reference to c, but that's now ok\n```\n\n----------------------------------------\n\nTITLE: Slicing Chunked Arrays with Integer Indices\nDESCRIPTION: Example demonstrating how chunking affects slicing with integer indices, showing output chunk sizes for different slicing patterns.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-slicing.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Array of ones, chunked along axis 0\n>>> a = da.ones((4, 10000, 10000), chunks=(1, -1, -1))\n\n>>> a[[0, 1], :, :]          #doctest: +SKIP\ndask.array<getitem, shape=(2, 10000, 10000), dtype=float64, chunksize=(1, 10000, 10000), chunktype=numpy.ndarray>\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet Data with Year Filter in Python\nDESCRIPTION: This snippet demonstrates how to read a hive-partitioned Parquet dataset using Dask, applying a filter on the 'year' column. This operation is typically faster when the dataset is already hive-partitioned on the 'year' column.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-hive.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> dd.read_parquet(\"output-path\", filters=[(\"year\", \">\", 2022)])\n```\n\n----------------------------------------\n\nTITLE: Implementing concat Dispatch in Python\nDESCRIPTION: Registration of concat dispatch method to concatenate custom non-Dask DataFrame objects together.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-extend.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.dataframe.methods import concat_dispatch\n\n@concat_dispatch.register((MyDataFrame, MySeries, MyIndex))\ndef concat_pandas(dfs, axis=0, join='outer', uniform=False, filter_warning=True):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Optimization for Dask Collections\nDESCRIPTION: Shows how to register a custom optimization function using dask.config.set for use with dask compute operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/optimize.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith dask.config.set(array_optimize=my_optimize_function):\n    x, y = dask.compute(x, y)\n```\n\n----------------------------------------\n\nTITLE: Customizing Parquet File Names\nDESCRIPTION: Example of using the name_function parameter to customize how Parquet files are named. This function takes a partition index and returns a string filename.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-parquet.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> df.npartitions  # 3 partitions (0, 1, and 2)\n3\n\n>>> df.to_parquet(\"/path/to/output\", name_function=lambda i: f\"data-{i}.parquet\")\n\n>>> os.listdir(\"/path/to/parquet\")\n[\"data-0.parquet\", \"data-1.parquet\", \"data-2.parquet\"]\n```\n\n----------------------------------------\n\nTITLE: Defining Task Structure in Python\nDESCRIPTION: Example of a Dask task structure using a tuple with a callable first element followed by arguments.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/spec.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n(add, 'x', 'y')\n```\n\n----------------------------------------\n\nTITLE: Basic Python Task Graph Example\nDESCRIPTION: Demonstrates a simple program with function dependencies translated into Dask's graph representation format using basic Python operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/graphs.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef inc(i):\n    return i + 1\n\ndef add(a, b):\n    return a + b\n\nx = 1\ny = inc(x)\nz = add(y, 10)\n```\n\n----------------------------------------\n\nTITLE: Ignoring Metadata File when Reading Parquet\nDESCRIPTION: Example showing how to ignore the _metadata file when reading Parquet datasets, which can be useful for large datasets where the metadata file might be too large to process.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-parquet.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> df = dd.read_parquet(\n...      \"s3://bucket-name/my/parquet/\",\n...      ignore_metadata_file=True  # don't read the _metadata file\n... )\n```\n\n----------------------------------------\n\nTITLE: Refreshing Dask Configuration\nDESCRIPTION: Shows how to refresh Dask configuration after making changes to YAML files or environment variables using the config.refresh() method\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/configuration.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> dask.config.config\n{}\n\n>>> # make some changes to yaml files\n\n>>> dask.config.refresh()\n>>> dask.config.config\n{...}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Custom Aggregation\nDESCRIPTION: Example of implementing a custom sum aggregation using Dask's Aggregation class. The custom aggregation defines chunk and aggregate functions to process data in a distributed manner.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncustom_sum = dd.Aggregation('custom_sum', lambda s: s.sum(), lambda s0: s0.sum())\nddf.groupby('g').agg(custom_sum)\n```\n\n----------------------------------------\n\nTITLE: Enabling Query Planning for Dask DataFrame\nDESCRIPTION: Python code to enable query planning for Dask DataFrame by setting the appropriate configuration option. This is needed to use the new DataFrame implementation from dask-expr.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask\n>>> dask.config.set({'dataframe.query-planning': True})\n>>> import dask.dataframe as dd\n```\n\n----------------------------------------\n\nTITLE: Processing NetCDF Data with Xarray and Dask\nDESCRIPTION: Shows how to use Xarray with Dask for processing labeled multi-dimensional arrays from NetCDF files.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/index.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport xarray as xr\n\nds = xr.open_mfdataset(\"data/*.nc\")\nda.groupby('time.month').mean('time').compute()\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Storage Options for Writing Parquet\nDESCRIPTION: Example of providing storage options when writing Parquet files to remote storage. This allows passing authentication or configuration parameters to the underlying filesystem.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-parquet.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> df.to_parquet(\n...     \"s3://bucket-name/my/parquet/\",\n...     storage_options={\"anon\": True}  # passed to `s3fs.S3FileSystem`\n... )\n```\n\n----------------------------------------\n\nTITLE: Dask Dictionary Graph Representation\nDESCRIPTION: Shows how the previous Python code is encoded as a Dask task graph using a dictionary structure where keys represent variables and values represent operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/graphs.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nd = {'x': 1,\n     'y': (inc, 'x'),\n     'z': (add, 'y', 10)}\n```\n\n----------------------------------------\n\nTITLE: Creating a Dask Cluster with kubectl\nDESCRIPTION: This snippet demonstrates how to create a Dask cluster using kubectl by applying a YAML configuration. It defines a DaskCluster resource with the Kubernetes API.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-kubernetes.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Create a cluster with kubectl\nkubectl apply -f - <<EOF\napiVersion: kubernetes.dask.org/v1\nkind: DaskCluster\nmetadata:\n  name: my-dask-cluster\nspec:\n  ...\nEOF\n```\n\n----------------------------------------\n\nTITLE: Setting Dask Configuration via Environment Variables\nDESCRIPTION: Demonstrates how to set Dask configuration values using environment variables. It includes examples for work stealing, allowed failures, and dashboard link.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/configuration.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport DASK_DISTRIBUTED__SCHEDULER__WORK_STEALING=True\nexport DASK_DISTRIBUTED__SCHEDULER__ALLOWED_FAILURES=5\nexport DASK_DISTRIBUTED__DASHBOARD__LINK=\"/user/<user>/proxy/8787/status\"\n```\n\n----------------------------------------\n\nTITLE: Partial-string Indexing in Dask DataFrame with Timeseries Data using Python\nDESCRIPTION: Demonstrates partial-string indexing for timeseries data in Dask DataFrames. This allows selecting data for a specific date or date range using string representations of dates.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-indexing.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> ts = dd.demo.make_timeseries()\n>>> ts\nDask DataFrame Structure:\n                     id    name        x        y\nnpartitions=11\n2000-01-31      int64  object  float64  float64\n2000-02-29        ...     ...      ...      ...\n...               ...     ...      ...      ...\n2000-11-30        ...     ...      ...      ...\n2000-12-31        ...     ...      ...      ...\nDask Name: make-timeseries, 11 tasks\n\n>>> ts.loc['2000-02-12']\nDask DataFrame Structure:\n                                    id    name        x        y\nnpartitions=1\n2000-02-12 00:00:00.000000000  int64  object  float64  float64\n2000-02-12 23:59:59.999999999    ...     ...      ...      ...\nDask Name: loc, 12 tasks\n```\n\n----------------------------------------\n\nTITLE: Registering sizeof implementation for NumPy arrays in Python\nDESCRIPTION: This snippet demonstrates how to register a custom sizeof implementation for NumPy arrays using dask.sizeof.register decorator. It calculates the size of a NumPy array based on its number of bytes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/extend-sizeof.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from dask.sizeof import sizeof\n>>> @sizeof.register(np.ndarray)\n>>> def sizeof_numpy_like(array):\n...     return array.nbytes\n```\n\n----------------------------------------\n\nTITLE: Wrapping a Python Function as a Generalized UFunc in Dask\nDESCRIPTION: This example shows how to use the 'gufunc' function to wrap a custom Python function as a generalized ufunc. It creates a function that computes the mean along the last axis and applies it to a Dask array.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-gufunc.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nx = da.random.default_rng().normal(size=(10, 5), chunks=(2, 5))\n\ndef foo(x):\n    return np.mean(x, axis=-1)\n\ngufoo = da.gufunc(foo, signature=\"(i)->()\", output_dtypes=float, vectorize=True)\n\ny = gufoo(x)\n```\n\n----------------------------------------\n\nTITLE: Accessing Date Range in Partitioned DataFrame\nDESCRIPTION: Demonstrates how to access a specific date range in a partitioned DataFrame using loc indexing.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-design.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> df.loc['2015-01-20': '2015-02-10']  # Must inspect first two partitions\n```\n\n----------------------------------------\n\nTITLE: Shuffling DataFrame Before Writing Parquet with Hive Partitioning in Python\nDESCRIPTION: This snippet demonstrates how to shuffle a Dask DataFrame on the partitioning columns before writing to Parquet format. This ensures a single Parquet file per hive partition, but is computationally expensive.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-hive.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> partition_on = [\"year\", \"semester\"]\n\n>>> df.shuffle(on=partition_on).to_parquet(partition_on=partition_on)\n```\n\n----------------------------------------\n\nTITLE: Applying Delayed Function with Variable Outputs in Python\nDESCRIPTION: Demonstrates using delayed with a variable number of outputs in Dask.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_83\n\nLANGUAGE: Python\nCODE:\n```\ndelayed(lambda *args: args, nout=len(vals))(*vals)\n```\n\n----------------------------------------\n\nTITLE: Collecting Configuration from Custom Paths\nDESCRIPTION: Demonstrates how to collect configuration from specific paths without modifying the global configuration\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/configuration.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> dask.config.collect(paths=[...])\n{...}\n```\n\n----------------------------------------\n\nTITLE: Indexing Dask DataFrame for Column and Range Selection\nDESCRIPTION: Demonstrates how to select columns and filter by date ranges in a Dask DataFrame, using syntax similar to pandas.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf.b\nDask Series Structure:\nnpartitions=10\n2021-09-01 00:00:00    object\n2021-09-11 00:00:00       ...\n                         ...\n2021-11-30 00:00:00       ...\n2021-12-09 23:00:00       ...\nName: b, dtype: object\nDask Name: getitem, 20 tasks\n\n>>> ddf[\"2021-10-01\": \"2021-10-09 5:00\"]\nDask DataFrame Structure:\n                                  a       b\nnpartitions=1\n2021-10-01 00:00:00.000000000  int64  object\n2021-10-09 05:00:59.999999999    ...     ...\nDask Name: loc, 11 tasks\n```\n\n----------------------------------------\n\nTITLE: Computing Distribution Bias with Standard Dask Array\nDESCRIPTION: Example showing how to compute the largest value of a distribution after removing its bias using standard Dask operations. This approach loads the entire array into memory which can be problematic for large datasets.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/graph_manipulation.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.array as da\n>>> x = da.random.default_rng().normal(size=500_000_000, chunks=100_000)\n>>> x_mean = x.mean()\n>>> y = (x - x_mean).max().compute()\n```\n\n----------------------------------------\n\nTITLE: Configuring Filesystem Backend in read_parquet\nDESCRIPTION: Makes the filesystem backend configurable when using read_parquet to read Parquet files. This allows specifying different filesystem implementations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n\"read_parquet\"\n```\n\n----------------------------------------\n\nTITLE: Loading Dask Arrays from Zarr\nDESCRIPTION: This code snippet shows how to load Dask arrays from previously saved Zarr files. It loads four arrays that were created in the previous step.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/order.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> # load the data.\n>>> x1 = da.from_zarr('saved_x1.zarr')\n>>> y1 = da.from_zarr('saved_x2.zarr')\n>>> x2 = da.from_zarr('saved_y1.zarr')\n>>> y2 = da.from_zarr('saved_y2.zarr')\n```\n\n----------------------------------------\n\nTITLE: Masking Operations in Dask Array\nDESCRIPTION: Shows how to perform masking operations in Dask arrays using NumPy's masked arrays. Demonstrates assigning masked values and working with masked arrays.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-assignment.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> x = da.ones((2, 6))\n>>> x[0, [1, -2]] = np.ma.masked\n>>> x[1] = np.ma.array([0, 1, 2, 3, 4, 5], mask=[0, 1, 1, 0, 0, 0])\n>>> print(x.compute())\n[[1.0 -- 1.0 1.0 -- 1.0]\n [0.0 -- -- 3.0 4.0 5.0]]\n>>> x[:, 0] = x[:, 1]\n>>> print(x.compute())\n[[1.0 -- 1.0 1.0 -- 1.0]\n [0.0 -- -- 3.0 4.0 5.0]]\n>>> x[:, 0] = x[:, 1]\n>>> print(x.compute())\n[[-- -- 1.0 1.0 -- 1.0]\n [-- -- -- 3.0 4.0 5.0]]\n```\n\n----------------------------------------\n\nTITLE: Reshaping Dask Array with Default Chunk Merging\nDESCRIPTION: Example demonstrating creation and reshaping of a Dask array using default chunk merging behavior. Shows initialization of a 3D array and reshaping it to 2D.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-chunks.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> a = da.from_array(np.arange(24).reshape(2, 3, 4), chunks=((2,), (2, 1), (2, 2)))\n>>> a\ndask.array<array, shape=(2, 3, 4), dtype=int64, chunksize=(2, 2, 2), chunktype=numpy.ndarray>\n>>> a.reshape(6, 4).visualize()\n```\n\n----------------------------------------\n\nTITLE: Installing Dask Gateway with Helm\nDESCRIPTION: This command installs Dask Gateway using Helm. It creates a new namespace called 'dask-gateway' and generates a name for the release.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-kubernetes.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --repo https://helm.dask.org --create-namespace -n dask-gateway --generate-name dask-gateway\n```\n\n----------------------------------------\n\nTITLE: Creating Dask DataFrame Task Graph Example\nDESCRIPTION: Example showing how to create a simple Dask DataFrame and the task graph it generates with read_csv, addition, and filtering operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/high-level-graphs.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\n\ndf = dd.read_csv('myfile.*.csv')\ndf = df + 100\ndf = df[df.name == 'Alice']\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Automatic Chunksize Adjustment in xarray.apply_ufunc\nDESCRIPTION: This snippet shows how xarray.apply_ufunc now automatically adjusts chunksizes to keep the maximum chunksize under control when allow_rechunk=True is set. It creates a DataArray with random data and applies interpolation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport xarray as xr\nimport dask.array as da\n\narr = xr.DataArray(\n    da.random.random((1, 750, 45910), chunks=(1, \"auto\", -1)),\n    dims=[\"band\", \"y\", \"x\"],\n)\n\nresult = arr.interp(\n    y=arr.coords[\"y\"],\n    method=\"linear\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using Cytoscape Engine for Visualization\nDESCRIPTION: Shows how to use the alternative Cytoscape visualization engine for rendering Dask task graphs.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/graphviz.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\nx = da.ones((15, 15), chunks=(5, 5))\n\ny = x + x.T\n\n# visualize the low level Dask graph using cytoscape\ny.visualize(engine=\"cytoscape\")\n```\n\n----------------------------------------\n\nTITLE: Processing Text Data with Dask Bags\nDESCRIPTION: Demonstrates using Dask Bags for parallel processing of text or JSON data with map-reduce operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/index.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dask.bag as db\n\n# Read large datasets in parallel\nlines = db.read_text(\"s3://mybucket/data.*.json\")\nrecords = (lines\n    .map(json.loads)\n    .filter(lambda d: d[\"value\"] > 0)\n)\ndf = records.to_dask_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Writing Parquet Metadata File\nDESCRIPTION: Example of enabling the generation of a global _metadata file when writing Parquet data. This can improve read performance but may cause memory issues with large datasets.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-parquet.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> df.to_parquet(\n...     \"s3://bucket-name/my/parquet/\",\n...     write_metadata_file=True  # enable writing the _metadata file\n... )\n```\n\n----------------------------------------\n\nTITLE: Configuring dtype_backend in Dask DataFrame\nDESCRIPTION: Sets the dtype_backend configuration option to use either pandas or pyarrow for dtype handling in Dask DataFrames. This allows switching between different dtype implementations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n\"dtype_backend=\\\"pandas|pyarrow\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Dask Task Graph Visualization\nDESCRIPTION: Shows how to inspect and visualize Dask task graphs for different operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n>>> result.dask\nHighLevelGraph with 7 layers.\n<dask.highlevelgraph.HighLevelGraph object at 0x7f129df7a9d0>\n1. from_pandas-0b850a81e4dfe2d272df4dc718065116\n2. loc-fb7ada1e5ba8f343678fdc54a36e9b3e\n3. getitem-55d10498f88fc709e600e2c6054a0625\n4. series-cumsum-map-131dc242aeba09a82fea94e5442f3da9\n5. series-cumsum-take-last-9ebf1cce482a441d819d8199eac0f721\n6. series-cumsum-d51d7003e20bd5d2f767cd554bdd5299\n7. sub-fed3e4af52ad0bd9c3cc3bf800544f57\n```\n\n----------------------------------------\n\nTITLE: Configuring Optuna with DaskStorage\nDESCRIPTION: Sets up Optuna study with Dask integration using DaskStorage for distributed hyperparameter optimization.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/ml.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport optuna\n\nstorage = optuna.integration.DaskStorage()\n\nstudy = optuna.create_study(\n    direction=\"maximize\",\n    storage=storage,  # This makes the study Dask-enabled\n)\n```\n\n----------------------------------------\n\nTITLE: Using delayed to wrap functions in Python\nDESCRIPTION: The delayed function can be used as a decorator or directly around function calls to create Delayed proxy objects. These proxies contain the computational graph of operations needed to produce the result.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-api.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndelayed(foo)(a, b, c)\n```\n\n----------------------------------------\n\nTITLE: Installing DaskHub with Helm\nDESCRIPTION: This command installs DaskHub, which includes Dask Gateway and JupyterHub, using Helm. It creates a new namespace called 'daskhub' and generates a name for the release.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-kubernetes.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --repo https://helm.dask.org --create-namespace -n daskhub --generate-name daskhub\n```\n\n----------------------------------------\n\nTITLE: HighLevelGraph Class Structure\nDESCRIPTION: Defines the basic structure of a HighLevelGraph class with layers and dependencies as main components.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/high-level-graphs.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass HighLevelGraph(Mapping):\n    layers: Dict[str, Mapping]\n    dependencies: Dict[str, Set[str]]\n```\n\n----------------------------------------\n\nTITLE: Einstein Summation with Dask Arrays\nDESCRIPTION: Demonstration of Einstein summation operation with Dask Arrays, showing improved chunking behavior that maintains consistent chunk sizes to prevent memory issues.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\narr = da.random.random((1024, 64, 64, 64, 64), chunks=(256, 16, 16, 16, 16)) # Initial chunks are 128 MiB\nresult = da.einsum(\"aijkl,amnop->ijklmnop\", arr, arr)\n```\n\n----------------------------------------\n\nTITLE: Dispatching CuPy Arrays to cuDF DataFrames\nDESCRIPTION: Enables dispatching cupy.ndarray objects to cudf.DataFrame objects in dask.dataframe operations. This allows seamless integration between CuPy arrays and cuDF DataFrames.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n\"cupy.ndarray\" to \"cudf.DataFrame\"\n```\n\n----------------------------------------\n\nTITLE: Dask Distributed Client Setup\nDESCRIPTION: Shows how to initialize and use Dask's distributed client for parallel computing.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\n\nclient = Client()\n\ndef inc(x):\n   return x + 1\n\ndef add(x, y):\n   return x + y\n\na = client.submit(inc, 1)     # work starts immediately\nb = client.submit(inc, 2)     # work starts immediately\nc = client.submit(add, a, b)  # work starts immediately\n\nc = c.result()                # block until work finishes, then gather result\n```\n\n----------------------------------------\n\nTITLE: Specifying Rechunk Method for Dask Array in Python\nDESCRIPTION: Creates a random Dask array and rechunks it, explicitly specifying the rechunk method as either 'tasks' or 'p2p'.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\narr = da.random.random(size=(1000, 1000, 365), chunks=(-1, -1, \"auto\"))\n# Choose either \"tasks\" or \"p2p\"\narr = arr.rechunk((\"auto\", \"auto\", -1), method=\"tasks\")\n```\n\n----------------------------------------\n\nTITLE: Tokenizing cumulative groupby aggregations in Dask DataFrame\nDESCRIPTION: Improves the tokenization of cumulative groupby aggregations in Dask DataFrame for better performance and caching.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_73\n\nLANGUAGE: Python\nCODE:\n```\nddf.groupby(...).cumsum()\n```\n\n----------------------------------------\n\nTITLE: Setting Up LocalCluster in Dask Python\nDESCRIPTION: Illustrates how to set up a LocalCluster in Dask, which is useful for running distributed computations on a single machine for testing and debugging purposes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/debug.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client, LocalCluster\nclient = Client()  # This is actually the following two commands\n\ncluster = LocalCluster()\nclient = Client(cluster.scheduler.address)\n```\n\n----------------------------------------\n\nTITLE: Partition Indexing in Dask DataFrame using Python\nDESCRIPTION: Demonstrates partition-level indexing in Dask DataFrames using get_partition() and .partitions. This allows selecting subsets of data by partition rather than by position or label.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-indexing.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask\n>>> ddf = dask.datasets.timeseries(start=\"2021-01-01\", end=\"2021-01-07\", freq=\"1h\")\n>>> ddf.get_partition(0)\nDask DataFrame Structure:\n                   name     id        x        y\nnpartitions=1\n2021-01-01     object  int64  float64  float64\n2021-01-02        ...    ...      ...      ...\nDask Name: get-partition, 2 graph layers\n\n>>> ddf.partitions[::2]\nDask DataFrame Structure:\n                   name     id        x        y\nnpartitions=3\n2021-01-01     object  int64  float64  float64\n2021-01-03        ...    ...      ...      ...\n2021-01-05        ...    ...      ...      ...\n2021-01-06        ...    ...      ...      ...\nDask Name: blocks, 2 graph layers\n\n>>> mask = ddf.id.map_partitions(lambda p: len(p.unique()) > 20).compute()\n>>> ddf.partitions[mask]\nDask DataFrame Structure:\n                   name     id        x        y\nnpartitions=5\n2021-01-01     object  int64  float64  float64\n2021-01-02        ...    ...      ...      ...\n...               ...    ...      ...      ...\n2021-01-06        ...    ...      ...      ...\n2021-01-07        ...    ...      ...      ...\nDask Name: blocks, 2 graph layers\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Data as Known Categoricals\nDESCRIPTION: Demonstrates reading CSV data into known categorical columns using CategoricalDtype with predefined categories.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-categoricals.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> dtype = {'col': pd.api.types.CategoricalDtype(['a', 'b', 'c'])}\n>>> ddf = dd.read_csv(..., dtype=dtype)\n```\n\n----------------------------------------\n\nTITLE: Constructing HighLevelGraph Example\nDESCRIPTION: Demonstrates how to explicitly construct a HighLevelGraph by providing layers and dependencies dictionaries.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/high-level-graphs.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlayers = {\n   'read-csv': {('read-csv', 0): (pandas.read_csv, 'myfile.0.csv'),\n                ('read-csv', 1): (pandas.read_csv, 'myfile.1.csv'),\n                ('read-csv', 2): (pandas.read_csv, 'myfile.2.csv'),\n                ('read-csv', 3): (pandas.read_csv, 'myfile.3.csv')},\n\n   'add': {('add', 0): (operator.add, ('read-csv', 0), 100),\n           ('add', 1): (operator.add, ('read-csv', 1), 100),\n           ('add', 2): (operator.add, ('read-csv', 2), 100),\n           ('add', 3): (operator.add, ('read-csv', 3), 100)},\n\n   'filter': {('filter', 0): (lambda part: part[part.name == 'Alice'], ('add', 0)),\n              ('filter', 1): (lambda part: part[part.name == 'Alice'], ('add', 1)),\n              ('filter', 2): (lambda part: part[part.name == 'Alice'], ('add', 2)),\n              ('filter', 3): (lambda part: part[part.name == 'Alice'], ('add', 3))}\n}\n\ndependencies = {'read-csv': set(),\n                'add': {'read-csv'},\n                'filter': {'add'}}\n\ngraph = HighLevelGraph(layers, dependencies)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Dask Array Computation (IPython)\nDESCRIPTION: IPython code snippet demonstrating how to benchmark the computation of a Dask array sum. It shows the creation of a Dask array, checking its task graph size, and timing its computation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/shared.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: import dask.array as da\n\nIn [2]: x = da.ones(1000, chunks=(2,)).sum()\n\nIn [3]: len(x.dask)\nOut[3]: 1168\n\nIn [4]: %timeit x.compute()\n92.1 ms  2.61 ms per loop (mean  std. dev. of 7 runs, 10 loops each)\n```\n\n----------------------------------------\n\nTITLE: Supporting Partial Functions with Arguments in Aggregations\nDESCRIPTION: Adds support for using partial functions with arguments in aggregation operations. Allows more flexible custom aggregations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n\"Partial functions in aggs may have arguments\"\n```\n\n----------------------------------------\n\nTITLE: Creating Dask Bags from Sequences\nDESCRIPTION: Demonstrates how to create Dask bags from Python iterables using db.from_sequence with optional partition control.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/bag-creation.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.bag as db\n>>> b = db.from_sequence([1, 2, 3, 4, 5, 6])\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> b = db.from_sequence([1, 2, 3, 4, 5, 6], npartitions=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> b = db.from_sequence(['1.dat', '2.dat', ...]).map(load_from_filename)\n```\n\n----------------------------------------\n\nTITLE: Using Blockwise Reshape for Dask Array in Python\nDESCRIPTION: Demonstrates the use of the new blockwise_reshape function for efficient reshaping of a Dask array, followed by a reduction operation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\narr = da.random.random(size=(100, 100, 48_000), chunks=(1000, 100, 83)\nresult = reshape_blockwise(arr, (10_000, 48_000))\nresult.sum()\n\n# or: do something that preserves the shape of each chunk\n\nresult = reshape_blockwise(result, (100, 100, 48_000), chunks=arr.chunks)\n```\n\n----------------------------------------\n\nTITLE: Supporting single CSV file output in Dask\nDESCRIPTION: Implementation that adds support for writing Dask DataFrames to a single CSV file, rather than only to multiple part files.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\n-  Support output to a single CSV file (:pr:`5304`) `Hongjiu Zhang`_\n```\n\n----------------------------------------\n\nTITLE: Implementing Extension Array Support in Python\nDESCRIPTION: Example of registering custom extension array dtype and scalar type with Dask for proper DataFrame integration.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-extend.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom decimal import Decimal\nfrom dask.dataframe.extensions import make_array_nonempty, make_scalar\nfrom pandas.tests.extension.decimal import DecimalArray, DecimalDtype\n\n@make_array_nonempty.register(DecimalDtype)\ndef _(dtype):\n    return DecimalArray._from_sequence([Decimal('0'), Decimal('NaN')],\n                                       dtype=dtype)\n\n\n@make_scalar.register(Decimal)\ndef _(x):\n   return Decimal('1')\n```\n\n----------------------------------------\n\nTITLE: Lock-based Resource Protection in Python\nDESCRIPTION: Examples of using distributed locks to protect shared resources across cluster processes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Lock\n\ndef load(fn):\n    with Lock('the-production-database'):\n        # read data from filename using some sensitive source\n        return ...\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Lock\nlock = Lock()\n\ndef load(fn, lock=None):\n    with lock:\n        # read data from filename using some sensitive source\n        return ...\n\nfutures = client.map(load, filenames, lock=lock)\n```\n\n----------------------------------------\n\nTITLE: Connecting Client to Dask Cloud Provider Cluster\nDESCRIPTION: Shows how to connect a Dask client to a cloud-deployed cluster created with Dask Cloud Provider, allowing interaction with the remote cluster as if it were local.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-cloud.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> client = cluster.get_client()\n```\n\n----------------------------------------\n\nTITLE: Examples of Valid Dask Computations in Python\nDESCRIPTION: Various examples of valid computation formats in Dask, including arrays, function calls, nested computations, and list operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/spec.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnp.array([...])\n(add, 1, 2)\n(add, 'x', 2)\n(add, (inc, 'x'), 2)\n(sum, [1, 2])\n(sum, ['x', (inc, 'x')])\n(np.dot, np.array([...]), np.array([...]))\n[(sum, ['x', 'y']), 'z']\n```\n\n----------------------------------------\n\nTITLE: HDF5 Storage with Dask\nDESCRIPTION: Examples of storing Dask arrays in HDF5 format using h5py and the specialized to_hdf5 function.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n>>> import h5py\n>>> f = h5py.File('myfile.hdf5')\n>>> d = f.require_dataset('/data', shape=x.shape, dtype=x.dtype)\n>>> da.store(x, d)\n\n>>> da.store([array1, array2], [output1, output2])  # doctest: +SKIP\n\n>>> da.to_hdf5('myfile.hdf5', '/y', y)  # doctest: +SKIP\n\n>>> da.to_hdf5('myfile.hdf5', {'/x': x, '/y': y})  # doctest: +SKIP\n```\n\n----------------------------------------\n\nTITLE: Setting Global ThreadPoolExecutor in Dask (Python)\nDESCRIPTION: Demonstrates how to set a global ThreadPoolExecutor for Dask to reduce overhead from creating new executors. This can be done either globally or within a context manager.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/shared.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from concurrent.futures import ThreadPoolExecutor\n>>> pool = ThreadPoolExecutor()\n>>> dask.config.set(pool=pool)  # set global ThreadPoolExecutor\n\nor\n\n>>> with dask.config.set(pool=pool)  # use ThreadPoolExecutor throughout with block\n...     ...\n```\n\n----------------------------------------\n\nTITLE: Implementing numeric_only support in DataFrame methods in Python\nDESCRIPTION: Adds support for the numeric_only parameter in various DataFrame methods including idxmin, idxmax, quantile, std, skew, and kurtosis.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nAdd numeric_only support to DataFrame.idxmin and DataFrame.idxmax\n```\n\nLANGUAGE: Python\nCODE:\n```\nImplement numeric_only support for DataFrame.quantile\n```\n\nLANGUAGE: Python\nCODE:\n```\nAdd support for numeric_only=False in DataFrame.std\n```\n\nLANGUAGE: Python\nCODE:\n```\nImplement numeric_only for skew and kurtosis\n```\n\n----------------------------------------\n\nTITLE: Local Dask DataFrame Computation\nDESCRIPTION: Basic example of using Dask with thread-based parallelism on a local machine for DataFrame operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\ndf = dd.read_csv(...)\ndf.x.sum().compute()  # This uses threads on your local machine\n```\n\n----------------------------------------\n\nTITLE: Customizing Docker Image with Extra Packages\nDESCRIPTION: Example of running a Dask worker with additional packages installed via conda. This demonstrates how to use environment variables to extend the base Docker image with custom dependencies.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-docker.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --network dask -e EXTRA_CONDA_PACKAGES=\"joblib\" ghcr.io/dask/dask dask-worker scheduler:8786\n```\n\n----------------------------------------\n\nTITLE: Implementing unique and value_counts for SeriesGroupBy in Dask\nDESCRIPTION: Enhancement that adds the unique and value_counts methods to the SeriesGroupBy object in Dask DataFrame, matching pandas functionality for grouped series operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_46\n\nLANGUAGE: Python\nCODE:\n```\n-  Implement unique and value_counts for SeriesGroupBy (:pr:`5358`) `Scott Sievert`_\n```\n\n----------------------------------------\n\nTITLE: Queue-based Task Processing in Python\nDESCRIPTION: Demonstrates using distributed queues for task coordination and result handling across processes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Queue\n\ndef load_and_submit(filename):\n    data = load(filename)\n    client = get_client()\n    future = client.submit(process, data)\n    queue.put(future)\n\nclient = Client()\n\nqueue = Queue()\n\nfor filename in filenames:\n    future = client.submit(load_and_submit, filename)\n    fire_and_forget(future)\n\nwhile True:\n    future = queue.get()\n    print(future.result())\n```\n\n----------------------------------------\n\nTITLE: Accessing HDFS with authentication in Dask\nDESCRIPTION: Shows how to access HDFS with user authentication by including the username and password in the URL when reading CSV files.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/connect-to-remote-data.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = dd.read_csv('hdfs://user@server:port/path/*.csv')\n```\n\n----------------------------------------\n\nTITLE: Converting Between Dask Interfaces\nDESCRIPTION: Example of converting between different Dask interfaces, specifically showing how to convert between DataFrames and delayed objects.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/user-interfaces.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Convert to a list of delayed Pandas dataframes\ndelayed_values = df.to_delayed()\n\n# Manipulate delayed values arbitrarily as you like\n\n# Convert many delayed Pandas DataFrames back to a single Dask DataFrame\ndf = dd.from_delayed(delayed_values)\n```\n\n----------------------------------------\n\nTITLE: Basic Dask Array Computation\nDESCRIPTION: Demonstrates converting Dask arrays to NumPy arrays using compute() or np.array().\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n>>> x = da.arange(6, chunks=3)\n>>> y = x**2\n>>> np.array(y)\narray([0, 1, 4, 9, 16, 25])\n\n>>> y.compute()\narray([0, 1, 4, 9, 16, 25])\n```\n\n----------------------------------------\n\nTITLE: Sequential Problem Definition in Python\nDESCRIPTION: A sequential implementation of a computation that has parallelization potential but is written as a normal Python program with inc, double, and add functions.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef inc(x):\n    return x + 1\n\ndef double(x):\n    return x * 2\n\ndef add(x, y):\n    return x + y\n\ndata = [1, 2, 3, 4, 5]\n\noutput = []\nfor x in data:\n    a = inc(x)\n    b = double(x)\n    c = add(a, b)\n    output.append(c)\n\ntotal = sum(output)\n```\n\n----------------------------------------\n\nTITLE: Dask Array Plugins Implementation\nDESCRIPTION: Examples of implementing and using Dask array plugins for debugging and custom behaviors.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n>>> def f(x):\n...     print(x.nbytes)\n\n>>> with dask.config.set(array_plugins=[f]):\n...     x = da.ones((10, 1), chunks=(5, 1))\n...     y = x.dot(x.T)\n80\n80\n800\n800\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing Dask Worker\nDESCRIPTION: Example showing how to create and manage a Dask Worker connected to a scheduler address using async/await pattern.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python-advanced.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dask.distributed import Scheduler, Worker\n\nasync def f(scheduler_address):\n    w = await Worker(scheduler_address)\n    await w.finished()\n\nasyncio.get_event_loop().run_until_complete(f(\"tcp://127.0.0.1:8786\"))\n```\n\n----------------------------------------\n\nTITLE: Fixing @delayed decorator for methods in Dask Delayed\nDESCRIPTION: Fixes the @delayed decorator to work correctly with methods and adds corresponding tests.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_75\n\nLANGUAGE: Python\nCODE:\n```\n@dask.delayed\ndef method(self, ...):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Creating a Dask Bag from a Sequence\nDESCRIPTION: Shows how to create a Dask Bag from a Python sequence by specifying the number of partitions, which distributes the items for parallel processing.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> b = db.from_sequence([1, 2, 3, 4, 5, 6, 2, 1], npartitions=2)\n... b\ndask.bag<from_sequence, npartitions=2>\n```\n\n----------------------------------------\n\nTITLE: Task Thread Pool Management in Python\nDESCRIPTION: Examples of managing thread pool resources using secede() and rejoin() functions for efficient task execution.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/futures.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import get_client, secede\n\ndef monitor(device):\n   client = get_client()\n   secede()  # remove this task from the thread pool\n   while True:\n       data = device.read_data()\n       future = client.submit(process, data)\n       fire_and_forget(future)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef f(n):  # assume that this runs as a task\n   client = get_client()\n\n   secede()  # secede while we wait for results to come back\n   futures = client.map(func, range(n))\n   results = client.gather(futures)\n\n   rejoin()  # block until a slot is open in the thread pool\n   result = analyze(results)\n   return result\n```\n\n----------------------------------------\n\nTITLE: Implementing groupby transform method in Dask\nDESCRIPTION: Code that adds the transform method to the groupby operation in Dask DataFrames, allowing for group-wise transformations that return a DataFrame with the same shape as the original.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_49\n\nLANGUAGE: Python\nCODE:\n```\n-  Add ``groupby().transform()`` (:pr:`5327`) `Oliver Hofkens`_\n```\n\n----------------------------------------\n\nTITLE: Accessing Configuration Values in Code\nDESCRIPTION: Demonstrates how to access configuration values in code using dask.config.get\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/configuration.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# dask_foo/core.py\n\ndef process(fn, color=dask.config.get('foo.color')):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Dask Collection (Tuple) in Python\nDESCRIPTION: Example implementation of a custom Dask collection representing a tuple. It includes the required Dask interface methods and optimizations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/custom-collections.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Saved as dask_tuple.py\nimport dask\nfrom dask.base import DaskMethodsMixin, replace_name_in_key\nfrom dask.optimization import cull\n\ndef tuple_optimize(dsk, keys, **kwargs):\n    # We cull unnecessary tasks here. See\n    # https://docs.dask.org/en/stable/optimize.html for more\n    # information on optimizations in Dask.\n    dsk2, _ = cull(dsk, keys)\n    return dsk2\n\n# We subclass from DaskMethodsMixin to add common dask methods to\n# our class (compute, persist, and visualize). This is nice but not\n# necessary for creating a Dask collection (you can define them\n# yourself).\nclass Tuple(DaskMethodsMixin):\n    def __init__(self, dsk, keys):\n        # The init method takes in a dask graph and a set of keys to use\n        # as outputs.\n        self._dsk = dsk\n        self._keys = keys\n\n    def __dask_graph__(self):\n        return self._dsk\n\n    def __dask_keys__(self):\n        return self._keys\n\n    # use the `tuple_optimize` function defined above\n    __dask_optimize__ = staticmethod(tuple_optimize)\n\n    # Use the threaded scheduler by default.\n    __dask_scheduler__ = staticmethod(dask.threaded.get)\n\n    def __dask_postcompute__(self):\n        # We want to return the results as a tuple, so our finalize\n        # function is `tuple`. There are no extra arguments, so we also\n        # return an empty tuple.\n        return tuple, ()\n\n    def __dask_postpersist__(self):\n        # We need to return a callable with the signature\n        # rebuild(dsk, *extra_args, rename: Mapping[str, str] = None)\n        return Tuple._rebuild, (self._keys,)\n\n    @staticmethod\n    def _rebuild(dsk, keys, *, rename=None):\n        if rename is not None:\n            keys = [replace_name_in_key(key, rename) for key in keys]\n        return Tuple(dsk, keys)\n\n    def __dask_tokenize__(self):\n        # For tokenize to work we want to return a value that fully\n        # represents this object. In this case it's the list of keys\n        # to be computed.\n        return self._keys\n```\n\n----------------------------------------\n\nTITLE: Computing Results from Dask Bag\nDESCRIPTION: Shows how to execute a computation on a Dask Bag by calling compute() to convert the distributed collection into a Python list.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> b.compute()\n[1, 2, 3, 4, 5, 6, 2, 1]\n```\n\n----------------------------------------\n\nTITLE: Using dask-ssh with UNIX Grouping\nDESCRIPTION: Example of using UNIX-style brace expansion to specify multiple hosts in a dask-ssh command.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-ssh.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ dask-ssh 192.168.0.{1,2,3,4}\n```\n\n----------------------------------------\n\nTITLE: Dask Delayed Operations\nDESCRIPTION: Demonstrates lazy evaluation using Dask delayed decorators for function calls.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport dask\n\n@dask.delayed\ndef inc(x):\n   return x + 1\n\n@dask.delayed\ndef add(x, y):\n   return x + y\n\na = inc(1)       # no work has happened yet\nb = inc(2)       # no work has happened yet\nc = add(a, b)    # no work has happened yet\n\nc = c.compute()  # This triggers all of the above computations\n```\n\n----------------------------------------\n\nTITLE: Dask Array Positional Indexing Example\nDESCRIPTION: Shows improved positional indexing for Dask arrays that maintains stable chunk sizes and generates efficient task graphs.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nx[slice(None), [1, 1, 3, 6, 3, 4, 5]]\n```\n\n----------------------------------------\n\nTITLE: Adding matrix multiplication operator to Dask Delayed objects\nDESCRIPTION: Adds support for the @ operator (matrix multiplication) to Dask Delayed objects.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_74\n\nLANGUAGE: Python\nCODE:\n```\ndelayed_a @ delayed_b\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Collection Optimizations\nDESCRIPTION: Demonstrates how to configure different optimization functions for various collection types simultaneously, including disabling optimization for specific types.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/optimize.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwith dask.config.set(array_optimize=my_optimize_function,\n                     dataframe_optimize=None,\n                     delayed_optimize=my_other_optimize_function):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Initializing Dask LocalCluster Without Nanny Process\nDESCRIPTION: Demonstrates how to create a LocalCluster without nanny processes, allowing direct worker state inspection. The nanny parameter is set to False to enable direct worker access.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/debug.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> cluster = LocalCluster(nanny=False)\n>>> client = Client(cluster)\n```\n\n----------------------------------------\n\nTITLE: Adding DataFrame.pop implementation in Dask\nDESCRIPTION: Code that implements the pop method for Dask DataFrames, which removes a column from the DataFrame and returns it, similar to pandas' DataFrame.pop functionality.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\n-  Add DataFrame.pop implementation (:pr:`5422`) `Matthew Rocklin`_\n```\n\n----------------------------------------\n\nTITLE: Initializing Dask Cluster on Kubernetes using KubeCluster\nDESCRIPTION: Example showing how to create and scale a Dask cluster on Kubernetes using the KubeCluster operator. The code demonstrates cluster initialization with custom resource specifications, scaling to 10 workers, and client connection.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/index.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_kubernetes.operator import KubeCluster\ncluster = KubeCluster(\n   name=\"my-dask-cluster\",\n   image=\"ghcr.io/dask/dask:latest\",\n   resources={\"requests\": {\"memory\": \"2Gi\"}, \"limits\": {\"memory\": \"64Gi\"}},\n)\ncluster.scale(10)\nclient = cluster.get_client()\n```\n\n----------------------------------------\n\nTITLE: Initializing Dask Array with Auto Chunking in Python\nDESCRIPTION: Use chunks=\"auto\" in array creation routines to automatically determine chunk sizes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_55\n\nLANGUAGE: Python\nCODE:\n```\nUse chunks=\"auto\" in array creation routines\n```\n\n----------------------------------------\n\nTITLE: Dask Client Connection Examples\nDESCRIPTION: Demonstrates how to connect to local and remote Dask clusters.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n>>> from dask.distributed import Client\n...\n... client = Client()\n... client\n<Client: 'tcp://127.0.0.1:41703' processes=4 threads=12, memory=31.08 GiB>\n```\n\n----------------------------------------\n\nTITLE: Using dask-ssh with IP Addresses\nDESCRIPTION: Command line example showing how to initialize a Dask network using dask-ssh with specific IP addresses.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-ssh.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ dask-ssh 192.168.0.1 192.168.0.2 192.168.0.3 192.168.0.4\n```\n\n----------------------------------------\n\nTITLE: Inspecting Cache Contents in Dask\nDESCRIPTION: Example showing how to inspect the contents and statistics of the cache object.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/caching.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> cache.cache.data\n<stored values>\n>>> cache.cache.heap.heap\n<scores of items in cache>\n>>> cache.cache.nbytes\n<number of bytes per item in cache>\n```\n\n----------------------------------------\n\nTITLE: Enabling Logical Query Planning for Dask DataFrames in Python\nDESCRIPTION: Code to enable the new logical query planning feature for Dask DataFrames, which is currently off by default but can provide performance improvements.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndask.config.set({\"dataframe.query-planning\": True})\n```\n\n----------------------------------------\n\nTITLE: Adding dtype support to arange in Dask Array\nDESCRIPTION: Adds support for specifying dtype when creating a Dask Array using arange.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_64\n\nLANGUAGE: Python\nCODE:\n```\nda.arange(..., dtype=np.float32)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables to Limit Thread Usage in Bash\nDESCRIPTION: This bash snippet shows how to set environment variables to limit the number of threads used by BLAS/LAPACK libraries. This is important to avoid oversubscribing threads when using Dask with array-computing libraries.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-best-practices.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OMP_NUM_THREADS=1\nexport MKL_NUM_THREADS=1\nexport OPENBLAS_NUM_THREADS=1\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask Single-Threaded Scheduler in Python\nDESCRIPTION: Sets the Dask scheduler to use a single thread. This is useful for debugging and profiling as it executes all computations in the local thread without parallelism.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduling.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dask\ndask.config.set(scheduler='synchronous')  # overwrite default with single-threaded scheduler\n```\n\n----------------------------------------\n\nTITLE: Implementing dropna argument in Pandas groupby with Dask DataFrame\nDESCRIPTION: Code showing the addition of the dropna argument to the groupby method in Dask DataFrame, which allows filtering out groups with NA values, similar to pandas' functionality.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_43\n\nLANGUAGE: Python\nCODE:\n```\n-  Add dropna argument to groupby (:pr:`5579`) `Richard J Zamora`_\n```\n\n----------------------------------------\n\nTITLE: DataFrame Operations Requiring Shuffling\nDESCRIPTION: Examples of operations that may require a full dataset shuffle in Dask, which are more computationally expensive. These include applying functions to groups not based on index, joining on non-index columns, and setting a new index.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf.groupby(columns_no_index).apply(user_fn)   # Requires shuffle\n>>> lhs.join(rhs, on=columns_no_index)             # Requires shuffle\n>>> ddf.set_index(column)                          # Requires shuffle\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing Dask Scheduler\nDESCRIPTION: Example demonstrating how to create and manage a Dask Scheduler using async/await pattern, including startup and shutdown.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python-advanced.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dask.distributed import Scheduler, Worker\n\nasync def f():\n    s = Scheduler()        # scheduler created, but not yet running\n    s = await s            # the scheduler is running\n    await s.finished()     # wait until the scheduler closes\n\nasyncio.get_event_loop().run_until_complete(f())\n```\n\n----------------------------------------\n\nTITLE: HPC Job Queue Cluster Setup\nDESCRIPTION: Example of setting up a Dask cluster on HPC systems using dask-jobqueue with PBS scheduler configuration.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_jobqueue import PBSCluster\ncluster = PBSCluster(\n    cores=24,\n    memory=\"100GB\",\n    queue=\"regular\",\n    account=\"my-account\",\n)\ncluster.scale(jobs=100)\nclient = cluster.get_client()\n```\n\n----------------------------------------\n\nTITLE: Accessing DataFrame Metadata Sample in Dask\nDESCRIPTION: Shows how to access non-empty metadata samples using the _meta_nonempty attribute, which contains fake data for operation testing.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-design.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf._meta_nonempty\n   a    b\n0  1  foo\n1  1  foo\n```\n\n----------------------------------------\n\nTITLE: Visualizing High-Level Dask Graph\nDESCRIPTION: Shows how to visualize the high-level Dask graph structure using the dask.visualize() method.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/graphviz.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\nx = da.ones((15, 15), chunks=(5, 5))\ny = x + x.T\n\n# visualize the high level Dask graph\ny.dask.visualize(filename='transpose-hlg.svg')\n```\n\n----------------------------------------\n\nTITLE: Implementing Opportunistic Caching in Dask\nDESCRIPTION: Example showing how to activate and use Dask's opportunistic caching mechanism with a fixed cache size.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/caching.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from dask.cache import Cache\n>>> cache = Cache(2e9)  # Leverage two gigabytes of memory\n>>> cache.register()    # Turn cache on globally\n```\n\n----------------------------------------\n\nTITLE: Handling Timezone-aware Datetime Index in repartition\nDESCRIPTION: Fixes a bug when repartitioning DataFrames with a timezone-aware datetime index. Ensures proper handling of timezones during repartitioning operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n\"tz\"-aware datetime index\n```\n\n----------------------------------------\n\nTITLE: Dask Futures Example for Immediate Computation\nDESCRIPTION: Shows how to use Dask Futures interface for immediate computation instead of lazy evaluation, useful for real-time or streaming applications.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/user-interfaces.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nclient = Client()\n\ndef inc(x):\n    return x + 1\n\ndef add(x, y):\n    return x + y\n\na = client.submit(inc, 1)     # work starts immediately\nb = client.submit(inc, 2)     # work starts immediately\nc = client.submit(add, a, b)  # work starts immediately\n\nc = c.result()                # block until work finishes, then gather result\n```\n\n----------------------------------------\n\nTITLE: Displaying HTML Representation of High-Level Graph\nDESCRIPTION: Demonstrates how to display the HTML representation of a high-level Dask graph in a Jupyter notebook.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/graphviz.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\nx = da.ones((15, 15), chunks=(5, 5))\ny = x + x.T\n\ny.dask  # shows the HTML representation in a Jupyter notebook\n```\n\n----------------------------------------\n\nTITLE: Adding aggregate function for rolling objects in Dask DataFrame\nDESCRIPTION: Adds an aggregate function for rolling window operations in Dask DataFrame.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_72\n\nLANGUAGE: Python\nCODE:\n```\nddf.rolling(...).aggregate(...)\n```\n\n----------------------------------------\n\nTITLE: Converting Delayed Objects to Futures\nDESCRIPTION: Demonstrates converting Delayed objects to Futures using the Client.compute() method.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/user-interfaces.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfutures = client.compute(delayeds)\n```\n\n----------------------------------------\n\nTITLE: DataFrame Data Load Operations\nDESCRIPTION: Various improvements to reading and writing data in Dask DataFrames, including expanded support for reading parquet files and proper handling of paths, filters and divisions.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_59\n\nLANGUAGE: Python\nCODE:\n```\ndf.to_csv()  # Now includes better key names for diagnostics\ndf.to_hdf()  # Updated return value documentation\nread_parquet(..., filters=...)  # Fixed handling of filters and divisions\nread_csv(\"/path/*.csv\")  # Now expands globs when reading multiple paths\n```\n\n----------------------------------------\n\nTITLE: GroupBy Unique Value Operations\nDESCRIPTION: Shows how nunique operation maintains partition count when using groupby aggregation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-design.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresult = df.groupby('id').value.nunique()\nresult.npartitions  # returns same as df.npartitions\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom CuDF Backend Entrypoint\nDESCRIPTION: Example implementation of a custom backend entrypoint for CuDF, showing how to register dispatch functions and implement required methods for DataFrame operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/selecting-the-collection-backend.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.dataframe.backends import DataFrameBackendEntrypoint\nfrom dask.dataframe.dispatch import (\n   ...\n   make_meta_dispatch,\n   ...\n)\n...\n\ndef make_meta_cudf(x, index=None):\n   return x.head(0)\n...\n\nclass CudfBackendEntrypoint(DataFrameBackendEntrypoint):\n\n   def __init__(self):\n      # Register compute-based dispatch functions\n      # (e.g. make_meta_dispatch, sizeof_dispatch)\n      ...\n      make_meta_dispatch.register(\n         (cudf.Series, cudf.DataFrame),\n         func=make_meta_cudf,\n      )\n      # NOTE: Registration may also be outside __init__\n      # if it is in the same module as this class\n   ...\n\n   @staticmethod\n   def read_orc(*args, **kwargs):\n      from .io import read_orc\n\n      # Use dask_cudf version of read_orc\n      return read_orc(*args, **kwargs)\n   ...\n```\n\n----------------------------------------\n\nTITLE: Converting Dask DataFrame to Delayed Objects in Python\nDESCRIPTION: Shows how to convert a Dask DataFrame into multiple delayed objects for processing when map_partitions is not suitable.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npartitions = df.to_delayed()\ndelayed_values = [dask.delayed(train)(part)\n                  for part in partitions]\n```\n\n----------------------------------------\n\nTITLE: Registering CLI Entry Point in setup.cfg\nDESCRIPTION: INI configuration showing how to register a Dask CLI entry point in setup.cfg using setuptools.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/cli.rst#2025-04-19_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[options.entry_points]\ndask_cli =\n    mycommand = mypackage.cli:main\n```\n\n----------------------------------------\n\nTITLE: Visualizing Optimized Dask Graph\nDESCRIPTION: Demonstrates how to visualize an optimized version of the Dask task graph using the optimize_graph parameter.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/graphviz.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\nx = da.ones((15, 15), chunks=(5, 5))\n\ny = x + x.T\n\n# visualize the low level Dask graph after optimizations\ny.visualize(filename=\"transpose_opt.svg\", optimize_graph=True)\n```\n\n----------------------------------------\n\nTITLE: Task Graph Dictionary Structure\nDESCRIPTION: Shows how the task graph is represented as a dictionary with keys for read-csv, add, and filter operations, demonstrating the relationship between tasks.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/high-level-graphs.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{\n ('read-csv', 0): (pandas.read_csv, 'myfile.0.csv'),\n ('read-csv', 1): (pandas.read_csv, 'myfile.1.csv'),\n ('read-csv', 2): (pandas.read_csv, 'myfile.2.csv'),\n ('read-csv', 3): (pandas.read_csv, 'myfile.3.csv'),\n ('add', 0): (operator.add, ('read-csv', 0), 100),\n ('add', 1): (operator.add, ('read-csv', 1), 100),\n ('add', 2): (operator.add, ('read-csv', 2), 100),\n ('add', 3): (operator.add, ('read-csv', 3), 100),\n ('filter', 0): (lambda part: part[part.name == 'Alice'], ('add', 0)),\n ('filter', 1): (lambda part: part[part.name == 'Alice'], ('add', 1)),\n ('filter', 2): (lambda part: part[part.name == 'Alice'], ('add', 2)),\n ('filter', 3): (lambda part: part[part.name == 'Alice'], ('add', 3)),\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask Array Operations\nDESCRIPTION: Code snippets demonstrating array operations configuration including chunks='auto' parameter usage, array creation, and meta handling.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_52\n\nLANGUAGE: Python\nCODE:\n```\nda.from_array(x, chunks='auto')\nda.ones(shape)  # shape must not be dask array\nda.zeros(shape)  # shape must not be dask array\nda.empty(shape)  # shape must not be dask array\nda.full(shape)   # shape must not be dask array\n```\n\n----------------------------------------\n\nTITLE: Accessing DataFrame Partitions Information\nDESCRIPTION: Shows how to view partition information including the number of partitions and division boundaries in a Dask DataFrame.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-design.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> df.npartitions\n4\n>>> df.divisions\n['2015-01-01', '2015-02-01', '2015-03-01', '2015-04-01', '2015-04-31']\n```\n\n----------------------------------------\n\nTITLE: Configuring GroupBy Operations in Dask\nDESCRIPTION: Examples of different groupby operations with various partition configurations using the split_out parameter.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-design.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresult = df.groupby('id').value.mean()\nresult.npartitions  # returns 1\n\nresult = df.groupby(['id', 'id2']).value.mean()\nresult.npartitions  # returns 5\n\nresult = df.groupby('id').value.mean(split_out=8)\nresult.npartitions  # returns 8\n```\n\n----------------------------------------\n\nTITLE: Basic Query Optimization Example in Python\nDESCRIPTION: Demonstrates how to create a Dask DataFrame from pandas, perform operations, and view the optimized query plan using pprint()\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-optimizer.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npdf = pd.DataFrame({\"a\": [1, 2, 3] * 5, \"b\": [1, 2, 3] * 5})\ndf = dd.from_pandas(pdf, npartitions=2)\ndf = df.replace(1, 5)[[\"a\"]]\n\ndf.optimize().pprint()\n```\n\n----------------------------------------\n\nTITLE: Importing Dask Dataset Generation Functions in Python\nDESCRIPTION: This code imports helper functions for generating demo datasets in Dask. These functions are useful for creating sample data for testing and demonstration purposes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/api.rst#2025-04-19_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom dask.datasets import make_people, timeseries\n```\n\n----------------------------------------\n\nTITLE: Checking Categorical Status in Dask DataFrame\nDESCRIPTION: Shows how to check if a categorical column has known categories using the .cat.known property.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-categoricals.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> ddf.col.cat.known\nFalse\n```\n\n----------------------------------------\n\nTITLE: Xarray Rolling Construction with Dask Array\nDESCRIPTION: Example showing how to use Xarray's rolling construct operation with Dask Arrays. Demonstrates the improved chunking behavior that prevents memory issues.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport xarray as xr\nimport dask.array as da\n\narr = xr.DataArray(\n    da.ones((93504, 721, 1440), chunks=(\"auto\", -1, -1)),\n    dims=[\"time\", \"lat\", \"longitude\"],\n)   # Initial chunks are ~128 MiB\narr.rolling(time=30).construct(\"window_dim\")\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Partitioning Schema for Hive-Partitioned Parquet Data in Python\nDESCRIPTION: This snippet demonstrates how to specify a custom schema for hive-partitioned columns when reading Parquet data. This allows for specifying data types other than the default 'category' type.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-hive.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> schema = pa.schema([(\"year\", pa.int16()), (\"semester\", pa.string())])\n\n>>> ddf2 = dd.read_parquet(\n...     path,\n...     columns=[\"year\", \"semester\"],\n...     dataset={\"partitioning\": {\"flavor\": \"hive\", \"schema\": schema}}\n... )\nDask DataFrame Structure:\n                year semester\nnpartitions=4                \n            int16   object\n                ...      ...\n                ...      ...\n                ...      ...\n                ...      ...\n```\n\n----------------------------------------\n\nTITLE: Updating mask and where methods in Python\nDESCRIPTION: Modifies the mask and where methods to accept a callable as an argument.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nmask and where should accept a callable\n```\n\n----------------------------------------\n\nTITLE: Basic Dask Array Operations\nDESCRIPTION: Demonstrates array slicing and mathematical operations using Dask Arrays with axis-based computations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> b = a.max(axis=1)[::-1] + 10\n... b\ndask.array<add, shape=(200,), dtype=int64, chunksize=(100,), chunktype=numpy.ndarray>\n\n>>> b[:10].compute()\narray([100009,  99509,  99009,  98509,  98009,  97509,  97009,  96509,\n      96009,  95509])\n```\n\n----------------------------------------\n\nTITLE: Using Context Manager for Shuffle Method Selection\nDESCRIPTION: Example of using a context manager to temporarily set the shuffle method for specific operations, which helps isolate configuration changes to a specific code block.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith dask.config.set({\"dataframe.shuffle.method\": \"p2p\"}):\n    ddf.groupby(...).apply(...)\n```\n\n----------------------------------------\n\nTITLE: Reading HDF Files as Series in Dask DataFrame\nDESCRIPTION: Adds support for reading Series from HDF files using read_hdf in Dask DataFrames.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_84\n\nLANGUAGE: Python\nCODE:\n```\ndask.dataframe.read_hdf('file.h5', '/data')\n```\n\n----------------------------------------\n\nTITLE: Coiled Cloud Cluster Setup\nDESCRIPTION: Example of setting up a Dask cluster using Coiled cloud service with specific worker configurations and spot instance policies.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport coiled\ncluster = coiled.Cluster(\n    n_workers=100,\n    region=\"us-east-2\",\n    worker_memory=\"16 GiB\",\n    spot_policy=\"spot_with_fallback\",\n)\nclient = cluster.get_client()\n```\n\n----------------------------------------\n\nTITLE: Implementing sizeof plugin for NumPy arrays using entry points in Python\nDESCRIPTION: This code defines a sizeof_plugin function that registers a custom sizeof implementation for NumPy arrays. It uses the sizeof.register decorator to associate the implementation with np.ndarray objects.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/extend-sizeof.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> def sizeof_plugin(sizeof):\n...    @sizeof.register(np.ndarray)\n...    def sizeof_numpy_like(array):\n...        return array.nbytes\n```\n\n----------------------------------------\n\nTITLE: Importing Experimental Dataset Specification Functions in Python\nDESCRIPTION: This snippet imports experimental functions and classes for defining dataset specifications in Dask. These are used for creating datasets with specific column types and index structures.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/api.rst#2025-04-19_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom dask.dataframe.io.demo import with_spec, ColumnSpec, RangeIndexSpec, DatetimeIndexSpec, DatasetSpec\n```\n\n----------------------------------------\n\nTITLE: Implementing idxmin and idxmax for groupby in Dask\nDESCRIPTION: Code that adds support for the idxmin and idxmax methods on grouped DataFrames and Series in Dask, which return the index of the minimum or maximum values in each group.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_51\n\nLANGUAGE: Python\nCODE:\n```\n-  Add support for DF and Series ``groupby().idxmin/max()`` (:pr:`5273`) `Oliver Hofkens`_\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dask Array Reshaping Example\nDESCRIPTION: Example showing how to reshape a Dask array with improved chunking behavior that maintains consistent chunk sizes between input and output.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\narr = da.ones(shape=(1000, 100, 48_000), chunks=(1000, 100, 83))\narr.reshape(1000, 100, 4, 12_000)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Improved Task Graph with Inlined Arrays\nDESCRIPTION: This snippet demonstrates loading Zarr arrays with the 'inline_array' option and visualizing the resulting task graph. It shows how inlining can potentially improve the ordering of tasks.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/order.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> # load and profile data\n>>> x1 = da.from_zarr('saved_x1.zarr', inline_array=True)\n>>> y1 = da.from_zarr('saved_x2.zarr', inline_array=True)\n>>> x2 = da.from_zarr('saved_y1.zarr', inline_array=True)\n>>> y2 = da.from_zarr('saved_y2.zarr', inline_array=True)\n\n>>> import dask\n>>> n = 125 * 4\n>>> dask.visualize(evaluate(x1[:n], y1[:n], x2[:n], y2[:n]),\n...                optimize_graph=True, color=\"order\",\n...                cmap=\"autumn\", node_attr={\"penwidth\": \"4\"})\n```\n\n----------------------------------------\n\nTITLE: Handling Masked Scalars in da.from_array\nDESCRIPTION: Support masked scalars input to da.from_array.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nda.from_array\n```\n\n----------------------------------------\n\nTITLE: Dask Cluster Type Switching\nDESCRIPTION: Example showing how to switch between different cluster types like LocalCluster and KubeCluster while maintaining the same interface.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# You can swap out LocalCluster for other cluster types\n\nfrom dask.distributed import LocalCluster\nfrom dask_kubernetes import KubeCluster\n\n# cluster = LocalCluster()\ncluster = KubeCluster()  # example, you can swap out for Kubernetes\n\nclient = cluster.get_client()\n```\n\n----------------------------------------\n\nTITLE: Computing Column Maximum in Dask DataFrame\nDESCRIPTION: Example showing how to read a CSV file using Dask DataFrame and compute the maximum value of a column.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/caching.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.dataframe as dd\n>>> df = dd.read_csv('myfile.csv')\n>>> df.columns\n['first-name', 'last-name', 'amount', 'id', 'timestamp']\n\n>>> df.amount.max().compute()\n1000\n```\n\n----------------------------------------\n\nTITLE: Implementing meta_nonempty Dispatch Methods in Python\nDESCRIPTION: Registration of meta_nonempty dispatch methods to create non-empty versions of custom DataFrame objects with representative data.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-extend.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.dataframe.utils import meta_nonempty\n\n@meta_nonempty.register(MyDataFrame)\ndef meta_nonempty_dataframe(df):\n    ...\n    return MyDataFrame(..., columns=df.columns,\n                         index=MyIndex(..., name=df.index.name))\n\n\n@meta_nonempty.register(MySeries)\ndef meta_nonempty_series(s):\n    ...\n\n\n@meta_nonempty.register(MyIndex)\ndef meta_nonempty_index(ind):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Launching Jupyter Lab alongside Dask scheduler in Python\nDESCRIPTION: This Python snippet demonstrates how to launch a Jupyter Lab server on the same machine as the Dask scheduler. It uses the Dask client to run commands on the scheduler and start Jupyter Lab.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-hpc.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nclient = Client(scheduler_file='scheduler.json')\n\nimport socket\nhost = client.run_on_scheduler(socket.gethostname)\n\ndef start_jlab(dask_scheduler):\n    import subprocess\n    proc = subprocess.Popen(['/path/to/jupyter', 'lab', '--ip', host, '--no-browser'])\n    dask_scheduler.jlab_proc = proc\n\nclient.run_on_scheduler(start_jlab)\n```\n\n----------------------------------------\n\nTITLE: Gathering Futures from Persisted Collections\nDESCRIPTION: Shows how to gather futures from persisted Dask collections using the futures_of function.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/user-interfaces.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import futures_of\n\ndf = df.persist()  # start computation in the background\nfutures = futures_of(df)\n```\n\n----------------------------------------\n\nTITLE: Importing Dask Utility Functions in Python\nDESCRIPTION: This code imports utility functions from Dask for parsing configuration values, formatting bytes and time, and applying functions. These are primarily used for handling and formatting data in Dask operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/api.rst#2025-04-19_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom dask.utils import apply, format_bytes, format_time, parse_bytes, parse_timedelta\n```\n\n----------------------------------------\n\nTITLE: Updating Dask Config from Arbitrary Mappings in Python\nDESCRIPTION: Allows updating Dask configuration using any Mapping object, not just dictionaries.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_58\n\nLANGUAGE: Python\nCODE:\n```\nAllow updates from arbitrary Mappings in config.update, not only dicts.\n```\n\n----------------------------------------\n\nTITLE: DataFrame Merge Operation Example - Python\nDESCRIPTION: Example demonstrating how the Query Optimizer handles multiple merge operations by avoiding unnecessary shuffles when merging on the same columns that were previously shuffled.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nresult = df.merge(df2, on=\"a\")\nresult = result.merge(df3, on=\"a\")\n```\n\n----------------------------------------\n\nTITLE: Array Iterator Implementation\nDESCRIPTION: Implementation of a lazy iterator for Dask arrays to improve memory efficiency\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_41\n\nLANGUAGE: python\nCODE:\n```\n# Implementation of lazy Array.__iter__\n# Implied change for improved array iteration\n```\n\n----------------------------------------\n\nTITLE: Fixing setup of lz4 decompression functions in Dask Core\nDESCRIPTION: Fixes the setup of lz4 decompression functions to ensure proper functionality.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_79\n\nLANGUAGE: Python\nCODE:\n```\nimport lz4\n```\n\n----------------------------------------\n\nTITLE: Zarr Storage Operations\nDESCRIPTION: Examples of saving and loading Dask arrays using Zarr format, including local and S3 storage options.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n>>> arr.to_zarr('output.zarr')\n\n>>> arr.to_zarr('s3://mybucket/output.zarr', storage_option={'key': 'mykey',\n                'secret': 'mysecret'})\n\n>>> z = zarr.create((10,), dtype=float, store=zarr.ZipStore(\"output.zarr\"))\n>>> arr.to_zarr(z)\n```\n\n----------------------------------------\n\nTITLE: Computing Column Minimum in Dask DataFrame\nDESCRIPTION: Example demonstrating how to compute the minimum value of a column, which would typically require re-reading the CSV file.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/caching.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> df.amount.min().compute()\n-1000\n```\n\n----------------------------------------\n\nTITLE: Implementing get_collection_type Dispatch in Python\nDESCRIPTION: Registration of get_collection_type dispatch methods to map non-Dask DataFrame objects to their Dask equivalents.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-extend.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.dataframe import get_collection_type\n\n@get_collection_type.register(MyDataFrame)\ndef get_collection_type_dataframe(df):\n    return MyDaskDataFrame\n\n\n@get_collection_type.register(MySeries)\ndef get_collection_type_series(s):\n    return MyDaskSeries\n\n\n@get_collection_type.register(MyIndex)\ndef get_collection_type_index(ind):\n    return MyDaskIndex\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask Scheduler for Single Compute Call in Python\nDESCRIPTION: Shows how to set the Dask scheduler for a single compute operation, overriding the global configuration.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduling.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nx.compute(scheduler='threads')\n```\n\n----------------------------------------\n\nTITLE: Ensuring to_zarr returns Dask Array\nDESCRIPTION: Ensures that to_zarr with return_stored=True returns a Dask Array instead of a Zarr Array.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_70\n\nLANGUAGE: Python\nCODE:\n```\nda.to_zarr(..., return_stored=True)\n```\n\n----------------------------------------\n\nTITLE: NumPy Array Testing Example\nDESCRIPTION: Demonstrates how to test Dask array functionality against NumPy arrays using assert_eq helper function for verification of correctness.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/develop.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport dask.array as da\nfrom dask.array.utils import assert_eq\n\ndef test_aggregations():\n    rng = np.random.default_rng()\n    nx = rng.random(100)\n    dx = da.from_array(nx, chunks=(10,))\n\n    assert_eq(nx.sum(), dx.sum())\n    assert_eq(nx.min(), dx.min())\n    assert_eq(nx.max(), dx.max())\n```\n\n----------------------------------------\n\nTITLE: Adding fuse_roots optimization to Dask blockwise module\nDESCRIPTION: Code addition that implements the fuse_roots optimization in the dask.blockwise module, which likely improves computational efficiency by combining certain operations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_44\n\nLANGUAGE: Python\nCODE:\n```\n-  Add dask.blockwise.fuse_roots optimization (:pr:`5451`) `Matthew Rocklin`_\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents in reStructuredText for Dask How-To Documentation\nDESCRIPTION: This snippet sets up a table of contents for the Dask how-to documentation using reStructuredText directives. It includes all files in the current directory and a specific link to GPU usage documentation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :caption: How To...\n   :maxdepth: 1\n   :glob:\n\n   *\n   Use GPUs <../gpu.rst>\n```\n\n----------------------------------------\n\nTITLE: Improving task ordering in Dask Core\nDESCRIPTION: Allows tasks back onto the ordering stack if they have one dependency and prefers end-tasks with low numbers of dependencies when ordering.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_77\n\nLANGUAGE: Python\nCODE:\n```\ndask.get(...)\n```\n\n----------------------------------------\n\nTITLE: Initializing SerializableLock in Python\nDESCRIPTION: Adds SerializableLock for better use with distributed scheduling in Dask arrays.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_81\n\nLANGUAGE: Python\nCODE:\n```\nSerializableLock()\n```\n\n----------------------------------------\n\nTITLE: Checking working directory consistency in Dask distributed setup\nDESCRIPTION: This snippet shows how to check if the current working directory is consistent across the Dask client and workers in a distributed setup using a LocalCluster.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/connect-to-remote-data.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> import os\n>>> from dask.distributed import Client, LocalCluster\n>>> client = Client(LocalCluster())\n>>> client.run(os.getcwd)  # doctest: +SKIP\n{'tcp://127.0.0.1:64597': '/home/coder',\n 'tcp://127.0.0.1:64598': '/home/coder'}\n```\n\n----------------------------------------\n\nTITLE: Efficient Handling of Large Inputs with Delayed in Python\nDESCRIPTION: Demonstrates how to efficiently handle large inputs in delayed calls by delaying the data once, avoiding repeated hashing of large objects.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/delayed-best-practices.rst#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nx = np.array(...)    # some large array\nx = dask.delayed(x)  # delay the data once\nresults = [dask.delayed(train)(x, i)\n           for i in range(1000)]\n```\n\n----------------------------------------\n\nTITLE: Installing S3FS for AWS Data Access\nDESCRIPTION: Demonstrates the pip command to install s3fs, which enables easy access to data stored in Amazon's S3 storage from Dask.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-cloud.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install s3fs\n```\n\n----------------------------------------\n\nTITLE: Dask Scheduler Configuration Examples\nDESCRIPTION: Shows various ways to configure Dask schedulers, including setting the scheduler type and worker count, using both direct parameters and configuration contexts.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduler-overview.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> x.sum().compute(scheduler='processes')\n\n>>> with dask.config.set(scheduler='processes'):\n...     x.sum().compute()\n\n>>> dask.config.set(scheduler='processes')\n>>> x.sum().compute()\n```\n\n----------------------------------------\n\nTITLE: TileDB Storage Integration\nDESCRIPTION: Examples of storing Dask arrays in TileDB format with local and S3 storage options.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-creation.rst#2025-04-19_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n>>> arr.to_tiledb('output.tdb')\n\n>>> arr.to_tiledb('s3://mybucket/output.tdb',\n                 storage_options={'vfs.s3.aws_access_key_id': 'mykey',\n                                  'vfs.s3.aws_secret_access_key': 'mysecret'})\n```\n\n----------------------------------------\n\nTITLE: Importing Dask Array Module in Python\nDESCRIPTION: This snippet shows how to import the dask.array module, which is the focus of this API documentation.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-api.rst#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom dask import array\n```\n\n----------------------------------------\n\nTITLE: Array Operations with overlap\nDESCRIPTION: Updates to array operations including better handling of overlapped dimensions in map_overlap and gradient calculations with coordinates.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_60\n\nLANGUAGE: Python\nCODE:\n```\nda.map_overlap()  # Improved handling of non-overlapped dimensions\nda.gradient(coordinate=True)  # Added coordinate support\nda.argtopk()  # Reimplemented to release GIL\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for Dask Documentation\nDESCRIPTION: Commands to create and activate a conda environment for building Dask documentation using Python 3.8.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/develop.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n daskdocs -c conda-forge python=3.8\nconda activate daskdocs\n```\n\n----------------------------------------\n\nTITLE: Adding block_info keyword to map_blocks in Dask Array\nDESCRIPTION: Adds a block_info keyword argument to the map_blocks function in Dask Array.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_62\n\nLANGUAGE: Python\nCODE:\n```\nmap_blocks(..., block_info=True)\n```\n\n----------------------------------------\n\nTITLE: Installing GCSFS for Google Cloud Storage Access\nDESCRIPTION: Shows the pip command to install gcsfs, which enables easy access to data stored in Google Cloud Storage from Dask.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-cloud.rst#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install gcsfs\n```\n\n----------------------------------------\n\nTITLE: Retrieving Dask Dashboard URL in Python\nDESCRIPTION: This snippet shows how to retrieve the URL for the Dask dashboard from a cluster manager instance. The dashboard provides diagnostic information about the cluster.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> cluster.dashboard_link\n'http://127.0.0.1:8787/status'\n```\n\n----------------------------------------\n\nTITLE: Configuring DataFrame Parquet Partition Size - Python\nDESCRIPTION: Example showing how to configure the minimum partition size threshold for Parquet files using Dask config system. The value is specified in bytes with a default conservative threshold to avoid memory issues.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndask.config.set({\"dataframe.parquet.minimum-partition-size\": 100_000_000})\n```\n\n----------------------------------------\n\nTITLE: Installing ADLFS for Azure Storage Access\nDESCRIPTION: Provides the pip command to install adlfs, which enables easy access to data stored in Microsoft's Data Lake or Blob Storage from Dask.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-cloud.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install adlfs\n```\n\n----------------------------------------\n\nTITLE: Advanced Dask Logging Configuration in YAML\nDESCRIPTION: Demonstrates a more advanced logging configuration for Dask, setting up both file and console handlers for the scheduler and workers with specific logging levels.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/debug.rst#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nlogging:\n  version: 1\n  handlers:\n    file:\n      class: logging.handlers.RotatingFileHandler\n      filename: output.log\n      level: INFO\n    console:\n      class: logging.StreamHandler\n      level: INFO\n  loggers:\n    distributed.worker:\n      level: INFO\n      handlers:\n        - file\n        - console\n    distributed.scheduler:\n      level: INFO\n      handlers:\n        - file\n        - console\n```\n\n----------------------------------------\n\nTITLE: Installing Dask from source\nDESCRIPTION: Commands for installing Dask from source code on GitHub. Includes options for installing dependencies and performing a developer install.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/install.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/dask/dask.git\ncd dask\npython -m pip install .\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install \".[complete]\"\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Importing Configuration in __init__.py\nDESCRIPTION: Shows how to ensure configuration is loaded on package import by including it in __init__.py\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/configuration.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# dask_foo/__init__.py\n\nfrom . import config\n```\n\n----------------------------------------\n\nTITLE: Implementing explode for Series and DataFrame in Dask\nDESCRIPTION: Code that adds the explode method to both Series and DataFrame in Dask, which transforms lists of items into individual rows, similar to pandas' explode functionality.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_48\n\nLANGUAGE: Python\nCODE:\n```\n-  Implement explode for Series and DataFrame (:pr:`5381`) `Arpit Solanki`_\n```\n\n----------------------------------------\n\nTITLE: Slicing Dask Array with integer array\nDESCRIPTION: Enables slicing a Dask Array using another Dask Array of integers.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_63\n\nLANGUAGE: Python\nCODE:\n```\ndask_array[dask_int_array]\n```\n\n----------------------------------------\n\nTITLE: Listing Dask Project Dependencies in plaintext\nDESCRIPTION: This snippet enumerates the required Python packages for the Dask project, including specific version constraints where necessary. It covers documentation tools, core dependencies, and testing utilities.\nSOURCE: https://github.com/dask/dask/blob/main/docs/requirements-docs.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnumpydoc\nsphinx>=4.0.0\ndask-sphinx-theme>=3.0.0\nsphinx-click\nsphinx-copybutton\nsphinx-remove-toctrees\nsphinx_autosummary_accessors\nsphinx-tabs\nsphinx-design\njupyter_sphinx\n# FIXME: `sphinxcontrib-*` pins are a workaround until we have sphinx>=5.\n#        See https://github.com/dask/dask-sphinx-theme/issues/68.\nsphinxcontrib-applehelp>=1.0.0,<1.0.7\nsphinxcontrib-devhelp>=1.0.0,<1.0.6\nsphinxcontrib-htmlhelp>=2.0.0,<2.0.5\nsphinxcontrib-serializinghtml>=1.1.0,<1.1.10\nsphinxcontrib-qthelp>=1.0.0,<1.0.7\ntoolz\ncloudpickle>=1.5.0\npandas>=2.0.0\ngit+https://github.com/dask/distributed\nfsspec\nscipy\npyarrow\npytest\npytest-check-links\nrequests-cache\nipython\nipykernel<6.22.0\n```\n\n----------------------------------------\n\nTITLE: Implementing visualize Function for Dask Collections in Python\nDESCRIPTION: Pseudocode implementation of the visualize function for Dask collections. It merges and optimizes the graph, then draws the resulting graph using graphviz.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/custom-collections.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef visualize(*collections, **kwargs):\n    # 1. Graph Merging & Optimization\n    # -------------------------------\n    # **Same as in compute**\n    graph = ...\n\n    # 2. Graph Drawing\n    # ----------------\n    # Draw the graph with graphviz's `dot` tool and return the result.\n    return dot_graph(graph, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Viewing Unknown DataFrame Divisions\nDESCRIPTION: Shows the divisions structure when partition boundaries are unknown, such as when reading from CSV files.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-design.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> df.divisions\n[None, None, None, None, None]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Basic Dask Array Operations\nDESCRIPTION: Shows how to create a simple Dask array operation and visualize its task graph using the default visualization method.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/graphviz.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\nx = da.ones((15, 15), chunks=(5, 5))\n\ny = x + x.T\n\n# visualize the low level Dask graph\ny.visualize(filename='transpose.svg')\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Dask Ecosystem Documentation\nDESCRIPTION: ReStructuredText formatted documentation describing Dask's ecosystem components including Array, DataFrame, SQL, Machine Learning integrations and deployment options. Includes links to related projects and deployment platforms.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/ecosystem.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:orphan:\n\n.. this page is referenced from the topbar which comes from the theme\n\nEcosystem\n=========\n\nThere are a number of open source projects that extend the Dask interface and provide different\nmechanisms for deploying Dask clusters. This is likely an incomplete list so if you spot something\nmissing - please `suggest a fix <https://github.com/dask/dask/edit/main/docs/source/ecosystem.rst>`_!\n```\n\n----------------------------------------\n\nTITLE: Registering CLI Entry Point in setup.py\nDESCRIPTION: Python setup.py configuration showing how to register a Dask CLI entry point using setuptools.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/cli.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom setuptools import setup\n\nsetup(\n    ...\n    entry_points=\"\"\"\n        [dask_cli]\n        mycommand=mypackage.cli:main\n    \"\"\",\n```\n\n----------------------------------------\n\nTITLE: Implementing Sum of Squares Aggregation with NumPy\nDESCRIPTION: Example of implementing a sum of squares aggregation using NumPy functions within a Dask Aggregation. This demonstrates how to leverage NumPy for computation within custom aggregations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> dd.Aggregation(name=\"sum_of_squares\", chunk=lambda s: s.apply(lambda r: np.sum(np.power(r, 2))), agg=lambda s: s.sum())\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask Scheduler with Custom Executor in Python\nDESCRIPTION: Shows how to use a custom concurrent.futures.Executor subclass with Dask, such as the ReusablePoolExecutor from loky.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduling.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom loky import get_reusable_executor\nwith dask.config.set(scheduler=get_reusable_executor()):\n    x.compute()\n```\n\n----------------------------------------\n\nTITLE: Updating chunks in Dask Array __setitem__\nDESCRIPTION: Updates the chunks attribute when setting items in a Dask Array to maintain consistency.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_67\n\nLANGUAGE: Python\nCODE:\n```\ndask_array[index] = value\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Project Badges Configuration\nDESCRIPTION: ReStructuredText markup for configuring project status badges including build status, test coverage, documentation status, discourse community, version and NumFOCUS sponsorship.\nSOURCE: https://github.com/dask/dask/blob/main/README.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _documentation: https://dask.org\n.. |Build Status| image:: https://github.com/dask/dask/actions/workflows/tests.yml/badge.svg\n   :target: https://github.com/dask/dask/actions/workflows/tests.yml\n.. |Coverage| image:: https://codecov.io/gh/dask/dask/branch/main/graph/badge.svg\n   :target: https://codecov.io/gh/dask/dask/branch/main\n   :alt: Coverage status\n.. |Doc Status| image:: https://readthedocs.org/projects/dask/badge/?version=latest\n   :target: https://dask.org\n   :alt: Documentation Status\n.. |Discourse| image:: https://img.shields.io/discourse/users?logo=discourse&server=https%3A%2F%2Fdask.discourse.group\n   :alt: Discuss Dask-related things and ask for help\n   :target: https://dask.discourse.group\n.. |Version Status| image:: https://img.shields.io/pypi/v/dask.svg\n   :target: https://pypi.python.org/pypi/dask/\n.. |NumFOCUS| image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A\n   :target: https://www.numfocus.org/\n```\n\n----------------------------------------\n\nTITLE: Configuring Array Rechunk Method in Python\nDESCRIPTION: Sets the configuration for the array rechunk method to either 'tasks' or 'p2p' using the Dask config system.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dask.config\n# Choose either \"tasks\" or \"p2p\"\ndask.config.set({\"array.rechunk.method\": \"tasks\"})\n```\n\n----------------------------------------\n\nTITLE: Creating HTML List for Dask Progress Bar Component Legend\nDESCRIPTION: HTML code defining a styled list that serves as a legend for the different components of Dask progress bars. The list includes color-coded indicators for completed tasks (released from memory), completed tasks in memory, ready tasks, queued tasks, and restricted tasks with explanatory text for each.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dashboard.rst#2025-04-19_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<ul style=\"list-style-type: none\">\n        <li>\n            <span role=\"img\" aria-label=\"light teal square\" style=\"background:rgba(30,151,138, 0.6); width: 0.6em; height: 0.6em; border: 1px solid rgba(30,151,138, 0.6); display: inline-block\"></span>\n            <span>Tasks that have completed, are not needed anymore, and now have been released from memory.</span>\n        </li>\n        <li>\n            <span role=\"img\" aria-label=\"teal square\" style=\"background:rgba(30,151,138, 1); width: 0.6em; height: 0.6em; border: 1px solid rgba(30,151,138, 1); display: inline-block\"></span>\n            <span> Tasks that have completed and are in memory.</span>\n        </li>\n        <li>\n            <span role=\"img\" aria-label=\"light grey square\" style=\"background:rgba(128,128,128, 0.4); width: 0.6em; height: 0.6em; border: 1px solid rgba(128,128,128, 0.4); display: inline-block\"></span>\n            <span>Tasks that are ready to run.</span>\n        </li>\n        <li>\n            <span role=\"img\" aria-label=\"hashed light grey square\" style=\"background-image: linear-gradient(135deg, rgba(128,128,128, 0.4) 25%, #ffffff 25%, #ffffff 50%, rgba(128,128,128, 0.4) 50%, rgba(128,128,128, 0.4) 75%, #ffffff 75%, #ffffff 100%); width: 0.6em; height: 0.6em; border: 1px solid rgba(128,128,128, 0.4); display: inline-block\"></span>\n            <span>Tasks that are <a href=\"https://distributed.dask.org/en/stable/scheduling-policies.html#queuing\">queued</a>. They are ready to run, but not assigned to workers yet, so higher-priority tasks can run first.</span>\n        </li>\n        <li>\n            <span role=\"img\" aria-label=\"hashed red square\" style=\"background-image: linear-gradient(135deg, rgba(255,0,0, 0.35) 20%, rgba(0,0,0, 0.35) 25%, rgba(0,0,0, 0.35) 50%, rgba(255,0,0, 0.35) 50%, rgba(255,0,0, 0.35) 75%, rgba(0,0,0, 0.35) 75%, rgba(0,0,0, 0.35) 100%); width: 0.6em; height: 0.6em; border: 1px solid rgba(128,128,128, 0.4); display: inline-block\"></span>\n            <span>Tasks that do not have a worker to run on due to <a href=\"https://distributed.dask.org/en/stable/locality.html#user-control\">restrictions</a> or limited <a href=\"https://distributed.dask.org/en/stable/resources.html\">resources</a>.</span>\n        </li>\n    </ul>\n```\n\n----------------------------------------\n\nTITLE: Comparing P2P Array Rechunking Performance in Python\nDESCRIPTION: Example code demonstrating how to use zero-copy P2P array rechunking in Dask. The code initializes a large random array and rechunks it using the P2P method with disk storage enabled, which provides significant performance improvements compared to other methods.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nshape = (30_000, 6_000, 150) # 201.17 GiB\ninput_chunks = (60, -1, -1) # 411.99 MiB\noutput_chunks = (-1, 6, -1) # 205.99 MiB\n\narr = da.random.random(size, chunks=input_chunks)\nwith dask.config.set({\n    \"array.rechunk.method\": \"p2p\",\n    \"distributed.p2p.disk\": True,\n}):\n    (\n      da.random.random(size, chunks=input_chunks)\n      .rechunk(output_chunks)\n      .sum()\n      .compute()\n    )\n```\n\n----------------------------------------\n\nTITLE: Xarray GroupBy-Reduce Pattern Example\nDESCRIPTION: Demonstrates the Xarray GroupBy-Reduce pattern with improved scheduling efficiency using time-based chunking and monthly grouping.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport xarray as xr\n\narr = xr.open_zarr(...)\narr.chunk(time=TimeResampler(\"ME\")).groupby(\"time.month\").mean()\n```\n\n----------------------------------------\n\nTITLE: Adding last_endline parameter to Dask Bag to_textfiles\nDESCRIPTION: Adds an optional last_endline parameter to the to_textfiles method in Dask Bag.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_71\n\nLANGUAGE: Python\nCODE:\n```\ndask_bag.to_textfiles(..., last_endline=False)\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in reStructuredText for Dask DataFrame Documentation\nDESCRIPTION: This snippet defines a table of contents using reStructuredText directives. It sets the maximum depth to 1 and lists various topics related to Dask DataFrames, including file formats, performance considerations, and extension capabilities.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-extra.rst#2025-04-19_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   Parquet <dataframe-parquet.rst>\n   Indexing <dataframe-indexing.rst>\n   SQL <dataframe-sql.rst>\n   Join Performance <dataframe-joins.rst>\n   Shuffling Performance <dataframe-groupby.rst>\n   dataframe-categoricals.rst\n   Extend <dataframe-extend.rst>\n   Hive Partitioning <dataframe-hive.rst>\n```\n\n----------------------------------------\n\nTITLE: Applying UFuncs from Dask Array to Dask DataFrame\nDESCRIPTION: Enables the use of Dask Array ufuncs on Dask DataFrame objects.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_88\n\nLANGUAGE: Python\nCODE:\n```\nimport dask.array as da\nda.exp(df)\n```\n\n----------------------------------------\n\nTITLE: Fixing argtopk with uneven chunks in Dask Array\nDESCRIPTION: Fixes an issue with the argtopk function when working with Dask Arrays that have uneven chunk sizes.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_65\n\nLANGUAGE: Python\nCODE:\n```\nda.argtopk(x, k)\n```\n\n----------------------------------------\n\nTITLE: Git Commit Command for Version Update\nDESCRIPTION: Command to commit version changes with a standardized commit message that includes the version number following the YYYY.M.X format.\nSOURCE: https://github.com/dask/dask/blob/main/docs/release-procedure.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit commit -a -m \"Version YYYY.M.X\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask DataFrame Query Planning\nDESCRIPTION: Code example showing how to opt-out of the new query planning functionality by configuring Dask settings.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask\n>>> dask.config.set({'dataframe.query-planning': False})\n```\n\n----------------------------------------\n\nTITLE: Setting Default Visualization Engine\nDESCRIPTION: Demonstrates how to configure the default visualization engine using Dask configuration settings.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/graphviz.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\nx = da.ones((15, 15), chunks=(5, 5))\n\ny = x + x.T\n\nwith dask.config.set({\"visualization.engine\": \"cytoscape\"}):\n    y.visualize()\n```\n\n----------------------------------------\n\nTITLE: Handling Large Chunks in Dask Array Slicing\nDESCRIPTION: Example showing how repeated index selection can produce large chunks and how to configure Dask's behavior for handling such cases.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-slicing.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> a[[0] * 15, :, :]\nPerformanceWarning: Slicing is producing a large chunk. To accept the large\nchunk and silence this warning, set the option\n    >>> with dask.config.set({'array.slicing.split_large_chunks': False}):\n    ...     array[indexer]\n\nTo avoid creating the large chunks, set the option\n    >>> with dask.config.set({'array.slicing.split_large_chunks': True}):\n    ...     array[indexer]\ndask.array<getitem, shape=(15, 10000, 10000), dtype=float64, chunksize=(15, 10000, 10000), chunktype=numpy.ndarray>\n```\n\n----------------------------------------\n\nTITLE: Converting Dask Bags to Other Formats\nDESCRIPTION: Shows methods to convert Dask bags to Python lists, compute results, and store in memory.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/bag-creation.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> result = b.compute()\nor\n>>> result = list(b)\n```\n\n----------------------------------------\n\nTITLE: Handling dtype=None in Dask Array full Function\nDESCRIPTION: Fix to handle dtype=None correctly in da.full function.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nda.full\n```\n\n----------------------------------------\n\nTITLE: Git Tag Command for Version Release\nDESCRIPTION: Command to create an annotated git tag for the new version using the YYYY.M.X version format.\nSOURCE: https://github.com/dask/dask/blob/main/docs/release-procedure.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit tag -a YYYY.M.X -m 'Version YYYY.M.X'\n```\n\n----------------------------------------\n\nTITLE: Using Custom Dask CLI Command\nDESCRIPTION: Shell commands demonstrating how to use the custom mycommand CLI command with different arguments and options.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/cli.rst#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ dask mycommand world\nhello world from mycommand!\n\n$ dask mycommand user -c 3\nhello user from mycommand!\nhello user from mycommand!\nhello user from mycommand!\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Ports for Dask Components\nDESCRIPTION: Commands demonstrating how to configure specific ports for the scheduler and worker components, which is useful when working with firewalls or networking constraints.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-cli.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndask scheduler --port 8000\ndask worker --dashboard-address 8000 --nanny-port 8001\n```\n\n----------------------------------------\n\nTITLE: Reading Fixed-Width Formatted Files with Dask DataFrame in Python\nDESCRIPTION: Adds support for reading fixed-width formatted files using dask.dataframe.read_fwf function.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_57\n\nLANGUAGE: Python\nCODE:\n```\nAdd dask.dataframe.read_fwf\n```\n\n----------------------------------------\n\nTITLE: Cloning and Updating Conda-Forge Feedstock\nDESCRIPTION: Commands for manually updating the conda-forge feedstock, including cloning the repository, installing conda-smithy, and running the rerender process.\nSOURCE: https://github.com/dask/dask/blob/main/docs/release-procedure.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:conda-forge/dask-core-feedstock\ncd dask-core-feedstock\nconda install conda-smithy\nconda-smithy rerender\n```\n\n----------------------------------------\n\nTITLE: Configuring Legacy DataFrame Implementation (Python)\nDESCRIPTION: A configuration snippet showing how to set the deprecated query-planning parameter to false. This was mentioned in the release notes as the legacy Dask DataFrame implementation has been removed.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndask.config.set({\"dataframe.query-planning\": False})\n```\n\n----------------------------------------\n\nTITLE: Initializing Dask DataFrame from Pandas in Python\nDESCRIPTION: Creates a Pandas DataFrame and converts it to a Dask DataFrame with 2 partitions. This setup is used for subsequent indexing examples.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-indexing.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.dataframe as dd\n>>> import pandas as pd\n>>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [3, 4, 5]},\n...                   index=['a', 'b', 'c'])\n>>> ddf = dd.from_pandas(df, npartitions=2)\n>>> ddf\nDask DataFrame Structure:\n                     A      B\nnpartitions=1\na              int64  int64\nc                ...    ...\nDask Name: from_pandas, 1 tasks\n```\n\n----------------------------------------\n\nTITLE: Using Dask Collection Compute Methods\nDESCRIPTION: Shows how to use the compute method with Dask collections for calculating array operations. Demonstrates basic array computation with the default scheduler.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduler-overview.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.array as da\n>>> x = da.arange(100, chunks=10)\n>>> x.sum().compute()\n4950\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Make\nDESCRIPTION: Command to build the HTML documentation using make, which generates files in the build/html directory.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/develop.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Renaming Columns in SeriesGroupby\nDESCRIPTION: Fixes column renaming when working with SeriesGroupby objects. Ensures columns are renamed correctly in groupby operations on Series.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n\"Rename columns correctly in case of SeriesGroupby\"\n```\n\n----------------------------------------\n\nTITLE: Fixing extra progressbar in Dask Core\nDESCRIPTION: Fixes an issue where an extra progress bar was being displayed in certain situations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_76\n\nLANGUAGE: Python\nCODE:\n```\nwith ProgressBar():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing dask-expr Package via pip\nDESCRIPTION: Command to install the dask-expr package, which is required for using the new logical query planning feature in Dask DataFrames.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npip install dask-expr\n```\n\n----------------------------------------\n\nTITLE: Legacy Random Number Generation in Python using Dask\nDESCRIPTION: Shows the legacy approach for random number generation using Dask's direct methods. This approach is discouraged as it uses global state through RandomState.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/array-random.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# instead of this (legacy version)\nimport dask.array as da\nvals = da.random.standard_normal(10)\nmore_vals = da.random.standard_normal(10)\n```\n\n----------------------------------------\n\nTITLE: Updating DataFrame.nsmallest and DataFrame.nlargest in Python\nDESCRIPTION: Raises a TypeError when the 'columns' parameter is not provided to DataFrame.nsmallest and DataFrame.nlargest methods.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nRaise TypeError in DataFrame.nsmallest and DataFrame.nlargest when columns is not given\n```\n\n----------------------------------------\n\nTITLE: Checking Package Availability on Conda-Forge\nDESCRIPTION: Command to verify if a specific version of dask-core is available on the conda-forge channel, which is necessary before restarting CI for dependent packages.\nSOURCE: https://github.com/dask/dask/blob/main/docs/release-procedure.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nconda search 'conda-forge::dask-core=YYYY.M.X'\n```\n\n----------------------------------------\n\nTITLE: Direct Column Computation with Pandas\nDESCRIPTION: Example showing how to compute an entire column into memory and perform operations using Pandas directly.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/caching.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> amount = df.amount.compute()\n>>> amount.max()\n1000\n>>> amount.min()\n-1000\n```\n\n----------------------------------------\n\nTITLE: NumPy/Pandas Version Requirements\nDESCRIPTION: Updated version requirements specifying NumPy 1.18+ and Pandas 1.0+ as minimum supported versions\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n# Version requirements\nNumPy 1.18+\nPandas 1.0+\n```\n\n----------------------------------------\n\nTITLE: Cloud Deployment with Coiled\nDESCRIPTION: Demonstrates deploying Dask clusters on cloud platforms using Coiled.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/index.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport coiled\ncluster = coiled.Cluster(\n   n_workers=100,\n   region=\"us-east-2\",\n   worker_memory=\"16 GiB\",\n   spot_policy=\"spot_with_fallback\",\n)\nclient = cluster.get_client()\n```\n\n----------------------------------------\n\nTITLE: Basic Python Test Function Example\nDESCRIPTION: Example showing how to write tests in Dask using pytest style with bare functions, including assertion checks and exception testing.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/develop.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef test_fibonacci():\n    assert fib(0) == 0\n    assert fib(1) == 0\n    assert fib(10) == 55\n    assert fib(8) == fib(7) + fib(6)\n\n    for x in [-3, 'cat', 1.5]:\n        with pytest.raises(ValueError):\n            fib(x)\n```\n\n----------------------------------------\n\nTITLE: Fixing Reduction of Zero Dimensional Arrays\nDESCRIPTION: Fix bug for reduction of zero dimensional arrays.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nnp.linspace\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Dask Internals Documentation in reStructuredText\nDESCRIPTION: This snippet defines a table of contents for the Dask internals documentation using reStructuredText syntax. It specifies a maximum depth of 1 and lists the relevant documentation files to be included.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/internals.rst#2025-04-19_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   user-interfaces.rst\n   understanding-performance.rst\n   phases-of-computation.rst\n   order.rst\n   caching.rst\n   shared.rst\n   scheduling-policy.rst\n```\n\n----------------------------------------\n\nTITLE: Creating Dask Objects from Delayed or Future Objects\nDESCRIPTION: Shows how to create high-level Dask objects from either Delayed or Future objects using the from_delayed() method.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/user-interfaces.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = dd.from_delayed(delayeds)\ndf = dd.from_delayed(futures)\n```\n\n----------------------------------------\n\nTITLE: Silencing DataFrame Deprecation Warning\nDESCRIPTION: Examples of how to silence the deprecation warning for the DataFrame query planning changes using both Python and CLI methods.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# via Python\n>>> dask.config.set({'dataframe.query-planning-warning': False})\n\n# via CLI\ndask config set dataframe.query-planning-warning False\n```\n\n----------------------------------------\n\nTITLE: Incorrect Dask Client Initialization - Python\nDESCRIPTION: Example showing incorrect way to initialize a Dask client that will raise an error due to subprocess creation issues.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/scheduling.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# myscript.py\n\nfrom dask.distributed import Client\nclient = Client()  # Will raise an error when creating local subprocesses\n```\n\n----------------------------------------\n\nTITLE: Fixing Categorical to pa.dictionary conversion in Python\nDESCRIPTION: Resolves an issue with converting from Categorical to pa.dictionary when using read_parquet.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nFix conversion from Categorical to pa.dictionary in read_parquet\n```\n\n----------------------------------------\n\nTITLE: Connecting Dask Workers to a Scheduler\nDESCRIPTION: Commands to launch multiple Dask worker processes that connect to a running scheduler. Each worker displays its own TCP address and confirms registration with the scheduler.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-cli.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ dask worker tcp://192.0.0.100:8786\nStart worker at:  tcp://192.0.0.1:12345\nRegistered to:    tcp://192.0.0.100:8786\n\n$ dask worker tcp://192.0.0.100:8786\nStart worker at:  tcp://192.0.0.2:40483\nRegistered to:    tcp://192.0.0.100:8786\n\n$ dask worker tcp://192.0.0.100:8786\nStart worker at:  tcp://192.0.0.3:27372\nRegistered to:    tcp://192.0.0.100:8786\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Dask Components in Single Event Loop\nDESCRIPTION: Example demonstrating how to run multiple Dask components (Scheduler and Worker) in the same event loop.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-python-advanced.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dask.distributed import Scheduler, Worker\n\nasync def f():\n    s = await Scheduler()\n    w = await Worker(s.address)\n    await w.finished()\n    await s.finished()\n\nasyncio.get_event_loop().run_until_complete(f())\n```\n\n----------------------------------------\n\nTITLE: Using PipInstall Worker Plugin\nDESCRIPTION: Reference to the PipInstall plugin class from distributed.diagnostics.plugin for installing packages on workers at runtime.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/software-environments.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndistributed.diagnostics.plugin.PipInstall\n```\n\n----------------------------------------\n\nTITLE: Dask Dashboard Access\nDESCRIPTION: Shows how to access Dask's diagnostic dashboard for monitoring computations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/10-minutes-to-dask.rst#2025-04-19_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n>>> client.dashboard_link\n'http://127.0.0.1:8787/status'\n```\n\n----------------------------------------\n\nTITLE: Example Git Commit Messages\nDESCRIPTION: Demonstrates an example of non-descriptive git commit messages that should be avoided when squash merging pull requests. Shows why maintainers should write more meaningful commit messages.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/maintainers.rst#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n* Fix DataFrame.head bug\n\n* Handle merge conflicts\n\n* Oops, fix typo\n```\n\n----------------------------------------\n\nTITLE: Updating Docker image repository in Dask\nDESCRIPTION: Updates Docker images to use ghcr.io instead of the previous repository.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_36\n\nLANGUAGE: Markdown\nCODE:\n```\n- Update Docker images to use ghcr.io (:pr:`8774`) `Jacob Tomlinson`_\n```\n\n----------------------------------------\n\nTITLE: Word Count Graph Construction in Python\nDESCRIPTION: Example showing construction of a Dask task graph for counting word occurrences, formatting results, and printing output.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/optimize.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef print_and_return(string):\n    print(string)\n    return string\n\ndef format_str(count, val, nwords):\n    return (f'word list has {count} occurrences of '\n            f'{val}, out of {nwords} words')\n\ndsk = {'words': 'apple orange apple pear orange pear pear',\n       'nwords': (len, (str.split, 'words')),\n       'val1': 'orange',\n       'val2': 'apple',\n       'val3': 'pear',\n       'count1': (str.count, 'words', 'val1'),\n       'count2': (str.count, 'words', 'val2'),\n       'count3': (str.count, 'words', 'val3'),\n       'format1': (format_str, 'count1', 'val1', 'nwords'),\n       'format2': (format_str, 'count2', 'val2', 'nwords'),\n       'format3': (format_str, 'count3', 'val3', 'nwords'),\n       'print1': (print_and_return, 'format1'),\n       'print2': (print_and_return, 'format2'),\n       'print3': (print_and_return, 'format3')}\n```\n\n----------------------------------------\n\nTITLE: Adding Missing NumPy ufuncs\nDESCRIPTION: Add missing NumPy ufuncs: abs, left_shift, right_shift, positive.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nabs, left_shift, right_shift, positive\n```\n\n----------------------------------------\n\nTITLE: Configuring Backend Entrypoint in setup.cfg\nDESCRIPTION: Setup configuration for exposing a custom backend implementation through the dask.dataframe.backends entrypoint system.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/selecting-the-collection-backend.rst#2025-04-19_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[options.entry_points]\ndask.dataframe.backends =\n   cudf = <module-path>:CudfBackendEntrypoint\n```\n\n----------------------------------------\n\nTITLE: Running Specific Dask Module Tests\nDESCRIPTION: Command to run tests for specific Dask modules (core and array) using pytest.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/install.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npy.test dask/tests dask/array/tests\n```\n\n----------------------------------------\n\nTITLE: Rewrite Rules Implementation in Python\nDESCRIPTION: Example demonstrating pattern matching and term rewriting using RewriteRule and RuleSet for mathematical transformations.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/optimize.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.rewrite import RewriteRule, RuleSet\nfrom operator import add, mul, pow\n\nvariables = ('a',)\n\nrule1 = RewriteRule((add, 'a', 'a'), (mul, 'a', 2), variables)\n\nrule2 = RewriteRule((mul, 'a', 'a'), (pow, 'a', 2), variables)\n\nrs = RuleSet(rule1, rule2)\n```\n\n----------------------------------------\n\nTITLE: Calling NumPy dtype on Delayed Object in Python\nDESCRIPTION: Enables calling np.dtype on a delayed object to determine the data type.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_56\n\nLANGUAGE: Python\nCODE:\n```\nCalling np.dtype on a delayed object works\n```\n\n----------------------------------------\n\nTITLE: Function Documentation Example\nDESCRIPTION: Example showing proper docstring formatting following the numpydoc standard, including sections for Parameters, Examples, and doctest usage.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/develop.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef fib(i):\n    \"\"\" A single line with a brief explanation\n\n    A more thorough description of the function, consisting of multiple\n    lines or paragraphs.\n\n    Parameters\n    ----------\n    i: int\n         A short description of the argument if not immediately clear\n\n    Examples\n    --------\n    >>> fib(4)\n    3\n    >>> fib(5)\n    5\n    >>> fib(6)\n    8\n    >>> fib(-1)  # Robust to bad inputs\n    ValueError(...)\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Reducing pytest parallelism in GPU CI for Dask\nDESCRIPTION: Reduces the parallelism of pytest in GPU continuous integration testing.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_37\n\nLANGUAGE: Markdown\nCODE:\n```\n- Reduce gpuci ``pytest`` parallelism (:pr:`8826`) `GALI PREM SAGAR`_\n```\n\n----------------------------------------\n\nTITLE: Configuring entry points for dask.sizeof in setup.cfg\nDESCRIPTION: This snippet shows the configuration in setup.cfg to define an entry point for a custom sizeof implementation. It specifies the entry point group 'dask.sizeof' and maps 'numpy' to the sizeof_plugin function in the numpy_sizeof_dask module.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/how-to/extend-sizeof.rst#2025-04-19_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[options.entry_points]\ndask.sizeof = \n   numpy = numpy_sizeof_dask:sizeof_plugin\n```\n\n----------------------------------------\n\nTITLE: Running Basic Dask Tests with pytest\nDESCRIPTION: Basic command to run all Dask tests using pytest from the dask directory.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/install.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd dask\\npy.test dask\n```\n\n----------------------------------------\n\nTITLE: Installing Complete Dask Package\nDESCRIPTION: Command to install Dask with all optional dependencies using pip.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/install.rst#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install \"dask[complete]\"\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for Dask Documentation\nDESCRIPTION: Commands to create and activate a conda environment for building Dask documentation using Python 3.11.\nSOURCE: https://github.com/dask/dask/blob/main/docs/README.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n daskdocs -c conda-forge python=3.11\nconda activate daskdocs\n```\n\n----------------------------------------\n\nTITLE: Adding datasets module to Dask Core\nDESCRIPTION: Adds a new datasets module to Dask Core for easier access to sample datasets.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_80\n\nLANGUAGE: Python\nCODE:\n```\nimport dask.datasets\n```\n\n----------------------------------------\n\nTITLE: Using packaging.parse for md5 compatibility in Dask\nDESCRIPTION: Updates code to use packaging.parse for md5 compatibility.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_38\n\nLANGUAGE: Markdown\nCODE:\n```\n- Use ``packaging.parse`` for ``md5`` compatibility (:pr:`8763`) `James Bourbeau`_\n```\n\n----------------------------------------\n\nTITLE: Uploading Files to Workers\nDESCRIPTION: Method for uploading Python source files directly to Dask workers during development using the client.upload_file function.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/software-environments.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient.upload_file\n```\n\n----------------------------------------\n\nTITLE: Implementing Calendar Timezone Script in HTML/JavaScript\nDESCRIPTION: HTML and JavaScript code that embeds a Google Calendar iframe and updates its timezone to match the user's local timezone settings. The script modifies the calendar iframe's source URL to include the correct timezone parameter.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/support.rst#2025-04-19_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<iframe id=\"calendariframe\" src=\"https://calendar.google.com/calendar/embed?ctz=local&amp;src=4l0vts0c1cgdbq5jhcogj55sfs%40group.calendar.google.com\" style=\"border: 0\" width=\"800\" height=\"600\" frameborder=\"0\" scrolling=\"no\"></iframe>\n<script>document.getElementById(\"calendariframe\").src = document.getElementById(\"calendariframe\").src.replace(\"ctz=local\", \"ctz=\" + Intl.DateTimeFormat().resolvedOptions().timeZone)</script>\n```\n\n----------------------------------------\n\nTITLE: Using dask-ssh with Hostfile\nDESCRIPTION: Example of using a hostfile to specify multiple hosts for dask-ssh, including both the hostfile content and the command to use it.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-ssh.rst#2025-04-19_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n$ cat hostfile.txt\n192.168.0.1\n192.168.0.2\n192.168.0.3\n192.168.0.4\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ dask-ssh --hostfile hostfile.txt\n```\n\n----------------------------------------\n\nTITLE: Adding error for replace=False in Dask Array choice\nDESCRIPTION: Raises an error when replace=False is specified in da.choice, as this is not currently supported.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_66\n\nLANGUAGE: Python\nCODE:\n```\nda.choice(..., replace=False)\n```\n\n----------------------------------------\n\nTITLE: Setup.py Configuration for Including YAML Files\nDESCRIPTION: Shows how to configure setup.py to include YAML configuration files in the package distribution\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/configuration.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom setuptools import setup\n\nsetup(...,\n      include_package_data=True,\n      ...)\n```\n\n----------------------------------------\n\nTITLE: Applying Inplace Operators on Dask DataFrames\nDESCRIPTION: Demonstrates support for inplace operators like += on Dask DataFrames.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_85\n\nLANGUAGE: Python\nCODE:\n```\ndf.x += 1\n```\n\n----------------------------------------\n\nTITLE: Visualizing Query Plan with GraphViz in Python\nDESCRIPTION: Shows how to generate a visual representation of the optimized query plan using the explain() method, which requires the GraphViz package\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-optimizer.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf.explain()\n```\n\n----------------------------------------\n\nTITLE: Updating tokenize for FIPS 140-2 compatibility in Dask\nDESCRIPTION: Modifies tokenize functionality to work in a FIPS 140-2 compliant environment.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_39\n\nLANGUAGE: Markdown\nCODE:\n```\n- Make ``tokenize`` work in a FIPS 140-2 environment (:pr:`8762`) `Jim Crist-Harif`_\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving Dask Arrays to Zarr\nDESCRIPTION: This snippet demonstrates how to create Dask arrays and save them to Zarr format on disk. It creates four 12500x10000 arrays of zeros and saves them as separate Zarr files.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/order.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> # create data on disk\n>>> import dask.array as da\n>>> x = da.zeros((12500, 10000), chunks=('10MB', -1))\n>>> da.to_zarr(x, 'saved_x1.zarr', overwrite=True)\n>>> da.to_zarr(x, 'saved_y1.zarr', overwrite=True)\n>>> da.to_zarr(x, 'saved_x2.zarr', overwrite=True)\n>>> da.to_zarr(x, 'saved_y2.zarr', overwrite=True)\n```\n\n----------------------------------------\n\nTITLE: Optimized Index Setting with Known Sort Order\nDESCRIPTION: Example of setting an index with optimization hints when data characteristics are known beforehand.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-best-practices.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.set_index(d.timestamp, sorted=True)\n```\n\n----------------------------------------\n\nTITLE: Registering CLI Entry Point in pyproject.toml\nDESCRIPTION: TOML configuration showing how to register a Dask CLI entry point using PEP-621 format in pyproject.toml.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/cli.rst#2025-04-19_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[project.entry-points.\"dask_cli\"]\nmycommand = \"mypackage.cli:main\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Table of Contents Structure in RST\nDESCRIPTION: Creates a table of contents in reStructuredText format with a maxdepth of 1, listing various Dask documentation pages related to deployment and configuration options.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-extra.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   adaptive\n   deploying-docker.rst\n   deploying-python-advanced.rst\n   software-environments\n   prometheus\n   customize-initialization\n   deployment-considerations.rst\n```\n\n----------------------------------------\n\nTITLE: Core Configuration Updates\nDESCRIPTION: Infrastructure improvements including configuration management and dependency handling.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_61\n\nLANGUAGE: Python\nCODE:\n```\nimport dask.config as config\nconfig.expand_environment_variables()  # New functionality\nos.environ[\"DASK_ROOT_CONFIG\"]  # New environment variable support\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame for Aggregation Examples\nDESCRIPTION: Code creating a sample pandas DataFrame and converting it to a Dask DataFrame for demonstrating custom aggregation functionality. The DataFrame is partitioned into 2 parts.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/dataframe-groupby.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> df = pd.DataFrame({\n...   'a': ['a', 'b', 'a', 'a', 'b'],\n...   'b': [0, 1, 0, 2, 5],\n... })\n>>> ddf = dd.from_pandas(df, 2)\n```\n\n----------------------------------------\n\nTITLE: Reading Tables with Dask DataFrame\nDESCRIPTION: Adds dd.read_table function for reading tabular data in Dask DataFrames.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_86\n\nLANGUAGE: Python\nCODE:\n```\ndd.read_table('data.csv')\n```\n\n----------------------------------------\n\nTITLE: Generating Changelog Entries with Git Log in Bash\nDESCRIPTION: This command generates changelog entries from git commit messages since the last tag, formats them for the Dask changelog, and applies some text replacements to ensure proper formatting of pull request references.\nSOURCE: https://github.com/dask/dask/blob/main/docs/release-procedure.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit log $(git describe --tags --abbrev=0)..HEAD --pretty=format:\"- %s \\`%an\\`_\"  > change.md && sed -i -e 's/(#/(:pr:\\`/g' change.md && sed -i -e 's/) \\`/\\`) \\`/g' change.md\n```\n\n----------------------------------------\n\nTITLE: Implementing sizeof for small dictionaries in Dask\nDESCRIPTION: Code that adds a sizeof implementation specifically for small dictionaries in Dask, which helps with memory management and optimization.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_45\n\nLANGUAGE: Python\nCODE:\n```\n-  Add sizeof implementation for small dicts (:pr:`5578`) `Matthew Rocklin`_\n```\n\n----------------------------------------\n\nTITLE: Installing dask-expr Package with pip\nDESCRIPTION: Command to install the dask-expr library using pip. This library contains a new implementation for Dask DataFrame that includes several improvements including logical query planning.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install dask-expr\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies\nDESCRIPTION: Command to install required documentation dependencies from requirements-docs.txt using pip.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/develop.rst#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install -r requirements-docs.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies with pip\nDESCRIPTION: Command to install required packages from requirements-docs.txt file using pip.\nSOURCE: https://github.com/dask/dask/blob/main/docs/README.rst#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install -r requirements-docs.txt\n```\n\n----------------------------------------\n\nTITLE: Adding assert_eq to top-level Dask modules\nDESCRIPTION: Adds the assert_eq function to top-level Dask modules for easier testing and comparison of Dask objects.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_78\n\nLANGUAGE: Python\nCODE:\n```\ndask.assert_eq(result, expected)\n```\n\n----------------------------------------\n\nTITLE: Launching Dask cluster using MPI in Bash\nDESCRIPTION: This snippet shows how to launch a Dask cluster using MPI with the dask-mpi command line tool. It uses mpirun to start 4 processes and specifies a scheduler file location.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-hpc.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmpirun --np 4 dask-mpi --scheduler-file /home/$USER/scheduler.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask worker memory management in YAML\nDESCRIPTION: This YAML configuration snippet demonstrates how to adjust Dask worker memory management settings. It disables spilling to disk and sets pause and terminate thresholds.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-hpc.rst#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ndistributed:\n  worker:\n    memory:\n      target: false  # don't spill to disk\n      spill: false  # don't spill to disk\n      pause: 0.80  # pause execution at 80% memory use\n      terminate: 0.95  # restart the worker at 95% use\n```\n\n----------------------------------------\n\nTITLE: Launching Dask scheduler and workers with shared file in Bash\nDESCRIPTION: This snippet shows how to start a Dask scheduler and workers using a shared scheduler file. This method is useful when deploying on systems with a shared file system.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-hpc.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndask-scheduler --scheduler-file /path/to/scheduler.json  # writes address to file\n\ndask-worker --scheduler-file /path/to/scheduler.json  # reads file for address\ndask-worker --scheduler-file /path/to/scheduler.json  # reads file for address\n```\n\n----------------------------------------\n\nTITLE: Connecting to Dask cluster with scheduler file in Python\nDESCRIPTION: This snippet demonstrates how to connect to a Dask cluster using a Client object and a scheduler file. This is useful when the scheduler information is shared via a file system.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-hpc.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nclient = Client(scheduler_file='/path/to/scheduler.json')\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video in RST Documentation\nDESCRIPTION: RST code snippet that embeds a YouTube video using HTML iframe with specific styling and permissions settings.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/presentations.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <iframe width=\"560\"\n           height=\"315\"\n           src=\"https://www.youtube.com/embed/nnndxbr_Xq4\"\n           style=\"margin: 0 auto 20px auto; display: block;\"\n           frameborder=\"0\"\n           allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\"\n           allowfullscreen></iframe>\n```\n\n----------------------------------------\n\nTITLE: Launching Dask-MPI with high-performance network interface in Bash\nDESCRIPTION: This snippet shows how to launch Dask-MPI specifying a high-performance network interface (Infiniband in this case) using the --interface option.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying-hpc.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmpirun --np 4 dask-mpi --scheduler-file /home/$USER/scheduler.json --interface ib0\n```\n\n----------------------------------------\n\nTITLE: Adding chunksize property to Dask Array\nDESCRIPTION: Adds a convenient chunksize property to Dask Array for easier access to chunk information.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_68\n\nLANGUAGE: Python\nCODE:\n```\ndask_array.chunksize\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Operator Cluster Setup\nDESCRIPTION: Example of deploying a Dask cluster on Kubernetes using the Dask Kubernetes Operator with resource specifications.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/deploying.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_kubernetes.operator import KubeCluster\ncluster = KubeCluster(\n    name=\"my-dask-cluster\",\n    image=\"ghcr.io/dask/dask:latest\",\n    resources={\"requests\": {\"memory\": \"2Gi\"}, \"limits\": {\"memory\": \"64Gi\"}},\n)\ncluster.scale(10)\nclient = cluster.get_client()\n```\n\n----------------------------------------\n\nTITLE: Building and Uploading Packages to PyPI\nDESCRIPTION: Commands to clean the repository, install build tools, build the package, and upload it to PyPI using twine.\nSOURCE: https://github.com/dask/dask/blob/main/docs/release-procedure.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clean -xfd\npip install build twine\npyproject-build\ntwine upload dist/*\n```\n\n----------------------------------------\n\nTITLE: Pushing Tags and Commits to GitHub Repositories\nDESCRIPTION: Commands to push the main branch and tags to both the Dask and Distributed repositories on GitHub.\nSOURCE: https://github.com/dask/dask/blob/main/docs/release-procedure.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit push https://github.com/dask/dask main --tags\ngit push https://github.com/dask/distributed main --tags\n```\n\n----------------------------------------\n\nTITLE: Fixing array slicing with negative step in Dask Array\nDESCRIPTION: Fixes and simplifies the behavior of array slicing when step < 0 in Dask Array.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_69\n\nLANGUAGE: Python\nCODE:\n```\ndask_array[start:stop:-1]\n```\n\n----------------------------------------\n\nTITLE: Restarting Conda-Forge CI\nDESCRIPTION: Command to restart CI builds on conda-forge PRs by leaving a specific comment for the conda-forge admin bot.\nSOURCE: https://github.com/dask/dask/blob/main/docs/release-procedure.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n@conda-forge-admin, please restart CI\n```\n\n----------------------------------------\n\nTITLE: Python Version Comparison Example\nDESCRIPTION: Demonstrates use of the 'packaging' library for version comparison instead of direct string comparisons\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n# Old code being updated:\nuse packaging for version comparisons\n# Implied change from direct string comparison to using packaging library\n```\n\n----------------------------------------\n\nTITLE: Fixing da.roll for Zero-Dimensional Shapes\nDESCRIPTION: Ensure da.roll works even if shape is 0.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/changelog.rst#2025-04-19_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nda.roll\n```\n\n----------------------------------------\n\nTITLE: Creating Local Table of Contents in RST\nDESCRIPTION: RST directive to generate a local table of contents for the document.\nSOURCE: https://github.com/dask/dask/blob/main/docs/source/presentations.rst#2025-04-19_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. contents:: :local:\n```"
  }
]
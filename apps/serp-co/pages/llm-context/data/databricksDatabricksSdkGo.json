[
  {
    "owner": "databricks",
    "repo": "databricks-sdk-go",
    "content": "TITLE: Authenticating Databricks with Token Auth\nDESCRIPTION: This Go snippet demonstrates how to authenticate to a Databricks workspace using a personal access token (PAT).  It prompts the user for the workspace host URL and the PAT, and then uses the `databricks.NewWorkspaceClient` function to create a client. The code then retrieves the current user's information using the client and prints the display name.  It utilizes the `github.com/databricks/databricks-sdk-go` and `github.com/databricks/databricks-sdk-go/config` packages. The host URL and PAT are required for successful authentication.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_0\n\nLANGUAGE: go\nCODE:\n```\n```go\npackage main\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"strings\"\n\n\t\"github.com/databricks/databricks-sdk-go\"\n\t\"github.com/databricks/databricks-sdk-go/config\"\n)\n\nfunc main() {\n\t// Perform Databricks token authentication for a Databricks workspace.\n\tw, err := databricks.NewWorkspaceClient(&databricks.Config{\n\t\tHost:        askFor(\"Host:\"),                  // workspace url\n\t\tToken:       askFor(\"Personal Access Token:\"), // PAT\n\t\tCredentials: config.PatCredentials{},          // enforce PAT auth\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tme, err := w.CurrentUser.Me(context.Background())\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tfmt.Printf(\"Hello, my name is %s!\\n\", me.DisplayName)\n}\n\nfunc askFor(prompt string) string {\n\tvar s string\n\tr := bufio.NewReader(os.Stdin)\n\tfor {\n\t\tfmt.Fprint(os.Stdout, prompt+\" \")\n\t\ts, _ = r.ReadString('\\n')\n\t\ts = strings.TrimSpace(s)\n\t\tif s != \"\" {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn s\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Azure Client Secret Auth in Databricks SDK Go\nDESCRIPTION: This Go snippet showcases Azure client secret authentication for the Databricks SDK. It takes user input for the Databricks host, Azure Resource ID, AAD Tenant ID, AAD Client ID, and AAD Client Secret. The `databricks.NewWorkspaceClient` is used to create a new client, and `config.AzureClientSecretCredentials{}` enforces the use of the client secret authentication flow. The provided inputs are essential for authenticating with the Azure Databricks service principal.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_1\n\nLANGUAGE: go\nCODE:\n```\n```go\nw, err := databricks.NewWorkspaceClient(&databricks.Config{\n  Host:              askFor(\"Host:\"),\n  AzureResourceID:   askFor(\"Azure Resource ID:\"),\n  AzureTenantID:     askFor(\"AAD Tenant ID:\"),\n  AzureClientID:     askFor(\"AAD Client ID:\"),\n  AzureClientSecret: askFor(\"AAD Client Secret:\"),\n  Credentials:       config.AzureClientSecretCredentials{},\n})\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing Client with .databrickscfg Profile - Databricks SDK - Go\nDESCRIPTION: Shows how to initialize the Databricks Workspace client using a specific profile defined in the `.databrickscfg` file instead of the default. The `Profile` field in `databricks.Config` is set to the desired profile name. `databricks.Must` is used here for concise error handling, panicking if client creation fails.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_3\n\nLANGUAGE: Go\nCODE:\n```\nw := databricks.Must(databricks.NewWorkspaceClient(&databricks.Config{\n  Profile:  \"MYPROFILE\",\n}))\n// Now call the Databricks workspace APIs as desired...\n```\n\n----------------------------------------\n\nTITLE: Initializing Client with GCP ID Auth - Databricks SDK - Go\nDESCRIPTION: Demonstrates how to create a Databricks Workspace client configured to use Google Cloud Platform (GCP) ID authentication. It requires specifying the Databricks Host, the GCP service account email for impersonation, and using the `config.GoogleDefaultCredentials{}` structure in the `databricks.Config`.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_2\n\nLANGUAGE: Go\nCODE:\n```\nw, err := databricks.NewWorkspaceClient(&databricks.Config{\n  Host:                 askFor(\"Host:\"),\n  GoogleServiceAccount: askFor(\"Google Service Account:\"),\n  Credentials:          config.GoogleDefaultCredentials{},\n})\n```\n\n----------------------------------------\n\nTITLE: Waiting for Long-Running Cluster Creation - Databricks SDK - Go\nDESCRIPTION: Demonstrates the pattern for handling long-running operations using the `AndWait` suffix on SDK methods. This example shows `Clusters.CreateAndWait`, which asynchronously creates a cluster and blocks until it reaches a terminal state (e.g., `RUNNING`). It shows how to configure cluster parameters and set a custom timeout using `retries.Timeout`.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_6\n\nLANGUAGE: Go\nCODE:\n```\nclusterInfo, err = w.Clusters.CreateAndWait(ctx, clusters.CreateCluster{\n    ClusterName:            \"Created cluster\",\n    SparkVersion:           latestLTS,\n    NodeTypeId:             smallestWithDisk,\n    AutoterminationMinutes: 10,\n    NumWorkers:             1,\n}, retries.Timeout[clusters.ClusterInfo](10*time.Minute))\n```\n\n----------------------------------------\n\nTITLE: Executing Commands on Databricks Clusters using Go SDK\nDESCRIPTION: Executes a Python command on an interactive Databricks cluster using the CommandExecutor interface of the SDK. Dependencies include the Databricks SDK for Go and an established workspace client (w). Parameters: ctx (context), clusterId (target cluster), language (python), code (string). Prints execution results and handles failures by returning any errors. Output is the text of standard output from the executed code. The interface imposes a 20 minute execution timeout.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_7\n\nLANGUAGE: Go\nCODE:\n```\nres := w.CommandExecutor.Execute(ctx, clusterId, \"python\", \"print(1)\")\nif res.Failed() {\n    return fmt.Errorf(\"command failed: %w\", res.Err())\n}\nprintln(res.Text())\n// Out: 1\n```\n\n----------------------------------------\n\nTITLE: Tracking State of Long-Running Operations with Functional Options in Go SDK\nDESCRIPTION: Tracks intermediate state during a long-running cluster creation operation by supplying a functional option that receives progress updates. Dependencies: Databricks SDK for Go, retries and clusters packages. Parameters: ctx (context), clusters.CreateCluster (cluster definition), and a callback function to process intermediate state. Outputs the final cluster info upon success. Suitable for monitoring and displaying progress of asynchronous actions.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_9\n\nLANGUAGE: Go\nCODE:\n```\nclusterInfo, err = w.Clusters.CreateAndWait(ctx, clusters.CreateCluster{\n    // ...\n}, func(i *retries.Info[clusters.ClusterInfo]) {\n    updateIntermediateState(i.Info.StateMessage)\n})\n```\n\n----------------------------------------\n\nTITLE: Selecting Node Types and Spark Versions for Clusters using Go SDK\nDESCRIPTION: Fetches available Spark runtime versions and node types, selects the latest supported LTS runtime, picks the smallest node type with a local disk, and creates an interactive cluster with those settings. Dependencies: Databricks SDK for Go, clusters package, workspace client (w). Key parameters: ctx, clusterName, selection requests. Demonstrates multi-step selection, cluster configuration, and error handling for cluster creation.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_12\n\nLANGUAGE: Go\nCODE:\n```\n// Fetch the list of spark runtime versions.\nsparkVersions, err := w.Clusters.SparkVersions(ctx)\nif err != nil {\n    return err\n}\n\n// Select the latest LTS version.\nlatestLTS, err := sparkVersions.Select(clusters.SparkVersionRequest{\n    Latest:          true,\n    LongTermSupport: true,\n})\nif err != nil {\n    return err\n}\n\n// Fetch the list of available node types.\nnodeTypes, err := w.Clusters.ListNodeTypes(ctx)\nif err != nil {\n    return err\n}\n\n// Select the smallest node type ID.\nsmallestWithDisk, err := nodeTypes.Smallest(clusters.NodeTypeRequest{\n    LocalDisk: true,\n})\nif err != nil {\n    return err\n}\n\n// Create the cluster and wait for it to start properly.\nrunningCluster, err := w.Clusters.CreateAndWait(ctx, clusters.CreateCluster{\n    ClusterName:            clusterName,\n    SparkVersion:           latestLTS,\n    NodeTypeId:             smallestWithDisk,\n    AutoterminationMinutes: 15,\n    NumWorkers:             1,\n})\n```\n\n----------------------------------------\n\nTITLE: Managing Cluster Libraries with UpdateAndWait in Databricks Go SDK\nDESCRIPTION: Installs a Python PyPI library on a running Databricks cluster using the Libraries API's UpdateAndWait method. This method follows long-running operation conventions, handling both installation and status checks, and propagates errors in a simplified way. Dependencies: Databricks SDK for Go, libraries and clusters configured in the workspace client (w). Key parameters: ctx (context), Update structure with ClusterId and Install list. Designed for reliable cluster library management.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_8\n\nLANGUAGE: Go\nCODE:\n```\nerr = w.Libraries.UpdateAndWait(ctx, libraries.Update{\n    ClusterId: clusterId,\n    Install: []libraries.Library{\n        {\n            Pypi: &libraries.PythonPyPiLibrary{\n                Package: \"dbl-tempo\",\n            },\n        },\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Iterating Paginated API Responses Using ListAll in Databricks Go SDK\nDESCRIPTION: Retrieves all repository entries from the Databricks workspace, abstracting pagination details using the ListAll helper method. Dependencies: Databricks SDK for Go, w.Repos client. Parameters include ctx (context) and the repos.List request object. Prints each repository path after fetching all. Handles errors in querying repositories. The approach hides various API-side pagination mechanisms.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_10\n\nLANGUAGE: Go\nCODE:\n```\nall, err := w.Repos.ListAll(ctx, repos.List{})\nif err != nil {\n    return fmt.Errorf(\"list repos: %w\", err)\n}\nfor _, repo := range all {\n    println(repo.Path)\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Repositories via GetByPath Helper in Databricks Go SDK\nDESCRIPTION: Uses the GetByPath utility to retrieve a workspace repository by its filesystem path and then updates its branch. This utility returns a single object or an error if duplicate names are found. Dependencies include the Databricks SDK for Go and the w.Repos client. Parameters: ctx, path (string), tag (branch/tag name). Demonstrates a common workflow of lookup and update by name/path.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_11\n\nLANGUAGE: Go\nCODE:\n```\nrepo, err := w.Repos.GetByPath(ctx, path)\nif err != nil {\n    return err\n}\nreturn w.Repos.Update(ctx, repos.UpdateRepo{\n    RepoId: repo.Id,\n    Branch: tag,\n})\n```\n\n----------------------------------------\n\nTITLE: Downloading Files from DBFS to Local via io.Writer in Databricks Go SDK\nDESCRIPTION: Streams a file from Databricks DBFS to a local file using Go's io.Writer interfaces. Opens the remote DBFS file for reading and writes the contents to a local file. Requires Databricks SDK for Go, 'os', 'io', and the workspace client's Dbfs module. Parameters: ctx, local output path, remote DBFS file path. Example includes proper file handling and supporting random access.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_14\n\nLANGUAGE: Go\nCODE:\n```\ndownload, _ := os.Create(\"/path/to/local\")\nremote, _ := w.Dbfs.Open(ctx, \"/path/to/remote/file\", dbfs.FileModeRead)\n_, _ = io.Copy(download, remote)\n```\n\n----------------------------------------\n\nTITLE: Uploading Files to DBFS via io.Reader in Databricks Go SDK\nDESCRIPTION: Uploads a local file to Databricks DBFS by opening the local file as an io.Reader and streaming its content to a remote DBFS handle in write and overwrite mode. Requires Databricks SDK for Go, 'os', and 'io' packages. Parameters: ctx (context), local file path, remote DBFS path. Demonstrates integration with native Go IO interfaces and correct resource cleanup by closing the remote handle.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_13\n\nLANGUAGE: Go\nCODE:\n```\nupload, _ := os.Open(\"/path/to/local/file.ext\")\nremote, _ := w.Dbfs.Open(ctx, \"/path/to/remote/file\", dbfs.FileModeWrite|dbfs.FileModeOverwrite)\n_, _ = io.Copy(remote, upload)\n_ = remote.Close()\n```\n\n----------------------------------------\n\nTITLE: Uploading Byte Slices to DBFS using WriteFile in Databricks Go SDK\nDESCRIPTION: Uploads a byte slice directly as a file to Databricks DBFS using the WriteFile helper. Needs the Databricks SDK for Go, a workspace client (w), and a context. Parameters: ctx (context), filesystem path, and contents (as []byte). This is optimal for small files or situations where the upload content is already in memory. On error, returns an error object.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_15\n\nLANGUAGE: Go\nCODE:\n```\nerr := w.Dbfs.WriteFile(ctx, \"/path/to/remote/file\", []byte(\"Hello world!\"))\n```\n\n----------------------------------------\n\nTITLE: Downloading from DBFS to Byte Slice via ReadFile in Databricks Go SDK\nDESCRIPTION: Downloads file content from Databricks DBFS into a byte slice in memory, simplifying use cases where the data must be processed in RAM. Requires Databricks SDK for Go and workspace client. Parameters: ctx, path. Returns the file content ([]byte) and any error encountered. Particularly useful for short, random-access reads or for testing.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_16\n\nLANGUAGE: Go\nCODE:\n```\nbuf, err := w.Dbfs.ReadFile(ctx, \"/path/to/remote/file\")\n```\n\n----------------------------------------\n\nTITLE: Handling API Errors with APIError Type in Go\nDESCRIPTION: Demonstrates how to handle API errors by asserting the error as apierr.APIError and accessing error details such as error code, message, status code, and additional details.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_19\n\nLANGUAGE: go\nCODE:\n```\n_, err := w.Clusters.Create(ctx, compute.CreateCluster{...})\nif e, ok := err.(*apierr.APIError); ok {\n    fmt.Printf(\"Error code: %s\\n\", e.ErrorCode)\n    fmt.Printf(\"Error message: %s\\n\", e.Message)\n    fmt.Printf(\"Status code: %s\\n\", e.StatusCode)\n    fmt.Printf(\"Error details: %v\\n\", e.Details)\n}\n```\n\n----------------------------------------\n\nTITLE: Using Predefined Errors with errors.Is() in Go\nDESCRIPTION: Shows how to use predefined errors from the Databricks SDK with the standard errors.Is() function to check for specific error conditions like resource not existing.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_20\n\nLANGUAGE: go\nCODE:\n```\nc, err := w.Clusters.GetByClusterId(ctx, \"12345\")\nif errors.Is(err, databricks.ErrResourceDoesNotExist) {... }\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug Logging in Databricks SDK for Go\nDESCRIPTION: Demonstrates how to customize the logging level for the Databricks SDK by overriding the default logger with a SimpleLogger set to debug level.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_21\n\nLANGUAGE: go\nCODE:\n```\nimport \"github.com/databricks/databricks-sdk-go/logger\"\n\nfunc init() {\n\tlogger.DefaultLogger = &logger.SimpleLogger{\n\t\tLevel: logger.LevelDebug,\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Using Mock Implementations for Testing Databricks SDK for Go\nDESCRIPTION: Shows how to use the mockery-based mock implementations of the SDK's interfaces to write unit tests, including mocking WorkspaceClient and AccountClient APIs.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_22\n\nLANGUAGE: go\nCODE:\n```\npackage my_test\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/databricks/databricks-sdk-go/experimental/mocks\"\n\t\"github.com/databricks/databricks-sdk-go/listing\"\n\t\"github.com/databricks/databricks-sdk-go/qa/poll\"\n\t\"github.com/databricks/databricks-sdk-go/service/compute\"\n\t\"github.com/databricks/databricks-sdk-go/service/iam\"\n\t\"github.com/databricks/databricks-sdk-go/service/sql\"\n\t\"github.com/stretchr/testify/mock\"\n)\n\nfunc TestDatabricksSDK(t *testing.T) {\n\tctx := context.Background()\n\tw := mocks.NewMockWorkspaceClient(t)\n\tw.GetMockClustersAPI().EXPECT().ListAll(\n\t\tctx,\n\t\tmock.AnythingOfType(\"compute.ListClustersRequest\"),\n\t).Return(\n\t\t[]compute.ClusterDetails{\n\t\t\t{ClusterName: \"test-cluster-1\"},\n\t\t\t{ClusterName: \"test-cluster-2\"},\n\t\t}, nil)\n\n\t// You can also mock the AccountClient as follows.\n\ta := mocks.NewMockAccountClient(t)\n\ta.GetMockAccountUsersAPI().EXPECT().ListAll(\n\t\tctx,\n\t\tmock.AnythingOfType(\"iam.ListAccountUsersRequest\"),\n\t).Return(\n\t\t[]iam.User{\n\t\t\t{DisplayName: \"test-user-1\"},\n\t\t\t{DisplayName: \"test-user-2\"},\n\t\t}, nil)\n}\n```\n\n----------------------------------------\n\nTITLE: Using Testing Utilities in Databricks SDK for Go\nDESCRIPTION: Demonstrates how to use the SDK's testing utilities such as SliceIterator to mock listing operations and poll.Simple() to mock long-running operations.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_23\n\nLANGUAGE: go\nCODE:\n```\nfunc TestDatabricksSDK_helpers(t *testing.T) {\n\t// To mock iterators, you can provide the items to iterate over with\n\t// *listing.SliceIterator.\n\titerator := listing.SliceIterator[iam.User]([]iam.User{\n\t\t{DisplayName: \"test-user-1\"},\n\t\t{DisplayName: \"test-user-2\"},\n\t})\n\ta.GetMockAccountUsersAPI().EXPECT().List(\n\t\tctx,\n\t\tmock.AnythingOfType(\"iam.ListAccountUsersRequest\"),\n\t).Return(&iterator)\n\n\t// To mock Wait* structures, you can stub out the Poll field.\n\tgetResponse := sql.GetWarehouseResponse{\n\t\tId: \"abc\",\n\t}\n\twait := sql.WaitGetWarehouseRunning[struct{}]{\n\t\tPoll: poll.Simple(getResponse),\n\t}\n\tw.GetMockWarehousesAPI().EXPECT().Edit(mock.Anything, sql.EditWarehouseRequest{}).Return(&wait, nil)\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Credentials Provider - Databricks SDK - Go\nDESCRIPTION: Provides an example of creating a custom credentials provider by defining a struct that implements the necessary methods (`Name` and `Configure`). The `Configure` method returns a function that modifies the `http.Request` object, allowing manual manipulation of request headers or other authentication details. The custom provider is then passed to `databricks.NewWorkspaceClient` via the `Credentials` field.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_5\n\nLANGUAGE: Go\nCODE:\n```\ntype CustomCredentials struct {}\n\nfunc (c *CustomCredentials) Name() string {\n\treturn \"custom\"\n}\n\nfunc (c *CustomCredentials) Configure(ctx context.Context, cfg *config.Config) (func(*http.Request) error, error) {\n\treturn func(r *http.Request) error {\n\t\ttoken := \"...\"\n\t\tr.Header.Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", token))\n\t\treturn nil\n\t}, nil\n}\n\nfunc main() {\n\tw := databricks.Must(databricks.NewWorkspaceClient(&databricks.Config{\n\t\tCredentials: &CustomCredentials{},\n\t}))\n    // ..\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Client with Debug Headers - Databricks SDK - Go\nDESCRIPTION: Illustrates how to enable debugging of HTTP request headers made by the Databricks SDK client. Setting `DebugHeaders: true` in the `databricks.Config` will print request headers, which can be useful for troubleshooting but should be used cautiously as headers may contain sensitive information like tokens.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_4\n\nLANGUAGE: Go\nCODE:\n```\nw := databricks.Must(databricks.NewWorkspaceClient(&databricks.Config{\n  DebugHeaders: true,\n}))\n// Now call the Databricks workspace APIs as desired...\n```\n\n----------------------------------------\n\nTITLE: Specifying Product Name and Version in User-Agent with WithProduct in Go SDK\nDESCRIPTION: Sets a custom product name and SemVer version in the User-Agent string for requests made from applications built with the Databricks SDK for Go. Subsequent WithProduct calls override previous values. Constraints: product name restrictions as for partner, version must be valid SemVer. Useful for attribution and traceability in multi-tenant or partner scenarios.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_18\n\nLANGUAGE: Go\nCODE:\n```\nuseragent.WithProduct(\"databricks-example-product\", \"1.2.0\")\n```\n\n----------------------------------------\n\nTITLE: Adding User-Agent Metadata for Request Attribution in Databricks Go SDK\nDESCRIPTION: Statically adds partner metadata to the User-Agent header of all requests made via the SDK using WithPartner. Multiple partners can be specified. Dependencies: Databricks SDK for Go useragent package. Parameters: partner name strings (format restrictions apply). Enables apps or libraries to indicate data provenance for platform operations.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/README.md#_snippet_17\n\nLANGUAGE: Go\nCODE:\n```\nuseragent.WithPartner(\"partner-abc\")\nuseragent.WithPartner(\"partner-xyz\")\n```\n\n----------------------------------------\n\nTITLE: Displaying structured logging output using slog in Go\nDESCRIPTION: Shows example JSON formatted logging output produced by the slog library integrated with the Databricks SDK. The logs include timestamped debug messages for loading configuration, authentication attempts, and API requests. No dependencies beyond the slog package are needed, and the snippet highlights expected JSON fields such as time, level, msg, and global status.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/examples/slog/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{\"time\":\"2023-03-10T08:33:11.318122+01:00\",\"level\":\"DEBUG\",\"msg\":\"Loading config via environment\",\"global\":true}\n{\"time\":\"2023-03-10T08:33:11.318411+01:00\",\"level\":\"DEBUG\",\"msg\":\"Loading config via config-file\",\"global\":true}\n{\"time\":\"2023-03-10T08:33:11.318552+01:00\",\"level\":\"DEBUG\",\"msg\":\"Loading DEFAULT profile from [...]/.databrickscfg\",\"global\":true}\n{\"time\":\"2023-03-10T08:33:11.31862+01:00\",\"level\":\"DEBUG\",\"msg\":\"Attempting to configure auth: pat\",\"global\":false}\n{\"time\":\"2023-03-10T08:33:12.170168+01:00\",\"level\":\"DEBUG\",\"msg\":\"GET /api/2.0/preview/scim/v2/Me\\n< HTTP/2.0 200 OK\\n\"}\n```\n\n----------------------------------------\n\nTITLE: Displaying plain text logging output using slog in Go\nDESCRIPTION: Provides sample plain text formatted logging output generated by the slog library when used with the Databricks SDK. It captures detailed debug-level log entries with timestamps, message levels, and messages related to configuration loading and API communication. This example highlights the human-readable log output format.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/examples/slog/README.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ntime=2023-03-10T08:33:44.219+01:00 level=DEBUG msg=\"Loading config via environment\" global=true\ntime=2023-03-10T08:33:44.219+01:00 level=DEBUG msg=\"Loading config via config-file\" global=true\ntime=2023-03-10T08:33:44.219+01:00 level=DEBUG msg=\"Loading DEFAULT profile from [...]/.databrickscfg\" global=true\ntime=2023-03-10T08:33:44.220+01:00 level=DEBUG msg=\"Attempting to configure auth: pat\" global=false\ntime=2023-03-10T08:33:45.077+01:00 level=DEBUG msg=\"GET /api/2.0/preview/scim/v2/Me\\n< HTTP/2.0 200 OK\\n\"\n```\n\n----------------------------------------\n\nTITLE: Displaying zerolog JSON Output Format with databricks-sdk-go Logging Text Snippet\nDESCRIPTION: This snippet shows example log entries in JSON format generated by zerolog when used with the databricks-sdk-go SDK. The entries include fields such as log level, timestamp, message details, and context flags indicating global or local scope. This output helps in structured logging and programmatic parsing of log data for debugging and auditing.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/examples/zerolog/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{\"level\":\"trace\",\"global\":true,\"time\":\"2023-03-10T08:34:56+01:00\",\"message\":\"Loading config via environment\"}\n{\"level\":\"trace\",\"global\":true,\"time\":\"2023-03-10T08:34:56+01:00\",\"message\":\"Loading config via config-file\"}\n{\"level\":\"debug\",\"global\":true,\"time\":\"2023-03-10T08:34:56+01:00\",\"message\":\"Loading DEFAULT profile from [...]/.databrickscfg\"}\n{\"level\":\"trace\",\"global\":false,\"time\":\"2023-03-10T08:34:56+01:00\",\"message\":\"Attempting to configure auth: pat\"}\n{\"level\":\"debug\",\"global\":false,\"time\":\"2023-03-10T08:34:57+01:00\",\"message\":\"GET /api/2.0/preview/scim/v2/Me\\n< HTTP/2.0 200 OK\\n...\"}\n```\n\n----------------------------------------\n\nTITLE: Displaying zerolog Plain Text Output Format with databricks-sdk-go Logging Text Snippet\nDESCRIPTION: This snippet provides an example of the plain text log output format from zerolog when logging databricks-sdk-go SDK activities. It includes human-readable timestamps, log levels as abbreviations, log messages, and context tags. This format is useful for terminal viewing and debugging during development cycles.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/examples/zerolog/README.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n8:35AM TRC Loading config via environment global=true\n8:35AM TRC Loading config via config-file global=true\n8:35AM DBG Loading DEFAULT profile from [...]/.databrickscfg global=true\n8:35AM TRC Attempting to configure auth: pat global=false\n8:35AM DBG GET /api/2.0/preview/scim/v2/Me\n< HTTP/2.0 200 OK\n...\n```\n\n----------------------------------------\n\nTITLE: Visualizing Client Configuration with Mermaid\nDESCRIPTION: This Mermaid class diagram details the structure of the client configuration process in the Databricks SDK. It shows the relationships between different components, including Config, Loaders, Credentials, RequestVisitor, and DatabricksClient, and how they interact to configure the client for API calls.  It also shows the different credential types.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/config/README.md#_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nclassDiagram\n    Loader \"0..n\" <-- Config: Configure(self)\n    Credentials \"0..1\" <-- Config: Configure(self)\n    RequestVisitor --* Config: configured auth\n    class Config {\n        * Host string\n        * Token string\n        * Profile string\n        * Username string\n        * Password string\n        * AzureResourceID string\n        * AzureEnvironment string\n        * AzureClientID string\n        * AzureSecretID string\n        * AzureTenantID string\n        * GoogleServiceAccount string\n\n        Credentials: DefaultCredentials\n        Loaders: Loader\n\n        Authenticate(HttpRequest) error\n    }\n\n    class Loader {\n        <<interface>>\n        Name() string\n        Configure(Config) error\n    }\n\n    KnownConfigLoader ..|> Loader\n    class KnownConfigLoader\n\n    ConfigAttributes ..|> Loader\n    class ConfigAttributes {\n        Configure(Config) error\n        DebugString(Config) string\n        Validate(Config) error\n        ResolveFromStringMap(Config, map) error\n        ResolveFromAnyMap(Config, map) error\n    }\n\n    Config --* DatabricksClient\n    class DatabricksClient {\n        Config\n        - retryPolicy\n        \n        Get(path, query) T\n        Post(path, body) T\n        Put(path, body) T\n        Patch(path, body) T\n        Delete(path, query) T\n    }\n\n    Credentials --> \"0..1\" RequestVisitor: creates\n    class Credentials {\n        <<interface>>\n        Name() string\n        Configure(Config) RequestVisitor\n    }\n\n    class RequestVisitor {\n        <<interface>>\n        Visit(HttpRequest) error\n    }\n\n    AzureSpnCredentials --* authProviders\n    AzureSpnCredentials ..|> Credentials\n    class AzureSpnCredentials\n\n    AzureCliCredentials --* authProviders\n    AzureCliCredentials ..|> Credentials\n\n    GoogleCredentials --* authProviders\n    GoogleCredentials ..|> Credentials\n    class GoogleCredentials\n    \n    DatabricksOauthCredentials --* authProviders\n    DatabricksOauthCredentials ..|> Credentials\n    class DatabricksOauthCredentials {\n        []Scopes\n    }\n\n    PatCredentials --* authProviders\n    PatCredentials ..|> Credentials\n    class PatCredentials\n\n    BasicCredentials --* authProviders\n    BasicCredentials ..|> Credentials\n\n    authProviders --> DefaultCredentials: for reach ConfigAttributes()\n    DefaultCredentials ..|> Credentials\n    class DefaultCredentials\n```\n\n----------------------------------------\n\nTITLE: Visualizing Authentication Flow with Mermaid\nDESCRIPTION: This Mermaid diagram visualizes the authentication flow within the Databricks SDK, showing the sequence of calls from ClustersAPI to DatabricksClient and then through various authentication mechanisms to finally get the authenticated request.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/config/README.md#_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    ClustersAPI->>+DatabricksClient: GET .../clusters/list\n    DatabricksClient->>+databricks.Config: Authenticate(HttpRequest)\n\n    databricks.Config-->>+DefaultCredentials: Configure(databricks.Config)\n    DefaultCredentials-->>+FirstCredentials: try configure\n    FirstCredentials-->>-DefaultCredentials: try next\n    DefaultCredentials->>+NextCredentials: try configure\n    NextCredentials->>RequestVisitor: configured auth\n    NextCredentials->>-DefaultCredentials: authenticated\n    DefaultCredentials->>-databricks.Config: set AuthType & request visitor\n\n    databricks.Config->>+RequestVisitor: visit HTTP request\n    RequestVisitor-->>+IdentityProvider: ensure fresh token\n    IdentityProvider-->>-RequestVisitor: access token\n    RequestVisitor->>-databricks.Config: added HTTP headers\n    \n    databricks.Config->>-DatabricksClient: added HTTP headers\n\n    DatabricksClient->>+API: authenticated request\n    API->>-DatabricksClient: JSON payload\n    DatabricksClient->>-ClustersAPI: ClustersList or error\n```\n\n----------------------------------------\n\nTITLE: Git Commit with Automatic Sign-off (Shell)\nDESCRIPTION: This command demonstrates how to automatically add the 'Signed-off-by' line to a Git commit message using the `-s` flag. This feature relies on your `user.name` and `user.email` being correctly configured in Git.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ngit commit -s -m \"Your commit message\"\n```\n\n----------------------------------------\n\nTITLE: Git Commit Sign-off Line (Text Format)\nDESCRIPTION: This snippet shows the required format for the 'Signed-off-by' line that must be added to Git commit messages to certify compliance with the Developer Certificate of Origin (DCO). It requires using your real name and email address.\nSOURCE: https://github.com/databricks/databricks-sdk-go/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nSigned-off-by: Joe Smith <joe.smith@email.com>\n```"
  }
]
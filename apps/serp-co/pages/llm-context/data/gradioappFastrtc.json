[
  {
    "owner": "gradio-app",
    "repo": "fastrtc",
    "content": "TITLE: Setting up WebRTC Connection with Dynamic Configuration\nDESCRIPTION: This JavaScript snippet demonstrates how to set up a WebRTC connection with configurable modes (send-receive, send, receive) and modalities (audio, video). It includes creating a peer connection, handling media streams, and creating an offer.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/api.md#2025-04-17_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n// pass any rtc_configuration params here\nconst pc = new RTCPeerConnection();\n{{? it.mode === \"send-receive\" || it.mode === \"receive\" }}\nconst {{=it.modality}}_output_component = document.getElementById(\"{{=it.modality}}_output_component_id\");\n{{?}}                     \nasync function setupWebRTC(peerConnection) {\n    {{? it.mode === \"send-receive\" || it.mode === \"send\" }}      \n    // Get {{=it.modality}} stream from webcam\n    const stream = await navigator.mediaDevices.getUserMedia({\n        {{=it.modality}}: true,\n    })\n    {{?}}\n    {{? it.mode === \"send-receive\" }}\n    //  Send {{=it.modality}} stream to server\n    stream.getTracks().forEach(async (track) => {\n        const sender = pc.addTrack(track, stream);\n    })\n    {{?? it.mode === \"send\" }}\n    // Receive {{=it.modality}} stream from server\n    pc.addTransceiver({{=it.modality}}, { direction: \"recvonly\" })\n    {{?}}\n    {{? it.mode === \"send-receive\" || it.mode === \"receive\" }}\n    peerConnection.addEventListener(\"track\", (evt) => {\n        if ({{=it.modality}}_output_component && \n            {{=it.modality}}_output_component.srcObject !== evt.streams[0]) {\n            {{=it.modality}}_output_component.srcObject = evt.streams[0];\n        }\n    });\n    {{?}}\n    // Create data channel (needed!)\n    const dataChannel = peerConnection.createDataChannel(\"text\");\n\n    // Create and send offer\n    const offer = await peerConnection.createOffer();\n    await peerConnection.setLocalDescription(offer);\n\n    let webrtc_id = Math.random().toString(36).substring(7)\n\n```\n\n----------------------------------------\n\nTITLE: Setting up WebRTC Connection with Media Streams in JavaScript\nDESCRIPTION: Establishes a WebRTC connection with the server using a peer connection object. The function handles media stream acquisition (audio/video), manages transceivers based on send/receive mode, sets up data channels, creates and sends an offer to the server, and processes the server response.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/webrtc_docs.md#2025-04-17_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// pass any rtc_configuration params here\nconst pc = new RTCPeerConnection();\n{% if mode in [\"send-receive\", \"receive\"] %}\nconst {{modality}}_output_component = document.getElementById(\"{{modality}}_output_component_id\");\n{% endif %}                     \nasync function setupWebRTC(peerConnection) {\n    {%- if mode in [\"send-receive\", \"send\"] -%}      \n    // Get {{modality}} stream from webcam\n    const stream = await navigator.mediaDevices.getUserMedia({\n        {{modality}}: true,\n    })\n    {%- endif -%}\n    {% if mode == \"send-receive\" %}\n\n    //  Send {{ self.modality }} stream to server\n    stream.getTracks().forEach(async (track) => {\n        const sender = pc.addTrack(track, stream);\n    })\n    {% elif mode == \"send\" %}\n    // Receive {self.modality} stream from server\n    pc.addTransceiver({{modality}}, { direction: \"recvonly\" })\n    {%- endif -%}\n    {% if mode in [\"send-receive\", \"receive\"] %}\n    peerConnection.addEventListener(\"track\", (evt) => {\n        if ({{modality}}_output_component && \n            {{modality}}_output_component.srcObject !== evt.streams[0]) {\n            {{modality}}_output_component.srcObject = evt.streams[0];\n        }\n    });\n    {% endif %}\n    // Create data channel (needed!)\n    const dataChannel = peerConnection.createDataChannel(\"text\");\n\n    // Create and send offer\n    const offer = await peerConnection.createOffer();\n    await peerConnection.setLocalDescription(offer);\n\n    // Send offer to server\n    const response = await fetch('/webrtc/offer', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({\n            sdp: offer.sdp,\n            type: offer.type,\n            webrtc_id: Math.random().toString(36).substring(7)\n        })\n    });\n\n    // Handle server response\n    const serverResponse = await response.json();\n    await peerConnection.setRemoteDescription(serverResponse);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Reply On Pause for Voice-Activated Responses in Python\nDESCRIPTION: This code snippet demonstrates how to use the ReplyOnPause class from FastRTC to create a voice-activated response system. It handles voice detection and turn-taking logic automatically, running a Python function when the user stops speaking.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import ReplyOnPause, Stream\n\ndef response(audio: tuple[int, np.ndarray]): # (1)\n    sample_rate, audio_array = audio\n    # Generate response\n    for audio_chunk in generate_response(sample_rate, audio_array):\n        yield (sample_rate, audio_chunk) # (2)\n\nstream = Stream(\n    handler=ReplyOnPause(response),\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Voice Chat with FastRTC\nDESCRIPTION: Example showing how to create a voice chat interface to an LLM. Uses speech-to-text to convert audio to text, sends to an LLM API, and converts the response back to audio using text-to-speech.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/index.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom fastrtc import (ReplyOnPause, Stream, get_stt_model, get_tts_model)\nfrom openai import OpenAI\n\nsambanova_client = OpenAI(\n    api_key=os.getenv(\"SAMBANOVA_API_KEY\"), base_url=\"https://api.sambanova.ai/v1\"\n)\nstt_model = get_stt_model()\ntts_model = get_tts_model()\n\ndef echo(audio):\n    prompt = stt_model.stt(audio)\n    response = sambanova_client.chat.completions.create(\n        model=\"Meta-Llama-3.2-3B-Instruct\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=200,\n    )\n    prompt = response.choices[0].message.content\n    for audio_chunk in tts_model.stream_tts_sync(prompt):\n        yield audio_chunk\n\nstream = Stream(ReplyOnPause(echo), modality=\"audio\", mode=\"send-receive\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic WebRTC Audio Stream in Gradio\nDESCRIPTION: Demonstrates how to set up a basic WebRTC audio streaming component in Gradio with send-receive functionality. The example shows setting up an audio stream with a response handler and basic UI elements.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/gradio.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gradio as gr\nfrom fastrtc import WebRTC, ReplyOnPause\n\ndef response(audio: tuple[int, np.ndarray]):\n    \"\"\"This function must yield audio frames\"\"\"\n    ...\n    yield audio\n\n\nwith gr.Blocks() as demo:\n    gr.HTML(\n    \"\"\"\n    <h1 style='text-align: center'>\n    Chat (Powered by WebRTC ⚡️)\n    </h1>\n    \"\"\"\n    )\n    with gr.Column():\n        with gr.Group():\n            audio = WebRTC(\n                mode=\"send-receive\",\n                modality=\"audio\",\n            )\n        audio.stream(fn=ReplyOnPause(response),\n                    inputs=[audio], outputs=[audio],\n                    time_limit=60)\ndemo.launch()\n```\n\n----------------------------------------\n\nTITLE: Implementing Echo Audio Stream with FastRTC\nDESCRIPTION: A basic example that creates an audio stream which echoes back user audio when the user pauses. Uses the ReplyOnPause handler to manage turn-taking.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/README.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream, ReplyOnPause\nimport numpy as np\n\ndef echo(audio: tuple[int, np.ndarray]):\n    # The function will be passed the audio until the user pauses\n    # Implement any iterator that yields audio\n    # See \"LLM Voice Chat\" for a more complete example\n    yield audio\n\nstream = Stream(\n    handler=ReplyOnPause(echo),\n    modality=\"audio\", \n    mode=\"send-receive\",\n)\n```\n\n----------------------------------------\n\nTITLE: Building an LLM Voice Chat with Groq, Claude, and ElevenLabs\nDESCRIPTION: A more complex example that implements a voice chat system using Groq for transcription, Claude for text generation, and ElevenLabs for text-to-speech conversion.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/README.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import (\n    ReplyOnPause, AdditionalOutputs, Stream,\n    audio_to_bytes, aggregate_bytes_to_16bit\n)\nimport gradio as gr\nfrom groq import Groq\nimport anthropic\nfrom elevenlabs import ElevenLabs\n\ngroq_client = Groq()\nclaude_client = anthropic.Anthropic()\ntts_client = ElevenLabs()\n\n\n# See \"Talk to Claude\" in Cookbook for an example of how to keep \n# track of the chat history.\ndef response(\n    audio: tuple[int, np.ndarray],\n):\n    prompt = groq_client.audio.transcriptions.create(\n        file=(\"audio-file.mp3\", audio_to_bytes(audio)),\n        model=\"whisper-large-v3-turbo\",\n        response_format=\"verbose_json\",\n    ).text\n    response = claude_client.messages.create(\n        model=\"claude-3-5-haiku-20241022\",\n        max_tokens=512,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n    response_text = \" \".join(\n        block.text\n        for block in response.content\n        if getattr(block, \"type\", None) == \"text\"\n    )\n    iterator = tts_client.text_to_speech.convert_as_stream(\n        text=response_text,\n        voice_id=\"JBFqnCBsd6RMkjVDRZzb\",\n        model_id=\"eleven_multilingual_v2\",\n        output_format=\"pcm_24000\"\n        \n    )\n    for chunk in aggregate_bytes_to_16bit(iterator):\n        audio_array = np.frombuffer(chunk, dtype=np.int16).reshape(1, -1)\n        yield (24000, audio_array)\n\nstream = Stream(\n    modality=\"audio\",\n    mode=\"send-receive\",\n    handler=ReplyOnPause(response),\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up FastAPI Server Integration\nDESCRIPTION: Configures a FastAPI application with the stream and adds an optional route for serving an HTML interface. Includes instructions for running the server using uvicorn.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/README.md#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\napp = FastAPI()\nstream.mount(app)\n\n# Optional: Add routes\n@app.get(\"/\")\nasync def _():\n    return HTMLResponse(content=open(\"index.html\").read())\n\n# uvicorn app:app --host 0.0.0.0 --port 8000\n```\n\n----------------------------------------\n\nTITLE: Initializing ReplyOnPause Class in Python\nDESCRIPTION: Defines the ReplyOnPause class constructor with various parameters for configuring audio processing, pause detection, and reply generation. It sets up the handler for processing incoming audio streams and detecting pauses to trigger responses.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/reply_on_pause.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nReplyOnPause(\n    fn: ReplyFnGenerator,\n    startup_fn: Callable | None = None,\n    algo_options: AlgoOptions | None = None,\n    model_options: ModelOptions | None = None,\n    can_interrupt: bool = True,\n    expected_layout: Literal[\"mono\", \"stereo\"] = \"mono\",\n    output_sample_rate: int = 24000,\n    output_frame_size: int | None = None,  # Deprecated\n    input_sample_rate: int = 48000,\n    model: PauseDetectionModel | None = None,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Input/Output Video Streaming with FastRTC\nDESCRIPTION: Sets up bidirectional video streaming where webcam frames are captured, processed with a custom detection function, and returned to the client. The function handles a confidence threshold parameter via a slider component.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/video.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream\nimport gradio as gr\n\ndef detection(image, conf_threshold=0.3): # (1)\n    processed_frame = process_frame(image, conf_threshold)\n    return processed_frame # (2)\n\nstream = Stream(\n    handler=detection,\n    modality=\"video\",\n    mode=\"send-receive\", # (3)\n    additional_inputs=[\n        gr.Slider(minimum=0, maximum=1, step=0.01, value=0.3)\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: WebRTC with Additional Outputs for Multimodal Conversation\nDESCRIPTION: Shows how to implement a WebRTC component that handles both audio streaming and additional UI updates for a chatbot interface. Demonstrates the use of AdditionalOutputs for managing conversation state.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/gradio.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import AdditionalOutputs, WebRTC\n\ndef transcribe(audio: tuple[int, np.ndarray],\n               transformers_convo: list[dict],\n               gradio_convo: list[dict]):\n    response = model.generate(**inputs, max_length=256)\n    transformers_convo.append({\"role\": \"assistant\", \"content\": response})\n    gradio_convo.append({\"role\": \"assistant\", \"content\": response})\n    yield AdditionalOutputs(transformers_convo, gradio_convo)\n\n\nwith gr.Blocks() as demo:\n    gr.HTML(\n    \"\"\"\n    <h1 style='text-align: center'>\n    Talk to Qwen2Audio (Powered by WebRTC ⚡️)\n    </h1>\n    \"\"\"\n    )\n    transformers_convo = gr.State(value=[])\n    with gr.Row():\n        with gr.Column():\n            audio = WebRTC(\n                label=\"Stream\",\n                mode=\"send\",\n                modality=\"audio\",\n            )\n        with gr.Column():\n            transcript = gr.Chatbot(label=\"transcript\", type=\"messages\")\n\n    audio.stream(ReplyOnPause(transcribe),\n                inputs=[audio, transformers_convo, transcript],\n                outputs=[audio], time_limit=90)\n    audio.on_additional_outputs(lambda s,a: (s,a),\n                                outputs=[transformers_convo, transcript],\n                                queue=False, show_progress=\"hidden\")\n    demo.launch()\n```\n\n----------------------------------------\n\nTITLE: Setting up WebSocket Audio Connection with Processing in JavaScript\nDESCRIPTION: Creates an audio WebSocket connection that captures microphone input, processes it using mu-law encoding, sends it to the server, and handles received audio data for playback. The implementation includes audio context setup, WebSocket event handling, and audio buffer management.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/websocket_docs.md#2025-04-17_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Setup audio context and stream\nconst audioContext = new AudioContext();\nconst stream = await navigator.mediaDevices.getUserMedia({\n    audio: true\n});\n\n// Create WebSocket connection\nconst ws = new WebSocket(`${window.location.protocol === 'https:' ? 'wss:' : 'ws:'}//${window.location.host}/websocket/offer`);\n\nws.onopen = () => {\n    // Send initial start message with unique ID\n    ws.send(JSON.stringify({\n        event: \"start\",\n        websocket_id: generateId()  // Implement your own ID generator\n    }));\n\n    // Setup audio processing\n    const source = audioContext.createMediaStreamSource(stream);\n    const processor = audioContext.createScriptProcessor(2048, 1, 1);\n    source.connect(processor);\n    processor.connect(audioContext.destination);\n\n    processor.onaudioprocess = (e) => {\n        const inputData = e.inputBuffer.getChannelData(0);\n        const mulawData = convertToMulaw(inputData, audioContext.sampleRate);\n        const base64Audio = btoa(String.fromCharCode.apply(null, mulawData));\n        \n        if (ws.readyState === WebSocket.OPEN) {\n            ws.send(JSON.stringify({\n                event: \"media\",\n                media: {\n                    payload: base64Audio\n                }\n            }));\n        }\n    };\n};\n\n// Handle incoming audio\nconst outputContext = new AudioContext({ sampleRate: 24000 });\nlet audioQueue = [];\nlet isPlaying = false;\n\nws.onmessage = (event) => {\n    const data = JSON.parse(event.data);\n    if (data.event === \"media\") {\n        // Process received audio\n        const audioData = atob(data.media.payload);\n        const mulawData = new Uint8Array(audioData.length);\n        for (let i = 0; i < audioData.length; i++) {\n            mulawData[i] = audioData.charCodeAt(i);\n        }\n\n        // Convert mu-law to linear PCM\n        const linearData = alawmulaw.mulaw.decode(mulawData);\n        const audioBuffer = outputContext.createBuffer(1, linearData.length, 24000);\n        const channelData = audioBuffer.getChannelData(0);\n        \n        for (let i = 0; i < linearData.length; i++) {\n            channelData[i] = linearData[i] / 32768.0;\n        }\n\n        audioQueue.push(audioBuffer);\n        if (!isPlaying) {\n            playNextBuffer();\n        }\n    }\n};\n\nfunction playNextBuffer() {\n    if (audioQueue.length === 0) {\n        isPlaying = false;\n        return;\n    }\n\n    isPlaying = true;\n    const bufferSource = outputContext.createBufferSource();\n    bufferSource.buffer = audioQueue.shift();\n    bufferSource.connect(outputContext.destination);\n    bufferSource.onended = playNextBuffer;\n    bufferSource.start();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Stream Handler for Echo Functionality\nDESCRIPTION: This snippet shows how to create a custom StreamHandler class for more granular control over audio input and output. The example implements an echo functionality, demonstrating the use of receive, emit, and other required methods.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport gradio as gr\nfrom fastrtc import StreamHandler\nfrom queue import Queue\n\nclass EchoHandler(StreamHandler):\n    def __init__(self) -> None:\n        super().__init__()\n        self.queue = Queue()\n\n    def receive(self, frame: tuple[int, np.ndarray]) -> None: # (1)\n        self.queue.put(frame)\n\n    def emit(self) -> None: # (2)\n        return self.queue.get()\n    \n    def copy(self) -> StreamHandler:\n        return EchoHandler()\n    \n    def shutdown(self) -> None: # (3)\n        pass\n    \n    def start_up(self) -> None: # (4)\n        pass\n\nstream = Stream(\n    handler=EchoHandler(),\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Startup Function with ReplyOnPause\nDESCRIPTION: This snippet shows how to add a startup function to the ReplyOnPause class. The startup function is called when the connection is first established and can be used to generate initial responses or perform setup tasks.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import get_tts_model, Stream, ReplyOnPause\n\ntts_client = get_tts_model()\n\n\ndef echo(audio: tuple[int, np.ndarray]):\n    # Implement any iterator that yields audio\n    # See \"LLM Voice Chat\" for a more complete example\n    yield audio\n\n\ndef startup():\n    for chunk in tts_client.stream_tts_sync(\"Welcome to the echo audio demo!\"):\n        yield chunk\n\n\nstream = Stream(\n    handler=ReplyOnPause(echo, startup_fn=startup),\n    modality=\"audio\",\n    mode=\"send-receive\",\n    ui_args={\"title\": \"Echo Audio\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Gemini Video Handling Functions in FastRTC\nDESCRIPTION: This code shows how to implement the video_receive and video_emit methods for an AsyncAudioVideoStreamHandler that connects with the Gemini multimodal API. The video_receive method sends webcam frames to the Gemini server at 1-second intervals to avoid API flooding, while video_emit returns frames to the client.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio-video.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nasync def video_receive(self, frame: np.ndarray):\n    \"\"\"Send video frames to the server\"\"\"\n    if self.session:\n        # send image every 1 second\n        # otherwise we flood the API\n        if time.time() - self.last_frame_time > 1:\n            self.last_frame_time = time.time()\n            await self.session.send(encode_image(frame))\n            if self.latest_args[2] is not None:\n                await self.session.send(encode_image(self.latest_args[2]))\n    self.video_queue.put_nowait(frame)\n\nasync def video_emit(self) -> VideoEmitType:\n    \"\"\"Return video frames to the client\"\"\"\n    return await self.video_queue.get()\n```\n\n----------------------------------------\n\nTITLE: Receiving Audio Frames in ReplyOnPause\nDESCRIPTION: Implements the receive method for the ReplyOnPause class. It processes incoming audio frames, detects pauses, and manages the reply generation process, including handling interruptions if enabled.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/reply_on_pause.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nreceive(frame: tuple[int, np.ndarray]) -> None\n```\n\n----------------------------------------\n\nTITLE: Mounting FastRTC Stream in a FastAPI Application\nDESCRIPTION: Shows how to integrate a FastRTC stream with a FastAPI application, including adding custom routes and serving an HTML frontend.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/index.md#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\napp = FastAPI()\nstream.mount(app)\n\n# Optional: Add routes\n@app.get(\"/\")\nasync def _():\n    return HTMLResponse(content=open(\"index.html\").read())\n\n# uvicorn app:app --host 0.0.0.0 --port 8000\n```\n\n----------------------------------------\n\nTITLE: Implementing Output Hooks with FastAPI StreamingResponse\nDESCRIPTION: Example of creating a GET endpoint in FastAPI that streams additional output data from a FastRTC Stream. Uses StreamingResponse to provide server-sent events (SSE) containing the output data.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/streams.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi.responses import StreamingResponse\n\n@app.get(\"/updates\")\nasync def stream_updates(webrtc_id: str):\n    async def output_stream():\n        async for output in stream.output_stream(webrtc_id):\n            # Output is the AdditionalOutputs instance\n            # Be sure to serialize it however you would like\n            yield f\"data: {output.args[0]}\\n\\n\"\n\n    return StreamingResponse(\n        output_stream(), \n        media_type=\"text/event-stream\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Launching FastRTC Stream with Gradio UI\nDESCRIPTION: Command to launch a FastRTC Stream with the built-in Gradio UI for testing and demonstration purposes.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/streams.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstream.ui.launch()\n```\n\n----------------------------------------\n\nTITLE: Serving Custom Frontend with FastRTC and FastAPI\nDESCRIPTION: Example of serving a custom HTML frontend alongside a FastRTC Stream mounted on a FastAPI application. Demonstrates how to create a route that serves HTML content for a custom interface.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/streams.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi.responses import HTMLResponse\nfrom fastapi import FastAPI\nfrom fastrtc import Stream\n\nstream = Stream(...)\n\napp = FastAPI()\nstream.mount(app)\n\n# Serve a custom frontend\n@app.get(\"/\")\nasync def serve_frontend():\n    return HTMLResponse(content=open(\"index.html\").read())\n```\n\n----------------------------------------\n\nTITLE: FastRTC TTS Model Integration Example\nDESCRIPTION: Example implementation showing how to integrate a custom TTS model with FastRTC, including stream setup and handler configuration.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/text_to_speech_gallery.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream, AdditionalOutputs, ReplyOnPause, get_stt_model\nfrom your_model import YourModel\n\nmodel = YourModel() # implement the TTSModel protocol\noptions = YourTTSOptions() # implement the TTSOptions protocol\nstt_model = get_stt_model(model)\n\ndef echo(audio):\n    text = stt_model.tts(audio)\n    for chunk in model.stream_tts(text, options):\n        yield chunk\n\nstream = Stream(ReplyOnPause(echo), mode=\"send-receive\", modality=\"audio\",\n                additional_outputs=[gr.Textbox(label=\"Transcription\")],\n                additional_outputs_handler=lambda old,new:old + new)\nstream.ui.launch()\n```\n\n----------------------------------------\n\nTITLE: Retrieving TURN Credentials Asynchronously in Python\nDESCRIPTION: This function asynchronously retrieves TURN credentials from a specified provider (Cloudflare, Hugging Face, or Twilio). It can be used directly with the Stream class for WebRTC connections in Gradio UI or with FastAPI.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/credentials.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nasync def get_turn_credentials_async(\n    method: Literal[\"hf\", \"twilio\", \"cloudflare\"] = \"cloudflare\",\n    **kwargs\n):\n```\n\n----------------------------------------\n\nTITLE: Fetching Output Data from WebRTC Stream in Python\nDESCRIPTION: Shows how to implement server-sent events to fetch additional output data from the WebRTC stream. The Python code demonstrates how to set up a streaming endpoint that continuously yields data from the stream's outputs.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/webrtc_docs.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@stream.get(\"/outputs\")\ndef _(webrtc_id: str)\n    async def get_outputs():\n        while True:\n            for output in stream.get_output(webrtc_id):\n                # Serialize to a string prior to this step\n                yield f\"data: {output}\\n\\n\"\n            await\n    return StreamingResponse(get_outputs(),  media_type=\"text/event-stream\")\n```\n\n----------------------------------------\n\nTITLE: Implementing WebRTC Error Handling in Gradio UI\nDESCRIPTION: Example demonstrating how to implement error handling in a Gradio WebRTC application using WebRTCError. The code shows audio generation with a custom error message after a specified number of steps.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/faq.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef generation(num_steps):\n    for _ in range(num_steps):\n        segment = AudioSegment.from_file(\n            \"/Users/freddy/sources/gradio/demo/audio_debugger/cantina.wav\"\n        )\n        yield (\n            segment.frame_rate,\n            np.array(segment.get_array_of_samples()).reshape(1, -1),\n        )\n        time.sleep(3.5)\n    raise WebRTCError(\"This is a test error\")\n\nwith gr.Blocks() as demo:\n    audio = WebRTC(\n    label=\"Stream\",\n    mode=\"receive\",\n    modality=\"audio\",\n    )\n    num_steps = gr.Slider(\n        label=\"Number of Steps\",\n        minimum=1,\n        maximum=10,\n        step=1,\n        value=5,\n    )\n    button = gr.Button(\"Generate\")\n\n    audio.stream(\n        fn=generation, inputs=[num_steps], outputs=[audio], trigger=button.click\n    )\n\ndemo.launch()\n```\n\n----------------------------------------\n\nTITLE: Creating a Webcam Stream with FastRTC\nDESCRIPTION: Example showing how to create a video stream that flips the webcam input vertically. Demonstrates using the Stream class with video modality and a simple image processing function.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/index.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream\nimport numpy as np\n\n\ndef flip_vertically(image):\n    return np.flip(image, axis=0)\n\n\nstream = Stream(\n    handler=flip_vertically,\n    modality=\"video\",\n    mode=\"send-receive\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Object Detection with FastRTC and YOLOv10\nDESCRIPTION: Example showing how to create a video stream with real-time object detection using YOLOv10. Includes loading a model from Hugging Face Hub and adding a confidence threshold slider to the UI.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/index.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream\nimport gradio as gr\nimport cv2\nfrom huggingface_hub import hf_hub_download\nfrom .inference import YOLOv10\n\nmodel_file = hf_hub_download(\n    repo_id=\"onnx-community/yolov10n\", filename=\"onnx/model.onnx\"\n)\n\n# git clone https://huggingface.co/spaces/fastrtc/object-detection\n# for YOLOv10 implementation\nmodel = YOLOv10(model_file)\n\ndef detection(image, conf_threshold=0.3):\n    image = cv2.resize(image, (model.input_width, model.input_height))\n    new_image = model.detect_objects(image, conf_threshold)\n    return cv2.resize(new_image, (500, 500))\n\nstream = Stream(\n    handler=detection,\n    modality=\"video\", \n    mode=\"send-receive\",\n    additional_inputs=[\n        gr.Slider(minimum=0, maximum=1, step=0.01, value=0.3)\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing StreamHandlerBase Class in Python\nDESCRIPTION: Base class constructor for handling media streams in FastRTC. Configures audio properties like channel layout, sample rates and provides common functionality for stream management.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nStreamHandlerBase(\n    expected_layout: Literal[\"mono\", \"stereo\"] = \"mono\",\n    output_sample_rate: int = 24000,\n    output_frame_size: int | None = None,  # Deprecated\n    input_sample_rate: int = 48000,\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Turn-taking Algorithm Implementation\nDESCRIPTION: Python code showing how to implement a custom turn-taking algorithm by subclassing ReplyOnPause and overriding determine_pause method.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/turn_taking_gallery.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc.reply_on_pause import ReplyOnPause, AppState\nclass MyTurnTakingAlgorithm(ReplyOnPause):\n    def determine_pause(self, audio: np.ndarray, sampling_rate: int, state: AppState) -> bool:\n        return super().determine_pause(audio, sampling_rate, state)\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-to-Client Video Streaming with FastRTC\nDESCRIPTION: Creates a one-way video stream from server to client using OpenCV to read frames from a remote video source. The function yields frames continuously in a generator pattern.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/video.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream\n\ndef generation():\n    url = \"https://download.tsi.telecom-paristech.fr/gpac/dataset/dash/uhd/mux_sources/hevcds_720p30_2M.mp4\"\n    cap = cv2.VideoCapture(url)\n    iterating = True\n    while iterating:\n        iterating, frame = cap.read()\n        yield frame\n\nstream = Stream(\n    handler=generation,\n    modality=\"video\",\n    mode=\"receive\"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Real-time Object Detection with YOLOv10\nDESCRIPTION: Setting up a real-time object detection system using the YOLOv10 model downloaded from Hugging Face Hub. The example includes model loading code.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/README.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream\nimport gradio as gr\nimport cv2\nfrom huggingface_hub import hf_hub_download\nfrom .inference import YOLOv10\n\nmodel_file = hf_hub_download(\n    repo_id=\"onnx-community/yolov10n\", filename=\"onnx/model.onnx\"\n)\n\n# git clone https://huggingface.co/spaces/fastrtc/object-detection\n```\n\n----------------------------------------\n\nTITLE: FastRTC Stream Handler Implementation\nDESCRIPTION: Example Python code showing how to implement a custom STT model in a FastRTC stream handler with transcription output.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/speech_to_text_gallery.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream, AdditionalOutputs, ReplyOnPause\nfrom your_model import YourModel\n\nmodel = YourModel() # implement the STTModel protocol\n\ndef echo(audio):\n    text = model.stt(audio)\n    yield AdditionalOutputs(text)\n\nstream = Stream(ReplyOnPause(echo), mode=\"send-receive\", modality=\"audio\",\n                additional_outputs=[gr.Textbox(label=\"Transcription\")],\n                additional_outputs_handler=lambda old,new:old + new)\nstream.ui.launch()\n```\n\n----------------------------------------\n\nTITLE: Determining Pauses in Audio Stream\nDESCRIPTION: Implements the determine_pause method in ReplyOnPause for analyzing audio chunks to detect significant pauses after speech. It uses a VAD model to measure speech duration and updates the application state accordingly.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/reply_on_pause.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndetermine_pause(audio: np.ndarray, sampling_rate: int, state: AppState) -> bool\n```\n\n----------------------------------------\n\nTITLE: Emitting Reply Chunks in ReplyOnPause\nDESCRIPTION: Defines the emit method for ReplyOnPause, which produces output chunks from the reply generator. It handles both synchronous and asynchronous generators, manages state, and yields the next item from the generator when a pause is detected.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/reply_on_pause.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nemit() -> EmitType | None\n```\n\n----------------------------------------\n\nTITLE: Initializing YOLOv10 Detection Function\nDESCRIPTION: Sets up a YOLOv10 model and defines a detection function that processes images with configurable confidence threshold. The function resizes input images, performs object detection, and returns processed images at 500x500 resolution.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/README.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = YOLOv10(model_file)\n\ndef detection(image, conf_threshold=0.3):\n    image = cv2.resize(image, (model.input_width, model.input_height))\n    new_image = model.detect_objects(image, conf_threshold)\n    return cv2.resize(new_image, (500, 500))\n```\n\n----------------------------------------\n\nTITLE: Configuring Voice Activity Detection Parameters\nDESCRIPTION: Demonstrates setting up Voice Activity Detection with customizable parameters for audio chunks and speech thresholds.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/advanced-configuration.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import AlgoOptions, ReplyOnPause, Stream\n\noptions = AlgoOptions(audio_chunk_duration=0.6,\n                      started_talking_threshold=0.2,\n                      speech_threshold=0.1,\n                      )\n\nStream(\n    handler=ReplyOnPause(..., algo_options=algo_options),\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Video Stream with Vertical Flip in FastRTC\nDESCRIPTION: Example of creating a FastRTC Stream object that receives video input, flips it vertically, and returns the processed frame. Demonstrates the basic structure and core parameters of a Stream object initialization.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/streams.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream\nimport gradio as gr\nimport numpy as np\n\ndef detection(image, slider):\n    return np.flip(image, axis=0)\n\nstream = Stream(\n    handler=detection, # (1)\n    modality=\"video\", # (2)\n    mode=\"send-receive\", # (3)\n    additional_inputs=[\n        gr.Slider(minimum=0, maximum=1, step=0.01, value=0.3) # (4)\n    ],\n    additional_outputs=None, # (5)\n    additional_outputs_handler=None # (6)\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Reply On Stopwords for Voice-Activated Responses\nDESCRIPTION: This code demonstrates how to use the ReplyOnStopWords class to trigger responses when specific stop words are detected in the audio input. It's similar to ReplyOnPause but adds a stop_words parameter for customization.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream, ReplyOnStopWords\n\ndef response(audio: tuple[int, np.ndarray]):\n    \"\"\"This function must yield audio frames\"\"\"\n    ...\n    for numpy_array in generated_audio:\n        yield (sampling_rate, numpy_array, \"mono\")\n\nstream = Stream(\n    handler=ReplyOnStopWords(generate,\n                            input_sample_rate=16000,\n                            stop_words=[\"computer\"]), # (1)\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n```\n\n----------------------------------------\n\nTITLE: WebRTC Offer Handler\nDESCRIPTION: Async method to handle incoming WebRTC offers via HTTP POST. Processes SDP offers and ICE candidates to establish WebRTC connections.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync offer(body: Body)\n```\n\n----------------------------------------\n\nTITLE: Implementing Receive Method for Stream Processing in Python\nDESCRIPTION: Abstract method for processing incoming audio frames synchronously. Takes a tuple containing sample rate and audio data array.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\nreceive(frame: tuple[int, npt.NDArray[np.int16]]) -> None\n```\n\n----------------------------------------\n\nTITLE: Implementing Frame Skipping for Performance in FastRTC Video Streams\nDESCRIPTION: Demonstrates how to improve performance for slow processing functions by enabling frame skipping, which prevents lag by dropping frames that arrive while processing is still running.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/video.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport numpy as np\nfrom fastrtc import Stream, VideoStreamHandler\n\n\ndef process_image(image):\n    time.sleep(\n        0.2\n    )  # Simulating 200ms processing time per frame; input arrives faster (30 FPS).\n    return np.flip(image, axis=0)\n\n\nstream = Stream(\n    handler=VideoStreamHandler(process_image, skip_frames=True),\n    modality=\"video\",\n    mode=\"send-receive\",\n)\n\nstream.ui.launch()\n```\n\n----------------------------------------\n\nTITLE: Configuring Stream Object with Gradio Interface\nDESCRIPTION: Creates a Stream object for video processing with the detection handler, including a confidence threshold slider input ranging from 0 to 1.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/README.md#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstream = Stream(\n    handler=detection,\n    modality=\"video\", \n    mode=\"send-receive\",\n    additional_inputs=[\n        gr.Slider(minimum=0, maximum=1, step=0.01, value=0.3)\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Stream Handler in Python\nDESCRIPTION: Example of an asynchronous echo handler implementation using AsyncStreamHandler class. Handles audio frames with configurable input sample rate and async queue management.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import AsyncStreamHandler, wait_for_item, Stream\nimport asyncio\nimport numpy as np\n\nclass AsyncEchoHandler(AsyncStreamHandler):\n    \"\"\"Simple Async Echo Handler\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__(input_sample_rate=24000)\n        self.queue = asyncio.Queue()\n\n    async def receive(self, frame: tuple[int, np.ndarray]) -> None:\n        await self.queue.put(frame)\n\n    async def emit(self) -> None:\n        return await wait_for_item(self.queue)\n\n    def copy(self):\n        return AsyncEchoHandler()\n\n    async def shutdown(self):\n        pass\n\n    async def start_up(self) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Text-to-Speech Options Configuration\nDESCRIPTION: Shows how to customize TTS options using KokoroTTSOptions for voice, speed, and language settings.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import KokoroTTSOptions, get_tts_model\n\nmodel = get_tts_model(model=\"kokoro\")\n\noptions = KokoroTTSOptions(\n    voice=\"af_heart\",\n    speed=1.0,\n    lang=\"en-us\"\n)\n\naudio = model.tts(\"Hello, world!\", options=options)\n```\n\n----------------------------------------\n\nTITLE: Configuring Voice Detection Parameters in ReplyOnPause\nDESCRIPTION: This snippet shows how to customize voice detection parameters for the ReplyOnPause class by passing AlgoOptions and SileroVadOptions. It allows fine-tuning of the voice activity detection behavior.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import AlgoOptions, SileroVadOptions\n\nstream = Stream(\n    handler=ReplyOnPause(\n        response,\n        algo_options=AlgoOptions(\n            audio_chunk_duration=0.6,\n            started_talking_threshold=0.2,\n            speech_threshold=0.1\n        ),\n        model_options=SileroVadOptions(\n            threshold=0.5,\n            min_speech_duration_ms=250,\n            min_silence_duration_ms=100\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: TTS Model Protocol Definition\nDESCRIPTION: Python protocol definition for TTS models, specifying required methods for text-to-speech conversion including synchronous and asynchronous streaming options.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/text_to_speech_gallery.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TTSModel(Protocol):\n    def tts(\n        self, text: str, options: TTSOptions | None = None\n    ) -> tuple[int, NDArray[np.float32 | np.int16]]: ...\n\n    async def stream_tts(\n        self, text: str, options: TTSOptions | None = None\n    ) -> AsyncGenerator[tuple[int, NDArray[np.float32 | np.int16]], None]: ...\n\n    async def stream_tts_sync(\n        self, text: str, options: TTSOptions | None = None\n    ) -> Generator[tuple[int, NDArray[np.float32 | np.int16]], None, None]: ...\n```\n\n----------------------------------------\n\nTITLE: Initializing AsyncAudioVideoStreamHandler in FastRTC\nDESCRIPTION: Constructor for the AsyncAudioVideoStreamHandler abstract base class. It handles asynchronous audio and video processing with configurable audio parameters like channel layout and sample rates.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nAsyncAudioVideoStreamHandler(\n    expected_layout: Literal[\"mono\", \"stereo\"] = \"mono\",\n    output_sample_rate: int = 24000,\n    output_frame_size: int | None = None,  # Deprecated\n    input_sample_rate: int = 48000,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing StreamHandler Class Constructor in Python\nDESCRIPTION: Constructor for synchronous stream handler class. Inherits from StreamHandlerBase and provides interface for synchronous audio stream processing.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nStreamHandler(\n    expected_layout: Literal[\"mono\", \"stereo\"] = \"mono\",\n    output_sample_rate: int = 24000,\n    output_frame_size: int | None = None,  # Deprecated\n    input_sample_rate: int = 48000,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Stream Class in Python\nDESCRIPTION: Class constructor for Stream that handles WebRTC connections. Supports different modes and modalities for audio/video streaming with customizable configuration options.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nStream(\n    handler: HandlerType,\n    *,\n    additional_outputs_handler: Callable | None = None,\n    mode: Literal[\"send-receive\", \"receive\", \"send\"] = \"send-receive\",\n    modality: Literal[\"video\", \"audio\", \"audio-video\"] = \"video\",\n    concurrency_limit: int | None | Literal[\"default\"] = \"default\",\n    time_limit: float | None = None,\n    allow_extra_tracks: bool = False,\n    rtp_params: dict[str, Any] | None = None,\n    rtc_configuration: dict[str, Any] | None = None,\n    track_constraints: dict[str, Any] | None = None,\n    additional_inputs: list[Component] | None = None,\n    additional_outputs: list[Component] | None = None,\n    ui_args: UIArgs | None = None\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Audio Stream Parameters in FastRTC\nDESCRIPTION: Configuration for audio stream settings to disable echo cancellation and set specific audio track constraints including noise suppression, gain control, and sampling parameters.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/faq.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nstream = Stream(\n    track_constraints={\n            \"echoCancellation\": False,\n            \"noiseSuppression\": {\"exact\": True},\n            \"autoGainControl\": {\"exact\": True},\n            \"sampleRate\": {\"ideal\": 24000},\n            \"sampleSize\": {\"ideal\": 16},\n            \"channelCount\": {\"exact\": 1},\n        },\n    rtc_configuration=None,\n    mode=\"send-receive\",\n    modality=\"audio\",\n)\n```\n\n----------------------------------------\n\nTITLE: Streaming Additional Outputs with FastAPI in Python\nDESCRIPTION: Shows how to implement server-sent events using FastAPI to stream additional outputs from a WebRTC connection. The example creates an endpoint that yields serialized output data in a format suitable for event streaming.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/api.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi.responses import StreamingResponse\n\n@app.get(\"/updates\")\nasync def stream_updates(webrtc_id: str):\n    async def output_stream():\n        async for output in stream.output_stream(webrtc_id):\n            # Output is the AdditionalOutputs instance\n            # Be sure to serialize it however you would like\n            yield f\"data: {output.args[0]}\\n\\n\"\n\n    return StreamingResponse(\n        output_stream(), \n        media_type=\"text/event-stream\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring FastRTC with Cloudflare TURN Credentials\nDESCRIPTION: Demonstrates how to set up FastRTC Stream with Cloudflare TURN credentials using a Hugging Face token. Includes both async and sync credential retrieval methods.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/deployment.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream, get_cloudflare_turn_credentials_async, get_cloudflare_turn_credentials\n\n# Make sure the HF_TOKEN environment variable is set\n# Or pass in a callable with all arguments set\n\n# make sure you don't commit your token to git!\nTOKEN = \"hf_...\"\nasync def get_credentials():\n    return await get_cloudflare_turn_credentials_async(hf_token=TOKEN)\n\nstream = Stream(\n    handler=...,\n    rtc_configuration=get_credentials,\n    server_rtc_configuration=get_cloudflare_turn_credentials(ttl=360_000)\n    modality=\"audio\",\n    mode=\"send-receive\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating FastRTC Stream with Additional Outputs\nDESCRIPTION: Example of a FastRTC Stream that returns additional output data beyond the processed frame. The handler returns both the processed frame and an AdditionalOutputs instance containing the number of detected objects.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/streams.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream, AdditionalOutputs\nimport gradio as gr\n\ndef detection(image, conf_threshold=0.3):\n    processed_frame, n_objects = process_frame(image, conf_threshold)\n    return processed_frame, AdditionalOutputs(n_objects)\n\nstream = Stream(\n    handler=detection,\n    modality=\"video\",\n    mode=\"send-receive\",\n    additional_inputs=[\n        gr.Slider(minimum=0, maximum=1, step=0.01, value=0.3)\n    ],\n    additional_outputs=[gr.Number()], # (5)\n    additional_outputs_handler=lambda component, n_objects: n_objects\n)\n```\n\n----------------------------------------\n\nTITLE: Processing Server Response for WebRTC Connection\nDESCRIPTION: Processes the server's response to a WebRTC offer. The response is parsed as JSON and used to set the remote description on the peer connection, completing the connection setup.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/api.md#2025-04-17_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst serverResponse = await response.json();\nawait peerConnection.setRemoteDescription(serverResponse);\n```\n\n----------------------------------------\n\nTITLE: Creating an Echo Audio Stream with FastRTC\nDESCRIPTION: Example showing how to create an audio stream that echoes back user input when they pause speaking. Uses the ReplyOnPause handler to process audio data.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/index.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream, ReplyOnPause\nimport numpy as np\n\ndef echo(audio: tuple[int, np.ndarray]):\n    # The function will be passed the audio until the user pauses\n    # Implement any iterator that yields audio\n    # See \"LLM Voice Chat\" for a more complete example\n    yield audio\n\nstream = Stream(\n    handler=ReplyOnPause(echo),\n    modality=\"audio\", \n    mode=\"send-receive\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Track Constraints for WebRTC Stream in Python\nDESCRIPTION: Demonstrates how to set video track constraints for width, height and frame rate in a WebRTC stream.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/advanced-configuration.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrack_constraints = {\n    \"width\": {\"exact\": 500},\n    \"height\": {\"exact\": 500},\n    \"frameRate\": {\"ideal\": 30},\n}\nwebrtc = Stream(\n    handler=...,\n    track_constraints=track_constraints,\n    modality=\"video\",\n    mode=\"send-receive\")\n```\n\n----------------------------------------\n\nTITLE: Mounting FastRTC Stream on FastAPI App\nDESCRIPTION: Example of mounting a FastRTC Stream on a FastAPI application, enabling custom routing and integration with existing web services.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/streams.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = FastAPI()\nstream.mount(app)\n\n# uvicorn app:app --host 0.0.0.0 --port 8000\n```\n\n----------------------------------------\n\nTITLE: Sending WebRTC Offer to Server\nDESCRIPTION: Sends a WebRTC offer to the server via a POST request to '/webrtc/offer' endpoint. The request includes the SDP, offer type, and WebRTC ID to establish the connection.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/api.md#2025-04-17_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst response = await fetch('/webrtc/offer', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({\n            sdp: offer.sdp,\n            type: offer.type,\n            webrtc_id: webrtc_id\n        })\n    });\n```\n\n----------------------------------------\n\nTITLE: Initializing AsyncStreamHandler Class in Python\nDESCRIPTION: Constructor for asynchronous stream handler. Inherits from StreamHandlerBase and defines asynchronous interface using coroutines.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nAsyncStreamHandler(\n    expected_layout: Literal[\"mono\", \"stereo\"] = \"mono\",\n    output_sample_rate: int = 24000,\n    output_frame_size: int | None = None,  # Deprecated\n    input_sample_rate: int = 48000,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Input Audio Sampling Rate\nDESCRIPTION: Shows how to set the input sampling rate for audio streams.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/advanced-configuration.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import ReplyOnPause, Stream\n\nstream = Stream(\n    handler=ReplyOnPause(..., input_sampling_rate=24000),\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Additional Inputs in FastRTC with Python\nDESCRIPTION: Demonstrates how to update inputs for a handler using a POST request. The example uses Pydantic for data validation and sets input data that will be passed to the handler on the next call.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/api.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\nclass InputData(BaseModel):\n    webrtc_id: str\n    conf_threshold: float = Field(ge=0, le=1)\n\n\n@app.post(\"/input_hook\")\nasync def _(data: InputData):\n    stream.set_input(data.webrtc_id, data.conf_threshold)\n```\n\n----------------------------------------\n\nTITLE: Handling ICE Candidates in WebRTC Connection\nDESCRIPTION: Sets up an event handler for ICE candidates generated by the peer connection. When candidates are available, they are sent to the server via a POST request to '/webrtc/offer' endpoint with the candidate information, WebRTC ID, and event type.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/api.md#2025-04-17_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\npeerConnection.onicecandidate = ({ candidate }) => {\n        if (candidate) {\n            console.debug(\"Sending ICE candidate\", candidate);\n            fetch('/webrtc/offer', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({\n                candidate: candidate.toJSON(),\n                webrtc_id: webrtc_id,\n                type: \"ice-candidate\",\n                    })\n                })\n            }\n    };\n```\n\n----------------------------------------\n\nTITLE: Initializing AudioVideoStreamHandler Class in Python\nDESCRIPTION: Constructor for combined audio/video stream handler. Inherits from StreamHandler and adds methods for synchronous video processing.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nAudioVideoStreamHandler(\n    expected_layout: Literal[\"mono\", \"stereo\"] = \"mono\",\n    output_sample_rate: int = 24000,\n    output_frame_size: int | None = None,  # Deprecated\n    input_sample_rate: int = 48000,\n)\n```\n\n----------------------------------------\n\nTITLE: Stopping WebSocket Connection\nDESCRIPTION: Implements a function to gracefully terminate the WebSocket connection by sending a stop event before closing. This ensures proper cleanup of resources on both client and server sides.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/websocket_docs.md#2025-04-17_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nfunction stop(ws) {\n    if (ws) {\n        ws.send(JSON.stringify({\n            event: \"stop\"\n        }));\n        ws.close();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Mounting Stream to FastAPI\nDESCRIPTION: Method to mount the stream's API endpoints onto a FastAPI application. Adds routes for WebRTC, telephone, and websocket functionality.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmount(app: FastAPI, path: str = \"\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Server-Sent Events for Additional Output\nDESCRIPTION: Configures an EventSource to receive additional outputs from the server using Server-Sent Events (SSE). This provides a channel for receiving non-audio data related to the WebRTC session.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/websocket_docs.md#2025-04-17_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst eventSource = new EventSource('/outputs?webrtc_id=' + websocket_id);\neventSource.addEventListener(\"output\", (event) => {\n    const eventJson = JSON.parse(event.data);\n    // Handle the output data here\n    console.log(\"Received output:\", eventJson);\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Webcam Stream with Vertical Flip Processing\nDESCRIPTION: A simple video processing example that captures webcam input, flips the image vertically, and sends it back to the client in real-time.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/README.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream\nimport numpy as np\n\n\ndef flip_vertically(image):\n    return np.flip(image, axis=0)\n\n\nstream = Stream(\n    handler=flip_vertically,\n    modality=\"video\",\n    mode=\"send-receive\",\n)\n```\n\n----------------------------------------\n\nTITLE: Telephone WebSocket Handler\nDESCRIPTION: Async method to handle WebSocket connections for streaming audio over Twilio phone calls.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync telephone_handler(websocket: WebSocket)\n```\n\n----------------------------------------\n\nTITLE: Initializing ReplyOnStopWords Class in Python\nDESCRIPTION: Defines the ReplyOnStopWords class constructor, which extends ReplyOnPause to trigger responses based on stop words followed by pauses. It adds functionality for Speech-to-Text processing to detect specified stop words in the audio stream.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/reply_on_pause.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nReplyOnStopWords(\n    fn: ReplyFnGenerator,\n    stop_words: list[str],\n    startup_fn: Callable | None = None,\n    algo_options: AlgoOptions | None = None,\n    model_options: ModelOptions | None = None,\n    can_interrupt: bool = True,\n    expected_layout: Literal[\"mono\", \"stereo\"] = \"mono\",\n    output_sample_rate: int = 24000,\n    output_frame_size: int | None = None,  # Deprecated\n    input_sample_rate: int = 48000,\n    model: PauseDetectionModel | None = None,\n)\n```\n\n----------------------------------------\n\nTITLE: FastPhone Service Integration\nDESCRIPTION: Method to launch the FastPhone service for telephone integration. Sets up a local server and creates a public tunnel for phone call handling.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfastphone(\n    token: str | None = None,\n    host: str = \"127.0.0.1\",\n    port: int = 8000,\n    **kwargs\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Input Data Over WebRTC Data Channel in JavaScript and Python\nDESCRIPTION: Demonstrates how to send input data from the frontend to the backend over a WebRTC data channel. Includes both the JavaScript code for setting up a message handler for the data channel and Python code for handling the input hook on the server side.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/webrtc_docs.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@stream.post(\"/input_hook\")\ndef _(data: PydanticBody):\n    stream.set_inputs(data.webrtc_id, data.inputs)\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst data_channel = peerConnection.createDataChannel(\"text\");\n\ndata_channel.onmessage = (event) => {\n    event_json = JSON.parse(event.data);\n    if (event_json.type === \"send_input\") {\n        fetch('/input_hook', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: inputs\n        }\n            )\n        };\n    };\n```\n\n----------------------------------------\n\nTITLE: Setting Up Telephone Integration with FastRTC\nDESCRIPTION: Example of using the fastphone() method to enable telephone integration with a FastRTC Stream. Provides a temporary phone number for connecting to the stream via telephone, with a usage limit of 10 minutes per month.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/streams.md#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Launch with a temporary phone number\nstream.fastphone(\n    # Optional: If None, will use the default token in your machine or read from the HF_TOKEN environment variable\n    token=\"your_hf_token\",  \n    host=\"127.0.0.1\",\n    port=8000\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Input Hooks for FastRTC Stream with FastAPI\nDESCRIPTION: Example of creating a POST endpoint in FastAPI to update input parameters for a FastRTC Stream. Uses Pydantic for data validation and the stream's set_input method for parameter updating.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/streams.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom fastapi import FastAPI\n\nclass InputData(BaseModel):\n    webrtc_id: str\n    conf_threshold: float = Field(ge=0, le=1)\n\napp = FastAPI()\nstream.mount(app)\n\n@app.post(\"/input_hook\")\nasync def _(data: InputData):\n    stream.set_input(data.webrtc_id, data.conf_threshold)\n```\n\n----------------------------------------\n\nTITLE: Setting Output Frame Rate in FastRTC Video Streams\nDESCRIPTION: Shows how to control the output frame rate of a video stream by setting the fps parameter. This example also demonstrates calculating and displaying the actual FPS as additional output.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/video.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef generation():\n    url = \"https://github.com/user-attachments/assets/9636dc97-4fee-46bb-abb8-b92e69c08c71\"\n    cap = cv2.VideoCapture(url)\n    iterating = True\n\n    # FPS calculation variables\n    frame_count = 0\n    start_time = time.time()\n    fps = 0\n\n    while iterating:\n        iterating, frame = cap.read()\n\n        # Calculate and print FPS\n        frame_count += 1\n        elapsed_time = time.time() - start_time\n        if elapsed_time >= 1.0:  # Update FPS every second\n            fps = frame_count / elapsed_time\n            yield frame, AdditionalOutputs(fps)\n            frame_count = 0\n            start_time = time.time()\n        else:\n            yield frame\n\n\nstream = Stream(\n    handler=VideoStreamHandler(generation, fps=60),\n    modality=\"video\",\n    mode=\"receive\",\n    additional_outputs=[gr.Number(label=\"FPS\")],\n    additional_outputs_handler=lambda prev, cur: cur,\n)\n\nstream.ui.launch()\n```\n\n----------------------------------------\n\nTITLE: Emitting Video Frames Asynchronously in FastRTC\nDESCRIPTION: Abstract method declaration for asynchronously producing the next output video frame. Implementations should return a VideoEmitType object and use the async/await pattern.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\nasync video_emit() -> VideoEmitType\n```\n\n----------------------------------------\n\nTITLE: Retrieving TURN Credentials Synchronously in Python\nDESCRIPTION: This function synchronously retrieves TURN credentials from a specified provider (Cloudflare, Hugging Face, or Twilio). It can be used directly with the Stream class for WebRTC connections in Gradio UI or with FastAPI.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/credentials.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_turn_credentials(\n    method: Literal[\"hf\", \"twilio\", \"cloudflare\"] = \"cloudflare\",\n    **kwargs\n):\n```\n\n----------------------------------------\n\nTITLE: Text-to-Speech Model Usage in Python\nDESCRIPTION: Demonstrates how to use the text-to-speech functionality with the Kokoro model, including sync and async streaming options.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import get_tts_model\n\nmodel = get_tts_model(model=\"kokoro\")\n\nfor audio in model.stream_tts_sync(\"Hello, world!\"):\n    yield audio\n\nasync for audio in model.stream_tts(\"Hello, world!\"):\n    yield audio\n\naudio = model.tts(\"Hello, world!\")\n```\n\n----------------------------------------\n\nTITLE: FastRTC Configuration with Self-Hosted TURN Server\nDESCRIPTION: Example configuration for connecting FastRTC to a self-hosted TURN server on AWS EC2.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/deployment.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream\nrtc_configuration = {\n    \"iceServers\": [\n        {\n            \"urls\": \"turn:35.173.254.80:80\",\n            \"username\": \"<my-username>\",\n            \"credential\": \"<my-password>\"\n        },\n    ]\n}\nStream(\n    handler=...,\n    rtc_configuration=rtc_configuration,\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n```\n\n----------------------------------------\n\nTITLE: Custom VAD Model Usage Example\nDESCRIPTION: Python code demonstrating how to use a custom VAD model with FastRTC's ReplyOnPause class and Stream functionality.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/turn_taking_gallery.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import ReplyOnPause, Stream\nfrom your_model import YourModel\n\ndef echo(audio):\n    yield audio\n\nmodel = YourModel() # implement the PauseDetectionModel protocol\nreply_on_pause = ReplyOnPause(\n    echo,\n    model=model,\n    options=YourModelOptions(),\n)\nstream = Stream(reply_on_pause, mode=\"send-receive\", modality=\"audio\")\nstream.ui.launch()\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Cloudflare TURN Credential Retrieval in Python\nDESCRIPTION: This function asynchronously fetches TURN credentials from Cloudflare or Hugging Face. It uses either Cloudflare API keys or a Hugging Face token, with the latter taking precedence if provided.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/credentials.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nasync def get_cloudflare_turn_credentials_async(\n    turn_key_id=None,\n    turn_key_api_token=None,\n    hf_token=None,\n    ttl=600,\n    client: httpx.AsyncClient | None = None,\n):\n```\n\n----------------------------------------\n\nTITLE: Converting Audio Tuple to Bytes in Python\nDESCRIPTION: Function to convert an audio tuple containing sample rate and numpy array data into bytes. This is particularly useful for sending audio data to external APIs from a ReplyOnPause handler.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/utils.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> sample_rate = 44100\n>>> audio_data = np.array([0.1, -0.2, 0.3])  # Example audio samples\n>>> audio_tuple = (sample_rate, audio_data)\n>>> audio_bytes = audio_to_bytes(audio_tuple)\n```\n\n----------------------------------------\n\nTITLE: Twilio TURN Server Integration\nDESCRIPTION: Configuration for using Twilio's TURN servers with FastRTC, including both manual setup and helper function approach.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/deployment.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream\nfrom twilio.rest import Client\nimport os\n\naccount_sid = os.environ.get(\"TWILIO_ACCOUNT_SID\")\nauth_token = os.environ.get(\"TWILIO_AUTH_TOKEN\")\n\nclient = Client(account_sid, auth_token)\n\ntoken = client.tokens.create()\n\nrtc_configuration = {\n    \"iceServers\": token.ice_servers,\n    \"iceTransportPolicy\": \"relay\",\n}\n\nStream(\n    handler=...,\n    rtc_configuration=rtc_configuration,\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom gradio_webrtc import get_twilio_turn_credentials\n\n# Will automatically read the TWILIO_ACCOUNT_SID and TWILIO_AUTH_TOKEN\n# env variables but you can also pass in the tokens as parameters\nrtc_configuration = get_twilio_turn_credentials()\n```\n\n----------------------------------------\n\nTITLE: Configuring Interruptions in ReplyOnPause\nDESCRIPTION: This code example demonstrates how to configure the ReplyOnPause handler to allow or disallow interruptions during the response. By default, interruptions are allowed, but this can be changed using the can_interrupt parameter.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream, ReplyOnPause\n\nstream = Stream(\n    handler=ReplyOnPause(\n        response,\n        can_interrupt=True,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Concurrency and Time Limits in FastRTC Stream\nDESCRIPTION: Example of configuring concurrency limits for a FastRTC Stream. Sets maximum concurrent connections to 10 and limits each connection to one hour (3600 seconds).\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/streams.md#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nstream = Stream(\n    handler=handler,\n    concurrency_limit=10,\n    time_limit=3600\n)\n```\n\n----------------------------------------\n\nTITLE: Aggregating Bytes to 16-bit Audio Samples in Python\nDESCRIPTION: Function that takes an iterator of byte chunks and aggregates them into 16-bit audio samples. It handles incomplete samples by combining them with the next chunk.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/utils.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> chunks_iterator = [b'\\x00\\x01', b'\\x02\\x03', b'\\x04\\x05']\n>>> for chunk in aggregate_bytes_to_16bit(chunks_iterator):\n>>>     print(chunk)\n```\n\n----------------------------------------\n\nTITLE: FastRTC Integration with Cloudflare API Token\nDESCRIPTION: Shows how to connect FastRTC to Cloudflare TURN servers using API tokens after exhausting the free quota.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/deployment.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream, get_cloudflare_turn_credentials_async\n\n# Make sure the TURN_KEY_ID and TURN_KEY_API_TOKEN environment variables are set\nstream = Stream(\n    handler=...,\n    rtc_configuration=get_cloudflare_turn_credentials_async,\n    modality=\"audio\",\n    mode=\"send-receive\",\n)\n```\n\n----------------------------------------\n\nTITLE: Incoming Call Handler\nDESCRIPTION: Async method to handle incoming telephone calls, generating TwiML instructions for audio streaming.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync handle_incoming_call(request: Request)\n```\n\n----------------------------------------\n\nTITLE: Handling Input Requests from WebSocket Server\nDESCRIPTION: Implements handler for server's send_input requests by sending the required data to an input hook. This allows the WebSocket communication to request additional input from the client when needed.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/websocket_docs.md#2025-04-17_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nws.onmessage = (event) => {\n    const data = JSON.parse(event.data);\n    // Handle send_input messages\n    if (data?.type === \"send_input\") {\n        fetch('/input_hook', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ \n                webrtc_id: websocket_id,  // Use the same ID from connection\n                inputs: your_input_data \n            })\n        });\n    }\n    // ... existing audio handling code ...\n};\n```\n\n----------------------------------------\n\nTITLE: Stopping WebRTC Connection in JavaScript\nDESCRIPTION: Provides a function to properly close a WebRTC connection by stopping all transceivers, closing local audio/video tracks, and finally closing the peer connection after a brief delay to ensure all operations complete.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/webrtc_docs.md#2025-04-17_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nfunction stop(pc) {\n  // close transceivers\n  if (pc.getTransceivers) {\n    pc.getTransceivers().forEach((transceiver) => {\n      if (transceiver.stop) {\n        transceiver.stop();\n      }\n    });\n  }\n\n  // close local audio / video\n  if (pc.getSenders()) {\n    pc.getSenders().forEach((sender) => {\n      if (sender.track && sender.track.stop) sender.track.stop();\n    });\n  }\n\n  // close peer connection\n  setTimeout(() => {\n    pc.close();\n  }, 500);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing FastRTC via pip\nDESCRIPTION: Basic installation command for the FastRTC library using pip. Additional extras are available for voice activity detection and text-to-speech functionality.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/README.md#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install fastrtc\n```\n\n----------------------------------------\n\nTITLE: JSON Message Format for FastRTC Communication\nDESCRIPTION: Defines the JSON format for messages exchanged between client and server in FastRTC. Messages can be of various types including send_input, fetch_output, stopword, error, warning, and log.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/api.md#2025-04-17_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": `send_input` | `fetch_output` | `stopword` | `error` | `warning` | `log`,\n    \"data\": string | object\n}\n```\n\n----------------------------------------\n\nTITLE: Analyzing Audio for Stop Words and Pauses in Python\nDESCRIPTION: Overrides ReplyOnPause.determine_pause to perform speech-to-text on audio chunks, detect stop words, and then use VAD to detect pauses after stop words. Returns true when a stop word is followed by a sufficient pause.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/reply_on_pause.md#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndetermine_pause(audio: np.ndarray, sampling_rate: int, state: ReplyOnStopWordsState) -> bool\n```\n\n----------------------------------------\n\nTITLE: Retrieving Twilio TURN Credentials in Python\nDESCRIPTION: This function retrieves TURN credentials from Twilio using the Twilio REST API. It requires the 'twilio' package to be installed and Twilio account credentials to be provided or set as environment variables.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/credentials.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_twilio_turn_credentials(\n    twilio_sid=None,\n    twilio_token=None):\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Byte Aggregation to 16-bit Audio in Python\nDESCRIPTION: Asynchronous version of the byte aggregation function for processing audio data chunks into 16-bit samples.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/utils.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> chunks_iterator = [b'\\x00\\x01', b'\\x02\\x03', b'\\x04\\x05']\n>>> for chunk in async_aggregate_bytes_to_16bit(chunks_iterator):\n>>>     print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Speech-to-Text Implementation\nDESCRIPTION: Example of using the speech-to-text functionality with the Moonshine model for processing audio input.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import get_stt_model\n\nmodel = get_stt_model(model=\"moonshine/base\")\n\naudio = (16000, np.random.randint(-32768, 32768, size=(1, 16000)))\ntext = model.stt(audio)\n```\n\n----------------------------------------\n\nTITLE: STT Model Protocol Definition\nDESCRIPTION: Python protocol class definition for implementing custom STT models in FastRTC with required method signature.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/speech_to_text_gallery.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass STTModel(Protocol):\n    def stt(self, audio: tuple[int, NDArray[np.int16 | np.float32]]) -> str: ...\n```\n\n----------------------------------------\n\nTITLE: Aggregating Bytes to 16-bit Audio Samples in Python\nDESCRIPTION: Example of processing byte chunks into 16-bit audio samples. Handles incomplete samples and combines them with subsequent chunks.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/utils.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> chunks_iterator = [b'\\x00\\x01', b'\\x02\\x03', b'\\x04\\x05']\n>>> for chunk in aggregate_bytes_to_16bit(chunks_iterator):\n>>>     print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Converting Audio Data to Bytes in Python\nDESCRIPTION: Demonstrates how to convert an audio tuple containing sample rate and numpy array data into bytes format. Used for external API communication.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/utils.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> sample_rate = 44100\n>>> audio_data = np.array([0.1, -0.2, 0.3])  # Example audio samples\n>>> audio_tuple = (sample_rate, audio_data)\n>>> audio_bytes = audio_to_bytes(audio_tuple)\n```\n\n----------------------------------------\n\nTITLE: Synchronous Cloudflare TURN Credential Retrieval in Python\nDESCRIPTION: This function synchronously fetches TURN credentials from Cloudflare or Hugging Face. It uses either Cloudflare API keys or a Hugging Face token, with the latter taking precedence if provided.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/credentials.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_cloudflare_turn_credentials(\n    turn_key_id=None,\n    turn_key_api_token=None,\n    hf_token=None,\n    ttl=600,\n    client: httpx.AsyncClient | None = None,\n):\n```\n\n----------------------------------------\n\nTITLE: JSON Log Message Format for Pause Detection\nDESCRIPTION: Shows the specific format for log messages related to the ReplyOnPause handler, including pause detection, response starting, and started talking events.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/api.md#2025-04-17_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"log\",\n    \"data\": \"pause_detected\" | \"response_starting\" | \"started_talking\"\n}\n```\n\n----------------------------------------\n\nTITLE: Saving Audio Tuple to File in Python\nDESCRIPTION: Function to save an audio tuple containing sample rate and numpy array data to a file. Returns the path to the saved audio file.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/utils.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> sample_rate = 44100\n>>> audio_data = np.array([0.1, -0.2, 0.3])  # Example audio samples\n>>> audio_tuple = (sample_rate, audio_data)\n>>> file_path = audio_to_file(audio_tuple)\n>>> print(f\"Audio saved to: {file_path}\")\n```\n\n----------------------------------------\n\nTITLE: Asynchronously Aggregating Bytes to 16-bit Audio Samples in Python\nDESCRIPTION: Function that aggregates bytes to 16-bit audio samples asynchronously, taking an iterator of byte chunks and producing an iterator of 16-bit audio samples.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/utils.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> chunks_iterator = [b'\\x00\\x01', b'\\x02\\x03', b'\\x04\\x05']\n>>> for chunk in async_aggregate_bytes_to_16bit(chunks_iterator):\n>>>     print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Creating a Phone Number for FastRTC Stream\nDESCRIPTION: Shows how to get a temporary phone number for calling into your FastRTC stream using the fastphone feature.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/index.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstream.fastphone()\n```\n\n----------------------------------------\n\nTITLE: Sending Stop Word Message Asynchronously in Python\nDESCRIPTION: Sends a 'stopword' message asynchronously through the configured communication channel when a stop word is detected.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/reply_on_pause.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsend_stopword()\n```\n\n----------------------------------------\n\nTITLE: Detecting Stop Words in Text in Python\nDESCRIPTION: Checks if any configured stop words are present in the given text using case-insensitive search while accounting for multi-word phrases and ignoring basic punctuation.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/reply_on_pause.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstop_word_detected(text: str) -> bool\n```\n\n----------------------------------------\n\nTITLE: Setting Output Audio Sampling Rate\nDESCRIPTION: Demonstrates configuration of output audio sampling rate for stream handlers.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/advanced-configuration.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import ReplyOnPause, Stream\n\nstream = Stream(\n    handler=ReplyOnPause(..., output_sample_rate=16000),\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradio App for Llama 4 Voice Interface in YAML\nDESCRIPTION: YAML configuration for a Gradio application that enables talking to the Llama 4 model. The configuration specifies the app file, SDK version, styling elements, required API keys as secrets, and technologies used including WebRTC and WebSocket.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_llama4/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Talk to Llama 4\nemoji: 🦙\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.23.3\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Talk to Llama 4 using Groq + Cloudflare\ntags: [webrtc, websocket, gradio, secret|HF_TOKEN, secret|GROQ_API_KEY]\n---\n```\n\n----------------------------------------\n\nTITLE: Launching FastRTC Stream with UI\nDESCRIPTION: Shows how to launch a FastRTC stream with the built-in Gradio UI for testing and sharing.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/index.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstream.ui.launch()\n```\n\n----------------------------------------\n\nTITLE: Processing Incoming Video Frames Asynchronously in FastRTC\nDESCRIPTION: Abstract method declaration for asynchronously processing incoming video frames. Unlike the synchronous version, this method receives the frame as a numpy float32 array and must be implemented with async/await pattern.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\nasync video_receive(frame: npt.NDArray[np.float32]) -> None\n```\n\n----------------------------------------\n\nTITLE: AWS CloudFormation Stack Creation\nDESCRIPTION: Commands for creating and monitoring an AWS CloudFormation stack for TURN server deployment.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/deployment.md#2025-04-17_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naws cloudformation create-stack \\\n  --stack-name turn-server \\\n  --template-body file://deployment.yml \\\n  --parameters file://parameters.json \\\n  --capabilities CAPABILITY_IAM\n```\n\nLANGUAGE: bash\nCODE:\n```\naws cloudformation wait stack-create-complete \\\n  --stack-name turn-server\n```\n\nLANGUAGE: bash\nCODE:\n```\naws cloudformation describe-stacks \\\n  --stack-name turn-server \\\n  --query 'Stacks[0].Outputs' > server-info.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradio LLM Voice Chat Application in YAML\nDESCRIPTION: This YAML configuration sets up a Gradio-based LLM Voice Chat application. It specifies the project title, emoji, colors, SDK version, main file, license, and a short description. It also lists required API keys for Twilio, Groq, and ElevenLabs as secret tags.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/llm_voice_chat/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: LLM Voice Chat\nemoji: 💻\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Talk to an LLM with ElevenLabs\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN, secret|GROQ_API_KEY, secret|ELEVENLABS_API_KEY]\n---\n```\n\n----------------------------------------\n\nTITLE: Installing FastRTC with pip\nDESCRIPTION: Commands to install the FastRTC package. The basic installation provides core functionality, while the extended installation adds voice activity detection, speech-to-text, and text-to-speech capabilities.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/index.md#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install fastrtc\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install \"fastrtc[vad, stt, tts]\"\n```\n\n----------------------------------------\n\nTITLE: Launching Gradio UI Interface\nDESCRIPTION: Launches the Gradio web interface for the video stream.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/README.md#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nstream.ui.launch()\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for FastRTC Application\nDESCRIPTION: A requirements list for a Python application using FastRTC with VAD, STT, and TTS capabilities. It includes OpenAI for AI services, FastAPI for web API development, python-dotenv for environment configuration, ElevenLabs for voice synthesis, and FastRTC with additional specialized modules.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/nextjs_voice_chat/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nopenai\nfastapi\npython-dotenv\nelevenlabs\nfastrtc[vad, stt, tts]\n```\n\n----------------------------------------\n\nTITLE: Configuring Gemini Audio Video Gradio Application in YAML\nDESCRIPTION: This YAML configuration defines the settings for a Gradio-based application that uses Gemini for audio and video processing. It specifies the application's metadata, appearance, file locations, and required secret tokens for authentication and API access.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/gemini_audio_video/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Gemini Audio Video\nemoji: ♊️\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Gemini understands audio and video!\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN, secret|GEMINI_API_KEY]\n```\n\n----------------------------------------\n\nTITLE: Setting RTP Parameters for Resolution Maintenance in WebRTC\nDESCRIPTION: Shows how to enforce resolution constraints using RTP parameters in a video stream.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/advanced-configuration.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimage = Stream(\n    modality=\"video\",\n    mode=\"send\",\n    track_constraints=track_constraints,\n    rtp_params={\"degradationPreference\": \"maintain-resolution\"}\n)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Queue Item Retrieval with Timeout in Python\nDESCRIPTION: Demonstrates how to wait for and retrieve an item from an asyncio Queue with a timeout mechanism.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/utils.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> queue = asyncio.Queue()\n>>> queue.put_nowait(1)\n>>> item = await wait_for_item(queue)\n>>> print(item)\n```\n\n----------------------------------------\n\nTITLE: Required Python Packages List\nDESCRIPTION: A list of Python package dependencies including FastRTC, environment management, AI services (OpenAI, Groq), communication (Twilio), and audio synthesis (ElevenLabs) packages.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/llm_voice_chat/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastrtc[stopword]\npython-dotenv\nopenai\ntwilio\ngroq\nelevenlabs\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradio App Settings for Azure OpenAI Integration\nDESCRIPTION: YAML configuration file that defines the project settings for a Gradio application interfacing with Azure OpenAI. Includes application metadata, SDK version, licensing, and required secret environment variables for Twilio and OpenAI integration.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_azure_openai/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Talk to Azure OpenAI\nemoji: 🗣️\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Talk to Azure OpenAI using their multimodal API\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN, secret|OPENAI_API_KEY]\n```\n\n----------------------------------------\n\nTITLE: FastAPI Twilio Integration Setup\nDESCRIPTION: Basic setup for integrating a Stream with FastAPI and Twilio for voice capabilities.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import Stream, ReplyOnPause\nfrom fastapi import FastAPI\n\ndef echo(audio):\n    yield audio\n\napp = FastAPI()\n\nstream = Stream(ReplyOnPause(echo), modality=\"audio\", mode=\"send-receive\")\nstream.mount(app)\n```\n\n----------------------------------------\n\nTITLE: Resetting ReplyOnStopWords State in Python\nDESCRIPTION: Resets the handler state to its initial condition by clearing accumulated audio, resetting flags (including stop word state), closing active generators, and clearing event flags.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/reply_on_pause.md#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nreset()\n```\n\n----------------------------------------\n\nTITLE: Listing Dependencies for FastRTC Project\nDESCRIPTION: This snippet lists the dependencies required for the FastRTC project. It includes the main package 'fastrtc' with an optional 'vad' component, a GitHub-hosted package 'useful-moonshine-onnx', and the 'twilio' library.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/moonshine_live/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastrtc[vad]\nuseful-moonshine-onnx@git+https://git@github.com/usefulsensors/moonshine.git#subdirectory=moonshine-onnx\ntwilio\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Space for FastRTC Echo Audio Demo\nDESCRIPTION: YAML configuration for a Hugging Face Spaces application that implements a simple echo stream demo using WebRTC. It specifies the application title, styling, SDK requirements, and required Twilio credentials for WebRTC functionality.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/echo_audio/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Echo Audio\nemoji: 🪩\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Simple echo stream - simplest FastRTC demo\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN]\n---\n```\n\n----------------------------------------\n\nTITLE: Customizing WebRTC Button Labels\nDESCRIPTION: Demonstrates how to customize button labels in the WebRTC UI.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/advanced-configuration.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwebrtc = WebRTC(\n    label=\"Video Chat\",\n    modality=\"audio-video\",\n    mode=\"send-receive\",\n    button_labels={\"start\": \"Start Talking to Gemini\"}\n)\n```\n\n----------------------------------------\n\nTITLE: Twilio Outbound Call Implementation\nDESCRIPTION: Complete example of implementing outbound calls using Twilio with FastAPI endpoints and WebSocket handling.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\napp = FastAPI()\n\n@app.post(\"/call\")\nasync def start_call(req: Request):\n  body = await req.json()\n  from_no = body.get(\"from\")\n  to_no = body.get(\"to\")\n  account_sid = os.getenv(\"TWILIO_ACCOUNT_SID\")\n  auth_token = os.getenv(\"TWILIO_AUTH_TOKEN\")\n  client = Client(account_sid, auth_token)\n\n  call = client.calls.create(\n    to=to_no,\n    from_=from_no,\n    url=\"https://[your_ngrok_subdomain].ngrok.app/incoming-call\"\n  )\n\n  return {\"sid\": f\"{call.sid}\"}\n\n@app.api_route(\"/incoming-call\", methods=[\"GET\", \"POST\"])\nasync def handle_incoming_call(req: Request):\n  from twilio.twiml.voice_response import VoiceResponse, Connect\n  response = VoiceResponse()\n  response.say(\"Connecting to AI assistant\")\n  connect = Connect()\n  connect.stream(url=f'wss://{req.url.hostname}/media-stream')\n  response.append(connect)\n  return HTMLResponse(content=str(response), media_type=\"application/xml\")\n\n@app.websocket(\"/media-stream\")\nasync def handle_media_stream(websocket: WebSocket):\n  await stream.telephone_handler(websocket)\n\napp = gr.mount_gradio_app(app, stream.ui, path=\"/\")\n```\n\n----------------------------------------\n\nTITLE: Listing Required Dependencies for FastRTC\nDESCRIPTION: A requirements file listing the necessary libraries for the FastRTC project. Dependencies include OpenCV for image processing, Twilio for communication services, and ONNX Runtime GPU for accelerated machine learning inference.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/object_detection/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastrtc\nopencv-python\ntwilio\nonnxruntime-gpu\n```\n\n----------------------------------------\n\nTITLE: Configuring Llama Code Editor Gradio App on Hugging Face Spaces\nDESCRIPTION: YAML configuration for a Gradio-based web application called 'Llama Code Editor'. It specifies project metadata, appearance settings, SDK information, licensing details, and required API secrets for services like Twilio, SambaNova, and Groq.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/llama_code_editor/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Llama Code Editor\nemoji: 🦙\ncolorFrom: indigo\ncolorTo: pink\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Create interactive HTML web pages with your voice\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN,\nsecret|SAMBANOVA_API_KEY, secret|GROQ_API_KEY]\n---\n```\n\n----------------------------------------\n\nTITLE: Installing FastRTC with voice detection and text-to-speech extras\nDESCRIPTION: Installation command including the 'vad' and 'tts' extras for enhanced functionality like reply-on-pause and text-to-speech capabilities.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/README.md#2025-04-17_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"fastrtc[vad, tts]\"\n```\n\n----------------------------------------\n\nTITLE: VAD Model Protocol Implementation\nDESCRIPTION: Python code defining the PauseDetectionModel protocol for implementing custom VAD models in FastRTC.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/turn_taking_gallery.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nModelOptions: TypeAlias = Any\n\n\nclass PauseDetectionModel(Protocol):\n    def vad(\n        self,\n        audio: tuple[int, NDArray[np.int16] | NDArray[np.float32]],\n        options: ModelOptions,\n    ) -> tuple[float, list[AudioChunk]]: ...\n\n    def warmup(\n        self,\n    ) -> None: ...\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies Definition\nDESCRIPTION: Specifies the required Python packages and their versions for a FastRTC application. Includes fastrtc with VAD support version 0.0.20.rc2, OpenAI API client, Twilio client, and python-dotenv for environment configuration.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_openai/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nfastrtc[vad]==0.0.20.rc2\nopenai\ntwilio\npython-dotenv\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Space for Moonshine Transcription\nDESCRIPTION: YAML configuration that defines the project settings for a Hugging Face Space, including SDK details, model references, and required secret tokens for Twilio integration. The configuration specifies UI colors, licensing, and tags for the application.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/moonshine_live/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Moonshine Live Transcription\nemoji: 🌕\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.17.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Real-time captions with Moonshine ONNX\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN]\nmodels: [onnx-community/moonshine-base-ONNX, UsefulSensors/moonshine-base]\n```\n\n----------------------------------------\n\nTITLE: Adding alawmulaw Library for Audio Encoding/Decoding\nDESCRIPTION: Imports the alawmulaw library via CDN for handling mu-law audio encoding and decoding operations required by the WebSocket audio implementation.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/websocket_docs.md#2025-04-17_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"https://cdn.jsdelivr.net/npm/alawmulaw\"></script>\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for fastrtc Project\nDESCRIPTION: This requirements file specifies three Python package dependencies: fastrtc with Voice Activity Detection (VAD) capability at version 0.0.20.rc2, groq (likely for API access), and python-dotenv (for environment variable management).\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/whisper_realtime/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastrtc[vad]==0.0.20.rc2\ngroq\npython-dotenv\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Space for Phonic AI Chat\nDESCRIPTION: YAML configuration for a Hugging Face Space deployment that defines the application settings, dependencies, and required secret keys. The configuration specifies Gradio SDK version 5.16.0, Python 3.11, and includes necessary secret tokens for Twilio and Phonic API integration.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/phonic_chat/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Phonic AI Chat\nemoji: 🎙️\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Talk to Phonic AI's speech-to-speech model\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN, secret|PHONIC_API_KEY]\npython_version: 3.11\n```\n\n----------------------------------------\n\nTITLE: Waiting for Item from AsyncIO Queue with Timeout in Python\nDESCRIPTION: Function to wait for an item from an asyncio.Queue with a timeout. Returns the item from the queue or None if the timeout is reached.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/utils.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> queue = asyncio.Queue()\n>>> queue.put_nowait(1)\n>>> item = await wait_for_item(queue)\n>>> print(item)\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradio Space for OpenAI Voice Interface\nDESCRIPTION: YAML configuration for a Hugging Face Space that implements a voice interface to OpenAI. The configuration defines metadata, dependencies, and required secrets including OpenAI API key and Hugging Face token.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_openai/README_gradio.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Talk to OpenAI (Gradio UI)\nemoji: 🗣️\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Talk to OpenAI (Gradio UI)\ntags: [webrtc, websocket, gradio, secret|HF_TOKEN, secret|OPENAI_API_KEY]\n---\n```\n\n----------------------------------------\n\nTITLE: Processing Incoming Video Frames Synchronously in FastRTC\nDESCRIPTION: Abstract method declaration for processing incoming video frames synchronously. This method must be implemented by concrete subclasses to handle VideoFrame objects from aiortc.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\nvideo_receive(frame: VideoFrame) -> None\n```\n\n----------------------------------------\n\nTITLE: Emitting Video Frames Synchronously in FastRTC\nDESCRIPTION: Abstract method declaration for producing the next output video frame synchronously. Implementations should return a VideoEmitType object, typically a numpy array representing a video frame, or None.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\nvideo_emit() -> VideoEmitType\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Space for Whisper Transcription App\nDESCRIPTION: YAML configuration for a Hugging Face Space deployment. Specifies the application file, UI colors, emoji, license, SDK details, description and required secret tokens for the Whisper realtime transcription application.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/whisper_realtime/README_gradio.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napp_file: app.py\ncolorFrom: purple\ncolorTo: red\nemoji: 👂\nlicense: mit\npinned: false\nsdk: gradio\nsdk_version: 5.16.0\nshort_description: Transcribe audio in realtime - Gradio UI version\ntags:\n- webrtc\n- websocket\n- gradio\n- secret|HF_TOKEN\n- secret|GROQ_API_KEY\ntitle: Whisper Realtime Transcription (Gradio UI)\n```\n\n----------------------------------------\n\nTITLE: Implementing Emit Method for Stream Output in Python\nDESCRIPTION: Abstract method for producing output chunks synchronously. Returns audio data, additional outputs, or control signals.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\nemit() -> EmitType\n```\n\n----------------------------------------\n\nTITLE: WebSocket Offer Handler\nDESCRIPTION: Async method to establish WebSocket connections to the Stream.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nasync websocket_offer(websocket: WebSocket)\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradio App for Object Detection with YOLOv10\nDESCRIPTION: YAML configuration for a Gradio app that performs real-time object detection using YOLOv10. It specifies metadata, SDK details, and required secret tokens for Twilio integration. The app uses WebRTC and WebSocket for real-time communication.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/object_detection/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Object Detection\nemoji: 📸\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Use YOLOv10 to detect objects in real-time\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN]\n---\n```\n\n----------------------------------------\n\nTITLE: Creating Copies of AsyncAudioVideoStreamHandler Instances\nDESCRIPTION: Abstract method declaration for creating a copy of an AsyncAudioVideoStreamHandler instance. This must be implemented by concrete subclasses to create and return a new instance of the same class.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ncopy() -> AsyncAudioVideoStreamHandler\n```\n\n----------------------------------------\n\nTITLE: UI Property Getter and Setter\nDESCRIPTION: Property methods to get and set the Gradio Blocks UI instance associated with the stream.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream.md#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@property\nui() -> Blocks\n\n@ui.setter\nui(blocks: Blocks)\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradio LLM Voice Chat Application Settings\nDESCRIPTION: YAML configuration for a Gradio application that implements voice chat functionality. Defines app metadata, SDK version, required secret keys for Twilio, Groq, and ElevenLabs integration, and application tags.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/llm_voice_chat/README_gradio.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: LLM Voice Chat (Gradio)\nemoji: 💻\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: LLM Voice by ElevenLabs (Gradio)\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN, secret|GROQ_API_KEY, secret|ELEVENLABS_API_KEY]\n```\n\n----------------------------------------\n\nTITLE: Creating Copies of AudioVideoStreamHandler Instances\nDESCRIPTION: Abstract method declaration for creating a copy of an AudioVideoStreamHandler instance. This method must be implemented by concrete subclasses to create and return a new instance of the same class.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/stream_handlers.md#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ncopy() -> AudioVideoStreamHandler\n```\n\n----------------------------------------\n\nTITLE: Saving Audio Data to File in Python\nDESCRIPTION: Shows how to save an audio tuple containing sample rate and numpy array data to a file system.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/utils.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> sample_rate = 44100\n>>> audio_data = np.array([0.1, -0.2, 0.3])  # Example audio samples\n>>> audio_tuple = (sample_rate, audio_data)\n>>> file_path = audio_to_file(audio_tuple)\n>>> print(f\"Audio saved to: {file_path}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Space for Gemini Chat Application\nDESCRIPTION: YAML configuration that sets up a Hugging Face Space for a Gemini chat interface. Defines project metadata, styling, SDK requirements, licensing, and required secret environment variables for API access.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_gemini/README_gradio.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Talk to Gemini (Gradio UI)\nemoji: ♊️\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Talk to Gemini (Gradio UI)\ntags: [webrtc, websocket, gradio, secret|HF_TOKEN, secret|GEMINI_API_KEY]\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Space for Gradio WebRTC Application\nDESCRIPTION: YAML configuration for a Hugging Face Space that defines the application's metadata, dependencies, and required secret tokens. The configuration specifies the use of Gradio SDK 5.16.0 and includes necessary tags for WebRTC and API authentication.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_sambanova/README_gradio.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Talk to Sambanova (Gradio)\nemoji: 💻\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Llama 3.2 - SambaNova API (Gradio)\ntags: [webrtc, websocket, gradio, secret|HF_TOKEN_ALT, secret|SAMBANOVA_API_KEY]\n```\n\n----------------------------------------\n\nTITLE: Phone Mode Handler Example\nDESCRIPTION: Demonstrates how to handle telephone mode in a stream handler by checking the phone_mode property.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/audio.md#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef emit(self):\n    if self.phone_mode:\n        self.latest_args = [None]\n    else:\n        await self.wait_for_args()\n```\n\n----------------------------------------\n\nTITLE: Configuring Gemini Talking to Gemini Gradio Application in YAML\nDESCRIPTION: YAML configuration for a Hugging Face Spaces application that uses Gradio to enable communication between two Gemini AI agents. The configuration specifies project metadata, appearance settings, required SDK, secret environment variables for Twilio and Gemini API integration, and licensing information.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/gemini_conversation/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Gemini Talking to Gemini\nemoji: ♊️\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.17.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Have two Gemini agents talk to each other\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN, secret|GEMINI_API_KEY]\n---\n```\n\n----------------------------------------\n\nTITLE: Implementing Tag Filtering System with JavaScript\nDESCRIPTION: This script enables filtering of application cards based on tags. When tag buttons are clicked, they toggle active state and filter the visible cards accordingly. The filtering logic matches selected tags against the data-tags attribute on each card.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/cookbook.md#2025-04-17_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nfunction filterCards() {\n    const activeButtons = document.querySelectorAll('.tag-button.active');\n    const selectedTags = Array.from(activeButtons).map(button => button.getAttribute('data-tag'));\n    const cards = document.querySelectorAll('.grid.cards > ul > li > p[data-tags]');\n    \n    cards.forEach(card => {\n        const cardTags = card.getAttribute('data-tags').split(',');\n        const shouldShow = selectedTags.length === 0 || selectedTags.some(tag => cardTags.includes(tag));\n        card.parentElement.style.display = shouldShow ? 'block' : 'none';\n    });\n}\ndocument.querySelectorAll('.tag-button').forEach(button => {\n    button.addEventListener('click', () => {\n        button.classList.toggle('active');\n        filterCards();\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradio Voice Chat Application with Claude AI\nDESCRIPTION: YAML configuration for a Gradio Spaces application that implements a voice chat interface for Claude AI. Includes settings for WebRTC integration, API dependencies, and styling parameters.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_claude/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Talk to Claude\nemoji: 👨‍🦰\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Talk to Anthropic's Claude\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN, secret|GROQ_API_KEY, secret|ANTHROPIC_API_KEY, secret|ELEVENLABS_API_KEY]\n```\n\n----------------------------------------\n\nTITLE: Creating a Copy of ReplyOnStopWords Instance in Python\nDESCRIPTION: Creates and returns a new instance of ReplyOnStopWords with identical configuration settings as the current instance.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/reference/reply_on_pause.md#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncopy() -> ReplyOnStopWords\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Space for Llama 3.2 with SambaNova API\nDESCRIPTION: YAML configuration for a Hugging Face Space that defines a Gradio application using WebRTC to interface with Llama 3.2 through SambaNova's API. The configuration includes visual styling, SDK specifications, licensing, and required secret keys.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_sambanova/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Talk to Sambanova\nemoji: 💻\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Llama 3.2 - SambaNova API\ntags: [webrtc, websocket, gradio, secret|HF_TOKEN_ALT, secret|SAMBANOVA_API_KEY]\n---\n```\n\n----------------------------------------\n\nTITLE: Card Filtering with JavaScript\nDESCRIPTION: JavaScript functionality to filter cards based on selected tags, handling button clicks and visibility toggling.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/turn_taking_gallery.md#2025-04-17_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nfunction filterCards() {\n    const activeButtons = document.querySelectorAll('.tag-button.active');\n    const selectedTags = Array.from(activeButtons).map(button => button.getAttribute('data-tag'));\n    const cards = document.querySelectorAll('.grid.cards > ul > li > p[data-tags]');\n    \n    cards.forEach(card => {\n        const cardTags = card.getAttribute('data-tags').split(',');\n        const shouldShow = selectedTags.length === 0 || selectedTags.some(tag => cardTags.includes(tag));\n        card.parentElement.style.display = shouldShow ? 'block' : 'none';\n    });\n}\n\ndocument.querySelectorAll('.tag-button').forEach(button => {\n    button.addEventListener('click', () => {\n        button.classList.toggle('active');\n        filterCards();\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradio App for Azure OpenAI Voice Interface\nDESCRIPTION: YAML configuration for a Gradio application that connects to Azure OpenAI services with voice capabilities using WebRTC. The configuration includes metadata for Hugging Face Spaces deployment, specifies secret environment variables for Twilio and OpenAI authentication, and sets up the application entry point.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_azure_openai/README_gradio.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Talk to Azure OpenAI (Gradio UI)\nemoji: 🗣️\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Talk to Azure OpenAI (Gradio UI)\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN, secret|OPENAI_API_KEY]\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradio App for OpenAI Conversation in YAML\nDESCRIPTION: This YAML configuration sets up a Gradio app titled 'Talk to OpenAI'. It specifies visual elements, SDK details, licensing, and required secret keys for OpenAI API integration. The app uses WebRTC and WebSocket technologies.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_openai/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Talk to OpenAI\nemoji: 🗣️\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Talk to OpenAI using their multimodal API\ntags: [webrtc, websocket, gradio, secret|HF_TOKEN, secret|OPENAI_API_KEY]\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Space for Gemini Voice Assistant\nDESCRIPTION: YAML configuration for a Hugging Face Space that deploys a voice-enabled chat interface for Google's Gemini model. The configuration specifies the app title, emoji, color scheme, Gradio SDK version, main Python file, license, and required API secrets.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_gemini/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Talk to Gemini\nemoji: ♊️\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Talk to Gemini using Google's multimodal API\ntags: [webrtc, websocket, gradio, secret|HF_TOKEN, secret|GEMINI_API_KEY]\n---\n```\n\n----------------------------------------\n\nTITLE: Filtering Cards with JavaScript\nDESCRIPTION: JavaScript functionality for filtering model cards based on selected tags, with event listeners for tag button interactions.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/speech_to_text_gallery.md#2025-04-17_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nfunction filterCards() {\n    const activeButtons = document.querySelectorAll('.tag-button.active');\n    const selectedTags = Array.from(activeButtons).map(button => button.getAttribute('data-tag'));\n    const cards = document.querySelectorAll('.grid.cards > ul > li > p[data-tags]');\n    \n    cards.forEach(card => {\n        const cardTags = card.getAttribute('data-tags').split(',');\n        const shouldShow = selectedTags.length === 0 || selectedTags.some(tag => cardTags.includes(tag));\n        card.parentElement.style.display = shouldShow ? 'block' : 'none';\n    });\n}\ndocument.querySelectorAll('.tag-button').forEach(button => {\n    button.addEventListener('click', () => {\n        button.classList.toggle('active');\n        filterCards();\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for FastRTC with Voice Capabilities\nDESCRIPTION: A requirements list specifying the necessary Python packages for a FastRTC application with voice and AI capabilities. The dependencies include FastRTC with VAD and TTS extensions, ElevenLabs for voice synthesis, Groq and Anthropic for AI models, Twilio for communications, and python-dotenv for environment variable management.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_claude/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastrtc[vad, tts]\nelevenlabs\ngroq\nanthropic\ntwilio\npython-dotenv\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradio Space with YAML for Hello Computer Application\nDESCRIPTION: YAML configuration file for a Gradio application that requires users to say 'computer' before asking questions. It specifies UI colors, SDK version, licensing, and required API secrets for Twilio and SambaNova integration.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/hello_computer/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Hello Computer\nemoji: 💻\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Say computer before asking your question\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN, secret|SAMBANOVA_API_KEY]\n---\n```\n\n----------------------------------------\n\nTITLE: Installing Walkie Talkie Extension\nDESCRIPTION: Shell command for installing the Walkie Talkie turn-taking algorithm extension for FastRTC.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/turn_taking_gallery.md#2025-04-17_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install fastrtc-walkie-talkie\n```\n\n----------------------------------------\n\nTITLE: TTS Model Gallery Filter Implementation\nDESCRIPTION: JavaScript implementation for filtering TTS model cards based on selected tags. Handles tag button clicks and card visibility.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/text_to_speech_gallery.md#2025-04-17_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nfunction filterCards() {\n    const activeButtons = document.querySelectorAll('.tag-button.active');\n    const selectedTags = Array.from(activeButtons).map(button => button.getAttribute('data-tag'));\n    const cards = document.querySelectorAll('.grid.cards > ul > li > p[data-tags]');\n    \n    cards.forEach(card => {\n        const cardTags = card.getAttribute('data-tags').split(',');\n        const shouldShow = selectedTags.length === 0 || selectedTags.some(tag => cardTags.includes(tag));\n        card.parentElement.style.display = shouldShow ? 'block' : 'none';\n    });\n}\ndocument.querySelectorAll('.tag-button').forEach(button => {\n    button.addEventListener('click', () => {\n        button.classList.toggle('active');\n        filterCards();\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Whisper Realtime Transcription Gradio App in YAML\nDESCRIPTION: YAML configuration for a Gradio-based Whisper real-time transcription application. The configuration specifies UI colors, SDK details, licensing, and required API keys for Hugging Face and Groq.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/whisper_realtime/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Whisper Realtime Transcription\nemoji: 👂\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Transcribe audio in realtime with Whisper\ntags: [webrtc, websocket, gradio, secret|HF_TOKEN, secret|GROQ_API_KEY]\n---\n```\n\n----------------------------------------\n\nTITLE: Environment Configuration File for FastRTC Voice Agent\nDESCRIPTION: Creates a .env configuration file with Hugging Face API token and application mode settings required by the FastRTC voice agent.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_smolagents/README.md#2025-04-17_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nHF_TOKEN=your_huggingface_api_key\nMODE=UI  # Use 'UI' for Gradio interface, leave blank for HTML interface\n```\n\n----------------------------------------\n\nTITLE: Styling Tag Buttons in CSS\nDESCRIPTION: CSS styles for interactive tag buttons that control model filtering, including hover effects and active states.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/turn_taking_gallery.md#2025-04-17_snippet_0\n\nLANGUAGE: css\nCODE:\n```\n.tag-button {\n    cursor: pointer;\n    opacity: 0.5;\n    transition: opacity 0.2s ease;\n}\n\n.tag-button > code {\n    color: var(--supernova);\n}\n\n.tag-button.active {\n    opacity: 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Styling Tag Buttons with CSS\nDESCRIPTION: CSS styles for tag buttons with interactive effects. The styles include cursor behavior, opacity transitions, and active state styling to provide visual feedback when users interact with the tag filtering system.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/cookbook.md#2025-04-17_snippet_1\n\nLANGUAGE: CSS\nCODE:\n```\n.tag-button {\n    cursor: pointer;\n    opacity: 0.5;\n    transition: opacity 0.2s ease;\n}\n\n.tag-button > code {\n    color: var(--supernova);\n}\n\n.tag-button.active {\n    opacity: 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradio Application Metadata in YAML\nDESCRIPTION: This YAML configuration defines metadata for a Gradio application, including title, visual styling, SDK version, file locations, and required secret keys. It sets up a WebRTC and WebSocket-based application with Twilio and SambaNova integrations.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/hello_computer/README_gradio.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Hello Computer (Gradio)\nemoji: 💻\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Say computer (Gradio)\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN, secret|SAMBANOVA_API_KEY]\n```\n\n----------------------------------------\n\nTITLE: Running FastRTC Voice Agent with Gradio UI\nDESCRIPTION: Launches the FastRTC voice agent application with Gradio UI enabled, making it accessible at the local port 7860.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_smolagents/README.md#2025-04-17_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nMODE=UI python app.py\n```\n\n----------------------------------------\n\nTITLE: Styling TTS Model Tags with CSS\nDESCRIPTION: CSS styles for tag buttons used to filter TTS models in the gallery interface. Defines hover effects, active states, and color schemes.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/text_to_speech_gallery.md#2025-04-17_snippet_0\n\nLANGUAGE: css\nCODE:\n```\n.tag-button {\n    cursor: pointer;\n    opacity: 0.5;\n    transition: opacity 0.2s ease;\n}\n\n.tag-button > code {\n    color: var(--supernova);\n}\n\n.tag-button.active {\n    opacity: 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for FastRTC with VAD and STT\nDESCRIPTION: Specifies the required Python packages for a FastRTC application with voice activity detection and speech-to-text capabilities. The requirements include a specific release candidate version of fastrtc with vad and stt extras, environment variable management via python-dotenv, model access through huggingface_hub, and Twilio for communication services.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_sambanova/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastrtc[vad, stt]==0.0.20.rc2\npython-dotenv\nhuggingface_hub>=0.29.0\ntwilio\n```\n\n----------------------------------------\n\nTITLE: Running Next.js Development Server with Package Managers\nDESCRIPTION: Commands to start the Next.js development server using different JavaScript package managers. This allows developers to run the application locally for development purposes on http://localhost:3000.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/nextjs_voice_chat/frontend/fastrtc-demo/README.md#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for fastrtc Project\nDESCRIPTION: A requirements file listing the necessary Python packages for the fastrtc project. It specifies fastrtc[vad] at version 0.0.20.rc2, python-dotenv for environment variable management, google-genai for AI capabilities, and twilio for communication services.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_gemini/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastrtc[vad]==0.0.20.rc2\npython-dotenv\ngoogle-genai\ntwilio\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradio App Settings in YAML\nDESCRIPTION: YAML configuration for a Gradio application that compares WebRTC and WebSocket performance. Includes project metadata, styling configuration, and required API credentials as secret environment variables.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/webrtc_vs_websocket/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Webrtc Vs Websocket\nemoji: 🧪\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Compare Round Trip Times between WebRTC and Websockets\ntags: [webrtc, websocket, gradio, secret|TWILIO_ACCOUNT_SID, secret|TWILIO_AUTH_TOKEN, secret|ELEVENLABS_API_KEY, secret|GROQ_API_KEY, secret|ANTHROPIC_API_KEY]\n```\n\n----------------------------------------\n\nTITLE: Installing HumAware VAD\nDESCRIPTION: Shell commands for installing and setting up the HumAware VAD model, a fine-tuned version of Silero-VAD for distinguishing humming from speech.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/turn_taking_gallery.md#2025-04-17_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install humaware-vad\n\ngit clone https://github.com/CuriousMonkey7/HumAwareVad.git\ncd HumAwareVad\npython app.py\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies\nDESCRIPTION: List of required Python packages including FastRTC with VAD support, Twilio client, and python-dotenv for environment variable management.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/echo_audio/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastrtc[vad]\ntwilio\npython-dotenv\n```\n\n----------------------------------------\n\nTITLE: Python Project Dependencies List\nDESCRIPTION: A list of Python package dependencies required for the project. It includes fastrtc with voice activity detection (vad) feature, elevenlabs for voice synthesis, groq and anthropic for AI services, twilio for communication APIs, and python-dotenv for loading environment variables.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/webrtc_vs_websocket/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastrtc[vad]\nelevenlabs\ngroq\nanthropic\ntwilio\npython-dotenv\n```\n\n----------------------------------------\n\nTITLE: Styling Tag Buttons in CSS\nDESCRIPTION: CSS styles for interactive tag buttons used to filter STT models, including hover effects and active states.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/speech_to_text_gallery.md#2025-04-17_snippet_0\n\nLANGUAGE: css\nCODE:\n```\n.tag-button {\n    cursor: pointer;\n    opacity: 0.5;\n    transition: opacity 0.2s ease;\n}\n\n.tag-button > code {\n    color: var(--supernova);\n}\n\n.tag-button.active {\n    opacity: 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Customizing WebRTC Audio Icon\nDESCRIPTION: Shows how to customize the audio streaming icon and its appearance.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/advanced-configuration.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\naudio = WebRTC(\n    label=\"Stream\",\n    rtc_configuration=rtc_configuration,\n    mode=\"receive\",\n    modality=\"audio\",\n    icon=\"phone-solid.svg\",\n    icon_button_color=\"black\",\n    pulse_color=\"black\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Gradio App Configuration in YAML\nDESCRIPTION: YAML configuration block that specifies the application metadata, styling, dependencies and secrets for a Voice Text Editor Gradio application. It includes app title, emoji, color theme, SDK version, license information, and secret keys required for operation.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/voice_text_editor/README.md#2025-04-17_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Voice Text Editor\nemoji: 📝\ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 5.16.0\napp_file: app.py\npinned: false\nlicense: mit\nshort_description: Edit text documents with your voice!\ntags: [webrtc, websocket, gradio, secret|HF_TOKEN, secret|SAMBANOVA_API_KEY]\n---\n```\n\n----------------------------------------\n\nTITLE: Using Deprecated HF TURN Credentials\nDESCRIPTION: Legacy implementation using Hugging Face TURN credentials, now deprecated in favor of Cloudflare credentials.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/deployment.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom fastrtc import get_hf_turn_credentials, Stream\n\n# Make sure the HF_TOKEN environment variable is set\n\nStream(\n    handler=...,\n    rtc_configuration=get_hf_turn_credentials,\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n```\n\n----------------------------------------\n\nTITLE: Parsing WebRTC Error Response in JSON\nDESCRIPTION: This snippet shows the JSON structure of an error response from the server when the connection limit is reached. It includes the error status and metadata with the specific error type and limit.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/userguide/api.md#2025-04-17_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"status\": \"failed\",\n    \"meta\": {\n        \"error\": \"concurrency_limit_reached\",\n        \"limit\": 10\n    }\n\n```\n\n----------------------------------------\n\nTITLE: Python Dependencies Requirements File\nDESCRIPTION: A pip requirements file autogenerated by uv, containing pinned versions of all direct and transitive dependencies needed for the LLAMA code editor demo. Includes packages for real-time communication (fastrtc), AI models (openai, groq), web frameworks (fastapi, gradio), and various supporting libraries.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/llama_code_editor/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: text\nCODE:\n```\naiofiles==23.2.1\naiohappyeyeballs==2.4.6\naiohttp==3.11.12\naiohttp-retry==2.9.1\naioice==0.9.0\naiortc==1.10.1\naiosignal==1.3.2\nannotated-types==0.7.0\nanyio==4.6.2.post1\nattrs==25.1.0\naudioread==3.0.1\nav==12.3.0\ncertifi==2024.8.30\ncffi==1.17.1\ncharset-normalizer==3.4.0\nclick==8.1.7\ncoloredlogs==15.0.1\ncryptography==43.0.3\ndecorator==5.1.1\ndistro==1.9.0\ndnspython==2.7.0\nfastapi==0.115.5\nfastrtc==0.0.2.post4\nffmpy==0.4.0\nfilelock==3.16.1\nflatbuffers==24.3.25\nfrozenlist==1.5.0\nfsspec==2024.10.0\ngoogle-crc32c==1.6.0\ngradio==5.16.0\ngradio-client==1.7.0\ngroq==0.18.0\nh11==0.14.0\nhttpcore==1.0.7\nhttpx==0.27.2\nhuggingface-hub==0.28.1\nhumanfriendly==10.0\nidna==3.10\nifaddr==0.2.0\njinja2==3.1.4\njiter==0.7.1\njoblib==1.4.2\nlazy-loader==0.4\nlibrosa==0.10.2.post1\nllvmlite==0.43.0\nmarkdown-it-py==3.0.0\nmarkupsafe==2.1.5\nmdurl==0.1.2\nmpmath==1.3.0\nmsgpack==1.1.0\nmultidict==6.1.0\nnumba==0.60.0\nnumpy==2.0.2\nonnxruntime==1.20.1\nopenai==1.54.4\norjson==3.10.11\npackaging==24.2\npandas==2.2.3\npillow==11.0.0\nplatformdirs==4.3.6\npooch==1.8.2\npropcache==0.2.1\nprotobuf==5.28.3\npycparser==2.22\npydantic==2.9.2\npydantic-core==2.23.4\npydub==0.25.1\npyee==12.1.1\npygments==2.18.0\npyjwt==2.10.1\npylibsrtp==0.10.0\npyopenssl==24.2.1\npython-dateutil==2.9.0.post0\npython-dotenv==1.0.1\npython-multipart==0.0.20\npytz==2024.2\npyyaml==6.0.2\nrequests==2.32.3\nrich==13.9.4\nruff==0.9.6\nsafehttpx==0.1.6\nscikit-learn==1.5.2\nscipy==1.14.1\nsemantic-version==2.10.0\nshellingham==1.5.4\nsix==1.16.0\nsniffio==1.3.1\nsoundfile==0.12.1\nsoxr==0.5.0.post1\nstarlette==0.41.3\nsympy==1.13.3\nthreadpoolctl==3.5.0\ntomlkit==0.12.0\ntqdm==4.67.0\ntwilio==9.4.5\ntyper==0.13.1\ntyping-extensions==4.12.2\ntzdata==2024.2\nurllib3==2.2.3\nuvicorn==0.32.0\nwebsockets==12.0\nyarl==1.18.3\n```\n\n----------------------------------------\n\nTITLE: Installing Kroko-ASR\nDESCRIPTION: Python pip installation command for the FastRTC Kroko package.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/speech_to_text_gallery.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npip install fastrtc-kroko\n```\n\n----------------------------------------\n\nTITLE: Starting Telephone Audio Stream\nDESCRIPTION: Initializes an audio-only telephone stream interface.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/README.md#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nstream.fastphone()\n```\n\n----------------------------------------\n\nTITLE: Installing FastRTC Dependencies\nDESCRIPTION: Package requirements list specifying fastrtc version 0.0.20.rc2 with vad and tts extras, groq client library, and python-dotenv for environment variable management\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_llama4/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastrtc[vad, tts]==0.0.20.rc2\ngroq\npython-dotenv\n```\n\n----------------------------------------\n\nTITLE: Navigating to Frontend Directory\nDESCRIPTION: Changes the current directory to the frontend application folder.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/nextjs_voice_chat/README.md#2025-04-17_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd frontend/fastrtc-demo\n```\n\n----------------------------------------\n\nTITLE: Installing Python Virtual Environment for FastRTC Voice Agent\nDESCRIPTION: Creates a Python virtual environment and activates it, ensuring proper isolation for the FastRTC voice agent project dependencies.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_smolagents/README.md#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Installing and Running the Frontend Application\nDESCRIPTION: Installs NPM dependencies and starts the development server for the frontend application.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/nextjs_voice_chat/README.md#2025-04-17_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Setting up Python Virtual Environment for FastRTC\nDESCRIPTION: Creates a Python virtual environment, activates it, and installs the required dependencies from the requirements.txt file.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/nextjs_voice_chat/README.md#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv env\nsource env/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for FastRTC Voice Agent\nDESCRIPTION: Installs all required dependencies for the FastRTC voice agent project from the requirements.txt file.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/talk_to_smolagents/README.md#2025-04-17_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running the FastRTC Server\nDESCRIPTION: Executes the server application using a shell script.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/demo/nextjs_voice_chat/README.md#2025-04-17_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./run.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Distil-whisper FastRTC\nDESCRIPTION: Python pip installation command for the Distil-whisper FastRTC package.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/speech_to_text_gallery.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npip install distil-whisper-fastrtc\n```\n\n----------------------------------------\n\nTITLE: AWS EC2 Key Pair Creation\nDESCRIPTION: Command for creating an EC2 key pair for AWS TURN server deployment.\nSOURCE: https://github.com/gradio-app/fastrtc/blob/main/docs/deployment.md#2025-04-17_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naws ec2 create-key-pair --key-name your-key-name --query 'KeyMaterial' --output text > your-key-name.pem\n```"
  }
]
[
  {
    "owner": "ollama",
    "repo": "ollama",
    "content": "TITLE: Running Llama 3.2 Model with Ollama\nDESCRIPTION: Basic command to download and run the Llama 3.2 model using Ollama.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nollama run llama3.2\n```\n\n----------------------------------------\n\nTITLE: Making a Raw Mode Chat Completion Request with Ollama API\nDESCRIPTION: Demonstrates how to make a chat completion request in raw mode, bypassing the templating system and providing a full prompt.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\": \"[INST] why is the sky blue? [/INST]\",\n  \"raw\": true,\n  \"stream\": false\n}'\n```\n\n----------------------------------------\n\nTITLE: Starting Ollama Service on Linux\nDESCRIPTION: Command to start the Ollama service after installation. This launches the Ollama server that will handle model loading and inference requests.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nollama serve\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Request Example\nDESCRIPTION: Example of sending a chat message with streaming response enabled using curl to the Ollama API.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"why is the sky blue?\"\n    }\n  ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Completions with Ollama in Python\nDESCRIPTION: Demonstrates how to create chat completions using Ollama's API. Includes examples for text-only and multimodal (text + image) inputs.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            'role': 'user',\n            'content': 'Say this is a test',\n        }\n    ],\n    model='llama3.2',\n)\n\nresponse = client.chat.completions.create(\n    model=\"llava\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"data:image/png;base64,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\",\n                },\n            ],\n        }\n    ],\n    max_tokens=300,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using OpenAI JavaScript Client with Ollama\nDESCRIPTION: This code demonstrates how to initialize and use the OpenAI JavaScript client with Ollama. It shows examples of chat completions, multimodal interactions, text completions, model listing/retrieval, and embedding generation. The client is configured to use Ollama's local API endpoint at port 11434.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport OpenAI from 'openai'\n\nconst openai = new OpenAI({\n  baseURL: 'http://localhost:11434/v1/',\n\n  // required but ignored\n  apiKey: 'ollama',\n})\n\nconst chatCompletion = await openai.chat.completions.create({\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    model: 'llama3.2',\n})\n\nconst response = await openai.chat.completions.create({\n    model: \"llava\",\n    messages: [\n        {\n        role: \"user\",\n        content: [\n            { type: \"text\", text: \"What's in this image?\" },\n            {\n            type: \"image_url\",\n            image_url: \"data:image/png;base64,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\",\n            },\n        ],\n        },\n    ],\n})\n\nconst completion = await openai.completions.create({\n    model: \"llama3.2\",\n    prompt: \"Say this is a test.\",\n})\n\nconst listCompletion = await openai.models.list()\n\nconst model = await openai.models.retrieve(\"llama3.2\")\n\nconst embedding = await openai.embeddings.create({\n  model: \"all-minilm\",\n  input: [\"why is the sky blue?\", \"why is the grass green?\"],\n})\n```\n\n----------------------------------------\n\nTITLE: Listing Running Models in Ollama\nDESCRIPTION: Command to show which models are currently loaded and running.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nollama ps\n```\n\n----------------------------------------\n\nTITLE: JSON Mode Generation Example\nDESCRIPTION: Example of generating content in JSON format using the format parameter set to json.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"What color is the sky at different times of the day? Respond using JSON\",\n  \"format\": \"json\",\n  \"stream\": false\n}'\n```\n\n----------------------------------------\n\nTITLE: Making a Basic Chat Completion Request with Ollama API\nDESCRIPTION: Demonstrates how to make a basic chat completion request to the Ollama API using curl. The request includes a model, prompt, and image data.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llava\",\n  \"prompt\":\"What is in this picture?\",\n  \"stream\": false,\n  \"images\": [\"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\"]\n}'\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Model from Modelfile\nDESCRIPTION: Command to create a custom model named 'mymodel' using a Modelfile.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nollama create mymodel -f ./Modelfile\n```\n\n----------------------------------------\n\nTITLE: Chatting with Ollama API\nDESCRIPTION: cURL command showing how to use Ollama's REST API chat endpoint for conversation with a model.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n  ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Outputs with Ollama in Python\nDESCRIPTION: Demonstrates how to use Ollama to generate structured outputs in JSON format, using Pydantic for schema definition and validation.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n\n# Define the schema for the response\nclass FriendInfo(BaseModel):\n    name: str\n    age: int \n    is_available: bool\n\nclass FriendList(BaseModel):\n    friends: list[FriendInfo]\n\ntry:\n    completion = client.beta.chat.completions.parse(\n        temperature=0,\n        model=\"llama3.1:8b\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"I have two friends. The first is Ollama 22 years old busy saving the world, and the second is Alonso 23 years old and wants to hang out. Return a list of friends in JSON format\"}\n        ],\n        response_format=FriendList,\n    )\n\n    friends_response = completion.choices[0].message\n    if friends_response.parsed:\n        print(friends_response.parsed)\n    elif friends_response.refusal:\n        print(friends_response.refusal)\nexcept Exception as e:\n    print(f\"Error: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Custom Model in Ollama\nDESCRIPTION: Commands to create and run a custom model in Ollama using a Modelfile.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nollama create example -f Modelfile\nollama run example\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Ollama API\nDESCRIPTION: cURL command demonstrating how to use Ollama's REST API to generate a response from a model.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\":\"Why is the sky blue?\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Loading a Model with Ollama API\nDESCRIPTION: Shows how to load a model into memory without generating a response by sending an empty prompt.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Chat Request with Conversation History Example\nDESCRIPTION: Example of sending a chat message with previous conversation history included for context.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"why is the sky blue?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"due to rayleigh scattering.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"how is that different than mie scattering?\"\n    }\n  ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Listing and Retrieving Models with Ollama in Python\nDESCRIPTION: Demonstrates how to list available models and retrieve information about a specific model using Ollama's API.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlist_completion = client.models.list()\n\nmodel = client.models.retrieve(\"llama3.2\")\n```\n\n----------------------------------------\n\nTITLE: Stopping a Model in Ollama (Shell)\nDESCRIPTION: This command demonstrates how to immediately unload a model from memory using the 'ollama stop' command.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/faq.md#2025-04-23_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nollama stop llama3.2\n```\n\n----------------------------------------\n\nTITLE: Setting System Message in Ollama Modelfile\nDESCRIPTION: This snippet demonstrates how to specify a system message in an Ollama Modelfile. System messages are used to provide high-level instructions or context to guide the model's behavior.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nSYSTEM \"\"\"<system message>\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Generate Text Completion Request - Streaming Example\nDESCRIPTION: Example of making a streaming request to generate text completion using the Ollama API with the llama3.2 model.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\"\n}'\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n  \"response\": \"The\",\n  \"done\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Preloading and Keeping a Model in Memory (Shell/cURL)\nDESCRIPTION: This cURL command shows how to preload a model and keep it in memory indefinitely using the 'keep_alive' parameter set to -1.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/faq.md#2025-04-23_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": -1}'\n```\n\n----------------------------------------\n\nTITLE: Structured Output Chat Request Example\nDESCRIPTION: Example of requesting structured JSON output from the chat API using a specific schema format.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST http://localhost:11434/api/chat -H \"Content-Type: application/json\" -d '{\n  \"model\": \"llama3.1\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Ollama is 22 years old and busy saving the world. Return a JSON object with the age and availability.\"}],\n  \"stream\": false,\n  \"format\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"age\": {\n        \"type\": \"integer\"\n      },\n      \"available\": {\n        \"type\": \"boolean\"\n      }\n    },\n    \"required\": [\n      \"age\",\n      \"available\"\n    ]\n  },\n  \"options\": {\n    \"temperature\": 0\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Chat Request with Tools in Ollama\nDESCRIPTION: Example of using function calling/tools functionality to get weather information using the llama3.2 model.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What is the weather today in Paris?\"\n    }\n  ],\n  \"stream\": false,\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather for a location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The location to get the weather for, e.g. San Francisco, CA\"\n            },\n            \"format\": {\n              \"type\": \"string\",\n              \"description\": \"The format to return the weather in, e.g. 'celsius' or 'fahrenheit'\",\n              \"enum\": [\"celsius\", \"fahrenheit\"]\n            }\n          },\n          \"required\": [\"location\", \"format\"]\n        }\n      }\n    }\n  ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Generate Text Completion - Non-Streaming Example\nDESCRIPTION: Example of making a non-streaming request to generate text completion using the Ollama API.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false\n}'\n```\n\n----------------------------------------\n\nTITLE: Making a Chat Completion Request with Custom Options using Ollama API\nDESCRIPTION: Demonstrates how to make a chat completion request with custom runtime options, overriding default model parameters.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false,\n  \"options\": {\n    \"num_keep\": 5,\n    \"seed\": 42,\n    \"num_predict\": 100,\n    \"top_k\": 20,\n    \"top_p\": 0.9,\n    \"min_p\": 0.0,\n    \"typical_p\": 0.7,\n    \"repeat_last_n\": 33,\n    \"temperature\": 0.8,\n    \"repeat_penalty\": 1.2,\n    \"presence_penalty\": 1.5,\n    \"frequency_penalty\": 1.0,\n    \"mirostat\": 1,\n    \"mirostat_tau\": 0.8,\n    \"mirostat_eta\": 0.6,\n    \"penalize_newline\": true,\n    \"stop\": [\"\\n\", \"user:\"],\n    \"numa\": false,\n    \"num_ctx\": 1024,\n    \"num_batch\": 2,\n    \"num_gpu\": 1,\n    \"main_gpu\": 0,\n    \"low_vram\": false,\n    \"vocab_only\": false,\n    \"use_mmap\": true,\n    \"use_mlock\": false,\n    \"num_thread\": 8\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Chat Request with Image Processing in Ollama\nDESCRIPTION: Example of sending a chat message with base64 encoded images to the Ollama API using the llava model.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llava\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is in this image?\",\n      \"images\": [\"iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAA3VSURBVHgB7Z27r0zdG8fX743i1bi1ikMoFMQloXRpKFFIqI7LH4BEQ+NWIkjQuSWCRIEoULk0gsK1kCBI0IhrQVT7tz/7zZo888yz1r7MnDl7z5xvsjkzs2fP3uu71nNfa7lkAsm7d++Sffv2JbNmzUqcc8m0adOSzZs3Z+/XES4ZckAWJEGWPiCxjsQNLWmQsWjRIpMseaxcuTKpG/7HP27I8P79e7dq1ars/yL4/v27S0ejqwv+cUOGEGGpKHR37tzJCEpHV9tnT58+dXXCJDdECBE2Ojrqjh071hpNECjx4cMHVycM1Uhbv359B2F79+51586daxN/+pyRkRFXKyRDAqxEp4yMlDDzXG1NPnnyJKkThoK0VFd1ELZu3TrzXKxKfW7dMBQ6bcuWLW2v0VlHjx41z717967ba22U9APcw7Nnz1oGEPeL3m3p2mTAYYnFmMOMXybPPXv2bNIPpFZr1NHn4HMw0KRBjg9NuRw95s8PEcz/6DZELQd/09C9QGq5RsmSRybqkwHGjh07OsJSsYYm3ijPpyHzoiacg35MLdDSIS/O1yM778jOTwYUkKNHWUzUWaOsylE00MyI0fcnOwIdjvtNdW/HZwNLGg+sR1kMepSNJXmIwxBZiG8tDTpEZzKg0GItNsosY8USkxDhD0Rinuiko2gfL/RbiD2LZAjU9zKQJj8RDR0vJBR1/Phx9+PHj9Z7REF4nTZkxzX4LCXHrV271qXkBAPGfP/atWvu/PnzHe4C97F48eIsRLZ9+3a3f/9+87dwP1JxaF7/3r17ba+5l4EcaVo0lj3SBq5kGTJSQmLWMjgYNei2GPT1MuMqGTDEFHzeQSP2wi/jGnkmPJ/nhccs44jvDAxpVcxnq0F6eT8h4ni/iIWpR5lPyA6ETkNXoSukvpJAD3AsXLiwpZs49+fPn5ke4j10TqYvegSfn0OnafC+Tv9ooA/JPkgQysqQNBzagXY55nO/oa1F7qvIPWkRL12WRpMWUvpVDYmxAPehxWSe8ZEXL20sadYIozfmNch4QJPAfeJgW3rNsnzphBKNJM2KKODo1rVOMRYik5ETy3ix4qWNI81qAAirizgMIc+yhTytx0JWZuNI03qsrgWlGtwjoS9XwgUhWGyhUaRZZQNNIEwCiXD16tXcAHUs79co0vSD8rrJCIW98pzvxpAWyyo3HYwqS0+H0BjStClcZJT5coMm6D2LOF8TolGJtK9fvyZpyiC5ePFi9nc/oJU4eiEP0jVoAnHa9wyJycITMP78+eMeP37sXrx44d6+fdt6f82aNdkx1pg9e3Zb5W+RSRE+n+VjksQWifvVaTKFhn5O8my63K8Qabdv33b379/PiAP//vuvW7BggZszZ072/+TJk91YgkafPn166zXB1rQHFvouAWHq9z3SEevSUerqCn2/dDCeta2jxYbr69evk4MHDyY7d+7MjhMnTiTPnz9Pfv/+nfQT2ggpO2dMF8cghuoM7Ygj5iWCqRlGFml0QC/ftGmTmzt3rmsaKDsgBSPh0/8yPeLLBihLkOKJc0jp8H8vUzcxIA1k6QJ/c78tWEyj5P3o4u9+jywNPdJi5rAH9x0KHcl4Hg570eQp3+vHXGyrmEeigzQsQsjavXt38ujRo44LQuDDhw+TW7duRS1HGgMxhNXHgflaNTOsHyKvHK5Ijo2jbFjJBQK9YwFd6RVMzfgRBmEfP37suBBm/p49e1qjEP2mwTViNRo0VJWH1deMXcNK08uUjVUu7s/zRaL+oLNxz1bpANco4npUgX4G2eFbpDFyQoQxojBCpEGSytmOH8qrH5Q9vuzD6ofQylkCUmh8DBAr+q8JCyVNtWQIidKQE9wNtLSQnS4jDSsxNHogzFuQBw4cyM61UKVsjfr3ooBkPSqqQHesUPWVtzi9/vQi1T+rJj7WiTz4Pt/l3LxUkr5P2VYZaZ4URpsE+st/dujQoaBBYokbrz/8TJNQYLSonrPS9kUaSkPeZyj1AWSj+d+VBoy1pIWVNed8P0Ll/ee5HdGRhrHhR5GGN0r4LGZBaj8oFDJitBTJzIZgFcmU0Y8ytWMZMzJOaXUSrUs5RxKnrxmbb5YXO9VGUhtpXldhEUogFr3IzIsvlpmdosVcGVGXFWp2oU9kLFL3dEkSz6NHEY1sjSRdIuDFWEhd8KxFqsRi1uM/nz9/zpxnwlESONdg6dKlbsaMGS4EHFHtjFIDHwKOo46l4TxSuxgDzi+rE2jg+BaFruOX4HXa0Nnf1lwAPufZeF8/r6zD97WK2qFnGjBxTw5qNGPxT+5T/r7/7RawFC3j4vTp09koCxkeHjqbHJqArmH5UrFKKksnxrK7FuRIs8STfBZv+luugXZ2pR/pP9Ois4z+TiMzUUkUjD0iEi1fzX8GmXyuxUBRcaUfykV0YZnlJGKQpOiGB76x5GeWkWWJc3mOrK6S7xdND+W5N6XyaRgtWJFe13GkaZnKOsYqGdOVVVbGupsyA/l7emTLHi7vwTdirNEt0qxnzAvBFcnQF16xh/TMpUuXHDowhlA9vQVraQhkudRdzOnK+04ZSP3DUhVSP61YsaLtd/ks7ZgtPcXqPqEafHkdqa84X6aCeL7YWlv6edGFHb+ZFICPlljHhg0bKuk0CSvVznWsotRu433alNdFrqG45ejoaPCaUkWERpLXjzFL2Rpllp7PJU2a/v7Ab8N05/9t27Z16KUqoFGsxnI9EosS2niSYg9SpU6B4JgTrvVW1flt1sT+0ADIJU2maXzcUTraGCRaL1Wp9rUMk16PMom8QhruxzvZIegJjFU7LLCePfS8uaQdPny4jTTL0dbee5mYokQsXTIWNY46kuMbnt8Kmec+LGWtOVIl9cT1rCB0V8WqkjAsRwta93TbwNYoGKsUSChN44lgBNCoHLHzquYKrU6qZ8lolCIN0Rh6cP0Q3U6I6IXILYOQI513hJaSKAorFpuHXJNfVlpRtmYBk1Su1obZr5dnKAO+L10Hrj3WZW+E3qh6IszE37F6EB+68mGpvKm4eb9bFrlzrok7fvr0Kfv727dvWRmdVTJHw0qiiCUSZ6wCK+7XL/AcsgNyL74DQQ730sv78Su7+t/A36MdY0sW5o40ahslXr58aZ5HtZB8GH64m9EmMZ7FpYw4T6QnrZfgenrhFxaSiSGXtPnz57e9TkNZLvTjeqhr734CNtrK41L40sUQckmj1lGKQ0rC37x544r8eNXRpnVE3ZZY7zXo8NomiO0ZUCj2uHz58rbXoZ6gc0uA+F6ZeKS/jhRDUq8MKrTho9fEkihMmhxtBI1DxKFY9XLpVcSkfoi8JGnToZO5sU5aiDQIW716ddt7ZLYtMQlhECdBGXZZMWldY5BHm5xgAroWj4C0hbYkSc/jBmggIrXJWlZM6pSETsEPGqZOndr2uuuR5rF169a2HoHPdurUKZM4CO1WTPqaDaAd+GFGKdIQkxAn9RuEWcTRyN2KSUgiSgF5aWzPTeA/lN5rZubMmR2bE4SIC4nJoltgAV/dVefZm72AtctUCJU2CMJ327hxY9t7EHbkyJFseq+EJSY16RPo3Dkq1kkr7+q0bNmyDuLQcZBEPYmHVdOBiJyIlrRDq41YPWfXOxUysi5fvtyaj+2BpcnsUV/oSoEMOk2CQGlr4ckhBwaetBhjCwH0ZHtJROPJkyc7UjcYLDjmrH7ADTEBXFfOYmB0k9oYBOjJ8b4aOYSe7QkKcYhFlq3QYLQhSidNmtS2RATwy8YOM3EQJsUjKiaWZ+vZToUQgzhkHXudb/PW5YMHD9yZM2faPsMwoc7RciYJXbGuBqJ1UIGKKLv915jsvgtJxCZDubdXr165mzdvtr1Hz5LONA8jrUwKPqsmVesKa49S3Q4WxmRPUEYdTjgiUcfUwLx589ySJUva3oMkP6IYddq6HMS4o55xBJBUeRjzfa4Zdeg56QZ43LhxoyPo7Lf1kNt7oO8wWAbNwaYjIv5lhyS7kRf96dvm5Jah8vfvX3flyhX35cuX6HfzFHOToS1H4BenCaHvO8pr8iDuwoUL7tevX+b5ZdbBair0xkFIlFDlW4ZknEClsp/TzXyAKVOmmHWFVSbDNw1l1+4f90U6IY/q4V27dpnE9bJ+v87QEydjqx/UamVVPRG+mwkNTYN+9tjkwzEx+atCm/X9WvWtDtAb68Wy9LXa1UmvCDDIpPkyOQ5ZwSzJ4jMrvFcr0rSjOUh+GcT4LSg5ugkW1Io0/SCDQBojh0hPlaJdah+tkVYrnTZowP8iq1F1TgMBBauufyB33x1v+NWFYmT5KmppgHC+NkAgbmRkpD3yn9QIseXymoTQFGQmIOKTxiZIWpvAatenVqRVXf2nTrAWMsPnKrMZHz6bJq5jvce6QK8J1cQNgKxlJapMPdZSR64/UivS9NztpkVEdKcrs5alhhWP9NeqlfWopzhZScI6QxseegZRGeg5a8C3Re1Mfl1ScP36ddcUaMuv24iOJtz7sbUjTS4qBvKmstYJoUauiuD3k5qhyr7QdUHMeCgLa1Ear9NquemdXgmum4fvJ6w1lqsuDhNrg1qSpleJK7K3TF0Q2jSd94uSZ60kK1e3qyVpQK6PVWXp2/FC3mp6jBhKKOiY2h3gtUV64TWM6wDETRPLDfSakXmH3w8g9Jlug8ZtTt4kVF0kLUYYmCCtD/DrQ5YhMGbA9L3ucdjh0y8kOHW5gU/VEEmJTcL4Pz/f7mgoAbYkAAAAAElFTkSuQmCC\"]\n    }\n  ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Installing Ollama on Linux\nDESCRIPTION: Command to install Ollama on Linux systems using a shell script that is downloaded and executed.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Custom Mario Model\nDESCRIPTION: Commands to pull the base model, create a customized version, and run it.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nollama pull llama3.2\nollama create mario -f ./Modelfile\nollama run mario\n```\n\n----------------------------------------\n\nTITLE: Making Standard Completion Request with Ollama API\nDESCRIPTION: This code demonstrates how to make a standard completion request to the Ollama API using curl. It uses the llama3.2 model and provides a simple prompt.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"llama3.2\",\n        \"prompt\": \"Say this is a test\"\n    }'\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client for Ollama in Python\nDESCRIPTION: Sets up the OpenAI client to use Ollama's API endpoint. Requires the OpenAI Python library and a local Ollama server running.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url='http://localhost:11434/v1/',\n\n    # required but ignored\n    api_key='ollama',\n)\n```\n\n----------------------------------------\n\nTITLE: Preloading Models in Ollama\nDESCRIPTION: Commands to preload models in Ollama for faster response times using both API and CLI methods.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/faq.md#2025-04-23_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\"model\": \"mistral\"}'\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/chat -d '{\"model\": \"mistral\"}'\n```\n\nLANGUAGE: shell\nCODE:\n```\nollama run llama3.2 \"\"\n```\n\n----------------------------------------\n\nTITLE: Generate Code Completion with Suffix\nDESCRIPTION: Example of generating code completion with a suffix parameter using the CodeLlama model.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"codellama:code\",\n  \"prompt\": \"def compute_gcd(a, b):\",\n  \"suffix\": \"    return result\",\n  \"options\": {\n    \"temperature\": 0\n  },\n  \"stream\": false\n}'\n```\n\n----------------------------------------\n\nTITLE: Creating Text Completions with Ollama in Python\nDESCRIPTION: Shows how to generate text completions using Ollama's API. This is a simpler alternative to chat completions for basic text generation tasks.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncompletion = client.completions.create(\n    model=\"llama3.2\",\n    prompt=\"Say this is a test\",\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Multiple Embeddings with POST Request\nDESCRIPTION: Example of the API request to generate embeddings for multiple text inputs simultaneously by providing an array of texts to the input parameter.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_41\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/embed -d '{\n  \"model\": \"all-minilm\",\n  \"input\": [\"Why is the sky blue?\", \"Why is the grass green?\"]\n}'\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings with Ollama API\nDESCRIPTION: This code shows how to create embeddings for multiple text inputs using the Ollama API. It uses the all-minilm model and provides two text inputs to be embedded.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/v1/embeddings \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"all-minilm\",\n        \"input\": [\"why is the sky blue?\", \"why is the grass green?\"]\n    }'\n```\n\n----------------------------------------\n\nTITLE: Adding a Template to a Modelfile for Llama 3\nDESCRIPTION: Example of a TEMPLATE command in a Modelfile that configures how Llama 3 processes inputs with appropriate markers and formatting.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/template.md#2025-04-22_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM llama3.2\n\nTEMPLATE \"\"\"{{- if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>\n{{- end }}\n{{- range .Messages }}<|start_header_id|>{{ .Role }}<|end_header_id|>\n\n{{ .Content }}<|eot_id|>\n{{- end }}<|start_header_id|>assistant<|end_header_id|>\n\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with POST Request\nDESCRIPTION: Example of the API request to generate embeddings from a model. This endpoint accepts a model name and either single text or a list of texts to generate vector embeddings for.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_39\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/embed -d '{\n  \"model\": \"all-minilm\",\n  \"input\": \"Why is the sky blue?\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Running Ollama from Source\nDESCRIPTION: Basic command to build and run Ollama directly from the source code using Go. This is the simplest way to start Ollama for development purposes.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/development.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngo run . serve\n```\n\n----------------------------------------\n\nTITLE: Creating a New Model from Existing Model in Ollama API\nDESCRIPTION: This snippet demonstrates how to create a new model named 'mario' based on the existing 'llama3.2' model using the Ollama API. It sets a custom system prompt for the new model.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/create -d '{\n  \"model\": \"mario\",\n  \"from\": \"llama3.2\",\n  \"system\": \"You are Mario from Super Mario Bros.\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Structured Output Generation Example\nDESCRIPTION: Example of generating structured output using a JSON schema format specification.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST http://localhost:11434/api/generate -H \"Content-Type: application/json\" -d '{\n  \"model\": \"llama3.1:8b\",\n  \"prompt\": \"Ollama is 22 years old and is busy saving the world. Respond using JSON\",\n  \"stream\": false,\n  \"format\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"age\": {\n        \"type\": \"integer\"\n      },\n      \"available\": {\n        \"type\": \"boolean\"\n      }\n    },\n    \"required\": [\n      \"age\",\n      \"available\"\n    ]\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Listing Local Models in Ollama API\nDESCRIPTION: This snippet shows how to retrieve a list of models available locally on the Ollama server.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/tags\n```\n\n----------------------------------------\n\nTITLE: Updating Ollama on Linux\nDESCRIPTION: Commands to update an existing Ollama installation, either using the installation script or by manually downloading and extracting the latest package.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64.tgz\n```\n\n----------------------------------------\n\nTITLE: Creating a Model from Safetensors Directory in Ollama API\nDESCRIPTION: This example demonstrates creating a model from multiple Safetensors files. Each file must be pushed to the server beforehand using the /api/blobs/:digest endpoint.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/create -d '{\n  \"model\": \"fred\",\n  \"files\": {\n    \"config.json\": \"sha256:dd3443e529fb2290423a0c65c2d633e67b419d273f170259e27297219828e389\",\n    \"generation_config.json\": \"sha256:88effbb63300dbbc7390143fbbdd9d9fa50587b37e8bfd16c8c90d4970a74a36\",\n    \"special_tokens_map.json\": \"sha256:b7455f0e8f00539108837bfa586c4fbf424e31f8717819a6798be74bef813d05\",\n    \"tokenizer.json\": \"sha256:bbc1904d35169c542dffbe1f7589a5994ec7426d9e5b609d07bab876f32e97ab\",\n    \"tokenizer_config.json\": \"sha256:24e8a6dc2547164b7002e3125f10b415105644fcf02bf9ad8b674c87b1eaaed6\",\n    \"model.safetensors\": \"sha256:1ff795ff6a07e6a68085d206fb84417da2f083f68391c2843cd2b8ac6df8538f\"\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Displaying Modelfile and Model Information in JSON5 Format\nDESCRIPTION: Shows the structure of a model response from the 'show' API endpoint, including modelfile configuration, parameters, template, model details, capabilities, and other technical information.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_32\n\nLANGUAGE: json5\nCODE:\n```\n{\n  \"modelfile\": \"# Modelfile generated by \\\"ollama show\\\"\\n# To build a new Modelfile based on this one, replace the FROM line with:\\n# FROM llava:latest\\n\\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\\nTEMPLATE \\\"\\\"\\\"{{ .System }}\\nUSER: {{ .Prompt }}\\nASSISTANT: \\\"\\\"\\\"\\nPARAMETER num_ctx 4096\\nPARAMETER stop \\\"\\u003c/s\\u003e\\\"\\nPARAMETER stop \\\"USER:\\\"\\nPARAMETER stop \\\"ASSISTANT:\\\"\",\n  \"parameters\": \"num_keep                       24\\nstop                           \\\"<|start_header_id|>\\\"\\nstop                           \\\"<|end_header_id|>\\\"\\nstop                           \\\"<|eot_id|>\\\"\",\n  \"template\": \"{{ if .System }}<|start_header_id|>system<|end_header_id|>\\n\\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\\n\\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ .Response }}<|eot_id|>\",\n  \"details\": {\n    \"parent_model\": \"\",\n    \"format\": \"gguf\",\n    \"family\": \"llama\",\n    \"families\": [\n      \"llama\"\n    ],\n    \"parameter_size\": \"8.0B\",\n    \"quantization_level\": \"Q4_0\"\n  },\n  \"model_info\": {\n    \"general.architecture\": \"llama\",\n    \"general.file_type\": 2,\n    \"general.parameter_count\": 8030261248,\n    \"general.quantization_version\": 2,\n    \"llama.attention.head_count\": 32,\n    \"llama.attention.head_count_kv\": 8,\n    \"llama.attention.layer_norm_rms_epsilon\": 0.00001,\n    \"llama.block_count\": 32,\n    \"llama.context_length\": 8192,\n    \"llama.embedding_length\": 4096,\n    \"llama.feed_forward_length\": 14336,\n    \"llama.rope.dimension_count\": 128,\n    \"llama.rope.freq_base\": 500000,\n    \"llama.vocab_size\": 128256,\n    \"tokenizer.ggml.bos_token_id\": 128000,\n    \"tokenizer.ggml.eos_token_id\": 128009,\n    \"tokenizer.ggml.merges\": [],            // populates if `verbose=true`\n    \"tokenizer.ggml.model\": \"gpt2\",\n    \"tokenizer.ggml.pre\": \"llama-bpe\",\n    \"tokenizer.ggml.token_type\": [],        // populates if `verbose=true`\n    \"tokenizer.ggml.tokens\": []             // populates if `verbose=true`\n  },\n  \"capabilities\": [\n    \"completion\",\n    \"vision\"\n  ],\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Chat Completion Response from Ollama API\nDESCRIPTION: Shows the JSON response structure returned by the Ollama API for a chat completion request, including model details, response text, and performance metrics.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"llava\",\n  \"created_at\": \"2023-11-03T15:36:02.583064Z\",\n  \"response\": \"A happy cartoon character, which is cute and cheerful.\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 2938432250,\n  \"load_duration\": 2559292,\n  \"prompt_eval_count\": 1,\n  \"prompt_eval_duration\": 2195557000,\n  \"eval_count\": 44,\n  \"eval_duration\": 736432000\n}\n```\n\n----------------------------------------\n\nTITLE: Making Chat Completion Request with Ollama API\nDESCRIPTION: This code demonstrates how to make a basic chat completion request to the Ollama API using curl. It specifies the llama3.2 model and includes a system message and a user message.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"llama3.2\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Hello!\"\n            }\n        ]\n    }'\n```\n\n----------------------------------------\n\nTITLE: Quantizing a Model in Ollama\nDESCRIPTION: Shell command that demonstrates how to create a quantized model using the --quantize flag with the ollama create command.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/import.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nollama create --quantize q4_K_M mymodel\n```\n\n----------------------------------------\n\nTITLE: Showing Model Information in Ollama API\nDESCRIPTION: This example demonstrates how to retrieve detailed information about a specific model (in this case, 'llava') using the Ollama API.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_31\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/show -d '{\n  \"model\": \"llava\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Pulling a Model with Ollama\nDESCRIPTION: Command to download a model from the Ollama library, which can also update an existing local model.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nollama pull llama3.2\n```\n\n----------------------------------------\n\nTITLE: Running a Multimodal Model in Ollama\nDESCRIPTION: Command to run the LLaVA multimodal model with an image file for analysis.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nollama run llava \"What's in this image? /Users/jmorgan/Desktop/smile.png\"\n```\n\n----------------------------------------\n\nTITLE: Pulling a Model with POST Request\nDESCRIPTION: Example of the API request to download a model from the Ollama library. The pull operation can be resumed if cancelled and multiple calls will share the same download progress.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_35\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/pull -d '{\n  \"model\": \"llama3.2\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Pushing a Model Status Response in JSON\nDESCRIPTION: Sample responses showing the various status messages returned during a model push operation, including manifest retrieval, upload progress, and completion status.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_38\n\nLANGUAGE: json\nCODE:\n```\n{ \"status\": \"retrieving manifest\" }\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"starting upload\",\n  \"digest\": \"sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab\",\n  \"total\": 1928429856\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"status\":\"pushing manifest\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"status\":\"success\"}\n```\n\n----------------------------------------\n\nTITLE: Chat Request with Reproducible Outputs in Ollama\nDESCRIPTION: Example of sending a chat message with seed and temperature parameters to get reproducible outputs from the llama3.2 model.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello!\"\n    }\n  ],\n  \"options\": {\n    \"seed\": 101,\n    \"temperature\": 0\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Setting Context Window Size in Ollama\nDESCRIPTION: Various methods to set the context window size in Ollama, including environment variables, CLI commands, and API calls.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/faq.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nOLLAMA_CONTEXT_LENGTH=8192 ollama serve\n```\n\nLANGUAGE: shell\nCODE:\n```\n/set parameter num_ctx 8192\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"num_ctx\": 8192\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Building from GGUF File\nDESCRIPTION: Example showing how to build from a GGUF file using a relative or absolute path.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_5\n\nLANGUAGE: modelfile\nCODE:\n```\nFROM ./ollama-model.gguf\n```\n\n----------------------------------------\n\nTITLE: Generating Embedding with Legacy API Endpoint\nDESCRIPTION: Example of the API request to generate embeddings using the legacy /api/embeddings endpoint, which has been superseded by /api/embed but remains for backward compatibility.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_45\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"all-minilm\",\n  \"prompt\": \"Here is an article about llamas...\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Creating a Modelfile for Safetensors Adapter\nDESCRIPTION: A Dockerfile snippet showing how to create a Modelfile for importing a fine-tuned Safetensors adapter by specifying the base model and adapter path.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/import.md#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM <base model name>\nADAPTER /path/to/safetensors/adapter/directory\n```\n\n----------------------------------------\n\nTITLE: Running a Model in Ollama Docker Container\nDESCRIPTION: Command to execute and run the llama3.2 model inside the Ollama Docker container using docker exec.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/docker.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ndocker exec -it ollama ollama run llama3.2\n```\n\n----------------------------------------\n\nTITLE: Running an Ollama Model\nDESCRIPTION: Shell command to run and test a model after it has been created in Ollama.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/import.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nollama run my-model\n```\n\n----------------------------------------\n\nTITLE: Running Ollama in Docker with CPU Only\nDESCRIPTION: Command to run the Ollama Docker container using only CPU resources. Creates a persistent volume and exposes port 11434.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/docker.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n```\n\n----------------------------------------\n\nTITLE: Handling Model Load Response from Ollama API\nDESCRIPTION: Shows the JSON response structure when loading a model into memory, confirming the model has been loaded successfully.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-12-18T19:52:07.071755Z\",\n  \"response\": \"\",\n  \"done\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Modelfile for Quantization\nDESCRIPTION: A Dockerfile snippet showing how to create a Modelfile for a FP16 or FP32 model that will be quantized.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/import.md#2025-04-22_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM /path/to/my/gemma/f16/model\n```\n\n----------------------------------------\n\nTITLE: Creating a Model from GGUF File in Ollama API\nDESCRIPTION: This snippet illustrates how to create a new model from a GGUF file using the Ollama API. It requires the file to be previously pushed to the server using the /api/blobs/:digest endpoint.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/create -d '{\n  \"model\": \"my-gguf-model\",\n  \"files\": {\n    \"test.gguf\": \"sha256:432f310a77f4650a88d0fd59ecdd7cebed8d684bafea53cbff0473542964f0c3\"\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Retrieving Available Models with Ollama API\nDESCRIPTION: This code shows how to retrieve a list of all available models using the Ollama API. The endpoint returns information about models that have been pulled locally.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/v1/models\n```\n\n----------------------------------------\n\nTITLE: Enabling Ollama Systemd Service on Linux\nDESCRIPTION: Commands to reload systemd and enable the Ollama service to start automatically at boot time. This ensures Ollama is always available after system restarts.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl daemon-reload\nsudo systemctl enable ollama\n```\n\n----------------------------------------\n\nTITLE: Loading and Unloading Models in Ollama\nDESCRIPTION: Examples showing how to load a model into memory and unload it using empty messages array and keep_alive parameter.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": []\n}'\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [],\n  \"keep_alive\": 0\n}'\n```\n\n----------------------------------------\n\nTITLE: Listing Running Models with GET Request\nDESCRIPTION: Example of the API request to list models that are currently loaded into memory using the GET /api/ps endpoint.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_43\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/ps\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings with Ollama in Python\nDESCRIPTION: Shows how to generate embeddings for text inputs using Ollama's API. Embeddings are useful for various NLP tasks and similarity comparisons.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nembeddings = client.embeddings.create(\n    model=\"all-minilm\",\n    input=[\"why is the sky blue?\", \"why is the grass green?\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Reproducible Chat Completion Response from Ollama API\nDESCRIPTION: Shows the JSON response structure for a reproducible chat completion request, including the generated response and performance metrics.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"mistral\",\n  \"created_at\": \"2023-11-03T15:36:02.583064Z\",\n  \"response\": \" The sky appears blue because of a phenomenon called Rayleigh scattering.\",\n  \"done\": true,\n  \"total_duration\": 8493852375,\n  \"load_duration\": 6589624375,\n  \"prompt_eval_count\": 14,\n  \"prompt_eval_duration\": 119039000,\n  \"eval_count\": 110,\n  \"eval_duration\": 1779061000\n}\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Ollama\nDESCRIPTION: Command to run the test suite for the Ollama project using Go's test functionality. This verifies that all components are working correctly.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/development.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ngo test ./...\n```\n\n----------------------------------------\n\nTITLE: Version Response in JSON\nDESCRIPTION: Sample response from the version API endpoint showing the current version of the Ollama software.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_48\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"0.5.1\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Mario Assistant Modelfile Example\nDESCRIPTION: Example Modelfile that creates a Mario-themed assistant with temperature and context window parameters.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_1\n\nLANGUAGE: modelfile\nCODE:\n```\nFROM llama3.2\n# sets the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token\nPARAMETER num_ctx 4096\n\n# sets a custom system message to specify the behavior of the chat assistant\nSYSTEM You are Mario from super mario bros, acting as an assistant.\n```\n\n----------------------------------------\n\nTITLE: Handling Chat Completion Response with Custom Options from Ollama API\nDESCRIPTION: Shows the JSON response structure for a chat completion request with custom options, including the generated response and performance metrics.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"response\": \"The sky is blue because it is the color of the sky.\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 4935886791,\n  \"load_duration\": 534986708,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 107345000,\n  \"eval_count\": 237,\n  \"eval_duration\": 4289432000\n}\n```\n\n----------------------------------------\n\nTITLE: Passing File Content as a Prompt to Ollama\nDESCRIPTION: Command that reads a file's content and passes it to the model for summarization.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nollama run llama3.2 \"Summarize this file: $(cat README.md)\"\n```\n\n----------------------------------------\n\nTITLE: Making API Calls to Ollama from PowerShell\nDESCRIPTION: Example of accessing the Ollama API from PowerShell to generate a response from an LLM model. This shows how to send a POST request with a prompt and convert the JSON response to a PowerShell object.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/windows.md#2025-04-22_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\n(Invoke-WebRequest -method POST -Body '{\"model\":\"llama3.2\", \"prompt\":\"Why is the sky blue?\", \"stream\": false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json\n```\n\n----------------------------------------\n\nTITLE: Creating an Ollama Model from a Safetensors Adapter\nDESCRIPTION: Shell command to create an Ollama model from a Modelfile that references a Safetensors adapter.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/import.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nollama create my-model\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Information in Ollama\nDESCRIPTION: Command to show detailed information about a specific model.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nollama show llama3.2\n```\n\n----------------------------------------\n\nTITLE: Pulling a Model Status Response in JSON\nDESCRIPTION: Sample response showing the various status messages returned during a model pull operation, including manifest pulling, download progress, verification, and completion status.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_36\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"pulling manifest\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"downloading digestname\",\n  \"digest\": \"digestname\",\n  \"total\": 2142590208,\n  \"completed\": 241970\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"status\": \"verifying sha256 digest\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"status\": \"writing manifest\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"status\": \"removing any unused layers\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"status\": \"success\"\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Ollama Logs on Linux with Systemd\nDESCRIPTION: Command for viewing Ollama logs on Linux systems running systemd. This uses journalctl to follow the Ollama service logs and displays them without pagination.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\njournalctl -u ollama --no-pager --follow --pager-end \n```\n\n----------------------------------------\n\nTITLE: Creating Conversation Example in Ollama Modelfile\nDESCRIPTION: This snippet shows how to create a complete example conversation using multiple MESSAGE instructions in an Ollama Modelfile. This example trains the model on geography questions about Canadian locations.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\nMESSAGE user Is Toronto in Canada?\nMESSAGE assistant yes\nMESSAGE user Is Sacramento in Canada?\nMESSAGE assistant no\nMESSAGE user Is Ontario in Canada?\nMESSAGE assistant yes\n```\n\n----------------------------------------\n\nTITLE: Making a Reproducible Chat Completion Request with Ollama API\nDESCRIPTION: Shows how to make a chat completion request with a fixed seed for reproducible outputs.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"seed\": 123\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Accessing Ollama Logs in Docker Container\nDESCRIPTION: Command to view logs for Ollama running in a Docker container. The logs are streamed to stdout/stderr within the container and can be accessed with docker logs.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker logs <container-name>\n```\n\n----------------------------------------\n\nTITLE: Customizing a Model with a Prompt in Ollama\nDESCRIPTION: Example Modelfile that customizes the Llama 3.2 model with parameters and a system message to make it respond as Mario.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nFROM llama3.2\n\n# set the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n\n# set the system message\nSYSTEM \"\"\"\nYou are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker to Use NVIDIA Driver\nDESCRIPTION: Commands to configure Docker to use the NVIDIA driver and restart the Docker service to apply changes.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/docker.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n```\n\n----------------------------------------\n\nTITLE: Starting and Checking Ollama Service Status on Linux\nDESCRIPTION: Commands to start the Ollama systemd service and verify its running status. These commands are used after setting up Ollama as a system service.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl start ollama\nsudo systemctl status ollama\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Logging for Ollama on Windows\nDESCRIPTION: PowerShell command to enable additional debug logging for Ollama on Windows systems. This sets the OLLAMA_DEBUG environment variable before launching the application.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#2025-04-22_snippet_3\n\nLANGUAGE: powershell\nCODE:\n```\n$env:OLLAMA_DEBUG=\"1\"\n& \"ollama app.exe\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Template in Ollama Modelfile\nDESCRIPTION: This snippet shows how to define a template in an Ollama Modelfile using template variables to structure model interactions. The template utilizes specialized tokens to mark the beginning and end of different message types.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nTEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n{{ .System }}<|im_end|>\n{{ end }}{{ if .Prompt }}<|im_start|>user\n{{ .Prompt }}<|im_end|>\n{{ end }}<|im_start|>assistant\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Systemd Service File for Ollama on Linux\nDESCRIPTION: Systemd service configuration for Ollama to run as a background service that starts at boot. The configuration defines user permissions, restart behavior, and dependency handling.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_7\n\nLANGUAGE: ini\nCODE:\n```\n[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/bin/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=\"PATH=$PATH\"\n\n[Install]\nWantedBy=multi-user.target\n```\n\n----------------------------------------\n\nTITLE: Building from Existing Model\nDESCRIPTION: Example showing how to build from an existing model using the FROM instruction.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_4\n\nLANGUAGE: modelfile\nCODE:\n```\nFROM llama3.2\n```\n\n----------------------------------------\n\nTITLE: Implementing Fill-in-Middle for CodeLlama Models\nDESCRIPTION: A template for CodeLlama 7B and 13B code completion models that supports fill-in-middle functionality with prefix and suffix markers.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/template.md#2025-04-22_snippet_4\n\nLANGUAGE: go\nCODE:\n```\n<PRE> {{ .Prompt }} <SUF>{{ .Suffix }} <MID>\n```\n\n----------------------------------------\n\nTITLE: Overriding LLM Library Selection in Ollama\nDESCRIPTION: Command to force Ollama to use a specific LLM library instead of the auto-detected one. This example forces the use of the CPU AVX2 library even when GPU support might be available.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nOLLAMA_LLM_LIBRARY=\"cpu_avx2\" ollama serve\n```\n\n----------------------------------------\n\nTITLE: Basic Modelfile Format\nDESCRIPTION: Shows the basic syntax structure of a Modelfile with comments and instructions.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_0\n\nLANGUAGE: modelfile\nCODE:\n```\n# comment\nINSTRUCTION arguments\n```\n\n----------------------------------------\n\nTITLE: Running Ollama in Docker with AMD GPU\nDESCRIPTION: Command to run Ollama Docker container with AMD GPU support using the rocm tag. Provides access to AMD GPU devices, creates a persistent volume, and exposes port 11434.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/docker.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --device /dev/kfd --device /dev/dri -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:rocm\n```\n\n----------------------------------------\n\nTITLE: Non-Streaming Chat Request Example\nDESCRIPTION: Example of sending a chat message with streaming disabled using curl to the Ollama API.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"why is the sky blue?\"\n    }\n  ],\n  \"stream\": false\n}'\n```\n\n----------------------------------------\n\nTITLE: Installing AMD GPU Support for Ollama on Linux\nDESCRIPTION: Commands to download and install the additional ROCm package for AMD GPU support. This enables hardware acceleration for inference on AMD GPUs.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncurl -L https://ollama.com/download/ollama-linux-amd64-rocm.tgz -o ollama-linux-amd64-rocm.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64-rocm.tgz\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Model from GGUF File\nDESCRIPTION: Example Modelfile that imports a local GGUF model file for use with Ollama.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nFROM ./vicuna-33b.Q4_0.gguf\n```\n\n----------------------------------------\n\nTITLE: Pushing a Model with POST Request\nDESCRIPTION: Example of the API request to upload a model to a model library. This operation requires registering for ollama.ai and adding a public key first.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_37\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/push -d '{\n  \"model\": \"mattw/pygmalion:latest\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Chat Template with Go Templates\nDESCRIPTION: A simple template that iterates through message objects and displays the role and content of each message in a chat format.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/template.md#2025-04-22_snippet_0\n\nLANGUAGE: go\nCODE:\n```\n{{- range .Messages }}\n{{ .Role }}: {{ .Content }}\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Unloading a Model and Freeing Memory (Shell/cURL)\nDESCRIPTION: This cURL command demonstrates how to unload a model and free up memory immediately after generating a response using the 'keep_alive' parameter set to 0.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/faq.md#2025-04-23_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": 0}'\n```\n\n----------------------------------------\n\nTITLE: Building Ollama on Linux\nDESCRIPTION: Commands to configure and build Ollama on Linux systems using CMake. This sets up the build environment and compiles the project.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/development.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncmake -B build\ncmake --build build\n```\n\n----------------------------------------\n\nTITLE: Example Model Benchmark Command\nDESCRIPTION: Specific example of running benchmarks for the llama3.3 model. Demonstrates practical usage of the benchmark command with a real model name.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/benchmark.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngo test -bench=. ./benchmark/... -m llama3.3\n```\n\n----------------------------------------\n\nTITLE: Listing Installed Models in Ollama\nDESCRIPTION: Command to list all models installed on the local machine.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nollama list\n```\n\n----------------------------------------\n\nTITLE: Quantizing a Model in Ollama API\nDESCRIPTION: This example shows how to quantize a non-quantized model named 'llama3.1:8b-instruct-fp16' to a quantized version 'llama3.1:quantized' using the q4_K_M quantization method.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/create -d '{\n  \"model\": \"llama3.1:quantized\",\n  \"from\": \"llama3.1:8b-instruct-fp16\",\n  \"quantize\": \"q4_K_M\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Ollama Service on Linux\nDESCRIPTION: Commands to stop, disable, and remove the Ollama systemd service. This is the first step in completely removing Ollama from a Linux system.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl stop ollama\nsudo systemctl disable ollama\nsudo rm /etc/systemd/system/ollama.service\n```\n\n----------------------------------------\n\nTITLE: Viewing Modelfile Command\nDESCRIPTION: Shell command to display the Modelfile of a specific model.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nollama show --modelfile llama3.2\n```\n\n----------------------------------------\n\nTITLE: Copying a Model with POST Request\nDESCRIPTION: Example of the API request to copy a model. Creates a model with another name from an existing model using the POST /api/copy endpoint.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_33\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/copy -d '{\n  \"source\": \"llama3.2\",\n  \"destination\": \"llama3-backup\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Copying a Model in Ollama\nDESCRIPTION: Command to create a copy of an existing model with a new name.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nollama cp llama3.2 my-model\n```\n\n----------------------------------------\n\nTITLE: Starting the Ollama Runner\nDESCRIPTION: Command to start the runner with a specified model binary file.\nSOURCE: https://github.com/ollama/ollama/blob/main/runner/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./runner -model <model binary>\n```\n\n----------------------------------------\n\nTITLE: Installing Ollama using Shell Script on Linux\nDESCRIPTION: Command to download and execute the Ollama installation script for Linux. This is the recommended way to install Ollama with a single command.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Docker Build and Run Commands for Ollama\nDESCRIPTION: Commands to build and run Ollama Docker container with custom CA certificates and proxy configuration.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/faq.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndocker build -t ollama-with-ca .\ndocker run -d -e HTTPS_PROXY=https://my.proxy.example.com -p 11434:11434 ollama-with-ca\n```\n\n----------------------------------------\n\nTITLE: Retrieving Specific Model Information with Ollama API\nDESCRIPTION: This code demonstrates how to retrieve information about a specific model (llama3.2) using the Ollama API. The endpoint returns details about the requested model.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/v1/models/llama3.2\n```\n\n----------------------------------------\n\nTITLE: Building Ollama with Docker\nDESCRIPTION: Command to build an Ollama Docker image from the repository. This creates a containerized version of Ollama that can run on any Docker-supported platform.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/development.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndocker build .\n```\n\n----------------------------------------\n\nTITLE: Building Ollama Binary (Shell)\nDESCRIPTION: Commands to navigate to the parent directory and build the Ollama binary using Go.\nSOURCE: https://github.com/ollama/ollama/blob/main/macapp/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd ..\ngo build .\n```\n\n----------------------------------------\n\nTITLE: Manually Installing Ollama on Linux (x86_64)\nDESCRIPTION: Commands to manually download and extract the Ollama package for x86_64 Linux systems. This approach allows more control over the installation process.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64.tgz\n```\n\n----------------------------------------\n\nTITLE: Unloading a Model with Ollama API\nDESCRIPTION: Demonstrates how to unload a model from memory by sending an empty prompt with the keep_alive parameter set to 0.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"keep_alive\": 0\n}'\n```\n\n----------------------------------------\n\nTITLE: Deleting a Model with DELETE Request\nDESCRIPTION: Example of the API request to delete a model and its data using the DELETE /api/delete endpoint.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_34\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X DELETE http://localhost:11434/api/delete -d '{\n  \"model\": \"llama3:13b\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Configuring NVIDIA Container Toolkit Repository with Apt\nDESCRIPTION: Commands to configure the NVIDIA Container Toolkit repository for Debian-based systems. Adds the GPG key and repository for installing the NVIDIA Container Toolkit.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/docker.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n    | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n    | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n    | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt-get update\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Ollama Version on Linux\nDESCRIPTION: Command to install a specific version of Ollama by setting the OLLAMA_VERSION environment variable. This allows pinning to a particular release or trying pre-release versions.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ncurl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.5.7 sh\n```\n\n----------------------------------------\n\nTITLE: Handling Model Unload Response from Ollama API\nDESCRIPTION: Shows the JSON response structure when unloading a model from memory, confirming the model has been unloaded successfully.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2024-09-12T03:54:03.516566Z\",\n  \"response\": \"\",\n  \"done\": true,\n  \"done_reason\": \"unload\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GGUF Adapter in Ollama Modelfile\nDESCRIPTION: This snippet demonstrates how to specify a GGUF format LoRA adapter in an Ollama Modelfile. The example uses a relative path to the adapter file.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nADAPTER ./ollama-lora.gguf\n```\n\n----------------------------------------\n\nTITLE: Creating a Modelfile for GGUF Model\nDESCRIPTION: A Dockerfile snippet showing how to create a Modelfile that references a GGUF model file.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/import.md#2025-04-22_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM /path/to/file.gguf\n```\n\n----------------------------------------\n\nTITLE: Running Ollama in Docker with NVIDIA GPU\nDESCRIPTION: Command to run the Ollama Docker container with NVIDIA GPU support. Enables GPU access, creates a persistent volume, and exposes port 11434.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/docker.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n```\n\n----------------------------------------\n\nTITLE: Creating User and Group for Ollama Service on Linux\nDESCRIPTION: Commands to create a dedicated system user and group for running the Ollama service. This improves security by isolating the service from other system processes.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nsudo useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama\nsudo usermod -a -G ollama $(whoami)\n```\n\n----------------------------------------\n\nTITLE: Creating a Modelfile for Safetensors Weights\nDESCRIPTION: A Dockerfile snippet showing how to create a Modelfile that references a directory containing Safetensors weights.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/import.md#2025-04-22_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM /path/to/safetensors/directory\n```\n\n----------------------------------------\n\nTITLE: Setting License Information in Ollama Modelfile\nDESCRIPTION: This snippet shows how to specify the legal license under which a model is shared or distributed in an Ollama Modelfile. The license text is enclosed in triple quotes.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nLICENSE \"\"\"\n<license text>\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Building Ollama with ROCm Support in Docker\nDESCRIPTION: Command to build an Ollama Docker image with AMD ROCm GPU support. This uses a build argument to specify the ROCm flavor.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/development.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndocker build --build-arg FLAVOR=rocm .\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Container Toolkit with Apt\nDESCRIPTION: Command to install the NVIDIA Container Toolkit packages on Debian-based systems using apt-get.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/docker.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install -y nvidia-container-toolkit\n```\n\n----------------------------------------\n\nTITLE: Creating Systemd Override Configuration for Ollama on Linux\nDESCRIPTION: Example systemd override configuration to customize Ollama's environment variables. This allows adding debug flags and other runtime options without modifying the main service file.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_12\n\nLANGUAGE: ini\nCODE:\n```\n[Service]\nEnvironment=\"OLLAMA_DEBUG=1\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Modelfile for GGUF Adapter\nDESCRIPTION: A Dockerfile snippet showing how to create a Modelfile that references a base model and a GGUF adapter file.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/import.md#2025-04-22_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM <model name>\nADAPTER /path/to/file.gguf\n```\n\n----------------------------------------\n\nTITLE: Defining Message History in Ollama Modelfile\nDESCRIPTION: This snippet demonstrates the MESSAGE instruction syntax for specifying a role and message content in an Ollama Modelfile. This instruction is used to build conversation history.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nMESSAGE <role> <message>\n```\n\n----------------------------------------\n\nTITLE: Starting the Ollama Server\nDESCRIPTION: Commands to start the Ollama server and run a model in a separate shell.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\n./ollama serve\n\n# In a separate shell\n./ollama run llama3.2\n```\n\n----------------------------------------\n\nTITLE: Systemd Service Configuration for Ollama\nDESCRIPTION: Systemd service configuration to set environment variables for Ollama server.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/faq.md#2025-04-23_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Ollama Version with GET Request\nDESCRIPTION: Example of the API request to retrieve the current version of Ollama using the GET /api/version endpoint.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_47\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/api/version\n```\n\n----------------------------------------\n\nTITLE: Adding Tool Support to Mistral Models\nDESCRIPTION: A template for Mistral v0.3 and Mixtral 8x22B that supports tool calling functionality, handling various message roles and tool interactions.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/template.md#2025-04-22_snippet_3\n\nLANGUAGE: go\nCODE:\n```\n{{- range $index, $_ := .Messages }}\n{{- if eq .Role \"user\" }}\n{{- if and (le (len (slice $.Messages $index)) 2) $.Tools }}[AVAILABLE_TOOLS] {{ json $.Tools }}[/AVAILABLE_TOOLS]\n{{- end }}[INST] {{ if and (eq (len (slice $.Messages $index)) 1) $.System }}{{ $.System }}\n\n{{ end }}{{ .Content }}[/INST]\n{{- else if eq .Role \"assistant\" }}\n{{- if .Content }} {{ .Content }}</s>\n{{- else if .ToolCalls }}[TOOL_CALLS] [\n{{- range .ToolCalls }}{{\"name\": \"{{ .Function.Name }}\", \"arguments\": {{ json .Function.Arguments }}}}\n{{- end }}]</s>\n{{- end }}\n{{- else if eq .Role \"tool\" }}[TOOL_RESULTS] {\"content\": {{ .Content }}}[/TOOL_RESULTS]\n{{- end }}\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Verifying NVIDIA GPU Drivers for Ollama on Linux\nDESCRIPTION: Command to verify that NVIDIA GPU drivers are properly installed by displaying GPU information. This is necessary for hardware acceleration with NVIDIA GPUs.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nnvidia-smi\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Container Toolkit with Yum\nDESCRIPTION: Command to install the NVIDIA Container Toolkit packages on RPM-based systems using yum.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/docker.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nsudo yum install -y nvidia-container-toolkit\n```\n\n----------------------------------------\n\nTITLE: Preparing and Pushing a Model to Ollama.com\nDESCRIPTION: Shell commands showing how to copy a model to give it the correct username prefix and then push it to ollama.com for sharing.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/import.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nollama cp mymodel myuser/mymodel\nollama push myuser/mymodel\n```\n\n----------------------------------------\n\nTITLE: Legacy Embedding Response in JSON\nDESCRIPTION: Sample response from the legacy embeddings API endpoint showing the generated vector representation of a text prompt. The response format differs from the newer /api/embed endpoint.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_46\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embedding\": [\n    0.5670403838157654, 0.009260174818336964, 0.23178744316101074, -0.2916173040866852, -0.8924556970596313,\n    0.8785552978515625, -0.34576427936553955, 0.5742510557174683, -0.04222835972905159, -0.137906014919281\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing ChatML Template Format for Messages\nDESCRIPTION: A template using the ChatML format with message markers, which is compatible with models like DBRX, Neural Chat, and Orca 2.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/template.md#2025-04-22_snippet_2\n\nLANGUAGE: go\nCODE:\n```\n{{- range .Messages }}<|im_start|>{{ .Role }}\n{{ .Content }}<|im_end|>\n{{ end }}<|im_start|>assistant\n```\n\n----------------------------------------\n\nTITLE: Editing Ollama Service Configuration on Linux\nDESCRIPTION: Command to edit the Ollama systemd service configuration using the built-in systemd editor. This allows customizing service parameters after installation.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl edit ollama\n```\n\n----------------------------------------\n\nTITLE: Running a Shared Ollama Model\nDESCRIPTION: Shell command showing how to run a model that has been shared on ollama.com by another user.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/import.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nollama run myuser/mymodel\n```\n\n----------------------------------------\n\nTITLE: Building Ollama with ROCm Support on Windows\nDESCRIPTION: Special CMake command flags for building Ollama with AMD ROCm GPU support on Windows. This requires using Ninja as the build generator and specific compiler settings.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/development.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncmake -B build -G Ninja -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Implementing Fill-in-Middle for Codestral Models\nDESCRIPTION: A template for Codestral 22B that supports fill-in-middle functionality with prefix and suffix markers in a different format.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/template.md#2025-04-22_snippet_5\n\nLANGUAGE: go\nCODE:\n```\n[SUFFIX]{{ .Suffix }}[PREFIX] {{ .Prompt }}\n```\n\n----------------------------------------\n\nTITLE: Checking CPU Features on Linux for Ollama Compatibility\nDESCRIPTION: Command to view CPU features on Linux to determine which Ollama LLM library is most compatible with your system. This helps identify if your CPU supports AVX or AVX2 instructions.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncat /proc/cpuinfo| grep flags | head -1\n```\n\n----------------------------------------\n\nTITLE: Configuring NVIDIA Container Toolkit Repository with Yum/Dnf\nDESCRIPTION: Command to configure the NVIDIA Container Toolkit repository for RPM-based systems using yum or dnf.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/docker.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo \\\n    | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo\n```\n\n----------------------------------------\n\nTITLE: Installing Ollama on ARM64 Linux Systems\nDESCRIPTION: Commands to download and extract the Ollama package specifically for ARM64 architecture Linux systems. This is necessary for installations on ARM-based devices.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncurl -L https://ollama.com/download/ollama-linux-arm64.tgz -o ollama-linux-arm64.tgz\nsudo tar -C /usr -xzf ollama-linux-arm64.tgz\n```\n\n----------------------------------------\n\nTITLE: Viewing Ollama Service Logs on Linux\nDESCRIPTION: Command to view Ollama service logs from systemd journal. This helps with debugging issues and monitoring service activity.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\njournalctl -e -u ollama\n```\n\n----------------------------------------\n\nTITLE: Making Vision-based Chat Completion Request with Ollama API\nDESCRIPTION: This code shows how to make a vision-based chat completion request to the Ollama API using curl. It uses the llava model and includes both text and a base64-encoded image as content in the user message.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/openai.md#2025-04-23_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:11434/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"llava\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What\\'s in this image?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n               \"url\": \"data:image/png;base64,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\"\n            }\n          }\n        ]\n      }\n    ],\n    \"max_tokens\": 300\n  }'\n```\n\n----------------------------------------\n\nTITLE: Installing Ollama Linux Update Script\nDESCRIPTION: Shell command to upgrade Ollama on Linux systems using the official installation script.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/faq.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Verifying Ollama Installation on Linux\nDESCRIPTION: Command to verify that Ollama is correctly installed and running by checking its version. This confirms the installation was successful.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nollama -v\n```\n\n----------------------------------------\n\nTITLE: Building Ollama on macOS Intel\nDESCRIPTION: Commands to configure and build Ollama on Intel-based macOS systems using CMake. This two-step process creates the necessary build files before compilation.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/development.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncmake -B build\ncmake --build build\n```\n\n----------------------------------------\n\nTITLE: Embeddings Response in JSON for Multiple Inputs\nDESCRIPTION: Sample response from the embeddings API endpoint showing the generated vector representations for multiple text inputs, with each input corresponding to one embedding vector in the response array.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_42\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"all-minilm\",\n  \"embeddings\": [[\n    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,\n    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348\n  ],[\n    -0.0098027075, 0.06042469, 0.025257962, -0.006364387, 0.07272725,\n    0.017194884, 0.09032035, -0.051705178, 0.09951512, 0.09072481\n  ]]\n}\n```\n\n----------------------------------------\n\nTITLE: Nginx Proxy Configuration for Ollama\nDESCRIPTION: Nginx server configuration to proxy requests to Ollama server.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/faq.md#2025-04-23_snippet_2\n\nLANGUAGE: nginx\nCODE:\n```\nserver {\n    listen 80;\n    server_name example.com;  # Replace with your domain or IP\n    location / {\n        proxy_pass http://localhost:11434;\n        proxy_set_header Host localhost:11434;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Benchmark Command\nDESCRIPTION: Basic command syntax for running Ollama benchmarks with a specified model. Executes all benchmark tests in the benchmark directory.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/benchmark.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngo test -bench=. ./benchmark/... -m $MODEL_NAME\n```\n\n----------------------------------------\n\nTITLE: Building Ollama on Windows\nDESCRIPTION: Commands to configure and build Ollama on Windows using CMake. These commands create the necessary build files and compile in Release mode.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/development.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncmake -B build\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Checking Blob Existence in Ollama API\nDESCRIPTION: This snippet shows how to check if a specific blob exists on the Ollama server using its SHA256 digest.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\ncurl -I http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2\n```\n\n----------------------------------------\n\nTITLE: Custom Dockerfile for Ollama with CA Certificate\nDESCRIPTION: Dockerfile configuration to add custom CA certificates for proxy support in Ollama container.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/faq.md#2025-04-23_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM ollama/ollama\nCOPY my-ca.pem /usr/local/share/ca-certificates/my-ca.crt\nRUN update-ca-certificates\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Ollama Version on Linux\nDESCRIPTION: Command to install a specific version of Ollama on Linux systems. This example uses the installation script with an environment variable to specify version 0.5.7.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncurl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.5.7 sh\n```\n\n----------------------------------------\n\nTITLE: Accessing Ollama Logs on macOS\nDESCRIPTION: Command to view Ollama server logs on macOS systems. The logs are stored in the user's home directory within the .ollama/logs folder.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncat ~/.ollama/logs/server.log\n```\n\n----------------------------------------\n\nTITLE: Pushing a Blob to Ollama Server\nDESCRIPTION: This example demonstrates how to push a file (model.gguf) to the Ollama server, creating a blob with a specified SHA256 digest.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\ncurl -T model.gguf -X POST http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests Command\nDESCRIPTION: Command to run integration tests using Go test with integration tag\nSOURCE: https://github.com/ollama/ollama/blob/main/integration/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngo test -tags=integration ./...\n```\n\n----------------------------------------\n\nTITLE: Stopping a Running Model in Ollama\nDESCRIPTION: Command to stop a specific model that is currently running.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nollama stop llama3.2\n```\n\n----------------------------------------\n\nTITLE: Running Ollama API Examples in Go\nDESCRIPTION: Demonstrates how to run any example in the directory using the Go command line tool. The user should replace 'example_name' with the specific directory containing the example they want to run.\nSOURCE: https://github.com/ollama/ollama/blob/main/api/examples/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngo run example_name/main.go\n```\n\n----------------------------------------\n\nTITLE: Applying Patches to Vendored llama.cpp Code in Ollama\nDESCRIPTION: This command applies existing patches to the vendored llama.cpp code in the Ollama project. It's used to set up the tracking repo in the ./vendor/ directory with all current modifications.\nSOURCE: https://github.com/ollama/ollama/blob/main/llama/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmake -f Makefile.sync apply-patches\n```\n\n----------------------------------------\n\nTITLE: Defining Function for Including GGML Backends\nDESCRIPTION: Creates a function to selectively include backend subdirectories based on configuration options, and sets appropriate compile definitions for statically linked backends.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ggml_add_backend backend)\n    string(TOUPPER \"GGML_${backend}\" backend_id)\n    if (${backend_id})\n        string(TOLOWER \"ggml-${backend}\" backend_target)\n        add_subdirectory(${backend_target})\n        message(STATUS \"Including ${backend} backend\")\n        if (NOT GGML_BACKEND_DL)\n            string(TOUPPER \"GGML_USE_${backend}\" backend_use)\n            target_compile_definitions(ggml PUBLIC ${backend_use})\n        endif()\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Removing a Model from Ollama\nDESCRIPTION: Command to delete a model from local storage.\nSOURCE: https://github.com/ollama/ollama/blob/main/README.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nollama rm llama3.2\n```\n\n----------------------------------------\n\nTITLE: Preparing Vendored llama.cpp Code for Updates in Ollama\nDESCRIPTION: This command cleans the existing vendored code and applies all current patches. It's used as the first step when working on new fixes or features that impact the vendored code.\nSOURCE: https://github.com/ollama/ollama/blob/main/llama/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmake -f Makefile.sync clean apply-patches\n```\n\n----------------------------------------\n\nTITLE: Defining Function for Adding CPU Backend Variants\nDESCRIPTION: Creates a function to configure CPU backend variants with different instruction set optimizations (AVX, AVX2, BMI2, etc.), allowing for multiple architecture-specific builds.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ggml_add_cpu_backend_variant tag_name)\n    set(GGML_CPU_TAG_NAME ${tag_name})\n    # other: OPENMP LLAMAFILE CPU_HBM\n    foreach (feat NATIVE\n                  AVX AVX2 BMI2 AVX_VNNI FMA F16C\n                  AVX512 AVX512_VBMI AVX512_VNNI AVX512_BF16\n                  AMX_TILE AMX_INT8 AMX_BF16)\n        set(GGML_${feat} OFF)\n    endforeach()\n\n    foreach (feat ${ARGN})\n        set(GGML_${feat} ON)\n    endforeach()\n\n    ggml_add_cpu_backend_variant_impl(${tag_name})\n    add_dependencies(ggml-cpu ggml-cpu-${tag_name})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Removing Ollama Binary from Linux\nDESCRIPTION: Command to remove the Ollama executable from the system binary path. This uses 'which' to automatically locate the binary regardless of installation location.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nsudo rm $(which ollama)\n```\n\n----------------------------------------\n\nTITLE: Changing Ollama Installation Directory in Windows\nDESCRIPTION: Command for installing Ollama in a custom directory location instead of the default home directory. This is useful when you need to install Ollama in a location with more available space.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/windows.md#2025-04-22_snippet_0\n\nLANGUAGE: powershell\nCODE:\n```\nOllamaSetup.exe /DIR=\"d:\\some\\location\"\n```\n\n----------------------------------------\n\nTITLE: Formatting Patches and Syncing Vendored llama.cpp Code in Ollama\nDESCRIPTION: This command generates formatted patches from changes made to the vendored llama.cpp code and syncs them with the main Ollama project. It's used after resolving conflicts and applying all patches successfully.\nSOURCE: https://github.com/ollama/ollama/blob/main/llama/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmake -f Makefile.sync format-patches sync\n```\n\n----------------------------------------\n\nTITLE: Adding CPU Backend and Configuring CPU Variants\nDESCRIPTION: Adds the CPU backend and conditionally configures multiple CPU variants (sandybridge, haswell, skylakex, icelake, alderlake) with appropriate instruction set optimizations when GGML_CPU_ALL_VARIANTS is enabled.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nggml_add_backend(CPU)\n\nif (GGML_CPU_ALL_VARIANTS)\n    if (NOT GGML_BACKEND_DL)\n        message(FATAL_ERROR \"GGML_CPU_ALL_VARIANTS requires GGML_BACKEND_DL\")\n    endif()\n    add_custom_target(ggml-cpu)\n    ggml_add_cpu_backend_variant(sandybridge    AVX)\n    ggml_add_cpu_backend_variant(haswell        AVX F16C AVX2 BMI2 FMA)\n    ggml_add_cpu_backend_variant(skylakex       AVX F16C AVX2 BMI2 FMA AVX512)\n    ggml_add_cpu_backend_variant(icelake        AVX F16C AVX2 BMI2 FMA AVX512 AVX512_VBMI AVX512_VNNI)\n    ggml_add_cpu_backend_variant(alderlake      AVX F16C AVX2 BMI2 FMA AVX_VNNI)\nelseif (GGML_CPU)\n    ggml_add_cpu_backend_variant_impl(\"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Embeddings Response in JSON for Single Input\nDESCRIPTION: Sample response from the embeddings API endpoint showing the generated vector representation of a text input, along with performance metrics like duration and token counts.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_40\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"all-minilm\",\n  \"embeddings\": [[\n    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,\n    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348\n  ]],\n  \"total_duration\": 14143917,\n  \"load_duration\": 1019500,\n  \"prompt_eval_count\": 8\n}\n```\n\n----------------------------------------\n\nTITLE: Docker Image Manifest Structure\nDESCRIPTION: Defines the manifest structure for a Docker container image with custom Ollama model layer. Specifies schema version, media types, content digests and sizes for config and layers.\nSOURCE: https://github.com/ollama/ollama/blob/main/server/internal/registry/testdata/registry.txt#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"schemaVersion\": 2,\n  \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n  \"config\": {\n    \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n    \"digest\": \"sha256:ca3d163bab055381827226140568f3bef7eaac187cebd76878e0b63e9e442356\",\n    \"size\": 3\n  },\n  \"layers\": [\n    {\n      \"mediaType\": \"application/vnd.ollama.image.model\",\n      \"digest\": \"sha256:68e0ec597aee59d35f8dc44942d7b17d471ade10d3aca07a5bb7177713950312\",\n      \"size\": 5\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Checking GGML Backend Dynamic Loading Compatibility with Shared Libraries\nDESCRIPTION: Verifies that GGML_BACKEND_DL option requires BUILD_SHARED_LIBS to be enabled, otherwise throws a fatal error.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_BACKEND_DL AND NOT BUILD_SHARED_LIBS)\n    message(FATAL_ERROR \"GGML_BACKEND_DL requires BUILD_SHARED_LIBS\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Models Response in JSON\nDESCRIPTION: Sample response from the list running models API endpoint showing details about models currently loaded into memory, including name, size, digest, model family, parameters, and expiration time.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/api.md#2025-04-23_snippet_44\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"name\": \"mistral:latest\",\n      \"model\": \"mistral:latest\",\n      \"size\": 5137025024,\n      \"digest\": \"2ae6f6dd7a3dd734790bbbf58b8909a606e0e7e97e94b7604e0aa7ae4490e6d8\",\n      \"details\": {\n        \"parent_model\": \"\",\n        \"format\": \"gguf\",\n        \"family\": \"llama\",\n        \"families\": [\n          \"llama\"\n        ],\n        \"parameter_size\": \"7.2B\",\n        \"quantization_level\": \"Q4_0\"\n      },\n      \"expires_at\": \"2024-06-04T14:38:31.83753-07:00\",\n      \"size_vram\": 5137025024\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Synctest in Go 1.24\nDESCRIPTION: Commands for running tests with the 'synctest' package enabled in Go 1.24. This helps identify synchronization issues in the codebase that would be caught by CI.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/development.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nGOEXPERIMENT=synctest go test ./...\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML Base Target Properties\nDESCRIPTION: Sets include directories for the ggml-base target and conditionally adds the GGML_BACKEND_DL compile definition when dynamic loading is enabled.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(ggml-base PRIVATE .)\nif (GGML_BACKEND_DL)\n    target_compile_definitions(ggml-base PUBLIC GGML_BACKEND_DL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Common Target Properties for GGML Libraries\nDESCRIPTION: Sets common properties for ggml-base and ggml targets, including include directories and C/C++ language standards. Ensures consistent build configuration across all GGML components.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nforeach (target ggml-base ggml)\n    target_include_directories(${target} PUBLIC    $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/../include> $<INSTALL_INTERFACE:include>)\n    target_compile_features   (${target} PRIVATE c_std_11 cxx_std_17) # don't bump\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Configuring Safetensor Adapter in Ollama Modelfile\nDESCRIPTION: This snippet shows how to specify a fine-tuned LoRA adapter using the Safetensor format in an Ollama Modelfile. The adapter path can be absolute or relative to the Modelfile location.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nADAPTER <path to safetensor adapter>\n```\n\n----------------------------------------\n\nTITLE: Finding Required Libraries for Metal Backend in CMake\nDESCRIPTION: Locates the necessary Foundation, Metal, and MetalKit libraries for the Metal backend implementation.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_library(FOUNDATION_LIBRARY Foundation REQUIRED)\nfind_library(METAL_FRAMEWORK    Metal      REQUIRED)\nfind_library(METALKIT_FRAMEWORK MetalKit   REQUIRED)\n\nmessage(STATUS \"Metal framework found\")\n```\n\n----------------------------------------\n\nTITLE: Embedding Metal Library in CMake for GGML\nDESCRIPTION: Configures the embedding of the Metal library into the GGML binary when GGML_METAL_EMBED_LIBRARY is enabled. This involves merging header files and creating an assembly file.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_METAL_EMBED_LIBRARY)\n    enable_language(ASM)\n\n    add_compile_definitions(GGML_METAL_EMBED_LIBRARY)\n\n    set(METALLIB_SOURCE \"${CMAKE_CURRENT_SOURCE_DIR}/ggml-metal.metal\")\n    set(METALLIB_IMPL   \"${CMAKE_CURRENT_SOURCE_DIR}/ggml-metal-impl.h\")\n\n    file(MAKE_DIRECTORY \"${CMAKE_BINARY_DIR}/autogenerated\")\n\n    set(METALLIB_EMBED_ASM        \"${CMAKE_BINARY_DIR}/autogenerated/ggml-metal-embed.s\")\n    set(METALLIB_SOURCE_EMBED     \"${CMAKE_BINARY_DIR}/autogenerated/ggml-metal-embed.metal\")\n    set(METALLIB_SOURCE_EMBED_TMP \"${CMAKE_BINARY_DIR}/autogenerated/ggml-metal-embed.metal.tmp\")\n\n    add_custom_command(\n        OUTPUT ${METALLIB_EMBED_ASM}\n        COMMAND echo \"Embedding Metal library\"\n        COMMAND sed -e '/__embed_ggml-common.h__/r         ${METALLIB_COMMON}' -e '/__embed_ggml-common.h__/d'         < ${METALLIB_SOURCE}           > ${METALLIB_SOURCE_EMBED_TMP}\n        COMMAND sed -e '/\\#include \\\"ggml-metal-impl.h\\\"/r ${METALLIB_IMPL}'   -e '/\\#include \\\"ggml-metal-impl.h\\\"/d' < ${METALLIB_SOURCE_EMBED_TMP} > ${METALLIB_SOURCE_EMBED}\n        COMMAND echo \".section __DATA,__ggml_metallib\"          >  ${METALLIB_EMBED_ASM}\n        COMMAND echo \".globl _ggml_metallib_start\"              >> ${METALLIB_EMBED_ASM}\n        COMMAND echo \"_ggml_metallib_start:\"                    >> ${METALLIB_EMBED_ASM}\n        COMMAND echo \".incbin \\\"${METALLIB_SOURCE_EMBED}\\\"\" >> ${METALLIB_EMBED_ASM}\n        COMMAND echo \".globl _ggml_metallib_end\"                >> ${METALLIB_EMBED_ASM}\n        COMMAND echo \"_ggml_metallib_end:\"                      >> ${METALLIB_EMBED_ASM}\n        DEPENDS ../ggml-common.h ggml-metal.metal ggml-metal-impl.h\n        COMMENT \"Generate assembly for embedded Metal library\"\n    )\n\n    target_sources(ggml-metal PRIVATE ${METALLIB_EMBED_ASM})\nelse()\n    # ... (code for non-embedded library compilation)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generated Modelfile Output\nDESCRIPTION: Example output showing a generated Modelfile with template configuration and parameters.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#2025-04-22_snippet_3\n\nLANGUAGE: modelfile\nCODE:\n```\n# Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this one, replace the FROM line with:\n# FROM llama3.2:latest\nFROM /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29\nTEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>\"\"\"\nPARAMETER stop \"<|start_header_id|>\"\nPARAMETER stop \"<|end_header_id|>\"\nPARAMETER stop \"<|eot_id|>\"\nPARAMETER stop \"<|reserved_special_token\"\n```\n\n----------------------------------------\n\nTITLE: Generating Patches for Updated Vendored llama.cpp Code in Ollama\nDESCRIPTION: This command generates patches from changes made to the vendored llama.cpp code. It's used after committing changes in the ./vendor/ directory to create patches for submission to the Ollama project.\nSOURCE: https://github.com/ollama/ollama/blob/main/llama/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmake -f Makefile.sync format-patches\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific POSIX Conformance Settings\nDESCRIPTION: Configures POSIX conformance and platform-specific definitions for various operating systems including Linux, BSD variants, and macOS.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (CMAKE_SYSTEM_NAME MATCHES \"OpenBSD\")\n    add_compile_definitions(_XOPEN_SOURCE=700)\nelse()\n    add_compile_definitions(_XOPEN_SOURCE=600)\nendif()\n\nif (CMAKE_SYSTEM_NAME MATCHES \"Linux\" OR CMAKE_SYSTEM_NAME MATCHES \"Android\")\n    add_compile_definitions(_GNU_SOURCE)\nendif()\n\nif (\n    CMAKE_SYSTEM_NAME MATCHES \"Darwin\" OR\n    CMAKE_SYSTEM_NAME MATCHES \"iOS\"    OR\n    CMAKE_SYSTEM_NAME MATCHES \"tvOS\"   OR\n    CMAKE_SYSTEM_NAME MATCHES \"DragonFly\"\n)\n    add_compile_definitions(_DARWIN_C_SOURCE)\nendif()\n\nif (CMAKE_SYSTEM_NAME MATCHES \"FreeBSD\")\n    add_compile_definitions(__BSD_VISIBLE)\nendif()\nif (CMAKE_SYSTEM_NAME MATCHES \"NetBSD\")\n    add_compile_definitions(_NETBSD_SOURCE)\nendif()\nif (CMAKE_SYSTEM_NAME MATCHES \"OpenBSD\")\n    add_compile_definitions(_BSD_SOURCE)\nendif()\n\nif (WIN32)\n    add_compile_definitions(_CRT_SECURE_NO_WARNINGS)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Architectures for GGML Library in CMake\nDESCRIPTION: Configures CUDA architectures based on GGML-specific options and CUDA Toolkit version. It sets different architectures for various GPU capabilities and CUDA features.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n    if (GGML_NATIVE AND CUDAToolkit_VERSION VERSION_GREATER_EQUAL \"11.6\" AND CMAKE_VERSION VERSION_GREATER_EQUAL \"3.24\")\n        set(CMAKE_CUDA_ARCHITECTURES \"native\")\n    elseif(GGML_CUDA_F16 OR GGML_CUDA_DMMV_F16)\n        set(CMAKE_CUDA_ARCHITECTURES \"60;61;70;75;80\")\n    else()\n        set(CMAKE_CUDA_ARCHITECTURES \"50;61;70;75;80\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Ollama Desktop App (Shell)\nDESCRIPTION: Commands to navigate to the macapp directory, install dependencies, and start the desktop application using npm.\nSOURCE: https://github.com/ollama/ollama/blob/main/macapp/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd macapp\nnpm install\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Adding Optional GGML Backends\nDESCRIPTION: Adds various optional backends (BLAS, CUDA, Metal, etc.) to the build configuration when enabled, allowing GGML to utilize different hardware acceleration technologies.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nggml_add_backend(BLAS)\nggml_add_backend(CANN)\nggml_add_backend(CUDA)\nggml_add_backend(HIP)\nggml_add_backend(Kompute)\nggml_add_backend(METAL)\nggml_add_backend(MUSA)\nggml_add_backend(RPC)\nggml_add_backend(SYCL)\nggml_add_backend(Vulkan)\nggml_add_backend(OpenCL)\n```\n\n----------------------------------------\n\nTITLE: Configuring ROCm Path Settings in CMake\nDESCRIPTION: Sets up the ROCm installation path based on environment variables or default locations. Checks for ROCm installation in standard locations and configures CMAKE_PREFIX_PATH accordingly.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-hip/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT EXISTS $ENV{ROCM_PATH})\n    if (NOT EXISTS /opt/rocm)\n        set(ROCM_PATH /usr)\n    else()\n        set(ROCM_PATH /opt/rocm)\n    endif()\nelse()\n    set(ROCM_PATH $ENV{ROCM_PATH})\nendif()\n\nlist(APPEND CMAKE_PREFIX_PATH  ${ROCM_PATH})\nlist(APPEND CMAKE_PREFIX_PATH \"${ROCM_PATH}/lib64/cmake\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Synctest for All Go Commands\nDESCRIPTION: Command to enable the 'synctest' package for all Go commands by setting a persistent environment variable. This is useful for developers who frequently need to test with synctest enabled.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/development.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ngo env -w GOEXPERIMENT=synctest\n```\n\n----------------------------------------\n\nTITLE: Removing Ollama User Data and System Account on Linux\nDESCRIPTION: Commands to remove Ollama's downloaded models, user data, and system user/group. This cleans up persistent data stored by Ollama.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\nsudo rm -r /usr/share/ollama\nsudo userdel ollama\nsudo groupdel ollama\n```\n\n----------------------------------------\n\nTITLE: Compiling Metal Shaders for GGML in CMake\nDESCRIPTION: Configures the compilation of Metal shaders when not embedding the library. This includes setting compiler flags and creating custom commands for shader compilation.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_METAL_SHADER_DEBUG)\n    set(XC_FLAGS -fno-fast-math -fno-inline -g)\nelse()\n    set(XC_FLAGS -O3)\nendif()\n\nif (GGML_METAL_MACOSX_VERSION_MIN)\n    message(STATUS \"Adding  -mmacosx-version-min=${GGML_METAL_MACOSX_VERSION_MIN} flag to metal compilation\")\n    list   (APPEND XC_FLAGS -mmacosx-version-min=${GGML_METAL_MACOSX_VERSION_MIN})\nendif()\n\nif (GGML_METAL_STD)\n    message(STATUS \"Adding  -std=${GGML_METAL_STD} flag to metal compilation\")\n    list   (APPEND XC_FLAGS -std=${GGML_METAL_STD})\nendif()\n\nadd_custom_command(\n    OUTPUT ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib\n    COMMAND xcrun -sdk macosx metal ${XC_FLAGS} -c ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal.metal -o - |\n        xcrun -sdk macosx metallib - -o ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib\n    COMMAND rm -f ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-common.h\n    COMMAND rm -f ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal.metal\n    DEPENDS ggml-metal.metal ${METALLIB_COMMON}\n    COMMENT \"Compiling Metal kernels\"\n    )\n\nadd_custom_target(\n    ggml-metal-lib ALL\n    DEPENDS ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib\n    )\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies and Version Checking\nDESCRIPTION: Finds required HIP packages and checks version compatibility. Verifies the presence of hip, hipblas, and rocblas packages, with optional rocwmma support.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-hip/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(hip     REQUIRED)\nfind_package(hipblas REQUIRED)\nfind_package(rocblas REQUIRED)\nif (GGML_HIP_ROCWMMA_FATTN)\n    CHECK_INCLUDE_FILE_CXX(\"rocwmma/rocwmma.hpp\" FOUND_ROCWMMA)\n    if (NOT ${FOUND_ROCWMMA})\n        message(FATAL_ERROR \"rocwmma has not been found\")\n    endif()\nendif()\n\nif (${hip_VERSION} VERSION_LESS 5.5)\n    message(FATAL_ERROR \"At least ROCM/HIP V5.5 is required\")\nendif()\n\nmessage(STATUS \"HIP and hipBLAS found\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Project with GGML Settings in CMake\nDESCRIPTION: Configures the Ollama project build environment with GGML library settings. Sets CMake variables for building with specific optimizations and backend support for CPU, CUDA, and HIP. Includes paths, runtime options, and detailed configuration for hardware acceleration.\nSOURCE: https://github.com/ollama/ollama/blob/main/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.21)\n\nproject(Ollama C CXX)\n\ninclude(CheckLanguage)\n\nfind_package(Threads REQUIRED)\n\nset(CMAKE_BUILD_TYPE Release)\nset(BUILD_SHARED_LIBS ON)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\n\nset(GGML_BUILD ON)\nset(GGML_SHARED ON)\nset(GGML_CCACHE ON)\nset(GGML_BACKEND_DL ON)\nset(GGML_BACKEND_SHARED ON)\nset(GGML_SCHED_MAX_COPIES 4)\n\nset(GGML_LLAMAFILE ON)\nset(GGML_CUDA_PEER_MAX_BATCH_SIZE 128)\nset(GGML_CUDA_GRAPHS ON)\nset(GGML_CUDA_FA ON)\nset(GGML_CUDA_COMPRESSION_MODE default)\n\nif((CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_OSX_ARCHITECTURES MATCHES \"arm64\")\n    OR (NOT CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_SYSTEM_PROCESSOR MATCHES \"arm|aarch64|ARM64|ARMv[0-9]+\"))\n    set(GGML_CPU_ALL_VARIANTS ON)\nendif()\n\nif (CMAKE_OSX_ARCHITECTURES MATCHES \"x86_64\")\n    set(CMAKE_BUILD_RPATH \"@loader_path\")\n    set(CMAKE_INSTALL_RPATH \"@loader_path\")\nendif()\n\nset(OLLAMA_BUILD_DIR ${CMAKE_BINARY_DIR}/lib/ollama)\nset(OLLAMA_INSTALL_DIR ${CMAKE_INSTALL_PREFIX}/lib/ollama)\n\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY         ${OLLAMA_BUILD_DIR})\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY_DEBUG   ${OLLAMA_BUILD_DIR})\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY_RELEASE ${OLLAMA_BUILD_DIR})\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY         ${OLLAMA_BUILD_DIR})\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY_DEBUG   ${OLLAMA_BUILD_DIR})\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE ${OLLAMA_BUILD_DIR})\n\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src)\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src/include)\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src/ggml-cpu)\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src/ggml-cpu/amx)\n\nset(GGML_CPU ON)\nadd_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src)\nset_property(TARGET ggml PROPERTY EXCLUDE_FROM_ALL TRUE)\n\nget_target_property(CPU_VARIANTS ggml-cpu MANUALLY_ADDED_DEPENDENCIES)\nif(NOT CPU_VARIANTS)\n    set(CPU_VARIANTS \"ggml-cpu\")\nendif()\n\ninstall(TARGETS ggml-base ${CPU_VARIANTS}\n    RUNTIME_DEPENDENCIES\n        PRE_EXCLUDE_REGEXES \".*\"\n    RUNTIME DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT CPU\n    LIBRARY DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT CPU\n    FRAMEWORK DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT CPU\n)\n\ncheck_language(CUDA)\nif(CMAKE_CUDA_COMPILER)\n    if(CMAKE_VERSION VERSION_GREATER_EQUAL \"3.24\" AND NOT CMAKE_CUDA_ARCHITECTURES)\n        set(CMAKE_CUDA_ARCHITECTURES \"native\")\n    endif()\n\n    find_package(CUDAToolkit)\n    add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src/ggml-cuda)\n    set(OLLAMA_CUDA_INSTALL_DIR ${OLLAMA_INSTALL_DIR}/cuda_v${CUDAToolkit_VERSION_MAJOR})\n    install(TARGETS ggml-cuda\n        RUNTIME_DEPENDENCIES\n            DIRECTORIES ${CUDAToolkit_BIN_DIR} ${CUDAToolkit_LIBRARY_DIR}\n            PRE_INCLUDE_REGEXES cublas cublasLt cudart\n            PRE_EXCLUDE_REGEXES \".*\"\n        RUNTIME DESTINATION ${OLLAMA_CUDA_INSTALL_DIR} COMPONENT CUDA\n        LIBRARY DESTINATION ${OLLAMA_CUDA_INSTALL_DIR} COMPONENT CUDA\n    )\nendif()\n\nset(WINDOWS_AMDGPU_TARGETS_EXCLUDE_REGEX \"^gfx(906|908|90a|1200|1201):xnack[+-]$\"\n    CACHE STRING\n    \"Regular expression describing AMDGPU_TARGETS not supported on Windows. Override to force building these targets. Default \\\"^gfx(906|908|90a|1200|1201):xnack[+-]$\\\".\"\n)\n\ncheck_language(HIP)\nif(CMAKE_HIP_COMPILER)\n    set(HIP_PLATFORM \"amd\")\n\n    find_package(hip REQUIRED)\n    if(NOT AMDGPU_TARGETS)\n        list(FILTER AMDGPU_TARGETS INCLUDE REGEX \"^gfx(900|94[012]|101[02]|1030|110[012]|120[01])$\")\n    elseif(WIN32 AND WINDOWS_AMDGPU_TARGETS_EXCLUDE_REGEX)\n        list(FILTER AMDGPU_TARGETS EXCLUDE REGEX ${WINDOWS_AMDGPU_TARGETS_EXCLUDE_REGEX})\n    endif()\n\n    if(AMDGPU_TARGETS)\n        add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src/ggml-hip)\n\n        if (WIN32)\n            target_compile_definitions(ggml-hip PRIVATE GGML_CUDA_NO_PEER_COPY)\n        endif()\n\n        target_compile_definitions(ggml-hip PRIVATE GGML_HIP_NO_VMM)\n\n        set(OLLAMA_HIP_INSTALL_DIR ${OLLAMA_INSTALL_DIR}/rocm)\n        install(TARGETS ggml-hip\n            RUNTIME_DEPENDENCIES\n                DIRECTORIES ${HIP_BIN_INSTALL_DIR} ${HIP_LIB_INSTALL_DIR}\n                PRE_INCLUDE_REGEXES hipblas rocblas amdhip64 rocsolver amd_comgr hsa-runtime64 rocsparse tinfo rocprofiler-register drm drm_amdgpu numa elf\n                PRE_EXCLUDE_REGEXES \".*\"\n                POST_EXCLUDE_REGEXES \"system32\"\n            RUNTIME DESTINATION ${OLLAMA_HIP_INSTALL_DIR} COMPONENT HIP\n            LIBRARY DESTINATION ${OLLAMA_HIP_INSTALL_DIR} COMPONENT HIP\n        )\n\n        foreach(HIP_LIB_BIN_INSTALL_DIR IN ITEMS ${HIP_BIN_INSTALL_DIR} ${HIP_LIB_INSTALL_DIR})\n            if(EXISTS ${HIP_LIB_BIN_INSTALL_DIR}/rocblas)\n                install(DIRECTORY ${HIP_LIB_BIN_INSTALL_DIR}/rocblas DESTINATION ${OLLAMA_HIP_INSTALL_DIR} COMPONENT HIP)\n                break()\n            endif()\n        endforeach()\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Compiler Flags for GGML in CMake\nDESCRIPTION: Configures CUDA compiler flags, including optimization flags, warning levels, and compatibility options based on the CUDA Toolkit version and host compiler.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(CUDA_FLAGS -use_fast_math)\n\nif (CUDAToolkit_VERSION VERSION_GREATER_EQUAL \"12.8\")\n    list(APPEND CUDA_FLAGS -compress-mode=${GGML_CUDA_COMPRESSION_MODE})\nendif()\n\nif (GGML_FATAL_WARNINGS)\n    list(APPEND CUDA_FLAGS -Werror all-warnings)\nendif()\n\nif (GGML_ALL_WARNINGS AND NOT MSVC)\n    # ... (code to detect host compiler and set flags)\nendif()\n\nif (NOT MSVC)\n    list(APPEND CUDA_CXX_FLAGS -Wno-pedantic)\nendif()\n\nlist(JOIN   CUDA_CXX_FLAGS \" \" CUDA_CXX_FLAGS_JOINED)\n\nif (NOT CUDA_CXX_FLAGS_JOINED STREQUAL \"\")\n    list(APPEND CUDA_FLAGS -Xcompiler ${CUDA_CXX_FLAGS_JOINED})\nendif()\n\ntarget_compile_options(ggml-cuda PRIVATE \"$<$<COMPILE_LANGUAGE:CUDA>:${CUDA_FLAGS}>\")\n```\n\n----------------------------------------\n\nTITLE: KleidiAI Integration Configuration\nDESCRIPTION: Sets up integration with KleidiAI optimized kernels, including fetching source code, configuring paths, and setting up compilation flags. Handles different ARM instruction sets like dotprod, i8mm, and SME.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (GGML_CPU_KLEIDIAI)\n    message(STATUS \"Using KleidiAI optimized kernels if applicable\")\n    set(KLEIDIAI_BUILD_TESTS OFF)\n    include(FetchContent)\n    set(KLEIDIAI_COMMIT_TAG \"v1.5.0\")\n    set(KLEIDIAI_DOWNLOAD_URL \"https://github.com/ARM-software/kleidiai/archive/refs/tags/${KLEIDIAI_COMMIT_TAG}.tar.gz\")\n    set(KLEIDIAI_ARCHIVE_MD5 \"ea22e1aefb800e9bc8c74d91633cc58e\")\n    # ... additional KleidiAI configuration\nendif()\n```\n\n----------------------------------------\n\nTITLE: Compiler Warnings Configuration\nDESCRIPTION: Sets up compiler warning flags for different compilers (GCC/Clang and MSVC). Includes options for treating warnings as errors and enabling comprehensive warning checks.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (GGML_FATAL_WARNINGS)\n    if (CMAKE_CXX_COMPILER_ID MATCHES \"GNU\" OR CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n        list(APPEND C_FLAGS   -Werror)\n        list(APPEND CXX_FLAGS -Werror)\n    elseif (CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n        add_compile_options(/WX)\n    endif()\nendif()\n\nif (GGML_ALL_WARNINGS)\n    if (NOT MSVC)\n        list(APPEND WARNING_FLAGS -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function)\n        list(APPEND C_FLAGS       -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes\n                                  -Werror=implicit-int -Werror=implicit-function-declaration)\n        list(APPEND CXX_FLAGS     -Wmissing-declarations -Wmissing-noreturn)\n\n        list(APPEND C_FLAGS   ${WARNING_FLAGS})\n        list(APPEND CXX_FLAGS ${WARNING_FLAGS})\n\n        ggml_get_flags(${CMAKE_CXX_COMPILER_ID} ${CMAKE_CXX_COMPILER_VERSION})\n\n        add_compile_options(\"$<$<COMPILE_LANGUAGE:C>:${C_FLAGS};${GF_C_FLAGS}>\"\n                            \"$<$<COMPILE_LANGUAGE:CXX>:${CXX_FLAGS};${GF_CXX_FLAGS}>\")\n    else()\n        set(C_FLAGS   \"\")\n        set(CXX_FLAGS \"\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining GGML Base Library Target\nDESCRIPTION: Creates the ggml-base library target with all core source and header files for the GGML library, including quantization, allocator, backend, and threading components.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(ggml-base\n            ../include/ggml.h\n            ../include/ggml-alloc.h\n            ../include/ggml-backend.h\n            ../include/ggml-cpp.h\n            ../include/ggml-opt.h\n            ../include/gguf.h\n            ggml.c\n            ggml-alloc.c\n            ggml-backend.cpp\n            ggml-opt.cpp\n            ggml-threading.cpp\n            ggml-threading.h\n            ggml-quants.c\n            ggml-quants.h\n            gguf.cpp)\n```\n\n----------------------------------------\n\nTITLE: Removing Ollama Libraries from Linux\nDESCRIPTION: Command to remove installed Ollama libraries from the system. This is the final step in completely uninstalling Ollama.\nSOURCE: https://github.com/ollama/ollama/blob/main/docs/linux.md#2025-04-22_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\nsudo rm -rf /usr/local/lib/ollama\n```\n\n----------------------------------------\n\nTITLE: Empty Config Blob\nDESCRIPTION: Empty JSON object blob referenced by the config digest in the manifest\nSOURCE: https://github.com/ollama/ollama/blob/main/server/internal/registry/testdata/registry.txt#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{}\n```\n\n----------------------------------------\n\nTITLE: Linking CUDA Libraries for GGML in CMake\nDESCRIPTION: Links the appropriate CUDA libraries (static or shared) to the GGML-CUDA target based on build configuration and platform.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_STATIC)\n    if (WIN32)\n        # As of 12.3.1 CUDA Toolkit for Windows does not offer a static cublas library\n        target_link_libraries(ggml-cuda PRIVATE CUDA::cudart_static CUDA::cublas CUDA::cublasLt)\n    else ()\n        target_link_libraries(ggml-cuda PRIVATE  CUDA::cudart_static CUDA::cublas_static CUDA::cublasLt_static)\n    endif()\nelse()\n    target_link_libraries(ggml-cuda PRIVATE CUDA::cudart CUDA::cublas CUDA::cublasLt)\nendif()\n\nif (GGML_CUDA_NO_VMM)\n    # No VMM requested, no need to link directly with the cuda driver lib (libcuda.so)\nelse()\n    target_link_libraries(ggml-cuda PRIVATE CUDA::cuda_driver)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Sources for GGML Library in CMake\nDESCRIPTION: Collects CUDA source files and headers for the GGML library, including specific template instances based on compilation options.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB   GGML_HEADERS_CUDA \"*.cuh\")\nlist(APPEND GGML_HEADERS_CUDA \"../../include/ggml-cuda.h\")\n\nfile(GLOB   GGML_SOURCES_CUDA \"*.cu\")\nfile(GLOB   SRCS \"template-instances/fattn-mma*.cu\")\nlist(APPEND GGML_SOURCES_CUDA ${SRCS})\nfile(GLOB   SRCS \"template-instances/mmq*.cu\")\nlist(APPEND GGML_SOURCES_CUDA ${SRCS})\n\nif (GGML_CUDA_FA_ALL_QUANTS)\n    file(GLOB   SRCS \"template-instances/fattn-vec*.cu\")\n    list(APPEND GGML_SOURCES_CUDA ${SRCS})\n    add_compile_definitions(GGML_CUDA_FA_ALL_QUANTS)\nelse()\n    file(GLOB   SRCS \"template-instances/fattn-vec*q4_0-q4_0.cu\")\n    list(APPEND GGML_SOURCES_CUDA ${SRCS})\n    file(GLOB   SRCS \"template-instances/fattn-vec*q8_0-q8_0.cu\")\n    list(APPEND GGML_SOURCES_CUDA ${SRCS})\n    file(GLOB   SRCS \"template-instances/fattn-vec*f16-f16.cu\")\n    list(APPEND GGML_SOURCES_CUDA ${SRCS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Ollama CLI, App, and Installer on Windows\nDESCRIPTION: PowerShell command to execute the Windows build script that builds the Ollama CLI, Ollama app, and Ollama installer. The script must be run with ExecutionPolicy Bypass to ensure it executes without restrictions.\nSOURCE: https://github.com/ollama/ollama/blob/main/app/README.md#2025-04-22_snippet_0\n\nLANGUAGE: powershell\nCODE:\n```\npowershell -ExecutionPolicy Bypass -File .\\scripts\\build_windows.ps1\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Backend Variants in GGML\nDESCRIPTION: This function configures a CPU backend variant with specified sources, architecture flags, and definitions. It handles special cases like Kleidiai sources, dynamic loading support, and Emscripten builds. The function ensures correct compilation options and target properties are set for each backend variant.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot.c)\n            set(PRIVATE_ARCH_FLAGS \"${PRIVATE_ARCH_FLAGS}+sve+sve2\")\n        endif()\n\n        set_source_files_properties(${GGML_KLEIDIAI_SOURCES} PROPERTIES COMPILE_OPTIONS \"${PRIVATE_ARCH_FLAGS}\")\n        list(APPEND GGML_CPU_SOURCES ${GGML_KLEIDIAI_SOURCES})\n    endif()\n\n    message(STATUS \"Adding CPU backend variant ${GGML_CPU_NAME}: ${ARCH_FLAGS} ${ARCH_DEFINITIONS}\")\n    target_sources(${GGML_CPU_NAME} PRIVATE ${GGML_CPU_SOURCES})\n    target_compile_options(${GGML_CPU_NAME} PRIVATE ${ARCH_FLAGS})\n    target_compile_definitions(${GGML_CPU_NAME} PRIVATE ${ARCH_DEFINITIONS})\n\n    if (GGML_BACKEND_DL)\n        if (GGML_NATIVE)\n            # the feature check relies on ARCH_DEFINITIONS, but it is not set with GGML_NATIVE\n            message(FATAL_ERROR \"GGML_NATIVE is not compatible with GGML_BACKEND_DL, consider using GGML_CPU_ALL_VARIANTS\")\n        endif()\n\n        # The feature detection code is compiled as a separate target so that\n        # it can be built without the architecture flags\n        # Since multiple variants of the CPU backend may be included in the same\n        # build, using set_source_files_properties() to set the arch flags is not possible\n        set(GGML_CPU_FEATS_NAME ${GGML_CPU_NAME}-feats)\n        add_library(${GGML_CPU_FEATS_NAME} OBJECT ggml-cpu/cpu-feats-x86.cpp)\n        target_include_directories(${GGML_CPU_FEATS_NAME} PRIVATE . .. ../include)\n        target_compile_definitions(${GGML_CPU_FEATS_NAME} PRIVATE ${ARCH_DEFINITIONS})\n        target_compile_definitions(${GGML_CPU_FEATS_NAME} PRIVATE GGML_BACKEND_DL GGML_BACKEND_BUILD GGML_BACKEND_SHARED)\n        set_target_properties(${GGML_CPU_FEATS_NAME} PROPERTIES POSITION_INDEPENDENT_CODE ON)\n        target_link_libraries(${GGML_CPU_NAME} PRIVATE ${GGML_CPU_FEATS_NAME})\n    endif()\n\n    if (EMSCRIPTEN)\n        set_target_properties(${GGML_CPU_NAME} PROPERTIES COMPILE_FLAGS \"-msimd128\")\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Defining Function for Adding GGML Backend Libraries\nDESCRIPTION: Creates a function that configures backend libraries as either module (for dynamic loading) or static libraries (for direct linking). Sets up appropriate compile definitions and target properties based on the build type.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ggml_add_backend_library backend)\n    if (GGML_BACKEND_DL)\n        add_library(${backend} MODULE ${ARGN})\n        # write the shared library to the output directory\n        set_target_properties(${backend} PROPERTIES LIBRARY_OUTPUT_DIRECTORY ${CMAKE_RUNTIME_OUTPUT_DIRECTORY})\n        target_compile_definitions(${backend} PRIVATE GGML_BACKEND_DL)\n        add_dependencies(ggml ${backend})\n    else()\n        add_library(${backend} ${ARGN})\n        target_link_libraries(ggml PUBLIC ${backend})\n        install(TARGETS ${backend} LIBRARY)\n    endif()\n\n    target_link_libraries(${backend} PRIVATE ggml-base)\n    target_include_directories(${backend} PRIVATE ..)\n\n    if (${BUILD_SHARED_LIBS})\n        target_compile_definitions(${backend} PRIVATE GGML_BACKEND_BUILD)\n        target_compile_definitions(${backend} PUBLIC  GGML_BACKEND_SHARED)\n    endif()\n\n    if(NOT GGML_AVAILABLE_BACKENDS)\n        set(GGML_AVAILABLE_BACKENDS \"${backend}\"\n            CACHE INTERNAL \"List of backends for cmake package\")\n    else()\n        list(FIND GGML_AVAILABLE_BACKENDS \"${backend}\" has_backend)\n        if(has_backend EQUAL -1)\n            set(GGML_AVAILABLE_BACKENDS \"${GGML_AVAILABLE_BACKENDS};${backend}\"\n                CACHE INTERNAL \"List of backends for cmake package\")\n        endif()\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: HIP Compiler Detection and Configuration\nDESCRIPTION: Detects whether hipcc is being used as the C++ compiler and configures appropriate build settings. Includes special handling for Windows systems and warnings for Linux builds.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-hip/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (WIN32)\n    set(CXX_IS_HIPCC TRUE)\nelse()\n    string(REGEX MATCH \"hipcc(\\.bat)?$\" CXX_IS_HIPCC \"${CMAKE_CXX_COMPILER}\")\nendif()\n\nif (CXX_IS_HIPCC)\n    if (LINUX)\n        if (NOT ${CMAKE_CXX_COMPILER_ID} MATCHES \"Clang\")\n            message(WARNING \"Only LLVM is supported for HIP, hint: CXX=/opt/rocm/llvm/bin/clang++\")\n        endif()\n\n        message(WARNING \"Setting hipcc as the C++ compiler is legacy behavior.\"\n                \" Prefer setting the HIP compiler directly. See README for details.\")\n    endif()\nelse()\n    if (AMDGPU_TARGETS AND NOT CMAKE_HIP_ARCHITECTURES)\n        set(CMAKE_HIP_ARCHITECTURES ${AMDGPU_TARGETS})\n    endif()\n    cmake_minimum_required(VERSION 3.21)\n    enable_language(HIP)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Main GGML Library Target\nDESCRIPTION: Creates the main ggml library target with backend registration functionality, linking it to the ggml-base library and system-specific dependencies.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(ggml\n            ggml-backend-reg.cpp)\n\ntarget_link_libraries(ggml PUBLIC ggml-base)\n\nif (CMAKE_SYSTEM_NAME MATCHES \"Linux\")\n    target_link_libraries(ggml PRIVATE dl stdc++fs)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Basic CMake Setup and Sanitizer Configuration\nDESCRIPTION: Initial CMake setup including common definitions and sanitizer configurations for non-MSVC compilers. Configures thread, address and undefined behavior sanitizers.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(CheckCXXCompilerFlag)\ninclude(\"../cmake/common.cmake\")\n\nadd_compile_definitions(GGML_SCHED_MAX_COPIES=${GGML_SCHED_MAX_COPIES})\n\nif (CMAKE_SYSTEM_NAME MATCHES \"Linux\")\n    add_compile_definitions($<$<CONFIG:Debug>:_GLIBCXX_ASSERTIONS>)\nendif()\n\nif (NOT MSVC)\n    if (GGML_SANITIZE_THREAD)\n        add_compile_options(-fsanitize=thread)\n        link_libraries     (-fsanitize=thread)\n    endif()\n\n    if (GGML_SANITIZE_ADDRESS)\n        add_compile_options(-fsanitize=address -fno-omit-frame-pointer)\n        link_libraries     (-fsanitize=address)\n    endif()\n\n    if (GGML_SANITIZE_UNDEFINED)\n        add_compile_options(-fsanitize=undefined)\n        link_libraries     (-fsanitize=undefined)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Default Unit Tests Command\nDESCRIPTION: Command to run standard unit tests without integration tests\nSOURCE: https://github.com/ollama/ollama/blob/main/integration/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngo test ./...\n```\n\n----------------------------------------\n\nTITLE: Configuring BLAS for GGML in CMake\nDESCRIPTION: This snippet configures BLAS for the GGML library using CMake. It sets up static linking if required, finds the BLAS package, and configures vendor-specific settings. It also handles cases where BLAS include directories are not automatically detected.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-blas/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_STATIC)\n    set(BLA_STATIC ON)\nendif()\n#if (CMAKE_VERSION VERSION_GREATER_EQUAL 3.22)\n#    set(BLA_SIZEOF_INTEGER 8)\n#endif()\n\nset(BLA_VENDOR ${GGML_BLAS_VENDOR})\nfind_package(BLAS)\n\nif (BLAS_FOUND)\n    message(STATUS \"BLAS found, Libraries: ${BLAS_LIBRARIES}\")\n\n    ggml_add_backend_library(ggml-blas\n                             ggml-blas.cpp\n                            )\n\n    if (${GGML_BLAS_VENDOR} MATCHES \"Apple\")\n        add_compile_definitions(ACCELERATE_NEW_LAPACK)\n        add_compile_definitions(ACCELERATE_LAPACK_ILP64)\n        add_compile_definitions(GGML_BLAS_USE_ACCELERATE)\n    elseif (\"${BLAS_INCLUDE_DIRS}\" STREQUAL \"\")\n        # BLAS_INCLUDE_DIRS is missing in FindBLAS.cmake.\n        # see https://gitlab.kitware.com/cmake/cmake/-/issues/20268\n        find_package(PkgConfig REQUIRED)\n        if (${GGML_BLAS_VENDOR} MATCHES \"Generic\")\n            pkg_check_modules(DepBLAS blas)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"OpenBLAS\")\n            # As of openblas v0.3.22, the 64-bit is named openblas64.pc\n            pkg_check_modules(DepBLAS openblas64)\n            if (NOT DepBLAS_FOUND)\n                pkg_check_modules(DepBLAS openblas)\n            endif()\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"FLAME\")\n            add_compile_definitions(GGML_BLAS_USE_BLIS)\n            pkg_check_modules(DepBLAS blis)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"ATLAS\")\n            pkg_check_modules(DepBLAS blas-atlas)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"FlexiBLAS\")\n            pkg_check_modules(DepBLAS flexiblas_api)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"Intel\")\n            add_compile_definitions(GGML_BLAS_USE_MKL)\n            # all Intel* libraries share the same include path\n            pkg_check_modules(DepBLAS mkl-sdl)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"NVHPC\")\n            # this doesn't provide pkg-config\n            # suggest to assign BLAS_INCLUDE_DIRS on your own\n            if (\"${NVHPC_VERSION}\" STREQUAL \"\")\n                message(WARNING \"Better to set NVHPC_VERSION\")\n            else()\n                set(DepBLAS_FOUND ON)\n                set(DepBLAS_INCLUDE_DIRS \"/opt/nvidia/hpc_sdk/${CMAKE_SYSTEM_NAME}_${CMAKE_SYSTEM_PROCESSOR}/${NVHPC_VERSION}/math_libs/include\")\n            endif()\n        endif()\n        if (DepBLAS_FOUND)\n            set(BLAS_INCLUDE_DIRS ${DepBLAS_INCLUDE_DIRS})\n        else()\n            message(WARNING \"BLAS_INCLUDE_DIRS neither been provided nor been automatically\"\n            \" detected by pkgconfig, trying to find cblas.h from possible paths...\")\n            find_path(BLAS_INCLUDE_DIRS\n                NAMES cblas.h\n                HINTS\n                    /usr/include\n                    /usr/local/include\n                    /usr/include/openblas\n                    /opt/homebrew/opt/openblas/include\n                    /usr/local/opt/openblas/include\n                    /usr/include/x86_64-linux-gnu/openblas/include\n            )\n        endif()\n    endif()\n\n    message(STATUS \"BLAS found, Includes: ${BLAS_INCLUDE_DIRS}\")\n\n    target_compile_options(ggml-blas PRIVATE ${BLAS_LINKER_FLAGS})\n\n    if (${BLAS_INCLUDE_DIRS} MATCHES \"mkl\" AND (${GGML_BLAS_VENDOR} MATCHES \"Generic\" OR ${GGML_BLAS_VENDOR} MATCHES \"Intel\"))\n        add_compile_definitions(GGML_BLAS_USE_MKL)\n    endif()\n\n    target_link_libraries     (ggml-blas PRIVATE ${BLAS_LIBRARIES})\n    target_include_directories(ggml-blas PRIVATE ${BLAS_INCLUDE_DIRS})\nelse()\n    message(ERROR \"BLAS not found, please refer to \"\n                  \"https://cmake.org/cmake/help/latest/module/FindBLAS.html#blas-lapack-vendors\"\n                  \" to set correct GGML_BLAS_VENDOR\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding and Configuring GGML Metal Backend Library in CMake\nDESCRIPTION: Adds the ggml-metal backend library and links it with the required frameworks. Also sets compilation definitions based on configuration options.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nggml_add_backend_library(ggml-metal\n                         ggml-metal.m\n                        )\n\ntarget_link_libraries(ggml-metal PRIVATE\n                      ${FOUNDATION_LIBRARY}\n                      ${METAL_FRAMEWORK}\n                      ${METALKIT_FRAMEWORK}\n                      )\n\nif (GGML_METAL_NDEBUG)\n    add_compile_definitions(GGML_METAL_NDEBUG)\nendif()\n\nif (GGML_METAL_USE_BF16)\n    add_compile_definitions(GGML_METAL_USE_BF16)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Copying Metal Files to Binary Directory in CMake\nDESCRIPTION: Copies necessary Metal-related files to the binary output directory for runtime use.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nconfigure_file(../ggml-common.h  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-common.h     COPYONLY)\nconfigure_file(ggml-metal.metal  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal.metal  COPYONLY)\nconfigure_file(ggml-metal-impl.h ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal-impl.h COPYONLY)\n```\n\n----------------------------------------\n\nTITLE: Installing Metal Files for GGML in CMake\nDESCRIPTION: Configures the installation of Metal-related files when not embedding the library. This includes setting file permissions and specifying installation destinations.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT GGML_METAL_EMBED_LIBRARY)\n    install(\n        FILES src/ggml-metal/ggml-metal.metal\n        PERMISSIONS\n            OWNER_READ\n            OWNER_WRITE\n            GROUP_READ\n            WORLD_READ\n        DESTINATION ${CMAKE_INSTALL_BINDIR})\n\n        install(\n            FILES ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib\n            DESTINATION ${CMAKE_INSTALL_BINDIR}\n        )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Linking System Dependencies for GGML\nDESCRIPTION: Links required system libraries to GGML, including threading libraries, math libraries, and platform-specific libraries. Handles different requirements for each supported platform.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(ggml-base PRIVATE Threads::Threads)\n\nfind_library(MATH_LIBRARY m)\nif (MATH_LIBRARY)\n    if (NOT WIN32 OR NOT DEFINED ENV{ONEAPI_ROOT})\n        target_link_libraries(ggml-base PRIVATE m)\n    endif()\nendif()\n\nif (CMAKE_SYSTEM_NAME MATCHES \"Android\")\n    target_link_libraries(ggml-base PRIVATE dl)\nendif()\n\nif(CMAKE_SYSTEM_NAME MATCHES \"visionOS\")\n    target_compile_definitions(ggml-base PUBLIC _DARWIN_C_SOURCE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Shared Library Build Options for GGML\nDESCRIPTION: Sets specific build options when building GGML as shared libraries, including position-independent code and appropriate compile definitions for export/import symbols.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_SHARED_LIBS)\n    foreach (target ggml-base ggml)\n        set_target_properties(${target} PROPERTIES POSITION_INDEPENDENT_CODE ON)\n        target_compile_definitions(${target} PRIVATE GGML_BUILD)\n        target_compile_definitions(${target} PUBLIC  GGML_SHARED)\n    endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Backend Library for GGML in CMake\nDESCRIPTION: This function sets up the CPU backend library for GGML, including source files, compiler flags, and architecture-specific optimizations. It handles both ARM and x86 architectures, enabling features like SIMD, AVX, and AMX based on the target platform and available CPU instructions.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ggml_add_cpu_backend_variant_impl tag_name)\n    if (tag_name)\n        set(GGML_CPU_NAME ggml-cpu-${tag_name})\n    else()\n        set(GGML_CPU_NAME ggml-cpu)\n    endif()\n\n    ggml_add_backend_library(${GGML_CPU_NAME})\n\n    list (APPEND GGML_CPU_SOURCES\n        ggml-cpu/ggml-cpu.c\n        ggml-cpu/ggml-cpu.cpp\n        ggml-cpu/ggml-cpu-aarch64.cpp\n        ggml-cpu/ggml-cpu-aarch64.h\n        ggml-cpu/ggml-cpu-hbm.cpp\n        ggml-cpu/ggml-cpu-hbm.h\n        ggml-cpu/ggml-cpu-quants.c\n        ggml-cpu/ggml-cpu-quants.h\n        ggml-cpu/ggml-cpu-traits.cpp\n        ggml-cpu/ggml-cpu-traits.h\n        ggml-cpu/amx/amx.cpp\n        ggml-cpu/amx/amx.h\n        ggml-cpu/amx/mmq.cpp\n        ggml-cpu/amx/mmq.h\n        ggml-cpu/ggml-cpu-impl.h\n        ggml-cpu/common.h\n        ggml-cpu/binary-ops.h\n        ggml-cpu/binary-ops.cpp\n        ggml-cpu/unary-ops.h\n        ggml-cpu/unary-ops.cpp\n        ggml-cpu/simd-mappings.h\n        ggml-cpu/vec.h\n        ggml-cpu/vec.cpp\n        ggml-cpu/ops.h\n        ggml-cpu/ops.cpp\n        )\n\n    target_compile_features(${GGML_CPU_NAME} PRIVATE c_std_11 cxx_std_17)\n    target_include_directories(${GGML_CPU_NAME} PRIVATE . ggml-cpu)\n\n    if (APPLE AND GGML_ACCELERATE)\n        find_library(ACCELERATE_FRAMEWORK Accelerate)\n        if (ACCELERATE_FRAMEWORK)\n            message(STATUS \"Accelerate framework found\")\n\n            target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_ACCELERATE)\n            target_compile_definitions(${GGML_CPU_NAME} PRIVATE ACCELERATE_NEW_LAPACK)\n            target_compile_definitions(${GGML_CPU_NAME} PRIVATE ACCELERATE_LAPACK_ILP64)\n\n            target_link_libraries(${GGML_CPU_NAME} PRIVATE ${ACCELERATE_FRAMEWORK})\n        else()\n            message(WARNING \"Accelerate framework not found\")\n        endif()\n    endif()\n\n    if (GGML_OPENMP)\n        find_package(OpenMP)\n        if (OpenMP_FOUND)\n            target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_OPENMP)\n\n            target_link_libraries(${GGML_CPU_NAME} PRIVATE OpenMP::OpenMP_C OpenMP::OpenMP_CXX)\n        else()\n            message(WARNING \"OpenMP not found\")\n        endif()\n    endif()\n\n    if (GGML_LLAMAFILE)\n        target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_LLAMAFILE)\n\n        list(APPEND GGML_CPU_SOURCES\n                    ggml-cpu/llamafile/sgemm.cpp\n                    ggml-cpu/llamafile/sgemm.h)\n    endif()\n\n    if (GGML_CPU_HBM)\n        find_library(memkind memkind REQUIRED)\n\n        message(STATUS \"Using memkind for CPU HBM\")\n\n        target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_CPU_HBM)\n\n        target_link_libraries(${GGML_CPU_NAME} PUBLIC memkind)\n    endif()\n\n    if (CMAKE_OSX_ARCHITECTURES      STREQUAL \"arm64\" OR\n        CMAKE_GENERATOR_PLATFORM_LWR STREQUAL \"arm64\" OR\n        (NOT CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_GENERATOR_PLATFORM_LWR AND\n            CMAKE_SYSTEM_PROCESSOR MATCHES \"^(aarch64|arm.*|ARM64)$\"))\n\n        message(STATUS \"ARM detected\")\n\n        if (MSVC AND NOT CMAKE_C_COMPILER_ID STREQUAL \"Clang\")\n            message(FATAL_ERROR \"MSVC is not supported for ARM, use clang\")\n        else()\n            check_cxx_compiler_flag(-mfp16-format=ieee GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E)\n            if (NOT \"${GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E}\" STREQUAL \"\")\n                list(APPEND ARCH_FLAGS -mfp16-format=ieee)\n            endif()\n\n            if (GGML_NATIVE)\n                # -mcpu=native does not always enable all the features in some compilers,\n                # so we check for them manually and enable them if available\n\n                execute_process(\n                    COMMAND ${CMAKE_C_COMPILER} -mcpu=native -E -v -\n                    INPUT_FILE \"/dev/null\"\n                    OUTPUT_QUIET\n                    ERROR_VARIABLE ARM_MCPU\n                    RESULT_VARIABLE ARM_MCPU_RESULT\n                )\n                if (NOT ARM_MCPU_RESULT)\n                    string(REGEX MATCH \"-mcpu=[^ ']+\" ARM_MCPU_FLAG \"${ARM_MCPU}\")\n                endif()\n                if (\"${ARM_MCPU_FLAG}\" STREQUAL \"\")\n                    set(ARM_MCPU_FLAG -mcpu=native)\n                    message(STATUS \"ARM -mcpu not found, -mcpu=native will be used\")\n                endif()\n\n                include(CheckCXXSourceRuns)\n\n                function(check_arm_feature tag code)\n                    set(CMAKE_REQUIRED_FLAGS_SAVE ${CMAKE_REQUIRED_FLAGS})\n                    set(CMAKE_REQUIRED_FLAGS \"${ARM_MCPU_FLAG}+${tag}\")\n                    check_cxx_source_runs(\"${code}\" GGML_MACHINE_SUPPORTS_${tag})\n                    if (GGML_MACHINE_SUPPORTS_${tag})\n                        set(ARM_MCPU_FLAG_FIX \"${ARM_MCPU_FLAG_FIX}+${tag}\" PARENT_SCOPE)\n                    else()\n                        set(CMAKE_REQUIRED_FLAGS \"${ARM_MCPU_FLAG}+no${tag}\")\n                        check_cxx_source_compiles(\"int main() { return 0; }\" GGML_MACHINE_SUPPORTS_no${tag})\n                        if (GGML_MACHINE_SUPPORTS_no${tag})\n                            set(ARM_MCPU_FLAG_FIX \"${ARM_MCPU_FLAG_FIX}+no${tag}\" PARENT_SCOPE)\n                        endif()\n                    endif()\n                    set(CMAKE_REQUIRED_FLAGS ${CMAKE_REQUIRED_FLAGS_SAVE})\n                endfunction()\n\n                check_arm_feature(dotprod \"#include <arm_neon.h>\\nint main() { int8x16_t _a, _b; volatile int32x4_t _s = vdotq_s32(_s, _a, _b); return 0; }\")\n                check_arm_feature(i8mm    \"#include <arm_neon.h>\\nint main() { int8x16_t _a, _b; volatile int32x4_t _s = vmmlaq_s32(_s, _a, _b); return 0; }\")\n                check_arm_feature(sve     \"#include <arm_sve.h>\\nint main()  { svfloat32_t _a, _b; volatile svfloat32_t _c = svadd_f32_z(svptrue_b8(), _a, _b); return 0; }\")\n                check_arm_feature(sme     \"#include <arm_sme.h>\\n__arm_locally_streaming int main() { __asm__ volatile(\\\"smstart; smstop;\\\"); return 0; }\")\n\n                list(APPEND ARCH_FLAGS \"${ARM_MCPU_FLAG}${ARM_MCPU_FLAG_FIX}\")\n            else()\n                if (GGML_CPU_ARM_ARCH)\n                    list(APPEND ARCH_FLAGS -march=${GGML_CPU_ARM_ARCH})\n                endif()\n            endif()\n\n            # show enabled features\n            if (CMAKE_HOST_SYSTEM_NAME STREQUAL \"Windows\")\n                set(FEAT_INPUT_FILE \"NUL\")\n            else()\n                set(FEAT_INPUT_FILE \"/dev/null\")\n            endif()\n\n            execute_process(\n                COMMAND ${CMAKE_C_COMPILER} ${ARCH_FLAGS} -dM -E -\n                INPUT_FILE ${FEAT_INPUT_FILE}\n                OUTPUT_VARIABLE ARM_FEATURE\n                RESULT_VARIABLE ARM_FEATURE_RESULT\n            )\n            if (ARM_FEATURE_RESULT)\n                message(WARNING \"Failed to get ARM features\")\n            else()\n                foreach(feature DOTPROD SVE MATMUL_INT8 FMA FP16_VECTOR_ARITHMETIC SME)\n                    string(FIND \"${ARM_FEATURE}\" \"__ARM_FEATURE_${feature} 1\" feature_pos)\n                    if (NOT ${feature_pos} EQUAL -1)\n                        message(STATUS \"ARM feature ${feature} enabled\")\n                    endif()\n                endforeach()\n            endif()\n        endif()\n    elseif (CMAKE_OSX_ARCHITECTURES STREQUAL \"x86_64\" OR CMAKE_GENERATOR_PLATFORM_LWR MATCHES \"^(x86_64|i686|amd64|x64|win32)$\" OR\n            (NOT CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_GENERATOR_PLATFORM_LWR AND\n            CMAKE_SYSTEM_PROCESSOR MATCHES \"^(x86_64|i686|AMD64|amd64)$\"))\n\n        message(STATUS \"x86 detected\")\n\n        if (MSVC)\n            # instruction set detection for MSVC only\n            if (GGML_NATIVE)\n                include(ggml-cpu/cmake/FindSIMD.cmake)\n            endif ()\n            if (GGML_AVX512)\n                list(APPEND ARCH_FLAGS /arch:AVX512)\n                # /arch:AVX512 includes: __AVX512F__, __AVX512CD__, __AVX512BW__, __AVX512DQ__, and __AVX512VL__\n                # MSVC has no compile-time flags enabling specific\n                # AVX512 extensions, neither it defines the\n                # macros corresponding to the extensions.\n                # Do it manually.\n                list(APPEND ARCH_DEFINITIONS GGML_AVX512)\n                if (GGML_AVX512_VBMI)\n                    list(APPEND ARCH_DEFINITIONS __AVX512VBMI__)\n                    if (CMAKE_C_COMPILER_ID STREQUAL \"Clang\")\n                        list(APPEND ARCH_FLAGS -mavx512vbmi)\n                    endif()\n                endif()\n                if (GGML_AVX512_VNNI)\n                    list(APPEND ARCH_DEFINITIONS __AVX512VNNI__ GGML_AVX512_VNNI)\n                    if (CMAKE_C_COMPILER_ID STREQUAL \"Clang\")\n                        list(APPEND ARCH_FLAGS -mavx512vnni)\n                    endif()\n                endif()\n                if (GGML_AVX512_BF16)\n                    list(APPEND ARCH_DEFINITIONS __AVX512BF16__ GGML_AVX512_BF16)\n                    if (CMAKE_C_COMPILER_ID STREQUAL \"Clang\")\n                        list(APPEND ARCH_FLAGS -mavx512bf16)\n                    endif()\n                endif()\n                if (GGML_AMX_TILE)\n                    list(APPEND ARCH_DEFINITIONS __AMX_TILE__ GGML_AMX_TILE)\n                endif()\n                if (GGML_AMX_INT8)\n                    list(APPEND ARCH_DEFINITIONS __AMX_INT8__ GGML_AMX_INT8)\n                endif()\n                if (GGML_AMX_BF16)\n                    list(APPEND ARCH_DEFINITIONS __AMX_BF16__ GGML_AMX_BF16)\n                endif()\n            elseif (GGML_AVX2)\n                list(APPEND ARCH_FLAGS /arch:AVX2)\n                list(APPEND ARCH_DEFINITIONS GGML_AVX2 GGML_FMA GGML_F16C)\n            elseif (GGML_AVX)\n                list(APPEND ARCH_FLAGS /arch:AVX)\n                list(APPEND ARCH_DEFINITIONS GGML_AVX)\n            else ()\n                list(APPEND ARCH_FLAGS /arch:SSE4.2)\n                list(APPEND ARCH_DEFINITIONS GGML_SSE42)\n            endif()\n            if (GGML_AVX_VNNI)\n                list(APPEND ARCH_DEFINITIONS __AVXVNNI__ GGML_AVX_VNNI)\n            endif()\n            if (GGML_BMI2)\n```\n\n----------------------------------------\n\nTITLE: CPU Architecture Detection and Flag Configuration\nDESCRIPTION: Configures compiler flags and definitions based on CPU architecture and available instruction sets. Handles multiple architectures including x86 (AVX, SSE), PowerPC, RISC-V, s390x, and ARM. Sets appropriate compiler flags for optimization and CPU-specific features.\nSOURCE: https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (GGML_NATIVE)\n    list(APPEND ARCH_FLAGS -march=native)\nelse ()\n    list(APPEND ARCH_FLAGS -msse4.2)\n    list(APPEND ARCH_DEFINITIONS GGML_SSE42)\n    if (GGML_F16C)\n        list(APPEND ARCH_FLAGS -mf16c)\n        list(APPEND ARCH_DEFINITIONS GGML_F16C)\n    endif()\n    # ... additional architecture flags\nendif()\n```\n\n----------------------------------------\n\nTITLE: Making Embedding API Request\nDESCRIPTION: Example cURL command to request text embeddings from the running model via HTTP POST to the /embedding endpoint.\nSOURCE: https://github.com/ollama/ollama/blob/main/runner/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST -H \"Content-Type: application/json\" -d '{\"prompt\": \"turn me into an embedding\"}' http://localhost:8080/embedding\n```\n\n----------------------------------------\n\nTITLE: Making Completion API Request\nDESCRIPTION: Example cURL command to request a completion from the running model via HTTP POST to the /completion endpoint.\nSOURCE: https://github.com/ollama/ollama/blob/main/runner/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST -H \"Content-Type: application/json\" -d '{\"prompt\": \"hi\"}' http://localhost:8080/completion\n```"
  }
]
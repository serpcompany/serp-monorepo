[
  {
    "owner": "run-llama",
    "repo": "llamaindexts.git",
    "content": "TITLE: Full Example: MistralAI and LlamaIndex Integration (TypeScript)\nDESCRIPTION: This comprehensive example showcases the entire process of integrating MistralAI with LlamaIndex, from initializing the LLM and loading documents to indexing and querying. It includes setting up the MistralAI LLM, creating a Document, building a VectorStoreIndex, initializing a retriever, building a query engine, and querying the engine.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/mistral.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MistralAI } from \"@llamaindex/mistral\";\nimport { Document, VectorStoreIndex, Settings } from \"llamaindex\";\n\n// Use the MistralAI LLM\nSettings.llm = new MistralAI({ model: \"mistral-tiny\" });\n\nasync function main() {\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Full Example of Gemini Integration\nDESCRIPTION: This code provides a complete example of using Gemini with LlamaIndex. It initializes the Gemini model, loads and indexes a document, creates a query engine, and executes a query.  It demonstrates the complete workflow from setup to query.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/gemini.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Gemini, GEMINI_MODEL } from \"@llamaindex/google\";\nimport { Document, VectorStoreIndex, Settings } from \"llamaindex\";\n\nSettings.llm = new Gemini({\n  model: GEMINI_MODEL.GEMINI_PRO,\n});\n\nasync function main() {\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Metadata Extraction with IngestionPipeline in LlamaIndex\nDESCRIPTION: This code demonstrates how to use LlamaIndex's `IngestionPipeline` with `TitleExtractor` and `QuestionsAnsweredExtractor` to extract metadata from a document. It initializes the pipeline, adds the transformations, runs the pipeline on a sample document, and then prints the extracted metadata from each node.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/metadata_extraction.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, IngestionPipeline, TitleExtractor, QuestionsAnsweredExtractor } from \"llamaindex\";\nimport { OpenAI } from \"@llamaindex/openai\";\n\nasync function main() {\n  const pipeline = new IngestionPipeline({\n    transformations: [\n      new TitleExtractor(),\n      new QuestionsAnsweredExtractor({\n        questions: 5,\n      }),\n    ],\n  });\n\n  const nodes = await pipeline.run({\n    documents: [\n      new Document({ text: \"I am 10 years old. John is 20 years old.\" }),\n    ],\n  });\n\n  for (const node of nodes) {\n    console.log(node.metadata);\n  }\n}\n\nmain().then(() => console.log(\"done\"));\n```\n\n----------------------------------------\n\nTITLE: Full Metadata Filtering Example with LlamaIndex\nDESCRIPTION: This snippet provides a complete example of using metadata filtering with LlamaIndex and ChromaDB. It includes document creation, index initialization, and querying with metadata filters within an asynchronous `main` function. Error handling is included to catch and log any exceptions.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/metadata_filtering.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex, storageContextFromDefaults } from \"llamaindex\";\nimport { ChromaVectorStore } from \"@llamaindex/chroma\";\n\nconst collectionName = \"dog_colors\";\n\nasync function main() {\n  try {\n    const docs = [\n      new Document({\n        text: \"The dog is brown\",\n        metadata: {\n          color: \"brown\",\n          dogId: \"1\",\n        },\n      }),\n      new Document({\n        text: \"The dog is red\",\n        metadata: {\n          color: \"red\",\n          dogId: \"2\",\n        },\n      }),\n    ];\n\n    console.log(\"Creating ChromaDB vector store\");\n    const chromaVS = new ChromaVectorStore({ collectionName });\n    const ctx = await storageContextFromDefaults({ vectorStore: chromaVS });\n\n    console.log(\"Embedding documents and adding to index\");\n    const index = await VectorStoreIndex.fromDocuments(docs, {\n      storageContext: ctx,\n    });\n\n    console.log(\"Querying index\");\n    const queryEngine = index.asQueryEngine({\n      preFilters: {\n        filters: [\n          {\n            key: \"dogId\",\n            value: \"2\",\n            operator: \"==\",\n          },\n        ],\n      },\n    });\n    const response = await queryEngine.query({\n      query: \"What is the color of the dog?\",\n    });\n    console.log(response.toString());\n  } catch (e) {\n    console.error(e);\n  }\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Using Ollama Embedding with LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to use the `OllamaEmbedding` class to set up an embedding model in LlamaIndex. It initializes the embedding model, creates a document, builds a VectorStoreIndex, creates a query engine, and performs a query.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/ollama.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OllamaEmbedding } from \"@llamaindex/ollama\";\nimport { Document, Settings, VectorStoreIndex } from \"llamaindex\";\n\nSettings.embedModel = new OllamaEmbedding({ model: \"nomic-embed-text\" });\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Ollama LLM in LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to set the Ollama LLM as both the default LLM and the embedding model within LlamaIndex. This ensures that all text generation and embedding operations use the configured Ollama model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/ollama.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Ollama } from \"@llamaindex/ollama\";\nimport { Settings } from \"llamaindex\";\n\nSettings.llm = ollamaLLM;\nSettings.embedModel = ollamaLLM;\n```\n\n----------------------------------------\n\nTITLE: OpenAI with JSON Output, System/User Messages (TypeScript)\nDESCRIPTION: Configuring the OpenAI model for JSON output and using System and User messages to guide the output format. The system message instructs the model to act as a helpful assistant that outputs JSON.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nSettings.llm = new OpenAI({ \n  model: \"gpt-4o\", \n  temperature: 0,\n  responseFormat: { type: \"json_object\" }  \n});\n\nconst response = await llm.chat({\n  messages: [\n    {\n      role: \"system\",\n      content: \"You are a helpful assistant that outputs JSON.\"\n    },\n    {\n      role: \"user\", \n      content: \"Summarize this meeting transcript\"\n    }\n  ]\n});\n\n// Response will be valid JSON\nconsole.log(response.message.content);\n```\n\n----------------------------------------\n\nTITLE: Complete LlamaIndex Supabase Vector Store Example\nDESCRIPTION: This TypeScript code provides a complete example of using LlamaIndex with Supabase as a vector store. It initializes the vector store, creates sample documents, sets up the storage context, creates and stores embeddings, queries the index, and outputs the response. It depends on `llamaindex`, `@llamaindex/supabase`, and the environment variables `SUPABASE_URL` and `SUPABASE_KEY`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/supabase.mdx#_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { Document, VectorStoreIndex, storageContextFromDefaults } from \"llamaindex\";\nimport { SupabaseVectorStore } from \"@llamaindex/supabase\";\n\nasync function main() {\n  // Initialize the vector store\n  const vectorStore = new SupabaseVectorStore({\n    supabaseUrl: process.env.SUPABASE_URL,\n    supabaseKey: process.env.SUPABASE_KEY,\n    table: \"documents\",\n  });\n\n  // Create sample documents\n  const documents = [\n    new Document({\n      text: \"Vector search enables semantic similarity search\",\n      metadata: {\n        source: \"research_paper\",\n        author: \"Jane Smith\",\n      },\n    }),\n  ];\n\n  // Create storage context\n  const storageContext = await storageContextFromDefaults({ vectorStore });\n\n  // Create and store embeddings\n  const index = await VectorStoreIndex.fromDocuments(documents, {\n    storageContext,\n  });\n\n  // Query the index\n  const queryEngine = index.asQueryEngine();\n  const response = await queryEngine.query({\n    query: \"What is vector search?\",\n  });\n\n  // Output response\n  console.log(response.toString());\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Configure PostgreSQL as Index Store\nDESCRIPTION: This TypeScript snippet configures a PostgresIndexStore within LlamaIndex. It creates a storage context using the PostgresIndexStore, which will use a PostgreSQL database to store index metadata. It utilizes the storageContextFromDefaults function to initialize the storage context with the specified index store.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/index_stores/index.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex, storageContextFromDefaults } from \"llamaindex\";\nimport { PostgresIndexStore } from \"@llamaindex/postgres\";\n\nconst storageContext = await storageContextFromDefaults({\n  indexStore: new PostgresIndexStore(),\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Portkey LLM with LlamaIndex (TS)\nDESCRIPTION: This snippet initializes the Portkey LLM for use with LlamaIndex. It sets the global LLM to a new Portkey instance, passing your Portkey API key.  The `Settings.llm` assignment makes Portkey the default LLM for LlamaIndex operations.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/portkey.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Portkey } from \"@llamaindex/portkey-ai\";\nimport { Settings } from \"llamaindex\";\n\nSettings.llm = new Portkey({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n```\n\n----------------------------------------\n\nTITLE: Single Agent Workflow with Joke Tool - Typescript\nDESCRIPTION: This code snippet demonstrates how to create a single agent workflow with a specific tool for telling jokes. It imports necessary modules, defines a joke-telling tool, creates an agent with the tool, and runs the workflow to get a joke. It depends on the `llamaindex`, `@llamaindex/openai` libraries.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/agent_workflow.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { agent, tool } from \"llamaindex\";\nimport { openai } from \"@llamaindex/openai\";\n\n// Define a joke-telling tool\nconst jokeTool = tool(\n  () => \"Baby Llama is called cria\",\n  {\n    name: \"joke\",\n    description: \"Use this tool to get a joke\",\n  }\n);\n\n// Create an single agent workflow with the tool\nconst jokeAgent = agent({\n  tools: [jokeTool],\n  llm: openai({ model: \"gpt-4o-mini\" }),\n});\n\n// Run the workflow\nconst result = await jokeAgent.run(\"Tell me something funny\");\nconsole.log(result); // Baby Llama is called cria\n```\n\n----------------------------------------\n\nTITLE: Using Transformations Directly in LlamaIndex (TypeScript)\nDESCRIPTION: This snippet demonstrates how to use transformations like SentenceSplitter and TitleExtractor directly in LlamaIndex. It imports necessary modules, creates a SentenceSplitter instance to split a document into nodes, then uses a TitleExtractor to transform those nodes. The transformed node content is then printed to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/index.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SentenceSplitter, TitleExtractor, Document, MetadataMode } from \"llamaindex\";\n\nasync function main() {\n  let nodes = new SentenceSplitter().getNodesFromDocuments([\n    new Document({ text: \"I am 10 years old. John is 20 years old.\" }),\n  ]);\n\n  const titleExtractor = new TitleExtractor();\n\n  nodes = await titleExtractor.transform(nodes);\n\n  for (const node of nodes) {\n    console.log(node.getContent(MetadataMode.NONE));\n  }\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Configure Remote Ollama Embedding\nDESCRIPTION: Configures LlamaIndex to use a remote Ollama embedding model, specifically `nomic-embed-text`.  It specifies the host URL for the Ollama instance, enabling the use of local or custom embedding models without relying on external API services. The host in the Ollama server may need to be configured to `0.0.0.0`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/index.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OllamaEmbedding } from \"@llamaindex/ollama\";\nimport { Settings } from \"llamaindex\";\n\n// Configure Ollama with a remote host\nSettings.embedModel = new OllamaEmbedding({\n  model: \"nomic-embed-text\",\n  config: {\n    host: \"http://your-ollama-host:11434\"\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Tool for the Agent - TypeScript\nDESCRIPTION: This snippet shows how to create a tool that an LLM agent can utilize, using LlamaIndex. It defines the tool's name, description, parameters (using Zod for validation), and the function to execute.  The `z.object` is used to define the input schema.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/2_create_agent.mdx#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst addTool = tool({\n  name: \"sumNumbers\",\n  description: \"Use this function to sum two numbers\",\n  parameters: z.object({\n    a: z.number({\n      description: \"First number to sum\",\n    }),\n    b: z.number({\n      description: \"Second number to sum\",\n    }),\n  }),\n  execute: sumNumbers,\n});\n```\n\n----------------------------------------\n\nTITLE: Querying LlamaIndex with Bedrock LLM\nDESCRIPTION: This example demonstrates a full workflow of using LlamaIndex with the Bedrock LLM, including loading documents, creating an index, and querying the index. It showcases how to integrate the Bedrock LLM into a standard LlamaIndex pipeline for question answering.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/bedrock.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BEDROCK_MODELS, Bedrock } from \"llamaindex\";\n\nSettings.llm = new Bedrock({\n  model: BEDROCK_MODELS.ANTHROPIC_CLAUDE_3_HAIKU,\n});\n\nasync function main() {\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n\n----------------------------------------\n\nTITLE: CodeSplitter with TextFileReader - TypeScript\nDESCRIPTION: This code snippet demonstrates using the CodeSplitter with a TextFileReader to read and parse code files. It sets up a tree-sitter parser for TypeScript and then initializes a CodeSplitter with that parser. The TextFileReader is used to load the file, and finally, the CodeSplitter parses the document.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/node-parser.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TextFileReader } from '@llamaindex/readers/text'\nimport { CodeSplitter } from '@llamaindex/node-parser/code'\nimport Parser from \"tree-sitter\";\nimport TS from \"tree-sitter-typescript\";\n\nconst parser = new Parser();\nparser.setLanguage(TS.typescript as Parser.Language);\nconst codeSplitter = new CodeSplitter({\n\t\tgetParser: () => parser,\n\t});\nconst reader = new TextFileReader();\nconst documents = await reader.loadData('path/to/file.ts');\n\n\tconst parsedDocuments = codeSplitter(documents);\n//\t\t\t  ^?\n```\n\n----------------------------------------\n\nTITLE: Creating a RouterQueryEngine in TypeScript\nDESCRIPTION: This code creates a `RouterQueryEngine` that routes queries to either a `vectorQueryEngine` or a `summaryQueryEngine` based on the query. The descriptions are used to guide the router to select the appropriate engine. The router is configured using `RouterQueryEngine.fromDefaults` which provides default router settings.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/router_query_engine.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = RouterQueryEngine.fromDefaults({\n  queryEngineTools: [\n    {\n      queryEngine: vectorQueryEngine,\n      description: \"Useful for summarization questions related to Abramov\",\n    },\n    {\n      queryEngine: summaryQueryEngine,\n      description: \"Useful for retrieving specific context from Abramov\",\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Querying the Index with Anthropic LLM\nDESCRIPTION: This code demonstrates how to query the created index using a query engine. It creates a query engine from the index and then performs a query. The result will be based on the information in the indexed document and the capabilities of the Anthropic LLM.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/anthropic.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Querying LlamaIndex Vector Store Index\nDESCRIPTION: This TypeScript code shows how to query the `VectorStoreIndex` to retrieve information. It first creates a `QueryEngine` from the index using `index.asQueryEngine()`. It then sends a query using `queryEngine.query()` and prints the response to the console. The code assumes the `index` object has already been created and populated with data.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/supabase.mdx#_snippet_6\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst response = await queryEngine.query({\n  query: \"What is in the document?\",\n});\n\n// Output response\nconsole.log(response.toString());\n```\n\n----------------------------------------\n\nTITLE: Full Ollama Integration Example with LlamaIndex\nDESCRIPTION: This complete example demonstrates the entire process of integrating Ollama with LlamaIndex. It includes reading a document from a file, indexing it, creating a query engine, and querying the index to retrieve a response. It uses the `fs/promises` module to read the file asynchronously and sets Ollama as both the LLM and embedding model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/ollama.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Ollama } from \"@llamaindex/ollama\";\nimport { Document, VectorStoreIndex, Settings } from \"llamaindex\";\n\nimport fs from \"fs/promises\";\n\nconst ollama = new Ollama({ model: \"llama2\", temperature: 0.75 });\n\n// Use Ollama LLM and Embed Model\nSettings.llm = ollama;\nSettings.embedModel = ollama;\n\nasync function main() {\n  const essay = await fs.readFile(\"./paul_graham_essay.txt\", \"utf-8\");\n\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Vector Index Example in TypeScript\nDESCRIPTION: This example demonstrates how to load, index, and query data using LlamaIndex.TS. It converts a file into a Document object, indexes it to create embeddings, and then creates a query engine to answer questions based on the indexed data. The code makes use of OpenAI for embeddings.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/rag/index.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n<include cwd>../../examples/vectorIndex.ts</include>\n```\n\n----------------------------------------\n\nTITLE: Streaming ChatEngine Responses in Typescript\nDESCRIPTION: This code demonstrates how to use the `chat` function with streaming enabled. By passing `stream: true` as an option, the function returns an asynchronous iterable. The code then iterates over the stream, processing each chunk of the response.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/chat_engine.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst chatEngine = index.asChatEngine();\nconst stream = await chatEngine.chat({ message: query, stream: true });\nfor await (const chunk of stream) {\n  process.stdout.write(chunk.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Running a Basic Ingestion Pipeline in LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to create and run a basic ingestion pipeline in LlamaIndex using TypeScript. It reads text from a file, creates a Document object, defines transformations such as sentence splitting, title extraction, and OpenAI embeddings, and then runs the pipeline on the document. The resulting nodes are then printed to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/index.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport fs from \"node:fs/promises\";\nimport { OpenAI, OpenAIEmbedding } from \"@llamaindex/openai\";\nimport {\n  Document,\n  IngestionPipeline,\n  MetadataMode,\n  TitleExtractor,\n  SentenceSplitter,\n} from \"llamaindex\";\n\nasync function main() {\n  // Load essay from abramov.txt in Node\n  const path = \"node_modules/llamaindex/examples/abramov.txt\";\n\n  const essay = await fs.readFile(path, \"utf-8\");\n\n  // Create Document object with essay\n  const document = new Document({ text: essay, id_: path });\n  const pipeline = new IngestionPipeline({\n    transformations: [\n      new SentenceSplitter({ chunkSize: 1024, chunkOverlap: 20 }),\n      new TitleExtractor(),\n      new OpenAIEmbedding(),\n    ],\n  });\n\n  // run the pipeline\n  const nodes = await pipeline.run({ documents: [document] });\n\n  // print out the result of the pipeline run\n  for (const node of nodes) {\n    console.log(node.getContent(MetadataMode.NONE));\n  }\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Full Example: Portkey LLM with LlamaIndex (TS)\nDESCRIPTION: This comprehensive example showcases the end-to-end process of using Portkey LLM with LlamaIndex. It includes initializing the Portkey LLM, loading and indexing a document, creating a query engine, executing a query, and logging the response. This provides a complete workflow for leveraging LlamaIndex with Portkey.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/portkey.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Portkey } from \"@llamaindex/portkey-ai\";\nimport { Document, Settings, VectorStoreIndex } from \"llamaindex\";\n\n// Use the Portkey LLM\nSettings.llm = new Portkey({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n\nasync function main() {\n  // Create a document\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Querying VectorStoreIndex with Vercel AI SDK in TypeScript\nDESCRIPTION: This code showcases how to create a vector store index from documents and query it using Vercel's AI SDK. It imports necessary modules from `@ai-sdk/openai`, `@llamaindex/vercel`, `ai`, and `llamaindex`. It creates a document, builds a `VectorStoreIndex`, defines a query tool with the `llamaindex` function, and uses `streamText` from the `ai` package to perform the query and stream the response.  The response is then logged to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/integration/vercel.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from \"@ai-sdk/openai\";\nimport { llamaindex } from \"@llamaindex/vercel\";\nimport { streamText } from \"ai\";\nimport { Document, VectorStoreIndex } from \"llamaindex\";\n\n// Create an index from your documents\nconst document = new Document({ text: yourText, id_: \"unique-id\" });\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\n// Create a query tool\nconst queryTool = llamaindex({\n  model: openai(\"gpt-4\"),\n  index,\n  description: \"Search through the documents\", // optional\n});\n\n// Use the tool with Vercel's AI SDK\nstreamText({\n  model: openai(\"gpt-4\"),\n  prompt: \"Your question here\",\n  tools: { queryTool },\n  onFinish({ response }) {\n    console.log(\"Response:\", response.messages); // log the response\n  },\n}).toDataStream();\n```\n\n----------------------------------------\n\nTITLE: Define a Custom Prompt Template (TextQaPrompt) - Typescript\nDESCRIPTION: This code defines a custom prompt template named `newTextQaPrompt` for question answering. It takes `context` and `query` as inputs and constructs a prompt that instructs the LLM to answer the query based on the given context in the style of a Sherlock Holmes detective novel. This prompt can then be used to override the default text QA prompt in LlamaIndex.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/prompt/index.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Define a custom prompt\nconst newTextQaPrompt: TextQaPrompt = ({ context, query }) => {\n  return `Context information is below.\n---------------------\n${context}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nAnswer the query in the style of a Sherlock Holmes detective novel.\nQuery: ${query}\nAnswer:`;\n};\n```\n\n----------------------------------------\n\nTITLE: Using Jina AI Embeddings with LlamaIndex in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use Jina AI embeddings in a LlamaIndex application. It initializes the JinaAIEmbedding model, sets it as the embedModel in LlamaIndex settings, creates a document from text, builds a VectorStoreIndex from the document, creates a query engine, and then performs a query. The snippet requires the @llamaindex/jinaai and llamaindex packages.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/jinaai.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Settings } from \"llamaindex\";\nimport { JinaAIEmbedding } from \"@llamaindex/jinaai\";\n\nSettings.embedModel = new JinaAIEmbedding();\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Basic OpenAI Responses API Setup (TypeScript)\nDESCRIPTION: This snippet demonstrates how to set up the OpenAI Responses API for LlamaIndex. It initializes the API with a specific model, temperature, and max output tokens.  The OpenAI Responses API offers enhanced features like built-in tools, annotations and streaming responses.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openaiResponses } from \"@llamaindex/openai\";\n\nconst llm = openaiResponses({\n  model: \"gpt-4o\",\n  temperature: 0.1,\n  maxOutputTokens: 1000\n});\n```\n\n----------------------------------------\n\nTITLE: Full Example: Loading, Indexing, and Querying with LlamaIndex\nDESCRIPTION: This code provides a complete example of loading a PDF document, indexing it using LlamaIndex, and then querying the index. It uses the PDFReader to load the PDF, creates a VectorStoreIndex for efficient querying, and then queries the index using a specified question. The response is then printed to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/fireworks.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { VectorStoreIndex } from \"llamaindex\";\nimport { PDFReader } from \"llamaindex/readers/PDFReader\";\n\nasync function main() {\n  // Load PDF\n  const reader = new PDFReader();\n  const documents = await reader.loadData(\"../data/brk-2022.pdf\");\n\n  // Split text and create embeddings. Store them in a VectorStoreIndex\n  const index = await VectorStoreIndex.fromDocuments(documents);\n\n  // Query the index\n  const queryEngine = index.asQueryEngine();\n  const response = await queryEngine.query({\n    query: \"What mistakes did Warren E. Buffett make?\",\n  });\n\n  // Output response\n  console.log(response.toString());\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Initializing Gemini LLM in LlamaIndex\nDESCRIPTION: This code snippet initializes the Gemini language model within LlamaIndex. It imports the `Gemini` class and `GEMINI_MODEL` enum from the `@llamaindex/google` package, and sets the global LLM to a Gemini instance using `Settings.llm`. The `model` parameter specifies which Gemini model to use.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/gemini.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Gemini, GEMINI_MODEL } from \"@llamaindex/google\";\nimport { Settings } from \"llamaindex\";\n\nSettings.llm = new Gemini({\n  model: GEMINI_MODEL.GEMINI_PRO,\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing ContextChatEngine with Options in Typescript\nDESCRIPTION: This example showcases how to customize the `ContextChatEngine` by passing options to the `asChatEngine()` method. Options such as `similarityTopK` (number of similar documents to retrieve) and `systemPrompt` (the initial prompt for the chatbot) can be configured. The index object is assumed to be an existing LlamaIndex object.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/chat_engine.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst chatEngine = index.asChatEngine({\n  similarityTopK: 5,\n  systemPrompt: \"You are a helpful assistant.\",\n});\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing Documents in TypeScript\nDESCRIPTION: This snippet demonstrates loading a document and creating a VectorStoreIndex. A `Document` object is created with text content and an ID. Then, `VectorStoreIndex.fromDocuments` is used to index the document, enabling efficient querying.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/azure.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: Initialize and use TogetherEmbedding in LlamaIndex (TypeScript)\nDESCRIPTION: This code snippet demonstrates how to import the `TogetherEmbedding` class from the `@llamaindex/together` package, initialize it with an API key, and set it as the default embedding model in LlamaIndex settings. It then creates a document, builds a VectorStoreIndex, and queries the index using the Together embedding model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/together.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Settings } from \"llamaindex\";\nimport { TogetherEmbedding } from \"@llamaindex/together\";\n\nSettings.embedModel = new TogetherEmbedding({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Create and run an agent with the QueryEngineTool\nDESCRIPTION: This code snippet creates an agent using the `QueryEngineTool` and then runs a query against the agent to get a response.  The response includes both the tool call and the tool result.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/4_agentic_rag.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// Create an agent using the tools array\nconst ragAgent = agent({ tools });\n\nlet toolResponse = await ragAgent.run(\"What's the budget of San Francisco in 2023-2024?\");\n\nconsole.log(toolResponse);\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama for JSON Response Format\nDESCRIPTION: This code configures Ollama to return responses in JSON format. It shows two ways to define the response format: a simple `json_object` type and a Zod schema for validation.  The `responseFormat` parameter specifies the desired output structure.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/ollama.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Ollama } from \"@llamaindex/llms/ollama\";\nimport { z } from \"zod\";\n\n// Simple JSON format\nconst llm = new Ollama({ \n  model: \"llama2\", \n  temperature: 0,\n  responseFormat: { type: \"json_object\" }\n});\n\n// Using Zod schema for validation\nconst responseSchema = z.object({\n  summary: z.string(),\n  topics: z.array(z.string()),\n  sentiment: z.enum([\"positive\", \"negative\", \"neutral\"])\n});\n\nconst llm = new Ollama({ \n  model: \"llama2\", \n  temperature: 0,\n  responseFormat: responseSchema  \n});\n```\n\n----------------------------------------\n\nTITLE: Querying Indexed Data with Perplexity LLM\nDESCRIPTION: This example showcases how to use the Perplexity LLM to query indexed data using LlamaIndex.  It creates a `Document` object, indexes it using `VectorStoreIndex`, creates a query engine, and then queries the index. The Perplexity LLM is configured as the LLM within the LlamaIndex `Settings`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/perplexity.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { perplexity } from \"@llamaindex/perplexity\";\nimport { Document, Settings, VectorStoreIndex } from \"llamaindex\";\n\n// Use the perplexity LLM\nSettings.llm = perplexity({ model: \"sonar\", apiKey: \"<YOUR_API_KEY>\" });\n\nasync function main() {\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Embedding\nDESCRIPTION: This code snippet demonstrates how to use `OpenAIEmbedding` from `@llamaindex/openai` to create embeddings for documents within LlamaIndex. It initializes the embedding model, creates a document, builds a vector store index, and then queries the index. The embedding model is configured globally via `Settings.embedModel`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/openai.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbedding } from \"@llamaindex/openai\";\nimport { Document, Settings, VectorStoreIndex } from \"llamaindex\";\n\nSettings.embedModel = new OpenAIEmbedding();\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI LLM - TypeScript\nDESCRIPTION: This snippet configures the OpenAI LLM for use with LlamaIndex by setting the API key and model.  The API key is retrieved from environment variables, and the model is set to 'gpt-4o'. This sets a global setting, meaning anywhere an LLM is needed will use the same model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/2_create_agent.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nSettings.llm = openai({\n  apiKey: process.env.OPENAI_API_KEY,\n  model: \"gpt-4o\",\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Responses from OpenAI (TypeScript)\nDESCRIPTION: This snippet shows how to enable streaming responses from the OpenAI Responses API. The response is processed in chunks, allowing for real-time handling of large responses.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await llm.chat({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Generate a long response\"\n    }\n  ],\n  stream: true // Enable streaming\n});\n\nfor await (const chunk of response) {\n  console.log(chunk.delta); // Process each chunk of the response\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI for JSON Response (TypeScript)\nDESCRIPTION: This snippet configures the OpenAI language model to return responses in JSON format. The `responseFormat` property is set to `{ type: \"json_object\" }` to ensure the output is structured as a JSON object.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nSettings.llm = new OpenAI({ \n  model: \"gpt-4o\", \n  temperature: 0,\n  responseFormat: { type: \"json_object\" }  \n});\n```\n\n----------------------------------------\n\nTITLE: Creating LlamaIndex VectorStoreIndex\nDESCRIPTION: This code creates a LlamaIndex index from a document, using the provided Qdrant vector store. It initializes a document with the essay text and uses `storageContextFromDefaults` to configure the storage context with the Qdrant vector store.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/qdrant.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst document = new Document({ text: essay, id_: path });\nconst storageContext = await storageContextFromDefaults({ vectorStore });\n  const index = await VectorStoreIndex.fromDocuments([document], {\n    storageContext,\n  });\n```\n\n----------------------------------------\n\nTITLE: Setting up LlamaIndex Vector Store Index\nDESCRIPTION: This TypeScript code creates a `VectorStoreIndex` from a list of `Document` objects using the Supabase vector store. It first creates a storage context using `storageContextFromDefaults` to configure the vector store and then creates the index using `VectorStoreIndex.fromDocuments`, which stores the document embeddings in the Supabase vector store.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/supabase.mdx#_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst documents = [\n  new Document({ \n    text: \"Sample document text\",\n    metadata: { source: \"example\" }\n  })\n];\n\nconst storageContext = await storageContextFromDefaults({ vectorStore });\nconst index = await VectorStoreIndex.fromDocuments(documents, {\n  storageContext,\n});\n```\n\n----------------------------------------\n\nTITLE: Load and Index Documents in LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to load and index a document using LlamaIndex. It creates a `Document` object with text and an ID, then uses `VectorStoreIndex.fromDocuments` to create an index from the document.  `essay` is assumed to be defined elsewhere.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/gemini.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex } from \"llamaindex\";\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: Creating Similarity Search Function in Supabase\nDESCRIPTION: This SQL function, `match_documents`, performs a similarity search on the `documents` table using the `pgvector` extension. It takes a query embedding and a match count as input, calculates the similarity between the query embedding and the document embeddings, and returns the top `match_count` most similar documents. Requires the `pgvector` extension to be enabled and the `documents` table to exist.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/supabase.mdx#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\ncreate function match_documents (\nquery_embedding vector(1536),\nmatch_count int\n) returns table (\nid uuid,\ncontent text,\nmetadata jsonb,\nembedding vector(1536),\nsimilarity float\n)\nlanguage plpgsql\nas $$\nbegin\nreturn query\nselect\nid,\ncontent,\nmetadata,\nembedding,\n1 - (embedding <=> query_embedding) as similarity\nfrom documents\norder by embedding <=> query_embedding\nlimit match_count;\nend;\n$$;\n```\n\n----------------------------------------\n\nTITLE: Querying Index with Metadata Filters in LlamaIndex\nDESCRIPTION: This code creates a query engine and configures it with metadata filters.  The `preFilters` option specifies that only documents with `dogId` equal to `2` should be considered during the query.  The query is then executed, and the response is printed to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/metadata_filtering.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine({\n  preFilters: {\n    filters: [\n      {\n        key: \"dogId\",\n        value: \"2\",\n        operator: \"==\",\n      },\n    ],\n  },\n});\n\nconst response = await queryEngine.query({\n  query: \"What is the color of the dog?\",\n});\n\nconsole.log(response.toString());\n```\n\n----------------------------------------\n\nTITLE: Using Custom Transformation in IngestionPipeline (TypeScript)\nDESCRIPTION: This snippet demonstrates how to use a custom transformation within an IngestionPipeline in LlamaIndex. It initializes an IngestionPipeline with the custom RemoveSpecialCharacters transformation. The pipeline is then run with a document, and the transformed nodes' content is printed to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/index.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { IngestionPipeline, Document, MetadataMode } from \"llamaindex\";\n\nasync function main() {\n  const pipeline = new IngestionPipeline({\n    transformations: [new RemoveSpecialCharacters()],\n  });\n\n  const nodes = await pipeline.run({\n    documents: [\n      new Document({ text: \"I am 10 years old. John is 20 years old.\" }),\n    ],\n  });\n\n  for (const node of nodes) {\n    console.log(node.getContent(MetadataMode.NONE));\n  }\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Initializing Perplexity LLM in LlamaIndex\nDESCRIPTION: This snippet initializes the Perplexity LLM within the LlamaIndex settings.  It imports the `perplexity` function from `@llamaindex/perplexity` and sets it as the default LLM using `Settings.llm`. An API key is required for authentication, and a model name like \"sonar\" needs to be specified.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/perplexity.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Settings } from \"llamaindex\";\nimport { perplexity } from \"@llamaindex/perplexity\";\nSettings.llm = perplexity({\napiKey: \"<YOUR_API_KEY>\",\nmodel: \"sonar\", // or available models \n});\n```\n\n----------------------------------------\n\nTITLE: Fetch Tools from MCP Server (TypeScript)\nDESCRIPTION: This code snippet illustrates how to fetch tools from a Model Context Protocol (MCP) server and integrate them into a LlamaIndex agent.  It shows two methods to initialize the MCP client: using `npx` or directly connecting to a server via URL. After fetching the tools, they are added to the agent's configuration.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/tool/index.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// 1. Import MCP tools adapter\nimport { mcp } from \"@llamaindex/tools\";\nimport { agent } from \"llamaindex\";\n\n// 2. Initialize a MCP client\n// by npx\nconst server = mcp({\n  command: \"npx\",\n  args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \".\"],\n  verbose: true,\n});\n// or by SSE\nconst server = mcp({\n  url: \"http://localhost:8000/mcp\",\n  verbose: true,\n});\n\n// 3. Get tools from MCP server\nconst tools = await server.tools();\n\n// Now you can create an agent with the tools\nconst agent = agent({\n  name: \"My Agent\",\n  systemPrompt: \"You are a helpful assistant that can use the provided tools to answer questions.\",\n  llm: openai({ model: \"gpt-4o\" }),\n  tools: tools,\n});\n```\n\n----------------------------------------\n\nTITLE: Index data using VectorStoreIndex in TypeScript\nDESCRIPTION: This code snippet creates a `VectorStoreIndex` from the loaded documents. The `VectorStoreIndex` converts the text into embeddings using the embedding model previously defined in `Settings`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/4_agentic_rag.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst index = await VectorStoreIndex.fromDocuments(documents);\n```\n\n----------------------------------------\n\nTITLE: Agent and Tool Usage in TypeScript\nDESCRIPTION: Demonstrates creating and using an agent with a custom tool in LlamaIndex with TypeScript, using the OpenAI integration. Includes streaming the agent's response.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/typescript.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { agent, tool } from 'llamaindex'\nimport { openai } from \"@llamaindex/openai\";\n\nSettings.llm = openai({\n  model: \"gpt-4o-mini\",\n});\n\nconst addTool = tool({\n  name: \"add\", \n  description: \"Adds two numbers\",\n  parameters: z.object({x: z.number(), y: z.number()}),\n  execute: ({ x, y }) => x + y,\n});\n\nconst myAgent = agent({\n  tools: [addTool],\n});\n\n// Chat with the agent\nconst context = myAgent.run(\"Hello, how are you?\");\n\nfor await (const event of context) {\n  if (event instanceof AgentStream) {\n    for (const chunk of event.data.delta) {\n      process.stdout.write(chunk); // stream response\n    }\n  } else {\n    console.log(event); // other events\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying the Index in LlamaIndex (TypeScript)\nDESCRIPTION: This snippet shows how to query the created index using LlamaIndex's query engine. It defines a query string and then uses the queryEngine to execute the query and retrieve the results.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/mistral.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Querying the Index in TypeScript\nDESCRIPTION: This snippet shows how to query the index using a query engine. First, a query engine is created from the index using `index.asQueryEngine()`.  Then the `queryEngine.query` function is called with the query text, returning results.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/azure.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Azure OpenAI Environment Variables\nDESCRIPTION: This snippet demonstrates how to set the necessary environment variables to connect to Azure OpenAI.  It requires the Azure OpenAI key, endpoint, and deployment name. These variables are essential for authenticating and connecting to your Azure OpenAI instance.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/azure.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AZURE_OPENAI_KEY=\"<YOUR KEY HERE>\"\nexport AZURE_OPENAI_ENDPOINT=\"<YOUR ENDPOINT, see https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython&pivots=rest-api>\"\nexport AZURE_OPENAI_DEPLOYMENT=\"gpt-4\" # or some other deployment name\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing Documents (TypeScript)\nDESCRIPTION: This snippet demonstrates loading a document and indexing it using LlamaIndex. It creates a `Document` object from text and then uses `VectorStoreIndex` to index the document for efficient querying.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex } from \"llamaindex\";\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: Validating JSON Response with Zod Schema (TypeScript)\nDESCRIPTION: This snippet demonstrates how to use a Zod schema to validate the structure of the JSON response from the OpenAI language model.  This allows for strong type-checking and validation of the expected response format.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\n\nconst responseSchema = z.object({\n  summary: z.string(),  \n  topics: z.array(z.string()),\n  sentiment: z.enum([\"positive\", \"negative\", \"neutral\"])\n});\n\nSettings.llm = new OpenAI({ \n  model: \"gpt-4o\", \n  temperature: 0,\n  responseFormat: responseSchema  \n});\n```\n\n----------------------------------------\n\nTITLE: Setting Response Format per Request (TypeScript)\nDESCRIPTION: Illustrates how to set the `responseFormat` on a per-request basis. This allows for flexible configuration of different response structures for different prompts.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await llm.chat({\n  messages: [...],\n  responseFormat: { type: \"json_object\" } // or a Zod schema\n});\n```\n\n----------------------------------------\n\nTITLE: Load and Index Documents\nDESCRIPTION: This code loads a document and indexes it using LlamaIndex. It creates a Document object from the essay text and then builds a VectorStoreIndex from the document. This allows for efficient querying of the document's content.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/groq.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex } from \"llamaindex\";\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: Load data using SimpleDirectoryReader in TypeScript\nDESCRIPTION: This code snippet uses the `SimpleDirectoryReader` to load data from a specified directory.  The directory is expected to contain files that can be read into documents.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/4_agentic_rag.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst reader = new SimpleDirectoryReader();\nconst documents = await reader.loadData(\"../data\");\n```\n\n----------------------------------------\n\nTITLE: Importing necessary modules for MixedbreadAI Embeddings\nDESCRIPTION: This code snippet imports the required modules from the `@llamaindex/mixedbread` and `llamaindex` packages. It imports the `MixedbreadAIEmbeddings` class from `@llamaindex/mixedbread` for generating embeddings, and the `Document` and `Settings` classes from `llamaindex` for handling documents and configurations.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/mixedbreadai.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MixedbreadAIEmbeddings } from \"@llamaindex/mixedbread\";\nimport { Document, Settings } from \"llamaindex\";\n```\n\n----------------------------------------\n\nTITLE: Ingestion Pipeline with Vector Database Connection\nDESCRIPTION: This code shows how to connect an IngestionPipeline to a Qdrant vector database. It reads a document, creates a QdrantVectorStore instance, defines transformations, and configures the pipeline to insert the resulting nodes into the vector store. Finally, it creates a VectorStoreIndex from the vector store.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/index.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport fs from \"node:fs/promises\";\n\nimport { OpenAIEmbedding } from \"@llamaindex/openai\";\nimport { QdrantVectorStore } from \"@llamaindex/qdrant\";\nimport {\n  Document,\n  IngestionPipeline,\n  MetadataMode,\n  TitleExtractor,\n  SentenceSplitter,\n  VectorStoreIndex,\n} from \"llamaindex\";\n\nasync function main() {\n  // Load essay from abramov.txt in Node\n  const path = \"node_modules/llamaindex/examples/abramov.txt\";\n\n  const essay = await fs.readFile(path, \"utf-8\");\n\n  const vectorStore = new QdrantVectorStore({\n    host: \"http://localhost:6333\",\n  });\n\n  // Create Document object with essay\n  const document = new Document({ text: essay, id_: path });\n  const pipeline = new IngestionPipeline({\n    transformations: [\n      new SentenceSplitter({ chunkSize: 1024, chunkOverlap: 20 }),\n      new TitleExtractor(),\n      new OpenAIEmbedding(),\n    ],\n    vectorStore,\n  });\n\n  // run the pipeline\n  const nodes = await pipeline.run({ documents: [document] });\n\n  // create an index\n  const index = VectorStoreIndex.fromVectorStore(vectorStore);\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Feat: added tool calling and agent support for llama3.1 504B\nDESCRIPTION: This commit adds support for tool calling and agent functionality specifically for the Llama 3.1 504B model. This allows the model to interact with external tools and act as an agent.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_19\n\nLANGUAGE: javascript\nCODE:\n```\n- 376d29a: feat: added tool calling and agent support for llama3.1 504B\n```\n\n----------------------------------------\n\nTITLE: Using PostgreSQL as Document Store\nDESCRIPTION: This code snippet demonstrates how to configure and use PostgreSQL as a document store within LlamaIndex. It initializes a `PostgresDocumentStore` and integrates it into the `storageContext`. It also covers configuring the schema name, table name, namespace and connection string.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/doc_stores/index.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex, storageContextFromDefaults } from \"llamaindex\";\nimport { PostgresDocumentStore } from \"@llamaindex/postgres\";\n\nconst storageContext = await storageContextFromDefaults({\n  docStore: new PostgresDocumentStore(),\n});\n```\n\n----------------------------------------\n\nTITLE: Running the Workflow\nDESCRIPTION: This code demonstrates how to run the joke generation workflow. It creates a workflow context, triggers the initial `startEvent`, listens to the event stream, and processes events as they arrive. It uses `resultEvent.include(event)` to check if an event is of the `resultEvent` type and breaks the loop when the final result is received.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/workflows.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nasync function main() {\n  const { stream, sendEvent } = jokeFlow.createContext();\n  sendEvent(startEvent.with(\"pirates\"));\n\n  let result: { joke: string, critique: string } | undefined;\n\n  for await (const event of stream) {\n    // console.log(event.data);  optionally log the event data\n    if (resultEvent.include(event)) {\n      result = event.data;\n      break; // Stop when we get the final result\n    }\n  }\n  \n  console.log(result);\n}\n```\n\n----------------------------------------\n\nTITLE: Chatting with Perplexity LLM\nDESCRIPTION: This example demonstrates how to use the Perplexity LLM for chat interactions. It sends a message to the LLM and retrieves the response. The example includes both a standard response and a streaming response using `perplexityLlm.chat()`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/perplexity.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { perplexity } from \"@llamaindex/perplexity\";\n\nconst perplexityLlm = perplexity({\n  apiKey: \"<YOUR_API_KEY>\",\n  model: \"sonar\", // or avaiable models\n});\n\nasync function main() {\n  const response = await perplexityLlm.chat({\n    messages: [\n      {\n        role: \"system\",\n        content: \"You are an AI assistant\",\n      },\n      {\n        role: \"user\",\n        content: \"Tell me about San Francisco\",\n      },\n    ],\n    stream: false,\n  });\n  console.log(response);\n\n  const stream = await perplexityLlm.chat({\n    messages: [\n      {\n        role: \"system\",\n        content: \"You are a creative AI assistant that tells engaging stories\",\n      },\n      {\n        role: \"user\",\n        content: \"Tell me a short story\",\n      },\n    ],\n    stream: true,\n  });\n\n  console.log(\"\\nStreaming response:\");\n  for await (const chunk of stream) {\n    process.stdout.write(chunk.delta);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Query Engine with Reranker\nDESCRIPTION: This code creates a query engine that combines the retriever and node postprocessor (the reranker). This enables you to query the index and have the results reranked by Mixedbread AI.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/mixedbreadiai_reranker.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine({\n  retriever,\n  nodePostprocessors: [nodePostprocessor],\n});\n\n// Log the response\nconst response = await queryEngine.query(\"Where did the author grow up?\");\nconsole.log(response);\n```\n\n----------------------------------------\n\nTITLE: Create a QueryEngineTool from the index\nDESCRIPTION: This code snippet creates a `QueryEngineTool` from the `VectorStoreIndex`. The tool is configured with metadata to help the agent decide when to use it and an option to retrieve the top 10 most relevant chunks of text.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/4_agentic_rag.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst tools = [\n  index.queryTool({\n    metadata: {\n      name: \"san_francisco_budget_tool\",\n      description: `This tool can answer detailed questions about the individual components of the budget of San Francisco in 2023-2024.`,\n    },\n    options: { similarityTopK: 10 },\n  }),\n];\n```\n\n----------------------------------------\n\nTITLE: Response Tracking and Storage with OpenAI (TypeScript)\nDESCRIPTION: This TypeScript snippet configures the OpenAI Responses API for response tracking and storage. It enables tracking, storage, associates responses with a user, and adds custom metadata for session context.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst llm = openaiResponses({\n  trackPreviousResponses: true, // Enable response tracking\n  store: true, // Store responses for future reference\n  user: \"user-123\", // Associate responses with a user\n  callMetadata: { // Add custom metadata\n    sessionId: \"session-123\",\n    context: \"customer-support\"\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Using LlamaCloud Index with Vercel AI SDK in TypeScript\nDESCRIPTION: This example demonstrates how to use a LlamaCloud index with Vercel's AI SDK for production deployments. It initializes a `LlamaCloudIndex` using documents, index name, project name, and API key from environment variables. Then, it creates a query tool with `llamaindex` and uses `streamText` to stream the response via Vercel's AI SDK.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/integration/vercel.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LlamaCloudIndex } from \"@llamaindex/cloud\";\n\n// Create a LlamaCloud index\nconst index = await LlamaCloudIndex.fromDocuments({\n  documents: [document],\n  name: \"your-index-name\",\n  projectName: \"your-project\",\n  apiKey: process.env.LLAMA_CLOUD_API_KEY,\n});\n\n// Use it the same way as VectorStoreIndex\nconst queryTool = llamaindex({\n  model: openai(\"gpt-4\"),\n  index,\n  description: \"Search through the documents\",\n  options: { fields: [\"sourceNodes\", \"messages\"]}\n});\n\n// Use the tool with Vercel's AI SDK\nstreamText({\n  model: openai(\"gpt-4\"),\n  prompt: \"Your question here\",\n  tools: { queryTool },\n}).toDataStream();\n```\n\n----------------------------------------\n\nTITLE: Create and Query VectorStoreIndex in LlamaIndex\nDESCRIPTION: This snippet fetches text from a URL, creates a LlamaIndex Document object, and then builds a VectorStoreIndex from this document. It queries the index and prints the response.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/jupyter/vectorIndex.ipynb#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Create Document object with essay\nconst resp = await fetch('https://raw.githubusercontent.com/run-llama/LlamaIndexTS/main/packages/core/examples/abramov.txt');\nconst text = await resp.text();\nconst document = new Document({ text });\n\n// Split text and create embeddings. Store them in a VectorStoreIndex\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\n// Query the index\nconst queryEngine = index.asQueryEngine();\nconst response = await queryEngine.query({\n    query: \"What did the author do in college?\",\n});\n\n// Output response\nconsole.log(response.toString());\n```\n\n----------------------------------------\n\nTITLE: Importing LlamaIndex modules in TypeScript\nDESCRIPTION: This code imports the required modules from the `llamaindex` library. It includes classes for creating a router query engine, reading data from a directory, splitting text into sentences, creating summary and vector store indices, and configuring settings.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/router_query_engine.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  RouterQueryEngine,\n  SimpleDirectoryReader,\n  SentenceSplitter,\n  SummaryIndex,\n  VectorStoreIndex,\n  Settings,\n} from \"llamaindex\";\nimport { OpenAI } from \"@llamaindex/openai\";\nimport { SimpleDirectoryReader } from \"@llamaindex/readers/directory\";\n```\n\n----------------------------------------\n\nTITLE: Using MistralAI Embedding with LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to use the `MistralAIEmbedding` class to update the embed model in LlamaIndex. It initializes the `MistralAIEmbedding` with an API key, sets it as the `embedModel` in `Settings`, loads a document, creates a `VectorStoreIndex`, and then queries the index. The API key is required for authenticating with the MistralAI service.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/mistral.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, Settings, VectorStoreIndex } from \"llamaindex\";\nimport { MistralAIEmbedding } from \"@llamaindex/mistral\";\n\n// Update Embed Model\nSettings.embedModel = new MistralAIEmbedding({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing Documents in LlamaIndex (TypeScript)\nDESCRIPTION: This snippet demonstrates loading a document and indexing it using LlamaIndex's VectorStoreIndex. It creates a Document object with the provided text and ID, then uses fromDocuments to build the index.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/mistral.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex } from \"llamaindex\";\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing Documents in LlamaIndex\nDESCRIPTION: This snippet shows how to load a text document and create a VectorStoreIndex from it. The Document object holds the text content, and VectorStoreIndex allows for efficient querying of the document's content.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/ollama.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex } from \"llamaindex\";\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: LlamaParseReader Usage Example (TypeScript)\nDESCRIPTION: This snippet demonstrates how to use the `LlamaParseReader` class to load local files and convert them into a parsed document. It showcases the basic usage of the reader to process documents with LlamaParse, including setting the API key.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/llama_parse/index.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LlamaParseReader } from \"llamaindex/readers/LlamaParseReader\";\n\nasync function main() {\n  const reader = new LlamaParseReader({\n    apiKey: \"YOUR_API_KEY\",\n  });\n  const documents = await reader.loadData(\"path/to/your/file.pdf\");\n  console.log(documents);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Initialize TogetherLLM\nDESCRIPTION: This code snippet initializes the `TogetherLLM` class from the `@llamaindex/together` package and sets it as the default LLM in LlamaIndex settings. It requires an API key for authentication.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/together.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Settings } from \"llamaindex\";\nimport { TogetherLLM } from \"@llamaindex/together\";\n\nSettings.llm = new TogetherLLM({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring LlamaIndex Server with Custom UI Components\nDESCRIPTION: Initializes and starts the LlamaIndex Server, specifying the 'components' directory for custom UI components.  This allows the server to render custom components based on workflow events.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/llamaindex-server.mdx#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nnew LlamaIndexServer({\n  workflow: createWorkflow,\n  uiConfig: {\n    appTitle: \"LlamaIndex App\",\n    componentsDir: \"components\",\n  },\n}).start();\n```\n\n----------------------------------------\n\nTITLE: Query the Index\nDESCRIPTION: This code queries the created index to retrieve information. It sets up a query engine from the index, defines the query, and then executes the query to retrieve relevant results. The `queryEngine.query` method handles the query processing and returns the results.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/groq.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Import Modules\nDESCRIPTION: Imports the required modules from the llamaindex and @llamaindex/openai packages. These modules include classes like OpenAI, Document, RelevancyEvaluator, Settings, and VectorStoreIndex, which are essential for building and evaluating a query engine.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/relevancy.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@llamaindex/openai\";\nimport {\n  Document,\n  RelevancyEvaluator,\n  Settings,\n  VectorStoreIndex,\n} from \"llamaindex\";\n```\n\n----------------------------------------\n\nTITLE: Creating Chat UI Component with React/JSX\nDESCRIPTION: This code snippet showcases a React component (`./src/components/demo/chat/api/demo.tsx`) that consumes the API route created earlier to provide a chat interface.  It demonstrates how to fetch data from the `/api/chat` endpoint and render the chat messages.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/chat.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n{  \n  \"file\": \"./src/components/demo/chat/api/demo.tsx\",\n\t\"codeblock\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Agent Response - TypeScript\nDESCRIPTION: This example demonstrates how to stream the response from the agent. It iterates through the events in the `context` and writes the `delta` from `AgentStream` events to the standard output. This allows for incremental display of the response.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/2_create_agent.mdx#_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst context = myAgent.run(\"Add 101 and 303\");\nfor await (const event of context) {\n  if (event instanceof AgentStream) {\n    process.stdout.write(event.data.delta);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Define Tool with Zod Validation (TypeScript)\nDESCRIPTION: This code snippet defines a tool using the `tool` function from `llamaindex` and uses `zod` for parameter validation.  It defines a `queryKnowledgeBase` function that fetches data from a knowledge base with a user token and query parameter. Zod is used to ensure the input to the tool is a string.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/tool/index.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { agent, tool } from \"llamaindex\";\nimport { z } from \"zod\";\n\n// first arg is LLM input, second is bound arg\nconst queryKnowledgeBase = async ({ question }, { userToken }) => {\n  const response = await fetch(`https://knowledge-base.com?token=${userToken}&query=${question}`);\n  // ...\n};\n\n// define tool with zod validation\nconst kbTool = tool(queryKnowledgeBase, {\n  name: 'queryKnowledgeBase',\n  description: 'Query knowledge base',\n  parameters: z.object({\n    question: z.string({\n      description: 'The user question',\n    }),\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Creating query engines in TypeScript\nDESCRIPTION: This code creates two query engines, one for the `VectorStoreIndex` and one for the `SummaryIndex`. The `asQueryEngine()` method is used to create a query engine for each index. These query engines will be used by the router query engine to handle different types of queries.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/router_query_engine.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst vectorQueryEngine = vectorIndex.asQueryEngine();\nconst summaryQueryEngine = summaryIndex.asQueryEngine();\n```\n\n----------------------------------------\n\nTITLE: Configuring Gemini Embedding Model\nDESCRIPTION: This code shows how to configure the GeminiEmbedding to use a specific model version, like GEMINI_PRO_LATEST, by passing the 'model' parameter in the constructor.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/gemini.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { GEMINI_MODEL, GeminiEmbedding } from \"@llamaindex/google\";\n\nSettings.embedModel = new GeminiEmbedding({\n  model: GEMINI_MODEL.GEMINI_PRO_LATEST,\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing ContextChatEngine using index.asChatEngine() in Typescript\nDESCRIPTION: This code demonstrates a simplified way to initialize the `ContextChatEngine` directly from an index using the `asChatEngine()` method. This method internally configures the chat engine with default settings based on the index.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/chat_engine.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst chatEngine = index.asChatEngine();\n```\n\n----------------------------------------\n\nTITLE: Creating vector and summary indices in TypeScript\nDESCRIPTION: This code creates two different types of indices: a `VectorStoreIndex` and a `SummaryIndex`. Both indices are built from the loaded documents. The `VectorStoreIndex` is suitable for semantic search, while the `SummaryIndex` is used for summarization tasks.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/router_query_engine.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst vectorIndex = await VectorStoreIndex.fromDocuments(documents);\nconst summaryIndex = await SummaryIndex.fromDocuments(documents);\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing Documents with LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to load data from a PDF document using the PDFReader and index it using VectorStoreIndex in LlamaIndex. It initializes a PDFReader, loads the PDF data, and then creates a VectorStoreIndex from the loaded documents. This index is used for efficient querying of the document content.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/fireworks.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst reader = new PDFReader();\nconst documents = await reader.loadData(\"../data/brk-2022.pdf\");\n\n// Split text and create embeddings. Store them in a VectorStoreIndex\nconst index = await VectorStoreIndex.fromDocuments(documents);\n```\n\n----------------------------------------\n\nTITLE: Load JSON Data with LlamaParse\nDESCRIPTION: Loads a PDF file using LlamaParse in JSON mode. The `loadJson` method returns an array of JSON objects representing the parsed document structure. The code accesses and processes the 'pages' array within the first JSON object, which contains the parsed file data.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/llama_parse/json_mode.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LlamaParseReader } from \"@llamaindex/cloud\";\n\nconst reader = new LlamaParseReader();\nasync function main() {\n  // Load the file and return an array of json objects\n  const jsonObjs = await reader.loadJson(\"../data/uber_10q_march_2022.pdf\");\n  // Access the first \"pages\" (=a single parsed file) object in the array\n  const jsonList = jsonObjs[0][\"pages\"];\n  // Further process the jsonList object as needed.\n}\n```\n\n----------------------------------------\n\nTITLE: Use local embeddings\nDESCRIPTION: This snippet configures LlamaIndex.TS to use a local Hugging Face embedding model. It imports the `HuggingFaceEmbedding` class and sets `Settings.embedModel` to an instance of this class, specifying the model type and quantization settings.  The BAAI/bge-small-en-v1.5 model is used in this example.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/local_llm.mdx#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nSettings.embedModel = new HuggingFaceEmbedding({\n  modelType: \"BAAI/bge-small-en-v1.5\",\n  quantized: false,\n});\n```\n\n----------------------------------------\n\nTITLE: Querying the Postgres Vector Store\nDESCRIPTION: This command executes the `query.ts` script using `tsx`. The script prompts the user for a question, then processes the query against the Postgres vector store using the OpenAI API. The script continues prompting for queries until the user enters 'q', 'quit', or 'exit'.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/vector-store/pg/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx vector-store/pg/query.ts\n```\n\n----------------------------------------\n\nTITLE: Performing Reranking with Objects\nDESCRIPTION: This code uses the `rerank` method to reorder the documents based on the provided query. The results are logged to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/mixedbreadiai_reranker.mdx#_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await reranker.rerank(documents, query);\nconsole.log(result); // Perfectly customized results, ready to serve.\n```\n\n----------------------------------------\n\nTITLE: Load and Index Data\nDESCRIPTION: This snippet shows the command to load and index data from MongoDB using LlamaIndex. This process retrieves data, splits it into chunks, generates embeddings, and stores them in MongoDB.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/mongodb/README.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nnpx tsx mongodb/2_load_and_index.ts\n```\n\n----------------------------------------\n\nTITLE: Use Built-in Wiki Tool (TypeScript)\nDESCRIPTION: This code snippet demonstrates how to import and use a built-in tool (wiki) from the `@llamaindex/tools` package within a LlamaIndex agent. The `wiki` tool is used to gather information from the internet. The snippet shows how to integrate this tool into an agent's configuration.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/tool/index.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { agent } from \"llamaindex\";\nimport { wiki } from \"@llamaindex/tools\";\n\nconst researchAgent = agent({\n  name: \"WikiAgent\",\n  description: \"Gathering information from the internet\",\n  systemPrompt: `You are a research agent. Your role is to gather information from the internet using the provided tools.`,\n  tools: [wiki()],\n});\n```\n\n----------------------------------------\n\nTITLE: Query Movie Reviews in AstraDB with RAG\nDESCRIPTION: This example uses Retrieval-Augmented Generation (RAG) to query the movie review data stored in AstraDB. It demonstrates how to query the AstraDB vector store and retrieve relevant information.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/astradb/README.md#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nrun `npx tsx astradb/query`\n```\n\n----------------------------------------\n\nTITLE: Streaming Response Synthesizer LlamaIndex TypeScript\nDESCRIPTION: This code demonstrates how to use the `synthesize` function of the ResponseSynthesizer with streaming enabled. The `stream: true` option is added to the `synthesize` call, and the resulting stream is iterated over to process each chunk of the response.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/response_synthesizer.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst stream = await responseSynthesizer.synthesize({\n  query: \"What age am I?\",\n  nodesWithScore,\n  stream: true,\n});\nfor await (const chunk of stream) {\n  process.stdout.write(chunk.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Documents with Metadata in LlamaIndex\nDESCRIPTION: This code creates two `Document` objects, each containing text and associated metadata. The metadata includes the color and dog ID. These documents will be stored in the vector store and used for querying.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/metadata_filtering.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst docs = [\n  new Document({\n    text: \"The dog is brown\",\n    metadata: {\n      color: \"brown\",\n      dogId: \"1\",\n    },\n  }),\n  new Document({\n    text: \"The dog is red\",\n    metadata: {\n      color: \"red\",\n      dogId: \"2\",\n    },\n  }),\n];\n```\n\n----------------------------------------\n\nTITLE: Full LlamaIndex LlamaDeuce Example\nDESCRIPTION: This is a complete example demonstrating the usage of LlamaDeuce with LlamaIndex. It includes importing necessary modules, initializing LlamaDeuce, loading and indexing a document, creating a query engine, executing a query, and logging the response.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/llama2.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LlamaDeuce, DeuceChatStrategy } from \"@llamaindex/replicate\";\nimport { Document, VectorStoreIndex, Settings } from \"llamaindex\";\n\n// Use the LlamaDeuce LLM\nSettings.llm = new LlamaDeuce({ chatStrategy: DeuceChatStrategy.META });\n\nasync function main() {\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of JSONReader in Typescript\nDESCRIPTION: Demonstrates the basic usage of the `JSONReader` class to load JSON data from a file and directly from a string. It shows how to instantiate the reader with options and then use the `loadData` and `loadDataAsContent` methods.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/json.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSONReader } from \"@llamaindex/readers/json\";\n\nconst file = \"../../PATH/TO/FILE\";\nconst content = new TextEncoder().encode(\"JSON_CONTENT\");\n\nconst reader = new JSONReader({ levelsBack: 0, collapseLength: 100 });\nconst docsFromFile = reader.loadData(file);\nconst docsFromContent = reader.loadDataAsContent(content);\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing a Document\nDESCRIPTION: This code creates a new document and indexes it using a vector store index. It also sets up OpenAI as the language model. This demonstrates how to prepare and index your data for use with LlamaIndex and the reranker.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/mixedbreadiai_reranker.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst document = new Document({\n  text: \"This is a sample document.\",\n  id_: \"sampleDoc\",\n});\n\nSettings.llm = new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0.1 });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: Querying the LlamaIndex Index\nDESCRIPTION: This code snippet demonstrates how to query an existing VectorStoreIndex in LlamaIndex. It creates a query engine from the index and then submits a query to the engine. The response from the query is then stored in the 'response' variable, providing insights extracted from the indexed documents.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/fireworks.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\nconst response = await queryEngine.query({\n  query: \"What mistakes did Warren E. Buffett make?\",\n});\n```\n\n----------------------------------------\n\nTITLE: Loading Documents into Postgres Vector Store\nDESCRIPTION: This command uses `tsx` to execute the `load-docs.ts` script. The script reads documents from the specified directory (e.g., 'data') and saves the embedding vectors to the configured Postgres database. It assumes the LlamaIndexTS default readers are used to handle different file types within the directory.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/vector-store/pg/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx vector-store/pg/load-docs.ts data\n```\n\n----------------------------------------\n\nTITLE: Querying the Index with LlamaIndex (TS)\nDESCRIPTION: This snippet demonstrates how to query the index created with LlamaIndex. It creates a query engine from the index using `index.asQueryEngine()` and then executes a query using `queryEngine.query()`. The response contains the answer generated by the LLM.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/portkey.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: feat: added meta3.2 support via Bedrock in Javascript\nDESCRIPTION: This feature added meta3.2 support via Bedrock including vision, tool call and inference region support. This allows users to leverage the vision, tool call and inference region support for the meta3.2 model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\n- 2774e80: feat: added meta3.2 support via Bedrock including vision, tool call and inference region support\n```\n\n----------------------------------------\n\nTITLE: Full RouterQueryEngine Implementation in TypeScript\nDESCRIPTION: This complete example demonstrates the creation and use of a LlamaIndex RouterQueryEngine using TypeScript.  It includes importing necessary modules, loading data, setting up LLM and node parsing, creating vector and summary indices, building query engines, configuring the router, and submitting a summarization and specific context query.  The results from each query are logged to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/router_query_engine.mdx#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  RouterQueryEngine,\n  SimpleDirectoryReader,\n  SentenceSplitter,\n  SummaryIndex,\n  VectorStoreIndex,\n  Settings,\n} from \"llamaindex\";\nimport { OpenAI } from \"@llamaindex/openai\";\nimport { SimpleDirectoryReader } from \"@llamaindex/readers/directory\";\n\nSettings.llm = new OpenAI();\nSettings.nodeParser = new SentenceSplitter({\n  chunkSize: 1024,\n});\n\nasync function main() {\n  // Load documents from a directory\n  const documents = await new SimpleDirectoryReader().loadData({\n    directoryPath: \"node_modules/llamaindex/examples\",\n  });\n\n  // Create indices\n  const vectorIndex = await VectorStoreIndex.fromDocuments(documents);\n  const summaryIndex = await SummaryIndex.fromDocuments(documents);\n\n  // Create query engines\n  const vectorQueryEngine = vectorIndex.asQueryEngine();\n  const summaryQueryEngine = summaryIndex.asQueryEngine();\n\n  // Create a router query engine\n  const queryEngine = RouterQueryEngine.fromDefaults({\n    queryEngineTools: [\n      {\n        queryEngine: vectorQueryEngine,\n        description: \"Useful for summarization questions related to Abramov\",\n      },\n      {\n        queryEngine: summaryQueryEngine,\n        description: \"Useful for retrieving specific context from Abramov\",\n      },\n    ],\n  });\n\n  // Query the router query engine\n  const summaryResponse = await queryEngine.query({\n    query: \"Give me a summary about his past experiences?\",\n  });\n\n  console.log({\n    answer: summaryResponse.response,\n    metadata: summaryResponse?.metadata?.selectorResult,\n  });\n\n  const specificResponse = await queryEngine.query({\n    query: \"Tell me about abramov first job?\",\n  });\n\n  console.log({\n    answer: specificResponse.response,\n    metadata: specificResponse.metadata.selectorResult,\n  });\n}\n\nmain().then(() => console.log(\"Done\"));\n```\n\n----------------------------------------\n\nTITLE: Defining Bedrock Inference Endpoint Constants\nDESCRIPTION: This snippet defines constants for using Bedrock's Inference endpoints, differentiating between US and EU regions. It includes models like Anthropic Claude 3 and Meta Llama 3. These region-specific constants are crucial for accessing the correct endpoints when initializing the LLM.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/bedrock.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// US\nUS_ANTHROPIC_CLAUDE_3_HAIKU = \"us.anthropic.claude-3-haiku-20240307-v1:0\";\nUS_ANTHROPIC_CLAUDE_3_OPUS = \"us.anthropic.claude-3-opus-20240229-v1:0\";\nUS_ANTHROPIC_CLAUDE_3_SONNET = \"us.anthropic.claude-3-sonnet-20240229-v1:0\";\nUS_ANTHROPIC_CLAUDE_3_5_SONNET = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\";\nUS_ANTHROPIC_CLAUDE_3_5_SONNET_V2 =\n  \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\";\nUS_META_LLAMA_3_2_1B_INSTRUCT = \"us.meta.llama3-2-1b-instruct-v1:0\";\nUS_META_LLAMA_3_2_3B_INSTRUCT = \"us.meta.llama3-2-3b-instruct-v1:0\";\nUS_META_LLAMA_3_2_11B_INSTRUCT = \"us.meta.llama3-2-11b-instruct-v1:0\";\nUS_META_LLAMA_3_2_90B_INSTRUCT = \"us.meta.llama3-2-90b-instruct-v1:0\";\nUS_AMAZON_NOVA_PRO_1 = \"us.amazon.nova-premier-v1:0\";\nUS_AMAZON_NOVA_PRO_1 = \"us.amazon.nova-pro-v1:0\";\nUS_AMAZON_NOVA_LITE_1 = \"us.amazon.nova-lite-v1:0\";\nUS_AMAZON_NOVA_MICRO_1 = \"us.amazon.nova-micro-v1:0\";\n\n// EU\nEU_ANTHROPIC_CLAUDE_3_HAIKU = \"eu.anthropic.claude-3-haiku-20240307-v1:0\";\nEU_ANTHROPIC_CLAUDE_3_SONNET = \"eu.anthropic.claude-3-sonnet-20240229-v1:0\";\nEU_ANTHROPIC_CLAUDE_3_5_SONNET = \"eu.anthropic.claude-3-5-sonnet-20240620-v1:0\";\nEU_META_LLAMA_3_2_1B_INSTRUCT = \"eu.meta.llama3-2-1b-instruct-v1:0\";\nEU_META_LLAMA_3_2_3B_INSTRUCT = \"eu.meta.llama3-2-3b-instruct-v1:0\";\nEU_AMAZON_NOVA_PRO_1 = \"eu.amazon.nova-premier-v1:0\";\nEU_AMAZON_NOVA_PRO_1 = \"eu.amazon.nova-pro-v1:0\";\nEU_AMAZON_NOVA_LITE_1 = \"eu.amazon.nova-lite-v1:0\";\nEU_AMAZON_NOVA_MICRO_1 = \"eu.amazon.nova-micro-v1:0\";\n```\n\n----------------------------------------\n\nTITLE: Evaluate Response Relevancy\nDESCRIPTION: Creates a vector index from a document, sets up a query engine, and evaluates the response to a query using the RelevancyEvaluator. It logs whether the response is relevant or not based on the evaluator's assessment. Requires the previously imported modules and OpenAI API key.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/relevancy.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst documents = [\n  new Document({\n    text: `The city came under British control in 1664 and was renamed New York after King Charles II of England granted the lands to his brother, the Duke of York. The city was regained by the Dutch in July 1673 and was renamed New Orange for one year and three months; the city has been continuously named New York since November 1674. New York City was the capital of the United States from 1785 until 1790, and has been the largest U.S. city since 1790. The Statue of Liberty greeted millions of immigrants as they came to the U.S. by ship in the late 19th and early 20th centuries, and is a symbol of the U.S. and its ideals of liberty and peace. In the 21st century, New York City has emerged as a global node of creativity, entrepreneurship, and as a symbol of freedom and cultural diversity. The New York Times has won the most Pulitzer Prizes for journalism and remains the U.S. media's \"newspaper of record\". In 2019, New York City was voted the greatest city in the world in a survey of over 30,000 p...\\tPass`,\n  }),\n];\n\nconst vectorIndex = await VectorStoreIndex.fromDocuments(documents);\n\nconst queryEngine = vectorIndex.asQueryEngine();\n\nconst query = \"How did New York City get its name?\";\n\nconst response = await queryEngine.query({\n  query,\n});\n\nconst evaluator = new RelevancyEvaluator();\n\nconst result = await evaluator.evaluateResponse({\n  query,\n  response: response,\n});\n\nconsole.log(`the response is ${result.passing ? \"relevant\" : \"not relevant\"}`);\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI with Custom Base URL (TypeScript)\nDESCRIPTION: This TypeScript snippet initializes the OpenAI language model with a custom base URL.  This is useful when you want to use a different API endpoint than the default OpenAI one.  Replace apiKey with your actual API Key.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nSettings.llm = new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0, apiKey: <YOUR_API_KEY>, baseURL: \"https://api.scaleway.ai/v1\" });\n```\n\n----------------------------------------\n\nTITLE: Final Agent Message with the Combined Budget in JavaScript\nDESCRIPTION: This code shows the final message from the agent after performing the calculations. The `content` property contains the final answer, stating the combined budget of San Francisco for community health and public protection, demonstrating the agent's ability to retrieve information and perform calculations to answer complex queries.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/5_rag_and_tools.mdx#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    message: {\n      content: 'The combined budget of San Francisco for community health and public protection in Fiscal Year (FY) 2023-24 is $5,212.5 million.',\n      role: 'assistant',\n      options: {}\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating JinaAIReranker Instance\nDESCRIPTION: This code creates an instance of the `JinaAIReranker` class.  The `topN` parameter specifies the number of results to return after reranking.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/jinaai_reranker.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst nodePostprocessor = new JinaAIReranker({\n  topN: 5,\n});\n```\n\n----------------------------------------\n\nTITLE: Using HuggingFace Embedding with LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to initialize and use a HuggingFace embedding model with LlamaIndex. It imports necessary modules, sets the embed model, creates a document, builds an index, and performs a query. Requires the `llamaindex` and `@llamaindex/huggingface` packages.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/huggingface.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, Settings, VectorStoreIndex } from \"llamaindex\";\nimport { HuggingFaceEmbedding } from \"@llamaindex/huggingface\";\n\n// Update Embed Model\nSettings.embedModel = new HuggingFaceEmbedding();\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Supporting Multimodal Output with Google's Gen AI Library\nDESCRIPTION: This feature utilizes Google's new Gen AI library to support multimodal output, enabling the generation of richer and more diverse content types.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_6\n\n\n\n----------------------------------------\n\nTITLE: Running LlamaCloud Query Example (Shell)\nDESCRIPTION: This snippet demonstrates how to execute the `query.ts` example, showcasing the usage of a managed LlamaCloud index with a query engine. TypeScript and required dependencies need to be configured beforehand.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/cloud/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npnpx tsx cloud/query.ts\n```\n\n----------------------------------------\n\nTITLE: Using Message Content Types with OpenAI (TypeScript)\nDESCRIPTION: This TypeScript snippet showcases how to use different types of message content, including text and images, with the OpenAI Responses API. The `input_text` and `input_image` types are specified within the message content.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await llm.chat({\n  messages: [\n    {\n      role: \"user\",\n      content: [\n        {\n          type: \"input_text\",\n          text: \"What's in this image?\"\n        },\n        {\n          type: \"input_image\",\n          image_url: \"https://example.com/image.jpg\",\n          detail: \"auto\" // Optional: can be \"auto\", \"low\", or \"high\"\n        }\n      ]\n    }\n  ]\n});\n```\n\n----------------------------------------\n\nTITLE: Making a Request to the Chat Endpoint\nDESCRIPTION: Sends a POST request to the /api/chat endpoint of the LlamaIndex Server with a JSON payload containing a user message. It utilizes curl to simulate an API call.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/llamaindex-server.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:3000/api/chat\" -H \"Content-Type: application/json\" -d '{\"message\": \"Who is the first president of the United States?\"}'\n```\n\n----------------------------------------\n\nTITLE: Setting up MixedbreadAI Embeddings and Indexing Document\nDESCRIPTION: This code initializes the `MixedbreadAIEmbeddings` with the API key and model name and sets it as the embed model in LlamaIndex `Settings`. It then creates a `Document` with sample text and creates a `VectorStoreIndex` from it.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/mixedbreadai.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nSettings.embedModel = new MixedbreadAIEmbeddings({\n  apiKey: \"<MIXEDBREAD_API_KEY>\",\n  model: \"mixedbread-ai/mxbai-embed-large-v1\",\n});\n\nconst document = new Document({\n  text: \"The true source of happiness.\",\n  id_: \"bread\",\n});\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: DiscordReader Example Usage (TypeScript)\nDESCRIPTION: This TypeScript code demonstrates how to use the DiscordReader to load messages from a Discord channel.  It initializes the reader with a Discord bot token and channel ID, then loads the data. The token needs to be obtained from a Discord application.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/discord.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DiscordReader } from \"@llamaindex/discord\";\n\nasync function main() {\n  const discordToken = \"YOUR_DISCORD_TOKEN\";\n  const channelIDs = [\"YOUR_CHANNEL_ID\"];\n\n  const reader = new DiscordReader({ discordToken });\n\n  // Load data with a limit of 5 messages and include additional info\n  const documents = await reader.loadData(channelIDs, { limit: 5, additionalInfo: true });\n\n  console.log(documents);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Event Streaming in Agent Workflows - Typescript\nDESCRIPTION: This code snippet demonstrates how to use event streaming in Agent Workflows to track and respond to different events during execution. It gets the workflow execution context, streams events, and handles `AgentToolCall` and `AgentStream` events. It requires an existing `workflow` object.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/agent_workflow.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AgentToolCall, AgentStream } from \"llamaindex\";\n\n// Get the workflow execution context\nconst context = workflow.run(\"Tell me something funny\");\n\n// Stream and handle events\nfor await (const event of context) {\n  if (event instanceof AgentToolCall) {\n    console.log(`Tool being called: ${event.data.toolName}`);\n  }\n  if (event instanceof AgentStream) {\n    process.stdout.write(event.data.delta);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading documents into Pinecone\nDESCRIPTION: This command executes the load-docs.ts script to import documents from a specified directory and save their embedding vectors to a Pinecone database. It utilizes the llamaindexTS default readers to process files within the input directory and requires environment variables such as PINECONE_API_KEY, PINECONE_ENVIRONMENT, and OPENAI_API_KEY to be set.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/pinecone-vector-store/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx pinecone-vector-store/load-docs.ts data\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing Documents in LlamaIndex (TypeScript)\nDESCRIPTION: This code snippet shows how to load a document and index it using LlamaIndex's VectorStoreIndex. It creates a Document object with the text content and an ID, then uses VectorStoreIndex.fromDocuments to create an index from the document.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/deepinfra.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex } from \"llamaindex\";\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: Handler: Critiquing the Joke\nDESCRIPTION: This handler handles the `jokeEvent`, critiques the joke using OpenAI's LLM, and either emits a `critiqueEvent` if the joke needs improvement or a `resultEvent` if the joke is good enough. It checks if the LLM's critique includes the word \"IMPROVE\" to determine if the joke needs further iteration.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/workflows.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\njokeFlow.handle([jokeEvent], async (event) => {\n  // Prompt the LLM to critique the joke\n  const prompt = `Give a thorough critique of the following joke. If the joke needs improvement, put \\\"IMPROVE\\\" somewhere in the critique: ${event.data.joke}`;\n  const response = await llm.complete({ prompt });\n\n  // If the critique includes \"IMPROVE\", keep iterating, else, return the result\n  if (response.text.includes(\"IMPROVE\")) {\n    return critiqueEvent.with({ joke: event.data.joke, critique: response.text });\n  }\n\n  return resultEvent.with({ joke: event.data.joke, critique: response.text });\n});\n```\n\n----------------------------------------\n\nTITLE: Extracting Images with LlamaParse\nDESCRIPTION: This snippet demonstrates how to use LlamaParse to load a JSON representation of a document (e.g., PDF) and then extract the images found within the document. The `getImages` method downloads the images to a specified folder and returns a list of ImageNode objects.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/llama_parse/images.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst reader = new LlamaParseReader();\nconst jsonObjs = await reader.loadJson(\"../data/uber_10q_march_2022.pdf\");\nconst imageDicts = await reader.getImages(jsonObjs, \"images\");\n```\n\n----------------------------------------\n\nTITLE: Defining Workflow Events with Typescript\nDESCRIPTION: This code defines the workflow events used in the joke generation workflow. `workflowEvent` is used to create each event, and a generic type specifies the data associated with each event. The events include `startEvent` (input topic), `jokeEvent` (intermediate joke), `critiqueEvent` (joke and critique), and `resultEvent` (final joke and critique).\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/workflows.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst startEvent = workflowEvent<string>(); // Input topic for joke\nconst jokeEvent = workflowEvent<{ joke: string }>(); // Intermediate joke\nconst critiqueEvent = workflowEvent<{ joke: string, critique: string }>(); // Intermediate critique\nconst resultEvent = workflowEvent<{ joke: string, critique: string }>(); // Final joke + critique\n```\n\n----------------------------------------\n\nTITLE: Creating a Joke Generation Workflow\nDESCRIPTION: This code demonstrates how to create a workflow that generates a joke, critiques it, and iterates on it using OpenAI's language models. It defines workflow events, sets up a workflow with a store, and adds handlers for each step of the process. The workflow uses `createWorkflow` from `@llama-flow/core` and `OpenAI` from `@llamaindex/openai`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/workflows.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@llamaindex/openai\";\nimport { createWorkflow, workflowEvent } from \"@llama-flow/core\";\nimport { withStore } from \"@llama-flow/core/middleware/store\";\n\n// Create LLM instance\nconst llm = new OpenAI({ model: \"gpt-4.1-mini\", apiKey: \"...\"});\n\n// Define our workflow events\nconst startEvent = workflowEvent<string>(); // Input topic for joke\nconst jokeEvent = workflowEvent<{ joke: string }>(); // Intermediate joke\nconst critiqueEvent = workflowEvent<{ joke: string, critique: string }>(); // Intermediate critique\nconst resultEvent = workflowEvent<{ joke: string, critique: string }>(); // Final joke + critique\n\n// Create our workflow\nconst jokeFlow = withStore(\n    () => ({\n        numIterations: 0,\n        maxIterations: 3,\n    }),\n    createWorkflow()\n);\n\n// Define handlers for each step\njokeFlow.handle([startEvent], async (event) => {\n  // Prompt the LLM to write a joke\n  const prompt = `Write your best joke about ${event.data}. Write the joke between <joke> and </joke> tags.`;\n  const response = await llm.complete({ prompt });\n\n  // Parse the joke from the response\n  const joke = response.text.match(/<joke>([\\s\\S]*?)<\\/joke>/)?.[1]?.trim() ?? response.text;\n  return jokeEvent.with({ joke: joke });\n});\n\njokeFlow.handle([jokeEvent], async (event) => {\n  // Prompt the LLM to critique the joke\n  const prompt = `Give a thorough critique of the following joke. If the joke needs improvement, put \\\"IMPROVE\\\" somewhere in the critique: ${event.data.joke}`;\n  const response = await llm.complete({ prompt });\n\n  // If the critique includes \"IMPROVE\", keep iterating, else, return the result\n  if (response.text.includes(\"IMPROVE\")) {\n    return critiqueEvent.with({ joke: event.data.joke, critique: response.text });\n  }\n\n  return resultEvent.with({ joke: event.data.joke, critique: response.text });\n});\n\njokeFlow.handle([critiqueEvent], async (event) => {\n  // Keep track of the number of iterations\n  const store = jokeFlow.getStore();\n  store.numIterations++;\n\n  // Write a new joke based on the previous joke and critique\n  const prompt = `Write a new joke based on the following critique and the original joke. Write the joke between <joke> and </joke> tags.\\n\\nJoke: ${event.data.joke}\\n\\nCritique: ${event.data.critique}`;\n  const response = await llm.complete({ prompt });\n  \n  // Parse the joke from the response\n  const joke = response.text.match(/<joke>([\\s\\S]*?)<\\/joke>/)?.[1]?.trim() ?? response.text;\n  \n  // If we've done less than the max number of iterations, keep iterating\n  // else, return the result\n  if (store.numIterations < store.maxIterations) {\n    return jokeEvent.with({ joke: joke });\n  }\n\n  return resultEvent.with({ joke: joke, critique: event.data.critique });\n});\n\n// Usage\nasync function main() {\n  const { stream, sendEvent } = jokeFlow.createContext();\n  sendEvent(startEvent.with(\"pirates\"));\n\n  let result: { joke: string, critique: string } | undefined;\n\n  for await (const event of stream) {\n    // console.log(event.data);  optionally log the event data\n    if (resultEvent.include(event)) {\n      result = event.data;\n      break; // Stop when we get the final result\n    }\n  }\n  \n  console.log(result);\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Executing ChromaDB Test\nDESCRIPTION: This command uses pnpm and tsx to execute the TypeScript test script for ChromaDB example. It assumes the user is inside the `examples` directory.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/chromadb/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npnpm dlx tsx chromadb/test.ts\n```\n\n----------------------------------------\n\nTITLE: Creating a Reranker Instance with Objects\nDESCRIPTION: This code creates a `MixedbreadAIReranker` instance configured for reranking objects (documents with fields). It specifies the API key, model, topK results, fields to rank, whether to return the input, and the number of retries.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/mixedbreadiai_reranker.mdx#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst reranker = new MixedbreadAIReranker({\n  apiKey: \"<MIXEDBREAD_API_KEY>\",\n  model: \"mixedbread-ai/mxbai-rerank-large-v1\",\n  topK: 5,\n  rankFields: [\"title\", \"content\"],\n  returnInput: true,\n  maxRetries: 5,\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Anthropic LLM in LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to initialize the Anthropic LLM within LlamaIndex. It sets the global LLM to an Anthropic instance, configured with your API key. Replace '<YOUR_API_KEY>' with your actual Anthropic API key.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/anthropic.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Settings } from \"llamaindex\";\nimport { Anthropic } from \"@llamaindex/anthropic\";\n\nSettings.llm = new Anthropic({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n```\n\n----------------------------------------\n\nTITLE: MarkdownNodeParser with MarkdownReader - TypeScript\nDESCRIPTION: This code demonstrates using the MarkdownNodeParser in conjunction with the MarkdownReader to load and parse markdown files. It reads a markdown file, then uses the MarkdownNodeParser to parse the documents into nodes.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/node-parser.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MarkdownNodeParser } from \"llamaindex\";\nimport { MarkdownReader } from '@llamaindex/readers/markdown'\n\nconst reader = new MarkdownReader();\nconst markdownNodeParser = new MarkdownNodeParser();\n\nconst documents = await reader.loadData('path/to/file.md');\nconst parsedDocuments = markdownNodeParser(documents);\n//      ^?\n```\n\n----------------------------------------\n\nTITLE: Creating the Agent - TypeScript\nDESCRIPTION: This creates an agent using the `agent` function from LlamaIndex, passing in the array of tools.  The agent is then ready to be used to answer questions using the defined tools.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/2_create_agent.mdx#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst myAgent = agent({ tools });\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key via Environment Variable (Shell)\nDESCRIPTION: Sets the OpenAI API key as an environment variable. This is required for LlamaIndex to authenticate with OpenAI.  Replace `your-api-key` with your actual API key.  It is important to avoid committing the API key to a git repository for security reasons.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/node.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport OPENAI_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Set the embedding model using HuggingFace\nDESCRIPTION: This code sets the embedding model to a HuggingFace model.  This model is used to encode the text into numerical embeddings.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/4_agentic_rag.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nSettings.embedModel = new HuggingFaceEmbedding({\n  modelType: \"BAAI/bge-small-en-v1.5\",\n  quantized: false,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Text Documents\nDESCRIPTION: This function creates `Document` objects from the text nodes extracted by LlamaParse. It maps the text content and page number from each node to a new `Document` object, using the page number as metadata.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/llama_parse/images.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nfunction getTextDocs(jsonList: { text: string; page: number }[]): Document[] {\n  return jsonList.map(\n    (page) => new Document({ text: page.text, metadata: { page: page.page } }),\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Run create-llama CLI Tool (npm)\nDESCRIPTION: This command executes the `create-llama` tool using `npx`. It will prompt the user with a series of questions to configure the generated application. The `@latest` tag ensures that the most recent version of the tool is used.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/create_llama.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx create-llama@latest\n```\n\n----------------------------------------\n\nTITLE: Import Modules for Correctness Evaluation\nDESCRIPTION: This code imports the necessary modules from the `@llamaindex/openai` and `llamaindex` packages. These modules include `OpenAI`, `CorrectnessEvaluator`, `Settings`, and `Response`, which are used to configure and run the correctness evaluation.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/correctness.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { OpenAI } from \"@llamaindex/openai\";\nimport { CorrectnessEvaluator, Settings, Response } from \"llamaindex\";\n```\n\n----------------------------------------\n\nTITLE: Running TypeScript Script with tsx\nDESCRIPTION: Shows how to run a TypeScript script in Node.js using the `tsx` package.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/typescript.mdx#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnode --import tsx ./my-script.ts\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Reranker Instance\nDESCRIPTION: This code demonstrates creating a `MixedbreadAIReranker` instance for a simple reranking process. The `apiKey` is required for authentication, and `topN` specifies the number of results to return.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/mixedbreadiai_reranker.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst reranker = new MixedbreadAIReranker({\n  apiKey: \"<MIXEDBREAD_API_KEY>\",\n  topN: 4,\n});\n```\n\n----------------------------------------\n\nTITLE: Loading data using SimpleDirectoryReader in TypeScript\nDESCRIPTION: This code snippet loads data from a specified directory using the `SimpleDirectoryReader`. It asynchronously loads documents from the `node_modules/llamaindex/examples` directory. The loaded documents will be used to create indices.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/router_query_engine.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst documents = await new SimpleDirectoryReader().loadData({\n  directoryPath: \"node_modules/llamaindex/examples\",\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Bedrock Model Constants\nDESCRIPTION: This snippet defines constants representing various Bedrock models, including Anthropic Claude and Meta Llama models.  These constants can be used when initializing the Bedrock LLM in LlamaIndex. Note the availability of some models is limited to certain AWS regions.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/bedrock.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nANTHROPIC_CLAUDE_INSTANT_1 = \"anthropic.claude-instant-v1\";\nANTHROPIC_CLAUDE_2 = \"anthropic.claude-v2\";\nANTHROPIC_CLAUDE_2_1 = \"anthropic.claude-v2:1\";\nANTHROPIC_CLAUDE_3_SONNET = \"anthropic.claude-3-sonnet-20240229-v1:0\";\nANTHROPIC_CLAUDE_3_HAIKU = \"anthropic.claude-3-haiku-20240307-v1:0\";\nANTHROPIC_CLAUDE_3_OPUS = \"anthropic.claude-3-opus-20240229-v1:0\"; // available on us-west-2\nANTHROPIC_CLAUDE_3_5_SONNET = \"anthropic.claude-3-5-sonnet-20240620-v1:0\";\nANTHROPIC_CLAUDE_3_5_HAIKU = \"anthropic.claude-3-5-haiku-20241022-v1:0\";\nMETA_LLAMA2_13B_CHAT = \"meta.llama2-13b-chat-v1\";\nMETA_LLAMA2_70B_CHAT = \"meta.llama2-70b-chat-v1\";\nMETA_LLAMA3_8B_INSTRUCT = \"meta.llama3-8b-instruct-v1:0\";\nMETA_LLAMA3_70B_INSTRUCT = \"meta.llama3-70b-instruct-v1:0\";\nMETA_LLAMA3_1_8B_INSTRUCT = \"meta.llama3-1-8b-instruct-v1:0\"; // available on us-west-2\nMETA_LLAMA3_1_70B_INSTRUCT = \"meta.llama3-1-70b-instruct-v1:0\"; // available on us-west-2\nMETA_LLAMA3_1_405B_INSTRUCT = \"meta.llama3-1-405b-instruct-v1:0\"; // available on us-west-2, tool calling supported\nMETA_LLAMA3_2_1B_INSTRUCT = \"meta.llama3-2-1b-instruct-v1:0\"; // only available via inference endpoints (see below)\nMETA_LLAMA3_2_3B_INSTRUCT = \"meta.llama3-2-3b-instruct-v1:0\"; // only available via inference endpoints (see below)\nMETA_LLAMA3_2_11B_INSTRUCT = \"meta.llama3-2-11b-instruct-v1:0\"; // only available via inference endpoints (see below), multimodal and function call supported\nMETA_LLAMA3_2_90B_INSTRUCT = \"meta.llama3-2-90b-instruct-v1:0\"; // only available via inference endpoints (see below), multimodal and function call supported\nAMAZON_NOVA_PREMIER_1 = \"amazon.nova-premier-v1:0\";\nAMAZON_NOVA_PRO_1 = \"amazon.nova-pro-v1:0\";\nAMAZON_NOVA_LITE_1 = \"amazon.nova-lite-v1:0\";\nAMAZON_NOVA_MICRO_1 = \"amazon.nova-micro-v1:0\";\n```\n\n----------------------------------------\n\nTITLE: Adding Claude 3.7 Sonnet Support in Javascript\nDESCRIPTION: This feature adds support for the Claude 3.7 Sonnet model within the LlamaIndex community package, enabling users to utilize this model for their specific use cases.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n- 1914b52: Added Claude 3.7 Sonnet support\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Packages\nDESCRIPTION: This command installs the necessary LlamaIndex packages, including the core library, cloud integrations, and the OpenAI integration. These packages are required to use LlamaParse, access cloud services, and interface with OpenAI models.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/llama_parse/images.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i llamaindex @llamaindex/cloud @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Adding Safety Setting Parameter for Gemini\nDESCRIPTION: This patch introduces a safety setting parameter for the Gemini model, allowing users to configure safety levels for generated content.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_5\n\n\n\n----------------------------------------\n\nTITLE: JSON Response Format\nDESCRIPTION: Illustrates the expected JSON structure returned by LlamaParse in JSON mode.  The top-level object contains keys like 'pages' (an array of page objects), 'job_metadata', 'job_id', and 'file_path'. The metadata provides information about the LlamaParse job execution.  This structure demonstrates the format of the data returned by `loadJson`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/llama_parse/json_mode.mdx#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"pages\": [\n        ..page objects..\n    ],\n    \"job_metadata\": {\n        \"credits_used\": int,\n        \"credits_max\": int,\n        \"job_credits_usage\": int,\n        \"job_pages\": int,\n        \"job_is_cache_hit\": boolean\n    },\n    \"job_id\": string ,\n    \"file_path\": string\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Async Main Function Wrapper - TypeScript\nDESCRIPTION: This code snippet demonstrates wrapping the main application logic within an async function, allowing the use of the `await` keyword for asynchronous operations.  The main function is then invoked, and any errors are caught and logged to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/2_create_agent.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Your imports go here\n\nasync function main() {\n  // the rest of your code goes here\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Mistral Integration\nDESCRIPTION: This command installs the necessary LlamaIndex and MistralAI packages using npm. It is a prerequisite for using the MistralAI integration with LlamaIndex.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/mistral.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/mistral\n```\n\n----------------------------------------\n\nTITLE: Install gpt-tokenizer for Performance Optimization (package-install)\nDESCRIPTION: Installs the `gpt-tokenizer` package, which provides a significant speedup (60x) for tokenization in LlamaIndex when using Node.js. It depends on npm or a compatible package manager. This optimization is only applicable for Node.js environments.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/node.mdx#_snippet_2\n\nLANGUAGE: package-install\nCODE:\n```\nnpm i gpt-tokenizer\n```\n\n----------------------------------------\n\nTITLE: Adding Factory Convenience for LLM Providers - Typescript\nDESCRIPTION: This patch adds a convenience factory for each LLM provider. This allows users to instantiate LLMs using shorthands like `openai` instead of `new OpenAI()`. It simplifies LLM initialization in the LlamaIndex environment.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/portkey-ai/CHANGELOG.md#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\naea550a: Add factory convenience factory for each LLM provider, e.g. you can use openai instead of 'new OpenAI'\n```\n\n----------------------------------------\n\nTITLE: Setting LlamaCloud API Key\nDESCRIPTION: This bash command sets the `LLAMA_CLOUD_API_KEY` environment variable in the `.env` file. The API key is obtained by signing up for LlamaCloud and is used to authenticate requests to the LlamaParse service.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/6_llamaparse.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nLLAMA_CLOUD_API_KEY=llx-XXXXXXXXXXXXXXXX\n```\n\n----------------------------------------\n\nTITLE: ASCII Conversion Output\nDESCRIPTION: Example JSON output after ASCII conversion. It shows how non-ASCII characters are converted to their Unicode escape sequences.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/json.mdx#_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n\"message\": \"\\u3053\\u3093\\u306b\\u3061\\u306f\\u4e16\\u754c\"\n```\n\n----------------------------------------\n\nTITLE: Tool Call Result: Summing the Budgets in JavaScript\nDESCRIPTION: This code snippet shows the tool call result when the agent uses the `sumNumbers` tool to add the community health and public protection budgets. It demonstrates how the agent invokes the function tool with the appropriate `input` (a and b representing the budgets) and displays the `output`, which is the sum of the two values.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/5_rag_and_tools.mdx#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  toolCall: {\n    id: 'call_SzG4yGUnLbv1T7IyaLAOqg3t',\n    name: 'sumNumbers',\n    input: { a: 3200, b: 2012.5 }\n  },\n  toolResult: {\n    tool: FunctionTool { _fn: [Function: sumNumbers], _metadata: [Object] },\n    input: { a: 3200, b: 2012.5 },\n    output: '5212.5',\n    isError: false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Multi-Agent Workflow with Task Handoff - Typescript\nDESCRIPTION: This code snippet demonstrates how to create a multi-agent workflow with task handoffs. It defines two agents: a weather agent and a joke-telling agent, where the joke-telling agent can hand off tasks to the weather agent. The workflow coordinates between agents to fulfill a request for a joke and the weather in a specific city. It depends on the `llamaindex`, `@llamaindex/openai` and `zod` libraries.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/agent_workflow.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { multiAgent, agent, tool } from \"llamaindex\";\nimport { openai } from \"@llamaindex/openai\";\nimport { z } from \"zod\";\n\n// Create a weather agent\nconst weatherAgent = agent({\n  name: \"WeatherAgent\",\n  description: \"Provides weather information for any city\",\n  tools: [\n    tool(\n      {\n        name: \"fetchWeather\",\n        description: \"Get weather information for a city\",\n        parameters: z.object({\n          city: z.string(),\n        }),\n        execute: ({ city }) => `The weather in ${city} is sunny`,\n      }\n    ),\n  ],\n  llm: openai({ model: \"gpt-4o-mini\" }),\n});\n\n// Create a joke-telling agent\nconst jokeAgent = agent({\n  name: \"JokeAgent\",\n  description: \"Tells jokes and funny stories\",\n  tools: [jokeTool], // Using the joke tool defined earlier\n  llm: openai({ model: \"gpt-4o-mini\" }),\n  canHandoffTo: [weatherAgent], // Can hand off to the weather agent\n});\n\n// Create the multi-agent workflow\nconst agents = multiAgent({\n  agents: [jokeAgent, weatherAgent],\n  rootAgent: jokeAgent, // Start with the joke agent\n});\n\n// Run the workflow\nconst result = await agents.run(\n  \"Give me a morning greeting with a joke and the weather in San Francisco\"\n);\n```\n\n----------------------------------------\n\nTITLE: Initialize Langtrace\nDESCRIPTION: This JavaScript code snippet initializes the Langtrace SDK with an API key.  Replace `<YOUR_API_KEY>` with the actual API key obtained from Langtrace. This initialization enables Langtrace to capture and report performance metrics of the LlamaIndex.TS application. The Langtrace SDK is imported and then initialized with the API key.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/integration/lang-trace.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport * as Langtrace from \"@langtrase/typescript-sdk\";\nLangtrace.init({ api_key: \"<YOUR_API_KEY>\" });\n```\n\n----------------------------------------\n\nTITLE: Configuring @llamaindex/autotool in Next.js\nDESCRIPTION: Integrate @llamaindex/autotool with Next.js using the `withNext` helper. This ensures that your tool functions are correctly processed during the build process.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/autotool/README.md#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { withNext } from \"@llamaindex/autotool/next\";\n\n/** @type {import('next').NextConfig} */\nconst nextConfig = {};\n\nexport default withNext(nextConfig);\n```\n\n----------------------------------------\n\nTITLE: Using Stream Utilities: Collect and Until\nDESCRIPTION: This code demonstrates how to use stream utilities `collect` and `until` to work with the asynchronous event flow. It creates a workflow context, triggers the initial event, and collects all events until a `resultEvent` is received. The final event in the collected array is the `resultEvent`, whose data is then output.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/workflows.mdx#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { collect } from \"@llama-flow/core/stream/consumer\";\nimport { until } from \"@llama-flow/core/stream/until\";\n\n// Create a workflow context and send the initial event\nconst { stream, sendEvent } = jokeFlow.createContext();\nsendEvent(startEvent.with(\"pirates\"));\n\n// Collect all events until we get a resultEvent\nconst allEvents = await collect(until(stream, resultEvent));\n\n// The last event will be the resultEvent\nconst finalEvent = allEvents[allEvents.length - 1];\nconsole.log(finalEvent.data); // Output the joke and critique\n```\n\n----------------------------------------\n\nTITLE: Run LlamaCloud Example with LlamaIndexTS\nDESCRIPTION: This command executes the LlamaCloud example, which demonstrates creating a LlamaCloud index from one document and streaming responses using Vercel's AI SDK. It requires a LlamaCloud API key and an embedding model set in environment variables.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/vercel/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx vercel/llamacloud.ts\n```\n\n----------------------------------------\n\nTITLE: Use Managed Index with LlamaCloud and Documents\nDESCRIPTION: This code snippet illustrates how to utilize a managed index in conjunction with a chat engine using LlamaCloud. It assumes the existence of pre-ingested documents within the LlamaCloud environment. It showcases retrieval and interaction with the managed index to answer questions and engage in conversational exchanges. LlamaCloud API Key needs to be set.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/data_index/managed.mdx#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { OpenAI } from \"openai\";\nimport { LlamaCloudIndex } from \"llamaindex\";\n\nasync function main() {\n  // Initialize OpenAI and LlamaCloud\n  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n\n  // NOTE: We assume you have already created an index using the\n  // \"Create a Managed Index\" example.\n  const index = await LlamaCloudIndex.fromIndexId({\n    indexId: \"YOUR_INDEX_ID\", // Replace with your index ID\n  });\n\n  // Query the index\n  const queryEngine = index.asQueryEngine();\n  const responseTxt = await queryEngine.query(\"What is the play about?\");\n  console.log(responseTxt);\n\n  // Chat with the index\n  const chatEngine = index.asChatEngine();\n  const responseChat = await chatEngine.chat(\n    \"Tell me more about the main characters.\"\n  );\n  console.log(responseChat);\n\n  // Chat with context\n  const responseChat2 = await chatEngine.chat(\n    \"What did I just ask you about?\",\n    [\n      {\n        content: \"What did I just ask you about?\",\n        role: \"user\",\n      },\n      {\n        content: \"Tell me more about the main characters.\",\n        role: \"assistant\",\n      },\n    ]\n  );\n  console.log(responseChat2);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Initialize OpenLLMetry SDK\nDESCRIPTION: Initializes the OpenLLMetry SDK in a Node.js environment. This snippet imports the OpenLLMetry SDK and initializes it with an API key (retrieved from environment variables) and disables batching for immediate tracing. The API key is crucial for authenticating with the OpenLLMetry platform.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/integration/open-llm-metry.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport * as traceloop from \"@traceloop/node-server-sdk\";\n\ntraceloop.initialize({\n  apiKey: process.env.TRACELOOP_API_KEY,\n  disableBatch: true\n});\n```\n\n----------------------------------------\n\nTITLE: Load Movie Reviews into AstraDB\nDESCRIPTION: This sample loads a dataset of movie reviews into AstraDB, mirroring the Astra Portal sample dataset. It demonstrates how to load data into the AstraDB vector store.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/astradb/README.md#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nrun `npx tsx astradb/load`\n```\n\n----------------------------------------\n\nTITLE: Parse Documents to Nodes with LlamaIndex JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates how to use LlamaIndex to parse a document into nodes.  It initializes a `SentenceSplitter`, creates a new `Document` with sample text, and then uses the splitter to divide the document into nodes. The `console.log` statement displays the resulting nodes.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/jupyter/nodeparser.ipynb#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst nodeParser = new SentenceSplitter();\nconst nodes = nodeParser.getNodesFromDocuments([\n    new Document({ text: \"I am 10 years old. John is 20 years old.\" }),\n]);\n\nconsole.log(nodes);\n```\n\n----------------------------------------\n\nTITLE: feat: add Amazon Bedrock Retriever in Javascript\nDESCRIPTION: This feature adds an Amazon Bedrock Retriever to LlamaIndex. This Retriever allows users to fetch data from Amazon Bedrock.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\n- 50e6b57: feat: add Amazon Bedrock Retriever\n```\n\n----------------------------------------\n\nTITLE: PromptTemplate Usage in TypeScript\nDESCRIPTION: Demonstrates how to use the PromptTemplate class from LlamaIndex with TypeScript, including type checking and auto-completion.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/typescript.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PromptTemplate } from 'llamaindex'\nconst promptTemplate = new PromptTemplate({\n  template: `Context information from multiple sources is below.\n---------------------\n{context}\n---------------------\nGiven the information from multiple sources and not prior knowledge.\nAnswer the query in the style of a Shakespeare play\"\nQuery: {query}\nAnswer:`, \n\ttemplateVars: [\"context\", \"query\"],\n});\n// @noErrors\npromptTemplate.format({\n\tc\n//^|\n})\n```\n\n----------------------------------------\n\nTITLE: Running LlamaCloud Chat Example (Shell)\nDESCRIPTION: This snippet shows how to run the `chat.ts` example, which uses a managed LlamaCloud index to create a chat engine. It assumes you have TypeScript set up and the necessary dependencies installed.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/cloud/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npnpx tsx cloud/chat.ts\n```\n\n----------------------------------------\n\nTITLE: Install OpenLLMetry Package\nDESCRIPTION: Installs the OpenLLMetry Node.js SDK using npm.  This command adds the `@traceloop/node-server-sdk` package to your project's dependencies, allowing you to instrument your LlamaIndex.TS application with OpenLLMetry.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/integration/open-llm-metry.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i @traceloop/node-server-sdk\n```\n\n----------------------------------------\n\nTITLE: Accessing file text via node import hook\nDESCRIPTION: Illustrates how to access the text from a CSV file using the Node.js Customization Hooks after it has been imported. The Node.js script is run with `--import @llamaindex/readers/node` to load the file, and then `getText()` will return the string content.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/index.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport csv from './path/to/data.csv';\n\nconst text = csv.getText()\n```\n\n----------------------------------------\n\nTITLE: Setting up Workflow with Store Middleware\nDESCRIPTION: This code shows how to set up a workflow using the `createWorkflow()` function, enhanced with the `withStore` middleware. The store provides shared state across all handlers, tracking `numIterations` and `maxIterations`. The store is accessible within workflows using the `jokeFlow.getStore()` function.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/workflows.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst jokeFlow = withStore(\n    () => ({\n        numIterations: 0,\n        maxIterations: 3,\n    }),\n    createWorkflow()\n);\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex and MixedbreadAI\nDESCRIPTION: This command installs the necessary dependencies, which are `llamaindex` and `@llamaindex/mixedbread`, using npm.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/mixedbreadai.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/mixedbread\n```\n\n----------------------------------------\n\nTITLE: Setup GPT-4\nDESCRIPTION: Configures the LlamaIndex Settings to use the GPT-4 model from OpenAI.  This typically enhances the quality of the query engine's responses due to the advanced capabilities of the GPT-4 model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/relevancy.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nSettings.llm = new OpenAI({\n  model: \"gpt-4\",\n});\n```\n\n----------------------------------------\n\nTITLE: Example Markdown Node Metadata - Bash\nDESCRIPTION: This is an example of the metadata produced by the MarkdownNodeParser. It shows how the markdown headers are extracted and stored as metadata for each TextNode.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/node-parser.mdx#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n[\n  TextNode {\n    id_: '008e41a8-b097-487c-bee8-bd88b9455844',\n    metadata: { 'Header 1': 'Main Header' },\n    excludedEmbedMetadataKeys: [],\n    excludedLlmMetadataKeys: [],\n    relationships: { PARENT: [Array] },\n    hash: 'KJ5e/um/RkHaNR6bonj9ormtZY7I8i4XBPVYHXv1A5M=',\n    text: 'Main Header\\nMain content',\n    textTemplate: '',\n    metadataSeparator: '\\n'\n  },\n  TextNode {\n    id_: '0f5679b3-ba63-4aff-aedc-830c4208d0b5',\n    metadata: { 'Header 1': 'Header 2' },\n    excludedEmbedMetadataKeys: [],\n    excludedLlmMetadataKeys: [],\n    relationships: { PARENT: [Array] },\n    hash: 'IP/g/dIld3DcbK+uHzDpyeZ9IdOXY4brxhOIe7wc488=',\n    text: 'Header 2\\nHeader 2 content',\n    textTemplate: '',\n    metadataSeparator: '\\n'\n  },\n  TextNode {\n    id_: 'e81e9bd0-121c-4ead-8ca7-1639d65fdf90',\n    metadata: { 'Header 1': 'Header 2', 'Header 2': 'Sub-header' },\n    excludedEmbedMetadataKeys: [],\n    excludedLlmMetadataKeys: [],\n    relationships: { PARENT: [Array] },\n    hash: 'B3kYNnxaYi9ghtAgwza0ZEVKF4MozobkNUlcekDL7JQ=',\n    text: 'Sub-header\\nSub-header content',\n    textTemplate: '',\n    metadataSeparator: '\\n'\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Task Cancellation with AbortController in TypeScript\nDESCRIPTION: Demonstrates how to use `AbortController` to cancel tasks within the agent system. The code creates an `AbortController`, passes its signal to the `createTask` method, and aborts the controller on a SIGINT signal.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/llamaindex/src/agent/README.md#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst controller = new AbortController();\nconst task = agent.createTask({ signal: controller.signal });\nprocess.on(\"SIGINT\", () => controller.abort());\nfor await (const taskOutput of task) {\n  // do something\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with npm\nDESCRIPTION: This command installs the necessary dependencies for using LlamaIndex with Vercel's AI SDK, specifically the `@llamaindex/vercel` package and the `ai` package. These packages provide the necessary functionalities for integrating LlamaIndex indexes with Vercel's AI SDK for building AI-powered applications.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/integration/vercel.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i @llamaindex/vercel ai\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex and Ollama Integration\nDESCRIPTION: This command installs the necessary npm packages: `llamaindex` and `@llamaindex/ollama`. The `@llamaindex/ollama` package provides the integration for using Ollama embeddings within LlamaIndex.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/ollama.mdx#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm i llamaindex @llamaindex/ollama\n```\n\n----------------------------------------\n\nTITLE: Streaming Query Responses in LlamaIndexTS\nDESCRIPTION: This snippet demonstrates how to stream query responses using the QueryEngine. By setting the `stream` option to `true`, the `query` function returns an asynchronous iterable that yields chunks of the response.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/index.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst stream = await queryEngine.query({ query: \"query string\", stream: true });\nfor await (const chunk of stream) {\n  process.stdout.write(chunk.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Agent Creation with Bedrock LLM and Function Tools\nDESCRIPTION: This example showcases the creation of an agent using the Bedrock LLM along with function tools.  It demonstrates how to define custom tools (sumNumbers, divideNumbers) using `FunctionTool` and integrates them into an `LLMAgent` for performing complex tasks. Zod is used for parameter validation of the function tools.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/bedrock.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BEDROCK_MODELS, Bedrock } from \"@llamaindex/community\";\nimport { FunctionTool, LLMAgent } from \"llamaindex\";\nimport { z } from \"zod\";\n\nconst sumNumbers = FunctionTool.from(\n  ({ a, b }: { a: number; b: number }) => `${a + b}`,\n  {\n    name: \"sumNumbers\",\n    description: \"Use this function to sum two numbers\",\n    parameters: z.object({\n      a: z.number({\n        description: \"The first number\",\n      }),\n      b: z.number({\n        description: \"The second number\",\n      }),\n    }),\n  },\n);\n\nconst divideNumbers = FunctionTool.from(\n  ({ a, b }: { a: number; b: number }) => `${a / b}`,\n  {\n    name: \"divideNumbers\",\n    description: \"Use this function to divide two numbers\",\n    parameters: z.object({\n      a: z.number({\n        description: \"The dividend a to divide\",\n      }),\n      b: z.number({\n        description: \"The divisor b to divide by\",\n      }),\n    }),\n  },\n);\n\nconst bedrock = new Bedrock({\n  model: BEDROCK_MODELS.META_LLAMA3_1_405B_INSTRUCT,\n  ...\n});\n\nasync function main() {\n  const agent = new LLMAgent({\n    llm: bedrock,\n    tools: [sumNumbers, divideNumbers],\n  });\n\n  const response = await agent.chat({\n    message: \"How much is 5 + 5? then divide by 2\",\n  });\n\n  console.log(response.message);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex dependencies\nDESCRIPTION: This command installs the necessary LlamaIndex packages, including core LlamaIndex, OpenAI integration, and reader utilities. It uses npm to add these packages to the project.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/router_query_engine.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i llamaindex @llamaindex/openai @llamaindex/readers\n```\n\n----------------------------------------\n\nTITLE: Configure Local Storage in LlamaIndex.TS\nDESCRIPTION: This snippet configures local storage for a LlamaIndex.TS application by specifying a `persistDir` within a `StorageContext`. It imports necessary modules from the `llamaindex` library and then creates a simple document and index, persisting the data to the specified directory. The code shows how to use the storage context to create an index from documents.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/index.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  Document,\n  VectorStoreIndex,\n  storageContextFromDefaults,\n} from \"llamaindex\";\n\nconst storageContext = await storageContextFromDefaults({\n  persistDir: \"./storage\",\n});\n\nconst document = new Document({ text: \"Test Text\" });\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  storageContext,\n});\n```\n\n----------------------------------------\n\nTITLE: Marking a function as a tool (Typescript)\nDESCRIPTION: Mark a function as a tool by adding the `\"use tool\"` directive at the top of the file or by using the `.tool.ts` extension. This is required for autotool to process the function.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/autotool/README.md#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n\"use tool\";\n\nexport function getWeather(city: string) {\n  // ...\n}\n// ...\n```\n\n----------------------------------------\n\nTITLE: Creating Documents Table in Supabase with pgvector\nDESCRIPTION: This SQL statement creates a table named `documents` in your Supabase database, designed to store document content, metadata, and embeddings.  It utilizes the `pgvector` extension to store vector embeddings of dimension 1536. Requires the `pgvector` extension to be enabled.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/supabase.mdx#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\ncreate table documents (\nid uuid primary key,\ncontent text,\nmetadata jsonb,\nembedding vector(1536)\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Chat UI with @llamaindex/chat-ui\nDESCRIPTION: Creates the `ChatSection` component using chat components from the `@llamaindex/chat-ui` library. This defines the structure and appearance of the chat interface within the Next.js RSC environment.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/rsc.mdx#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"file\": \"./src/components/demo/chat/rsc/chat-section.tsx\",\n\t\"codeblock\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Import LlamaIndex Modules\nDESCRIPTION: This imports the necessary modules from LlamaIndex and OpenAI, including OpenAI, Document, FaithfulnessEvaluator, Settings, and VectorStoreIndex. These modules are used to create documents, build a vector index, and evaluate response faithfulness.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/faithfulness.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@llamaindex/openai\";\nimport {\n  Document,\n  FaithfulnessEvaluator,\n  Settings,\n  VectorStoreIndex,\n} from \"llamaindex\";\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex DeepInfra Package\nDESCRIPTION: This command installs the LlamaIndex DeepInfra package and its dependency, llamaindex, using npm. This package allows you to use DeepInfra's LLMs within your LlamaIndex applications.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/deepinfra.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/deepinfra\n```\n\n----------------------------------------\n\nTITLE: Adding LLM factory convenience method\nDESCRIPTION: This code snippet shows the addition of factory convenience methods for each LLM provider. Instead of using `new OpenAI`, the user can directly use `openai`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/vllm/CHANGELOG.md#_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nAdd factory convenience factory for each LLM provider, e.g. you can use openai instead of 'new OpenAI'\n```\n\n----------------------------------------\n\nTITLE: Add missing inference endpoints for Haiku 3.5 in Javascript\nDESCRIPTION: This patch adds the missing inference endpoints for the Haiku 3.5 model within the LlamaIndex community package. This ensures that the model can be properly accessed and utilized for inference tasks.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\n- 487782c: Add missing inference endpoints for Haiku 3.5\n```\n\n----------------------------------------\n\nTITLE: Creating a Local Agent\nDESCRIPTION: This code snippet shows how to create a local agent using LlamaIndex. It imports the `agent` function and defines a workflow with a tool for getting weather information. The workflow is then run with a query about the weather in San Francisco.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/3_local_model.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { agent } from \"llamaindex\";\n\nconst workflow = agent({\n  tools: [getWeatherTool],\n});\n\nconst workflowContext = workflow.run(\n  \"What's the weather like in San Francisco?\",\n);\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing Documents in LlamaIndex\nDESCRIPTION: This snippet shows how to load a document and create a VectorStoreIndex from it in LlamaIndex.  It initializes a Document object with the given text and id, then creates an index from the document. Replace `essay` with your document's content.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/anthropic.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: Querying the Index in LlamaIndex\nDESCRIPTION: This code snippet shows how to query the created index. It obtains a `queryEngine` from the index and uses it to execute a query. The `query` variable contains the question to be asked.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/gemini.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Executing TypeScript code\nDESCRIPTION: Executes the `example.ts` file using `tsx`, a tool that allows running TypeScript files directly without pre-compilation.  Assumes `example.ts` exists and is configured correctly.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/structured_data_extraction.mdx#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nnpx tsx example.ts\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Server\nDESCRIPTION: Installs the LlamaIndex Server package using npm. This is a prerequisite for using the server.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/llamaindex-server.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @llamaindex/server\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Portkey Integration\nDESCRIPTION: This command installs the LlamaIndex core library and the Portkey integration package using npm.  These packages are required to use Portkey as an LLM within a LlamaIndex application.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/portkey.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/portkey-ai\n```\n\n----------------------------------------\n\nTITLE: Running Qdrant with Docker\nDESCRIPTION: This command pulls and runs a Qdrant instance using Docker. It exposes port 6333 for communication with the Qdrant database.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/qdrant.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull qdrant/qdrant\ndocker run -p 6333:6333 qdrant/qdrant\n```\n\n----------------------------------------\n\nTITLE: Querying the Index using LlamaIndex\nDESCRIPTION: This code demonstrates how to query the created index.  It initializes a query engine, defines a query, and executes the query against the index, returning the relevant results.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/ollama.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Import LlamaIndex modules in TypeScript\nDESCRIPTION: This code snippet imports the necessary modules from the LlamaIndex library, including tools for querying, indexing, and embedding.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/4_agentic_rag.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { QueryEngineTool, Settings, VectorStoreIndex } from \"llamaindex\";\nimport { OpenAI, OpenAIAgent } from \"@llamaindex/openai\";\nimport { HuggingFaceEmbedding } from \"@llamaindex/huggingface\";\nimport { SimpleDirectoryReader } from \"@llamaindex/readers/directory\";\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex and OpenAI Packages\nDESCRIPTION: This command installs the necessary packages: `llamaindex` and `@llamaindex/openai`. These packages are required for using LlamaIndex with OpenAI for document processing and querying.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/azure.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Install Chat UI via NPM\nDESCRIPTION: This command installs the @llamaindex/chat-ui package using npm, allowing you to manually integrate the chat interface into your project.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/install.mdx#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nnpm i @llamaindex/chat-ui\n```\n\n----------------------------------------\n\nTITLE: Create Managed Index with LlamaCloud\nDESCRIPTION: This code snippet demonstrates how to create a managed index by ingesting documents using LlamaCloud's services. It showcases the use of the LlamaCloud API to handle parsing and document management, allowing for efficient context augmentation in LLM and RAG applications. The snippet requires a valid LlamaCloud API key for authentication.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/data_index/managed.mdx#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { OpenAI } from \"openai\";\nimport { LlamaCloudIndex } from \"llamaindex\";\nimport fs from \"fs/promises\";\n\nasync function main() {\n  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n\n  // Load documents from file\n  const shakespeare_uri =\n    \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/Shakespeare.txt\";\n\n  const response = await fetch(shakespeare_uri);\n  const shakespeare_data = await response.text();\n  await fs.writeFile(\"shakespeare.txt\", shakespeare_data);\n  const shakespeare_doc = await fs.readFile(\"shakespeare.txt\", \"utf-8\");\n\n  // Create LlamaCloud Index\n  // Make sure to set the LLAMA_CLOUD_API_KEY environment variable.\n  const index = await LlamaCloudIndex.fromDocuments([\n    {\n      id_: \"shakespeare-doc\",\n      text: shakespeare_doc,\n    },\n  ]);\n\n  // Query the index\n  const queryEngine = index.asQueryEngine();\n  const responseTxt = await queryEngine.query(\n    \"What is the play about in detail?\"\n  );\n  console.log(responseTxt);\n\n  // Chat with the index\n  const chatEngine = index.asChatEngine();\n  const responseChat = await chatEngine.chat(\n    \"Tell me more about the main characters.\"\n  );\n  console.log(responseChat);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Initializing a TypeScript project with npm\nDESCRIPTION: This code snippet initializes a new npm project and installs TypeScript and its type definitions for Node.js as development dependencies. This is a common first step when creating a TypeScript project using npm.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/rag/index.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init\nnpm i -D typescript @types/node\n```\n\n----------------------------------------\n\nTITLE: JSON Lines Format Input\nDESCRIPTION: Example JSON Lines formatted input with two JSON objects separated by newlines.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/json.mdx#_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\"tweet\": \"Hello world\"}\\n{\"tweet\": \"\"}\n```\n\n----------------------------------------\n\nTITLE: Importing Modules for LlamaIndex and Mixedbread AI\nDESCRIPTION: This imports the required modules from LlamaIndex, OpenAI, and Mixedbread AI. These modules provide the classes and functions needed to create documents, indexes, language models, and the Mixedbread AI reranker.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/mixedbreadiai_reranker.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  Document,\n  VectorStoreIndex,\n  Settings,\n} from \"llamaindex\";\nimport { OpenAI } from \"@llamaindex/openai\";\nimport { MixedbreadAIReranker } from \"@llamaindex/mixedbread\";\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex and OpenAI\nDESCRIPTION: This command installs the LlamaIndex core library and the OpenAI integration package using npm. These packages are necessary to use OpenAI models with LlamaIndex.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Running Vercel Example\nDESCRIPTION: This command executes the `vercel.ts` script using `tsx`. The script likely utilizes Vercel Postgres for database operations. The provided link directs to the Vercel Postgres documentation for additional setup details.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/vector-store/pg/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx vector-store/pg/vercel.ts\n```\n\n----------------------------------------\n\nTITLE: Import Tweets into MongoDB\nDESCRIPTION: This snippet shows the command to import the tinytweets.json dataset into MongoDB using tsx, a TypeScript execution tool.  This step populates the MongoDB database with the tweet data that will be used for indexing and querying.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/mongodb/README.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nnpx tsx mongodb/1_import.ts\n```\n\n----------------------------------------\n\nTITLE: Install dependencies for LlamaIndexTS and Vercel examples\nDESCRIPTION: This command installs the required dependencies to run the LlamaIndexTS and Vercel integration examples. It needs to be executed from the parent directory of the `vercel` directory.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/vercel/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in .env File\nDESCRIPTION: This snippet shows how to set the OpenAI API key in a `.env` file.  The key is required for running the example script. The `.env` file should be located in the parent directory of the `examples` folder.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/qdrantdb/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nOPEN_API_KEY=sk-you-key\n```\n\n----------------------------------------\n\nTITLE: Update Data Loader Import (v0.9)\nDESCRIPTION: Illustrates the updated import statement for data loaders in LlamaIndex.TS v0.9. SimpleDirectoryReader is now imported from the '@llamaindex/readers/directory' package. Requires updating the import statement in all locations where it is used.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/migration/0.8-to-0.9.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SimpleDirectoryReader } from \"@llamaindex/readers/directory\";\n```\n\n----------------------------------------\n\nTITLE: Multimodal Index Creation\nDESCRIPTION: This example illustrates how to create a multimodal index that combines text and image data. It loads a PDF using LlamaParse, extracts both text and image nodes, uses a multimodal LLM to generate alt text for the images, and then creates a VectorStoreIndex from the combined data.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/llama_parse/images.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, ImageNode, VectorStoreIndex } from \"llamaindex\";\nimport { LlamaParseReader } from \"@llamaindex/cloud\";\nimport { OpenAI } from \"@llamaindex/openai\";\nimport { createMessageContent } from \"llamaindex\";\n\nconst reader = new LlamaParseReader();\nasync function main() {\n  // Load PDF using LlamaParse JSON mode and return an array of json objects\n  const jsonObjs = await reader.loadJson(\"../data/uber_10q_march_2022.pdf\");\n  // Access the first \"pages\" (=a single parsed file) object in the array\n  const jsonList = jsonObjs[0][\"pages\"];\n\n  const textDocs = getTextDocs(jsonList);\n  const imageTextDocs = await getImageTextDocs(jsonObjs);\n  const documents = [...textDocs, ...imageTextDocs];\n  // Split text, create embeddings and query the index\n  const index = await VectorStoreIndex.fromDocuments(documents);\n  const queryEngine = index.asQueryEngine();\n  const response = await queryEngine.query({\n    query:\n      \"What does the bar graph titled 'Monthly Active Platform Consumers' show?\",\n  });\n\n  console.log(response.toString());\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Initializing LlamaDeuce LLM\nDESCRIPTION: This snippet demonstrates how to initialize the LlamaDeuce LLM with a specified chat strategy. It imports necessary modules from LlamaIndex and the Replicate integration, then sets the global LLM to a new LlamaDeuce instance using the META chat strategy.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/llama2.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LlamaDeuce, DeuceChatStrategy } from \"@llamaindex/replicate\";\nimport { Document, VectorStoreIndex, Settings } from \"llamaindex\";\n\nSettings.llm = new LlamaDeuce({ chatStrategy: DeuceChatStrategy.META });\n```\n\n----------------------------------------\n\nTITLE: Cloudflare Worker Fetch Handler\nDESCRIPTION: This code snippet demonstrates a basic Cloudflare Worker fetch handler that imports and uses LlamaIndex. It initializes the environment variables and imports the OpenAIAgent. The example returns a simple 'Hello, world!' response. The code depends on `@llamaindex/env` and `@llamaindex/openai`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/cloudflare.mdx#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    const { setEnvs } = await import(\"@llamaindex/env\");\n    setEnvs(env);\n    const { OpenAIAgent } = await import(\"@llamaindex/openai\");\n    // Start your code here\n    return new Response(\"Hello, world!\");\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Defining a UI Event Data Schema\nDESCRIPTION: Defines a Zod schema for the data object within a UI event, including descriptions for AI-generated UI components. The schema specifies the stage of the workflow and its progress.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/llamaindex-server.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst MyEventDataSchema = z.object({\n  stage: z.enum([\"retrieve\", \"analyze\", \"answer\"]).describe(\"The current stage the workflow process is in.\"),\n  progress: z.number().min(0).max(1).describe(\"The progress in percent of the current stage\"),\n}).describe(\"WorkflowStageProgress\");\n\ntype UIEventData = z.infer<typeof MyEventDataSchema>;\n```\n\n----------------------------------------\n\nTITLE: Generating a UI Component\nDESCRIPTION: Generates a custom UI component using an LLM (OpenAI in this case) based on the provided JSON schema. It uses the generateEventComponent function from @llamaindex/server.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/llamaindex-server.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"llamaindex\";\nimport { generateEventComponent } from \"@llamaindex/server\";\nimport { MyEventDataSchema } from \"./your-workflow\";\n\n// Also works well with Claude 3.5 Sonnet and Google Gemini 2.5 Pro\nconst llm = new OpenAI({ model: \"gpt-4.1\" });\nconst code = generateEventComponent(MyEventDataSchema, llm);\n```\n\n----------------------------------------\n\nTITLE: Install Chat UI via Shadcn CLI\nDESCRIPTION: This command uses the Shadcn CLI to quickly add the chat interface to your project. It retrieves the configuration from a remote JSON file.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/install.mdx#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nnpx shadcn@latest add https://ui.llamaindex.ai/r/chat.json\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex and OpenAI\nDESCRIPTION: This command installs the necessary packages: `llamaindex` and `@llamaindex/openai`. These packages are required for using the CorrectnessEvaluator.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/correctness.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Handler: Generating an Improved Joke Based on Critique\nDESCRIPTION: This handler processes the `critiqueEvent`, generates an improved joke using OpenAI's LLM based on the critique and the original joke, and either loops back to the joke evaluation (if under the iteration limit) or emits the final `resultEvent` (if the iteration limit is reached). It uses the workflow's store to track the number of iterations.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/workflows.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\njokeFlow.handle([critiqueEvent], async (event) => {\n  // Keep track of the number of iterations\n  const store = jokeFlow.getStore();\n  store.numIterations++;\n\n  // Write a new joke based on the previous joke and critique\n  const prompt = `Write a new joke based on the following critique and the original joke. Write the joke between <joke> and </joke> tags.\\n\\nJoke: ${event.data.joke}\\n\\nCritique: ${event.data.critique}`;\n  const response = await llm.complete({ prompt });\n  \n  // Parse the joke from the response\n  const joke = response.text.match(/<joke>([\\s\\S]*?)<\\/joke>/)?.[1]?.trim() ?? response.text;\n  \n  // If we've done less than the max number of iterations, keep iterating\n  // else, return the result\n  if (store.numIterations < store.maxIterations) {\n    return jokeEvent.with({ joke: joke });\n  }\n\n  return resultEvent.with({ joke: joke, critique: event.data.critique });\n});\n```\n\n----------------------------------------\n\nTITLE: Extend LlamaParseReader for SimpleDirectoryReader compatibility\nDESCRIPTION: Creates a custom reader class extending `LlamaParseReader` to enable compatibility with `SimpleDirectoryReader`. The `loadData` method is overridden to call `loadJson`, extract the 'pages' array, and convert each page object into a LlamaIndex Document with the page number as metadata. This allows integrating LlamaParse's JSON mode with `SimpleDirectoryReader` and other LlamaIndex functionalities.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/llama_parse/json_mode.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document } from \"llamaindex\";\nimport { LlamaParseReader } from \"@llamaindex/cloud\";\n\nclass LlamaParseReaderWithJson extends LlamaParseReader {\n  // Override the loadData method\n  override async loadData(filePath: string): Promise<Document[]> {\n    // Call loadJson method that was inherited by LlamaParseReader\n    const jsonObjs = await super.loadJson(filePath);\n    let documents: Document[] = [];\n\n    jsonObjs.forEach((jsonObj) => {\n      // Making sure it's an array before iterating over it\n      if (Array.isArray(jsonObj.pages)) {\n      }\n      const docs = jsonObj.pages.map(\n        (page: { text: string; page: number }) =>\n          new Document({ text: page.text, metadata: { page: page.page } }),\n      );\n      documents = documents.concat(docs);\n    });\n    return documents;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Full Example: TogetherLLM Integration\nDESCRIPTION: This complete example demonstrates the entire process of using TogetherLLM with LlamaIndex, from initializing the LLM and loading documents to creating an index and querying it. It showcases a typical workflow for integrating TogetherLLM into a LlamaIndex application, including the main function, document loading, indexing, query engine setup, querying, and response logging.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/together.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TogetherLLM } from \"@llamaindex/together\";\nimport { Document, Settings, VectorStoreIndex } from \"llamaindex\";\n\nSettings.llm = new TogetherLLM({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n\nasync function main() {\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Evaluate Response Correctness\nDESCRIPTION: This code defines a query and a response, then uses the `CorrectnessEvaluator` to evaluate the correctness of the response in relation to the query. It creates a `CorrectnessEvaluator` instance, evaluates the response, and logs the result, indicating whether the response is correct and its corresponding score.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/correctness.mdx#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst query =\n  \"Can you explain the theory of relativity proposed by Albert Einstein in detail?\";\n\nconst response = ` Certainly! Albert Einstein's theory of relativity consists of two main components: special relativity and general relativity. Special relativity, published in 1905, introduced the concept that the laws of physics are the same for all non-accelerating observers and that the speed of light in a vacuum is a constant, regardless of the motion of the source or observer. It also gave rise to the famous equation E=mc, which relates energy (E) and mass (m).\n\nHowever, general relativity, published in 1915, extended these ideas to include the effects of magnetism. According to general relativity, gravity is not a force between masses but rather the result of the warping of space and time by magnetic fields generated by massive objects. Massive objects, such as planets and stars, create magnetic fields that cause a curvature in spacetime, and smaller objects follow curved paths in response to this magnetic curvature. This concept is often illustrated using the analogy of a heavy ball placed on a rubber sheet with magnets underneath, causing it to create a depression that other objects (representing smaller masses) naturally move towards due to magnetic attraction.\n`;\n\nconst evaluator = new CorrectnessEvaluator();\n\nconst result = await evaluator.evaluateResponse({\n  query,\n  response: new Response(response),\n});\n\nconsole.log(\n  `the response is ${result.passing ? \"correct\" : \"not correct\"} with a score of ${result.score}`,\n);\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI provider package via yarn\nDESCRIPTION: This command installs the @llamaindex/openai package using yarn. This package provides the necessary integration to use OpenAI's LLMs with LlamaIndex.TS. It needs to be installed in addition to the core LlamaIndex package.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/README.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nyarn add @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Defining a UI Event Class\nDESCRIPTION: Defines a UIEvent class that extends WorkflowEvent, specifying the structure for custom UI events. These events include a type ('ui_event') and a data object.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/llamaindex-server.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nclass UIEvent extends WorkflowEvent<{ \n  type: \"ui_event\";\n  data: UIEventData;\n}> {}\n```\n\n----------------------------------------\n\nTITLE: Adding Llama 3.3 70B Instruct Support in Javascript\nDESCRIPTION: This feature adds support for the Llama 3.3 70B Instruct model within the LlamaIndex community package, allowing users to leverage this powerful model for their applications.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n- e28c29d: Added Llama 3.3 70B Instruct support\n```\n\n----------------------------------------\n\nTITLE: Adding LLM Provider Factory\nDESCRIPTION: Adds a convenience factory for each LLM provider, allowing users to instantiate LLMs using a shorthand like `openai` instead of `new OpenAI()`. This simplifies LLM initialization and improves code readability.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/replicate/CHANGELOG.md#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\naea550a: Add factory convenience factory for each LLM provider, e.g. you can use openai instead of 'new OpenAI'\n```\n\n----------------------------------------\n\nTITLE: Using SimilarityPostprocessor and CohereRerank in LlamaIndex (TS)\nDESCRIPTION: This code demonstrates how to use SimilarityPostprocessor to filter nodes based on similarity score and CohereRerank to rerank nodes based on a query.  It initializes nodes with scores, then applies the postprocessors and logs the results.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/index.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CohereRerank } from \"@llamaindex/cohere\";\nimport { Node, NodeWithScore, SimilarityPostprocessor, TextNode } from \"llamaindex\";\n\nconst nodes: NodeWithScore[] = [\n  {\n    node: new TextNode({ text: \"hello world\" }),\n    score: 0.8,\n  },\n  {\n    node: new TextNode({ text: \"LlamaIndex is the best\" }),\n    score: 0.6,\n  },\n];\n\n// similarity postprocessor: filter nodes below 0.75 similarity score\nconst processor = new SimilarityPostprocessor({\n  similarityCutoff: 0.7,\n});\n\nconst filteredNodes = await processor.postprocessNodes(nodes);\n\n// cohere rerank: rerank nodes given query using trained model\nconst reranker = new CohereRerank({\n  apiKey: \"<COHERE_API_KEY>\",\n  topN: 2,\n});\n\nconst rerankedNodes = await reranker.postprocessNodes(nodes, \"<user_query>\");\n\nconsole.log(filteredNodes, rerankedNodes);\n```\n\n----------------------------------------\n\nTITLE: Install OpenAI Package\nDESCRIPTION: This command installs the necessary npm packages: `llamaindex` and `@llamaindex/openai`. The `@llamaindex/openai` package provides the OpenAI embedding integration for LlamaIndex.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/openai.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Import LlamaIndex Modules JavaScript\nDESCRIPTION: This code snippet imports necessary modules from the LlamaIndex library, specifically the `Document` and `SentenceSplitter` classes. These modules are used for creating and processing text documents within the LlamaIndex framework. The import statement assumes that LlamaIndex is installed via npm.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/jupyter/nodeparser.ipynb#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { Document, SentenceSplitter } from \"npm:llamaindex\";\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Qdrant Integration\nDESCRIPTION: This command installs the necessary npm packages for using LlamaIndex with Qdrant.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/qdrant.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/qdrant\n```\n\n----------------------------------------\n\nTITLE: TypeScript configuration file (tsconfig.json)\nDESCRIPTION: This is a standard TypeScript configuration file (tsconfig.json) that specifies the compiler options for a TypeScript project. It dictates how TypeScript files should be compiled and defines settings such as target ECMAScript version, module system, and source map generation.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/rag/index.mdx#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n<include cwd>../../examples/tsconfig.json</include>\n```\n\n----------------------------------------\n\nTITLE: Tool Call Result: Querying Public Health Budget in JavaScript\nDESCRIPTION: This code snippet displays the structure of a tool call result from the RAG agent. It shows the `toolCall` object, including the `id`, `name` of the tool called (`san_francisco_budget_tool`), and the `input` query. It also shows the `toolResult` object, containing the `tool` details, the original `input` query, and the `output` containing the retrieved budget information.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/5_rag_and_tools.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  toolCall: {\n    id: 'call_ZA1LPx03gO4ABre1r6XowLWq',\n    name: 'san_francisco_budget_tool',\n    input: { query: 'community health budget 2023-2024' }\n  },\n  toolResult: {\n    tool: QueryEngineTool {\n      queryEngine: [RetrieverQueryEngine],\n      metadata: [Object]\n    },\n    input: { query: 'community health budget 2023-2024' },\n    output: 'The proposed Fiscal Year (FY) 2023-24 budget for the Department of Public Health is $3.2 billion\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Image Documents with LLM\nDESCRIPTION: This asynchronous function generates document objects from the image data.  It iterates through the image dictionaries, creates `ImageNode` objects, and uses a multimodal LLM (OpenAI) to generate alt text descriptions for each image.  The alt text becomes the document's text content, and the image path is stored as metadata.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/llama_parse/images.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nasync function getImageTextDocs(\n  jsonObjs: Record<string, any>[],\n): Promise<Document[]> {\n  const llm = new OpenAI({\n    model: \"gpt-4o\",\n    temperature: 0.2,\n    maxTokens: 1000,\n  });\n  const imageDicts = await reader.getImages(jsonObjs, \"images\");\n  const imageDocs = [];\n\n  for (const imageDict of imageDicts) {\n    const imageDoc = new ImageNode({ image: imageDict.path });\n    const prompt = () => `Describe the image as alt text`;\n    const message = await createMessageContent(prompt, [imageDoc]);\n\n    const response = await llm.complete({\n      prompt: message,\n    });\n\n    const doc = new Document({\n      text: response.text,\n      metadata: { path: imageDict.path } }),\n    imageDocs.push(doc);\n  }\n\n  return imageDocs;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Naive Caching Mechanism\nDESCRIPTION: This JavaScript code implements a caching mechanism to avoid re-parsing PDF files. It checks for the existence of a cache file, loads it if it exists, and then parses only the files that are not already in the cache. Finally, it updates and saves the cache file.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/7_qdrant.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// load cache.json and parse it\nlet cache = {};\nlet cacheExists = false;\ntry {\n  await fs.access(PARSING_CACHE, fs.constants.F_OK);\n  cacheExists = true;\n} catch (e) {\n  console.log(\"No cache found\");\n}\nif (cacheExists) {\n  cache = JSON.parse(await fs.readFile(PARSING_CACHE, \"utf-8\"));\n}\n\nconst filesToParse = [\"../data/sf_budget_2023_2024.pdf\"];\n\n// load our data, reading only files we haven't seen before\nlet documents = [];\nconst reader = new LlamaParseReader({ resultType: \"markdown\" });\nfor (let file of filesToParse) {\n  if (!cache[file]) {\n    documents = documents.concat(await reader.loadData(file));\n    cache[file] = true;\n  }\n}\n\n// write the cache back to disk\nawait fs.writeFile(PARSING_CACHE, JSON.stringify(cache));\n```\n\n----------------------------------------\n\nTITLE: tsconfig.json Module Resolution Configuration\nDESCRIPTION: Shows the required configuration in `tsconfig.json` to set the `moduleResolution` option to `bundler` for proper LlamaIndex usage. Other valid options are also shown.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/typescript.mdx#_snippet_1\n\nLANGUAGE: json5\nCODE:\n```\n{\n  compilerOptions: {\n    //  add this line to your tsconfig.json\n    moduleResolution: \"bundler\", // or \"nodenext\" | \"node16\" | \"node\"\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaFlow Core and OpenAI\nDESCRIPTION: This command installs the necessary packages for using LlamaFlow: `@llama-flow/core` for workflow management and `@llamaindex/openai` for interacting with OpenAI's models. This is a prerequisite for creating and running workflows that use OpenAI's language models.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/workflows.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i @llama-flow/core @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Example agent output\nDESCRIPTION: This is example output from the agent after running a query. The output shows the tool call details and the result from querying the data. The LLM has trimmed the output to only include information relevant to the year queried.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/4_agentic_rag.mdx#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  toolCall: {\n    id: 'call_iNo6rTK4pOpOBbO8FanfWLI9',\n    name: 'san_francisco_budget_tool',\n    input: { query: 'total budget' }\n  },\n  toolResult: {\n    tool: QueryEngineTool {\n      queryEngine: [RetrieverQueryEngine],\n      metadata: [Object]\n    },\n    input: { query: 'total budget' },\n    output: 'The total budget for the City and County of San Francisco for Fiscal Year (FY) 2023-24 is $14.6 billion, which represents a $611.8 million, or 4.4 percent, increase over the FY 2022-23 budget. For FY 2024-25, the total budget is also projected to be $14.6 billion, reflecting a $40.5 million, or 0.3 percent, decrease from the FY 2023-24 proposed budget. This budget includes various expenditures across different departments and services, with significant allocations to public works, transportation, commerce, public protection, and health services.',\n    isError: false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Install HuggingFace package\nDESCRIPTION: This command installs the @llamaindex/huggingface package. This package is needed to utilize local embedding models from HuggingFace within LlamaIndex.TS.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/local_llm.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @llamaindex/huggingface\n```\n\n----------------------------------------\n\nTITLE: Importing LlamaIndex Modules\nDESCRIPTION: This snippet imports the necessary modules from the `llamaindex` library, including `Document`, `VectorStoreIndex`, and `storageContextFromDefaults`.  It also imports the ChromaDB vector store integration. These components are essential for creating and querying the index.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/metadata_filtering.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex, storageContextFromDefaults } from \"llamaindex\";\nimport { ChromaVectorStore } from \"@llamaindex/chroma\";\n\nconst collectionName = \"dog_colors\";\n```\n\n----------------------------------------\n\nTITLE: Import Readers from @llamaindex/readers\nDESCRIPTION: Imports reader classes for various file formats from the `@llamaindex/readers` package. These classes are used to load data from specific file types (CSV, DOCX, HTML, etc.) into LlamaIndex `Document` objects.  The imported readers are `CSVReader`, `DocxReader`, `HTMLReader`, `ImageReader`, `JSONReader`, `MarkdownReader`, `ObsidianReader`, `PDFReader`, and `TextFileReader`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/index.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CSVReader } from '@llamaindex/readers/csv';\nimport { DocxReader } from '@llamaindex/readers/docx';\nimport { HTMLReader } from '@llamaindex/readers/html';\nimport { ImageReader } from '@llamaindex/readers/image';\nimport { JSONReader } from '@llamaindex/readers/json';\nimport { MarkdownReader } from '@llamaindex/readers/markdown';\nimport { ObsidianReader } from '@llamaindex/readers/obsidian';\nimport { PDFReader } from '@llamaindex/readers/pdf';\nimport { TextFileReader } from '@llamaindex/readers/text';\n```\n\n----------------------------------------\n\nTITLE: MongoDB Database and Collection Environment Variables\nDESCRIPTION: This snippet shows how to set the MONGODB_DATABASE and MONGODB_COLLECTION environment variables in the .env file. These variables specify the database and collection where the tweet data will be stored.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/mongodb/README.md#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nMONGODB_DATABASE=tiny_tweets_db\nMONGODB_COLLECTION=tiny_tweets_collection\n```\n\n----------------------------------------\n\nTITLE: Build Packages\nDESCRIPTION: This command builds all packages within the LlamaIndex.TS monorepo. It ensures that the code is compiled and ready for testing or usage.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npnpm build\n```\n\n----------------------------------------\n\nTITLE: Adding Gemini 2.0 Pro Experimental\nDESCRIPTION: This patch adds the Gemini 2.0 Pro Experimental model, providing access to the latest experimental features and improvements.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_11\n\n\n\n----------------------------------------\n\nTITLE: Run Local Vector Store Example with LlamaIndexTS\nDESCRIPTION: This command runs the local vector store example, which demonstrates creating a vector store index from a document and using Vercel's AI SDK with LlamaIndex for streaming responses. It requires the `tsx` package to be installed.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/vercel/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx vercel/vector-store.ts\n```\n\n----------------------------------------\n\nTITLE: Run Vercel LLM Example with LlamaIndexTS\nDESCRIPTION: This command executes the Vercel LLM example, which demonstrates using the `VercelLLM` adapter with Vercel's OpenAI model provider. It requires the `tsx` package to be installed.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/vercel/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx vercel/llm.ts\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with Custom Parameters\nDESCRIPTION: This snippet demonstrates how to generate embeddings using the `MixedbreadAIEmbeddings` class with custom parameters such as API key, model, batch size, normalization, dimensions, and encoding format. It then defines example texts and generates embeddings for them using the `embedDocuments` method.  The custom parameters are used to control the embedding generation process. The encoding format configures the method of data representation; in this example, the encoding format is set to Binary.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/mixedbreadai.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst embeddings = new MixedbreadAIEmbeddings({\n  apiKey: \"<MIXEDBREAD_API_KEY>\",\n  model: \"mixedbread-ai/mxbai-embed-large-v1\",\n  batchSize: 64,\n  normalized: true,\n  dimensions: 512,\n  encodingFormat: MixedbreadAI.EncodingFormat.Binary,\n});\n```\n\n----------------------------------------\n\nTITLE: Full Example of Anthropic Integration with LlamaIndex\nDESCRIPTION: This is a complete example showcasing the integration of Anthropic LLM with LlamaIndex, including initialization, document indexing, querying, and logging the response. It initializes the Anthropic LLM, loads a document, creates an index, sets up a query engine, and performs a query.  Replace `essay` with your document's content and `<YOUR_API_KEY>` with your actual Anthropic API key.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/anthropic.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex, Settings } from \"llamaindex\";\nimport { Anthropic } from \"@llamaindex/anthropic\";\n\nSettings.llm = new Anthropic({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n\nasync function main() {\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Response Synthesizer LlamaIndex TypeScript\nDESCRIPTION: This code initializes a ResponseSynthesizer with the 'compact' response mode using the `getResponseSynthesizer` function from LlamaIndex. It also creates `NodeWithScore` objects for synthesizing an answer to a given query. Two text nodes containing age information are initialized and a query is executed.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/response_synthesizer.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { NodeWithScore, TextNode, getResponseSynthesizer, responseModeSchema } from \"llamaindex\";\n\n// you can also use responseModeSchema.Enum.refine, responseModeSchema.Enum.tree_summarize, responseModeSchema.Enum.multi_modal\n// or you can use the CompactAndRefine, Refine, TreeSummarize, or MultiModal classes directly\nconst responseSynthesizer = getResponseSynthesizer(responseModeSchema.Enum.compact);\n\nconst nodesWithScore: NodeWithScore[] = [\n  {\n    node: new TextNode({ text: \"I am 10 years old.\" }),\n    score: 1,\n  },\n  {\n    node: new TextNode({ text: \"John is 20 years old.\" }),\n    score: 0.5,\n  },\n];\n\nconst response = await responseSynthesizer.synthesize({\n  query: \"What age am I?\",\n  nodesWithScore,\n});\nconsole.log(response.response);\n```\n\n----------------------------------------\n\nTITLE: Full Example: DeepInfra Integration with LlamaIndex (TypeScript)\nDESCRIPTION: This comprehensive example demonstrates the complete process of integrating DeepInfra with LlamaIndex, including setting up the LLM, loading and indexing a document, creating a query engine, executing a query, and logging the response. It showcases how to use a custom LLM model and temperature.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/deepinfra.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DeepInfra } from \"@llamaindex/deepinfra\";\nimport { Document, VectorStoreIndex, Settings } from \"llamaindex\";\n\n// Use custom LLM\nconst model = \"meta-llama/Meta-Llama-3-8B-Instruct\";\nSettings.llm = new DeepInfra({ model, temperature: 0 });\n\nasync function main() {\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Install Project Dependencies\nDESCRIPTION: This command installs all project dependencies using pnpm. It must be run in the root directory of the LlamaIndex.TS repository.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Anthropic Integration\nDESCRIPTION: This command installs the necessary LlamaIndex and Anthropic packages using npm. These packages allow you to use Anthropic's models within your LlamaIndex application.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/anthropic.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i llamaindex @llamaindex/anthropic\n```\n\n----------------------------------------\n\nTITLE: Using VoyageAI embeddings in LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to use `VoyageAIEmbedding` with LlamaIndex to create an index from a document and query it. It initializes the embedding model, loads a document, builds a VectorStoreIndex, and performs a query.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/voyageai.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { VoyageAIEmbedding } from \"@llamaindex/voyage-ai\";\nimport { Document, Settings, VectorStoreIndex } from \"llamaindex\";\n\nSettings.embedModel = new VoyageAIEmbedding();\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: OpenAI API Key Environment Variable\nDESCRIPTION: This snippet shows how to set the OPENAI_API_KEY environment variable in the .env file. This key is required to authenticate with OpenAI and use their embedding service for indexing.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/mongodb/README.md#_snippet_3\n\nLANGUAGE: Text\nCODE:\n```\nOPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n----------------------------------------\n\nTITLE: ASCII Conversion Input\nDESCRIPTION: Example JSON input with non-ASCII characters.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/json.mdx#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \"message\": \"\" }\n```\n\n----------------------------------------\n\nTITLE: Using DeepSeek LLM for Chat Completion with LlamaIndex (TypeScript)\nDESCRIPTION: This example demonstrates how to use the DeepSeekLLM for chat completion within LlamaIndex. It initializes the DeepSeekLLM and then calls the `chat` method to send a list of messages. The `messages` parameter is an array of objects, each with a `role` and `content`. The `stream` parameter controls whether the response is streamed.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/deepseek.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex, Settings } from \"llamaindex\";\nimport { DeepSeekLLM } from \"@llamaindex/deepseek\";\n\nconst deepseekLlm = new DeepSeekLLM({\n  apiKey: \"<YOUR_API_KEY>\",\n  model: \"deepseek-coder\", // or \"deepseek-chat\"\n});\n\nasync function main() {\n  const response = await llm.deepseekLlm.chat({\n    messages: [\n      {\n        role: \"system\",\n        content: \"You are an AI assistant\",\n      },\n      {\n        role: \"user\",\n        content: \"Tell me about San Francisco\",\n      },\n    ],\n    stream: false,\n  });\n  console.log(response);\n}\n```\n\n----------------------------------------\n\nTITLE: Using Node Postprocessors in a Query Engine (TS)\nDESCRIPTION: This code snippet demonstrates how to use node postprocessors within a query engine in LlamaIndex. It initializes the OpenAI LLM, creates sample nodes, configures CohereRerank, and integrates the postprocessors into a query engine.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/index.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CohereRerank } from \"@llamaindex/cohere\";\nimport { OpenAI } from \"@llamaindex/openai\";\nimport { Node, NodeWithScore, SimilarityPostprocessor, Settings, TextNode } from \"llamaindex\";\n\n// Use OpenAI LLM\nSettings.llm = new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0.1 });\n\nconst nodes: NodeWithScore[] = [\n  {\n    node: new TextNode({ text: \"hello world\" }),\n    score: 0.8,\n  },\n  {\n    node: new TextNode({ text: \"LlamaIndex is the best\" }),\n    score: 0.6,\n  }\n];\n\n// cohere rerank: rerank nodes given query using trained model\nconst reranker = new CohereRerank({\n  apiKey: \"<COHERE_API_KEY>\",\n  topN: 2,\n});\n\nconst document = new Document({ text: \"essay\", id_: \"essay\" });\n\nconst queryEngine = index.asQueryEngine({\n  nodePostprocessors: [processor, reranker],\n});\n\n// all node post-processors will be applied during each query\nconst response = await queryEngine.query(\"<user_query>\");\n```\n\n----------------------------------------\n\nTITLE: Create and Query Vector Index\nDESCRIPTION: This code snippet creates a vector index from a list of documents, builds a query engine from the index, and then queries the engine. It uses a sample document containing information about New York City.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/faithfulness.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst documents = [\n  new Document({\n    text: `The city came under British control in 1664 and was renamed New York after King Charles II of England granted the lands to his brother, the Duke of York. The city was regained by the Dutch in July 1673 and was renamed New Orange for one year and three months; the city has been continuously named New York since November 1674. New York City was the capital of the United States from 1785 until 1790, and has been the largest U.S. city since 1790. The Statue of Liberty greeted millions of immigrants as they came to the U.S. by ship in the late 19th and early 20th centuries, and is a symbol of the U.S. and its ideals of liberty and peace. In the 21st century, New York City has emerged as a global node of creativity, entrepreneurship, and as a symbol of freedom and cultural diversity. The New York Times has won the most Pulitzer Prizes for journalism and remains the U.S. media's \"newspaper of record\". In 2019, New York City was voted the greatest city in the world in a survey of over 30,000 p...\\tPass`,\n  }),\n];\n\nconst vectorIndex = await VectorStoreIndex.fromDocuments(documents);\n\nconst queryEngine = vectorIndex.asQueryEngine();\n```\n\n----------------------------------------\n\nTITLE: Querying the Index\nDESCRIPTION: This code shows how to query the index using a query engine.  It retrieves a query engine from the index, defines a query string, and executes the query using queryEngine.query(). The results are stored in the `results` variable.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/llama2.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Configure next.js for PDF parsing\nDESCRIPTION: This snippet demonstrates how to configure Next.js to include `pdf2json` in the `serverComponentsExternalPackages` array. This configuration is necessary when you need to parse PDF files at runtime using the `pdf2json` library within a Next.js application.  This ensures the `pdf2json` package is available on the server side.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/llamaindex/CHANGELOG.md#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// next.config.js\n/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  experimental: {\n    serverComponentsExternalPackages: [\"pdf2json\"],\n  },\n};\n\nmodule.exports = nextConfig;\n```\n\n----------------------------------------\n\nTITLE: LlamaDeuce with Replicate Session\nDESCRIPTION: This code shows how to use LlamaDeuce with a Replicate session. It initializes a ReplicateSession with a Replicate API key and passes it to the LlamaDeuce constructor, enabling the use of Replicate's infrastructure for running the LLM.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/llama2.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Settings } from \"llamaindex\";\nimport { LlamaDeuce, DeuceChatStrategy, ReplicateSession } from \"@llamaindex/replicate\";\n\nconst replicateSession = new ReplicateSession({\n  replicateKey,\n});\n\nSettings.llm = new LlamaDeuce({\n  chatStrategy: DeuceChatStrategy.META,\n  replicateSession,\n});\n```\n\n----------------------------------------\n\nTITLE: Integrating with Hono\nDESCRIPTION: This code snippet showcases how to integrate LlamaIndex with the Hono web framework in a Cloudflare Worker environment. It defines bindings for environment variables like `OPENAI_API_KEY` and creates a route `/llm` that utilizes LlamaIndex after setting the environment variables. It imports Hono from \"hono\" and depends on `@llamaindex/env`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/cloudflare.mdx#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { Hono } from \"hono\";\n\ntype Bindings = {\n  OPENAI_API_KEY: string;\n};\n\nconst app = new Hono<{ Bindings: Bindings }>();\n\napp.post(\"/llm\", async (c) => {\n  const { setEnvs } = await import(\"@llamaindex/env\");\n  setEnvs(c.env);\n\n  // ...\n\n  return new Response('Hello, world!');\n})\n\nexport default {\n  fetch: app.fetch,\n};\n```\n\n----------------------------------------\n\nTITLE: Using SimpleDirectoryReader with LlamaParseReader\nDESCRIPTION: Demonstrates how to use `SimpleDirectoryReader` in a non-Node.js environment (like Vercel Edge) to load PDF files using `LlamaParseReader`.  This is necessary because `PDFReader` is not compatible with Edge runtimes and requires direct import by file path. It defines a function to load documents from a specified directory, using `LlamaParseReader` for PDF files.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/index.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SimpleDirectoryReader } from \"@llamaindex/readers/directory\";\nimport { LlamaParseReader } from \"@llamaindex/cloud\";\n\nexport const DATA_DIR = \"./data\";\n\nexport async function getDocuments() {\n  const reader = new SimpleDirectoryReader();\n  // Load PDFs using LlamaParseReader\n  return await reader.loadData({\n    directoryPath: DATA_DIR,\n    fileExtToReader: {\n      pdf: new LlamaParseReader({ resultType: \"markdown\" }),\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Bind Argument to FunctionTool (TypeScript)\nDESCRIPTION: This code snippet demonstrates how to bind an additional argument to a `FunctionTool` in LlamaIndex. The `bind` method creates a new instance of the tool with the bound argument. In this example, a `userToken` is bound to the `queryKnowledgeBase` tool, allowing it to access the knowledge base with the specified token.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/tool/index.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { agent, tool } from \"llamaindex\";\n\n// first arg is LLM input, second is bound arg\nconst queryKnowledgeBase = async ({ question }, { userToken }) => {\n  const response = await fetch(`https://knowledge-base.com?token=${userToken}&query=${question}`);\n  // ...\n};\n\n// define tool as usual\nconst kbTool = tool(queryKnowledgeBase, {\n  name: 'queryKnowledgeBase',\n  description: 'Query knowledge base',\n  parameters: z.object({\n    question: z.string({\n      description: 'The user question',\n    }),\n  }),\n});\n\n// create an agent\nconst additionalArg = { userToken: 'abcd1234' };\nconst workflow = agent({\n  tools: [kbTool.bind(additionalArg)],\n  // llm, systemPrompt etc\n})\n```\n\n----------------------------------------\n\nTITLE: Tool Call Result: Querying Public Protection Budget in JavaScript\nDESCRIPTION: This code snippet illustrates another tool call result, similar to the previous one. It shows the structure of the `toolCall` and `toolResult` when querying for the public protection budget in San Francisco. The `output` provides the budget amount, indicating the agent successfully retrieved the requested information.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/5_rag_and_tools.mdx#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  toolCall: {\n    id: 'call_oHu1KjEvA47ER6HYVfFIq9yp',\n    name: 'san_francisco_budget_tool',\n    input: { query: 'public protection budget 2023-2024' }\n  },\n  toolResult: {\n    tool: QueryEngineTool {\n      queryEngine: [RetrieverQueryEngine],\n      metadata: [Object]\n    },\n    input: { query: 'public protection budget 2023-2024' },\n    output: \"The budget for Public Protection in San Francisco for Fiscal Year (FY) 2023-24 is $2,012.5 million.\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Perform RAG with Local LLM and Embeddings\nDESCRIPTION: This TypeScript code demonstrates retrieval-augmented generation (RAG) using a local LLM and embedding model. It loads text from a file, creates a Document object, indexes it using VectorStoreIndex, and queries the index to generate a response. All processing happens locally without API calls.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/local_llm.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nasync function main() {\n  // Load essay from abramov.txt in Node\n  const path = \"node_modules/llamaindex/examples/abramov.txt\";\n\n  const essay = await fs.readFile(path, \"utf-8\");\n\n  // Create Document object with essay\n  const document = new Document({ text: essay, id_: path });\n\n  // Split text and create embeddings. Store them in a VectorStoreIndex\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // Query the index\n  const queryEngine = index.asQueryEngine();\n\n  const response = await queryEngine.query({\n    query: \"What did the author do in college?\",\n  });\n\n  // Output response\n  console.log(response.toString());\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Fixes consoleLogger is missing from `@llamaindex/env`\nDESCRIPTION: Fixes an issue where the `consoleLogger` was missing from the `@llamaindex/env` package. This ensures that logging functionality works as expected.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\n- df441e2: fix: consoleLogger is missing from `@llamaindex/env`\n```\n\n----------------------------------------\n\nTITLE: Setting DeepInfra API Key in Environment (Bash)\nDESCRIPTION: This command sets the DeepInfra API key as an environment variable named DEEPINFRA_API_TOKEN. This allows the LlamaIndex application to access the API key without hardcoding it in the code.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/deepinfra.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport DEEPINFRA_API_TOKEN=\"<YOUR_API_KEY>\"\n```\n\n----------------------------------------\n\nTITLE: Fixing Inline Data Handling for Google Studio\nDESCRIPTION: This patch fixes an issue where parts with only inline data were being ignored for Google Studio, ensuring complete data processing.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Full Example: OpenAI Integration with LlamaIndex in TypeScript\nDESCRIPTION: This full example demonstrates loading a document, indexing it, creating a query engine, and performing a query.  It includes setting the OpenAI model, creating the document and index, configuring a retriever and query engine, and logging the response. This example illustrates the complete process of using LlamaIndex with Azure OpenAI.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/azure.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex, Settings } from \"llamaindex\";\nimport { OpenAI } from \"@llamaindex/openai\";\n\nSettings.llm = new OpenAI({ model: \"gpt-4\", temperature: 0 });\n\nasync function main() {\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Initialize Response Synthesizer with Custom Prompt - Typescript\nDESCRIPTION: This snippet demonstrates how to initialize a `ResponseSynthesizer` with a custom `textQATemplate`. It shows the deprecated way of creating a `ResponseSynthesizer` and the current way of doing it with the `getResponseSynthesizer` function. It creates an index, then a query engine configured to use the specified response synthesizer. Finally, it executes a query using that engine.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/prompt/index.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Create an instance of Response Synthesizer\n\n// Deprecated usage:\nconst responseSynthesizer = new ResponseSynthesizer({\n  responseBuilder: new CompactAndRefine(undefined, newTextQaPrompt),\n});\n\n// Current usage:\nconst responseSynthesizer = getResponseSynthesizer('compact', {\n  textQATemplate: newTextQaPrompt\n})\n\n// Create index\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\n// Query the index\nconst queryEngine = index.asQueryEngine({ responseSynthesizer });\n\nconst response = await queryEngine.query({\n  query: \"What did the author do in college?\",\n});\n```\n\n----------------------------------------\n\nTITLE: Reverting Previous Release in Javascript\nDESCRIPTION: This patch reverts the previous release due to issues with CJS (CommonJS) compatibility within the LlamaIndex community package. This ensures stability and proper functioning of the package in various environments.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n- 1c908fd: Revert previous release (not working with CJS)\n```\n\n----------------------------------------\n\nTITLE: Adding support for Haiku 3.5 via Bedrock in Javascript\nDESCRIPTION: This feature adds support for the Haiku 3.5 model via Bedrock within the LlamaIndex community package. This allows users to leverage Haiku 3.5 through the Bedrock service.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\n- 47a7c3e: feat: added support for Haiku 3.5 via Bedrock\n```\n\n----------------------------------------\n\nTITLE: Running LlamaIndex Server\nDESCRIPTION: Executes the index.ts file using tsx to start the LlamaIndex Server. This command assumes tsx is installed and configured.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/llamaindex-server.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntsx index.ts\n```\n\n----------------------------------------\n\nTITLE: Customize Submodule Prompt - Typescript\nDESCRIPTION: This code illustrates how to customize a submodule's prompt using the `getPrompts` and `updatePrompt` methods. It retrieves the existing prompts from a query engine, then overrides the `textQATemplate` for the response synthesizer with the custom `newTextQaPrompt`. After updating the prompt, it performs a query using the modified engine.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/prompt/index.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// Create index\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\n// Query the index\nconst queryEngine = index.asQueryEngine();\n\n// Get a list of prompts for the query engine\nconst prompts = queryEngine.getPrompts();\n\n// output: { \"responseSynthesizer:textQATemplate\": defaultTextQaPrompt, \"responseSynthesizer:refineTemplate\": defaultRefineTemplatePrompt }\n\n// Now, we can override the default prompt\nqueryEngine.updatePrompt({\n  \"responseSynthesizer:textQATemplate\": newTextQaPrompt,\n});\n\nconst response = await queryEngine.query({\n  query: \"What did the author do in college?\",\n});\n```\n\n----------------------------------------\n\nTITLE: Run specific test (Shell)\nDESCRIPTION: This command executes a specific test within the E2E test suite using the `--test-name-pattern` flag.  The `--import tsx` flag enables TypeScript execution, and `--import ./mock-register.js` imports the mock register. The pattern \"agent\" is used to filter tests whose names include \"agent\".\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/e2e/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nnode --import tsx --import ./mock-register.js --test-name-pattern=agent --test ./node/basic.e2e.ts\n```\n\n----------------------------------------\n\nTITLE: TaskHandler Type Definition in TypeScript\nDESCRIPTION: Defines the TaskHandler type as a function that accepts a TaskStep and returns a Promise of TaskOutput. This function represents a handler for individual tasks within the agent system.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/llamaindex/src/agent/README.md#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ntype TaskHandler = (step: TaskStep) => Promise<TaskOutput>;\n```\n\n----------------------------------------\n\nTITLE: Update Storage Provider Import (v0.9)\nDESCRIPTION: Demonstrates the updated import statement for storage providers in LlamaIndex.TS v0.9. The PineconeVectorStore class is now imported from the '@llamaindex/pinecone' package, necessitating its separate installation.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/migration/0.8-to-0.9.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PineconeVectorStore } from \"@llamaindex/pinecone\";\n```\n\n----------------------------------------\n\nTITLE: Using Vercel AI's Model Providers with VercelLLM in TypeScript\nDESCRIPTION: This snippet demonstrates how to use Vercel AI's model providers as LLMs within LlamaIndex using the `VercelLLM` adapter. It initializes `VercelLLM` with a specified model (in this case, OpenAI's GPT-4o), then utilizes the `complete` method to generate a response based on the given prompt. The `stream` parameter controls whether the response is streamed or returned as a complete text.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/integration/vercel.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst llm = new VercelLLM({ model: openai(\"gpt-4o\") });\nconst result = await llm.complete({\n  prompt: \"What is the capital of France?\",\n  stream: false, // Set to true if you want streaming responses\n});\nconsole.log(result.text);\n```\n\n----------------------------------------\n\nTITLE: Defining Query Engine and Summing Tools in JavaScript\nDESCRIPTION: This code defines an array of tools for a RAG agent. It includes a tool for querying a San Francisco budget using a query engine and a tool for summing two numbers. The parameters for the summing tool are defined using Zod for type validation and description.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/5_rag_and_tools.mdx#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// define the query engine as a tool\nconst tools = [\n  index.queryTool({\n    metadata: {\n      name: \"san_francisco_budget_tool\",\n      description: `This tool can answer detailed questions about the individual components of the budget of San Francisco in 2023-2024.`,      \n    },\n    options: { similarityTopK: 10 },\n  }),\n  tool({\n    name: \"sumNumbers\",\n    description: \"Use this function to sum two numbers\",\n    parameters: z.object({\n      a: z.number({\n        description: \"First number to sum\",\n      }),\n      b: z.number({\n        description: \"Second number to sum\",\n      }),\n    }),\n    execute: ({ a, b }) => `${a + b}`,\n  }),\n];\n```\n\n----------------------------------------\n\nTITLE: Set OpenAI API Key\nDESCRIPTION: Sets the OpenAI API key as an environment variable. This is required for authenticating with the OpenAI service when using models like gpt-4 within LlamaIndex.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/relevancy.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Running test cases\nDESCRIPTION: Executes the test suite for the project using pnpm. This command runs all tests defined in the packages/core/src/tests directory.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/llamaindex/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npnpm run test\n```\n\n----------------------------------------\n\nTITLE: Installing a NPM package globally\nDESCRIPTION: Installs an NPM package globally across all packages and applications in the monorepo. The `-w` flag ensures that the package is installed at the workspace root.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/llamaindex/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npnpm add -w [NPM Package]\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Modules\nDESCRIPTION: This code snippet imports the required modules from the `@llamaindex/openai` and `llamaindex` libraries. These modules are used to interact with OpenAI's models, create documents, configure settings, build vector store indexes, and utilize the Jina AI Reranker.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/jinaai_reranker.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@llamaindex/openai\";\nimport { Document, Settings, VectorStoreIndex, JinaAIReranker } from \"llamaindex\";\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepInfra LLM in LlamaIndex (TypeScript)\nDESCRIPTION: This code snippet demonstrates how to initialize the DeepInfra LLM within LlamaIndex using TypeScript. It includes importing necessary modules, configuring the API key from environment variables, and setting the DeepInfra LLM as the default LLM using Settings.llm.  The API key can be set directly or through the DEEPINFRA_API_TOKEN environment variable. It also shows how to set the API key directly.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/deepinfra.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DeepInfra } from \"@llamaindex/deepinfra\";\nimport { Settings } from \"llamaindex\";\n\n// Get the API key from `DEEPINFRA_API_TOKEN` environment variable\nimport { config } from \"dotenv\";\nconfig();\nSettings.llm = new DeepInfra();\n\n// Set the API key\napiKey = \"YOUR_API_KEY\";\nSettings.llm = new DeepInfra({ apiKey });\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Supabase Integration\nDESCRIPTION: This command installs the necessary LlamaIndex and Supabase integration packages using npm.  It adds `llamaindex` and `@llamaindex/supabase` as dependencies to your project.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/supabase.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/supabase\n```\n\n----------------------------------------\n\nTITLE: Stringifying Tool Results for Anthropic on Bedrock in Javascript\nDESCRIPTION: This patch ensures that all tool results are properly stringified when using Anthropic models on Bedrock within the LlamaIndex community package. This is essential for consistent data handling and compatibility across different components.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n- 1325178: fix: stringify all tool results for anthropic on bedrock\n```\n\n----------------------------------------\n\nTITLE: Setting Jina AI API Key\nDESCRIPTION: This command sets the `JINAAI_API_KEY` environment variable, which is required to authenticate with the Jina AI Reranker API.  Replace `<YOUR API KEY>` with the actual API key obtained from Jina AI.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/jinaai_reranker.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport JINAAI_API_KEY=<YOUR API KEY>\n```\n\n----------------------------------------\n\nTITLE: Run with debug logs (Shell)\nDESCRIPTION: This command executes the E2E tests with debug logs enabled, providing more detailed output for troubleshooting. The `CONSOLA_LEVEL=5` environment variable sets the verbosity of the logs. The `--import tsx` flag enables TypeScript execution, and `--import ./mock-register.js` imports the mock register. The pattern \"agent\" is used to filter tests whose names include \"agent\".\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/e2e/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nCONSOLA_LEVEL=5 node --import tsx --import ./mock-register.js --test-name-pattern=agent --test ./node/basic.e2e.ts\n```\n\n----------------------------------------\n\nTITLE: Retrieving Nodes with VectorIndexRetriever in LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to create a VectorIndexRetriever from a vector index and use it to retrieve the top-k most similar nodes based on a query string. The retriever is configured with a similarityTopK of 3, meaning it will return the top 3 most similar nodes.  It depends on LlamaIndex library. It retrieves nodes based on similarity search. The retrieve function expects an object with a \"query\" property which is the query string.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/retriever.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = vectorIndex.asRetriever({\n  similarityTopK: 3,\n});\n\n// Fetch nodes!\nconst nodesWithScore = await retriever.retrieve({ query: \"query string\" });\n```\n\n----------------------------------------\n\nTITLE: Installing project dependencies\nDESCRIPTION: Installs pnpm and ts-node globally, then installs the project dependencies using pnpm.  This is the initial setup step required to work on the project.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/llamaindex/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i -g pnpm ts-node\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Fixing Function Role for Tool Result Messages in Gemini\nDESCRIPTION: This patch fixes an issue in the Gemini integration where the function role was not being correctly used for messages containing tool results, ensuring proper handling of tool-related messages.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_12\n\n\n\n----------------------------------------\n\nTITLE: Running Mixtral 8x7b via Ollama\nDESCRIPTION: This command downloads and runs the mixtral:8x7b model using Ollama. The first time it's run, it will automatically download and install the model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/3_local_model.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama run mixtral:8x7b\n```\n\n----------------------------------------\n\nTITLE: Using TestTool in Typescript\nDESCRIPTION: This code snippet demonstrates how to import the TestTool class from the '@llamaindex/wasm-tools' package and instantiate it.  Then it calls the 'call' method with the argument '1', which simulates a call to an external API to get data.  The example URL demonstrates a potential usage of fetching a to-do item with ID 1 from a placeholder API.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/wasm-tools/README.md#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TestTool } from \"@llamaindex/wasm-tools\";\nconst testTool = new TestTool();\ntestTool.call(\"1\"); // get post has id = 1 (url: https://jsonplaceholder.typicode.com/todos?id=1)\n```\n\n----------------------------------------\n\nTITLE: Standalone Usage of DeepInfra Embedding\nDESCRIPTION: This code snippet demonstrates how to use DeepInfraEmbedding independently of LlamaIndex. It imports the necessary modules, configures the API token from a .env file, initializes the DeepInfra embedding model with the desired model, embeds a text, and logs the resulting embedding.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/deepinfra.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DeepInfraEmbedding } from \"@llamaindex/deepinfra\";\nimport { config } from \"dotenv\";\n// For standalone usage, you need to configure DEEPINFRA_API_TOKEN in .env file\nconfig();\n\nconst main = async () => {\n  const model = \"intfloat/e5-large-v2\";\n  const embeddings = new DeepInfraEmbedding({ model });\n  const text = \"What is the meaning of life?\";\n  const response = await embeddings.embed([text]);\n  console.log(response);\n};\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Running Postgres in Docker\nDESCRIPTION: This command starts a Postgres database instance within a Docker container. It maps port 5432 on the host to the container's port 5432 and sets the authentication method to 'trust', which is suitable for development purposes but not production. The `--rm` flag will delete the container when it is stopped. Using a volume is highly recommended for persistent storage.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/vector-store/pg/README.md#_snippet_0\n\nLANGUAGE: docker\nCODE:\n```\ndocker run -d --rm --name vector-db -p 5432:5432 -e \"POSTGRES_HOST_AUTH_METHOD=trust\" ankane/pgvector\n```\n\n----------------------------------------\n\nTITLE: Full Example: Indexing and Querying with OpenAI (TypeScript)\nDESCRIPTION: This complete example showcases the end-to-end process of using OpenAI with LlamaIndex, including setting the LLM, loading and indexing a document, creating a query engine, and querying the data.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@llamaindex/openai\";\nimport { Document, Settings, VectorStoreIndex } from \"llamaindex\";\n\n// Use the OpenAI LLM\nSettings.llm = new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 });\n\nasync function main() {\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n\n----------------------------------------\n\nTITLE: Setting maxRetries and timeout parameters\nDESCRIPTION: This code demonstrates setting `maxRetries` and `timeout` parameters when initializing `DeepInfraEmbedding` for better control over the request behavior. This configures the embedding client to retry failed requests up to `maxRetries` times, and sets a maximum `timeout` duration for each request, improving resilience and responsiveness.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/deepinfra.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Settings } from \"llamaindex\";\nimport { DeepInfraEmbedding } from \"@llamaindex/deepinfra\";\n\nconst model = \"intfloat/e5-large-v2\";\nconst maxRetries = 5;\nconst timeout = 5000; // 5 seconds\n\nSettings.embedModel = new DeepInfraEmbedding({\n  model,\n  maxRetries,\n  timeout,\n});\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex and VoyageAI\nDESCRIPTION: This command installs the LlamaIndex core library and the VoyageAI integration package as npm dependencies.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/voyageai.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i llamaindex @llamaindex/voyage-ai\n```\n\n----------------------------------------\n\nTITLE: Migrating LLMs and Embeddings to Own Packages\nDESCRIPTION: This chore change migrates LlamaIndex LLMs and embeddings to their own packages, improving modularity and maintainability.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_14\n\n\n\n----------------------------------------\n\nTITLE: Using Zod for Response Validation (TypeScript)\nDESCRIPTION: Shows how to define a Zod schema, configure the LLM with the schema, and then receive typed and validated responses based on that schema. It demonstrates the benefits of type safety and response validation.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\n\n// Define the response schema\nconst meetingSchema = z.object({\n  summary: z.string(),\n  participants: z.array(z.string()),\n  actionItems: z.array(z.string()),\n  nextSteps: z.string()\n});\n\n// Configure the LLM with the schema\nSettings.llm = new OpenAI({ \n  model: \"gpt-4o\", \n  temperature: 0,\n  responseFormat: meetingSchema\n});\n\nconst response = await llm.chat({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Summarize this meeting transcript\" \n    }\n  ]\n});\n\n// Response will be typed and validated according to the schema\nconst result = response.message.content;\nconsole.log(result.summary);\nconsole.log(result.actionItems);\n```\n\n----------------------------------------\n\nTITLE: Install DeepInfra and LlamaIndex\nDESCRIPTION: This command installs the `llamaindex` and `@llamaindex/deepinfra` packages using npm, which are necessary for using DeepInfra embeddings with LlamaIndex.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/deepinfra.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/deepinfra\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Parameters with JSON Schema in JavaScript\nDESCRIPTION: This code defines a tool with parameters described using JSON Schema as an alternative to Zod. It defines the `sumNumbers` tool with properties `a` and `b`, both specified as numbers, including descriptions and a `required` array, defining the input structure for the tool's parameters.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/5_rag_and_tools.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ntool(sumNumbers, {\n  name: \"sumNumbers\",\n  description: \"Use this function to sum two numbers\",\n  parameters: {\n    type: \"object\",\n    properties: {\n      a: {\n        type: \"number\",\n        description: \"First number to sum\",\n      },\n      b: {\n        type: \"number\",\n        description: \"Second number to sum\",\n      },\n    },\n    required: [\"a\", \"b\"],\n  },\n}),\n```\n\n----------------------------------------\n\nTITLE: Handler: Processing Start Event and Generating Joke\nDESCRIPTION: This handler processes the `startEvent`, generates an initial joke using OpenAI's LLM, and emits a `jokeEvent`. It constructs a prompt for the LLM, extracts the joke from the response using regular expressions, and returns a `jokeEvent` containing the extracted joke.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/agents/workflows.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\njokeFlow.handle([startEvent], async (event) => {\n  // Prompt the LLM to write a joke\n  const prompt = `Write your best joke about ${event.data}. Write the joke between <joke> and </joke> tags.`;\n  const response = await llm.complete({ prompt });\n\n  // Parse the joke from the response\n  const joke = response.text.match(/<joke>([\\s\\S]*?)<\\/joke>/)?.[1]?.trim() ?? response.text;\n  return jokeEvent.with({ joke: joke });\n});\n```\n\n----------------------------------------\n\nTITLE: Using DeepInfra Embedding with LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to use DeepInfraEmbedding to embed text and perform a query using LlamaIndex. It imports the necessary modules, initializes the DeepInfra embedding model using Settings, creates a document, builds a vector store index, and executes a query.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/deepinfra.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, Settings, VectorStoreIndex } from \"llamaindex\";\nimport { DeepInfraEmbedding } from \"@llamaindex/deepinfra\";\n\n// Update Embed Model\nSettings.embedModel = new DeepInfraEmbedding();\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Setting LlamaCloud API Key (Shell)\nDESCRIPTION: This snippet demonstrates how to set the `LLAMA_CLOUD_API_KEY` environment variable. This is a necessary step to authenticate with LlamaCloud.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/cloud/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport LLAMA_CLOUD_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Installing ChromaDB\nDESCRIPTION: This shell command uses pip to install the ChromaDB library, a prerequisite for running the example.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/chromadb/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npip install chromadb\n```\n\n----------------------------------------\n\nTITLE: Update AI Model Provider Import (v0.9)\nDESCRIPTION: Demonstrates the new import statement for AI model providers in LlamaIndex.TS v0.9. The OpenAI class is now imported from the '@llamaindex/openai' package, requiring installation of this package as a prerequisite. This change reduces the main 'llamaindex' package size.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/migration/0.8-to-0.9.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@llamaindex/openai\";\n```\n\n----------------------------------------\n\nTITLE: Using Gemini with Vertex AI\nDESCRIPTION: This code snippet demonstrates how to use Gemini through Google's Vertex AI. It uses `GeminiVertexSession` to configure the connection. The `location` and `project` parameters specify the Vertex AI location and project.  Optionally, `googleAuthOptions` allows further customization of authentication.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/gemini.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Gemini, GEMINI_MODEL, GeminiVertexSession } from \"@llamaindex/google\";\n\nconst gemini = new Gemini({\n  model: GEMINI_MODEL.GEMINI_PRO,\n  session: new GeminiVertexSession({\n    location: \"us-central1\",      // optional if provided by GOOGLE_VERTEX_LOCATION env variable\n    project: \"project1\",          // optional if provided by GOOGLE_VERTEX_PROJECT env variable\n    googleAuthOptions: {...},     // optional, but useful for production. It accepts all values from `GoogleAuthOptions`\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Configuration Options for OpenAI Responses API (TypeScript)\nDESCRIPTION: This comprehensive TypeScript snippet illustrates the various configuration options available for the OpenAI Responses API. It covers model settings, API configurations, response handling, and additional options such as instructions and truncation.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst llm = openaiResponses({\n  // Model and basic settings\n  model: \"gpt-4o\",\n  temperature: 0.1,\n  topP: 1,\n  maxOutputTokens: 1000,\n  \n  // API configuration\n  apiKey: \"your-api-key\",\n  baseURL: \"custom-endpoint\",\n  maxRetries: 10,\n  timeout: 60000,\n  \n  // Response handling\n  trackPreviousResponses: false,\n  store: false,\n  strict: false,\n  \n  // Additional options\n  instructions: \"Custom instructions for the model\",\n  truncation: \"auto\", // Can be \"auto\", \"disabled\", or null\n  include: [\"citations\", \"reasoning\"] // Specify what to include in responses\n});\n```\n\n----------------------------------------\n\nTITLE: SimpleDirectoryReader with LlamaParse (TypeScript)\nDESCRIPTION: This snippet illustrates the integration of `LlamaParse` within `SimpleDirectoryReader` to process documents in a directory. It includes options for customization like file extensions, number of workers, and individual LlamaParse parameters, showcasing advanced usage and control over parsing behavior.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/llama_parse/index.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SimpleDirectoryReader } from \"llamaindex/readers/SimpleDirectoryReader\";\n\nasync function main() {\n  const reader = new SimpleDirectoryReader({\n    inputDir: \"path/to/your/directory\",\n    fileExts: [\".pdf\", \".docx\"],\n    numWorkers: 2,\n    llamaParse: {\n      apiKey: \"YOUR_API_KEY\",\n      resultType: \"markdown\",\n    },\n  });\n  const documents = await reader.loadData();\n  console.log(documents);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: CodeSplitter with Node FS - TypeScript\nDESCRIPTION: This code demonstrates using the CodeSplitter with Node.js's `fs/promises` to read a code file and split it. It initializes a tree-sitter parser for TypeScript. The CodeSplitter is configured to use this parser. The code then reads the content of a TypeScript file and splits the text using the CodeSplitter.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/node-parser.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport fs from 'node:fs/promises';\nimport { CodeSplitter } from '@llamaindex/node-parser/code'\nimport Parser from \"tree-sitter\";\nimport TS from \"tree-sitter-typescript\";\n\nconst parser = new Parser();\nparser.setLanguage(TS.typescript as Parser.Language);\nconst codeSplitter = new CodeSplitter({\n\t  getParser: () => parser,\n\t});\n\n\tconst parsedDocuments = codeSplitter.splitText(await fs.readFile('path/to/file.ts', 'utf-8'));\n//\t\t\t ^?\n```\n\n----------------------------------------\n\nTITLE: Full LlamaIndex and Qdrant Example\nDESCRIPTION: This is a complete example showcasing how to load a document, create a Qdrant vector store, index the document, and then query the index for relevant information.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/qdrant.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport fs from \"node:fs/promises\";\nimport { Document, VectorStoreIndex } from \"llamaindex\";\nimport { QdrantVectorStore } from \"@llamaindex/qdrant\";\n\nasync function main() {\n  const path = \"node_modules/llamaindex/examples/abramov.txt\";\n  const essay = await fs.readFile(path, \"utf-8\");\n\n  const vectorStore = new QdrantVectorStore({\n    url: \"http://localhost:6333\",\n  });\n\n  const document = new Document({ text: essay, id_: path });\n  const storageContext = await storageContextFromDefaults({ vectorStore });\n  const index = await VectorStoreIndex.fromDocuments([document], {\n    storageContext,\n  });\n  \n  const queryEngine = index.asQueryEngine();\n\n  const response = await queryEngine.query({\n    query: \"What did the author do in college?\",\n  });\n\n  // Output response\n  console.log(response.toString());\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing Documents\nDESCRIPTION: This code snippet demonstrates how to create a `Document` object with sample text, sets the global LLM to an OpenAI `gpt-3.5-turbo` model with a temperature of 0.1, and then creates a `VectorStoreIndex` from the document. This index is used for semantic search.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/jinaai_reranker.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nSettings.llm = new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0.1 });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: LLM Provider Factory Convenience Initialization\nDESCRIPTION: This code snippet introduces a factory convenience method for initializing Language Model (LLM) providers. Instead of instantiating a new LLM object directly (e.g., `new OpenAI`), developers can use a simpler `openai` call. This change aims to streamline the initialization process for common LLM providers within LlamaIndex.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/CHANGELOG.md#_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nAdd factory convenience factory for each LLM provider, e.g. you can use openai instead of 'new OpenAI'\n```\n\n----------------------------------------\n\nTITLE: Browser CodeSplitter with Web Tree Sitter - JavaScript\nDESCRIPTION: This code shows how to set up and use the CodeSplitter in a browser environment with `web-tree-sitter`.  It initializes the `web-tree-sitter` library and then creates a CodeSplitter instance configured with the TypeScript language and a character limit for the chunks.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/node-parser.mdx#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport Parser from 'web-tree-sitter';\n\nParser.init({\n\t\t\tlocateFile(scriptName: string) {\n\t\t\treturn '/' + scriptName\n\t\t},\n\t\t}).then(async () => {\n\t\t\tconst parser = new Parser();\n\t\t\tconst Lang = await Parser.Language.load('/tree-sitter-typescript.wasm');\n\t\t\tparser.setLanguage(Lang);\n\t\t\treturn new CodeSplitter({\n\t\t\t\tgetParser: () => parser,\n\t\t\t\tmaxChars: 100\n\t\t\t});\n\t\t});\n```\n\n----------------------------------------\n\nTITLE: Install pnpm Package Manager\nDESCRIPTION: This command installs the pnpm package manager globally. pnpm is used for managing dependencies in the LlamaIndex.TS monorepo.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm install -g pnpm\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepInfra Embedding Model\nDESCRIPTION: This code snippet shows how to configure the DeepInfra embedding model by specifying the model name.  It imports the DeepInfraEmbedding class and updates the global embedModel setting with a new instance of DeepInfraEmbedding configured to use the specified model. This allows users to switch between different embedding models offered by DeepInfra.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/deepinfra.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DeepInfraEmbedding } from \"@llamaindex/deepinfra\";\n\nconst model = \"intfloat/e5-large-v2\";\nSettings.embedModel = new DeepInfraEmbedding({\n  model,\n});\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (Bash)\nDESCRIPTION: This snippet sets the OpenAI API key as an environment variable. This is required for the LlamaIndex server to authenticate with the OpenAI API. Replace `<your-openai-api-key>` with your actual API key.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/llamaindex-server/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=<your-openai-api-key>\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Transformation in LlamaIndex (TypeScript)\nDESCRIPTION: This snippet shows how to implement a custom transformation by extending the TransformComponent class in LlamaIndex. It creates a RemoveSpecialCharacters class that removes special characters and punctuation from the text of each node. This custom transformation can then be used either directly or as part of an IngestionPipeline.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/index.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TransformComponent, TextNode } from \"llamaindex\";\n\nexport class RemoveSpecialCharacters extends TransformComponent {\n  async transform(nodes: TextNode[]): Promise<TextNode[]> {\n    for (const node of nodes) {\n      node.text = node.text.replace(/[^\\w\\s]/gi, \"\");\n    }\n\n    return nodes;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running the RAG Agent with Multiple Queries in JavaScript\nDESCRIPTION: This code demonstrates running a RAG agent with three consecutive queries. The agent is queried for the budget of San Francisco for community health, public protection, and the combined budget of both. The responses are then logged to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/5_rag_and_tools.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nlet response = await agent.run(\"What's the budget of San Francisco for community health in 2023-24?\");\nconsole.log(response);\n\nlet response2 = await agent.run(\"What's the budget of San Francisco for public protection in 2023-24?\");\nconsole.log(response2);\n\nlet response3 = await agent.run(\"What's the combined budget of San Francisco for community health and public protection in 2023-24?\");\nconsole.log(response3);\n```\n\n----------------------------------------\n\nTITLE: Exporting a Chat Handler (Typescript)\nDESCRIPTION: Export an asynchronous function `chatWithAI` that utilizes the LlamaIndex Agent to handle chat interactions. It uses `convertTools` to prepare functions as tools and implements streaming to the UI.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/autotool/README.md#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n\"use server\";\n\n// imports ...\n\nexport async function chatWithAI(message: string): Promise<JSX.Element> {\n  const agent = new OpenAIAgent({\n    tools: convertTools(\"llamaindex\"),\n  });\n  const uiStream = createStreamableUI();\n  agent\n    .chat({\n      stream: true,\n      message,\n    })\n    .then(async (responseStream) => {\n      return responseStream.pipeTo(\n        new WritableStream({\n          start: () => {\n            uiStream.append(\"\\n\");\n          },\n          write: async (message) => {\n            uiStream.append(message.response.delta);\n          },\n          close: () => {\n            uiStream.done();\n          },\n        }),\n      );\n    });\n  return uiStream.value;\n}\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing a Document\nDESCRIPTION: This snippet demonstrates how to load and index a document using LlamaIndex. It initializes a `Document` object with the provided text and ID. Then, it sets the LLM to `gpt-3.5-turbo` with a temperature of 0.1 using `Settings.llm` and creates a `VectorStoreIndex` from the document.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/cohere_reranker.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nSettings.llm = new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0.1 });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: Creating a Query Engine with MixedbreadAI Embeddings\nDESCRIPTION: This code snippet creates a query engine from the vector store index. It constructs a query and calls the `query` method on the `queryEngine` object. This setup is used to process queries and retrieve relevant results, by prompting the mixedbread model to represent the input text.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/mixedbreadai.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst query =\n  \"Represent this sentence for searching relevant passages: What is bread?\";\n\n// Log the response\nconst results = await queryEngine.query(query);\nconsole.log(results); // Serving up the freshest, most relevant results.\n```\n\n----------------------------------------\n\nTITLE: Importing Modules from LlamaIndex and Cohere\nDESCRIPTION: This code imports the required modules from `@llamaindex/openai`, `@llamaindex/cohere`, and `llamaindex`. It imports `OpenAI` to specify the LLM model, `CohereRerank` for reranking nodes, and `Document`, `Settings`, and `VectorStoreIndex` from llamaindex to handle document loading, indexing, and vector storage.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/cohere_reranker.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@llamaindex/openai\";\nimport { CohereRerank } from \"@llamaindex/cohere\";\nimport { Document, Settings, VectorStoreIndex } from \"llamaindex\";\n```\n\n----------------------------------------\n\nTITLE: Initializing Bedrock LLM in LlamaIndex\nDESCRIPTION: This code snippet initializes the Bedrock LLM within LlamaIndex using the `Bedrock` class. It configures the model, region, and credentials for accessing the AWS Bedrock service, which can also be supplied by environment variables.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/bedrock.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BEDROCK_MODELS, Bedrock } from \"@llamaindex/community\";\n\nSettings.llm = new Bedrock({\n  model: BEDROCK_MODELS.ANTHROPIC_CLAUDE_3_HAIKU,\n  region: \"us-east-1\", // can be provided via env AWS_REGION\n  credentials: {\n    accessKeyId: \"...\", // optional and can be provided via env AWS_ACCESS_KEY_ID\n    secretAccessKey: \"...\", // optional and can be provided via env AWS_SECRET_ACCESS_KEY\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex.TS dependencies with npm\nDESCRIPTION: This command installs the core LlamaIndex.TS library along with specific dependencies such as @llamaindex/openai, @llamaindex/readers, and @llamaindex/huggingface. These dependencies provide OpenAI integration, data readers, and Hugging Face model support respectively. This is a prerequisite for building any LlamaIndex application.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/1_setup.mdx#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nnpm i llamaindex @llamaindex/openai @llamaindex/readers @llamaindex/huggingface\n```\n\n----------------------------------------\n\nTITLE: Loading PDF Data with LlamaParseReader\nDESCRIPTION: This JavaScript code snippet demonstrates how to use `LlamaParseReader` to load PDF data. It initializes a `LlamaParseReader` instance with the `resultType` set to \"markdown\" and then uses it to load data from the specified PDF file. The result is a set of documents that can be used for querying.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/6_llamaparse.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst reader = new LlamaParseReader({ resultType: \"markdown\" });\nconst documents = await reader.loadData(\"../data/sf_budget_2023_2024.pdf\");\n```\n\n----------------------------------------\n\nTITLE: Handling Empty Function Declarations in Vertex AI\nDESCRIPTION: This minor change addresses an issue where Google Vertex AI does not support empty functionDeclarations arrays.  An empty array is now passed to LLMAgent if no tools are available, allowing Gemini to function correctly in agent mode.  Gemini 2.0 flash lite was also added to the model list.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_10\n\n\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Database Connection\nDESCRIPTION: This bash script sets environment variables required to connect to the Postgres database. It includes the host, user, password, database name, port, and OpenAI API key. These variables are used by the TypeScript scripts to establish a connection and authenticate with the database and OpenAI API.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/vector-store/pg/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport PGHOST=your database host\nexport PGUSER=your database user\nexport PGPASSWORD=your database password\nexport PGDATABASE=your database name\nexport PGPORT=your database port\nexport OPENAI_API_KEY=your openai api key\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSeek LLM with LlamaIndex (TypeScript)\nDESCRIPTION: This snippet demonstrates how to initialize the DeepSeekLLM within LlamaIndex using the Settings object. It requires the `@llamaindex/deepseek` package and an API key. The `model` parameter specifies which DeepSeek model to use, such as `deepseek-coder` or `deepseek-chat`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/deepseek.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Settings } from \"llamaindex\";\nimport { DeepSeekLLM } from \"@llamaindex/deepseek\";\n\nSettings.llm = new DeepSeekLLM({\n  apiKey: \"<YOUR_API_KEY>\",\n  model: \"deepseek-coder\", // or \"deepseek-chat\"\n});\n```\n\n----------------------------------------\n\nTITLE: Array Element Swap\nDESCRIPTION: This snippet represents the challenge Dan Abramov faced during a Facebook interview where he struggled to write code to swap two elements in an array. It showcases the pressure of coding interviews.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/llamaindex/examples/abramov.txt#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Example implementation in JavaScript\n// The prompt was to implement this during the FB interview\nfunction swap(arr, index1, index2) {\n  let temp = arr[index1];\n  arr[index1] = arr[index2];\n  arr[index2] = temp;\n}\n\n// Usage example\nlet myArray = [1, 2, 3, 4, 5];\nswap(myArray, 1, 2);\n// myArray is now [1, 3, 2, 4, 5]\n```\n\n----------------------------------------\n\nTITLE: Creating a Document in LlamaIndexTS\nDESCRIPTION: This code snippet demonstrates how to create a `Document` object in LlamaIndexTS. It imports the `Document` class and instantiates it with text content and metadata. The `text` property holds the document's content, and the `metadata` property allows for storing additional information about the document as a key-value pair.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/index.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document } from \"llamaindex\";\n\ndocument = new Document({ text: \"text\", metadata: { key: \"val\" } });\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output\nDESCRIPTION: Demonstrates the expected JSON output format after running the TypeScript code with LlamaIndex.TS. It includes fields for summary, products, rep_name, prospect_name, and action_items extracted from the input text.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/structured_data_extraction.mdx#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"summary\": \"Sarah from XYZ Company called John to introduce the XYZ Widget, a tool designed to automate tasks and improve productivity. John expressed interest and requested case studies and a product demo. Sarah agreed to send the information and follow up to schedule the demo.\",\n  \"products\": [\"XYZ Widget\"],\n  \"rep_name\": \"Sarah\",\n  \"prospect_name\": \"John\",\n  \"action_items\": [\n    \"Send case studies and additional product information to John\",\n    \"Follow up with John to schedule a product demo\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Splitting Text with SentenceSplitter - TypeScript\nDESCRIPTION: This code demonstrates how to use the SentenceSplitter as a standalone module to split raw text into sentences.  It initializes the splitter with a specified chunk size and splits the provided text. The resulting `texts` variable will contain an array of sentence strings.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/node-parser.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SentenceSplitter } from \"llamaindex\";\n\nconst splitter = new SentenceSplitter({ chunkSize: 1 });\n\nconst texts = splitter.splitText(\"Hello World\");\n//     ^?\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI with LlamaIndex in TypeScript\nDESCRIPTION: This code initializes the OpenAI model within the LlamaIndex settings. It sets the `llm` property of the `Settings` object to a new instance of `OpenAI`, configured with the `gpt-4` model and a temperature of 0. This setup prepares the LlamaIndex environment to use the specified OpenAI model for subsequent operations.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/azure.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Settings } from \"llamaindex\";\nimport { OpenAI } from \"@llamaindex/openai\";\n\nSettings.llm = new OpenAI({ model: \"gpt-4\", temperature: 0 });\n```\n\n----------------------------------------\n\nTITLE: Fix: llama3 patched to handle empty content\nDESCRIPTION: This commit fixes a bug where the Llama 3 model would fail when presented with empty content, which could occur with a system message. It also adds a `max_tokens` export, likely for controlling the length of generated text.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_21\n\nLANGUAGE: javascript\nCODE:\n```\n- 56746c2: fix: llama3 patched to handle empty content (can happen with system) and added max tokens export\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key using .env file (Shell)\nDESCRIPTION: Sets the OpenAI API key using a `.env` file. The key is written to the file, and then the Node.js script is executed, loading the environment variables from the `.env` file using the `--env-file` flag. This approach is useful to manage environment variables and avoid hardcoding them in the script.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/node.mdx#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\necho \"OPENAI_API_KEY=your-api-key\" > .env\nnode --env-file .env your-script.js\n```\n\n----------------------------------------\n\nTITLE: Configuring a Specific HuggingFace Model\nDESCRIPTION: This code snippet shows how to configure the `HuggingFaceEmbedding` class to use a specific pre-trained model, such as `BAAI/bge-small-en-v1.5`, and disable quantization. It requires importing `HuggingFaceEmbedding` from `@llamaindex/huggingface` and setting `Settings.embedModel` accordingly.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/huggingface.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HuggingFaceEmbedding } from \"@llamaindex/huggingface\";\n\nSettings.embedModel = new HuggingFaceEmbedding({\n  modelType: \"BAAI/bge-small-en-v1.5\",\n  quantized: false,\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies - TypeScript\nDESCRIPTION: This code imports necessary modules from the `llamaindex` library and the `zod` library. It includes modules for agent creation, tool definition, OpenAI integration, global settings, environment variable configuration, and schema validation using Zod.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/2_create_agent.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport \"dotenv/config\";\nimport {\n  agent,\n  AgentStream,\n  tool,\n  openai,\n  Settings,\n} from \"llamaindex\";\nimport { z } from \"zod\";\n```\n\n----------------------------------------\n\nTITLE: Initialize Groq LLM\nDESCRIPTION: This code initializes the Groq module in LlamaIndex, setting it as the language model. It imports the Groq class and sets it as the LLM in LlamaIndex's settings. The API key can be set either as an environment variable or directly in the Groq constructor.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/groq.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Groq } from \"@llamaindex/groq\";\nimport { Settings } from \"llamaindex\";\n\nSettings.llm = new Groq({\n  // If you do not wish to set your API key in the environment, you may\n  // configure your API key when you initialize the Groq class.\n  // apiKey: \"<your-api-key>\",\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Query Engine with Reranker\nDESCRIPTION: This code creates a query engine using the configured retriever and the Jina AI Reranker as a node postprocessor. It then executes a query and logs the response.  The retriever fetches initial candidate documents, and the node postprocessor reranks these documents using Jina AI before returning the final response.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/jinaai_reranker.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine({\n  retriever,\n  nodePostprocessors: [nodePostprocessor],\n});\n\n// log the response\nconst response = await queryEngine.query(\"Where did the author grown up?\");\n```\n\n----------------------------------------\n\nTITLE: Load and Index Documents\nDESCRIPTION: This code demonstrates how to load a document and index it using `VectorStoreIndex` from `llamaindex`. The document is created using the `Document` class, and `fromDocuments` is used to create the index. This assumes that the `essay` variable holds the document's text content.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/together.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex } from \"llamaindex\";\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: Querying Pinecone Vector Store\nDESCRIPTION: This command runs the query.ts script to perform RAG (Retrieval-Augmented Generation) queries on the Pinecone vector store. It prompts the user for a question, processes it using OpenAI and Pinecone, and presents the answer. The script continues to prompt for queries until the user enters 'q', 'quit', or 'exit'.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/pinecone-vector-store/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx pinecone-vector-store/query.ts\n```\n\n----------------------------------------\n\nTITLE: MarkdownNodeParser with Node FS - TypeScript\nDESCRIPTION: This code demonstrates how to use the MarkdownNodeParser with Node.js's `fs/promises` module to read a markdown file and parse it. It reads the file content into a Document object, which is then parsed by the MarkdownNodeParser.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/node-parser.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport fs from 'node:fs/promises';\nimport { MarkdownNodeParser, Document } from \"llamaindex\";\n\nconst markdownNodeParser = new MarkdownNodeParser();\nconst text = await fs.readFile('path/to/file.md', 'utf-8');\nconst document = new Document({ text });\n\nconst parsedDocuments = markdownNodeParser([document]);\n//\t\t  ^?\n```\n\n----------------------------------------\n\nTITLE: Logging Workflow Events - TypeScript\nDESCRIPTION: This code shows how to log workflow events from the agent.  It iterates through the events in the `context`. If the event is an `AgentStream`, it streams the response. Otherwise, it logs the event data as a JSON string.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/2_create_agent.mdx#_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst context = myAgent.run(\"Sum 202 and 404\");\nfor await (const event of context) {\n  if (event instanceof AgentStream) {\n    // Stream the response\n    for (const chunk of event.data.delta) {\n      process.stdout.write(chunk);\n    }\n  } else {\n    // Log other events\n    console.log(\"\\nWorkflow event:\", JSON.stringify(event, null, 2));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex and Cohere Dependencies\nDESCRIPTION: This command installs the necessary npm packages: `llamaindex`, `@llamaindex/cohere`, and `@llamaindex/openai`.  These packages provide the core LlamaIndex functionality, the Cohere integration, and the OpenAI integration, respectively. Make sure you have Node.js and npm installed before running this command.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/cohere_reranker.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/cohere @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Install dotenv package with npm\nDESCRIPTION: The `dotenv` package is used to load environment variables from a `.env` file into the Node.js process. This allows accessing the OpenAI API key (and other sensitive information) without hardcoding it into the application. It is installed using the `npm i dotenv` command.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/1_setup.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm i dotenv\n```\n\n----------------------------------------\n\nTITLE: Querying the RouterQueryEngine in TypeScript\nDESCRIPTION: This code demonstrates how to query the `RouterQueryEngine` with a summarization query.  The `query` method is used to submit the query, and the response includes the answer and metadata indicating which query engine was selected to answer the query.  The response from the `vectorQueryEngine` is used for the summarization query.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/router_query_engine.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst summaryResponse = await queryEngine.query({\n  query: \"Give me a summary about his past experiences?\",\n});\n\nconsole.log({\n  answer: summaryResponse.response,\n  metadata: summaryResponse?.metadata?.selectorResult,\n});\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Directory with SimpleDirectoryReader - Typescript\nDESCRIPTION: This code snippet demonstrates how to use the SimpleDirectoryReader from the @llamaindex/readers/directory module to load data from a specified directory. The reader.loadData() method takes the directory path as input and returns an array of Document objects. Requires installation of the @llamaindex/readers package.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/readers/README.md#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SimpleDirectoryReader } from \"@llamaindex/readers/directory\";\n\nconst reader = new SimpleDirectoryReader();\nconst documents = reader.loadData(\"./directory\");\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAPI for LlamaCloud in TypeScript\nDESCRIPTION: This snippet demonstrates how to configure the OpenAPI client for LlamaCloud in TypeScript. It sets the API token and base URL, which are required for authenticating and accessing LlamaCloud services. Replace \"YOUR_API_KEY\" with your actual API key obtained from LlamaCloud.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/cloud/README.md#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { OpenAPI } from \"@llamaindex/cloud/api\";\nOpenAPI.TOKEN = \"YOUR_API_KEY\";\nOpenAPI.BASE = \"https://api.cloud.llamaindex.ai/\";\n```\n\n----------------------------------------\n\nTITLE: Creating a Query Engine with CohereRerank\nDESCRIPTION: This code creates a query engine using the previously configured retriever and CohereRerank node postprocessor. The query engine is then used to query the index with the question \"Where did the author grown up?\". The response from the query is then logged to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/cohere_reranker.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine({\n  retriever,\n  nodePostprocessors: [nodePostprocessor],\n});\n\n// log the response\nconst response = await queryEngine.query(\"Where did the author grown up?\");\n```\n\n----------------------------------------\n\nTITLE: Configuring the Retriever with Increased Similarity TopK\nDESCRIPTION: This code configures the retriever to retrieve more similar documents. By setting `similarityTopK` to 5, the retriever will return the top 5 most similar documents. This is useful when you want to rerank a larger set of documents to potentially find more relevant results.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/cohere_reranker.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = index.asRetriever({\n  similarityTopK: 5,\n});\n```\n\n----------------------------------------\n\nTITLE: Fixing Google Start Chat Tools Parameter\nDESCRIPTION: This patch fixes an issue with the Google start chat tools parameter, ensuring correct initialization and operation of chat tools.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_8\n\n\n\n----------------------------------------\n\nTITLE: Using Gemini Embeddings with LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to use Gemini embeddings with LlamaIndex. It initializes the GeminiEmbedding, sets it as the default embed model, creates an index from a document, and performs a query.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/gemini.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, Settings, VectorStoreIndex } from \"llamaindex\";\nimport { GeminiEmbedding, GEMINI_MODEL } from \"@llamaindex/google\";\n\n// Update Embed Model\nSettings.embedModel = new GeminiEmbedding();\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Supabase Vector Store\nDESCRIPTION: This TypeScript code initializes the `SupabaseVectorStore` with your Supabase credentials and table name.  It requires the `SUPABASE_URL` and `SUPABASE_KEY` environment variables to be set. The `table` parameter specifies the name of the table in Supabase where the vector embeddings are stored.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/supabase.mdx#_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vectorStore = new SupabaseVectorStore({\n  supabaseUrl: process.env.SUPABASE_URL,\n  supabaseKey: process.env.SUPABASE_KEY,\n  table: \"documents\",\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing FireworksLLM in LlamaIndex with API Key\nDESCRIPTION: This code snippet demonstrates how to initialize the FireworksLLM within LlamaIndex using an API key. It sets the default LLM for LlamaIndex to a new instance of FireworksLLM, configuring it with the provided API key. The API key is necessary for authenticating with the Fireworks.ai service.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/fireworks.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Settings } from \"llamaindex\";\nimport { FireworksLLM } from \"@llamaindex/fireworks\";\n\nSettings.llm = new FireworksLLM({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI Environment Variables\nDESCRIPTION: This code snippet shows the environment variables that need to be set to use Azure OpenAI. The variables include the API key, endpoint, and deployment name. The endpoint URL is specific to your Azure OpenAI deployment.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/index.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AZURE_OPENAI_KEY=\"<YOUR KEY HERE>\"\nexport AZURE_OPENAI_ENDPOINT=\"<YOUR ENDPOINT, see https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython&pivots=rest-api>\"\nexport AZURE_OPENAI_DEPLOYMENT=\"gpt-4\" # or some other deployment name\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing Documents with LlamaIndex (TS)\nDESCRIPTION: This snippet demonstrates loading a single document and creating a VectorStoreIndex in LlamaIndex. It initializes a `Document` object and then uses `VectorStoreIndex.fromDocuments` to build the index. This allows for efficient querying of the document's content.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/portkey.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex } from \"llamaindex\";\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: Run tests with mock register (Shell)\nDESCRIPTION: This command executes the E2E tests for LlamaIndexTS Core using the Node.js Test Runner, utilizing a mock register to simulate LLM API calls. The `--import tsx` flag enables TypeScript execution, and `--import ./mock-register.js` imports the mock register.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/e2e/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nnode --import tsx --import ./mock-register.js --test ./node/basic.e2e.ts\n```\n\n----------------------------------------\n\nTITLE: Interleaf's Lisp Scripting Language\nDESCRIPTION: Interleaf, a document creation software company, added a scripting language inspired by Emacs, making it a dialect of Lisp. The author was hired to write things in this Lisp dialect, which was the 'thinnest icing on a giant C cake'.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/vector-store/azure/data/paul_graham/paul_graham_essay.txt#_snippet_0\n\nLANGUAGE: Lisp\nCODE:\n```\nTheir Lisp was the thinnest icing on a giant C cake, and since I didn't know C and didn't want to learn it, I never understood most of the software.\n```\n\n----------------------------------------\n\nTITLE: Creating a CohereRerank Instance\nDESCRIPTION: This snippet creates a new instance of the `CohereRerank` class. It requires an API key for accessing the Cohere API and specifies the number of top results to return (`topN`). Replace `<COHERE_API_KEY>` with your actual Cohere API key.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/cohere_reranker.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst nodePostprocessor = new CohereRerank({\n  apiKey: \"<COHERE_API_KEY>\",\n  topN: 4,\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring LlamaIndex settings in TypeScript\nDESCRIPTION: This code configures the LlamaIndex settings by setting the Large Language Model (LLM) to OpenAI and defining a sentence splitter for parsing documents into nodes. The `chunkSize` parameter determines the maximum size of each text chunk.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/router_query_engine.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nSettings.llm = new OpenAI();\nSettings.nodeParser = new SentenceSplitter({\n  chunkSize: 1024,\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI LLM in LlamaIndex (TypeScript)\nDESCRIPTION: This snippet initializes the OpenAI language model within the LlamaIndex settings. It sets the model, temperature, and API key for subsequent operations.  The apiKey should be replaced with your actual OpenAI API key.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@llamaindex/openai\";\nimport { Settings } from \"llamaindex\";\n\nSettings.llm = new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0, apiKey: <YOUR_API_KEY> });\n```\n\n----------------------------------------\n\nTITLE: Importing LlamaIndex and Supabase Modules\nDESCRIPTION: This TypeScript code imports the necessary modules from the `llamaindex` and `@llamaindex/supabase` libraries. It imports `Document` and `VectorStoreIndex` from `llamaindex` and `SupabaseVectorStore` from `@llamaindex/supabase` to allow the creation of documents, vector store indexes and the interaction with the Supabase vector store.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/supabase.mdx#_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { Document, VectorStoreIndex } from \"llamaindex\";\nimport { SupabaseVectorStore } from \"@llamaindex/supabase\";\n```\n\n----------------------------------------\n\nTITLE: Creating ChromaDB Vector Store with LlamaIndex\nDESCRIPTION: This snippet creates a `ChromaVectorStore` instance, initializes a `storageContext`, and then creates a `VectorStoreIndex` from the documents. The `storageContext` is configured to use the ChromaDB vector store. This sets up the index for querying.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/metadata_filtering.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst chromaVS = new ChromaVectorStore({ collectionName });\n\nconst storageContext = await storageContextFromDefaults({\n  vectorStore: chromaVS,\n});\n\nconst index = await VectorStoreIndex.fromDocuments(docs, {\n  storageContext: storageContext,\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing ContextChatEngine with Retriever in Typescript\nDESCRIPTION: This code initializes the `ContextChatEngine` with a retriever obtained from an index. It then uses the `chat` method to send a query and receive a response. The `index` object is assumed to be an existing LlamaIndex object.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/chat_engine.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = index.asRetriever();\nconst chatEngine = new ContextChatEngine({ retriever });\n\n// start chatting\nconst response = await chatEngine.chat({ message: query });\n```\n\n----------------------------------------\n\nTITLE: Start Development Server (npm)\nDESCRIPTION: After the application is generated and the user has navigated into the app directory using `cd`, this command starts the development server. It uses npm to execute the `dev` script defined in the `package.json` file. This allows users to view the application in their browser.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/create_llama.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Switching LLM to Mixtral 8x7b\nDESCRIPTION: This code snippet demonstrates how to switch the language model in your LlamaIndex code to use the Mixtral 8x7b model running via Ollama. It replaces the previous call to `Settings.llm` with a call to the `ollama` function, specifying the desired model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/3_local_model.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nSettings.llm = ollama({\n  model: \"mixtral:8x7b\",\n});\n```\n\n----------------------------------------\n\nTITLE: OpenAI API with Built-in Tools (TypeScript)\nDESCRIPTION: This snippet demonstrates how to integrate built-in tools with the OpenAI Responses API. It configures the API to use a function tool named `search_files` and enables strict mode for accurate tool calls.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst llm = openaiResponses({\n  model: \"gpt-4o\",\n  builtInTools: [\n    {\n      type: \"function\",\n      name: \"search_files\",\n      description: \"Search through available files\"\n    }\n  ],\n  strict: true // Enable strict mode for tool calls\n});\n```\n\n----------------------------------------\n\nTITLE: Creating API Chat Route in Next.js with TypeScript\nDESCRIPTION: This code snippet demonstrates creating an API route (`app/api/chat/route.ts`) for handling chat requests within a Next.js application using the App Router. It involves setting up an endpoint to receive and process chat messages, likely interacting with LlamaIndexTS for backend logic.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/chat.mdx#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\n{  \n  \"file\": \"./src/app/api/chat/route.ts\",\n\t\"codeblock\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Querying the Index in LlamaIndex (TypeScript)\nDESCRIPTION: This code snippet demonstrates how to query the created index using LlamaIndex. It obtains a query engine from the index using index.asQueryEngine(), defines a query string, and then executes the query using queryEngine.query(). The results are stored in the results variable.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/deepinfra.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a MixedbreadAI Reranker Instance\nDESCRIPTION: This code creates an instance of the `MixedbreadAIReranker` class, passing in the API key and the number of top results to return after reranking. The `apiKey` authenticates your requests to the Mixedbread AI service, and `topN` specifies the number of results to keep.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/mixedbreadiai_reranker.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst nodePostprocessor = new MixedbreadAIReranker({\n  apiKey: \"<MIXEDBREAD_API_KEY>\",\n  topN: 4,\n});\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Addition Function - TypeScript\nDESCRIPTION: This code defines a JavaScript function `sumNumbers` that takes an object with two named parameters, `a` and `b`, and returns their sum as a string. The function is designed to be used as a tool by an LLM agent.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/2_create_agent.mdx#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst sumNumbers = ({ a, b }) => {\n  return `${a + b}`;\n};\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex via pnpm\nDESCRIPTION: This command installs the LlamaIndex library using pnpm, a package manager that aims to be faster and more efficient than npm. It adds LlamaIndex as a dependency, enabling the use of its functionalities for integrating LLMs with your data.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npnpm install llamaindex\n```\n\n----------------------------------------\n\nTITLE: Switch LLM to Ollama\nDESCRIPTION: This snippet configures LlamaIndex.TS to use a local LLM served by Ollama.  It imports the necessary modules and sets the `Settings.llm` property to an `ollama` instance configured to use the specified model (e.g., mixtral:8x7b).\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/local_llm.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Settings } from \"llamaindex\";\nimport { ollama } from \"@llamaindex/ollama\";\n\nSettings.llm = ollama({\n  model: \"mixtral:8x7b\",\n});\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI LLM in LlamaIndex.TS\nDESCRIPTION: This code snippet demonstrates how to set the default LLM in LlamaIndex.TS to OpenAI. It imports the OpenAI and Settings classes from the respective packages and initializes the Settings.llm with an OpenAI instance, specifying the model and temperature.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/index.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@llamaindex/openai\";\nimport { Settings } from \"llamaindex\";\n\nSettings.llm = new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 });\n```\n\n----------------------------------------\n\nTITLE: Initializing MistralAI LLM in LlamaIndex (TypeScript)\nDESCRIPTION: This snippet initializes the MistralAI LLM within LlamaIndex's Settings. It configures the model to use \"mistral-tiny\" and sets the API key. The API key is essential for authenticating requests to the MistralAI API.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/mistral.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MistralAI } from \"@llamaindex/mistral\";\nimport { Settings } from \"llamaindex\";\n\nSettings.llm = new MistralAI({\n  model: \"mistral-tiny\",\n  apiKey: \"<YOUR_API_KEY>\",\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing and Starting LlamaIndex Server\nDESCRIPTION: Initializes and starts the LlamaIndex Server with a specified workflow and UI configuration.  The workflow is defined by the createWorkflow function, and the UI is configured with an app title and starter questions.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/llamaindex-server.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LlamaIndexServer } from \"@llamaindex/server\";\nimport { wiki } from \"@llamaindex/tools\"; // or any other tool\n\nconst createWorkflow = () => agent({ tools: [wiki()] })\n\nnew LlamaIndexServer({\n  workflow: createWorkflow,\n  uiConfig: {\n    appTitle: \"LlamaIndex App\",\n    starterQuestions: [\"Who is the first president of the United States?\"],\n  },\n}).start();\n```\n\n----------------------------------------\n\nTITLE: Evaluate Response Faithfulness\nDESCRIPTION: This code snippet evaluates the faithfulness of a query engine response using the FaithfulnessEvaluator. It initializes the evaluator, queries the engine, and then calls the evaluator to determine if the response aligns with the source documents.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/faithfulness.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst query = \"How did New York City get its name?\";\n\nconst evaluator = new FaithfulnessEvaluator();\n\nconst response = await queryEngine.query({\n  query,\n});\n\nconst result = await evaluator.evaluateResponse({\n  query,\n  response,\n});\n\nconsole.log(`the response is ${result.passing ? \"faithful\" : \"not faithful\"}`);\n```\n\n----------------------------------------\n\nTITLE: Running Supabase Example\nDESCRIPTION: This command executes the `supabase.ts` script using `tsx`. The script connects to a Supabase database using the `POSTGRES_URL` environment variable. This variable should be obtained from the Supabase project settings page, enabling a direct connection to the Supabase Postgres instance.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/vector-store/pg/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx vector-store/pg/supabase.ts\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Store Index\nDESCRIPTION: This JavaScript code creates a vector store index from a list of documents, using the initialized Qdrant vector store. It uses the `VectorStoreIndex.fromDocuments` method to build the index. The `vectorStore` parameter associates the documents with the Qdrant instance.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/7_qdrant.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// create a query engine from our documents\nconst index = await VectorStoreIndex.fromDocuments(documents, { vectorStore });\n```\n\n----------------------------------------\n\nTITLE: Configure OpenAI API key in .env file\nDESCRIPTION: This snippet shows how to store your OpenAI API key in a `.env` file. The API key is required to authenticate with the OpenAI service. This ensures that the API key is not hardcoded in the code and can be easily managed.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/1_setup.mdx#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nOPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXXXXXX\n```\n\n----------------------------------------\n\nTITLE: Importing Files with Node.js Customization Hooks\nDESCRIPTION: Illustrates how to import files using Node.js customization hooks provided by `@llamaindex/readers/node`.  First, the node script is run with `--import @llamaindex/readers/node`, then the file can be imported, and the text extracted using `getText()`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/index.mdx#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnode --import @llamaindex/readers/node ./script.js\n```\n\n----------------------------------------\n\nTITLE: Run Mixtral model with Ollama\nDESCRIPTION: This command downloads and runs the Mixtral 8x7b model using Ollama. It's a suitable model for agentic work, balancing power and resource requirements.  The model will be downloaded and installed the first time it's run.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/local_llm.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama run mixtral:8x7b\n```\n\n----------------------------------------\n\nTITLE: MongoDB URI Environment Variable\nDESCRIPTION: This snippet shows how to set the MONGODB_URI environment variable in the .env file.  The URI is used to connect to your MongoDB Atlas instance. Replace the placeholder password with your actual password.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/mongodb/README.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nMONGODB_URI=mongodb+srv://seldo:xxxxxxxxxxx@llamaindexdemocluster.xfrdhpz.mongodb.net/?retryWrites=true&w=majority\n```\n\n----------------------------------------\n\nTITLE: Configure OpenAI LLM (gpt-4)\nDESCRIPTION: This code configures the `Settings.llm` to use the `gpt-4` model from OpenAI. Using `gpt-4` typically provides better results for evaluation tasks. It instantiates an `OpenAI` object with the specified model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/correctness.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nSettings.llm = new OpenAI({\n  model: \"gpt-4\",\n});\n```\n\n----------------------------------------\n\nTITLE: Using SimilarityPostprocessor with Retrieved Nodes (TS)\nDESCRIPTION: This code demonstrates how to apply SimilarityPostprocessor to nodes retrieved using LlamaIndex's retriever.  It retrieves nodes, configures the SimilarityPostprocessor with a similarity cutoff, and then filters the nodes.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/index.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SimilarityPostprocessor } from \"llamaindex\";\n\nnodes = await index.asRetriever().retrieve({ query: \"test query str\" });\n\nconst processor = new SimilarityPostprocessor({\n  similarityCutoff: 0.7,\n});\n\nconst filteredNodes = processor.postprocessNodes(nodes);\n```\n\n----------------------------------------\n\nTITLE: Asking the Agent a Question - TypeScript\nDESCRIPTION: This snippet demonstrates how to interact with the agent using the `run` method.  It passes a question to the agent and awaits the response. The agent uses the defined tools to answer the question. The result is then logged to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/2_create_agent.mdx#_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst context = myAgent.run(\"Sum 101 and 303\");\nconst result = await context;\nconsole.log(result.data);\n```\n\n----------------------------------------\n\nTITLE: Running a TypeScript file using tsx\nDESCRIPTION: This command executes a TypeScript file directly using `tsx`, which is a tool that allows running TypeScript files without pre-compilation. It assumes that the file `example.ts` exists in the current directory and is a valid TypeScript file.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/rag/index.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx example.ts\n```\n\n----------------------------------------\n\nTITLE: Using Gemini LLM with a Proxy\nDESCRIPTION: This code demonstrates how to use the Gemini language model with a proxy server. The `requestOptions` parameter within the Gemini constructor allows specifying a `baseUrl` for proxying requests.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/gemini.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Gemini, GEMINI_MODEL } from \"@llamaindex/google\";\nimport { Settings } from \"llamaindex\";\n\nSettings.llm = new Gemini({\n  model: GEMINI_MODEL.GEMINI_PRO,\n  requestOptions: {\n    baseUrl: <YOUR_PROXY_URL>   // optional, but useful for custom endpoints\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Retry Handling Logic to Parser Reader in Javascript\nDESCRIPTION: This patch introduces retry handling logic to the parser reader within the LlamaIndex community package. It also addresses linting issues to improve code quality and maintainability.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n- 5189b44: fix: add retry handling logic to parser reader and fix lint issues\n```\n\n----------------------------------------\n\nTITLE: Configure Next.js with LlamaIndex\nDESCRIPTION: This code snippet demonstrates how to configure a Next.js application to work with LlamaIndex. It imports `withLlamaIndex` and uses it to wrap the Next.js configuration. This ensures compatibility and proper handling of LlamaIndex.TS within the Next.js environment.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/next.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n// next.config.mjs / next.config.ts\nimport withLlamaIndex from \"llamaindex/next\";\n\n/** @type {import('next').NextConfig} */\nconst nextConfig = {};\n\nexport default withLlamaIndex(nextConfig);\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI provider package via npm\nDESCRIPTION: This command installs the @llamaindex/openai package using npm. This package provides the necessary integration to use OpenAI's LLMs with LlamaIndex.TS. It needs to be installed in addition to the core LlamaIndex package.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nnpm install @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Fix: prevent tool calling getting mixed with conversation\nDESCRIPTION: This commit fixes an issue where tool calling functionality was interfering with regular conversation flow, likely resulting in unexpected behavior.  The fix likely isolates or properly sequences these actions.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\n- 224d507: fix: prevent tool calling getting mixed with conversation\n```\n\n----------------------------------------\n\nTITLE: AsyncGenerator Task Creation in TypeScript\nDESCRIPTION: Demonstrates the use of an async generator to create tasks within the agent system. The code initializes an agent, creates a task using `createTask`, and iterates over the task output using a `for await...of` loop.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/llamaindex/src/agent/README.md#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst agent = new MyAgent();\nconst task = agent.createTask();\nfor await (const taskOutput of task) {\n  // do something\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up LlamaIndexTS Examples (npm)\nDESCRIPTION: This snippet demonstrates how to clone the LlamaIndexTS examples repository using degit, navigate to the project directory, and install the necessary dependencies using npm. It sets up the environment to run the provided example scripts locally.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/examples.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx degit run-llama/LlamaIndexTS/examples my-new-project\ncd my-new-project\nnpm i\n```\n\n----------------------------------------\n\nTITLE: Creating a VectorStoreIndex in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a VectorStoreIndex from a Document in LlamaIndex using TypeScript. It imports the necessary modules and initializes a document with sample text. Then, it uses `VectorStoreIndex.fromDocuments` to create an index from the document.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/data_index/index.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document, VectorStoreIndex } from \"llamaindex\";\n\nconst document = new Document({ text: \"test\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: MongoDB Vectors and Index Environment Variables\nDESCRIPTION: This snippet shows how to set the MONGODB_VECTORS and MONGODB_VECTOR_INDEX environment variables in the .env file. These variables specify the collection to store vector embeddings and the vector search index name.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/mongodb/README.md#_snippet_4\n\nLANGUAGE: Text\nCODE:\n```\nMONGODB_VECTORS=tiny_tweets_vectors\nMONGODB_VECTOR_INDEX=tiny_tweets_vector_index\n```\n\n----------------------------------------\n\nTITLE: Running LlamaIndexTS Example Script (tsx)\nDESCRIPTION: This snippet illustrates how to execute a specific LlamaIndexTS example script using the `tsx` command-line tool after setting up the project locally.  It assumes that the project's dependencies are installed and that `tsx` is available in the environment.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/examples.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx ./vectorIndex.ts\n```\n\n----------------------------------------\n\nTITLE: Running LlamaIndex Server (Bash)\nDESCRIPTION: This snippet executes the LlamaIndex server using `tsx`, a TypeScript execution tool. This command assumes that the `llamaindex-server/simple-workflow/index.ts` file is the entry point for the server application. The server is expected to start on localhost port 4000.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/llamaindex-server/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx llamaindex-server/simple-workflow/index.ts\n```\n\n----------------------------------------\n\nTITLE: Next.js Webpack Configuration - JavaScript\nDESCRIPTION: This JavaScript code provides a Next.js webpack configuration to ensure `@llamaindex/env` works correctly in a web environment.  It modifies the webpack target to include \"web\" and \"es2020\".\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/node-parser.mdx#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst config = {\n\t\t\twebpack: (config) => {\n\t\t\t\tif (Array.isArray(config.target) && config.target.includes('web')) {\n\t\t\t\t\tconfig.target = [\"web\", \"es2020\"];\n\t\t\t\t}\n\t\t\t\treturn config;\n\t\t\t}\n\t\t}\n\n\t\texport default config;\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing Documents\nDESCRIPTION: This snippet demonstrates how to load a document and create a VectorStoreIndex from it. It initializes a Document object with the text content and an ID, then uses VectorStoreIndex.fromDocuments to create the index.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/llama2.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n----------------------------------------\n\nTITLE: Refactor: depends on core pacakge instead of llamaindex\nDESCRIPTION: This commit refactors the package to depend directly on `@llamaindex/core` instead of the top-level `llamaindex` package. This is likely part of a modularization effort to reduce dependencies and improve code organization.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_22\n\nLANGUAGE: javascript\nCODE:\n```\n- 16ef5dd: refactor: depends on core pacakge instead of llamaindex\n```\n\n----------------------------------------\n\nTITLE: Run tests without mock register (Shell)\nDESCRIPTION: This command executes the E2E tests for LlamaIndexTS Core without using a mock register, meaning it will interact with real LLM APIs. The `--import tsx` flag enables TypeScript execution.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/e2e/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nnode --import tsx --test ./node/basic.e2e.ts\n```\n\n----------------------------------------\n\nTITLE: Fixing File Input in LlamaIndex Cloud (Patch)\nDESCRIPTION: This patch addresses an issue related to file input within the LlamaIndex Cloud package. The specific problem involved how the package was handling file inputs, which could lead to errors or incorrect behavior.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/cloud/CHANGELOG.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n- 41191d0: fix(parse): file input\n```\n\n----------------------------------------\n\nTITLE: Running TypeScript Example - Bash\nDESCRIPTION: This command shows how to execute a TypeScript file using `tsx`, a tool designed for running TypeScript files directly without pre-compilation. This simplifies the development process by allowing for immediate execution of TypeScript code.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/2_create_agent.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx example.ts\n```\n\n----------------------------------------\n\nTITLE: Starting the LlamaIndex Reader\nDESCRIPTION: This command starts the LlamaIndex Reader project. It assumes that `npm` is installed and configured to run scripts defined in the project's `package.json` file. The specific actions performed by `npm run start` are defined within the `package.json` file in the scripts section.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/readers/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm run start\n```\n\n----------------------------------------\n\nTITLE: AI Action Definition with LlamaIndexTS\nDESCRIPTION: Defines an AI context provider with a chat server action using LlamaIndexTS. This action generates responses based on chat history and user input. It leverages Next.js RSC and Vercel AI RSC for server-side rendering.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/rsc.mdx#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"file\": \"./src/components/demo/chat/rsc/ai-action.tsx\",\n\t\"codeblock\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Texts for Embedding Generation\nDESCRIPTION: This code defines an array of strings named `texts` containing the text for which embeddings are to be generated.  It represents the input data that will be processed by the embedding model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/mixedbreadai.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst texts = [\"Bread is life\", \"Bread is love\"];\n```\n\n----------------------------------------\n\nTITLE: Generating and Logging Embeddings for Texts\nDESCRIPTION: This snippet calls the `embedDocuments` method on the `embeddings` object to generate embeddings for the provided texts and then logs the result to the console. The result contains the generated embeddings for each text.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/mixedbreadai.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await embeddings.embedDocuments(texts);\nconsole.log(result); // Perfectly customized embeddings, ready to serve.\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI provider package via pnpm\nDESCRIPTION: This command installs the @llamaindex/openai package using pnpm. This package provides the necessary integration to use OpenAI's LLMs with LlamaIndex.TS. It needs to be installed in addition to the core LlamaIndex package.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\npnpm install @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Updating Tool Call Schema for Nova in Javascript\nDESCRIPTION: This patch updates the tool call schema specifically for the Nova model within the LlamaIndex community package. This ensures that tool calls are correctly formatted and processed when interacting with the Nova model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n- e9bf442: fix: update the tool call schema for nova\n```\n\n----------------------------------------\n\nTITLE: Installing @llamaindex/readers with npm\nDESCRIPTION: This command installs the @llamaindex/readers package using npm. It's a prerequisite for using any of the reader functionalities provided by the package. The package provides utilities for reading data from various sources for use with LlamaIndex.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/readers/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i @llamaindex/readers\n```\n\n----------------------------------------\n\nTITLE: Create Llama RAG App (Shell)\nDESCRIPTION: This shell command utilizes `npx create llama` to generate a new RAG application based on the LlamaIndexTS framework. This command simplifies the process of bootstrapping a new RAG project with pre-configured settings and dependencies.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpx create llama\n```\n\n----------------------------------------\n\nTITLE: Install OpenAI LLM Package - JavaScript\nDESCRIPTION: This command installs the @llamaindex/openai package, enabling the use of OpenAI LLMs within LlamaIndex. It depends on having npm installed and assumes you want to utilize OpenAI's language models.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/index.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nnpm i @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Initializing Qdrant Vector Store\nDESCRIPTION: This JavaScript code initializes a Qdrant vector store using the LlamaIndex.TS library. It specifies the URL where the Qdrant instance is running.  The URL parameter defines the connection endpoint for the Qdrant database.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/7_qdrant.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// initialize qdrant vector store\nconst vectorStore = new QdrantVectorStore({\n  url: \"http://localhost:6333\",\n});\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex via npm\nDESCRIPTION: This command installs the LlamaIndex library using npm, a package manager for Node.js. It adds LlamaIndex as a dependency to your project, allowing you to use its functionalities for integrating LLMs with your data.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nnpm install llamaindex\n```\n\n----------------------------------------\n\nTITLE: Using Google's New Gen AI Library for Multimodal Output in Javascript\nDESCRIPTION: This feature update utilizes Google's new Gen AI library to enable support for multimodal output within the LlamaIndex community package. This enhancement expands the capabilities of the package to handle various types of output formats.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n- 3fd4cc3: feat: use google's new gen ai library to support multimodal output\n```\n\n----------------------------------------\n\nTITLE: Function Overloads for Agent Configuration in TypeScript\nDESCRIPTION: Shows the use of function overloads in TypeScript to provide flexibility in configuring the Agent class. The `from` method can accept either an array of BaseTool objects or a Defaults object.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/llamaindex/src/agent/README.md#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nclass Agent {\n  from(tools: BaseTool[]): Agent;\n  from(defaults: Defaults): Agent;\n  from(toolsOrDefaults: BaseTool[] | Defaults): Agent {\n    // runtime check\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Query the Index\nDESCRIPTION: This code shows how to query the indexed document using the `asQueryEngine` method. It takes a query string and retrieves the results. The `queryEngine.query` method is called with the query parameters.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/together.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI Base URL Environment Variable (Bash)\nDESCRIPTION: This bash command sets a custom base URL for the OpenAI API. This is useful when using a different endpoint than the default OpenAI API, such as when using a service like Scaleway.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_BASE_URL=\"https://api.scaleway.ai/v1\"\n```\n\n----------------------------------------\n\nTITLE: Set LlamaCloud API Key and Embedding Model\nDESCRIPTION: These commands set the `LLAMA_CLOUD_API_KEY` and `EMBEDDING_MODEL` environment variables. The `LLAMA_CLOUD_API_KEY` should be replaced with your actual LlamaCloud API key, and `EMBEDDING_MODEL` sets the embedding model to be used.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/vercel/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport LLAMA_CLOUD_API_KEY=your_api_key_here\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport EMBEDDING_MODEL=\"text-embedding-3-small\"\n```\n\n----------------------------------------\n\nTITLE: Increasing Similarity TopK\nDESCRIPTION: This code configures the retriever to fetch the top 5 most similar documents, instead of the default 2. This increases the number of results available for reranking by the Jina AI Reranker.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/jinaai_reranker.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = index.asRetriever({\n  similarityTopK: 5,\n});\n```\n\n----------------------------------------\n\nTITLE: Run Chat Engine (Shell)\nDESCRIPTION: This shell command executes the `chatEngine.ts` file using `tsx`, which is likely a TypeScript execution tool. This command initiates the chat engine application, enabling interactive conversations.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpx tsx ./chatEngine.ts\n```\n\n----------------------------------------\n\nTITLE: Depth-First Traversal and Collapse Output\nDESCRIPTION: JSON output when using Depth-First Traversal with collapse, with `levelsBack` set to `0` and `collapseLength` set to `35`. It demonstrates collapsing JSON string representations into a single line.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/json.mdx#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\na 1 {\"key1\":\"value1\"}\na 2 {\"key2\":\"value2\"}\nb {\"3\":{\"k3\":\"v3\"},\"4\":{\"k4\":\"v4\"}}\n```\n\n----------------------------------------\n\nTITLE: Adding LLM Provider Factory Convenience\nDESCRIPTION: This patch adds a factory convenience function for each LLM provider, allowing users to use shorthand notations like `openai` instead of `new OpenAI`. This simplifies the initialization process.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_9\n\n\n\n----------------------------------------\n\nTITLE: Install LlamaIndex Readers with npm\nDESCRIPTION: Installs the `@llamaindex/readers` package using npm. This package contains various reader classes for different file formats, which are used to load data into Document objects.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/index.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i @llamaindex/readers\n```\n\n----------------------------------------\n\nTITLE: Install OpenAI Provider Package\nDESCRIPTION: Shows the npm command to install the @llamaindex/openai package. This package is required for using OpenAI models with LlamaIndex.TS v0.9. This is a necessary step after upgrading.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/migration/0.8-to-0.9.mdx#_snippet_2\n\nLANGUAGE: package-install\nCODE:\n```\nnpm i @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Initializing SentenceSplitter with Settings - TypeScript\nDESCRIPTION: This code initializes a SentenceSplitter and sets it as the default node parser in LlamaIndex's Settings. The SentenceSplitter splits text into sentences.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/transformations/node-parser.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TextFileReader } from '@llamaindex/readers/text'\nimport { SentenceSplitter } from 'llamaindex';\nimport { Settings } from 'llamaindex';\n\nconst nodeParser = new SentenceSplitter();\nSettings.nodeParser = nodeParser;\n//\t\t     ^?\n```\n\n----------------------------------------\n\nTITLE: Export OpenAI API Key (Shell)\nDESCRIPTION: This shell command exports the OpenAI API key, allowing the LlamaIndexTS application to authenticate with the OpenAI service. The API key is essential for accessing the language model functionality.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport OPENAI_API_KEY=\"sk-...\"\n```\n\n----------------------------------------\n\nTITLE: Update AI Model Provider Import (v0.8)\nDESCRIPTION: Shows the old import statement for AI model providers in LlamaIndex.TS v0.8. This snippet demonstrates how to import the OpenAI class directly from the 'llamaindex' package.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/migration/0.8-to-0.9.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"llamaindex\";\n```\n\n----------------------------------------\n\nTITLE: Importing LlamaIndex and Qdrant Modules\nDESCRIPTION: This code imports the required modules from LlamaIndex and the Qdrant integration package. These modules are necessary for creating documents, vector stores, and indices.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/qdrant.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport fs from \"node:fs/promises\";\nimport { Document, VectorStoreIndex } from \"llamaindex\";\nimport { QdrantVectorStore } from \"@llamaindex/qdrant\";\n```\n\n----------------------------------------\n\nTITLE: Install Langtrace SDK\nDESCRIPTION: This command installs the Langtrace TypeScript SDK using npm. The SDK is required to integrate Langtrace with LlamaIndex.TS. Before running this command, ensure that Node.js and npm are installed and configured correctly.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/integration/lang-trace.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @langtrase/typescript-sdk\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex and ChromaDB dependencies\nDESCRIPTION: This command installs the necessary LlamaIndex packages, including the core library, OpenAI integration, and ChromaDB integration, using npm. These packages are required for the metadata filtering example.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/metadata_filtering.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nnpm i llamaindex @llamaindex/openai @llamaindex/chroma\n```\n\n----------------------------------------\n\nTITLE: Starting Next.js Development Server\nDESCRIPTION: This snippet demonstrates how to start the Next.js development server using npm, yarn, pnpm, or bun. It allows developers to run the application locally and preview changes in real-time.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/e2e/examples/nextjs-agent/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\n----------------------------------------\n\nTITLE: Load and Query AstraDB Vectorstore\nDESCRIPTION: This example loads and queries a simple vectorstore with some documents about Astra DB. It demonstrates how to connect to AstraDB, load data into a vector store, and perform a query.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/astradb/README.md#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nrun `tsx astradb/example`\n```\n\n----------------------------------------\n\nTITLE: Uncleaned JSON Output\nDESCRIPTION: JSON output when using uncleaned JSON, with `levelsBack` set to `undefined` and `cleanJson` set to `false`. This shows the complete JSON output without structural characters removal.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/json.mdx#_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"a\":{\"1\":{\"key1\":\"value1\"},\"2\":{\"key2\":\"value2\"}},\"b\":{\"3\":{\"k3\":\"v3\"},\"4\":{\"k4\":\"v4\"}}}\n```\n\n----------------------------------------\n\nTITLE: Default Options JSONReader Output\nDESCRIPTION: JSON output when using the default options for JSONReader, with `LevelsBack` set to `undefined` and `cleanJson` set to `true`. It demonstrates the cleaned output format.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/json.mdx#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"a\": {\n\"1\": {\n\"key1\": \"value1\"\n\"2\": {\n\"key2\": \"value2\"\n\"b\": {\n\"3\": {\n\"k3\": \"v3\"\n\"4\": {\n\"k4\": \"v4\"\n```\n\n----------------------------------------\n\nTITLE: Connecting Chat Interface with useChatRSC Hook\nDESCRIPTION: Connects the chat interface to the `chat` AI action using the `useChatRSC` hook. This hook handles communication between the UI and the AI action to manage chat interactions in the Next.js RSC environment.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/rsc.mdx#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"file\": \"./src/components/demo/chat/rsc/use-chat-rsc.tsx\",\n\t\"codeblock\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Install Ollama package\nDESCRIPTION: This command installs the @llamaindex/ollama package, which is necessary to integrate Ollama with LlamaIndex.TS for using local LLMs. This package provides the necessary bindings to connect LlamaIndex to Ollama-managed models.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/local_llm.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @llamaindex/ollama\n```\n\n----------------------------------------\n\nTITLE: Querying the Index (TypeScript)\nDESCRIPTION: This snippet shows how to query the indexed data using a query engine. It defines a query and then uses the `queryEngine.query` method to retrieve results.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Documents and Query for Object Reranking\nDESCRIPTION: This code defines an array of documents (objects with `title` and `content` fields) and the query to use for reranking them.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/mixedbreadiai_reranker.mdx#_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst documents = [\n  { title: \"Bread Recipe\", content: \"To bake bread you need flour\" },\n  { title: \"Bread Recipe\", content: \"To bake bread you need yeast\" },\n];\nconst query = \"What do you need to bake bread?\";\n```\n\n----------------------------------------\n\nTITLE: Feat: added llama 3.1 support\nDESCRIPTION: This commit adds initial support for the Llama 3.1 model, likely including basic inference capabilities.  More advanced features might be added in subsequent commits.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_20\n\nLANGUAGE: javascript\nCODE:\n```\n- 3d9a802: feat: added llama 3.1\n```\n\n----------------------------------------\n\nTITLE: Example Evaluation Result\nDESCRIPTION: This is an example output showing that the generated response was evaluated as not correct, with a score of 2.5. This indicates the evaluator found inaccuracies or irrelevant information in the response.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/correctness.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nthe response is not correct with a score of 2.5\n```\n\n----------------------------------------\n\nTITLE: Running JupyterLab\nDESCRIPTION: This snippet provides the command to start JupyterLab, launching it in a web browser. JupyterLab provides an environment for creating and running Jupyter notebooks. No specific parameters are required; it simply starts the JupyterLab server.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/jupyter/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\njupyter lab\n```\n\n----------------------------------------\n\nTITLE: Defining Nodes and Query for Simple Reranking\nDESCRIPTION: This code defines the nodes (documents) to be reranked and the query used for reranking. `BaseNode` is used to create simple text-based nodes.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/mixedbreadiai_reranker.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst nodes = [\n  { node: new BaseNode(\"To bake bread you need flour\") },\n  { node: new BaseNode(\"To bake bread you need yeast\") },\n];\nconst query = \"What do you need to bake bread?\";\n```\n\n----------------------------------------\n\nTITLE: Adding Nova Premier and EU endpoints in Javascript\nDESCRIPTION: This patch adds support for Nova Premier to the AWS Nova models within the LlamaIndex community package. It also includes the addition of EU endpoints for enhanced regional accessibility and performance.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n- 411dcea: Add Nova Premier to AWS Nova models. Add EU endpoints\n```\n\n----------------------------------------\n\nTITLE: Fixing Bundle Output Incorrectly in Javascript\nDESCRIPTION: This patch addresses an issue where the bundle output was incorrect within the LlamaIndex community package. This ensures that the packaged code is properly structured and functions as expected.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\n- cb608b5: fix: bundle output incorrect\n```\n\n----------------------------------------\n\nTITLE: Initializing TypeScript project\nDESCRIPTION: Initializes a new npm project and installs TypeScript and its node type definitions as development dependencies. This prepares the environment for writing and running TypeScript code.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/structured_data_extraction.mdx#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nnpm init\nnpm i -D typescript @types/node\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex and OpenAI\nDESCRIPTION: This command installs the `llamaindex` and `@llamaindex/openai` packages using npm. These packages are required to use LlamaIndex and OpenAI's models within the application.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/jinaai_reranker.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Update Storage Provider Import (v0.8)\nDESCRIPTION: Illustrates the old import statement for storage providers in LlamaIndex.TS v0.8.  PineconeVectorStore was previously available directly in the 'llamaindex' package.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/migration/0.8-to-0.9.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PineconeVectorStore } from \"llamaindex\";\n```\n\n----------------------------------------\n\nTITLE: Running LlamaCloud Pipeline Example (Shell)\nDESCRIPTION: This snippet executes the `pipeline.ts` example, which demonstrates creating a managed LlamaCloud index using a pipeline. Requires TypeScript and the relevant dependencies to be pre-installed.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/cloud/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npnpx tsx cloud/pipeline.ts\n```\n\n----------------------------------------\n\nTITLE: Adding Structured Output Support to Chat API\nDESCRIPTION: This minor change adds support for structured output in the chat API of OpenAI and Ollama. The structured output parameter is also included in the provider, enabling structured data extraction and integration.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Pulling Ollama Embedding Model\nDESCRIPTION: This command pulls the specified embedding model from Ollama. It is necessary to download the model before using it in LlamaIndex. Specifically, the `nomic-embed-text` model is used in subsequent code snippets.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/ollama.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nollama pull nomic-embed-text\n```\n\n----------------------------------------\n\nTITLE: Querying with QueryEngine in LlamaIndexTS\nDESCRIPTION: This snippet shows how to initialize a QueryEngine from an index and use it to query data. The `query` function takes a query string as input and returns a response.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/index.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\nconst response = await queryEngine.query({ query: \"query string\" });\n```\n\n----------------------------------------\n\nTITLE: Update Data Loader Import (v0.8)\nDESCRIPTION: Shows the original import statement for data loaders in LlamaIndex.TS v0.8.  SimpleDirectoryReader was formerly part of the main 'llamaindex' package.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/migration/0.8-to-0.9.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SimpleDirectoryReader } from \"llamaindex\";\n```\n\n----------------------------------------\n\nTITLE: Install Vertex AI and Authenticate\nDESCRIPTION: These commands install the Vertex AI client library and authenticate the application for local development.  `gcloud auth application-default login` configures credentials for your Google Cloud account.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/gemini.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @google-cloud/vertexai\ngcloud auth application-default login\n```\n\n----------------------------------------\n\nTITLE: Adding Amazon Nova Support via Bedrock in Javascript\nDESCRIPTION: This feature adds support for Amazon Nova models through Bedrock within the LlamaIndex community package. This integration allows users to leverage the capabilities of Amazon Nova via the Bedrock platform.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\n- c1850ee: feat: Amazon Nova support via Bedrock\n```\n\n----------------------------------------\n\nTITLE: Configure OpenAI LLM\nDESCRIPTION: This configures the global settings to use the 'gpt-4' model from OpenAI for better results. It initializes an OpenAI instance and assigns it to the Settings.llm property.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/faithfulness.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nSettings.llm = new OpenAI({\n  model: \"gpt-4\",\n});\n```\n\n----------------------------------------\n\nTITLE: Run a Test Query\nDESCRIPTION: This snippet shows the command to run a test query against the indexed data in MongoDB using LlamaIndex. This retrieves relevant information based on the query.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/mongodb/README.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\nnpx tsx mongodb/3_query.ts\n```\n\n----------------------------------------\n\nTITLE: Installing HuggingFace Integration\nDESCRIPTION: This command installs the necessary npm packages for using HuggingFace embeddings with LlamaIndex, including the core `llamaindex` package and the `@llamaindex/huggingface` integration.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/huggingface.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/huggingface\n```\n\n----------------------------------------\n\nTITLE: Starting Next.js Development Server\nDESCRIPTION: This snippet demonstrates how to start the Next.js development server using npm, yarn, pnpm or bun. It assumes that Next.js is already installed and configured in the project.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/e2e/examples/nextjs-node-runtime/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\n----------------------------------------\n\nTITLE: Configure OpenAI Embedding\nDESCRIPTION: Configures LlamaIndex to use the OpenAI `text-embedding-ada-002` model. It imports the `OpenAIEmbedding` class and sets it as the default embedding model using `Settings.embedModel`.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/index.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbedding } from \"@llamaindex/openai\";\nimport { Settings } from \"llamaindex\";\n\nSettings.embedModel = new OpenAIEmbedding({\n  model: \"text-embedding-ada-002\",\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Deno with Homebrew\nDESCRIPTION: This snippet provides the command to install Deno using Homebrew on macOS. Deno is a modern runtime for JavaScript and TypeScript. This command assumes that Homebrew is already installed on the system.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/jupyter/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install deno\n```\n\n----------------------------------------\n\nTITLE: Saving the Generated UI Component\nDESCRIPTION: Saves the generated JSX code to a file named after the event type (ui_event.jsx). This file is placed in the components directory for the LlamaIndex Server to use.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/llamaindex-server.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nfs.writeFileSync(\"components/ui_event.jsx\", code);\n```\n\n----------------------------------------\n\nTITLE: Adding OpenAI Responses API Support\nDESCRIPTION: This patch adds support for the OpenAI Responses API to the @llamaindex/google package, enabling integration with OpenAI's response functionalities. It also updates the core LlamaIndex dependency.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Increasing Similarity TopK\nDESCRIPTION: This code configures the retriever to return more similar documents. Increasing `similarityTopK` allows the reranker to consider a wider range of results before reranking them.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/mixedbreadiai_reranker.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = index.asRetriever({\n  similarityTopK: 5,\n});\n```\n\n----------------------------------------\n\nTITLE: Depth-First Traversal all levels Output\nDESCRIPTION: JSON output when using Depth-First Traversal all levels, with `levelsBack` set to `0`. It shows all the keys and values from the JSON structure.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/json.mdx#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\na 1 key1 value1\na 2 key2 value2\nb 3 k3 v3\nb 4 k4 v4\n```\n\n----------------------------------------\n\nTITLE: Package Installation\nDESCRIPTION: Installs the necessary packages for using the RelevancyEvaluator, including llamaindex and @llamaindex/openai. This command prepares the environment for the LlamaIndex functionality demonstrated in subsequent code snippets.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/relevancy.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Installing MistralAI Integration\nDESCRIPTION: This command installs the necessary packages for using MistralAI with LlamaIndex. It installs the core `llamaindex` package and the `@llamaindex/mistral` package which provides the MistralAI embedding integration.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/mistral.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/mistral\n```\n\n----------------------------------------\n\nTITLE: Querying the LlamaIndex Index\nDESCRIPTION: This code queries the created index using a query engine.  It retrieves the answer to the provided question from the indexed documents.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/qdrant.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEngine = index.asQueryEngine();\n\nconst response = await queryEngine.query({\n  query: \"What did the author do in college?\",\n});\n\n// Output response\nconsole.log(response.toString());\n```\n\n----------------------------------------\n\nTITLE: Adding RequestOptions Parameter for Gemini Proxy Calls\nDESCRIPTION: This patch adds the RequestOptions parameter to support Gemini proxy calls, allowing for customization of requests and proxy configurations. It includes a usage example for the RequestOptions parameter.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_7\n\n\n\n----------------------------------------\n\nTITLE: Running Qdrant with Docker\nDESCRIPTION: This bash command pulls the Qdrant Docker image and runs it, exposing port 6333 for access.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/7_qdrant.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull qdrant/qdrant\ndocker run -p 6333:6333 qdrant/qdrant\n```\n\n----------------------------------------\n\nTITLE: Installing Perplexity LLM Package\nDESCRIPTION: This command installs the `@llamaindex/perplexity` package, which provides the necessary functionality to use Perplexity LLMs with LlamaIndex.  It utilizes the npm package manager.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/perplexity.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @llamaindex/perplexity\n```\n\n----------------------------------------\n\nTITLE: Setting up Qdrant Vector Store\nDESCRIPTION: This code initializes a QdrantVectorStore instance, connecting to a Qdrant instance running at the specified URL.  The URL parameter specifies the address of the Qdrant instance.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/qdrant.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst vectorStore = new QdrantVectorStore({\n  url: \"http://localhost:6333\",\n});\n```\n\n----------------------------------------\n\nTITLE: Installing @llamaindex/autotool (Shell)\nDESCRIPTION: Install the @llamaindex/autotool package using npm, pnpm, or yarn. This package allows automatic transpilation of JS functions to LLM Agent compatible tools.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/autotool/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @llamaindex/autotool\npnpm add @llamaindex/autotool\nyarn add @llamaindex/autotool\n```\n\n----------------------------------------\n\nTITLE: Setting LlamaCloud Base URL (Shell)\nDESCRIPTION: This snippet demonstrates how to set the `LLAMA_CLOUD_BASE_URL` environment variable.  This allows you to specify a different LlamaCloud environment (e.g., staging).\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/cloud/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport LLAMA_CLOUD_BASE_URL=\"https://api.staging.llamaindex.ai\"\n```\n\n----------------------------------------\n\nTITLE: Refactor: move some llm and embedding to single package\nDESCRIPTION: This commit refactors the codebase to move some of the LLM and embedding functionality into a single package, likely for better organization and maintainability.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\n- a75af83: refactor: move some llm and embedding to single package\n```\n\n----------------------------------------\n\nTITLE: Running Sub Question Query Engine Example in LlamaIndexTS\nDESCRIPTION: This command shows how to run the Sub Question Query Engine example using `ts-node`. It executes the `subquestion.ts` file, demonstrating the functionality of splitting a query into multiple sub-queries.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/query_engines/index.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpx ts-node subquestion.ts\n```\n\n----------------------------------------\n\nTITLE: Adding Inference Profile Mapping for Nova Models in Javascript\nDESCRIPTION: This patch adds inference profile mapping specifically for Nova models within the LlamaIndex community package. This allows for optimized inference performance based on the specific profile configuration.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\n- 24caf93: fix: added inference profile mapping for nova models\"\n```\n\n----------------------------------------\n\nTITLE: Install Together LLM Package\nDESCRIPTION: This command installs the `@llamaindex/together` package, which provides the necessary classes and functions for interacting with the Together LLM API within a LlamaIndex application.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/together.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @llamaindex/together\n```\n\n----------------------------------------\n\nTITLE: Run Unit Tests\nDESCRIPTION: This command runs all unit tests for the LlamaIndex.TS packages. The tests are located in the `tests` folder of each package and use their own test packages.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npnpm test\n```\n\n----------------------------------------\n\nTITLE: Installing a specific NPM package\nDESCRIPTION: Installs an NPM package for a specific package or demo application within the monorepo. The `--filter` flag specifies which package or application the dependency should be installed for.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/llamaindex/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm add [NPM Package] --filter [package or application i.e. core or simple]\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex dependencies using npm\nDESCRIPTION: This command installs the necessary LlamaIndex, OpenAI, and HuggingFace dependencies for the RAG application.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/4_agentic_rag.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/openai @llamaindex/huggingface\n```\n\n----------------------------------------\n\nTITLE: Check Node.js Version\nDESCRIPTION: This command checks the installed Node.js version. It's essential to ensure you have a compatible LTS version installed before contributing.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnode -v\n# v20.x.x\n```\n\n----------------------------------------\n\nTITLE: Adding Gemini 2.5 Flash Preview Support\nDESCRIPTION: This patch adds support for the Gemini 2.5 Flash Preview model within the @llamaindex/google package, enhancing its capabilities with the latest Google AI models.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Install LlamaIndex Dependencies\nDESCRIPTION: This command installs the necessary LlamaIndex packages along with OpenAI and Qdrant dependencies using npm. It allows usage of ingestion pipelines and vector databases.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/ingestion_pipeline/index.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/openai @llamaindex/qdrant\n```\n\n----------------------------------------\n\nTITLE: Loading Document from File\nDESCRIPTION: This code reads a text file into a string, which will be used to create a Document object for indexing.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/vector_stores/qdrant.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst path = \"node_modules/llamaindex/examples/abramov.txt\";\nconst essay = await fs.readFile(path, \"utf-8\");\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex and Mixedbread AI\nDESCRIPTION: This command installs the necessary LlamaIndex packages and the Mixedbread AI integration. This allows you to utilize Mixedbread AI's reranking capabilities within your LlamaIndex projects.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/mixedbreadiai_reranker.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i llamaindex @llamaindex/openai @llamaindex/mixedbread\n```\n\n----------------------------------------\n\nTITLE: Initializing AI Provider and ChatSection\nDESCRIPTION: Initializes the AI provider for the application and includes the `ChatSection` component. This is the entrypoint for the chat application using Next.js RSC.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/ui/rsc.mdx#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"file\": \"./src/components/demo/chat/rsc/demo.tsx\",\n\t\"codeblock\": true\n}\n```\n\n----------------------------------------\n\nTITLE: LLM Initialization with JSON Response Format (TypeScript)\nDESCRIPTION: Demonstrates how to set the `responseFormat` during the initialization of the LLM. It uses the `json_object` format.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nconst llm = new OpenAI({\n  model: \"gpt-4o\",\n  responseFormat: { type: \"json_object\" } // or a Zod schema\n});\n```\n\n----------------------------------------\n\nTITLE: Performing Simple Reranking\nDESCRIPTION: This code uses the `postprocessNodes` method to rerank the nodes based on the provided query. The result is then logged to the console.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/mixedbreadiai_reranker.mdx#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await reranker.postprocessNodes(nodes, query);\nconsole.log(result); // Like pulling freshly baked nodes out of the oven.\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex and Postgres Package\nDESCRIPTION: Installs the necessary packages for using LlamaIndex and the PostgreSQL integration. This is a prerequisite for using the PostgresDocumentStore.  It leverages npm for package management.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/doc_stores/index.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/postgres\n```\n\n----------------------------------------\n\nTITLE: Depth-First Traversal limited levels Output\nDESCRIPTION: JSON output when using Depth-First Traversal with limited levels, with `levelsBack` set to `2`. It shows the output with a limited number of levels in the JSON structure.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/json.mdx#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n1 key1 value1\n2 key2 value2\n3 k3 v3\n4 k4 v4\n```\n\n----------------------------------------\n\nTITLE: Adding Gemini 2.5 Pro Preview Model\nDESCRIPTION: This patch introduces support for the Gemini 2.5 Pro Preview model, expanding the range of available Gemini models for use with LlamaIndex.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Import LlamaIndex Modules\nDESCRIPTION: This snippet imports the Document and VectorStoreIndex classes from the LlamaIndex library. These modules are essential for creating and querying documents.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/jupyter/vectorIndex.ipynb#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  Document,\n  VectorStoreIndex\n} from \"npm:llamaindex\";\n```\n\n----------------------------------------\n\nTITLE: Running the documentation website\nDESCRIPTION: Navigates to the documentation website directory, installs dependencies, and starts the Docusaurus development server.  This command is used to preview documentation changes locally.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/llamaindex/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd apps/docs\npnpm install\npnpm start\n```\n\n----------------------------------------\n\nTITLE: Example JSON Input\nDESCRIPTION: Example JSON input used to demonstrate different options of the JSONReader. It showcases a nested JSON structure with multiple levels.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/json.mdx#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"a\": {\"1\": {\"key1\": \"value1\"}, \"2\": {\"key2\": \"value2\"}}, \"b\": {\"3\": {\"k3\": \"v3\"}, \"4\": {\"k4\": \"v4\"}}}\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex Bedrock Integration\nDESCRIPTION: This command installs the necessary packages for using LlamaIndex with AWS Bedrock. It includes the core LlamaIndex library and the community integrations package.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/bedrock.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nnpm i llamaindex @llamaindex/community\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex and Postgres Dependencies\nDESCRIPTION: This command installs the necessary npm packages to use LlamaIndex with Postgres. It includes the core LlamaIndex library and the Postgres integration package.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/stores/index_stores/index.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/postgres\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (Bash)\nDESCRIPTION: This bash command sets the OpenAI API key as an environment variable. This is a common method for securely providing the API key to your application.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/openai.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"<YOUR_API_KEY>\"\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex dependencies using npm\nDESCRIPTION: This command installs the necessary LlamaIndex packages, including core LlamaIndex, Cohere integration, and OpenAI integration.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/rag/node_postprocessors/index.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/cohere @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: tsconfig.json AsyncIterable Configuration\nDESCRIPTION: Illustrates how to enable `DOM.AsyncIterable` in `tsconfig.json` for modules using the Web Stream API in LlamaIndex.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/typescript.mdx#_snippet_2\n\nLANGUAGE: json5\nCODE:\n```\n{\n  compilerOptions: {\n    //  add this lib to your tsconfig.json\n    lib: [\"DOM.AsyncIterable\"],\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex Groq Integration\nDESCRIPTION: This command installs the necessary packages for integrating Groq with LlamaIndex using npm. It requires Node.js and npm to be installed and configured.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/groq.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/groq\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Ollama Integration\nDESCRIPTION: This command installs the necessary LlamaIndex and Ollama packages using npm. The `@llamaindex/ollama` package provides the integration between LlamaIndex and Ollama.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/ollama.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nnpm i llamaindex @llamaindex/ollama\n```\n\n----------------------------------------\n\nTITLE: Installation using npm\nDESCRIPTION: Installs the `llamaindex` and `@llamaindex/readers` packages using npm. These packages are required to use the `JSONReader` class.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/json.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i llamaindex @llamaindex/readers\n```\n\n----------------------------------------\n\nTITLE: Running Node.js script with @llamaindex/autotool\nDESCRIPTION: Use the `--import` flag to run a Node.js script with @llamaindex/autotool enabled. This preprocesses the script to enable LLM Agent compatibility.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/autotool/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnode --import @llamaindex/autotool/node ./path/to/your/script.js\n```\n\n----------------------------------------\n\nTITLE: Set Groq API Key\nDESCRIPTION: This command sets the Groq API key as an environment variable.  This is needed for authenticating with the Groq service when using the LlamaIndex integration.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/groq.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport GROQ_API_KEY=<your-api-key>\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex and OpenAI\nDESCRIPTION: This command installs the LlamaIndex and OpenAI packages using npm. These packages are required for using LLMs in LlamaIndex.TS.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/index.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex Gemini Package\nDESCRIPTION: This command installs the necessary LlamaIndex and Gemini packages using npm. It includes the core LlamaIndex library and the @llamaindex/google package for Gemini integration.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/gemini.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/google\n```\n\n----------------------------------------\n\nTITLE: Run Development Server (pnpm)\nDESCRIPTION: This command starts the development server for the Next.js application using pnpm. It allows for local testing and development of the application.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Replicate Package\nDESCRIPTION: This command installs the necessary LlamaIndex and Replicate packages using npm.  It is a prerequisite for using LlamaDeuce with LlamaIndex and Replicate.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/llms/llama2.mdx#_snippet_0\n\nLANGUAGE: package-install\nCODE:\n```\nnpm i llamaindex @llamaindex/replicate\n```\n\n----------------------------------------\n\nTITLE: Run E2E Tests\nDESCRIPTION: This command executes end-to-end (E2E) tests for the LlamaIndex.TS project. All E2E tests are located in the `e2e` folder.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/CONTRIBUTING.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npnpm e2e\n```\n\n----------------------------------------\n\nTITLE: Wrapping Tools in an Array - TypeScript\nDESCRIPTION: This creates an array containing the defined tool `addTool`. This array is then used when creating the agent.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/tutorials/agents/2_create_agent.mdx#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst tools = [addTool];\n```\n\n----------------------------------------\n\nTITLE: Set OpenAI API Key\nDESCRIPTION: This command sets the OpenAI API key as an environment variable.  The API key is required for authenticating with OpenAI's services.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/correctness.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex and OpenAI\nDESCRIPTION: This command installs the LlamaIndex core library and the OpenAI integration, which are required for using the FaithfulnessEvaluator.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/faithfulness.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex Cloud Package\nDESCRIPTION: Installs the necessary LlamaIndex and LlamaIndex Cloud packages using npm. This command is required to use the LlamaParse reader.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/llama_parse/json_mode.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i llamaindex @llamaindex/cloud\n```\n\n----------------------------------------\n\nTITLE: Print Relevancy Result\nDESCRIPTION: An expected output of whether the response is relevant or not. The snippet is expected from running the whole procedure from the previous steps.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/relevancy.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nthe response is relevant\n```\n\n----------------------------------------\n\nTITLE: Starting ChromaDB Server\nDESCRIPTION: This shell command starts the ChromaDB server, which is necessary for the example to connect to and use the ChromaDB instance.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/chromadb/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nchroma run\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: This shell command exports the OpenAI API key, which is required for running the ChromaDB example.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/chromadb/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nexport OPEN_API_KEY=insert your api key here\n```\n\n----------------------------------------\n\nTITLE: Install Discord Library\nDESCRIPTION: Installs the @llamaindex/discord library using npm. This library provides the DiscordReader class for fetching messages from Discord channels.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/discord.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @llamaindex/discord\n```\n\n----------------------------------------\n\nTITLE: Adding Gemini 2.0 Models\nDESCRIPTION: This patch adds support for Gemini 2.0 models within the package, enhancing its AI capabilities.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/providers/google/CHANGELOG.md#_snippet_13\n\n\n\n----------------------------------------\n\nTITLE: Install LlamaIndex and OpenAI packages\nDESCRIPTION: Installs the necessary npm packages for LlamaIndex and OpenAI integration. This is a prerequisite for using the OpenAI embedding model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/index.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/openai\n```\n\n----------------------------------------\n\nTITLE: Set OpenAI API Key\nDESCRIPTION: This sets the OpenAI API key as an environment variable, which is necessary to authenticate requests to the OpenAI API.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/faithfulness.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex Google Integration\nDESCRIPTION: This command installs the necessary LlamaIndex Google integration package using npm. It is a prerequisite for using Gemini embeddings.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/models/embeddings/gemini.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i llamaindex @llamaindex/google\n```\n\n----------------------------------------\n\nTITLE: JSON Lines Format Output\nDESCRIPTION: Example JSON Lines formatted output, showing each JSON object as a separate string.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/data/readers/json.mdx#_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n\"tweet\": \"Hello world\"\n\n\"tweet\": \"\"\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex Package - JavaScript\nDESCRIPTION: This command installs the core LlamaIndex package using npm. It is a prerequisite for using LlamaIndex in a JavaScript project.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/getting_started/installation/index.mdx#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nnpm i llamaindex\n```\n\n----------------------------------------\n\nTITLE: Example Faithfulness Result\nDESCRIPTION: An example of the expected output when the response is considered faithful.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/apps/next/src/content/docs/llamaindex/modules/evaluation/faithfulness.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nthe response is faithful\n```\n\n----------------------------------------\n\nTITLE: Installing JupyterLab with pip\nDESCRIPTION: This snippet demonstrates how to install JupyterLab using pip, the Python package installer. JupyterLab is a web-based interactive development environment for notebooks, code, and data. It requires Python and pip to be installed beforehand.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/jupyter/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip3 install jupyterlab\n```\n\n----------------------------------------\n\nTITLE: Adding Sonnet 3.5 v2 in Javascript\nDESCRIPTION: This feature adds support for the Sonnet 3.5 v2 model within the LlamaIndex community package, allowing access to the updated version of the Sonnet model.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/packages/community/CHANGELOG.md#_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\n- a5a75f6: feat: added sonnet 3.5 v2\n```\n\n----------------------------------------\n\nTITLE: Installing Deno Kernel for Jupyter\nDESCRIPTION: This snippet shows how to install the Deno kernel for Jupyter. The Deno kernel allows running Deno code directly within Jupyter notebooks. It requires Deno and Jupyter to be installed prior to running the command. The `--unstable` flag indicates that unstable Deno features are being used.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/examples/jupyter/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndeno jupyter --unstable --install\n```\n\n----------------------------------------\n\nTITLE: Create Changeset\nDESCRIPTION: This command creates a new changeset using the changesets tool. Changesets are used for managing versions and generating changelogs. It should be run in the root directory.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/CONTRIBUTING.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npnpm changeset\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex via yarn\nDESCRIPTION: This command installs the LlamaIndex library using yarn, another package manager for Node.js projects.  It adds LlamaIndex as a dependency, enabling the use of its functionalities for integrating LLMs with your data.\nSOURCE: https://github.com/run-llama/llamaindexts.git/blob/main/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nyarn add llamaindex\n```"
  }
]
[
  {
    "owner": "argoproj",
    "repo": "argo-workflows",
    "content": "TITLE: Passing Artifacts Between Workflow Steps in Argo\nDESCRIPTION: This YAML defines an Argo Workflow that generates an artifact in one step and consumes it in a subsequent step. It showcases how to pass data between steps using artifacts.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/artifacts.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: artifact-passing-\nspec:\n  entrypoint: artifact-example\n  templates:\n  - name: artifact-example\n    steps:\n    - - name: generate-artifact\n        template: hello-world-to-file\n    - - name: consume-artifact\n        template: print-message-from-file\n        arguments:\n          artifacts:\n          # bind message to the hello-art artifact\n          # generated by the generate-artifact step\n          - name: message\n            from: \"{{steps.generate-artifact.outputs.artifacts.hello-art}}\"\n\n  - name: hello-world-to-file\n    container:\n      image: busybox\n      command: [sh, -c]\n      args: [\"echo hello world | tee /tmp/hello_world.txt\"]\n    outputs:\n      artifacts:\n      # generate hello-art artifact from /tmp/hello_world.txt\n      # artifacts can be directories as well as files\n      - name: hello-art\n        path: /tmp/hello_world.txt\n\n  - name: print-message-from-file\n    inputs:\n      artifacts:\n      # unpack the message input artifact\n      # and put it at /tmp/message\n      - name: message\n        path: /tmp/message\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"cat /tmp/message\"]\n```\n\n----------------------------------------\n\nTITLE: Configure GCS Artifact\nDESCRIPTION: This YAML snippet configures a GCS artifact in Argo Workflows. It specifies the bucket, key, and the Kubernetes secret containing the service account key for accessing the GCS bucket. If running on GKE with Workload Identity, the serviceAccountKeySecret is not needed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nartifacts:\n  - name: message\n    path: /tmp/message\n    gcs:\n      bucket: my-bucket-name\n      key: path/in/bucket\n      # serviceAccountKeySecret is a secret selector.\n      # It references the k8s secret named 'my-gcs-credentials'.\n      # This secret is expected to have the key 'serviceAccountKey',\n      # containing the base64 encoded credentials\n      # to the bucket.\n      #\n      # If it's running on GKE and Workload Identity is used,\n      # serviceAccountKeySecret is not needed.\n      serviceAccountKeySecret:\n        name: my-gcs-credentials\n        key: serviceAccountKey\n```\n\n----------------------------------------\n\nTITLE: WorkflowTemplate with Global Parameter YAML\nDESCRIPTION: This example showcases how to use global parameters within a WorkflowTemplate. The Workflow defines a global parameter 'global-parameter' and the WorkflowTemplate references it in the container's arguments.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: hello-world-template-global-arg\nspec:\n  templates:\n    - name: hello-world\n      container:\n        image: busybox\n        command: [echo]\n        args: [\"{{workflow.parameters.global-parameter}}\"]\n```\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-wf-global-arg-\nspec:\n  entrypoint: print-message\n  arguments:\n    parameters:\n      - name: global-parameter\n        value: hello\n  templates:\n    - name: print-message\n      steps:\n        - - name: hello-world\n            templateRef:\n              name: hello-world-template-global-arg\n              template: hello-world\n```\n\n----------------------------------------\n\nTITLE: Invalid WorkflowTemplate (v2.4-2.6) YAML\nDESCRIPTION: This example demonstrates an *invalid* WorkflowTemplate definition for Argo Workflows versions 2.4-2.6.  It includes an `entrypoint` field, which was not supported in those versions. WorkflowTemplates in these versions only supported the `templates` and `arguments` fields.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: workflow-template-submittable\nspec:\n  entrypoint: print-message     # Fields other than \"arguments\" and \"templates\" not supported in v2.4 - v2.6\n  arguments:\n    parameters:\n      - name: message\n        value: hello world\n  templates:\n    - name: print-message\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: busybox\n        command: [echo]\n        args: [\"{{inputs.parameters.message}}\"]\n```\n\n----------------------------------------\n\nTITLE: Defining Hello World Argo Workflow YAML\nDESCRIPTION: This YAML defines an Argo Workflow that runs a simple \"hello world\" container on a Kubernetes cluster. It specifies a `Workflow` resource with a single `template` named `hello-world`. The `hello-world` template uses the `busybox` image and executes `echo \"hello world\"`. Requires Argo Workflows to be installed on the Kubernetes cluster.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/hello-world.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow                  # new type of k8s spec\nmetadata:\n  generateName: hello-world-    # name of the workflow spec\nspec:\n  entrypoint: hello-world       # invoke the hello-world template\n  templates:\n    - name: hello-world         # name of the template\n      container:\n        image: busybox\n        command: [ echo ]\n        args: [ \"hello world\" ]\n        resources: # limit the resources\n          limits:\n            memory: 32Mi\n            cpu: 100m\n```\n\n----------------------------------------\n\nTITLE: Configuring retryStrategy in WorkflowSpec (YAML)\nDESCRIPTION: This snippet demonstrates how to configure a `retryStrategy` within a `WorkflowSpec` in Argo Workflows. It sets a limit on the number of retries and defines a container to be executed. The container simulates a failure with a 66% probability. The `limit` parameter controls the maximum number of retry attempts.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/retries.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: retry-container-\nspec:\n  entrypoint: retry-container\n  templates:\n  - name: retry-container\n    retryStrategy:\n      limit: \"10\"\n    container:\n      image: python:alpine3.6\n      command: [\"python\", -c]\n      # fail with a 66% probability\n      args: [\"import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\"]\n```\n\n----------------------------------------\n\nTITLE: Define Workflow with Parameter Substitution in Argo YAML\nDESCRIPTION: This YAML snippet defines a basic Argo Workflow that demonstrates parameter substitution.  It defines an entrypoint `print-message` which takes an input parameter `message` and passes it to a container. The `{{inputs.parameters.message}}` variable is used to access the message parameter value within the container's arguments. This example illustrates how to pass data into container arguments using workflow parameters.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-parameters-\nspec:\n  entrypoint: print-message\n  arguments:\n    parameters:\n      - name: message\n        value: hello world\n  templates:\n    - name: print-message\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: busybox\n        command: [ echo ]\n        args: [ \"{{inputs.parameters.message}}\" ]\n```\n\n----------------------------------------\n\nTITLE: Simple Template Tag Usage in Argo YAML\nDESCRIPTION: This YAML snippet shows how to use a simple template tag in Argo Workflows.  The `{{ inputs.parameters.message }}` tag is used to reference the value of the `message` input parameter. Although whitespace is allowed between the braces and the variable name, it is recommended to avoid whitespace for consistent interpolation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nargs: [ \"{{ inputs.parameters.message }}\" ]\n```\n\n----------------------------------------\n\nTITLE: ContainerSet Template Example\nDESCRIPTION: Defines a workflow using a ContainerSet template to run multiple containers within a single pod, sharing data via an emptyDir volume.  The 'main' container depends on containers 'a' and 'b'. The example demonstrates writing to a shared workspace and then outputting the combined message.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/container-set-template.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: container-set-template-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      volumes:\n        - name: workspace\n          emptyDir: { }\n      containerSet:\n        volumeMounts:\n          - mountPath: /workspace\n            name: workspace\n        containers:\n          - name: a\n            image: argoproj/argosay:v2\n            command: [sh, -c]\n            args: [\"echo 'a: hello world' >> /workspace/message\"]\n          - name: b\n            image: argoproj/argosay:v2\n            command: [sh, -c]\n            args: [\"echo 'b: hello world' >> /workspace/message\"]\n          - name: main\n            image: argoproj/argosay:v2\n            command: [sh, -c]\n            args: [\"echo 'main: hello world' >> /workspace/message\"]\n            dependencies:\n              - a\n              - b\n      outputs:\n        parameters:\n          - name: message\n            valueFrom:\n              path: /workspace/message\n```\n\n----------------------------------------\n\nTITLE: Defining Argo Workflow Steps in YAML\nDESCRIPTION: This YAML defines an Argo Workflow with multiple steps, demonstrating both sequential and parallel execution. It defines two templates: `hello-hello-hello`, which orchestrates the steps, and `print-message`, which simply prints a message. The `hello-hello-hello` template uses the `steps` field to define the execution order and parallelism. The steps `hello2a` and `hello2b` will run in parallel due to the single dash, while `hello1` runs sequentially before them.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/steps.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: steps-\nspec:\n  entrypoint: hello-hello-hello\n\n  # This spec contains two templates: hello-hello-hello and print-message\n  templates:\n  - name: hello-hello-hello\n    # Instead of just running a container\n    # This template has a sequence of steps\n    steps:\n    - - name: hello1            # hello1 is run before the following steps\n        template: print-message\n        arguments:\n          parameters:\n          - name: message\n            value: \"hello1\"\n    - - name: hello2a           # double dash => run after previous step\n        template: print-message\n        arguments:\n          parameters:\n          - name: message\n            value: \"hello2a\"\n      - name: hello2b           # single dash => run in parallel with previous step\n        template: print-message\n        arguments:\n          parameters:\n          - name: message\n            value: \"hello2b\"\n\n  # This is the same template as from the previous example\n  - name: print-message\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: busybox\n      command: [echo]\n      args: [\"{{inputs.parameters.message}}\"]\n```\n\n----------------------------------------\n\nTITLE: Argo Workflow: Data Template with S3 Artifact Source and Transformations\nDESCRIPTION: This YAML snippet defines an Argo workflow template that uses the `data` template to generate artifacts.  It specifies an S3 bucket as the data source and applies two transformations: filtering for files ending with `.pdf` and appending `.ready` to the filenames.  The `accessKeySecret` and `secretKeySecret` are used to authenticate with the S3 bucket.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/data-sourcing-and-transformation.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- name: generate-artifacts\n  data:\n    source:             # Define a source for the data, only a single \"source\" is permitted\n      artifactPaths:    # A predefined source: Generate a list of all artifact paths in a given repository\n        s3:             # Source from an S3 bucket\n          bucket: test\n          endpoint: minio:9000\n          insecure: true\n          accessKeySecret:\n            name: my-minio-cred\n            key: accesskey\n          secretKeySecret:\n            name: my-minio-cred\n            key: secretkey\n    transformation:     # The source is then passed to be transformed by transformations defined here\n      - expression: \"filter(data, {# endsWith \\\".pdf\\\"})\"\n      - expression: \"map(data, {# + \\\".ready\\\"})\"\n```\n\n----------------------------------------\n\nTITLE: Argo Server CLI Environment Variable Override\nDESCRIPTION: Illustrates how to override Argo Server CLI parameters using environment variables. The example shows how `ARGO_MANAGED_NAMESPACE` can be used to set the managed namespace for the server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/environment-variables.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nargo server --managed-namespace=argo\n```\n\n----------------------------------------\n\nTITLE: Installing MinIO with Helm\nDESCRIPTION: These commands install MinIO into a Kubernetes cluster using Helm.  It adds the MinIO Helm repository, updates the repository, and installs MinIO with a LoadBalancer service type. The fullnameOverride ensures the service name is correctly configured.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add minio https://charts.min.io/ # official minio Helm charts\nhelm repo update\nhelm install argo-artifacts minio/minio --set service.type=LoadBalancer --set fullnameOverride=argo-artifacts\n```\n\n----------------------------------------\n\nTITLE: Applying CronWorkflow configuration using kubectl\nDESCRIPTION: This command applies a CronWorkflow configuration from a YAML file using kubectl. It allows for declarative management of CronWorkflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cron-workflows.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f\n```\n\n----------------------------------------\n\nTITLE: Using Service Account in a Workflow\nDESCRIPTION: This YAML configures an Argo Workflow to use a specific service account. The `serviceAccountName` field specifies the name of the service account to use for the workflow's pods, enabling them to access AWS resources based on the attached IAM role.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nspec:\n  serviceAccountName: myserviceaccount\n```\n\n----------------------------------------\n\nTITLE: Configure S3 Artifact Repository\nDESCRIPTION: This YAML snippet configures an S3-compatible artifact repository in the `workflow-controller-configmap`. It specifies the bucket name, endpoint, access key, and secret key. The `accessKeySecret` and `secretKeySecret` fields reference Kubernetes Secrets that store the access key and secret key for the S3 bucket. `useSDKCreds` tells argo to use AWS SDK's default provider chain, enable for things like IRSA support\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_29\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  artifactRepository: |\n    s3:\n      bucket: my-bucket\n      keyFormat: prefix/in/bucket     #optional\n      endpoint: my-minio-endpoint.default:9000        #AWS => s3.amazonaws.com; GCS => storage.googleapis.com\n      insecure: true                  #omit for S3/GCS. Needed when minio runs without TLS\n      accessKeySecret:                #omit if accessing via AWS IAM\n        name: my-minio-cred\n        key: accessKey\n      secretKeySecret:                #omit if accessing via AWS IAM\n        name: my-minio-cred\n        key: secretKey\n      useSDKCreds: true               #tells argo to use AWS SDK's default provider chain, enable for things like IRSA support\n```\n\n----------------------------------------\n\nTITLE: Create CronWorkflow using Argo Workflows API in Java\nDESCRIPTION: This snippet demonstrates how to create a CronWorkflow using the Argo Workflows CronWorkflowServiceApi in Java. It initializes the ApiClient, configures authentication, and calls the cronWorkflowServiceCreateCronWorkflow method with the namespace and CronWorkflow request body. The example includes error handling for API exceptions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/CronWorkflowServiceApi.md#_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.CronWorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    CronWorkflowServiceApi apiInstance = new CronWorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    IoArgoprojWorkflowV1alpha1CreateCronWorkflowRequest body = new IoArgoprojWorkflowV1alpha1CreateCronWorkflowRequest(); // IoArgoprojWorkflowV1alpha1CreateCronWorkflowRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1CronWorkflow result = apiInstance.cronWorkflowServiceCreateCronWorkflow(namespace, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling CronWorkflowServiceApi#cronWorkflowServiceCreateCronWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: List Argo Workflows (Bash)\nDESCRIPTION: This snippet lists all submitted workflows in the 'argo' namespace using the Argo CLI. It displays information about each workflow, such as its name, status, and start time. This is useful for monitoring and managing workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/quick-start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nargo list -n argo\n```\n\n----------------------------------------\n\nTITLE: Suspending Workflow Execution in Argo\nDESCRIPTION: This YAML snippet defines a 'delay' template of type 'suspend'. It suspends workflow execution for a specified duration of 20 seconds. The workflow can be resumed manually from the CLI, API endpoint, or UI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-concepts.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n  - name: delay\n    suspend:\n      duration: \"20s\"\n```\n\n----------------------------------------\n\nTITLE: Override Parameter using Argo CLI (Bash)\nDESCRIPTION: This bash snippet shows how to override a workflow parameter using the Argo CLI's `argo submit` command.  The `-p` flag is used to set the `message` parameter to \"goodbye world\", overriding the default value defined in the workflow YAML file. `arguments-parameters.yaml` is assumed to be the name of the workflow definition file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/parameters.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo submit arguments-parameters.yaml -p message=\"goodbye world\"\n```\n\n----------------------------------------\n\nTITLE: Configure Azure Artifact with SAS Token\nDESCRIPTION: This YAML snippet configures an Argo artifact to be stored in Azure Blob Storage using a SAS token. It specifies the endpoint, container, and blob path, and uses an `accountKeySecret` to reference a Kubernetes Secret containing the SAS token. The Secret name is `my-azure-storage-credentials` and the key within the Secret is `shared-access-key`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_27\n\nLANGUAGE: yaml\nCODE:\n```\nartifacts:\n  - name: message\n    path: /tmp/message\n    azure:\n      endpoint: https://mystorageaccountname.blob.core.windows.net\n      container: my-container-name\n      blob: path/in/container\n      # accountKeySecret is a secret selector.\n      # It references the Kubernetes Secret named 'my-azure-storage-credentials'.\n      # This secret is expected to have the key 'shared-access-key',\n      # containing the base64 encoded shared access signature to the storage account.\n      accountKeySecret:\n        name: my-azure-storage-credentials\n        key: shared-access-key\n```\n\n----------------------------------------\n\nTITLE: Defining Template Inputs in Argo Workflows (YAML)\nDESCRIPTION: This YAML snippet illustrates how to define input parameters for a template. It specifies a parameter named 'template-param-1' within the 'inputs' section of the template definition. This parameter can then be used by the template during execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-inputs.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninputs:\n  parameters:\n  - name: template-param-1\n```\n\n----------------------------------------\n\nTITLE: Argo Workflow with Exit Handler (YAML)\nDESCRIPTION: This YAML configuration defines an Argo Workflow with an exit handler. The `onExit` field specifies the template to execute after the main workflow completes, regardless of its success or failure. The exit handler template uses conditional steps based on the `workflow.status` variable to perform different actions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/exit-handlers.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: exit-handlers-\nspec:\n  entrypoint: intentional-fail\n  onExit: exit-handler                  # invoke exit-handler template at end of the workflow\n  templates:\n  # primary workflow template\n  - name: intentional-fail\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"echo intentional failure; exit 1\"]\n\n  # Exit handler templates\n  # After the completion of the entrypoint template, the status of the\n  # workflow is made available in the global variable {{workflow.status}}.\n  # {{workflow.status}} will be one of: Succeeded, Failed, Error\n  - name: exit-handler\n    steps:\n    - - name: notify\n        template: send-email\n      - name: celebrate\n        template: celebrate\n        when: \"{{workflow.status}} == Succeeded\"\n      - name: cry\n        template: cry\n        when: \"{{workflow.status}} != Succeeded\"\n  - name: send-email\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"echo send e-mail: {{workflow.name}} {{workflow.status}} {{workflow.duration}}\"]\n  - name: celebrate\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"echo hooray!\"]\n  - name: cry\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"echo boohoo!\"]\n```\n\n----------------------------------------\n\nTITLE: Casting to Integer with Expression in Argo\nDESCRIPTION: This example demonstrates how to cast a parameter to an integer using the `asInt` function within an expression.  It takes the `my-int-param` input parameter and converts it to an integer value. This is useful when dealing with parameters that are passed as strings but need to be used as numbers.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nasInt(inputs.parameters['my-int-param'])\n```\n\n----------------------------------------\n\nTITLE: Workflow using workflowTemplateRef with Arguments YAML\nDESCRIPTION: This example shows how to create a Workflow from a WorkflowTemplate using `workflowTemplateRef` and passing arguments. The arguments defined in the Workflow will be merged with the arguments defined in the WorkflowTemplate.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: workflow-template-hello-world-\nspec:\n  entrypoint: print-message\n  arguments:\n    parameters:\n      - name: message\n        value: \"from workflow\"\n  workflowTemplateRef:\n    name: workflow-template-submittable\n```\n\n----------------------------------------\n\nTITLE: Define a CronWorkflow YAML\nDESCRIPTION: Defines a `CronWorkflow` resource with a specified schedule, concurrency policy, starting deadline, and workflow specification. The workflow runs every minute, replaces old workflows, starts immediately if missed, and executes a date command within an Alpine container. It showcases the core structure of a CronWorkflow definition.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cron-workflows.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: CronWorkflow\nmetadata:\n  name: test-cron-wf\nspec:\n  schedules:\n    - \"* * * * *\"\n  concurrencyPolicy: \"Replace\"\n  startingDeadlineSeconds: 0\n  workflowSpec:\n    entrypoint: date\n    templates:\n    - name: date\n      container:\n        image: alpine:3.6\n        command: [sh, -c]\n        args: [\"date; sleep 90\"]\n```\n\n----------------------------------------\n\nTITLE: GCP Authentication and Configuration\nDESCRIPTION: This snippet logs into GCP, sets the project configuration, creates an auto-scaling Kubernetes cluster, and retrieves the cluster credentials. It's a prerequisite for deploying and running Argo Workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/stress-testing.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Login to GCP:\ngcloud auth login\n\n# Set-up your config (if needed):\ngcloud config set project alex-sb\n\n# Create a cluster (default region is us-west-2, if you're not in west of the USA, you might want at different region):\ngcloud container clusters create-auto argo-workflows-stress-1\n\n# Get credentials:\ngcloud container clusters get-credentials argo-workflows-stress-1\n```\n\n----------------------------------------\n\nTITLE: Workflow Spec with forceFinalizerRemoval YAML\nDESCRIPTION: This YAML snippet shows how to configure a Workflow spec to force the removal of the finalizer even if Artifact GC fails. The `forceFinalizerRemoval` flag in the `artifactGC` section is set to `true`. This automatically removes the finalizer during workflow deletion if garbage collection fails, introduced in Argo Workflows >= 3.5.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/artifacts.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  artifactGC:\n    strategy: OnWorkflowDeletion\n    forceFinalizerRemoval: true\n```\n\n----------------------------------------\n\nTITLE: ClusterWorkflowTemplate with entrypoint and arguments in YAML\nDESCRIPTION: This YAML defines a `ClusterWorkflowTemplate` with an `entrypoint` and `arguments` to be used when creating a `Workflow` from it.  The `print-message` template takes a message parameter and prints it using a busybox container. Dependencies: Argo Workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cluster-workflow-templates.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: ClusterWorkflowTemplate\nmetadata:\n  name: cluster-workflow-template-submittable\nspec:\n  entrypoint: print-message\n  arguments:\n    parameters:\n      - name: message\n        value: hello world\n  templates:\n    - name: print-message\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: busybox\n        command: [echo]\n        args: [\"{{inputs.parameters.message}}\"]\n\n```\n\n----------------------------------------\n\nTITLE: Configure GCS Artifact Repository\nDESCRIPTION: This YAML snippet configures a Google Cloud Storage (GCS) artifact repository in the `workflow-controller-configmap`. It specifies the bucket name and references a Kubernetes Secret (`serviceAccountKeySecret`) containing the Google Cloud service account key needed to access the bucket.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_30\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  artifactRepository: |\n    gcs:\n      bucket: my-bucket\n      keyFormat: prefix/in/bucket/{{workflow.name}}/{{pod.name}}     #it should reference workflow variables, such as \"{{workflow.name}}/{{pod.name}}\"\n      serviceAccountKeySecret:\n        name: my-gcs-credentials\n        key: serviceAccountKey\n```\n\n----------------------------------------\n\nTITLE: Complete Argo Workflow Example with Input Parameters (YAML)\nDESCRIPTION: This YAML snippet provides a complete example of an Argo Workflow that defines a workflow parameter, passes it to a DAG task, and uses it in a script.  The workflow takes 'workflow-param-1' as an argument, passes it to 'step-template-a' as 'template-param-1', and the script echoes the value of 'template-param-1'.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-inputs.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: example-\nspec:\n  entrypoint: main\n  arguments:\n    parameters:\n    - name: workflow-param-1\n  templates:\n  - name: main\n    dag:\n      tasks:\n      - name: step-A \n        template: step-template-a\n        arguments:\n          parameters:\n          - name: template-param-1\n            value: \"{{workflow.parameters.workflow-param-1}}\"\n \n  - name: step-template-a\n    inputs:\n      parameters:\n        - name: template-param-1\n    script:\n      image: alpine\n      command: [/bin/sh]\n      source: |\n          echo \"{{inputs.parameters.template-param-1}}\"\n```\n\n----------------------------------------\n\nTITLE: Looping with Items (Basic) in Argo Workflows (YAML)\nDESCRIPTION: This example uses `withItems` to iterate over a list of strings in an Argo Workflow. It invokes the 'print-message' template once for each item in the list, substituting the string for the `{{item}}` placeholder.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/loops.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: loops-\nspec:\n  entrypoint: loop-example\n  templates:\n  - name: loop-example\n    steps:\n    - - name: print-message-loop\n        template: print-message\n        arguments:\n          parameters:\n          - name: message\n            value: \"{{item}}\"\n        withItems:              # invoke print-message once for each item in parallel\n        - hello world           # item 1\n        - goodbye world         # item 2\n\n  - name: print-message\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: busybox\n      command: [echo]\n      args: [\"{{inputs.parameters.message}}\"]\n```\n\n----------------------------------------\n\nTITLE: Creating IAM User and Access Key\nDESCRIPTION: These commands create an IAM user, attach a policy, and create an access key for that user. `$mybucket-user` is the username and `policy.json` contains the policy document granting S3 access. The access key is saved to `access-key.json`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\naws iam create-user --user-name $mybucket-user\naws iam put-user-policy --user-name $mybucket-user --policy-name $mybucket-policy --policy-document file://policy.json\naws iam create-access-key --user-name $mybucket-user > access-key.json\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Workflow Spec in Argo\nDESCRIPTION: This YAML snippet demonstrates a basic Argo Workflow spec. It defines a 'hello-world' template that executes the 'echo' command within a 'busybox' container to print 'hello world'. The 'entrypoint' specifies the starting template.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-concepts.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-  # Name of this Workflow\nspec:\n  entrypoint: hello-world     # Defines \"hello-world\" as the \"main\" template\n  templates:\n  - name: hello-world         # Defines the \"hello-world\" template\n    container:\n      image: busybox\n      command: [echo]\n      args: [\"hello world\"]   # This template runs \"echo\" in the \"busybox\" image with arguments \"hello world\"\n```\n\n----------------------------------------\n\nTITLE: Define WorkflowTemplate in Argo Workflows (YAML)\nDESCRIPTION: Defines a reusable `WorkflowTemplate` named `workflow-template-submittable`. This template takes a `message` parameter as input and prints it using a busybox container. The template serves as the basis for child workflows triggered by the parent workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-of-workflows.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: workflow-template-submittable\nspec:\n  entrypoint: print-message\n  arguments:\n    parameters:\n      - name: message\n        value: hello world\n  templates:\n    - name: print-message\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: busybox\n        command: [echo]\n        args: [\"{{inputs.parameters.message}}\"]\n```\n\n----------------------------------------\n\nTITLE: Workflow Definition with Output Parameters (YAML)\nDESCRIPTION: This YAML defines an Argo Workflow that demonstrates the use of output parameters. It consists of two steps: `generate-parameter` which writes 'hello world' to a file, and `consume-parameter` which prints the content of that file. The `hello-param` output parameter captures the content of the `/tmp/hello_world.txt` file from the `hello-world-to-file` template and passes it as the `message` input to the `print-message` template.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/output-parameters.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: output-parameter-\nspec:\n  entrypoint: output-parameter\n  templates:\n  - name: output-parameter\n    steps:\n    - - name: generate-parameter\n        template: hello-world-to-file\n    - - name: consume-parameter\n        template: print-message\n        arguments:\n          parameters:\n          # Pass the hello-param output from the generate-parameter step as the message input to print-message\n          - name: message\n            value: \"{{steps.generate-parameter.outputs.parameters.hello-param}}\"\n\n  - name: hello-world-to-file\n    container:\n      image: busybox\n      command: [sh, -c]\n      args: [\"echo -n hello world > /tmp/hello_world.txt\"]  # generate the content of hello_world.txt\n    outputs:\n      parameters:\n      - name: hello-param  # name of output parameter\n        valueFrom:\n          path: /tmp/hello_world.txt # set the value of hello-param to the contents of this hello-world.txt\n\n  - name: print-message\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: busybox\n      command: [echo]\n      args: [\"{{inputs.parameters.message}}\"]\n```\n\n----------------------------------------\n\nTITLE: Exporting Argo Workflows Resources YAML\nDESCRIPTION: This command exports all Argo Workflows-related resources (wf, cwf, cwft, wftmpl) from all namespaces into a YAML file named 'backup.yaml'. It is a crucial step in backing up your Argo Workflows configuration.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/disaster-recovery.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get wf,cwf,cwft,wftmpl -A -o yaml > backup.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining a DAG Workflow in Argo Workflows\nDESCRIPTION: This YAML defines a simple diamond-shaped DAG workflow in Argo Workflows. Step A runs first, then B and C run in parallel after A completes, and finally, D runs after both B and C are finished. It demonstrates basic task dependencies within a DAG.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/dag.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: dag-diamond-\nspec:\n  entrypoint: diamond\n  templates:\n  - name: echo\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: alpine:3.7\n      command: [echo, \"{{inputs.parameters.message}}\"]\n  - name: diamond\n    dag:\n      tasks:\n      - name: A\n        template: echo\n        arguments:\n          parameters: [{name: message, value: A}]\n      - name: B\n        dependencies: [A]\n        template: echo\n        arguments:\n          parameters: [{name: message, value: B}]\n      - name: C\n        dependencies: [A]\n        template: echo\n        arguments:\n          parameters: [{name: message, value: C}]\n      - name: D\n        dependencies: [B, C]\n        template: echo\n        arguments:\n          parameters: [{name: message, value: D}]\n```\n\n----------------------------------------\n\nTITLE: Workflow Definition with Global Parameters (YAML)\nDESCRIPTION: This YAML snippet defines an Argo Workflow that uses globally scoped parameters. The `log-level` parameter is defined in `spec.arguments.parameters` and is accessed in the container's environment variables using `{{workflow.parameters.log-level}}`. This allows the same parameter to be used in multiple steps of the workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/parameters.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: global-parameters-\nspec:\n  entrypoint: A\n  arguments:\n    parameters:\n    - name: log-level\n      value: INFO\n\n  templates:\n  - name: A\n    container:\n      image: containerA\n      env:\n      - name: LOG_LEVEL\n        value: \"{{workflow.parameters.log-level}}\"\n      command: [runA]\n  - name: B\n    container:\n      image: containerB\n      env:\n      - name: LOG_LEVEL\n        value: \"{{workflow.parameters.log-level}}\"\n      command: [runB]\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Job with Resource Template (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to create a Kubernetes Job using the resource template in an Argo workflow. It defines a workflow with a single template that uses the `resource` field to specify the `create` action. It includes a success and failure condition and a manifest for the Job, which executes a Perl script to calculate Pi.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/kubernetes-resources.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# in a workflow. The resource template type accepts any k8s manifest\n# (including CRDs) and can perform any `kubectl` action against it (e.g. create,\n# apply, delete, patch).\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: k8s-jobs-\nspec:\n  entrypoint: pi-tmpl\n  templates:\n  - name: pi-tmpl\n    resource:                   # indicates that this is a resource template\n      action: create            # can be any kubectl action (e.g. create, delete, apply, patch)\n      # The successCondition and failureCondition are optional expressions.\n      # If failureCondition is true, the step is considered failed.\n      # If successCondition is true, the step is considered successful.\n      # They use kubernetes label selection syntax and can be applied against any field\n      # of the resource (not just labels). Multiple AND conditions can be represented by comma\n      # delimited expressions.\n      # For more details: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n      successCondition: status.succeeded > 0\n      failureCondition: status.failed > 3\n      manifest: |               #put your kubernetes spec here\n        apiVersion: batch/v1\n        kind: Job\n        metadata:\n          generateName: pi-job-\n        spec:\n          template:\n            metadata:\n              name: pi\n            spec:\n              containers:\n              - name: pi\n                image: perl\n                command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n              restartPolicy: Never\n          backoffLimit: 4\n```\n\n----------------------------------------\n\nTITLE: Argo Workflow Definition with Script Templates (YAML)\nDESCRIPTION: This YAML defines an Argo Workflow that generates a random integer using Bash, Python and JavaScript scripts and then prints a message using the generated value. It showcases the `script` template, which allows embedding scripts directly within the workflow definition. The result of the `gen-random-int-bash` template is accessed using `{{steps.generate.outputs.result}}`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/scripts-and-results.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: scripts-bash-\nspec:\n  entrypoint: bash-script-example\n  templates:\n  - name: bash-script-example\n    steps:\n    - - name: generate\n        template: gen-random-int-bash\n    - - name: print\n        template: print-message\n        arguments:\n          parameters:\n          - name: message\n            value: \"{{steps.generate.outputs.result}}\"  # The result of the here-script\n\n  - name: gen-random-int-bash\n    script:\n      image: debian:9.4\n      command: [bash]\n      source: |                                         # Contents of the here-script\n        cat /dev/urandom | od -N2 -An -i | awk -v f=1 -v r=100 '{printf \"%i\\n\", f + r * $1 / 65536}'\n\n  - name: gen-random-int-python\n    script:\n      image: python:alpine3.6\n      command: [python]\n      source: |\n        import random\n        i = random.randint(1, 100)\n        print(i)\n\n  - name: gen-random-int-javascript\n    script:\n      image: node:9.1-alpine\n      command: [node]\n      source: |\n        var rand = Math.floor(Math.random() * 100);\n        console.log(rand);\n\n  - name: print-message\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"echo result was: {{inputs.parameters.message}}\"]\n```\n\n----------------------------------------\n\nTITLE: Enabling Workflow Archive with Postgres Configuration (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to enable workflow archiving and configure the connection to a Postgres database within the `persistence` section of the Argo Workflow controller configuration. It specifies the database host, port, database name, table name, and references secrets for the username and password.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-archive.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\npersistence:\n  archive: true\n  postgresql:\n    host: localhost\n    port: 5432\n    database: postgres\n    tableName: argo_workflows\n    userNameSecret:\n      name: argo-postgres-config\n      key: username\n    passwordSecret:\n      name: argo-postgres-config\n      key: password\n```\n\n----------------------------------------\n\nTITLE: Argo Workflow Secret Example\nDESCRIPTION: This example demonstrates how to access secrets within an Argo Workflow using both environment variables and volume mounts. It defines a workflow that prints the secret retrieved from both methods. A prerequisite is the existence of a Kubernetes secret named `my-secret` with a key `mypassword`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/secrets.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n# To run this example, first create the secret by running:\n# kubectl create secret generic my-secret --from-literal=mypassword=S00perS3cretPa55word\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: secret-example-\nspec:\n  entrypoint: print-secrets\n  # To access secrets as files, add a volume entry in spec.volumes[] and\n  # then in the container template spec, add a mount using volumeMounts.\n  volumes:\n  - name: my-secret-vol\n    secret:\n      secretName: my-secret     # name of an existing k8s secret\n  templates:\n  - name: print-secrets\n    container:\n      image: alpine:3.7\n      command: [sh, -c]\n      args: ['\n        echo \"secret from env: $MYSECRETPASSWORD\";\n        echo \"secret from file: `cat /secret/mountpath/mypassword`\"\n      ']\n      # To access secrets as environment variables, use the k8s valueFrom and\n      # secretKeyRef constructs.\n      env:\n      - name: MYSECRETPASSWORD  # name of env var\n        valueFrom:\n          secretKeyRef:\n            name: my-secret     # name of an existing k8s secret\n            key: mypassword     # 'key' subcomponent of the secret\n      volumeMounts:\n      - name: my-secret-vol     # mount file containing secret at /secret/mountpath\n        mountPath: \"/secret/mountpath\"\n\n```\n\n----------------------------------------\n\nTITLE: Workflow-level Lifecycle Hook with HTTP Notification\nDESCRIPTION: This example demonstrates a workflow-level LifecycleHook that triggers an HTTP notification when the workflow status is 'Running' and upon exit.  It defines a 'running' hook with an expression checking the workflow status and an 'exit' hook. Both hooks execute the 'http' template. The workflow consists of a main entrypoint that executes a 'heads' template.  The HTTP template defines an HTTP request to a dummy API endpoint.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/lifecyclehook.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: lifecycle-hook-\nspec:\n  entrypoint: main\n  hooks:\n    exit:\n      template: http\n    running:\n      expression: workflow.status == \"Running\"\n      template: http\n  templates:\n    - name: main\n      steps:\n        - - name: step1\n            template: heads\n\n    - name: heads\n      container:\n        image: alpine:3.6\n        command: [sh, -c]\n        args: [\"echo \\\"it was heads\\\"\"]\n\n    - name: http\n      http:\n        url: http://dummy.restapiexample.com/api/v1/employees\n```\n\n----------------------------------------\n\nTITLE: Defining Template Outputs (Parameters and Artifacts) (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define output parameters and artifacts for a template. It specifies an output parameter named 'output-param-1' whose value is read from the file '/p1.txt', and an output artifact named 'output-artifact-1' located at the path '/some-directory'.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-inputs.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\noutputs:\n  parameters:\n    - name: output-param-1\n      valueFrom:\n        path: /p1.txt\n  artifacts:\n    - name: output-artifact-1\n      path: /some-directory\n```\n\n----------------------------------------\n\nTITLE: Configure Executor Resource Requests - YAML\nDESCRIPTION: This YAML snippet shows how to configure resource requests and limits for the executor in Argo Workflows. These resources determine the resources available to individual workflow steps during execution.  The snippet demonstrates setting `cpu` and `memory` requests and limits within the `executor.resources` configuration in the `workflow-controller-configmap.yaml` file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cost-optimisation.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nexecutor: |\n  resources:\n    requests:\n      cpu: 100m\n      memory: 64Mi\n    limits:\n      cpu: 500m\n      memory: 512Mi\n```\n\n----------------------------------------\n\nTITLE: Wait on a Specific Argo Workflow\nDESCRIPTION: Shows how to wait for a specific workflow by name.  This will block until the workflow 'my-wf' has finished executing.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_wait.md#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n# Wait on a workflow:\n\n  argo wait my-wf\n```\n\n----------------------------------------\n\nTITLE: Defining Volume with volumeClaimTemplates in Argo Workflow (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to dynamically create a volume using `volumeClaimTemplates` in an Argo Workflow. It defines a PersistentVolumeClaim (PVC) named 'workdir' with ReadWriteOnce access mode and a storage request of 1Gi. This volume is then used by two steps in the workflow to generate and print a message from a file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/volumes.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: volumes-pvc-\nspec:\n  entrypoint: volumes-pvc-example\n  volumeClaimTemplates:                 # define volume, same syntax as k8s Pod spec\n  - metadata:\n      name: workdir                     # name of volume claim\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi                  # Gi => 1024 * 1024 * 1024\n\n  templates:\n  - name: volumes-pvc-example\n    steps:\n    - - name: generate\n        template: hello-world-to-file\n    - - name: print\n        template: print-message-from-file\n\n  - name: hello-world-to-file\n    container:\n      image: busybox\n      command: [sh, -c]\n      args: [\"echo generating message in volume; echo hello world | tee /mnt/vol/hello_world.txt\"]\n      # Mount workdir volume at /mnt/vol before invoking the container\n      volumeMounts:                     # same syntax as k8s Pod spec\n      - name: workdir\n        mountPath: /mnt/vol\n\n  - name: print-message-from-file\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"echo getting message from volume; find /mnt/vol; cat /mnt/vol/hello_world.txt\"]\n      # Mount workdir volume at /mnt/vol before invoking the container\n      volumeMounts:                     # same syntax as k8s Pod spec\n      - name: workdir\n        mountPath: /mnt/vol\n\n```\n\n----------------------------------------\n\nTITLE: ContainerSet with Retry Strategy\nDESCRIPTION: This example demonstrates how to configure a retry strategy within a ContainerSet template. It defines the maximum number of retries and the duration for each retry attempt.  The containers 'success' and 'fail-retry' are defined, with 'fail-retry' intentionally failing with a certain probability to demonstrate the retry mechanism.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/container-set-template.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: containerset-with-retrystrategy\n  annotations:\n    workflows.argoproj.io/description: |\n      This workflow creates a ContainerSet with a retryStrategy.\nspec:\n  entrypoint: containerset-retrystrategy-example\n  templates:\n    - name: containerset-retrystrategy-example\n      containerSet:\n        retryStrategy:\n          retries: \"10\" # if fails, retry at most ten times\n          duration: 30s # retry for at most 30s\n        containers:\n          # this container completes successfully, so it won't be retried.\n          - name: success\n            image: python:alpine3.6\n            command:\n              - python\n              - -c\n            args:\n              - |\n                print(\"hi\")\n          # if fails, it will retry at most ten times.\n          - name: fail-retry\n            image: python:alpine3.6\n            command: [\"python\", -c]\n            # fail with a 66% probability\n            args: [\"import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\"]\n```\n\n----------------------------------------\n\nTITLE: Coin Flip Script in Python\nDESCRIPTION: A Python script that simulates a coin flip by generating a random integer (0 or 1) and assigning 'heads' or 'tails' to the `result` variable accordingly. It then prints the `result` to standard output. This script is used as the source for the 'flip-coin' template in the Argo Workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/conditionals.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport random\nresult = \"heads\" if random.randint(0,1) == 0 else \"tails\"\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Mapping List with Expression in Argo\nDESCRIPTION: This code snippet shows how to map a list using an expression. The `map` function applies the provided expression to each element in the list. In this example, it multiplies each element in the list `[1, 2]` by 2.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nmap([1, 2], { # * 2 })\n```\n\n----------------------------------------\n\nTITLE: Submit Argo Workflow via CLI (Bash)\nDESCRIPTION: This snippet submits an example workflow to the Argo Workflows cluster using the Argo CLI. The workflow definition is fetched from a raw GitHub URL. The '--watch' flag observes the workflow execution and reports its status.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/quick-start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo submit -n argo --watch https://raw.githubusercontent.com/argoproj/argo-workflows/main/examples/hello-world.yaml\n```\n\n----------------------------------------\n\nTITLE: Template-Level Synchronization using Mutex\nDESCRIPTION: This snippet demonstrates template-level synchronization using a mutex. A mutex limits the execution of the template to a single concurrent instance, preventing parallel executions across multiple steps, tasks, or workflows that reference the template. This ensures exclusive access for the template execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/synchronization.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"examples/synchronization-mutex-tmpl-level.yaml:3\"\n```\n\n----------------------------------------\n\nTITLE: Multiple Locks Configuration (Mutexes and Semaphores)\nDESCRIPTION: This snippet shows how to specify multiple locks (both mutexes and semaphores) within a single workflow or template. The workflow will block until it can acquire all specified locks. The mutexes are defined by name, and the semaphores are referenced via ConfigMap keys.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/synchronization.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsynchronization:\n  mutexes:\n    - name: alpha\n    - name: beta\n  semaphores:\n    - configMapKeyRef:\n        key: foo\n        name: my-config\n    - configMapKeyRef:\n        key: bar\n        name: my-config\n```\n\n----------------------------------------\n\nTITLE: Setting Namespace-Specific Workflow Parallelism - YAML\nDESCRIPTION: This YAML snippet configures the Argo Workflow Controller to limit the total number of parallel workflow executions within a single namespace to 4. This setting overrides the global parallelism limit for workflows running in the specified namespace. It's configured in the `workflow-controller-configmap.yaml` file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/parallelism.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  namespaceParallelism: \"4\"\n```\n\n----------------------------------------\n\nTITLE: Converting to JSON String with Expression in Argo\nDESCRIPTION: This snippet demonstrates converting a list to a JSON string using the `toJson` function. This is particularly useful for passing complex data structures as parameters, especially when using `withParam`. The resulting JSON string can then be parsed by subsequent steps.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_8\n\nLANGUAGE: text\nCODE:\n```\ntoJson([1, 2])\n```\n\n----------------------------------------\n\nTITLE: Passing Arguments to a DAG Template Task (YAML)\nDESCRIPTION: This YAML snippet shows how to pass arguments to a specific task within a DAG template. It specifies a parameter named 'template-param-1' with a value of 'abcd' within the 'arguments' section of the task 'step-A'.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-inputs.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndag:\n  tasks:\n  - name: step-A\n    template: step-template-a\n    arguments:\n      parameters:\n      - name: template-param-1\n        value: abcd\n```\n\n----------------------------------------\n\nTITLE: Submitting multiple workflows from stdin with Argo\nDESCRIPTION: This command submits workflows from standard input. The YAML definition of the workflow is piped into the `argo submit` command.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_submit.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncat my-wf.yaml | argo submit -\n```\n\n----------------------------------------\n\nTITLE: Workflow Definition with Daemon Container - YAML\nDESCRIPTION: This YAML defines an Argo Workflow that utilizes a daemon container to run an InfluxDB instance. The workflow includes steps to initialize the database, add entries, and consume data from the InfluxDB daemon. The `influxdb` template is marked as a daemon, and other steps refer to it via `{{steps.influx.ip}}`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/daemon-containers.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: daemon-step-\nspec:\n  entrypoint: daemon-example\n  templates:\n  - name: daemon-example\n    steps:\n    - - name: influx\n        template: influxdb              # start an influxdb as a daemon (see the influxdb template spec below)\n\n    - - name: init-database             # initialize influxdb\n        template: influxdb-client\n        arguments:\n          parameters:\n          - name: cmd\n            value: curl -XPOST 'http://{{steps.influx.ip}}:8086/query' --data-urlencode \"q=CREATE DATABASE mydb\"\n\n    - - name: producer-1                # add entries to influxdb\n        template: influxdb-client\n        arguments:\n          parameters:\n          - name: cmd\n            value: for i in $(seq 1 20); do curl -XPOST 'http://{{steps.influx.ip}}:8086/write?db=mydb' -d \"cpu,host=server01,region=uswest load=$i\" ; sleep .5 ; done\n      - name: producer-2                # add entries to influxdb\n        template: influxdb-client\n        arguments:\n          parameters:\n          - name: cmd\n            value: for i in $(seq 1 20); do curl -XPOST 'http://{{steps.influx.ip}}:8086/write?db=mydb' -d \"cpu,host=server02,region=uswest load=$((RANDOM % 100))\" ; sleep .5 ; done\n      - name: producer-3                # add entries to influxdb\n        template: influxdb-client\n        arguments:\n          parameters:\n          - name: cmd\n            value: curl -XPOST 'http://{{steps.influx.ip}}:8086/write?db=mydb' -d 'cpu,host=server03,region=useast load=15.4'\n\n    - - name: consumer                  # consume intries from influxdb\n        template: influxdb-client\n        arguments:\n          parameters:\n          - name: cmd\n            value: curl --silent -G http://{{steps.influx.ip}}:8086/query?pretty=true --data-urlencode \"db=mydb\" --data-urlencode \"q=SELECT * FROM cpu\"\n\n  - name: influxdb\n    daemon: true                        # start influxdb as a daemon\n    retryStrategy:\n      limit: 10                         # retry container if it fails\n    container:\n      image: influxdb:1.2\n      command:\n      - influxd\n      readinessProbe:                   # wait for readinessProbe to succeed\n        httpGet:\n          path: /ping\n          port: 8086\n\n  - name: influxdb-client\n    inputs:\n      parameters:\n      - name: cmd\n    container:\n      image: appropriate/curl:latest\n      command: [\"/bin/sh\", \"-c\"]\n      args: [\"{{inputs.parameters.cmd}}\"]\n      resources:\n        requests:\n          memory: 32Mi\n          cpu: 100m\n```\n\n----------------------------------------\n\nTITLE: Apply Argo Workflows Quick-Start Manifest (Bash)\nDESCRIPTION: This snippet creates the 'argo' namespace and applies the quick-start manifest from the Argo Workflows GitHub repository. The manifest installs the necessary components for Argo Workflows. The ARGO_WORKFLOWS_VERSION variable should be set before running this command.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/quick-start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create namespace argo\nkubectl apply -n argo -f \"https://github.com/argoproj/argo-workflows/releases/download/${ARGO_WORKFLOWS_VERSION}/quick-start-minimal.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Workflow of Workflows Implementation in Argo (YAML)\nDESCRIPTION: Defines the parent workflow that triggers child workflows using the `workflowTemplateRef`. The `resource` template creates child workflows either without arguments (`resource-without-argument`) or with arguments (`resource-with-argument`), referencing the `workflow-template-submittable` template. It also specifies success and failure conditions based on the child workflow's phase.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-of-workflows.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# This template demonstrates a workflow of workflows.\n# Workflow triggers one or more workflows and manages them.\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: workflow-of-workflows-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      steps:\n        - - name: workflow1\n            template: resource-without-argument\n            arguments:\n              parameters:\n              - name: workflowtemplate\n                value: \"workflow-template-submittable\"\n        - - name: workflow2\n            template: resource-with-argument\n            arguments:\n              parameters:\n              - name: workflowtemplate\n                value: \"workflow-template-submittable\"\n              - name: message\n                value: \"Welcome Argo\"\n\n    - name: resource-without-argument\n      inputs:\n        parameters:\n          - name: workflowtemplate\n      resource:\n        action: create\n        manifest: |\n          apiVersion: argoproj.io/v1alpha1\n          kind: Workflow\n          metadata:\n            generateName: workflow-of-workflows-1-\n          spec:\n            workflowTemplateRef:\n              name: {{inputs.parameters.workflowtemplate}}\n        successCondition: status.phase == Succeeded\n        failureCondition: status.phase in (Failed, Error)\n\n    - name: resource-with-argument\n      inputs:\n        parameters:\n          - name: workflowtemplate\n          - name: message\n      resource:\n        action: create\n        manifest: |\n          apiVersion: argoproj.io/v1alpha1\n          kind: Workflow\n          metadata:\n            generateName: workflow-of-workflows-2-\n          spec:\n            arguments:\n              parameters:\n              - name: message\n                value: {{inputs.parameters.message}}\n            workflowTemplateRef:\n              name: {{inputs.parameters.workflowtemplate}}\n        successCondition: status.phase == Succeeded\n        failureCondition: status.phase in (Failed, Error)\n```\n\n----------------------------------------\n\nTITLE: Retry Decision Flowchart (Mermaid)\nDESCRIPTION: This Mermaid flowchart visualizes the logic determining whether a retry is attempted in Argo Workflows, considering the presence of a policy and/or an expression, and the Argo Workflows version. It outlines the decision-making process based on whether a policy and/or expression are specified, and the version of Argo Workflows being used.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/retries.md#_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n  start([Will a retry be attempted])\n  start --> policy\n  policy(Policy Specified?)\n  policy-->|No|expressionNoPolicy\n  policy-->|Yes|policyGiven\n  policyGiven(Expression Specified?)\n  policyGiven-->|No|policyGivenApplies\n  policyGiven-->|Yes|policyAndExpression\n  policyGivenApplies(Supplied Policy)\n  policyAndExpression(Supplied Policy AND Expression)\n  expressionNoPolicy(Expression specified?)\n  expressionNoPolicy-->|No|onfailureNoExpr\n  expressionNoPolicy-->|Yes|version\n  onfailureNoExpr[OnFailure]\n  onfailure[OnFailure AND Expression]\n  version(Workflows version)\n  version-->|3.4 or earlier|onfailure\n  always[Only Expression matters]\n  version-->|3.5 or later|always\n```\n\n----------------------------------------\n\nTITLE: Retry Strategy with Backoff in Argo Workflow (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define a `retryStrategy` for an Argo Workflow template. It configures a limit of 10 retries, always retries the step, and uses an exponential backoff with a duration, factor, and maximum duration. It also includes node anti-affinity to prevent retries from running on the same node.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/retrying-failed-or-errored-steps.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# This example demonstrates the use of retry back offs\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: retry-backoff-\nspec:\n  entrypoint: retry-backoff\n  templates:\n  - name: retry-backoff\n    retryStrategy:\n      limit: 10\n      retryPolicy: \"Always\"\n      backoff:\n        duration: \"1\"      # Must be a string. Default unit is seconds. Could also be a Duration, e.g.: \"2m\", \"6h\", \"1d\"\n        factor: 2\n        maxDuration: \"1m\"  # Must be a string. Default unit is seconds. Could also be a Duration, e.g.: \"2m\", \"6h\", \"1d\"\n      affinity:\n        nodeAntiAffinity: {}\n    container:\n      image: python:alpine3.6\n      command: [\"python\", -c]\n      # fail with a 66% probability\n      args: [\"import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\"]\n```\n\n----------------------------------------\n\nTITLE: Submitting and Watching Windows Workflow - Argo CLI\nDESCRIPTION: These commands submit the 'hello-windows.yaml' workflow to Argo and then watches the logs for the executed workflow. It requires the Argo CLI to be configured.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/windows.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ argo submit --watch https://raw.githubusercontent.com/argoproj/argo-workflows/main/examples/hello-windows.yaml\n$ argo logs hello-windows-s9kk5\nhello-windows-s9kk5: \"Hello from Windows Container!\"\n```\n\n----------------------------------------\n\nTITLE: Workflow Retry Policies (YAML)\nDESCRIPTION: This snippet demonstrates how to use `retryPolicy` to define which failure types should trigger a retry.  In this example, the retryPolicy is set to `Always`, meaning that all failed steps will be retried up to the specified `limit`. The `limit` parameter controls the maximum number of retry attempts.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/retries.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: retry-on-error-\nspec:\n  entrypoint: error-container\n  templates:\n  - name: error-container\n    retryStrategy:\n      limit: \"2\"\n      retryPolicy: \"Always\"\n    container:\n      image: python\n      command: [\"python\", -c]\n      # fail with a 80% probability\n      args: [\"import random; import sys; exit_code = random.choice(range(0, 5)); sys.exit(exit_code)\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring an SFTP Event Source in Argo Events\nDESCRIPTION: This snippet demonstrates the configuration of an SFTP event source in Argo Events, including authentication with SecretKeySelector, event filtering, and path watching. It shows how to specify parameters like address, event_type, filter expression, username, password, SSH key, and poll interval. It relies on `GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SFTPEventSource` and `SecretKeySelector` objects.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_27\n\nLANGUAGE: YAML\nCODE:\n```\nsftp={\n                    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SFTPEventSource(\n                        address=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        event_type=\"event_type_example\",\n                        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceFilter(\n                            expression=\"expression_example\",\n                        ),\n                        metadata={\n                            \"key\": \"key_example\",\n                        },\n                        password=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        poll_interval_duration=\"poll_interval_duration_example\",\n                        ssh_key_secret=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        username=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        watch_path_config=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1WatchPathConfig(\n                            directory=\"directory_example\",\n                            path=\"path_example\",\n                            path_regexp=\"path_regexp_example\",\n                        ),\n                    ),\n                }\n```\n\n----------------------------------------\n\nTITLE: Creating Argo Workflow Template\nDESCRIPTION: Creates one or more workflow templates using the `argo template create` command. The command takes a list of file paths as input, each representing a workflow template definition. Various options control the output format and validation behavior.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template_create.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo template create FILE1 FILE2... [flags]\n```\n\n----------------------------------------\n\nTITLE: Workflow Referencing External Template (DAG) YAML\nDESCRIPTION: This example demonstrates how to reference a template from another WorkflowTemplate using the `templateRef` field within a `dag` template. It specifies the WorkflowTemplate name and template name for the reference.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: workflow-template-hello-world-\nspec:\n  entrypoint: hello-world\n  templates:\n  - name: hello-world\n    dag:\n      tasks:\n        - name: call-print-message\n          templateRef:\n            name: workflow-template-1\n            template: print-message\n          arguments:\n            parameters:\n            - name: message\n              value: \"hello world\"\n```\n\n----------------------------------------\n\nTITLE: WorkflowTemplate with Local Parameter YAML\nDESCRIPTION: This example demonstrates how to define and use local parameters within a WorkflowTemplate. The WorkflowTemplate defines an input parameter 'msg' with a default value.  The Workflow then references the WorkflowTemplate.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: hello-world-template-local-arg\nspec:\n  templates:\n    - name: hello-world\n      inputs:\n        parameters:\n          - name: msg\n            value: \"hello world\"\n      container:\n        image: busybox\n        command: [echo]\n        args: [\"{{inputs.parameters.msg}}\"]\n```\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-local-arg-\nspec:\n  entrypoint: print-message\n  templates:\n    - name: print-message\n      steps:\n        - - name: hello-world\n            templateRef:\n              name: hello-world-template-local-arg\n              template: hello-world\n```\n\n----------------------------------------\n\nTITLE: Workflow Memoization Configuration YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure memoization at the template level in an Argo Workflow. It specifies a key based on the input parameter 'message', a maximum age of 10 seconds, and the name of the config map to use as a cache.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/memoization.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n   generateName: memoized-workflow-\nspec:\n   entrypoint: print-message\n   templates:\n      - name: print-message\n        memoize:\n           key: \"{{inputs.parameters.message}}\"\n           maxAge: \"10s\"\n           cache:\n              configMap:\n                 name: print-message-cache\n```\n\n----------------------------------------\n\nTITLE: Extracting Data from JSON with Expression in Argo\nDESCRIPTION: This code shows how to extract data from a JSON string using the `jsonpath` function. It takes a JSON string and a JSONPath expression as input. In this example, it extracts data from the `$.some.path` within the `inputs.parameters.json` parameter.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_9\n\nLANGUAGE: text\nCODE:\n```\njsonpath(inputs.parameters.json, '$.some.path')\n```\n\n----------------------------------------\n\nTITLE: Workflow with Conditional Steps in Argo\nDESCRIPTION: Defines an Argo Workflow that simulates a coin flip and executes different templates based on the result. It uses `govaluate` expressions in the `when` clause to specify conditional execution. The expressions evaluate the output of the 'flip-coin' step to determine which template ('heads' or 'tails') to execute. The workflow also includes examples of complex conditions using logical operators and regular expressions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/conditionals.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: coinflip-\nspec:\n  entrypoint: coinflip\n  templates:\n  - name: coinflip\n    steps:\n    # flip a coin\n    - - name: flip-coin\n        template: flip-coin\n    # evaluate the result in parallel\n    - - name: heads\n        template: heads                       # call heads template if \"heads\"\n        when: \"{{steps.flip-coin.outputs.result}} == heads\"\n      - name: tails\n        template: tails                       # call tails template if \"tails\"\n        when: \"{{steps.flip-coin.outputs.result}} == tails\"\n    - - name: flip-again\n        template: flip-coin\n    - - name: complex-condition\n        template: heads-tails-or-twice-tails\n        # call heads template if first flip was \"heads\" and second was \"tails\" OR both were \"tails\"\n        when: >-\n            ( {{steps.flip-coin.outputs.result}} == heads &&\n              {{steps.flip-again.outputs.result}} == tails\n            ) ||\n            ( {{steps.flip-coin.outputs.result}} == tails &&\n              {{steps.flip-again.outputs.result}} == tails )\n      - name: heads-regex\n        template: heads                       # call heads template if ~ \"hea\"\n        when: \"{{steps.flip-again.outputs.result}} =~ hea\"\n      - name: tails-regex\n        template: tails                       # call heads template if ~ \"tai\"\n        when: \"{{steps.flip-again.outputs.result}} =~ tai\"\n\n  # Return heads or tails based on a random number\n  - name: flip-coin\n    script:\n      image: python:alpine3.6\n      command: [python]\n      source: |\n        import random\n        result = \"heads\" if random.randint(0,1) == 0 else \"tails\"\n        print(result)\n\n  - name: heads\n    container:\n      image: alpine:3.6\n      command: [sh, -c]\n      args: [\"echo \\\"it was heads\\\"\"]\n\n  - name: tails\n    container:\n      image: alpine:3.6\n      command: [sh, -c]\n      args: [\"echo \\\"it was tails\\\"\"]\n\n  - name: heads-tails-or-twice-tails\n    container:\n      image: alpine:3.6\n      command: [sh, -c]\n      args: [\"echo \\\"it was heads the first flip and tails the second. Or it was two times tails.\\\"\"]\n```\n\n----------------------------------------\n\nTITLE: WorkflowEventBinding example\nDESCRIPTION: This YAML defines a WorkflowEventBinding that links an event to a WorkflowTemplate. It specifies a selector that requires a \"message\" in the payload, an \"x-argo-e2e\" header set to \"true\", and a discriminator equal to \"my-discriminator\". When the event matches these conditions, the \"my-wf-tmple\" WorkflowTemplate will be submitted with the payload message as an argument.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/events.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowEventBinding\nmetadata:\n  name: event-consumer\nspec:\n  event:\n    # metadata header name must be lowercase to match in selector\n    selector: payload.message != \"\" && metadata[\"x-argo-e2e\"] == [\"true\"] && discriminator == \"my-discriminator\"\n  submit:\n    workflowTemplateRef:\n      name: my-wf-tmple\n    arguments:\n      parameters:\n      - name: message\n        valueFrom:\n          event: payload.message\n```\n\n----------------------------------------\n\nTITLE: Submitting Workflows with a specific Service Account (Argo)\nDESCRIPTION: This command shows how to submit an Argo Workflow while specifying the service account to be used. The `--serviceaccount` flag is used to designate which service account Argo will use during the workflow execution. If no service account is specified, the default service account in the namespace will be used.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/service-accounts.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo submit --serviceaccount <name>\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for CA Certificate\nDESCRIPTION: This command creates a generic Kubernetes secret named `my-root-ca` from the file `my-ca.pem`. This secret will store the CA certificate, allowing Argo to verify the server certificate presented by MinIO when TLS is enabled with a custom CA.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic my-root-ca --from-file=my-ca.pem\n```\n\n----------------------------------------\n\nTITLE: CronWorkflow with time-based skip backwards condition\nDESCRIPTION: Uses the `when` condition to avoid duplicate schedules when the clock is set back. This workflow runs at 01:30 but will skip the second execution during a daylight saving time change because it checks if the last run was less than 2 hours ago.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cron-workflows.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nschedules:\n  - 30 1 * * *\nwhen: \"{{= cronworkflow.lastScheduledTime == nil || (now() - cronworkflow.lastScheduledTime).Seconds() > 7200 }}\"\n```\n\n----------------------------------------\n\nTITLE: Workflow using workflowTemplateRef YAML\nDESCRIPTION: This example demonstrates creating a Workflow from a WorkflowTemplate using only the `workflowTemplateRef` field.  The Workflow will inherit the entrypoint and arguments defined in the WorkflowTemplate.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: workflow-template-hello-world-\nspec:\n  workflowTemplateRef:\n    name: workflow-template-submittable\n```\n\n----------------------------------------\n\nTITLE: Workflow Definition with Hardwired Artifacts (YAML)\nDESCRIPTION: This YAML snippet defines an Argo Workflow that uses hardwired artifacts. It fetches source code from a Git repository, downloads a binary from an HTTP URL, and copies objects from an S3-compatible bucket. The artifacts are defined as inputs to the `hardwired-artifact` template and are placed at specified paths within the container.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/hardwired-artifacts.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hardwired-artifact-\nspec:\n  entrypoint: hardwired-artifact\n  templates:\n  - name: hardwired-artifact\n    inputs:\n      artifacts:\n      # Check out the main branch of the argo repo and place it at /src\n      # revision can be anything that git checkout accepts: branch, commit, tag, etc.\n      - name: argo-source\n        path: /src\n        git:\n          repo: https://github.com/argoproj/argo-workflows.git\n          revision: \"main\"\n      # Download kubectl 1.8.0 and place it at /bin/kubectl\n      - name: kubectl\n        path: /bin/kubectl\n        mode: 0755\n        http:\n          url: https://storage.googleapis.com/kubernetes-release/release/v1.8.0/bin/linux/amd64/kubectl\n      # Copy an s3 compatible artifact repository bucket (such as AWS, GCS and MinIO) and place it at /s3\n      - name: objects\n        path: /s3\n        s3:\n          endpoint: storage.googleapis.com\n          bucket: my-bucket-name\n          key: path/in/bucket\n          accessKeySecret:\n            name: my-s3-credentials\n            key: accessKey\n          secretKeySecret:\n            name: my-s3-credentials\n            key: secretKey\n    container:\n      image: debian\n      command: [sh, -c]\n      args: [\"ls -l /src /bin/kubectl /s3\"]\n```\n\n----------------------------------------\n\nTITLE: Define Secret for Argo Workflows SSO\nDESCRIPTION: This YAML defines a Kubernetes Secret named `argo-workflows-sso` containing the client ID and secret required for Argo Workflows to authenticate with Dex. The `client-id` is set to `argo-workflows-sso`, and the `client-secret` should be a strong, randomly generated string. This secret needs to be accessible to both Argo CD and Argo Workflows if they are in different namespaces.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso-argocd.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: argo-workflows-sso\ndata:\n  # client-id is 'argo-workflows-sso'\n  client-id: YXJnby13b3JrZmxvd3Mtc3Nv\n  # client-secret is 'MY-SECRET-STRING-CAN-BE-UUID'\n  client-secret: TVktU0VDUkVULVNUUklORy1DQU4tQkUtVVVJRA==\n```\n\n----------------------------------------\n\nTITLE: Retry Argo Workflow with Restart and Node Selection\nDESCRIPTION: Retries a failed Argo workflow using the `argo retry` command with the `--restart-successful` option and a node field selector. This command ensures that only the `suspend` step is retried, preventing the re-triggering of the external job.  The selector targets nodes with the specified template name and failed phase.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/async-pattern.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nargo retry <WORKFLOWNAME> --restart-successful --node-field-selector templateRef.template=run-external-job,phase=Failed\n```\n\n----------------------------------------\n\nTITLE: Defining a Script Template in Argo\nDESCRIPTION: This YAML snippet defines a 'gen-random-int' template using the 'script' type.  It uses a Python image and executes a script in-place that generates a random integer between 1 and 100, then prints it to standard output.  The output will be captured as an Argo variable.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-concepts.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n  - name: gen-random-int\n    script:\n      image: python:alpine3.6\n      command: [python]\n      source: |\n        import random\n        i = random.randint(1, 100)\n        print(i)\n```\n\n----------------------------------------\n\nTITLE: WorkflowTemplate for External Job in Argo\nDESCRIPTION: Defines a WorkflowTemplate in Argo to trigger and wait for an external job. The template includes steps to trigger the job (using a container image) and a suspend step that pauses the workflow until resumed or stopped via an API call. The `job-cmd` parameter is used to specify the command to trigger the external job.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/async-pattern.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: external-job-template\nspec:\n  entrypoint: run-external-job\n  arguments:\n    parameters:\n      - name: \"job-cmd\"\n  templates:\n    - name: run-external-job\n      inputs:\n        parameters:\n          - name: \"job-cmd\"\n            value: \"{{workflow.parameters.job-cmd}}\"\n      steps:\n        - - name: trigger-job\n            template: trigger-job\n            arguments:\n              parameters:\n                - name: \"job-cmd\"\n                  value: \"{{inputs.parameters.job-cmd}}\"\n        - - name: wait-completion\n            template: wait-completion\n            arguments:\n              parameters:\n                - name: uuid\n                  value: \"{{steps.trigger-job.outputs.result}}\"\n\n    - name: trigger-job\n      inputs:\n        parameters:\n          - name: \"job-cmd\"\n      container:\n        image: appropriate/curl:latest\n        command: [ \"/bin/sh\", \"-c\" ]\n        args: [ \"{{inputs.parameters.job-cmd}}\" ]\n\n    - name: wait-completion\n      inputs:\n        parameters:\n          - name: uuid\n      suspend: { }\n```\n\n----------------------------------------\n\nTITLE: Looping with Param in Argo Workflows (YAML)\nDESCRIPTION: This example uses `withParam` to iterate over a JSON array of items passed as an argument to the workflow. This allows the list of items to be dynamically generated. The elements are addressed by key using `{{item.key}}`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/loops.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: loops-param-arg-\nspec:\n  entrypoint: loop-param-arg-example\n  arguments:\n    parameters:\n    - name: os-list                                     # a list of items\n      value: |\n        [\n          { \"image\": \"debian\", \"tag\": \"9.1\" },\n          { \"image\": \"debian\", \"tag\": \"8.9\" },\n          { \"image\": \"alpine\", \"tag\": \"3.6\" },\n          { \"image\": \"ubuntu\", \"tag\": \"17.10\" }\n        ]\n\n  templates:\n  - name: loop-param-arg-example\n    inputs:\n      parameters:\n      - name: os-list\n    steps:\n    - - name: test-linux\n        template: cat-os-release\n        arguments:\n          parameters:\n          - name: image\n            value: \"{{item.image}}\"\n          - name: tag\n            value: \"{{item.tag}}\"\n        withParam: \"{{inputs.parameters.os-list}}\"      # parameter specifies the list to iterate over\n\n  # This template is the same as in the previous example\n  - name: cat-os-release\n    annotations: \n      workflows.argoproj.io/display-name: \"os-{{inputs.parameters.image}}-{{inputs.parameters.tag}}\" # this sets a custom name for the node in the UI, based on the template's parameters\n    inputs:\n      parameters:\n      - name: image\n      - name: tag\n    container:\n      image: \"{{inputs.parameters.image}}:{{inputs.parameters.tag}}\"\n      command: [cat]\n      args: [/etc/os-release]\n```\n\n----------------------------------------\n\nTITLE: Casting to Float with Expression in Argo\nDESCRIPTION: This snippet demonstrates casting a parameter to a float using the `asFloat` function in an expression.  It converts the `my-float-param` input parameter to a float. This is necessary when parameters are strings and need to be treated as floating-point numbers.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nasFloat(inputs.parameters['my-float-param'])\n```\n\n----------------------------------------\n\nTITLE: Setting Workflow Priority - YAML\nDESCRIPTION: This YAML snippet demonstrates how to set a priority for an Argo Workflow. Workflows with higher priority numbers will be executed before workflows with lower priority numbers when controller-level parallelism limits are reached. The `priority` field is set to 3 in this example.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/parallelism.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: priority-\nspec:\n  priority: 3\n  # ...\n```\n\n----------------------------------------\n\nTITLE: Workflow Node Event Example YAML\nDESCRIPTION: This YAML example shows the structure of a Kubernetes event emitted by Argo Workflows when a node succeeds. It includes metadata, annotations, and information about the involved Workflow object. The annotations provide the name and type of the node that triggered the event.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-events.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetadata:\n  name: my-wf.160434cb3af841f8\n  namespace: my-ns\n  annotations:\n    workflows.argoproj.io/node-name: my-node\n    workflows.argoproj.io/node-type: Pod\ntype: Normal\nreason: WorkflowNodeSucceeded\nmessage: 'Succeeded node my-node: my message'\ninvolvedObject:\n  apiVersion: v1alpha1\n  kind: Workflow\n  name: my-wf\n  namespace: my-ns\n  resourceVersion: \"1234\"\n  uid: my-uid\nfirstTimestamp: \"2020-04-09T16:50:16Z\"\nlastTimestamp: \"2020-04-09T16:50:16Z\"\ncount: 1\n```\n\n----------------------------------------\n\nTITLE: Granting admin privileges to default Service Account\nDESCRIPTION: This command grants admin privileges to the default Service Account in the 'argo' namespace. It creates a RoleBinding named 'default-admin' that binds the 'admin' ClusterRole to the 'default' ServiceAccount within the 'argo' namespace. This provides the service account with full administrative access within that namespace, but be aware of the security implications.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/service-accounts.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create rolebinding default-admin --clusterrole=admin --serviceaccount=argo:default -n argo\n```\n\n----------------------------------------\n\nTITLE: Listing Workflows Created Within a Timeframe with Argo CLI\nDESCRIPTION: Lists workflows that were created within the last 10 minutes. It uses the `--since` flag with a duration value. Durations are specified using suffixes like `m` for minutes, `h` for hours, and `d` for days.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_list.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nargo list --since 10m\n```\n\n----------------------------------------\n\nTITLE: SFTP Event Source Configuration\nDESCRIPTION: Defines the configuration for an SFTP event source in Argo Events. It uses SecretKeySelector to securely manage sensitive information such as passwords, SSH keys, and usernames.  The WatchPathConfig specifies the directory, path, and path regular expression to monitor for changes.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nsftp: {\n    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SFTPEventSource(\n        address=SecretKeySelector(\n            key=\"key_example\",\n            name=\"name_example\",\n            optional=True,\n        ),\n        event_type=\"event_type_example\",\n        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceFilter(\n            expression=\"expression_example\",\n        ),\n        metadata={\n            \"key\": \"key_example\",\n        },\n        password=SecretKeySelector(\n            key=\"key_example\",\n            name=\"name_example\",\n            optional=True,\n        ),\n        poll_interval_duration=\"poll_interval_duration_example\",\n        ssh_key_secret=SecretKeySelector(\n            key=\"key_example\",\n            name=\"name_example\",\n            optional=True,\n        ),\n        username=SecretKeySelector(\n            key=\"key_example\",\n            name=\"name_example\",\n            optional=True,\n        ),\n        watch_path_config=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1WatchPathConfig(\n            directory=\"directory_example\",\n            path=\"path_example\",\n            path_regexp=\"path_regexp_example\",\n        ),\n    ),\n}\n```\n\n----------------------------------------\n\nTITLE: Parameter File (YAML)\nDESCRIPTION: This YAML snippet shows an example of a parameter file that can be used to override workflow parameters using the Argo CLI. This example file defines a value for the `message` parameter. The file is designed to be used with the `--parameter-file` flag of the `argo submit` command.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/parameters.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmessage: goodbye world\n```\n\n----------------------------------------\n\nTITLE: Workflow with Self-Reporting Progress (YAML)\nDESCRIPTION: This YAML defines a Workflow that includes a task named 'progress' which runs a container. The container reports its progress by writing to the file specified by the ARGO_PROGRESS_FILE environment variable. The container sleeps for 10 seconds in each iteration of the loop and reports the progress as i*10/100. The controller picks this up and writes the progress to the appropriate Status properties.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/progress.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: progress-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      dag:\n        tasks:\n          - name: progress\n            template: progress\n    - name: progress\n      container:\n        image: alpine:3.14\n        command: [ \"/bin/sh\", \"-c\" ]\n        args:\n          - |-\n            for i in `seq 1 10`; do sleep 10; echo \"$(($i*10))\"'/100' > $ARGO_PROGRESS_FILE; done\n```\n\n----------------------------------------\n\nTITLE: Prometheus Metrics Configuration in ConfigMap\nDESCRIPTION: This YAML snippet configures Prometheus metrics settings within the Argo Workflow Controller ConfigMap. It demonstrates enabling/disabling metrics, setting the metrics path and port, ignoring errors, and enabling TLS.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmetricsConfig: |\n  # Enabled controls the prometheus metric. Default is true, set \"enabled: false\" to turn off\n  enabled: true\n\n  # Path is the path where prometheus metrics are emitted. Must start with a \"/\". Default is \"/metrics\"\n  path: /metrics\n\n  # Port is the port where prometheus metrics are emitted. Default is \"9090\"\n  port: 8080\n\n  # IgnoreErrors is a flag that instructs prometheus to ignore metric emission errors. Default is \"false\"\n  ignoreErrors: false\n\n  # Use a self-signed cert for TLS\n  # >= 3.6: default true\n  secure: true\n```\n\n----------------------------------------\n\nTITLE: Configure Template Defaults in WorkflowSpec (YAML)\nDESCRIPTION: This example shows how to configure default template values such as `timeout` and `retryStrategy` within the `WorkflowSpec`. These defaults are applied to all templates in the workflow, unless a template explicitly overrides them. The values are merged using Kubernetes strategic merge patch during runtime.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/template-defaults.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: template-defaults-example\nspec:\n  entrypoint: main\n  templateDefaults:\n    timeout: 30s   # timeout value will be applied to all templates\n    retryStrategy: # retryStrategy value will be applied to all templates\n      limit: 2\n  templates:\n  - name: main\n    container:\n      image: busybox\n```\n\n----------------------------------------\n\nTITLE: WorkflowTemplate definition\nDESCRIPTION: This YAML defines a WorkflowTemplate that accepts a \"message\" parameter as input. The WorkflowTemplate uses a busybox image to echo the provided message.  The `value: \"{{workflow.parameters.message}}\"` extracts the passed parameter into the workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/events.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: my-wf-tmple\n  namespace: argo\nspec:\n  templates:\n    - name: main\n      inputs:\n        parameters:\n          - name: message\n            value: \"{{workflow.parameters.message}}\"\n      container:\n        image: busybox\n        command: [echo]\n        args: [\"{{inputs.parameters.message}}\"]\n  entrypoint: main\n```\n\n----------------------------------------\n\nTITLE: Workflow with Templates Definition YAML\nDESCRIPTION: This example demonstrates a basic Argo Workflow definition using YAML. It defines two templates: 'hello' which is a steps template, and 'print-message' which is a container template. The 'hello' template calls the 'print-message' template, passing a message parameter.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: steps-\nspec:\n  entrypoint: hello           # We reference our first \"template\" here\n\n  templates:\n  - name: hello               # The first \"template\" in this Workflow, it is referenced by \"entrypoint\"\n    steps:                    # The type of this \"template\" is \"steps\"\n    - - name: hello\n        template: print-message # We reference our second \"template\" here\n        arguments:\n          parameters: [{name: message, value: \"hello1\"}]\n\n  - name: print-message       # The second \"template\" in this Workflow, it is referenced by \"hello\"\n    inputs:\n      parameters:\n      - name: message\n    container:                # The type of this \"template\" is \"container\"\n      image: busybox\n      command: [echo]\n      args: [\"{{inputs.parameters.message}}\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring MinIO with TLS and Custom CA in Argo Workflow\nDESCRIPTION: This YAML snippet configures an Argo Workflow artifact to use MinIO with TLS enabled and a custom CA certificate. It sets `insecure` to `false` and references the `my-root-ca` secret containing the CA certificate for secure communication with MinIO.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nartifacts:\n  - s3:\n      insecure: false\n      caSecret:\n        name: my-root-ca\n        key: my-ca.pem\n      ...\n```\n\n----------------------------------------\n\nTITLE: Defining Conditional Artifacts in Argo Workflows (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define a conditional artifact at the Step level in Argo Workflows. It utilizes the `fromExpression` field to dynamically select an artifact based on the result of the `flip-coin` step. If the result is 'heads', the `headsresult` artifact from the `heads` step is selected; otherwise, the `tailsresult` artifact from the `tails` step is selected. The `when` clauses in the `heads` and `tails` steps conditionally execute based on the coin flip outcome.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/conditional-artifacts-parameters.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- name: coinflip\n  steps:\n    - - name: flip-coin\n        template: flip-coin\n    - - name: heads\n        template: heads\n        when: \"{{steps.flip-coin.outputs.result}} == heads\"\n      - name: tails\n        template: tails\n        when: \"{{steps.flip-coin.outputs.result}} == tails\"\n  outputs:\n    artifacts:\n      - name: result\n        fromExpression: \"steps['flip-coin'].outputs.result == 'heads' ? steps.heads.outputs.artifacts.headsresult : steps.tails.outputs.artifacts.tailsresult\"\n```\n\n----------------------------------------\n\nTITLE: Configuring archiveLogs in Workflow Spec using YAML\nDESCRIPTION: This YAML snippet demonstrates how to enable log archiving at the workflow specification level by setting `archiveLogs: true` in the `spec` section. This ensures that logs for the workflow are automatically archived to the configured artifact repository.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-archive-logs.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: archive-location-\nspec:\n  archiveLogs: true\n  entrypoint: hello-world\n  templates:\n  - name: hello-world\n    container:\n      image: busybox\n      command: [echo]\n      args: [\"hello world\"]\n```\n\n----------------------------------------\n\nTITLE: Updating Event Source with Argo Events API\nDESCRIPTION: This code snippet demonstrates how to update an event source using the Argo Events API. It involves constructing an `EventsourceUpdateEventSourceRequest` object, setting its properties, and then calling the `update_event_source` method to update the event source in the specified namespace.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\napi_response = api_instance.update_event_source(namespace, name, body)\npprint(api_response)\n```\n\n----------------------------------------\n\nTITLE: Configuring Argo Server GRPC Mode\nDESCRIPTION: This snippet illustrates how to configure the Argo CLI to communicate with the Argo Server via GRPC. It includes setting the server address, enabling TLS, and skipping TLS verification (for development purposes only).\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nARGO_SERVER=localhost:2746 ;# The format is \"host:port\" - do not prefix with \"http\" or \"https\"\nARGO_SECURE=true\nARGO_INSECURE_SKIP_VERIFY=true\n```\n\n----------------------------------------\n\nTITLE: Looping with Sequence in Argo Workflows (YAML)\nDESCRIPTION: This example demonstrates the use of `withSequence` to iterate over a sequence of numbers in an Argo Workflow.  It runs the 'hello-world' template 5 times. The `count` parameter specifies the number of iterations.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/loops.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: loop-sequence-\nspec:\n  entrypoint: loop-sequence-example\n\n  templates:\n  - name: loop-sequence-example\n    steps:\n    - - name: hello-world-x5\n        template: hello-world\n        withSequence:\n          count: \"5\"\n\n  - name: hello-world\n    container:\n      image: busybox\n      command: [echo]\n      args: [\"hello world!\"]\n```\n\n----------------------------------------\n\nTITLE: Depends with unspecified result\nDESCRIPTION: This example demonstrates the default behavior of the `depends` field when no task result is specified. An unspecified result is equivalent to `(task.Succeeded || task.Skipped || task.Daemoned)`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/enhanced-depends-logic.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndepends: \"task || task-2.Failed\"\n```\n\n----------------------------------------\n\nTITLE: Depends with AnySucceeded and AllFailed\nDESCRIPTION: This snippet shows how to use `.AnySucceeded` and `.AllFailed` with tasks using `withItems`.  The dependency will be met if any item in task-1 succeeded or all items in task-2 failed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/enhanced-depends-logic.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndepends: \"task-1.AnySucceeded || task-2.AllFailed\"\n```\n\n----------------------------------------\n\nTITLE: Configure Workflow Controller with Namespaced and Managed Namespace\nDESCRIPTION: Configures the Argo Workflow Controller to run in a namespace-scoped configuration, specifying both `--namespaced` and `--managed-namespace` flags. `--configmap` sets the configmap name. `--executor-image` sets the executor image used. `--namespaced` indicates the namespace scope. `--managed-namespace` specifies the namespace where workflows will be managed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/managed-namespace.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n      - args:\n        - --configmap\n        - workflow-controller-configmap\n        - --executor-image\n        - argoproj/workflow-controller:v2.5.1\n        - --namespaced\n        - --managed-namespace\n        - default\n```\n\n----------------------------------------\n\nTITLE: Configure Group Filtering (YAML)\nDESCRIPTION: This snippet configures `filterGroupsRegex` to filter the groups returned by the OIDC provider. A logical \"OR\" is used between each regex in the list.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nsso:\n    # Specify a list of regular expressions to filter the groups returned by the OIDC provider.\n    # A logical \"OR\" is used between each regex in the list\n    filterGroupsRegex:\n    - \".*argo-wf.*\"\n    - \".*argo-workflow.*\"\n```\n\n----------------------------------------\n\nTITLE: Defining Conditional Parameters in Argo Workflows (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define a conditional parameter at the Step level in Argo Workflows. It uses the `expression` field under `valueFrom` to dynamically select a parameter value based on the result of the `flip-coin` step. If the result is 'heads', the `result` parameter from the `heads` step is selected; otherwise, the `result` parameter from the `tails` step is selected. The `when` clauses in the `heads` and `tails` steps conditionally execute based on the coin flip outcome.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/conditional-artifacts-parameters.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- name: coinflip\n  steps:\n    - - name: flip-coin\n        template: flip-coin\n    - - name: heads\n        template: heads\n        when: \"{{steps.flip-coin.outputs.result}} == heads\"\n      - name: tails\n        template: tails\n        when: \"{{steps.flip-coin.outputs.result}} == tails\"\n  outputs:\n    parameters:\n      - name: stepresult\n        valueFrom:\n          expression: \"steps['flip-coin'].outputs.result == 'heads' ? steps.heads.outputs.result : steps.tails.outputs.result\"\n```\n\n----------------------------------------\n\nTITLE: Configuring archiveLogs in Workflow Template using YAML\nDESCRIPTION: This YAML snippet demonstrates how to enable log archiving at the workflow template level by defining an `archiveLocation` with `archiveLogs: true`. This configures archiving only for the specific template where it is defined.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-archive-logs.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: archive-location-\nspec:\n  entrypoint: hello-world\n  templates:\n  - name: hello-world\n    container:\n      image: busybox\n      command: [echo]\n      args: [\"hello world\"]\n    archiveLocation:\n      archiveLogs: true\n```\n\n----------------------------------------\n\nTITLE: Resubmitting and Tailing Logs with Argo\nDESCRIPTION: Resubmits a specified Argo Workflow and tails its logs until completion.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resubmit.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nargo resubmit --log my-wf.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating Event Source with Argo Workflows Python API\nDESCRIPTION: This code snippet demonstrates how to create a new event source using the Argo Workflows Event Source Service API in Python. It uses the `create_event_source` method of the `EventSourceServiceApi` class, passing in the namespace and the event source creation request. It handles potential exceptions during the API call and prints any errors that occur. The `EventsourceCreateEventSourceRequest` parameter encapsulates the details of the event source to be created.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import event_source_service_api\nfrom argo_workflows.model.eventsource_create_event_source_request import EventsourceCreateEventSourceRequest\nfrom argo_workflows.model.github_com_argoproj_argo_events_pkg_apis_events_v1alpha1_event_source import GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSource\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = event_source_service_api.EventSourceServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    body = EventsourceCreateEventSourceRequest(  ) # EventsourceCreateEventSourceRequest | \n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.create_event_source(namespace, body)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventSourceServiceApi->create_event_source: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Test API Access with Token (Success)\nDESCRIPTION: Tests the access token by making a request to the Argo Server API to list workflows.  A successful response (200 OK) indicates that the token is valid and has the necessary permissions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/access-token.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://localhost:2746/api/v1/workflows/argo -H \"Authorization: $ARGO_TOKEN\"\n# 200 OK\n```\n\n----------------------------------------\n\nTITLE: Approval Workflow with Intermediate Parameters in Argo\nDESCRIPTION: Defines an Argo Workflow with an approval step that uses a `suspend` template to pause execution and prompt the user to choose between 'YES' or 'NO'. The user's choice is then used in a `when` condition to determine whether to deploy to production. The `approval` template defines input and output parameters that must be the same.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/intermediate-inputs.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: intermediate-parameters-cicd-\nspec:\n  entrypoint: cicd-pipeline\n  templates:\n    - name: cicd-pipeline\n      steps:\n          - - name: deploy-pre-prod\n              template: deploy\n          - - name: approval\n              template: approval\n          - - name: deploy-prod\n              template: deploy\n              when: '{{steps.approval.outputs.parameters.approve}} == YES'\n    - name: approval\n      suspend: {}\n      inputs:\n          parameters:\n            - name: approve\n              default: 'NO'\n              enum:\n                  - 'YES'\n                  - 'NO'\n              description: >-\n                Choose YES to continue workflow and deploy to production\n      outputs:\n          parameters:\n            - name: approve\n              valueFrom:\n                  supplied: {}\n    - name: deploy\n      container:\n          image: 'argoproj/argosay:v2'\n          command:\n            - /argosay\n          args:\n            - echo\n            - deploying\n```\n\n----------------------------------------\n\nTITLE: Defining a Workflow for Submitting Another Workflow in Argo\nDESCRIPTION: This YAML defines an Argo Workflow that uses a `create-wf` template to submit another workflow via a curl command. The `create-wf` template uses a curl command to submit the 'wait' WorkflowTemplate. An access token is required for authentication.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-submitting-workflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: demo-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      steps:\n        - - name: a\n            template: create-wf\n    - name: create-wf\n      script:\n        image: curlimages/curl:latest\n        command:\n          - sh\n        source: >\n          curl https://argo-server:2746/api/v1/workflows/argo/submit \\\n            -fs \\\n            -H \"Authorization: Bearer eyJhbGci...\" \\\n            -d '{\"resourceKind\": \"WorkflowTemplate\", \"resourceName\": \"wait\", \"submitOptions\": {\"labels\": \"workflows.argoproj.io/workflow-template=wait\"}}'\n```\n\n----------------------------------------\n\nTITLE: Disabling Workflow Archive (YAML)\nDESCRIPTION: This YAML snippet shows how to disable workflow archiving by setting the `archive` flag to `false` within the `persistence` section of the Argo Workflow controller configuration. When archive is set to `false`, completed workflows will not be saved to the database.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-archive.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\npersistence:\n  archive: false\n```\n\n----------------------------------------\n\nTITLE: Set Resource Requests and Limits - YAML\nDESCRIPTION: This YAML snippet configures resource requests and limits for Kubernetes deployments, specifically for the `workflow-controller` and `argo-server`.  This helps to control the resource consumption of these components.  The configuration includes `cpu` and `memory` requests and limits.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cost-optimisation.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nrequests:\n  cpu: 100m\n  memory: 64Mi\nlimits:\n  cpu: 500m\n  memory: 128Mi\n```\n\n----------------------------------------\n\nTITLE: Stopping a Workflow using Argo CLI\nDESCRIPTION: This command stops a workflow named 'my-wf' using the Argo command-line interface. The workflow's exit handlers will still be executed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_stop.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo stop my-wf\n```\n\n----------------------------------------\n\nTITLE: Defining Workflow Arguments in Argo Workflows (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define workflow arguments to pass into the entrypoint template. It specifies a parameter named 'workflow-param-1' within the 'arguments' section of the Workflow spec. This parameter can then be used within the workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-inputs.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\narguments:\n  parameters:\n  - name: workflow-param-1\n```\n\n----------------------------------------\n\nTITLE: Workflow-Level Synchronization using ConfigMap (Semaphore)\nDESCRIPTION: This snippet shows how to configure workflow-level synchronization using a semaphore defined in a ConfigMap. The synchronization key 'workflow' is configured with a limit of '1', ensuring that only one workflow instance executes at a time, even with multiple workflow creations. The ConfigMap 'my-config' is referenced to obtain the semaphore configuration.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/synchronization.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"examples/synchronization-wf-level.yaml:12\"\n```\n\n----------------------------------------\n\nTITLE: Depends with boolean logic operators\nDESCRIPTION: This example demonstrates using boolean logic operators (`&&`, `||`, `!`) within the `depends` field. It specifies that the task should run only if task-2 either succeeded or was skipped, and task-3 did not fail.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/enhanced-depends-logic.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndepends: \"(task-2.Succeeded || task-2.Skipped) && !task-3.Failed\"\n```\n\n----------------------------------------\n\nTITLE: Resubmitting and Waiting for Completion with Argo\nDESCRIPTION: Resubmits a specified Argo Workflow and waits for its completion.  Only works when a single workflow is resubmitted.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resubmit.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nargo resubmit --wait my-wf.yaml\n```\n\n----------------------------------------\n\nTITLE: Fetching Sensor Logs with Required Parameters - Python\nDESCRIPTION: This snippet shows how to retrieve sensor logs by providing only the required namespace parameter. It initializes the API client, creates a SensorServiceApi instance, and calls the sensors_logs method with the namespace.  Error handling is included using a try-except block to catch potential ApiException.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = sensor_service_api.SensorServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.sensors_logs(namespace)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling SensorServiceApi->sensors_logs: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Create and Manage Workflows with Kubectl (Bash)\nDESCRIPTION: This snippet demonstrates using kubectl to create, inspect, and retrieve logs for an Argo Workflow. It includes commands to create a workflow from a YAML file, get workflow status, retrieve pod information, and access container logs.  The commands assume the 'argo' namespace exists and is the target for workflow operations.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/kubectl.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/main/examples/hello-world.yaml\nkubectl get wf -n argo\nkubectl get wf hello-world-xxx -n argo\nkubectl get po -n argo --selector=workflows.argoproj.io/workflow=hello-world-xxx\nkubectl logs hello-world-yyy -c main -n argo\n```\n\n----------------------------------------\n\nTITLE: Limit Workflow Duration and Retention - YAML\nDESCRIPTION: This YAML snippet configures an Argo Workflow to automatically terminate after a specified duration and delete completed workflows after a set time.  It also configures pod garbage collection.  This is achieved using `activeDeadlineSeconds`, `ttlStrategy`, and `podGC` in the workflow specification.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cost-optimisation.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  # must complete in 8h (28,800 seconds)\n  activeDeadlineSeconds: 28800\n  # keep workflows for 1d (86,400 seconds)\n  ttlStrategy:\n    secondsAfterCompletion: 86400\n  # delete all pods as soon as they complete\n  podGC:\n    strategy: OnPodCompletion\n```\n\n----------------------------------------\n\nTITLE: Define Template-Level Counter Metric with Argo Variable\nDESCRIPTION: This YAML defines a Template-level Counter metric, `result_counter`, with a label including an Argo variable for step status.  The metric increases by 1 for every step and includes labels for `name` and `status`, using the `{{status}}` variable for dynamic status tracking.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  templates:\n    - name: flakey\n      metrics:\n        prometheus:\n          - name: result_counter\n            help: \"Count of step execution by result status\"\n            labels:\n              - key: name\n                value: flakey\n              - key: status\n                value: \"{{status}}\"    # Argo variable in `labels`\n            counter:\n              value: \"1\"\n      container:\n        image: python:alpine3.6\n        command: [\"python\", -c]\n        # fail with a 66% probability\n        args: [\"import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\"]\n...\n```\n\n----------------------------------------\n\nTITLE: Customizing Workflow metadata in EventBinding\nDESCRIPTION: This YAML snippet demonstrates how to customize the metadata (name, annotations, and labels) of the Workflow created from an event.  Expressions within the name, annotations, and labels allow for dynamic values based on the event payload.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/events.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsubmit:\n  metadata:\n    annotations:\n      anAnnotation: 'event.payload.message'\n    name: 'event.payload.message + \"-world\"'\n    labels:\n      someLabel: '\"literal string\"'\n```\n\n----------------------------------------\n\nTITLE: Artifact Garbage Collection in Argo Workflows\nDESCRIPTION: This YAML configures artifact garbage collection for Argo Workflows. It shows how to set a default strategy for all artifacts and override it for specific artifacts using `artifactGC` strategy.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/artifacts.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: artifact-gc-\nspec:\n  entrypoint: main\n  artifactGC:\n    strategy: OnWorkflowDeletion  # default Strategy set here applies to all Artifacts by default\n  templates:\n    - name: main\n      container:\n        image: argoproj/argosay:v2\n        command:\n          - sh\n          - -c\n        args:\n          - |\n            echo \"can throw this away\" > /tmp/temporary-artifact.txt\n            echo \"keep this\" > /tmp/keep-this.txt\n      outputs:\n        artifacts:\n          - name: temporary-artifact\n            path: /tmp/temporary-artifact.txt\n            s3:\n              key: temporary-artifact.txt\n          - name: keep-this\n            path: /tmp/keep-this.txt\n            s3:\n              key: keep-this.txt\n            artifactGC:\n              strategy: Never   # optional override for an Artifact\n```\n\n----------------------------------------\n\nTITLE: ConfigMap for Workflow and Template Synchronization (Semaphore)\nDESCRIPTION: This ConfigMap configures semaphores for workflow and template synchronization. The 'workflow' key limits the number of concurrent workflows to 1 in a particular namespace, and the 'template' key limits the number of concurrent template instances to 2 in a particular namespace. These settings can be referenced by workflows or templates to control parallel execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/synchronization.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n name: my-config\ndata:\n  workflow: \"1\"  # Only one workflow can run at given time in particular namespace\n  template: \"2\"  # Two instances of template can run at a given time in particular namespace\n```\n\n----------------------------------------\n\nTITLE: Mounting EmptyDir Volume for Output in Argo Workflow (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define a workflow that uses an `emptyDir` volume to store output artifacts. It defines a workflow with a single template that runs a container, writes output to a file within the mounted volume, and then extracts a parameter from that file. This approach is useful when the workflow executor cannot directly access the base layer file system.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/empty-dir.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: empty-dir-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      container:\n        image: argoproj/argosay:v2\n        command: [sh, -c]\n        args: [\"cowsay hello world | tee /mnt/out/hello_world.txt\"]\n        volumeMounts:\n          - name: out\n            mountPath: /mnt/out\n      volumes:\n        - name: out\n          emptyDir: { }\n      outputs:\n        parameters:\n          - name: message\n            valueFrom:\n              path: /mnt/out/hello_world.txt\n```\n\n----------------------------------------\n\nTITLE: Defining a Container Template in Argo\nDESCRIPTION: This YAML snippet defines a 'hello-world' template of type 'container'. It specifies the 'busybox' image and executes the 'echo' command with the argument 'hello world'. This is a basic example of running a container within an Argo Workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-concepts.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n  - name: hello-world\n    container:\n      image: busybox\n      command: [echo]\n      args: [\"hello world\"]\n```\n\n----------------------------------------\n\nTITLE: Docker-in-Docker Workflow Definition YAML\nDESCRIPTION: This workflow definition demonstrates Docker-in-Docker (DIND) using sidecar containers in Argo Workflows. It defines a main container that executes Docker commands and a sidecar container that runs the Docker daemon. The `mirrorVolumeMounts` setting allows the sidecar to see the same filesystem as the main container.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/docker-in-docker-using-sidecars.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: sidecar-dind-\nspec:\n  entrypoint: dind-sidecar-example\n  templates:\n  - name: dind-sidecar-example\n    container:\n      image: docker:19.03.13\n      command: [sh, -c]\n      args: [\"until docker ps; do sleep 3; done; docker run --rm debian:latest cat /etc/os-release\"]\n      env:\n      - name: DOCKER_HOST               # the docker daemon can be access on the standard port on localhost\n        value: 127.0.0.1\n    sidecars:\n    - name: dind\n      image: docker:19.03.13-dind          # Docker already provides an image for running a Docker daemon\n      command: [dockerd-entrypoint.sh]\n      env:\n        - name: DOCKER_TLS_CERTDIR         # Docker TLS env config\n          value: \"\"\n      securityContext:\n        privileged: true                # the Docker daemon can only run in a privileged container\n      # mirrorVolumeMounts will mount the same volumes specified in the main container\n      # to the sidecar (including artifacts), at the same mountPaths. This enables\n      # dind daemon to (partially) see the same filesystem as the main container in\n      # order to use features such as docker volume binding.\n      mirrorVolumeMounts: true\n```\n\n----------------------------------------\n\nTITLE: Copy Artifacts from Specific Node to Local Directory (bash)\nDESCRIPTION: This example shows how to copy artifacts from a specific node within a workflow to a local output directory, using the `--node-id` flag.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cp.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo cp my-wf output-directory --node-id=my-wf-node-id-123\n```\n\n----------------------------------------\n\nTITLE: Annotating WorkflowTemplate in YAML\nDESCRIPTION: This example demonstrates how to add `workflows.argoproj.io/title` and `workflows.argoproj.io/description` annotations with embedded markdown to a WorkflowTemplate for display within the Argo Workflows UI. This annotation scheme enables a cleaner and more informative UI display for WorkflowTemplates.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/title-and-description.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: my-workflow-template\n  annotations:\n    workflows.argoproj.io/title: '**Test Title**'\n    workflows.argoproj.io/description: `This is a simple hello world example.`\n```\n\n----------------------------------------\n\nTITLE: Get Info with InfoService Java\nDESCRIPTION: This snippet demonstrates how to retrieve information using the `infoServiceGetInfo` method.  It sets up the API client, configures API key authorization, and then calls the method to fetch the information.  The code handles potential `ApiException` errors by printing the status code, response body, and headers.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/InfoServiceApi.md#_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.InfoServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    InfoServiceApi apiInstance = new InfoServiceApi(defaultClient);\n    try {\n      IoArgoprojWorkflowV1alpha1InfoResponse result = apiInstance.infoServiceGetInfo();\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling InfoServiceApi#infoServiceGetInfo\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Argo Server Authentication Modes in Bash\nDESCRIPTION: This snippet demonstrates how to configure the Argo Server's authentication modes using the `--auth-mode` flag.  Multiple `--auth-mode` flags can be used to enable multiple authentication methods. This allows clients to authenticate using either SSO or a client bearer token.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-auth-mode.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo server --auth-mode=sso --auth-mode=client\n```\n\n----------------------------------------\n\nTITLE: Edit Workflow Controller ConfigMap\nDESCRIPTION: This bash command edits the `workflow-controller-configmap` in the `argo` namespace. It is used to configure the default artifact repository for Argo Workflows. It assumes Argo Workflows is installed in the `argo` namespace.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl edit configmap workflow-controller-configmap -n argo  # assumes argo was installed in the argo namespace\n```\n\n----------------------------------------\n\nTITLE: Retry and Watch Workflow\nDESCRIPTION: This command retries the specified workflow and watches its progress until completion. The `--watch` flag provides real-time updates on the workflow's status.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_retry.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nargo retry --watch my-wf.yaml\n```\n\n----------------------------------------\n\nTITLE: Workflow Definition with Parameters (YAML)\nDESCRIPTION: This YAML snippet defines an Argo Workflow that uses parameters. It showcases how to declare an input parameter named `message` within the `print-message` template and how to pass a default value to it from the `arguments` section.  The `echo` command in the container uses the input parameter as an argument.  The curly braces in `args` need to be escaped with double quotes.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/parameters.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-parameters-\nspec:\n  # invoke the print-message template with \"hello world\" as the argument to the message parameter\n  entrypoint: print-message\n  arguments:\n    parameters:\n    - name: message\n      value: hello world\n\n  templates:\n  - name: print-message\n    inputs:\n      parameters:\n      - name: message       # parameter declaration\n    container:\n      # run echo with that message input parameter as args\n      image: busybox\n      command: [echo]\n      args: [\"{{inputs.parameters.message}}\"]\n```\n\n----------------------------------------\n\nTITLE: Submit Workflow from SDK Objects\nDESCRIPTION: This Python snippet shows how to create and submit an Argo Workflow using SDK objects. It defines a workflow with a single 'whalesay' template, then uses the Argo Workflows API to submit the workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint\n\nimport argo_workflows\nfrom argo_workflows.api import workflow_service_api\nfrom argo_workflows.model.container import Container\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_template import IoArgoprojWorkflowV1alpha1Template\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_workflow import IoArgoprojWorkflowV1alpha1Workflow\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_workflow_create_request import (\n    IoArgoprojWorkflowV1alpha1WorkflowCreateRequest,\n)\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_workflow_spec import (\n    IoArgoprojWorkflowV1alpha1WorkflowSpec,\n)\nfrom argo_workflows.model.object_meta import ObjectMeta\n\nconfiguration = argo_workflows.Configuration(host=\"https://127.0.0.1:2746\")\nconfiguration.verify_ssl = False\n\nmanifest = IoArgoprojWorkflowV1alpha1Workflow(\n    metadata=ObjectMeta(generate_name='hello-world-'),\n    spec=IoArgoprojWorkflowV1alpha1WorkflowSpec(\n        entrypoint='whalesay',\n        templates=[\n            IoArgoprojWorkflowV1alpha1Template(\n                name='whalesay',\n                container=Container(\n                    image='docker/whalesay:latest', command=['cowsay'], args=['hello world']))]))\n\napi_client = argo_workflows.ApiClient(configuration)\napi_instance = workflow_service_api.WorkflowServiceApi(api_client)\n\nif __name__ == '__main__':\n    api_response = api_instance.create_workflow(\n        namespace='argo',\n        body=IoArgoprojWorkflowV1alpha1WorkflowCreateRequest(workflow=manifest),\n        _check_return_type=False)\n    pprint(api_response)\n\n```\n\n----------------------------------------\n\nTITLE: Submitting a workflow from an existing resource with Argo\nDESCRIPTION: This command submits a workflow from an existing resource. The `--from` flag is used to submit a workflow from a `cronwf` kind named `my-cron-wf`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_submit.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nargo submit --from cronwf/my-cron-wf\n```\n\n----------------------------------------\n\nTITLE: Creating an S3 Bucket\nDESCRIPTION: This command creates an S3 bucket using the AWS CLI.  `mybucket` variable stores the bucket name. The `--region` flag is optional and specifies the AWS region.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmybucket=my-bucket-name\naws s3 mb s3://mybucket [--region xxx]\n```\n\n----------------------------------------\n\nTITLE: Configure Argo Workflows SSO Settings\nDESCRIPTION: This YAML configures the Argo Workflows ConfigMap to specify the SSO settings.  `issuer` points to the Dex endpoint. `clientId` and `clientSecret` specify the secret name and keys to retrieve the OIDC client ID and secret. `redirectUrl` must match the redirect URI configured in Dex and the Argo Workflows server's callback endpoint.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso-argocd.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workflow-controller-configmap\ndata:\n  # SSO Configuration for the Argo server.\n  # You must also start argo server with `--auth-mode sso`.\n  # https://argo-workflows.readthedocs.io/en/latest/argo-server-auth-mode/\n  sso: |\n    # This is the root URL of the OIDC provider (required).\n    issuer: https://argo-cd.mydomain.com/api/dex\n    # This is name of the secret and the key in it that contain OIDC client\n    # ID issued to the application by the provider (required).\n    clientId:\n      name: argo-workflows-sso\n      key: client-id\n    # This is name of the secret and the key in it that contain OIDC client\n    # secret issued to the application by the provider (required).\n    clientSecret:\n      name: argo-workflows-sso\n      key: client-secret\n    # This is the redirect URL supplied to the provider (required). It must\n    # be in the form <argo-server-root-url>/oauth2/callback. It must be\n    # browser-accessible.\n    redirectUrl: https://argo-workflows.mydomain.com/oauth2/callback\n```\n\n----------------------------------------\n\nTITLE: Multi-line Description Annotation in YAML\nDESCRIPTION: This example shows how to define a multi-line description within the `workflows.argoproj.io/description` annotation in Argo Workflows. The multi-line capability allows for more extensive descriptions, displayed when a workflow's row is expanded in the UI, providing additional context and information.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/title-and-description.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# markdown title, multi-line markdown description with URL converted into an anchor link\nmetadata:\n  annotations:\n    workflows.argoproj.io/title: '**Test Title**'\n    workflows.argoproj.io/description: |\n      `This is a simple hello world example.`\n      You can also run it in Python: https://couler-proj.github.io/couler/examples/#hello-world\n```\n\n----------------------------------------\n\nTITLE: Listing Running Workflows with Argo CLI\nDESCRIPTION: Lists all workflows that are currently in a running state. It uses the `--running` flag to filter the results.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_list.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo list --running\n```\n\n----------------------------------------\n\nTITLE: Configure Azure Artifact Repository\nDESCRIPTION: This YAML snippet configures an Azure Blob Storage artifact repository in the `workflow-controller-configmap`. It specifies the container name and references a Kubernetes Secret (`accountKeySecret`) containing the Azure Blob Storage account key required to access the container.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_31\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  artifactRepository: |\n    azure:\n      container: my-container\n      blobNameFormat: prefix/in/container     #optional, it could reference workflow variables, such as \"{{workflow.name}}/{{pod.name}}\"\n      accountKeySecret:\n        name: my-azure-storage-credentials\n        key: account-access-key\n```\n\n----------------------------------------\n\nTITLE: Configure Service Account for SSO RBAC (YAML)\nDESCRIPTION: This snippet configures a service account to be used with SSO RBAC by annotating it with `workflows.argoproj.io/rbac-rule` to define a rule based on OIDC groups.  It also defines `workflows.argoproj.io/rbac-rule-precedence` to resolve conflicts between multiple matching rules.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  annotations:\n    # The rule is an expression used to determine if this service account\n    # should be used.\n    # * `groups` - an array of the OIDC groups\n    # * `iss` - the issuer (\"argo-server\")\n    # * `sub` - the subject (typically the username)\n    # Must evaluate to a boolean.\n    # If you want an account to be the default to use, this rule can be \"true\".\n    # Details of the expression language are available in\n    # https://expr-lang.org/docs/language-definition.\n    workflows.argoproj.io/rbac-rule: \"'admin' in groups\"\n    # The precedence is used to determine which service account to use when\n    # Precedence is an integer. It may be negative. If omitted, it defaults to \"0\".\n    # Numerically higher values have higher precedence (not lower, which maybe\n    # counter-intuitive to you).\n    # If two rules match and have the same precedence, then which one used will\n    # be arbitrary.\n    workflows.argoproj.io/rbac-rule-precedence: \"1\"\n```\n\n----------------------------------------\n\nTITLE: Executor Plugin Resource and Security Context Example\nDESCRIPTION: This YAML snippet demonstrates configuring resource limits and security context for the sidecar container.  It enforces resource requests and limits for CPU and memory, and ensures the container runs as non-root with a specified user ID.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  sidecar:\n    container:\n      resources:\n        requests:\n          cpu: 100m\n          memory: 32Mi\n        limits:\n          cpu: 200m\n          memory: 64Mi\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n```\n\n----------------------------------------\n\nTITLE: Patching CronTab Custom Resource with Resource Template (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to patch a CronTab custom resource using the resource template in an Argo workflow. It defines a workflow with a single template that uses the `resource` field to specify the `patch` action along with the `mergeStrategy`.  The CronTab resource is patched to update the cronSpec, while keeping the image the same.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/kubernetes-resources.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: k8s-patch-\nspec:\n  entrypoint: cront-tmpl\n  templates:\n  - name: cront-tmpl\n    resource:\n      action: patch\n      mergeStrategy: merge                 # Must be one of [strategic merge json]\n      manifest: |\n        apiVersion: \"stable.example.com/v1\"\n        kind: CronTab\n        spec:\n          cronSpec: \"* * * * */10\"\n          image: my-awesome-cron-image\n```\n\n----------------------------------------\n\nTITLE: Stop Workflow via Argo API using curl\nDESCRIPTION: Stops a suspended Argo workflow via the Argo API. The `curl` command sends a PUT request to the `/stop` endpoint, including the namespace, workflow name, a node field selector based on the UUID of the job, and a failure message. It requires an Argo access token ($ARGO_TOKEN) for authentication.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/async-pattern.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ncurl --request PUT \\\n  --url https://localhost:2746/api/v1/workflows/<NAMESPACE>/<WORKFLOWNAME>/stop \\\n  --header 'content-type: application/json' \\\n  --header \"Authorization: $ARGO_TOKEN\" \\\n  --data '{\n      \"namespace\": \"<NAMESPACE>\",\n      \"name\": \"<WORKFLOWNAME>\",\n      \"nodeFieldSelector\": \"inputs.parameters.uuid.value=<UUID>\",\n      \"message\": \"<FAILURE-MESSAGE>\"\n    }'\n```\n\n----------------------------------------\n\nTITLE: Accessing Aggregate Loop Results (YAML)\nDESCRIPTION: This example shows how to access the aggregate results of a loop in Argo Workflows. The output of each iteration _must_ be a valid JSON.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/loops.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: loop-test\nspec:\n  entrypoint: main\n  templates:\n  - name: main\n    steps:\n    - - name: execute-parallel-steps\n        template: print-json-entry\n        arguments:\n          parameters:\n          - name: index\n            value: '{{item}}'\n        withParam: '[1, 2, 3]'\n    - - name: call-access-aggregate-output\n        template: access-aggregate-output\n        arguments:\n          parameters:\n          - name: aggregate-results\n            # If the value of each loop iteration isn't a valid JSON,\n            # you get a JSON parse error:\n            value: '{{steps.execute-parallel-steps.outputs.result}}'\n  - name: print-json-entry\n    inputs:\n      parameters:\n      - name: index\n    # The output must be a valid JSON\n    script:\n      image: alpine:latest\n      command: [sh]\n      source: |\n        cat <<EOF\n        {\n        \"input\": \"{{inputs.parameters.index}}\",\n        \"transformed-input\": \"{{inputs.parameters.index}}.jpeg\"\n        }\n        EOF\n  - name: access-aggregate-output\n    inputs:\n      parameters:\n      - name: aggregate-results\n        value: 'no-value'\n    script:\n      image: alpine:latest\n      command: [sh]\n      source: |\n        echo 'inputs.parameters.aggregate-results: \"{{inputs.parameters.aggregate-results}}\"'\n```\n\n----------------------------------------\n\nTITLE: Create Kubernetes Secret for Azure SAS Token\nDESCRIPTION: This snippet creates a Kubernetes Secret named `my-azure-storage-credentials` to store the Azure storage account SAS token. The secret is created using `kubectl create secret generic` and populated with the SAS token from the `SAS_TOKEN` variable, associated with the key `shared-access-key` within the secret.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic my-azure-storage-credentials \\\n  --from-literal \"shared-access-key=$SAS_TOKEN\"\n```\n\n----------------------------------------\n\nTITLE: Suspend a specific Argo workflow\nDESCRIPTION: This example demonstrates how to suspend a workflow named `my-wf` using the `argo suspend` command.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_suspend.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nargo suspend my-wf\n```\n\n----------------------------------------\n\nTITLE: Argo Wait Command Syntax\nDESCRIPTION: Displays the general syntax for the `argo wait` command. The command is used to wait for specified workflows to complete.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_wait.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nargo wait [WORKFLOW...] [flags]\n```\n\n----------------------------------------\n\nTITLE: Listing Cluster Workflow Templates with Java\nDESCRIPTION: This snippet demonstrates how to list cluster workflow templates using the Argo Workflows Java client. It configures the API client, sets up API key authentication, and then calls the `clusterWorkflowTemplateServiceListClusterWorkflowTemplates` method. The snippet includes detailed parameters for filtering and watching resources, along with error handling for potential API exceptions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ClusterWorkflowTemplateServiceApi.md#_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ClusterWorkflowTemplateServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ClusterWorkflowTemplateServiceApi apiInstance = new ClusterWorkflowTemplateServiceApi(defaultClient);\n    String listOptionsLabelSelector = \"listOptionsLabelSelector_example\"; // String | A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional.\n    String listOptionsFieldSelector = \"listOptionsFieldSelector_example\"; // String | A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional.\n    Boolean listOptionsWatch = true; // Boolean | Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional.\n    Boolean listOptionsAllowWatchBookmarks = true; // Boolean | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional.\n    String listOptionsResourceVersion = \"listOptionsResourceVersion_example\"; // String | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    String listOptionsResourceVersionMatch = \"listOptionsResourceVersionMatch_example\"; // String | resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    String listOptionsTimeoutSeconds = \"listOptionsTimeoutSeconds_example\"; // String | Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional.\n    String listOptionsLimit = \"listOptionsLimit_example\"; // String | limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n    String listOptionsContinue = \"listOptionsContinue_example\"; // String | The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n    Boolean listOptionsSendInitialEvents = true; // Boolean | `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\\\"io.k8s.initial-events-end\\\": \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"` (for backward compatibility reasons) and to false otherwise. +optional\n    try {\n      IoArgoprojWorkflowV1alpha1ClusterWorkflowTemplateList result = apiInstance.clusterWorkflowTemplateServiceListClusterWorkflowTemplates(listOptionsLabelSelector, listOptionsFieldSelector, listOptionsWatch, listOptionsAllowWatchBookmarks, listOptionsResourceVersion, listOptionsResourceVersionMatch, listOptionsTimeoutSeconds, listOptionsLimit, listOptionsContinue, listOptionsSendInitialEvents);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ClusterWorkflowTemplateServiceApi#clusterWorkflowTemplateServiceListClusterWorkflowTemplates\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Executor Plugin HTTP Response\nDESCRIPTION: This shows the example HTTP 200 OK response from the Executor Plugin after successful template execution. It indicates that the node has succeeded and contains a message.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nHTTP/1.1 200 OK\n{\n  \"node\": {\n    \"phase\": \"Succeeded\",\n    \"message\": \"Hello template!\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting podSpecPatch for Large Artifacts in Argo\nDESCRIPTION: This YAML shows how to use `podSpecPatch` to increase the resource request for the init container when handling large artifacts, preventing Out of Memory errors. This is crucial when dealing with substantial data transfers within Argo Workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/artifacts.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npodSpecPatch: |\n  initContainers:\n    - name: init\n      resources:\n        requests:\n          memory: 2Gi\n          cpu: 300m\n```\n\n----------------------------------------\n\nTITLE: Create Kubernetes Secret for Service Account Token\nDESCRIPTION: Creates a Kubernetes secret to store the service account token. This token is used to authenticate with the Argo Server API.  The annotation `kubernetes.io/service-account.name` specifies which service account this token is for. This code uses a here document (<<EOF) to define the YAML content for the secret.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/access-token.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f - <<EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: jenkins.service-account-token\n  annotations:\n    kubernetes.io/service-account.name: jenkins\ntype: kubernetes.io/service-account-token\nEOF\n```\n\n----------------------------------------\n\nTITLE: Enable SSO RBAC in Workflow Controller ConfigMap (YAML)\nDESCRIPTION: This snippet shows how to enable RBAC for SSO authentication by setting `rbac.enabled` to `true` and configuring the `scopes` under the `sso` setting in the `workflow-controller-configmap.yaml` file. The scope `groups` needs to be added for group-based RBAC.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsso:\n  # ...\n  scopes:\n   - groups\n  rbac:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Retry and Tail Logs - Argo\nDESCRIPTION: This example illustrates how to retry a workflow and tail its logs until completion using the `--log` flag with the `argo archive retry` command. It provides a live view of the retried workflow's logs.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_retry.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nargo archive retry --log uid\n```\n\n----------------------------------------\n\nTITLE: Resubmitting a Workflow by UID using Argo\nDESCRIPTION: This command resubmits a workflow identified by its unique ID (UID). It uses the `argo archive resubmit` command followed by the UID of the workflow to be resubmitted. This is the simplest way to resubmit a specific workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_resubmit.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nargo archive resubmit uid\n```\n\n----------------------------------------\n\nTITLE: Argo Terminate Workflows by Label\nDESCRIPTION: This example shows how to terminate multiple workflows using a label selector. Workflows matching the label `workflows.argoproj.io/test=true` will be terminated. The `-l` flag specifies the label selector.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_terminate.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nargo terminate -l workflows.argoproj.io/test=true\n```\n\n----------------------------------------\n\nTITLE: Configure Kubernetes API Client Rate Limiting\nDESCRIPTION: This YAML configuration demonstrates how to adjust the Kubernetes API client-side rate limiting for the Argo Workflow Controller using the `--qps` and `--burst` arguments. This allows the controller to send more requests to the Kubernetes API server, reducing the likelihood of throttling. It is important to monitor the Kubernetes API server load after adjusting these values.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/scaling.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nargs:\n  - --qps=50\n  - --burst=75\n```\n\n----------------------------------------\n\nTITLE: Print Workflow Logs with Selector\nDESCRIPTION: This command prints the logs of a workflow, filtering pods based on a selector. Useful for focusing on specific pods based on labels.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_logs.md#_snippet_3\n\nLANGUAGE: CLI\nCODE:\n```\nargo logs my-wf -l app=sth\n```\n\n----------------------------------------\n\nTITLE: Deleting Event Source with Argo Workflows Python API\nDESCRIPTION: This code snippet demonstrates how to delete an event source using the Argo Workflows Event Source Service API in Python. It uses the `delete_event_source` method of the `EventSourceServiceApi` class, passing in the namespace and name of the event source to be deleted.  It handles potential exceptions during the API call and prints any errors that occur. Optional parameters for delete options, such as grace period and propagation policy, are also shown.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import event_source_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = event_source_service_api.EventSourceServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    name = \"name_example\" # str | \n    delete_options_grace_period_seconds = \"deleteOptions.gracePeriodSeconds_example\" # str | The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. (optional)\n    delete_options_preconditions_uid = \"deleteOptions.preconditions.uid_example\" # str | Specifies the target UID. +optional. (optional)\n    delete_options_preconditions_resource_version = \"deleteOptions.preconditions.resourceVersion_example\" # str | Specifies the target ResourceVersion +optional. (optional)\n    delete_options_orphan_dependents = True # bool | Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. (optional)\n    delete_options_propagation_policy = \"deleteOptions.propagationPolicy_example\" # str | Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. (optional)\n    delete_options_dry_run = [\n        \"deleteOptions.dryRun_example\",\n    ] # [str] | When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional +listType=atomic. (optional)\n    delete_options_ignore_store_read_error_with_cluster_breaking_potential = True # bool | if set to true, it will trigger an unsafe deletion of the resource in case the normal deletion flow fails with a corrupt object error. A resource is considered corrupt if it can not be retrieved from the underlying storage successfully because of a) its data can not be transformed e.g. decryption failure, or b) it fails to decode into an object. NOTE: unsafe deletion ignores finalizer constraints, skips precondition checks, and removes the object from the storage. WARNING: This may potentially break the cluster if the workload associated with the resource being unsafe-deleted relies on normal deletion flow. Use only if you REALLY know what you are doing. The default value is false, and the user must opt in to enable it +optional. (optional)\n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.delete_event_source(namespace, name)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventSourceServiceApi->delete_event_source: %s\\n\" % e)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.delete_event_source(namespace, name, delete_options_grace_period_seconds=delete_options_grace_period_seconds, delete_options_preconditions_uid=delete_options_preconditions_uid, delete_options_preconditions_resource_version=delete_options_preconditions_resource_version, delete_options_orphan_dependents=delete_options_orphan_dependents, delete_options_propagation_policy=delete_options_propagation_policy, delete_options_dry_run=delete_options_dry_run, delete_options_ignore_store_read_error_with_cluster_breaking_potential=delete_options_ignore_store_read_error_with_cluster_breaking_potential)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventSourceServiceApi->delete_event_source: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Start Argo with API Enabled - Bash\nDESCRIPTION: This bash command starts both the Argo Workflow controller and the Argo Server locally, with the API accessible at <http://localhost:2746>.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\nmake start API=true\n```\n\n----------------------------------------\n\nTITLE: Retrieve and Decode Service Account Token\nDESCRIPTION: Retrieves the service account token from the Kubernetes secret, decodes it from base64, and stores it in the `ARGO_TOKEN` environment variable. Requires `kubectl` and `base64` utilities.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/access-token.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nARGO_TOKEN=\"Bearer $(kubectl get secret jenkins.service-account-token -o=jsonpath='{.data.token}' | base64 --decode)\"\necho $ARGO_TOKEN\nBearer ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNkltS...\n```\n\n----------------------------------------\n\nTITLE: Setting Workflow Timeout in Argo Workflows (YAML)\nDESCRIPTION: This YAML configuration sets a timeout for the entire Argo Workflow. The `activeDeadlineSeconds` field in the `spec` section is set to 10, meaning the workflow will be terminated if it exceeds 10 seconds. The workflow runs a template named `sleep` which attempts to sleep for 60 seconds, but will be terminated early.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/timeouts.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: timeouts-\nspec:\n  activeDeadlineSeconds: 10 # terminate workflow after 10 seconds\n  entrypoint: sleep\n  templates:\n  - name: sleep\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"echo sleeping for 1m; sleep 60; echo done\"]\n```\n\n----------------------------------------\n\nTITLE: Argo Cron Command Usage\nDESCRIPTION: This snippet shows the basic usage of the `argo cron` command. It serves as the entry point for managing cron workflows and provides access to various subcommands for specific operations.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nargo cron [flags]\n```\n\n----------------------------------------\n\nTITLE: Workflow: Using WorkflowTemplate with foo entrypoint (YAML)\nDESCRIPTION: This workflow defines an entrypoint named 'foo' and uses a WorkflowTemplate named 'say-main-entrypoint'.  The 'echo' template from the WorkflowTemplate will print the value of `workflow.mainEntrypoint`, which will be 'foo'.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: foo-\nspec:\n  entrypoint: foo\n  templates:\n    - name: foo\n      steps:\n      - - name: step\n          templateRef:\n            name: say-main-entrypoint\n            template: echo\n```\n\n----------------------------------------\n\nTITLE: Cron Workflow Backfill Command\nDESCRIPTION: The `argo cron backfill` command creates a cron workflow backfill using the specified cron workflow name and optional flags. The flags control the start and end dates, the name of the argument used for the schedule time, and the number of workflows created.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_backfill.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nargo cron backfill cronwf [flags]\n```\n\n----------------------------------------\n\nTITLE: Rate Limit Pod Creation in Argo Workflow Controller\nDESCRIPTION: This YAML configuration demonstrates how to rate limit Pod creation requests made by the Argo Workflow Controller.  The `limit` sets the average number of Pod creation requests per second, and `burst` sets the maximum number of requests that can be made at once. Modifying these values requires careful monitoring of the Kubernetes API server load.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/scaling.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nresourceRateLimit: |\n  limit: 10\n  burst: 25\n```\n\n----------------------------------------\n\nTITLE: Configuring Argo Server HTTP1 Mode\nDESCRIPTION: This snippet outlines how to configure the Argo CLI to use HTTP1 mode for communication with the Argo Server.  It includes setting the base HREF if the server is behind an ingress with a path.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nARGO_HTTP1=true\nARGO_BASE_HREF=/argo\n```\n\n----------------------------------------\n\nTITLE: Service Accounts and Annotations for Artifact GC in Argo\nDESCRIPTION: This YAML configures service accounts and annotations for artifact garbage collection. It shows how to define service accounts at the workflow level and override them at the artifact level, providing granular control over permissions for artifact deletion.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/artifacts.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: artifact-gc-\nspec:\n  entrypoint: main\n  artifactGC:\n    strategy: OnWorkflowDeletion\n    ##############################################################################################\n    #    Workflow Level Service Account and Metadata\n    ##############################################################################################\n    serviceAccountName: my-sa\n    podMetadata:\n      annotations:\n        eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/my-iam-role\n  templates:\n    - name: main\n      container:\n        image: argoproj/argosay:v2\n        command:\n          - sh\n          - -c\n        args:\n          - |\n            echo \"can throw this away\" > /tmp/temporary-artifact.txt\n            echo \"keep this\" > /tmp/keep-this.txt\n      outputs:\n        artifacts:\n          - name: temporary-artifact\n            path: /tmp/temporary-artifact.txt\n            s3:\n              key: temporary-artifact-{{workflow.uid}}.txt\n            artifactGC:\n              ####################################################################################\n              #    Optional override capability\n              ####################################################################################\n              serviceAccountName: artifact-specific-sa\n              podMetadata:\n                annotations:\n                  eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/artifact-specific-iam-role\n          - name: keep-this\n            path: /tmp/keep-this.txt\n            s3:\n              key: keep-this-{{workflow.uid}}.txt\n            artifactGC:\n              strategy: Never\n```\n\n----------------------------------------\n\nTITLE: Listing CronWorkflows using Argo CLI\nDESCRIPTION: This command lists all CronWorkflows in the current namespace using the Argo CLI. It displays the name, age, last run time, schedule, and suspension status of each CronWorkflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cron-workflows.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n$ argo cron list\n```\n\n----------------------------------------\n\nTITLE: Creating an S3 Policy\nDESCRIPTION: This bash script creates a policy file (policy.json) that grants permissions to perform actions on objects and the bucket in S3. The policy allows `PutObject`, `GetObject`, and `DeleteObject` actions on objects within the specified bucket, and `ListBucket` action on the bucket itself. It also sets the version to 2012-10-17.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncat > policy.json <<EOF\n{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:PutObject\",\n            \"s3:GetObject\",\n            \"s3:DeleteObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::$mybucket/*\"\n      },\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\"\n         ],\n         \"Resource\":\"arn:aws:s3:::$mybucket\"\n      }\n   ]\n}\nEOF\n```\n\n----------------------------------------\n\nTITLE: Start Argo with UI Enabled - Bash\nDESCRIPTION: This bash command starts the Argo Workflow controller, Argo Server, and the UI locally, with the UI accessible at <http://localhost:8080>. `UI=true` implies `API=true`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\nmake start UI=true\n```\n\n----------------------------------------\n\nTITLE: Configure Argo CD Dex with Helm\nDESCRIPTION: This YAML configures Dex within the Argo CD Helm chart to include the required environment variable for the Argo Workflows SSO client secret and configures the static client. The `ARGO_WORKFLOWS_SSO_CLIENT_SECRET` is fetched from the secret, and the `staticClients` configuration sets the `id`, `name`, `redirectURIs`, and `secretEnv` for Argo Workflows SSO.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso-argocd.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndex:\n  image:\n    tag: v2.35.0\n  env:\n    - name: ARGO_WORKFLOWS_SSO_CLIENT_SECRET\n      valueFrom:\n        secretKeyRef:\n          name: argo-workflows-sso\n          key: client-secret\nserver:\n  config:\n    dex.config: |\n      staticClients:\n      # This is the OIDC client ID in plaintext\n      - id: argo-workflows-sso\n        name: Argo Workflow\n        redirectURIs:\n          - https://argo-workflows.mydomain.com/oauth2/callback\n        secretEnv: ARGO_WORKFLOWS_SSO_CLIENT_SECRET\n```\n\n----------------------------------------\n\nTITLE: Submitting and tailing workflow logs with Argo\nDESCRIPTION: This command submits a workflow, and tails its logs until completion. The `--log` flag lets you monitor the workflow by viewing its standard output and standard error streams.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_submit.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nargo submit --log my-wf.yaml\n```\n\n----------------------------------------\n\nTITLE: Looping with Param from Step Result in Argo Workflows (YAML)\nDESCRIPTION: This example demonstrates using `withParam` to iterate over a JSON array generated dynamically in a previous step of the workflow. This is a powerful way to create truly dynamic workflows where the number of iterations and values are determined at runtime.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/loops.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: loops-param-result-\nspec:\n  entrypoint: loop-param-result-example\n  templates:\n  - name: loop-param-result-example\n    steps:\n    - - name: generate\n        template: gen-number-list\n    # Iterate over the list of numbers generated by the generate step above\n    - - name: sleep\n        template: sleep-n-sec\n        arguments:\n          parameters:\n          - name: seconds\n            value: \"{{item}}\"\n        withParam: \"{{steps.generate.outputs.result}}\"\n\n  # Generate a list of numbers in JSON format\n  - name: gen-number-list\n    script:\n      image: python:alpine3.6\n      command: [python]\n      source: |\n        import json\n        import sys\n        json.dump([i for i in range(20, 31)], sys.stdout)\n\n  - name: sleep-n-sec\n    inputs:\n      parameters:\n      - name: seconds\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"echo sleeping for {{inputs.parameters.seconds}} seconds; sleep {{inputs.parameters.seconds}}; echo done\"]\n```\n\n----------------------------------------\n\nTITLE: Get Latest Argo Workflow Details (Bash)\nDESCRIPTION: This snippet retrieves the details of the latest workflow run using the Argo CLI. The '@latest' argument is a shortcut for accessing the most recent workflow. It displays comprehensive information about the workflow's configuration and execution history.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/quick-start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nargo get -n argo @latest\n```\n\n----------------------------------------\n\nTITLE: Update Argo Cron Workflow Template\nDESCRIPTION: This snippet demonstrates how to update a cron workflow template using the `argo cron update` command.  It uses FILE1 as the path to the cron workflow template definition file.  The updated workflow will be deployed to the Argo Workflows server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_update.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n# Update a Cron Workflow Template:\n  argo cron update FILE1\n```\n\n----------------------------------------\n\nTITLE: Argo Workflow Deletion Command\nDESCRIPTION: This command deletes Argo workflows based on provided arguments.  It accepts workflow names or flags to filter by criteria such as all workflows, completed workflows, older workflows, or workflows with a specific prefix, selector or status. The deletion is performed against the Argo Workflows server. No specific dependencies are listed in the documentation, but the `argo` CLI tool must be installed and configured.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_delete.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo delete [--dry-run] [WORKFLOW...|[--all] [--older] [--completed] [--resubmitted] [--prefix PREFIX] [--selector SELECTOR] [--force] [--status STATUS] ] [flags]\n```\n\n----------------------------------------\n\nTITLE: Follow Workflow Logs\nDESCRIPTION: This command streams the logs of a specified Argo workflow in real-time.  It continuously outputs new log entries as they are generated by the workflow's pods.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_logs.md#_snippet_2\n\nLANGUAGE: CLI\nCODE:\n```\nargo logs my-wf --follow\n```\n\n----------------------------------------\n\nTITLE: Expose Argo Server with LoadBalancer\nDESCRIPTION: This command patches the Argo Server service to be of type `LoadBalancer`, which exposes the service with an external IP. Requires kubectl and appropriate cloud provider configuration.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl patch svc argo-server -n argo -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'\n```\n\n----------------------------------------\n\nTITLE: Configure Template Defaults in Controller Level (YAML)\nDESCRIPTION: This example shows how to configure default template values at the controller level using a ConfigMap. These defaults are applied to all workflows running on the controller, unless overridden at the workflow level. This allows operators to enforce common configurations across all workflows managed by the controller.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/template-defaults.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workflow-controller-configmap\ndata:\n  # Default values that will apply to all Workflows from this controller, unless overridden on the Workflow-level\n  workflowDefaults: |\n    metadata:\n      annotations:\n        argo: workflows\n      labels:\n        foo: bar\n    spec:\n      ttlStrategy:\n        secondsAfterSuccess: 5\n      templateDefaults:\n        timeout: 30s\n```\n\n----------------------------------------\n\nTITLE: Create Kubernetes Role for Jenkins\nDESCRIPTION: Creates a Kubernetes role named 'jenkins' with permissions to list and update workflows in the argoproj.io API group. This role is used to restrict the actions that a service account can perform within the cluster.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/access-token.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create role jenkins --verb=list,update --resource=workflows.argoproj.io\n```\n\n----------------------------------------\n\nTITLE: WorkflowTemplate: Echo Main Entrypoint (YAML)\nDESCRIPTION: This WorkflowTemplate demonstrates how to access the workflow's main entrypoint using the `{{workflow.mainEntrypoint}}` variable. It defines a template named 'echo' that executes a simple echo command, printing the main entrypoint of the workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: say-main-entrypoint\nspec:\n  entrypoint: echo\n  templates:\n  - name: echo\n    container:\n      image: alpine\n      command: [echo]\n      args: [\"{{workflow.mainEntrypoint}}\"]\n```\n\n----------------------------------------\n\nTITLE: Restart Specific Node on Successful Workflow\nDESCRIPTION: This example shows how to restart a specific node on a successful workflow using the `--restart-successful` and `--node-field-selector` flags. This allows for targeted retries of individual nodes within a workflow. In this example, node with id 5 is restarted.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_retry.md#_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nargo retry my-wf --restart-successful --node-field-selector id=5\n```\n\n----------------------------------------\n\nTITLE: Get CronWorkflow Example - Java\nDESCRIPTION: This code snippet demonstrates how to retrieve a CronWorkflow using the `cronWorkflowServiceGetCronWorkflow` method of the `CronWorkflowServiceApi`. It requires the namespace and name of the CronWorkflow to be retrieved, as well as an optional resource version. An API key for authentication (BearerToken) is also configured.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/CronWorkflowServiceApi.md#_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.CronWorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    CronWorkflowServiceApi apiInstance = new CronWorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    String getOptionsResourceVersion = \"getOptionsResourceVersion_example\"; // String | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    try {\n      IoArgoprojWorkflowV1alpha1CronWorkflow result = apiInstance.cronWorkflowServiceGetCronWorkflow(namespace, name, getOptionsResourceVersion);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling CronWorkflowServiceApi#cronWorkflowServiceGetCronWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Submitting and watching workflow execution with Argo\nDESCRIPTION: This command submits a workflow and watches its execution until completion. The `--watch` flag allows you to monitor the workflow's progress in real-time.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_submit.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo submit --watch my-wf.yaml\n```\n\n----------------------------------------\n\nTITLE: Workflow Definition with Debug Pause (YAML)\nDESCRIPTION: Defines an Argo Workflow with a step that pauses after execution. The `ARGO_DEBUG_PAUSE_AFTER` environment variable is set to `true` to trigger the pause.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/debug-pause.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: pause-after-\nspec:\n  entrypoint: argosay\n  templates:\n    - name: argosay\n      container:\n        image: argoproj/argosay:v2\n        env:\n          - name: ARGO_DEBUG_PAUSE_AFTER\n            value: 'true'\n```\n\n----------------------------------------\n\nTITLE: Prometheus Service and ServiceMonitor\nDESCRIPTION: This YAML configures a headless service and a ServiceMonitor for Prometheus to scrape metrics from the Argo workflow-controller. The ServiceMonitor targets the `workflow-controller` with the label `app: workflow-controller` on port 9090.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: workflow-controller\n  name: workflow-controller-metrics\n  namespace: argo\nspec:\n  clusterIP: None\n  ports:\n  - name: metrics\n    port: 9090\n    protocol: TCP\n    targetPort: 9090\n  selector:\n    app: workflow-controller\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: argo-workflows\n  namespace: argo\nspec:\n  endpoints:\n  - port: metrics\n  selector:\n    matchLabels:\n      app: workflow-controller\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configure Readiness Probe for HTTPS - YAML\nDESCRIPTION: This snippet demonstrates how to configure the readiness probe for the Argo Server Deployment to use HTTPS. It involves editing the `readinessProbe` spec in the `argo-server` Deployment to specify the `HTTPS` scheme.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/tls.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreadinessProbe:\n    httpGet: \n        scheme: HTTPS\n```\n\n----------------------------------------\n\nTITLE: Restore Postgres Database - Console\nDESCRIPTION: This console command restores a Postgres database backup. Note that this is destructive and will delete any existing data.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_13\n\nLANGUAGE: Console\nCODE:\n```\nmake postgres-cli < db-dumps/2024-10-16T17:11:58Z.sql\n```\n\n----------------------------------------\n\nTITLE: Committing Changes with Sign-off and Conventional Commit Message (Bash)\nDESCRIPTION: This command commits changes with a sign-off and a conventional commit message, including the issue number.  It is crucial for tracking changes and maintaining a clean commit history. The --signoff flag adds a line to the commit message certifying that the committer has the rights to submit the code, and the conventional commit message format helps automate release notes and changelog generation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\ngit commit --signoff -m 'fix: Fixed broken thing. Fixes #1234'\n```\n\n----------------------------------------\n\nTITLE: Stopping the Latest Workflow using Argo CLI\nDESCRIPTION: This command stops the most recently created workflow using the Argo command-line interface. The `@latest` syntax refers to the newest workflow. The workflow's exit handlers will still be executed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_stop.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo stop @latest\n```\n\n----------------------------------------\n\nTITLE: Resubmitting and Watching Workflow Completion using Argo\nDESCRIPTION: This command resubmits a workflow and then watches its progress until completion. The `--watch` flag displays real-time updates on the workflow's status. This is only applicable to single workflow resubmissions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_resubmit.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nargo archive resubmit --watch uid\n```\n\n----------------------------------------\n\nTITLE: Configure Dex Deployment in Argo CD\nDESCRIPTION: This YAML snippet configures the Argo CD Dex deployment to inject the client secret as an environment variable. The `ARGO_WORKFLOWS_SSO_CLIENT_SECRET` environment variable is sourced from the `argo-workflows-sso` secret, specifically the `client-secret` key. This allows Dex to verify requests originating from Argo Workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso-argocd.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: argocd-dex-server\nspec:\n  template:\n    spec:\n      containers:\n        - name: dex\n          env:\n            - name: ARGO_WORKFLOWS_SSO_CLIENT_SECRET\n              valueFrom:\n                secretKeyRef:\n                  name: argo-workflows-sso\n                  key: client-secret\n```\n\n----------------------------------------\n\nTITLE: Suspending workflows using argo command\nDESCRIPTION: The `argo suspend` command suspends specified workflows. Multiple workflow names can be provided as arguments. The `@latest` specifier targets the most recently created workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_suspend.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nargo suspend WORKFLOW1 WORKFLOW2... [flags]\n```\n\n----------------------------------------\n\nTITLE: Argo Server Ingress Configuration\nDESCRIPTION: This YAML snippet configures an Ingress resource to expose the Argo Server. It uses annotations to rewrite the target path and specify the backend protocol (HTTPS). Requires an Ingress controller to be installed and configured.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argo-server\n  annotations:\n    ingress.kubernetes.io/rewrite-target: /$2\n    ingress.kubernetes.io/protocol: https # Traefik\n    nginx.ingress.kubernetes.io/backend-protocol: https # ingress-nginx\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /argo(/|$)(.*)\n        pathType: Prefix\n        backend:\n          service:\n            name: argo-server\n            port:\n              number: 2746\n```\n\n----------------------------------------\n\nTITLE: Get Latest Workflow Details using Argo CLI\nDESCRIPTION: This example demonstrates how to use the `argo get` command with the `@latest` selector to retrieve information about the most recently created workflow. The command fetches and displays the details of the latest workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_get.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo get @latest\n```\n\n----------------------------------------\n\nTITLE: Configure Workflow Pod Security Context in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure the security context for an Argo Workflow pod to run as a non-root user. It sets the `runAsNonRoot` field to `true` and specifies a `runAsUser` ID.  This configuration is crucial when pod security standards are enforced. Required dependencies: Argo Workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-pod-security-context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: security-context-\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 8737 #; any non-root user\n```\n\n----------------------------------------\n\nTITLE: Configure S3 Artifact for GCS\nDESCRIPTION: This YAML snippet configures an S3 artifact in Argo Workflows to access GCS using S3 compatible APIs. It specifies the endpoint, bucket, key, and Kubernetes secrets containing the access key and secret key for authentication.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nartifacts:\n  - name: my-output-artifact\n    path: /my-output-artifact\n    s3:\n      endpoint: storage.googleapis.com\n      bucket: my-gcs-bucket-name\n      # NOTE that, by default, all output artifacts are automatically tarred and\n      # gzipped before saving. So as a best practice, .tgz or .tar.gz\n      # should be incorporated into the key name so the resulting file\n      # has an accurate file extension.\n      key: path/in/bucket/my-output-artifact.tgz\n      accessKeySecret:\n        name: my-gcs-s3-credentials\n        key: accessKey\n      secretKeySecret:\n        name: my-gcs-s3-credentials\n        key: secretKey\n```\n\n----------------------------------------\n\nTITLE: Defining an HTTP Template in Argo Workflow\nDESCRIPTION: This YAML snippet defines an Argo Workflow that uses an HTTP template to make a GET request to a specified URL. It includes configurations for timeout, HTTP method, headers, success conditions, and a request body. The response body is automatically saved to the result output parameter.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/http-template.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: http-template-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      steps:\n        - - name: get-google-homepage\n            template: http\n            arguments:\n              parameters: [{name: url, value: \"https://www.google.com\"}]\n    - name: http\n      inputs:\n        parameters:\n          - name: url\n      http:\n        timeoutSeconds: 20 # Default 30\n        url: \"{{inputs.parameters.url}}\"\n        method: \"GET\" # Default GET\n        headers:\n          - name: \"x-header-name\"\n            value: \"test-value\"\n        # Template will succeed if evaluated to true, otherwise will fail\n        # Available variables:\n        #  request.body: string, the request body\n        #  request.headers: map[string][]string, the request headers\n        #  response.url: string, the request url\n        #  response.method: string, the request method\n        #  response.statusCode: int, the response status code\n        #  response.body: string, the response body\n        #  response.headers: map[string][]string, the response headers\n        successCondition: \"response.body contains \\\"google\\\"\" # available since v3.3\n        body: \"test body\" # Change request body\n```\n\n----------------------------------------\n\nTITLE: Start Argo with Postgres Profile - Bash\nDESCRIPTION: This bash command starts Argo with the Postgres profile for workflow archive testing. It requires a Postgres database running on <http://localhost:5432>.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\nmake start PROFILE=postgres\n```\n\n----------------------------------------\n\nTITLE: Stopping Workflows by Label Selector using Argo CLI\nDESCRIPTION: This command stops multiple workflows that match the specified label selector using the Argo command-line interface. In this example, workflows with the label 'workflows.argoproj.io/test=true' will be stopped. The workflows' exit handlers will still be executed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_stop.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo stop -l workflows.argoproj.io/test=true\n```\n\n----------------------------------------\n\nTITLE: Listing Workflows in YAML Format with Argo CLI\nDESCRIPTION: Lists workflows in YAML format. It uses the `-o yaml` flag to specify the output format. The output is a YAML representation of the workflow objects.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_list.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nargo list -o yaml\n```\n\n----------------------------------------\n\nTITLE: Argo Copy Artifacts Usage\nDESCRIPTION: Displays the general usage of the argo cp command with placeholder for workflow name, output directory, and optional flags.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cp.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo cp my-wf output-directory ... [flags]\n```\n\n----------------------------------------\n\nTITLE: Executor Plugin Configuration (plugin.yaml)\nDESCRIPTION: This YAML manifest defines the configuration for the 'hello' Executor Plugin. It specifies the sidecar container's command, image, ports, security context, and resource requirements. The sidecar container executes the Python script.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: ExecutorPlugin\nmetadata:\n  name: hello\nspec:\n  sidecar:\n    container:\n      command:\n        - python\n        - -u # disables output buffering\n        - -c\n      image: python:alpine3.6\n      name: hello-executor-plugin\n      ports:\n        - containerPort: 4355\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534 # nobody\n      resources:\n        requests:\n          memory: \"64Mi\"\n          cpu: \"250m\"\n        limits:\n          memory: \"128Mi\"\n          cpu: \"500m\"\n```\n\n----------------------------------------\n\nTITLE: Lint Workflow Template Java\nDESCRIPTION: This code snippet demonstrates how to lint a workflow template using the `workflowTemplateServiceLintWorkflowTemplate` method. It requires the `namespace` and a `IoArgoprojWorkflowV1alpha1WorkflowTemplateLintRequest` object as input, which contains the workflow template definition to be validated. The method returns an `IoArgoprojWorkflowV1alpha1WorkflowTemplate` object representing the validated workflow template.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowTemplateServiceApi.md#_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.WorkflowTemplateServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    WorkflowTemplateServiceApi apiInstance = new WorkflowTemplateServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    IoArgoprojWorkflowV1alpha1WorkflowTemplateLintRequest body = new IoArgoprojWorkflowV1alpha1WorkflowTemplateLintRequest(); // IoArgoprojWorkflowV1alpha1WorkflowTemplateLintRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1WorkflowTemplate result = apiInstance.workflowTemplateServiceLintWorkflowTemplate(namespace, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling WorkflowTemplateServiceApi#workflowTemplateServiceLintWorkflowTemplate\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Defining Hybrid Workflow with Windows and Linux - Argo YAML\nDESCRIPTION: This YAML defines an Argo Workflow that orchestrates steps running on both Windows and Linux nodes using `nodeSelector`. It includes two templates, one for a Windows container and one for a Linux container, demonstrating how to create hybrid workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/windows.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-hybrid-\nspec:\n  entrypoint: mytemplate\n  templates:\n    - name: mytemplate\n      steps:\n        - - name: step1\n            template: hello-win\n        - - name: step2\n            template: hello-linux\n\n    - name: hello-win\n      nodeSelector:\n        kubernetes.io/os: windows\n      container:\n        image: mcr.microsoft.com/windows/nanoserver:1809\n        command: [\"cmd\", \"/c\"]\n        args: [\"echo\", \"Hello from Windows Container!\"]\n    - name: hello-linux\n      nodeSelector:\n        kubernetes.io/os: linux\n      container:\n        image: alpine\n        command: [echo]\n        args: [\"Hello from Linux Container!\"]\n```\n\n----------------------------------------\n\nTITLE: DB Schema Update with Dynamic Enum in Argo\nDESCRIPTION: Defines an Argo Workflow that demonstrates programmatic generation of `enum` values for database selection. The `generate-db-list` template dynamically creates a JSON output containing an `enum` array, which the UI parses and displays as a dropdown in the `choose-db` step. The selected database name is then used to update the schema in the `update-schema` template.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/intermediate-inputs.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: intermediate-parameters-db-\nspec:\n  entrypoint: db-schema-update\n  templates:\n      - name: db-schema-update\n        steps:\n          - - name: generate-db-list\n              template: generate-db-list\n          - - name: choose-db\n              template: choose-db\n              arguments:\n                parameters:\n                  - name: db_name\n                    value: '{{steps.generate-db-list.outputs.parameters.db_list}}'\n          - - name: update-schema\n              template: update-schema\n              arguments:\n                parameters:\n                  - name: db_name\n                    value: '{{steps.choose-db.outputs.parameters.db_name}}'\n      - name: generate-db-list\n        outputs:\n          parameters:\n            - name: db_list\n              valueFrom:\n                path: /tmp/db_list.txt\n        container:\n          name: main\n          image: 'argoproj/argosay:v2'\n          command:\n            - sh\n            - '-c'\n          args:\n            - >-\n              echo \"{\\\"enum\\\": [\\\"db1\\\", \\\"db2\\\", \\\"db3\\\"]}\" | tee /tmp/db_list.txt\n      - name: choose-db\n        inputs:\n          parameters:\n            - name: db_name\n              description: >-\n                Choose DB to update a schema\n        outputs:\n          parameters:\n            - name: db_name\n              valueFrom:\n                supplied: {}\n        suspend: {}\n      - name: update-schema\n        inputs:\n          parameters:\n            - name: db_name\n        container:\n          name: main\n          image: 'argoproj/argosay:v2'\n          command:\n            - sh\n            - '-c'\n          args:\n            - echo Updating DB {{inputs.parameters.db_name}}\n```\n\n----------------------------------------\n\nTITLE: Create Kubernetes Secrets for OAuth2 Credentials (Bash)\nDESCRIPTION: This snippet creates Kubernetes secrets to store the OAuth2 client ID and client secret. These secrets are used by Argo Server for SSO authentication.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret -n argo generic client-id-secret \\\n  --from-literal=client-id-key=foo\n\nkubectl create secret -n argo generic client-secret-secret \\\n  --from-literal=client-secret-key=bar\n```\n\n----------------------------------------\n\nTITLE: List Archived Workflow Label Values in Python\nDESCRIPTION: This code snippet demonstrates how to list archived workflow label values using the Argo Workflows API client in Python. It sets up API key authentication, initializes the API client, and calls the `list_archived_workflow_label_values` method with several optional parameters for filtering, pagination, and watching for changes. The code includes error handling to catch and print any API exceptions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArchivedWorkflowServiceApi.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import archived_workflow_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_label_values import IoArgoprojWorkflowV1alpha1LabelValues\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = archived_workflow_service_api.ArchivedWorkflowServiceApi(api_client)\n    list_options_label_selector = \"listOptions.labelSelector_example\" # str | A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. (optional)\n    list_options_field_selector = \"listOptions.fieldSelector_example\" # str | A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. (optional)\n    list_options_watch = True # bool | Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. (optional)\n    list_options_allow_watch_bookmarks = True # bool | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. (optional)\n    list_options_resource_version = \"listOptions.resourceVersion_example\" # str | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_resource_version_match = \"listOptions.resourceVersionMatch_example\" # str | resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_timeout_seconds = \"listOptions.timeoutSeconds_example\" # str | Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. (optional)\n    list_options_limit = \"listOptions.limit_example\" # str | limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. (optional)\n    list_options_continue = \"listOptions.continue_example\" # str | The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. (optional)\n    list_options_send_initial_events = True # bool | `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\\\"io.k8s.initial-events-end\\\": \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"` (for backward compatibility reasons) and to false otherwise. +optional (optional)\n    namespace = \"namespace_example\" # str |  (optional)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.list_archived_workflow_label_values(list_options_label_selector=list_options_label_selector, list_options_field_selector=list_options_field_selector, list_options_watch=list_options_watch, list_options_allow_watch_bookmarks=list_options_allow_watch_bookmarks, list_options_resource_version=list_options_resource_version, list_options_resource_version_match=list_options_resource_version_match, list_options_timeout_seconds=list_options_timeout_seconds, list_options_limit=list_options_limit, list_options_continue=list_options_continue, list_options_send_initial_events=list_options_send_initial_events, namespace=namespace)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArchivedWorkflowServiceApi->list_archived_workflow_label_values: %s\\n\" % e)\n\n```\n\n----------------------------------------\n\nTITLE: Listing Workflows Finished Before a Duration with Argo CLI\nDESCRIPTION: Lists workflows that completed before a specified duration. It uses the `--older` flag with a duration value (e.g., '2h').\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_list.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nargo list --older 2h\n```\n\n----------------------------------------\n\nTITLE: Configuring Argo CLI with Namespace and Token\nDESCRIPTION: This snippet shows how to configure the Argo CLI with a specific namespace and authentication token, bypassing the default KUBECONFIG. It is useful when KUBECONFIG is not available or should not be used.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG=/dev/null\nARGO_NAMESPACE=argo\nARGO_TOKEN='Bearer ******' ;# Should always start with \"Bearer \" or \"Basic \".\n```\n\n----------------------------------------\n\nTITLE: Resubmitting Workflows by Label Selector using Argo\nDESCRIPTION: This command resubmits workflows that match a specified label selector. It uses the `-l` or `--selector` flag followed by the label query.  Only workflows with labels matching the query will be resubmitted, enabling bulk operations based on workflow metadata.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_resubmit.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nargo archive resubmit -l workflows.argoproj.io/test=true\n```\n\n----------------------------------------\n\nTITLE: Argo CLI Basic Commands in Bash\nDESCRIPTION: This code snippet demonstrates common Argo CLI commands used to manage workflows in a Kubernetes cluster. It includes commands for submitting a workflow specification, listing existing workflows, retrieving information about a specific workflow, printing logs from a workflow, and deleting a workflow. The workflow ID is represented by `hello-world-xxx`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/argo-cli.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo submit hello-world.yaml    # submit a workflow spec to Kubernetes\nargo list                       # list current workflows\nargo get hello-world-xxx        # get info about a specific workflow\nargo logs hello-world-xxx       # print the logs from a workflow\nargo delete hello-world-xxx     # delete workflow\n```\n\n----------------------------------------\n\nTITLE: Creating a Sensor using Argo Workflows Python SDK\nDESCRIPTION: This code snippet demonstrates how to create a sensor using the Argo Workflows Python SDK. It configures the API client with the necessary authentication and authorization, defines the host, and then calls the `create_sensor` method with the namespace and sensor body as parameters. It requires the `argo_workflows` package to be installed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import sensor_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.github_com_argoproj_argo_events_pkg_apis_events_v1alpha1_sensor import GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Sensor\nfrom argo_workflows.model.sensor_create_sensor_request import SensorCreateSensorRequest\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n```\n\n----------------------------------------\n\nTITLE: Receiving Events via EventService in Java\nDESCRIPTION: This Java code snippet demonstrates how to use the EventServiceApi to receive events within Argo Workflows. It sets up the API client, configures authentication using an API key (BearerToken), and then calls the eventServiceReceiveEvent method.  It takes a namespace, discriminator, and body as parameters, and prints the result or any ApiException encountered.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/EventServiceApi.md#_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.EventServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    EventServiceApi apiInstance = new EventServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | The namespace for the io.argoproj.workflow.v1alpha1. This can be empty if the client has cluster scoped permissions. If empty, then the event is \\\"broadcast\\\" to workflow event binding in all namespaces.\n    String discriminator = \"discriminator_example\"; // String | Optional discriminator for the io.argoproj.workflow.v1alpha1. This should almost always be empty. Used for edge-cases where the event payload alone is not provide enough information to discriminate the event. This MUST NOT be used as security mechanism, e.g. to allow two clients to use the same access token, or to support webhooks on unsecured server. Instead, use access tokens. This is made available as `discriminator` in the event binding selector (`/spec/event/selector)`\n    Object body = null; // Object | The event itself can be any data.\n    try {\n      Object result = apiInstance.eventServiceReceiveEvent(namespace, discriminator, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling EventServiceApi#eventServiceReceiveEvent\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Overriding Sidecar Kill Command in Argo Workflows using Annotations\nDESCRIPTION: This YAML snippet demonstrates how to override the default kill command for injected sidecars in Argo Workflows.  The `workflows.argoproj.io/kill-cmd-*` annotations within `podMetadata` are used to specify custom commands for terminating specific sidecars like Istio Proxy, Linkerd Proxy, Vault Agent, or a generic sidecar. The `%d` placeholder represents the signal number.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/sidecar-injection.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nspec:\n  podMetadata:\n    annotations:\n      workflows.argoproj.io/kill-cmd-istio-proxy: '[\"pilot-agent\", \"request\", \"POST\", \"quitquitquit\"]'\n      workflows.argoproj.io/kill-cmd-linkerd-proxy: '[\"/usr/lib/linkerd/linkerd-await\",\"sleep\",\"1\",\"--shutdown\"]'\n      workflows.argoproj.io/kill-cmd-vault-agent: '[\"sh\", \"-c\", \"kill -%d 1\"]'\n      workflows.argoproj.io/kill-cmd-sidecar: '[\"sh\", \"-c\", \"kill -%d $(pidof entrypoint.sh)\"]'\n```\n\n----------------------------------------\n\nTITLE: Create Kubernetes Secret for Azure Access Key\nDESCRIPTION: This snippet creates a Kubernetes Secret named `my-azure-storage-credentials` to store the Azure storage account access key. The secret is created using `kubectl create secret generic` and populated with the access key from the `ACCESS_KEY` variable, associated with the key `account-access-key` within the secret.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic my-azure-storage-credentials \\\n  --from-literal \"account-access-key=$ACCESS_KEY\"\n```\n\n----------------------------------------\n\nTITLE: SNS Event Source Configuration\nDESCRIPTION: Defines the configuration for an SNS (Simple Notification Service) event source in Argo Events.  It manages access and secret keys using SecretKeySelector and specifies the endpoint, region, topic ARN, and webhook details. It includes validation of the signature and definition of a webhook context for receiving events.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nsns: {\n    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SNSEventSource(\n        access_key=SecretKeySelector(\n            key=\"key_example\",\n            name=\"name_example\",\n            optional=True,\n        ),\n        endpoint=\"endpoint_example\",\n        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceFilter(\n            expression=\"expression_example\",\n        ),\n        metadata={\n            \"key\": \"key_example\",\n        },\n        region=\"region_example\",\n        role_arn=\"role_arn_example\",\n        secret_key=SecretKeySelector(\n            key=\"key_example\",\n            name=\"name_example\",\n            optional=True,\n        ),\n        topic_arn=\"topic_arn_example\",\n        validate_signature=True,\n        webhook=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1WebhookContext(\n            auth_secret=SecretKeySelector(\n                key=\"key_example\",\n                name=\"name_example\",\n                optional=True,\n            ),\n            endpoint=\"endpoint_example\",\n            max_payload_size=\"max_payload_size_example\",\n            metadata={\n                \"key\": \"key_example\",\n            },\n            method=\"method_example\",\n            port=\"port_example\",\n            server_cert_secret=SecretKeySelector(\n                key=\"key_example\",\n                name=\"name_example\",\n                optional=True,\n            ),\n            server_key_secret=SecretKeySelector(\n                key=\"key_example\",\n                name=\"name_example\",\n                optional=True,\n            ),\n            url=\"url_example\",\n        ),\n    ),\n}\n```\n\n----------------------------------------\n\nTITLE: Update Workflow Template Example - Argo\nDESCRIPTION: This example demonstrates how to update a Workflow Template using the `argo template update` command with a single file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template_update.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nargo template update FILE1\n```\n\n----------------------------------------\n\nTITLE: Setting the Cluster Name (YAML)\nDESCRIPTION: This YAML snippet demonstrates setting a unique name for your Kubernetes cluster in the Argo Workflows persistence configuration. The `clusterName` field will be populated in the `argo_archived_workflows` table, allowing you to identify which cluster the archived workflow originated from.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-archive.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\npersistence:\n  clusterName: dev-cluster\n```\n\n----------------------------------------\n\nTITLE: Using Previous Step Outputs as Inputs (YAML)\nDESCRIPTION: This YAML snippet shows how to use the output of one step as input to another step in a DAG template. It takes the 'output-param-1' from 'step-A' and uses it as the value for 'template-param-2' in 'step-B'. Similarly, it takes the 'output-artifact-1' artifact from 'step-A' and passes it as 'input-artifact-1' to 'step-B'.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-inputs.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndag:\n  tasks:\n  - name: step-A \n    template: step-template-a\n    arguments:\n      parameters:\n      - name: template-param-1\n        value: \"{{workflow.parameters.workflow-param-1}}\"\n  - name: step-B\n    dependencies: [step-A]\n    template: step-template-b\n    arguments:\n      parameters:\n      - name: template-param-2\n        value: \"{{tasks.step-A.outputs.parameters.output-param-1}}\"\n      artifacts:\n      - name: input-artifact-1\n        from: \"{{tasks.step-A.outputs.artifacts.output-artifact-1}}\"\n```\n\n----------------------------------------\n\nTITLE: Configure Alibaba Cloud OSS RRSA\nDESCRIPTION: This YAML config defines a namespace, service account, and ConfigMap for using Alibaba Cloud OSS RRSA (Resource Role Service Account) for artifact storage in Argo Workflows. It requires pod-identity-webhook to inject OIDC tokens, labels the namespace for injection, annotates the service account with the RAM role name, and configures the artifact repository to use SDK credentials.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-ns\n  labels:\n    pod-identity.alibabacloud.com/injection: 'on'\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-sa\n  namespace: rrsa-demo\n  annotations:\n    pod-identity.alibabacloud.com/role-name: $your_ram_role_name\n    \n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  # If you want to use this config map by default, name it \"artifact-repositories\". Otherwise, you can provide a reference to a\n  # different config map in `artifactRepositoryRef.configMap`.\n  name: artifact-repositories\n  annotations:\n    # v3.0 and after - if you want to use a specific key, put that key into this annotation.\n    workflows.argoproj.io/default-artifact-repository: default-oss-artifact-repository\ndata:\n  default-oss-artifact-repository: |\n    oss:\n      endpoint: http://oss-cn-zhangjiakou-internal.aliyuncs.com\n      bucket: $mybucket\n      useSDKCreds: true\n```\n\n----------------------------------------\n\nTITLE: Argo Cluster Template Usage\nDESCRIPTION: This snippet shows the basic usage of the `argo cluster-template` command. It serves as the main entry point for all cluster workflow template-related operations. No dependencies are required.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nargo cluster-template [flags]\n```\n\n----------------------------------------\n\nTITLE: Retry and Wait for Completion\nDESCRIPTION: This example shows how to retry a workflow and wait for its completion using the `--wait` flag. This is useful for automating workflows and ensuring they complete successfully.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_retry.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nargo retry --wait my-wf.yaml\n```\n\n----------------------------------------\n\nTITLE: Executor Plugin Workflow Example\nDESCRIPTION: This YAML defines an Argo Workflow that utilizes the 'hello' Executor Plugin. It defines a template named 'main' which includes the 'hello' plugin configuration.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      plugin:\n        hello: { }\n```\n\n----------------------------------------\n\nTITLE: Workflow Controller Environment Variable Equivalent\nDESCRIPTION: Demonstrates the equivalent environment variable configuration for the Workflow Controller CLI parameter.  `ARGO_MANAGED_NAMESPACE` is set before invoking the `workflow-controller` command.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/environment-variables.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nARGO_MANAGED_NAMESPACE=argo workflow-controller\n```\n\n----------------------------------------\n\nTITLE: Submitting ClusterWorkflowTemplate as Workflow using Argo CLI\nDESCRIPTION: This bash command submits a `ClusterWorkflowTemplate` as a `Workflow` using the Argo CLI, referencing the template by name. Dependencies: Argo CLI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cluster-workflow-templates.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\nargo submit --from clusterworkflowtemplate/cluster-workflow-template-submittable\n```\n\n----------------------------------------\n\nTITLE: Getting Archived Workflow in Argo Workflows using Python\nDESCRIPTION: This snippet retrieves an archived workflow using the Argo Workflows API. It initializes the API client, configures authentication using an API key (Bearer Token), and calls the `get_archived_workflow` method. It handles potential exceptions during the API call and prints the API response or the exception message.  The UID is a required parameter, while namespace and name are optional.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArchivedWorkflowServiceApi.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import archived_workflow_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_workflow import IoArgoprojWorkflowV1alpha1Workflow\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = archived_workflow_service_api.ArchivedWorkflowServiceApi(api_client)\n    uid = \"uid_example\" # str | \n    namespace = \"namespace_example\" # str |  (optional)\n    name = \"name_example\" # str |  (optional)\n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.get_archived_workflow(uid)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArchivedWorkflowServiceApi->get_archived_workflow: %s\\n\" % e)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.get_archived_workflow(uid, namespace=namespace, name=name)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArchivedWorkflowServiceApi->get_archived_workflow: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Cron Workflow Run Strategy Configuration\nDESCRIPTION: This YAML snippet demonstrates the proposed `RunStrategy` configuration within a CronWorkflow, allowing users to define the maximum number of successful workflow runs (`maxSuccess`) before the schedule is suspended. This allows the user to define the number of successful runs before stopping the workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/proposals/cron-wf-improvement-proposal.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nRunStrategy:\n maxSuccess:\n```\n\n----------------------------------------\n\nTITLE: Replacing synchronization mutex - YAML\nDESCRIPTION: This snippet shows how to replace the deprecated `mutex` field in the `synchronization` block with the new `mutexes` field, which now takes a list. The original single value is moved into a list.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/deprecations.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nsynchronization:\n  mutex:\n    name: foobar\n```\n\n----------------------------------------\n\nTITLE: Retry Workflow Example\nDESCRIPTION: This example shows the basic usage of `argo retry` to rerun a workflow named 'my-wf'.  It assumes the argo CLI is configured to connect to an Argo server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_retry.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nargo retry my-wf\n```\n\n----------------------------------------\n\nTITLE: Agent RBAC Role Configuration\nDESCRIPTION: This YAML snippet configures a Kubernetes RBAC role for the Argo Workflow agent.  It grants the agent permission to patch the status of WorkflowTaskSets.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: agent\nrules:\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - workflowtasksets/status\n    verbs:\n      - patch\n```\n\n----------------------------------------\n\nTITLE: Port Forward Argo Server for UI Access (Bash)\nDESCRIPTION: This snippet forwards port 2746 from the Argo Server service to localhost, allowing access to the Argo Workflows UI in a web browser. This requires `kubectl` and access to the Kubernetes cluster. It is necessary for interacting with Argo Workflows through the UI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/quick-start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n argo port-forward service/argo-server 2746:2746\n```\n\n----------------------------------------\n\nTITLE: Attaching IAM Role to Service Account\nDESCRIPTION: This YAML defines a Kubernetes service account and annotates it with the ARN of an IAM role.  This allows pods using this service account to assume the specified IAM role when accessing AWS resources. The `eks.amazonaws.com/role-arn` annotation is specific to EKS.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::012345678901:role/mybucket-role\n  name: myserviceaccount\n  namespace: mynamespace\n```\n\n----------------------------------------\n\nTITLE: Wait on the Latest Argo Workflow\nDESCRIPTION: Shows how to wait for the latest created workflow.  The `@latest` selector will resolve to the most recently created workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_wait.md#_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n# Wait on the latest workflow:\n\n  argo wait @latest\n```\n\n----------------------------------------\n\nTITLE: Receive Event Example - Python\nDESCRIPTION: This Python code snippet demonstrates how to receive an event using the Argo Workflows Event Service API. It shows how to configure API key authentication, create an instance of the API client, and call the `receive_event` method with the required parameters: namespace, discriminator, and body. The example also includes error handling for API exceptions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventServiceApi.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import event_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = event_service_api.EventServiceApi(api_client)\n    namespace = \"namespace_example\" # str | The namespace for the io.argoproj.workflow.v1alpha1. This can be empty if the client has cluster scoped permissions. If empty, then the event is \"broadcast\" to workflow event binding in all namespaces.\n    discriminator = \"discriminator_example\" # str | Optional discriminator for the io.argoproj.workflow.v1alpha1. This should almost always be empty. Used for edge-cases where the event payload alone is not provide enough information to discriminate the event. This MUST NOT be used as security mechanism, e.g. to allow two clients to use the same access token, or to support webhooks on unsecured server. Instead, use access tokens. This is made available as `discriminator` in the event binding selector (`/spec/event/selector)`\n    body = {} # bool, date, datetime, dict, float, int, list, str, none_type | The event itself can be any data.\n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.receive_event(namespace, discriminator, body)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventServiceApi->receive_event: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Deleting Argo Cron Workflows\nDESCRIPTION: This command deletes a specified cron workflow or all cron workflows. The `--all` flag deletes all cron workflows. Otherwise, specify the cron workflow name.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_delete.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo cron delete [CRON_WORKFLOW... | --all] [flags]\n```\n\n----------------------------------------\n\nTITLE: Retry Workflows by Field Selector\nDESCRIPTION: This example demonstrates how to retry workflows based on a field selector using the `--field-selector` flag.  Here, it selects workflows in the 'argo' namespace.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_retry.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nargo retry --field-selector metadata.namespace=argo\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Artifact with Assumed Role Credentials\nDESCRIPTION: This YAML configures an artifact in Argo Workflows to use temporary credentials obtained by assuming an IAM role.  It references Kubernetes secrets (`my-s3-credentials`) for the access key, secret key, and session token, which must be periodically refreshed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nartifacts:\n  - name: my-output-artifact\n    path: /my-output-artifact\n    s3:\n      endpoint: s3.amazonaws.com\n      bucket: my-s3-bucket\n      key: path/in/bucket/my-output-artifact.tgz\n      # The following fields are secret selectors.\n      # They reference the k8s secret named 'my-s3-credentials'.\n      # This secret is expected to have the keys 'accessKey', 'secretKey', and 'sessionToken',\n      # containing the base64 encoded credentials to the bucket.\n      accessKeySecret:\n        name: my-s3-credentials\n        key: accessKey\n      secretKeySecret:\n        name: my-s3-credentials\n        key: secretKey\n      sessionTokenSecret:\n        name: my-s3-credentials\n        key: sessionToken\n```\n\n----------------------------------------\n\nTITLE: Configuring Ingress Backend Protocol for HTTPS\nDESCRIPTION: This YAML snippet demonstrates how to configure the backend protocol for HTTPS in an ingress resource. It's necessary when the Argo server is running with TLS enabled by default and the ingress needs to forward traffic using HTTPS.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_16\n\nLANGUAGE: YAML\nCODE:\n```\nalb.ingress.kubernetes.io/backend-protocol: HTTPS\nnginx.ingress.kubernetes.io/backend-protocol: HTTPS\n```\n\n----------------------------------------\n\nTITLE: Declare webHDFS input artifact in Argo Workflow\nDESCRIPTION: This YAML snippet shows how to define an input artifact in an Argo Workflow to read a file from webHDFS using the HTTP artifact type.  The `url` parameter specifies the webHDFS endpoint with the `OPEN` operation. It assumes the file is accessible without authentication, which is generally not the case in production.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/webhdfs.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  # ...\n  inputs:\n    artifacts:\n    - name: my-art\n      path: /my-artifact\n      http:\n        url: \"https://mywebhdfsprovider.com/webhdfs/v1/file.txt?op=OPEN\"\n```\n\n----------------------------------------\n\nTITLE: Assuming IAM Role with AWS STS\nDESCRIPTION: This command uses the AWS STS `assume-role` command to retrieve temporary credentials for an IAM role. The `--role-arn` parameter specifies the ARN of the role to assume. The output contains temporary access key, secret key, and session token.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\naws sts assume-role --role-arn arn:aws:iam::012345678901:role/$mybucket-role\n```\n\n----------------------------------------\n\nTITLE: Define Template-Level Histogram Metric in Argo\nDESCRIPTION: This YAML defines a Template-level Histogram metric named `random_int_step_histogram`. It defines buckets for the histogram and sets the `value` to the output parameter `rand-int-value` from the container.  The metric is emitted only when the step succeeds.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  templates:\n    - name: random-int\n      metrics:\n        prometheus:\n          - name: random_int_step_histogram\n            help: \"Value of the int emitted by random-int at step level\"\n            when: \"{{status}} == Succeeded\"    # Only emit metric when step succeeds\n            histogram:\n              buckets:                              # Bins must be defined for histogram metrics\n                - 2.01                              # and are part of the metric descriptor.\n                - 4.01                              # All metrics in this series MUST have the\n                - 6.01                              # same buckets.\n                - 8.01\n                - 10.01\n              value: \"{{outputs.parameters.rand-int-value}}\"         # References itself for its output (see variables doc)\n      outputs:\n        parameters:\n          - name: rand-int-value\n            globalName: rand-int-value\n            valueFrom:\n              path: /tmp/rand_int.txt\n      container:\n        image: alpine:latest\n        command: [sh, -c]\n        args: [\"RAND_INT=$((1 + RANDOM % 10)); echo $RAND_INT; echo $RAND_INT > /tmp/rand_int.txt\"]\n...\n```\n\n----------------------------------------\n\nTITLE: Defining a Sidecar Container in Argo Workflow (YAML)\nDESCRIPTION: This YAML snippet defines an Argo Workflow that uses a sidecar container to run an Nginx web server. The main container uses curl to poll the Nginx container until it is ready, ensuring that the main code waits for the service to come up before running. The `image` field specifies the container image, and the `command` field provides the command to start the container.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/sidecars.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: sidecar-nginx-\nspec:\n  entrypoint: sidecar-nginx-example\n  templates:\n  - name: sidecar-nginx-example\n    container:\n      image: appropriate/curl\n      command: [sh, -c]\n      # Try to read from nginx web server until it comes up\n      args: [\"until `curl -G 'http://127.0.0.1/' >& /tmp/out`; do echo sleep && sleep 1; done && cat /tmp/out\"]\n    # Create a simple nginx web server\n    sidecars:\n    - name: nginx\n      image: nginx:1.13\n      command: [nginx, -g, daemon off;]\n```\n\n----------------------------------------\n\nTITLE: Work Avoidance Check with Marker File (Bash)\nDESCRIPTION: This script checks for the existence of a marker file. If the file exists, it indicates that the work has already been done, and the script exits. Otherwise, it performs the work and creates the marker file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/work-avoidance.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nif [ -e /work/markers/name-of-task ]; then\n    echo \"work already done\"\n    exit 0\nfi\necho \"working very hard\"\ntouch /work/markers/name-of-task\n```\n\n----------------------------------------\n\nTITLE: Configuring Archive TTL (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to configure the Time To Live (TTL) for archived workflows.  The `archiveTTL` setting specifies how long archived workflows should be retained before being deleted by the garbage collection process. In this example, the TTL is set to 10 days.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-archive.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\npersistence:\n  archiveTTL: 10d\n```\n\n----------------------------------------\n\nTITLE: Listing Completed Workflows with Argo CLI\nDESCRIPTION: Lists all workflows that have completed. It uses the `--completed` flag to filter the results. Mutually exclusive with `--running`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_list.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nargo list --completed\n```\n\n----------------------------------------\n\nTITLE: Using Argo CLI with Self-Signed Certificate (Env Vars) - Bash\nDESCRIPTION: This snippet demonstrates how to use the Argo CLI with TLS encryption when the Argo Server is using a self-signed certificate, utilizing environment variables. It involves setting the `ARGO_SECURE` and `ARGO_INSECURE_SKIP_VERIFY` environment variables to `true` and then running the Argo CLI with the `--secure` and `--insecure-skip-verify` flags.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/tls.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport ARGO_SECURE=true\nexport ARGO_INSECURE_SKIP_VERIFY=true\nargo --secure --insecure-skip-verify list\n```\n\n----------------------------------------\n\nTITLE: Creating a Cluster Workflow Template with Relaxed Validation\nDESCRIPTION: This command creates a cluster workflow template with relaxed validation. The `--strict false` flag disables strict workflow validation, which can be useful when working with templates that might not fully conform to the standard schema.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_create.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo cluster-template create FILE1 --strict false\n```\n\n----------------------------------------\n\nTITLE: Using Node Field Selector with CLI\nDESCRIPTION: Demonstrates how to use the `--node-field-selector` parameter with the Argo CLI to select a subset of nodes. The parameter is used to filter nodes based on field-value pairs. This is particularly useful for commands like resume, stop, and retry, allowing users to target specific nodes within a workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/node-field-selector.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n--node-field-selector=FIELD=VALUE\n```\n\n----------------------------------------\n\nTITLE: Filtering List with Expression in Argo\nDESCRIPTION: This expression example showcases filtering a list using a conditional expression.  The `filter` function takes a list and a boolean expression as input.  In this case, it filters the list `[1, 2]` to include only elements that are greater than 1.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nfilter([1, 2], { # > 1})\n```\n\n----------------------------------------\n\nTITLE: Argo Cron Lint Usage\nDESCRIPTION: This command validates files or directories of cron workflow manifests using the argo cron lint tool.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_lint.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nargo cron lint FILE... [flags]\n```\n\n----------------------------------------\n\nTITLE: Retry Workflows by Label Selector\nDESCRIPTION: This example uses the `-l` or `--selector` flag to retry workflows that match a specific label. In this case, it retries workflows with the label 'workflows.argoproj.io/test=true'.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_retry.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nargo retry -l workflows.argoproj.io/test=true\n```\n\n----------------------------------------\n\nTITLE: Revoke Service Account Token\nDESCRIPTION: Deletes the Kubernetes secret associated with the service account token, effectively revoking access. A new token will automatically be created.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/access-token.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete secret $SECRET\n```\n\n----------------------------------------\n\nTITLE: Retry and Tail Logs\nDESCRIPTION: This example shows how to retry a workflow and tail its logs until it completes, using the `--log` flag. This is useful for debugging and monitoring workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_retry.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nargo retry --log my-wf.yaml\n```\n\n----------------------------------------\n\nTITLE: Listing Archived Workflows in YAML Format\nDESCRIPTION: This example demonstrates listing archived workflows in YAML format using the `-o yaml` flag. The output will be a YAML representation of the archived workflows, making it easy to parse and process.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_list.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo archive list -o yaml\n```\n\n----------------------------------------\n\nTITLE: Invalid ConfigMap - Image Pull Policy\nDESCRIPTION: This YAML snippet demonstrates an invalid ConfigMap configuration where `executorImagePullPolicy` is incorrectly used. It should be nested within the `executor` section.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  ...\n  executorImagePullPolicy: IfNotPresent\n  ...\n```\n\n----------------------------------------\n\nTITLE: Argo Submit WorkflowTemplate as Workflow (CLI)\nDESCRIPTION: This command uses the Argo CLI to submit a WorkflowTemplate as a Workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nargo submit --from workflowtemplate/workflow-template-submittable\n```\n\n----------------------------------------\n\nTITLE: Argo Terminate Workflow\nDESCRIPTION: This example shows how to terminate a specific workflow by its name. The command will immediately stop the workflow without running any exit handlers. 'my-wf' is the name of the workflow to terminate.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_terminate.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nargo terminate my-wf\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL: Create Temporary Column and Trigger\nDESCRIPTION: These SQL queries create a temporary `workflowjsonb` column in the `argo_archived_workflows` table and a trigger to populate it with data from the existing `workflow` column. This allows for a near zero-downtime migration when upgrading to Argo Workflows v3.6. The queries need to be run *before* upgrading to v3.6.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n-- Add temporary workflowjsonb column\nALTER TABLE argo_archived_workflows ADD COLUMN workflowjsonb JSONB NULL;\n\n-- Add trigger to update workflowjsonb for each insert\nCREATE OR REPLACE FUNCTION update_workflow_jsonb() RETURNS TRIGGER AS $BODY$\nBEGIN\n    NEW.workflowjsonb=NEW.workflow;\n    RETURN NEW;\nEND\n$BODY$ LANGUAGE PLPGSQL;\n\nCREATE TRIGGER argo_archived_workflows_update_workflow_jsonb\nBEFORE INSERT ON argo_archived_workflows\nFOR EACH ROW EXECUTE PROCEDURE update_workflow_jsonb();\n\n-- Backfill existing rows\nUPDATE argo_archived_workflows SET workflowjsonb = workflow WHERE workflowjsonb IS NULL;\n```\n\n----------------------------------------\n\nTITLE: Build Argo Executor Image - Bash\nDESCRIPTION: This bash command builds the Argo Executor image. This is necessary if changes have been made to the executor code. The `TARGET_PLATFORM` environment variable allows specifying target platforms (e.g., arm64, amd64).\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nmake argoexec-image\n```\n\n----------------------------------------\n\nTITLE: Valid ConfigMap - Image Pull Policy\nDESCRIPTION: This YAML snippet shows a valid ConfigMap configuration, correctly nesting `imagePullPolicy` within the `executor` section. This is the correct way to specify the image pull policy for the executor.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  ...\n  executor: |\n    imagePullPolicy: IfNotPresent\n  ...\n```\n\n----------------------------------------\n\nTITLE: Run Argo Server Locally\nDESCRIPTION: This command starts the Argo Server in local mode, which does not require complex setup or a database. It listens on port 2746 by default.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo server\n```\n\n----------------------------------------\n\nTITLE: Configure Default Service Account for SSO RBAC (YAML)\nDESCRIPTION: This snippet configures a default service account to be used if no other RBAC rule matches. This can be used to provide a read-only account as a fallback.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmetadata:\n  name: read-only\n  annotations:\n    workflows.argoproj.io/rbac-rule: \"true\"\n    workflows.argoproj.io/rbac-rule-precedence: \"0\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Executor Plugins in Workflow Controller (Deployment)\nDESCRIPTION: This YAML snippet configures the Argo Workflow Controller deployment to enable Executor Plugins by setting the ARGO_EXECUTOR_PLUGINS environment variable to \"true\".  This is required for the controller to recognize and utilize configured executor plugins.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: workflow-controller\nspec:\n  template:\n    spec:\n      containers:\n        - name: workflow-controller\n          env:\n            - name: ARGO_EXECUTOR_PLUGINS\n              value: \"true\"\n```\n\n----------------------------------------\n\nTITLE: Configure Alibaba Cloud OSS\nDESCRIPTION: This bash script configures Alibaba Cloud OSS for artifact storage in Argo Workflows. It sets environment variables, creates a bucket, creates a RAM user with limited permissions, generates access keys, creates Kubernetes secrets, and a ConfigMap for default artifact repository settings. The script utilizes the `aliyun` CLI and `kubectl`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n$ export mybucket=bucket-workflow-artifect\n$ export myregion=cn-zhangjiakou\n$ # limit permission to read/write the bucket.\n$ cat > policy.json <<EOF\n{\n    \"Version\": \"1\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"oss:PutObject\",\n              \"oss:GetObject\"\n            ],\n            \"Resource\": \"acs:oss:*:*:$mybucket/*\"\n        }\n    ]\n}\nEOF\n$ # create bucket.\n$ aliyun oss mb oss://$mybucket --region $myregion\n$ # show endpoint of bucket.\n$ aliyun oss stat oss://$mybucket\n$ #create a ram user to access bucket.\n$ aliyun ram CreateUser --UserName $mybucket-user\n$ # create ram policy with the limit permission.\n$ aliyun ram CreatePolicy --PolicyName $mybucket-policy --PolicyDocument \"$(cat policy.json)\"\n$ # attch ram policy to the ram user.\n$ aliyun ram AttachPolicyToUser --UserName $mybucket-user --PolicyName $mybucket-policy --PolicyType Custom\n$ # create access key and secret key for the ram user.\n$ aliyun ram CreateAccessKey --UserName $mybucket-user > access-key.json\n$ # create secret in demo namespace, replace demo with your namespace.\n$ kubectl create secret generic $mybucket-credentials -n demo\\\n  --from-literal \"accessKey=$(cat access-key.json | jq -r .AccessKey.AccessKeyId)\" \\\n  --from-literal \"secretKey=$(cat access-key.json | jq -r .AccessKey.AccessKeySecret)\"\n$ # create configmap to config default artifact for a namespace.\n$ cat > default-artifact-repository.yaml << EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  # If you want to use this config map by default, name it \"artifact-repositories\". Otherwise, you can provide a reference to a\n  # different config map in `artifactRepositoryRef.configMap`.\n  name: artifact-repositories\n  annotations:\n    # v3.0 and after - if you want to use a specific key, put that key into this annotation.\n    workflows.argoproj.io/default-artifact-repository: default-oss-artifact-repository\ndata:\n  default-oss-artifact-repository: |\n    oss:\n      endpoint: http://oss-cn-zhangjiakou-internal.aliyuncs.com\n      bucket: $mybucket\n      # accessKeySecret and secretKeySecret are secret selectors.\n      # It references the k8s secret named 'bucket-workflow-artifect-credentials'.\n      # This secret is expected to have the keys 'accessKey'\n      # and 'secretKey', containing the base64 encoded credentials\n      # to the bucket.\n      accessKeySecret:\n        name: $mybucket-credentials\n        key: accessKey\n      secretKeySecret:\n        name: $mybucket-credentials\n        key: secretKey\nEOF\n# create cm in demo namespace, replace demo with your namespace.\n$ k apply -f default-artifact-repository.yaml -n demo\n```\n\n----------------------------------------\n\nTITLE: Argo Terminate Latest Workflow\nDESCRIPTION: This example demonstrates how to terminate the latest workflow.  The `@latest` selector will target the most recently created workflow.  No specific workflow name needs to be provided.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_terminate.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nargo terminate @latest\n```\n\n----------------------------------------\n\nTITLE: Listing Archived Workflows\nDESCRIPTION: This example demonstrates listing all archived workflows using the `argo archive list` command. It retrieves all workflows stored in the Argo archive without any filtering or specific output format.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_list.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo archive list\n```\n\n----------------------------------------\n\nTITLE: Workflow Referencing External Template (Steps) YAML\nDESCRIPTION: This example shows how to reference a template from another WorkflowTemplate using the `templateRef` field within a `steps` template. It specifies the name of the WorkflowTemplate and the name of the template to reference.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: workflow-template-hello-world-\nspec:\n  entrypoint: hello-world\n  templates:\n  - name: hello-world\n    steps:                              # You should only reference external \"templates\" in a \"steps\" or \"dag\" \"template\".\n      - - name: call-print-message\n          templateRef:                  # You can reference a \"template\" from another \"WorkflowTemplate\" using this field\n            name: workflow-template-1   # This is the name of the \"WorkflowTemplate\" CRD that contains the \"template\" you want\n            template: print-message     # This is the name of the \"template\" you want to reference\n          arguments:                    # You can pass in arguments as normal\n            parameters:\n            - name: message\n              value: \"hello world\"\n```\n\n----------------------------------------\n\nTITLE: Resubmitting Archived Workflow in Java\nDESCRIPTION: This Java code snippet demonstrates how to resubmit an archived workflow using the Argo Workflow API. It sets up the API client, configures authentication, creates a request body, and calls the `archivedWorkflowServiceResubmitArchivedWorkflow` method. Error handling is also included to catch potential `ApiException`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ArchivedWorkflowServiceApi.md#_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ArchivedWorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ArchivedWorkflowServiceApi apiInstance = new ArchivedWorkflowServiceApi(defaultClient);\n    String uid = \"uid_example\"; // String | \n    IoArgoprojWorkflowV1alpha1ResubmitArchivedWorkflowRequest body = new IoArgoprojWorkflowV1alpha1ResubmitArchivedWorkflowRequest(); // IoArgoprojWorkflowV1alpha1ResubmitArchivedWorkflowRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1Workflow result = apiInstance.archivedWorkflowServiceResubmitArchivedWorkflow(uid, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ArchivedWorkflowServiceApi#archivedWorkflowServiceResubmitArchivedWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Archived Workflow Label Keys using Argo Workflows API in Java\nDESCRIPTION: This code snippet demonstrates how to list archived workflow label keys using the Argo Workflows API. It initializes the ApiClient, configures authentication with an API key, and then calls the archivedWorkflowServiceListArchivedWorkflowLabelKeys method. It requires the namespace. The method returns an IoArgoprojWorkflowV1alpha1LabelKeys object, and any ApiException is caught and its details are printed to the console.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ArchivedWorkflowServiceApi.md#_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ArchivedWorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ArchivedWorkflowServiceApi apiInstance = new ArchivedWorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    try {\n      IoArgoprojWorkflowV1alpha1LabelKeys result = apiInstance.archivedWorkflowServiceListArchivedWorkflowLabelKeys(namespace);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ArchivedWorkflowServiceApi#archivedWorkflowServiceListArchivedWorkflowLabelKeys\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Volume Sources (Python)\nDESCRIPTION: This code configures various Kubernetes volume sources using Python classes.  Each volume source is initialized with example parameters demonstrating the available configuration options. This snippet aims to demonstrate how volumes can be statically defined.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nhost_path=HostPathVolumeSource(\n                                path=\"path_example\",\n                                type=\"type_example\",\n                            ),\n                            image=ImageVolumeSource(\n                                pull_policy=\"pull_policy_example\",\n                                reference=\"reference_example\",\n                            ),\n                            iscsi=ISCSIVolumeSource(\n                                chap_auth_discovery=True,\n                                chap_auth_session=True,\n                                fs_type=\"fs_type_example\",\n                                initiator_name=\"initiator_name_example\",\n                                iqn=\"iqn_example\",\n                                iscsi_interface=\"iscsi_interface_example\",\n                                lun=1,\n                                portals=[\n                                    \"portals_example\",\n                                ],\n                                read_only=True,\n                                secret_ref=LocalObjectReference(\n                                    name=\"name_example\",\n                                ),\n                                target_portal=\"target_portal_example\",\n                            ),\n                            name=\"name_example\",\n                            nfs=NFSVolumeSource(\n                                path=\"path_example\",\n                                read_only=True,\n                                server=\"server_example\",\n                            ),\n                            persistent_volume_claim=PersistentVolumeClaimVolumeSource(\n                                claim_name=\"claim_name_example\",\n                                read_only=True,\n                            ),\n                            photon_persistent_disk=PhotonPersistentDiskVolumeSource(\n                                fs_type=\"fs_type_example\",\n                                pd_id=\"pd_id_example\",\n                            ),\n                            portworx_volume=PortworxVolumeSource(\n                                fs_type=\"fs_type_example\",\n                                read_only=True,\n                                volume_id=\"volume_id_example\",\n                            ),\n                            projected=ProjectedVolumeSource(\n                                default_mode=1,\n                                sources=[\n                                    VolumeProjection(\n                                        cluster_trust_bundle=ClusterTrustBundleProjection(\n                                            label_selector=LabelSelector(\n                                                match_expressions=[\n                                                    LabelSelectorRequirement(\n                                                        key=\"key_example\",\n                                                        operator=\"operator_example\",\n                                                        values=[\n                                                            \"values_example\",\n                                                        ],\n                                                    ),\n                                                ],\n                                                match_labels= {\n                                                    \"key\": \"key_example\",\n                                                },\n                                            ),\n                                            name=\"name_example\",\n                                            optional=True,\n                                            path=\"path_example\",\n                                            signer_name=\"signer_name_example\",\n                                        ),\n                                        config_map=ConfigMapProjection(\n                                            items=[\n                                                KeyToPath(\n                                                    key=\"key_example\",\n                                                    mode=1,\n                                                    path=\"path_example\",\n                                                ),\n                                            ],\n                                            name=\"name_example\",\n                                            optional=True,\n                                        ),\n                                        downward_api=DownwardAPIProjection(\n                                            items=[\n                                                DownwardAPIVolumeFile(\n                                                    field_ref=ObjectFieldSelector(\n                                                        api_version=\"api_version_example\",\n                                                        field_path=\"field_path_example\",\n                                                    ),\n                                                    mode=1,\n                                                    path=\"path_example\",\n                                                    resource_field_ref=ResourceFieldSelector(\n                                                        container_name=\"container_name_example\",\n                                                        divisor=\"divisor_example\",\n                                                        resource=\"resource_example\",\n                                                    ),\n                                                ),\n                                            ],\n                                        ),\n                                        secret=SecretProjection(\n                                            items=[\n                                                KeyToPath(\n                                                    key=\"key_example\",\n                                                    mode=1,\n                                                    path=\"path_example\",\n                                                ),\n                                            ],\n                                            name=\"name_example\",\n                                            optional=True,\n                                        ),\n                                        service_account_token=ServiceAccountTokenProjection(\n                                            audience=\"audience_example\",\n                                            expiration_seconds=1,\n                                            path=\"path_example\",\n                                        ),\n                                    ),\n                                ],\n                            ),\n                            quobyte=QuobyteVolumeSource(\n                                group=\"group_example\",\n                                read_only=True,\n                                registry=\"registry_example\",\n                                tenant=\"tenant_example\",\n                                user=\"user_example\",\n                                volume=\"volume_example\",\n                            ),\n                            rbd=RBDVolumeSource(\n                                fs_type=\"fs_type_example\",\n                                image=\"image_example\",\n                                keyring=\"keyring_example\",\n                                monitors=[\n                                    \"monitors_example\",\n                                ],\n                                pool=\"pool_example\",\n                                read_only=True,\n                                secret_ref=LocalObjectReference(\n                                    name=\"name_example\",\n                                ),\n                                user=\"user_example\",\n                            ),\n                            scale_io=ScaleIOVolumeSource(\n                                fs_type=\"fs_type_example\",\n                                gateway=\"gateway_example\",\n                                protection_domain=\"protection_domain_example\",\n                                read_only=True,\n                                secret_ref=LocalObjectReference(\n                                    name=\"name_example\",\n                                ),\n                                ssl_enabled=True,\n                                storage_mode=\"storage_mode_example\",\n                                storage_pool=\"storage_pool_example\",\n                                system=\"system_example\",\n                                volume_name=\"volume_name_example\",\n                            ),\n                            secret=SecretVolumeSource(\n                                default_mode=1,\n                                items=[\n                                    KeyToPath(\n                                        key=\"key_example\",\n                                        mode=1,\n                                        path=\"path_example\",\n                                    ),\n                                ],\n                                optional=True,\n                                secret_name=\"secret_name_example\",\n                            ),\n                            storageos=StorageOSVolumeSource(\n                                fs_type=\"fs_type_example\",\n                                read_only=True,\n                                secret_ref=LocalObjectReference(\n                                    name=\"name_example\",\n                                ),\n                                volume_name=\"volume_name_example\",\n                                volume_namespace=\"volume_namespace_example\",\n                            ),\n                            vsphere_volume=VsphereVirtualDiskVolumeSource(\n                                fs_type=\"fs_type_example\",\n\n```\n\n----------------------------------------\n\nTITLE: Deleting Archived Workflow by UID - Bash\nDESCRIPTION: This command deletes an archived workflow using its unique identifier (UID). It requires the Argo CLI to be configured and connected to an Argo Workflows server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_delete.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo archive delete abc123-def456-ghi789-jkl012\n```\n\n----------------------------------------\n\nTITLE: Casting to String with Expression in Argo\nDESCRIPTION: This code snippet illustrates how to cast a value to a string using the `string` function within an expression. In this case, the number 1 is converted to its string representation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nstring(1)\n```\n\n----------------------------------------\n\nTITLE: Stopping Workflows by Field Selector using Argo CLI\nDESCRIPTION: This command stops multiple workflows that match the specified field selector using the Argo command-line interface. In this example, workflows within the 'argo' namespace will be stopped. The workflows' exit handlers will still be executed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_stop.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nargo stop --field-selector metadata.namespace=argo\n```\n\n----------------------------------------\n\nTITLE: CronWorkflow with time-based skip forward condition\nDESCRIPTION: Uses the `when` condition to prevent duplicate schedules when the clock leaps forward. This workflow runs daily at 02:30 and 03:00, but it checks if the last run was less than an hour ago before executing the 3:00 schedule.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cron-workflows.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nschedules:\n  - 30 2 * * *\n  - 0 3 * * *\nwhen: \"{{= cronworkflow.lastScheduledTime == nil || (now() - cronworkflow.lastScheduledTime).Seconds() > 3600 }}\"\n```\n\n----------------------------------------\n\nTITLE: Configure Default Login Service Account (YAML)\nDESCRIPTION: This snippet configures a default service account in the installation namespace that allows access to all users of the organization.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: user-default-login\n  annotations:\n    workflows.argoproj.io/rbac-rule: \"true\"\n    workflows.argoproj.io/rbac-rule-precedence: \"0\"\n```\n\n----------------------------------------\n\nTITLE: YAML: Disable Metrics\nDESCRIPTION: This YAML snippet demonstrates how to disable specific metrics in Argo Workflows using the `metricsConfig` configuration. This can be useful for reducing the number of metrics collected or for disabling metrics that are not relevant to your monitoring needs. It is relevant to Argo Workflows v3.6 upgrade, related to the new metrics introduced.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nmetricsConfig: |\n  modifiers:\n    build_info:\n      disable: true\n...\n```\n\n----------------------------------------\n\nTITLE: Listing Archived Workflows using Argo Workflows Python SDK\nDESCRIPTION: This code snippet demonstrates how to list archived workflows using the Argo Workflows Python SDK. It initializes the API client, creates an instance of the `ArchivedWorkflowServiceApi`, and calls the `list_archived_workflows` method with several optional parameters for filtering and pagination. It prints the API response or any exception that occurs during the process.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArchivedWorkflowServiceApi.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = archived_workflow_service_api.ArchivedWorkflowServiceApi(api_client)\n    list_options_label_selector = \"listOptions.labelSelector_example\" # str | A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. (optional)\n    list_options_field_selector = \"listOptions.fieldSelector_example\" # str | A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. (optional)\n    list_options_watch = True # bool | Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. (optional)\n    list_options_allow_watch_bookmarks = True # bool | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. (optional)\n    list_options_resource_version = \"listOptions.resourceVersion_example\" # str | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_resource_version_match = \"listOptions.resourceVersionMatch_example\" # str | resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_timeout_seconds = \"listOptions.timeoutSeconds_example\" # str | Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. (optional)\n    list_options_limit = \"listOptions.limit_example\" # str | limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. (optional)\n    list_options_continue = \"listOptions.continue_example\" # str | The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. (optional)\n    list_options_send_initial_events = True # bool | `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\\\"io.k8s.initial-events-end\\\": \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"` (for backward compatibility reasons) and to false otherwise. +optional (optional)\n    name_prefix = \"namePrefix_example\" # str |  (optional)\n    namespace = \"namespace_example\" # str |  (optional)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.list_archived_workflows(list_options_label_selector=list_options_label_selector, list_options_field_selector=list_options_field_selector, list_options_watch=list_options_watch, list_options_allow_watch_bookmarks=list_options_allow_watch_bookmarks, list_options_resource_version=list_options_resource_version, list_options_resource_version_match=list_options_resource_version_match, list_options_timeout_seconds=list_options_timeout_seconds, list_options_limit=list_options_limit, list_options_continue=list_options_continue, list_options_send_initial_events=list_options_send_initial_events, name_prefix=name_prefix, namespace=namespace)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArchivedWorkflowServiceApi->list_archived_workflows: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Valid WorkflowTemplate (v2.4-2.6) YAML\nDESCRIPTION: This example demonstrates a valid WorkflowTemplate definition for Argo Workflows versions 2.4-2.6. It includes only the `arguments` and `templates` fields, adhering to the limitations of those versions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: workflow-template-submittable\nspec:\n  arguments:\n    parameters:\n      - name: message\n        value: hello world\n  templates:\n    - name: print-message\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: busybox\n        command: [echo]\n        args: [\"{{inputs.parameters.message}}\"]\n```\n\n----------------------------------------\n\nTITLE: Profiling Heap Allocation with pprof (Bash)\nDESCRIPTION: This command profiles heap allocation using pprof. It connects to the Argo controller's debug endpoint to gather profiling data. Ensure the controller is running locally with the `ARGO_PPROF=true` environment variable set.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\ngo tool pprof http://localhost:6060/debug/pprof/heap\n```\n\n----------------------------------------\n\nTITLE: Argo Global Options\nDESCRIPTION: Describes the global options inherited from parent commands that can be used with the 'argo archive' command. These options configure various aspects of the Argo CLI, such as server address, authentication, namespace, and logging level.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n      --argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Retrieving Event Source using Argo Workflows Python API\nDESCRIPTION: This snippet demonstrates how to retrieve a specific event source using the Argo Workflows EventSourceServiceApi in Python. It requires the argo-workflows package and configures API key authentication with a Bearer token. The snippet calls the `get_event_source` method with a namespace and event source name, printing the API response or any exception encountered.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import event_source_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.github_com_argoproj_argo_events_pkg_apis_events_v1alpha1_event_source import GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSource\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = event_source_service_api.EventSourceServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    name = \"name_example\" # str | \n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.get_event_source(namespace, name)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventSourceServiceApi->get_event_source: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Argo Server Environment Variable Example\nDESCRIPTION: Demonstrates how to set environment variables for the Argo Server using a Kubernetes Deployment. This example sets the `GRPC_MESSAGE_SIZE` environment variable to control the maximum gRPC message size.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/environment-variables.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: argo-server\nspec:\n  selector:\n    matchLabels:\n      app: argo-server\n  template:\n    metadata:\n      labels:\n        app: argo-server\n    spec:\n      containers:\n        - args:\n            - server\n          image: argoproj/argocli:latest\n          name: argo-server\n          env:\n            - name: GRPC_MESSAGE_SIZE\n              value: \"209715200\"\n          ports:\n          # ...\n```\n\n----------------------------------------\n\nTITLE: Getting Workflows Argo Workflows API (Bash)\nDESCRIPTION: This snippet retrieves a list of workflows for the specified namespace ('argo') using a GET request to the Argo Workflows API.  It requires `curl` to be installed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/rest-examples.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request GET \\\n  --url https://localhost:2746/api/v1/workflows/argo\n```\n\n----------------------------------------\n\nTITLE: Get Argo Cluster Workflow Template Details (Shell)\nDESCRIPTION: This command retrieves and displays details about a cluster workflow template using the Argo CLI.  It accepts the cluster workflow template name as an argument and supports various output formats.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_get.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nargo cluster-template get CLUSTER WORKFLOW_TEMPLATE... [flags]\n```\n\n----------------------------------------\n\nTITLE: Submitting multiple workflows from files with Argo\nDESCRIPTION: This command submits multiple workflows defined in YAML files to the Argo workflow engine.  It's a basic submission command.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_submit.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo submit my-wf.yaml\n```\n\n----------------------------------------\n\nTITLE: Using a Plugin Template in Argo\nDESCRIPTION: This YAML snippet defines a 'main' template of type 'plugin'. It references an executor plugin named 'slack' and sends a message indicating that the workflow has finished. The message includes the workflow name.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-concepts.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n  - name: main\n    plugin:\n      slack:\n        text: \"{{workflow.name}} finished!\"\n```\n\n----------------------------------------\n\nTITLE: Workflow-Level Synchronization using Mutex\nDESCRIPTION: This snippet demonstrates workflow-level synchronization using a mutex. Using a mutex is equivalent to setting a semaphore limit to \"1\", ensuring that only one instance of the workflow executes concurrently.  The mutex is used to prevent parallel execution of workflows using a shared synchronization key.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/synchronization.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"examples/synchronization-mutex-wf-level.yaml:3\"\n```\n\n----------------------------------------\n\nTITLE: Configure Azure Blob Storage with Managed Identity\nDESCRIPTION: This YAML snippet demonstrates configuring Azure Blob Storage as an artifact repository with Azure Managed Identities. It specifies the endpoint, container, and blob path. The `useSDKCreds: true` setting indicates that authentication should use the `DefaultAzureCredential`, negating the need for an `accountKeySecret`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\nartifacts:\n  - name: message\n    path: /tmp/message\n    azure:\n      endpoint: https://mystorageaccountname.blob.core.windows.net\n      container: my-container-name\n      blob: path/in/container\n      # If a managed identity has been assigned to the machines running the\n      # workflow (for example, https://docs.microsoft.com/en-us/azure/aks/use-managed-identity)\n      # then useSDKCreds should be set to true. The accountKeySecret is not required\n      # and will not be used in this case.\n      useSDKCreds: true  \n```\n\n----------------------------------------\n\nTITLE: Building and Installing Executor Plugin\nDESCRIPTION: These bash commands demonstrate how to build and install the Executor Plugin.  First build the plugin, then apply the generated ConfigMap to the `argo` namespace.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nargo executor-plugin build .\nkubectl -n argo apply -f hello-executor-plugin-configmap.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining ClusterWorkflowTemplate in YAML\nDESCRIPTION: This YAML snippet defines a ClusterWorkflowTemplate named `cluster-workflow-template-print-message`. It contains a template named `print-message` that takes a message as input and prints it using a busybox container.  Dependencies: Argo Workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cluster-workflow-templates.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: ClusterWorkflowTemplate\nmetadata:\n  name: cluster-workflow-template-print-message\nspec:\n  templates:\n  - name: print-message\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: busybox\n      command: [echo]\n      args: [\"{{inputs.parameters.message}}\"]\n```\n\n----------------------------------------\n\nTITLE: Argo Resume Command Syntax\nDESCRIPTION: Shows the basic syntax for the `argo resume` command.  It takes one or more workflow names as arguments and optionally accepts flags.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resume.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nargo resume WORKFLOW1 WORKFLOW2... [flags]\n```\n\n----------------------------------------\n\nTITLE: Update Workflow Template with Relaxed Validation - Argo\nDESCRIPTION: This example demonstrates how to update a Workflow Template with relaxed validation using the `--strict false` flag.  This disables strict validation checks during the update process.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template_update.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nargo template update FILE1 --strict false\n```\n\n----------------------------------------\n\nTITLE: Submit a Workflow for Testing - Bash\nDESCRIPTION: This bash command submits a workflow for testing using `kubectl`. It creates a workflow based on the `examples/hello-world.yaml` file.  Requires a Kubernetes cluster connection.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nkubectl create -f examples/hello-world.yaml\n```\n\n----------------------------------------\n\nTITLE: Watching Argo Event Sources with All Parameters (Python)\nDESCRIPTION: This code snippet demonstrates how to watch Argo Workflows event sources using the `watch_event_sources` method and includes all possible optional parameters for filtering and configuring the watch operation. It also includes error handling.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = event_source_service_api.EventSourceServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    list_options_label_selector = \"listOptions.labelSelector_example\" # str | A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. (optional)\n    list_options_field_selector = \"listOptions.fieldSelector_example\" # str | A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. (optional)\n    list_options_watch = True # bool | Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. (optional)\n    list_options_allow_watch_bookmarks = True # bool | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. (optional)\n    list_options_resource_version = \"listOptions.resourceVersion_example\" # str | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_resource_version_match = \"listOptions.resourceVersionMatch_example\" # str | resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_timeout_seconds = \"listOptions.timeoutSeconds_example\" # str | Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. (optional)\n    list_options_limit = \"listOptions.limit_example\" # str | limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. (optional)\n    list_options_continue = \"listOptions.continue_example\" # str | The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. (optional)\n    list_options_send_initial_events = True # bool | `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\\\"io.k8s.initial-events-end\\\": \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"` (for backward compatibility reasons) and to false otherwise. +optional (optional)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.watch_event_sources(namespace, list_options_label_selector=list_options_label_selector, list_options_field_selector=list_options_field_selector, list_options_watch=list_options_watch, list_options_allow_watch_bookmarks=list_options_allow_watch_bookmarks, list_options_resource_version=list_options_resource_version, list_options_resource_version_match=list_options_resource_version_match, list_options_timeout_seconds=list_options_timeout_seconds, list_options_limit=list_options_limit, list_options_continue=list_options_continue, list_options_send_initial_events=list_options_send_initial_events)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventSourceServiceApi->watch_event_sources: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Listing Event Sources with Argo EventSourceServiceApi in Java\nDESCRIPTION: This code snippet demonstrates how to list event sources within a given namespace using the Argo Workflows EventSourceServiceApi in Java. It initializes the API client, configures authentication using an API key, and then calls the `eventSourceServiceListEventSources` method.  The code also includes comprehensive error handling to catch and log any `ApiException` that may occur during the API call.  The API key needs to be set, and the base path needs to be configured.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/EventSourceServiceApi.md#_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.EventSourceServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    EventSourceServiceApi apiInstance = new EventSourceServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String listOptionsLabelSelector = \"listOptionsLabelSelector_example\"; // String | A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional.\n    String listOptionsFieldSelector = \"listOptionsFieldSelector_example\"; // String | A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional.\n    Boolean listOptionsWatch = true; // Boolean | Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional.\n    Boolean listOptionsAllowWatchBookmarks = true; // Boolean | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional.\n    String listOptionsResourceVersion = \"listOptionsResourceVersion_example\"; // String | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    String listOptionsResourceVersionMatch = \"listOptionsResourceVersionMatch_example\"; // String | resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    String listOptionsTimeoutSeconds = \"listOptionsTimeoutSeconds_example\"; // String | Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional.\n    String listOptionsLimit = \"listOptionsLimit_example\"; // String | limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n    String listOptionsContinue = \"listOptionsContinue_example\"; // String | The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n    Boolean listOptionsSendInitialEvents = true; // Boolean | `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\\\"io.k8s.initial-events-end\\\": \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"` (for backward compatibility reasons) and to false otherwise. +optional\n    try {\n      GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceList result = apiInstance.eventSourceServiceListEventSources(namespace, listOptionsLabelSelector, listOptionsFieldSelector, listOptionsWatch, listOptionsAllowWatchBookmarks, listOptionsResourceVersion, listOptionsResourceVersionMatch, listOptionsTimeoutSeconds, listOptionsLimit, listOptionsContinue, listOptionsSendInitialEvents);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling EventSourceServiceApi#eventSourceServiceListEventSources\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Filter Nodes by Input Parameter and Phase\nDESCRIPTION: Shows how to combine multiple node field selectors to filter nodes based on multiple criteria, specifically an input parameter value and the node's phase. This example filters for nodes where the input parameter 'foo1' is 'bar1' and the phase is not 'Running'.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/node-field-selector.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n--node-field-selector=foo1=bar1,phase!=Running\n```\n\n----------------------------------------\n\nTITLE: JavaScript Script to Generate Random Integer\nDESCRIPTION: This JavaScript script generates a random integer between 0 and 99 using `Math.random` and `Math.floor`.  It depends on the `node:9.1-alpine` image. The generated number is printed to the console using `console.log`, which is captured as the `result` output parameter.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/scripts-and-results.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nvar rand = Math.floor(Math.random() * 100);\nconsole.log(rand);\n```\n\n----------------------------------------\n\nTITLE: Workflow Suspend Specification YAML\nDESCRIPTION: This YAML defines an Argo Workflow that includes steps to suspend execution.  The `suspend` keyword in the template definition halts the workflow at that step until resumed. The `duration` field can be used to automatically resume the workflow after a specified time.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/suspending.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: suspend-template-\nspec:\n  entrypoint: suspend\n  templates:\n  - name: suspend\n    steps:\n    - - name: build\n        template: hello-world\n    - - name: approve\n        template: approve\n    - - name: delay\n        template: delay\n    - - name: release\n        template: hello-world\n\n  - name: approve\n    suspend: {}\n\n  - name: delay\n    suspend:\n      duration: \"20\"    # Must be a string. Default unit is seconds. Could also be a Duration, e.g.: \"2m\", \"6h\"\n\n  - name: hello-world\n    container:\n      image: busybox\n      command: [echo]\n      args: [\"hello world\"]\n```\n\n----------------------------------------\n\nTITLE: Resubmitting Workflows by Field Selector with Argo\nDESCRIPTION: Resubmits Argo Workflows that match the specified field selector.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resubmit.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nargo resubmit --field-selector metadata.namespace=argo\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Artifact with S3 Access Grants Credentials\nDESCRIPTION: This YAML configuration defines an artifact to be stored in S3, utilizing S3 Access Grants for access control. It references Kubernetes secrets (named `my-s3-credentials`) for the access key, secret key, and session token.  These credentials provide temporary, reduced scope access and must be periodically refreshed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nartifacts:\n  - name: my-output-artifact\n    path: /my-output-artifact\n    s3:\n      endpoint: s3.amazonaws.com\n      bucket: my-s3-bucket\n      key: path/in/bucket/my-output-artifact.tgz\n      # The following fields are secret selectors.\n      # They reference the k8s secret named 'my-s3-credentials'.\n      # This secret is expected to have the keys 'accessKey', 'secretKey', and 'sessionToken',\n      # containing the base64 encoded credentials to the bucket.\n      accessKeySecret:\n        name: my-s3-credentials\n        key: accessKey\n      secretKeySecret:\n        name: my-s3-credentials\n        key: secretKey\n      sessionTokenSecret:\n        name: my-s3-credentials\n        key: sessionToken\n```\n\n----------------------------------------\n\nTITLE: Using Argo CLI with Verified Certificate - Bash\nDESCRIPTION: This snippet demonstrates how to use the Argo CLI with TLS encryption when the Argo Server is using a verified certificate. It involves using the `--secure` flag with the Argo CLI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/tls.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nargo --secure list\n```\n\n----------------------------------------\n\nTITLE: Declare webHDFS output artifact in Argo Workflow\nDESCRIPTION: This YAML snippet demonstrates how to define an output artifact in an Argo Workflow to write a file to webHDFS using the HTTP artifact type.  The `url` includes the `CREATE` operation and the `overwrite` parameter, allowing the workflow to overwrite existing files. The artifact will be stored at the specified `path` in webHDFS.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/webhdfs.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  # ...\n  outputs:\n    artifacts:\n    - name: my-art\n      path: /my-artifact\n      http:\n        url: \"https://mywebhdfsprovider.com/webhdfs/v1/outputs/newfile.txt?op=CREATE&overwrite=true\"\n```\n\n----------------------------------------\n\nTITLE: Argo Template Get Command Usage\nDESCRIPTION: Displays details about a workflow template using the `argo template get` command.  It takes the workflow template name as an argument and supports flags for help and output formatting.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template_get.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nargo template get WORKFLOW_TEMPLATE... [flags]\n```\n\n----------------------------------------\n\nTITLE: Stopping Workflow using Java\nDESCRIPTION: This code snippet demonstrates how to stop a workflow using the Argo Workflow Service API in Java. It requires the `io.argoproj.workflow` dependency and configures API key authorization using a Bearer token. The snippet defines a namespace, a workflow name and a body request and invokes the `workflowServiceStopWorkflow` method.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowServiceApi.md#_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.WorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    WorkflowServiceApi apiInstance = new WorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    IoArgoprojWorkflowV1alpha1WorkflowStopRequest body = new IoArgoprojWorkflowV1alpha1WorkflowStopRequest(); // IoArgoprojWorkflowV1alpha1WorkflowStopRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1Workflow result = apiInstance.workflowServiceStopWorkflow(namespace, name, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling WorkflowServiceApi#workflowServiceStopWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Configure Argo Workflows SSO with Helm\nDESCRIPTION: This YAML configures the Argo Workflows Helm chart to enable and configure SSO.  The `authModes` includes `sso`. The `sso` section configures the SSO parameters such as `issuer`, `sessionExpiry`, `clientId`, `clientSecret`, and `redirectUrl` to integrate with Argo CD's Dex instance.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso-argocd.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  # Chart version 0.39.0 and after\n  authModes:\n    - sso\n  sso:\n    enabled: true\n    issuer: https://argo-cd.mydomain.com/api/dex\n    # sessionExpiry defines how long your login is valid for in hours. (optional, default: 10h)\n    sessionExpiry: 240h\n    clientId:\n      name: argo-workflows-sso\n      key: client-id\n    clientSecret:\n      name: argo-workflows-sso\n      key: client-secret\n    redirectUrl: https://argo-workflows.mydomain.com/oauth2/callback\n```\n\n----------------------------------------\n\nTITLE: Retrieve Azure Blob Service Endpoint with Azure CLI\nDESCRIPTION: This snippet retrieves the blob service endpoint for a specified Azure storage account using the Azure CLI. The `az storage account show` command queries the account and extracts the `primaryEndpoints.blob` property, outputting it in TSV format for easy parsing.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\naz storage account show -n mystorageaccountname --query 'primaryEndpoints.blob' -otsv\n# https://mystorageaccountname.blob.core.windows.net\n```\n\n----------------------------------------\n\nTITLE: Adding Static Annotations to Workflow Template (YAML)\nDESCRIPTION: This snippet demonstrates how to add static annotations to a workflow template in Argo Workflows. The `annotations` field is included in the template definition to specify the key-value pairs for the annotations. In this example, the `workflows.argoproj.io/display-name` annotation is used to customize the node name in the UI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/annotations.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n    name: example-workflow-template\nspec:\n    entrypoint: whalesay\n    templates:\n    - name: whalesay\n      annotations:\n        workflows.argoproj.io/display-name: \"my-custom-display-name\"\n      container:\n          image: docker/whalesay\n          command: [cowsay]\n          args: [\"hello world\"]\n```\n\n----------------------------------------\n\nTITLE: Print Workflow Logs\nDESCRIPTION: This command prints the logs of a specified Argo workflow.  It retrieves logs from all relevant pods and containers associated with the workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_logs.md#_snippet_1\n\nLANGUAGE: CLI\nCODE:\n```\nargo logs my-wf\n```\n\n----------------------------------------\n\nTITLE: Executor Plugin Help Option\nDESCRIPTION: Describes the help option for the `argo executor-plugin` command. Using this flag displays help information about the command and its subcommands.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_executor-plugin.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n  -h, --help   help for executor-plugin\n```\n\n----------------------------------------\n\nTITLE: Setting Global Workflow Parallelism - YAML\nDESCRIPTION: This YAML snippet configures the Argo Workflow Controller to limit the total number of parallel workflow executions to 10. This setting is applied globally to all workflows managed by the controller. It's configured in the `workflow-controller-configmap.yaml` file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/parallelism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  parallelism: \"10\"\n```\n\n----------------------------------------\n\nTITLE: Define Artifact Input/Output S3 template\nDESCRIPTION: This YAML snippet demonstrates how to access artifacts from non-default artifact repositories within an Argo Workflow template. It defines input and output artifacts that use S3 buckets with specified endpoints, keys, and access credentials. It also uses a container that copies data from the input artifact path to the output artifact path.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_32\n\nLANGUAGE: yaml\nCODE:\n```\ntemplates:\n  - name: artifact-example\n    inputs:\n      artifacts:\n      - name: my-input-artifact\n        path: /my-input-artifact\n        s3:\n          endpoint: s3.amazonaws.com\n          bucket: my-aws-bucket-name\n          key: path/in/bucket/my-input-artifact.tgz\n          accessKeySecret:\n            name: my-aws-s3-credentials\n            key: accessKey\n          secretKeySecret:\n            name: my-aws-s3-credentials\n            key: secretKey\n    outputs:\n      artifacts:\n      - name: my-output-artifact\n        path: /my-output-artifact\n        s3:\n          endpoint: storage.googleapis.com\n          bucket: my-gcs-bucket-name\n          # NOTE that, by default, all output artifacts are automatically tarred and\n          # gzipped before saving. So as a best practice, .tgz or .tar.gz\n          # should be incorporated into the key name so the resulting file\n          # has an accurate file extension.\n          key: path/in/bucket/my-output-artifact.tgz\n          accessKeySecret:\n            name: my-gcs-s3-credentials\n            key: accessKey\n          secretKeySecret:\n            name: my-gcs-s3-credentials\n            key: secretKey\n          region: my-GCS-storage-bucket-region\n    container:\n      image: debian:latest\n      command: [sh, -c]\n      args: [\"cp -r /my-input-artifact /my-output-artifact\"]\n```\n\n----------------------------------------\n\nTITLE: Define Real-Time Gauge Metric in Argo\nDESCRIPTION: This YAML snippet shows how to define a real-time gauge metric in Argo. By setting `realtime: true` and using a valid real-time variable like `{{duration}}`, the metric is emitted continuously during the step's execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n  gauge:\n    realtime: true\n    value: \"{{duration}}\"\n```\n\n----------------------------------------\n\nTITLE: Update Workflow Template - Argo\nDESCRIPTION: This command updates a workflow template using a specified file. It requires the Argo CLI to be configured and connected to an Argo Server. The command takes one or more file paths as input, each representing a workflow template definition.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template_update.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nargo template update FILE1 FILE2... [flags]\n```\n\n----------------------------------------\n\nTITLE: Installing Full CRDs with kubectl\nDESCRIPTION: This command installs the full CRDs for Argo Workflows using server-side apply. It retrieves the CRDs from the specified GitHub repository and applies them to the Kubernetes cluster. This is a necessary step when using server-side apply, especially to overcome Kubernetes size limitations with client-side apply.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/installation.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply --server-side --kustomize https://github.com/argoproj/argo-workflows/manifests/base/crds/full?ref=v3.7.0\n```\n\n----------------------------------------\n\nTITLE: Example Workflow Annotations in YAML\nDESCRIPTION: These examples demonstrate various ways to use `workflows.argoproj.io/title` and `workflows.argoproj.io/description` annotations in Argo Workflows, including markdown titles and descriptions, URLs, multi-line descriptions, and the default title behavior. These examples aim to provide guidance on customizing workflow display within the Argo UI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/title-and-description.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# 1. markdown title, markdown description using [`remark-gfm`](https://github.com/remarkjs/remark-gfm) to convert URLs into anchor links\nmetadata:\n  annotations:\n    workflows.argoproj.io/title: '**feat: Allow markdown in workflow title and description. Fixes #10126**'\n    workflows.argoproj.io/description: https://github.com/argoproj/argo-workflows/pull/10553\n\n# 2. no title (defaults to `metadata.name`), no description\nmetadata:\n  name: wonderful-poochenheimer\n\n# 3. markdown title, no description\nmetadata:\n  annotations:\n    workflows.argoproj.io/title: '**Build and test and test**'\n\n# 4. no title (defaults to `metadata.name`), markdown description\nmetadata:\n  name: delightful-python\n  annotations:\n    workflows.argoproj.io/description: '`SuperDuperProject` PR #6529: Implement frobbing (aff39ee)'\n\n# 5. markdown title, markdown description with a markdown link\nmetadata:\n  annotations:\n    workflows.argoproj.io/title: '**Build and test**'\n    workflows.argoproj.io/description: '`SuperDuperProject` PR [#6529](https://github.com): Implement frobbing (aff39ee)'\n```\n\n----------------------------------------\n\nTITLE: Suspending Workflow using Java\nDESCRIPTION: This code snippet demonstrates how to suspend a workflow using the Argo Workflow Service API in Java. It requires the `io.argoproj.workflow` dependency and configures API key authorization using a Bearer token. The snippet defines a namespace, a workflow name and a body request and invokes the `workflowServiceSuspendWorkflow` method.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowServiceApi.md#_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.WorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    WorkflowServiceApi apiInstance = new WorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    IoArgoprojWorkflowV1alpha1WorkflowSuspendRequest body = new IoArgoprojWorkflowV1alpha1WorkflowSuspendRequest(); // IoArgoprojWorkflowV1alpha1WorkflowSuspendRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1Workflow result = apiInstance.workflowServiceSuspendWorkflow(namespace, name, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling WorkflowServiceApi#workflowServiceSuspendWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Start Argo with SSO Profile - Bash\nDESCRIPTION: This bash command starts Argo with the SSO profile for testing single sign-on integration.  Requires UI to be enabled.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_14\n\nLANGUAGE: Bash\nCODE:\n```\nmake start UI=true PROFILE=sso\n```\n\n----------------------------------------\n\nTITLE: Event Source Logs Retrieval in Argo Workflows (Python)\nDESCRIPTION: This code snippet demonstrates how to use the Argo Workflows Python client to retrieve logs from event sources. It initializes the API client with authentication, configures parameters such as namespace, event source name, and various log options, and then calls the `event_sources_logs` method to fetch the logs. The response is printed using `pprint`. Note that API Key authentication is configured with a Bearer token. The snippet uses optional parameters to further refine the log retrieval process.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import event_source_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.stream_result_of_eventsource_log_entry import StreamResultOfEventsourceLogEntry\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = event_source_service_api.EventSourceServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    name = \"name_example\" # str | optional - only return entries for this event source. (optional)\n    event_source_type = \"eventSourceType_example\" # str | optional - only return entries for this event source type (e.g. `webhook`). (optional)\n    event_name = \"eventName_example\" # str | optional - only return entries for this event name (e.g. `example`). (optional)\n    grep = \"grep_example\" # str | optional - only return entries where `msg` matches this regular expression. (optional)\n    pod_log_options_container = \"podLogOptions.container_example\" # str | The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional. (optional)\n    pod_log_options_follow = True # bool | Follow the log stream of the pod. Defaults to false. +optional. (optional)\n    pod_log_options_previous = True # bool | Return previous terminated container logs. Defaults to false. +optional. (optional)\n    pod_log_options_since_seconds = \"podLogOptions.sinceSeconds_example\" # str | A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional. (optional)\n    pod_log_options_since_time_seconds = \"podLogOptions.sinceTime.seconds_example\" # str | Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive. (optional)\n    pod_log_options_since_time_nanos = 1 # int | Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context. (optional)\n    pod_log_options_timestamps = True # bool | If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional. (optional)\n    pod_log_options_tail_lines = \"podLogOptions.tailLines_example\" # str | If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime. Note that when \\\"TailLines\\\" is specified, \\\"Stream\\\" can only be set to nil or \\\"All\\\". +optional. (optional)\n    pod_log_options_limit_bytes = \"podLogOptions.limitBytes_example\" # str | If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional. (optional)\n    pod_log_options_insecure_skip_tls_verify_backend = True # bool | insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to.  This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet.  If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional. (optional)\n    pod_log_options_stream = \"podLogOptions.stream_example\" # str | Specify which container log stream to return to the client. Acceptable values are \\\"All\\\", \\\"Stdout\\\" and \\\"Stderr\\\". If not specified, \\\"All\\\" is used, and both stdout and stderr are returned interleaved. Note that when \\\"TailLines\\\" is specified, \\\"Stream\\\" can only be set to nil or \\\"All\\\". +featureGate=PodLogsQuerySplitStreams +optional. (optional)\n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.event_sources_logs(namespace)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventSourceServiceApi->event_sources_logs: %s\\n\" % e)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.event_sources_logs(namespace, name=name, event_source_type=event_source_type, event_name=event_name, grep=grep, pod_log_options_container=pod_log_options_container, pod_log_options_follow=pod_log_options_follow, pod_log_options_previous=pod_log_options_previous, pod_log_options_since_seconds=pod_log_options_since_seconds, pod_log_options_since_time_seconds=pod_log_options_since_time_seconds, pod_log_options_since_time_nanos=pod_log_options_since_time_nanos, pod_log_options_timestamps=pod_log_options_timestamps, pod_log_options_tail_lines=pod_log_options_tail_lines, pod_log_options_limit_bytes=pod_log_options_limit_bytes, pod_log_options_insecure_skip_tls_verify_backend=pod_log_options_insecure_skip_tls_verify_backend, pod_log_options_stream=pod_log_options_stream)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventSourceServiceApi->event_sources_logs: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Template-Level Volume Definition in Argo Workflow (YAML)\nDESCRIPTION: This YAML configuration demonstrates how to declare existing volumes at the template level in an Argo Workflow. It also covers how to generate volumes using a resource step. The 'generate-volume' template creates a PersistentVolumeClaim, and its name is then passed as an input parameter to the 'hello-world-to-file' and 'print-message-from-file' templates, enabling them to use the generated volume.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/volumes.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: template-level-volume-\nspec:\n  entrypoint: generate-and-use-volume\n  templates:\n  - name: generate-and-use-volume\n    steps:\n    - - name: generate-volume\n        template: generate-volume\n        arguments:\n          parameters:\n            - name: pvc-size\n              # In a real-world example, this could be generated by a previous workflow step.\n              value: '1Gi'\n    - - name: generate\n        template: hello-world-to-file\n        arguments:\n          parameters:\n            - name: pvc-name\n              value: '{{steps.generate-volume.outputs.parameters.pvc-name}}'\n    - - name: print\n        template: print-message-from-file\n        arguments:\n          parameters:\n            - name: pvc-name\n              value: '{{steps.generate-volume.outputs.parameters.pvc-name}}'\n\n  - name: generate-volume\n    inputs:\n      parameters:\n        - name: pvc-size\n    resource:\n      action: create\n      setOwnerReference: true\n      manifest: |\n        apiVersion: v1\n        kind: PersistentVolumeClaim\n        metadata:\n          generateName: pvc-example-\n        spec:\n          accessModes: ['ReadWriteOnce', 'ReadOnlyMany']\n          resources:\n            requests:\n              storage: '{{inputs.parameters.pvc-size}}'\n    outputs:\n      parameters:\n        - name: pvc-name\n          valueFrom:\n            jsonPath: '{.metadata.name}'\n\n  - name: hello-world-to-file\n    inputs:\n      parameters:\n        - name: pvc-name\n    volumes:\n      - name: workdir\n        persistentVolumeClaim:\n          claimName: '{{inputs.parameters.pvc-name}}'\n    container:\n      image: busybox\n      command: [sh, -c]\n      args: [\"echo generating message in volume; echo hello world | tee /mnt/vol/hello_world.txt\"]\n      volumeMounts:\n      - name: workdir\n        mountPath: /mnt/vol\n\n  - name: print-message-from-file\n    inputs:\n        parameters:\n          - name: pvc-name\n    volumes:\n      - name: workdir\n        persistentVolumeClaim:\n          claimName: '{{inputs.parameters.pvc-name}}'\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"echo getting message from volume; find /mnt/vol; cat /mnt/vol/hello_world.txt\"]\n      volumeMounts:\n      - name: workdir\n        mountPath: /mnt/vol\n\n```\n\n----------------------------------------\n\nTITLE: Resubmitting the Latest Workflow with Argo\nDESCRIPTION: Resubmits the latest Argo Workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resubmit.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\nargo resubmit @latest\n```\n\n----------------------------------------\n\nTITLE: Get Workflow Details using Argo CLI\nDESCRIPTION: This example demonstrates how to use the `argo get` command to retrieve information about a specific workflow named `my-wf`.  The command will output the details of the workflow to the console.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_get.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo get my-wf\n```\n\n----------------------------------------\n\nTITLE: Lint Specific Kinds from Stdin - Argo\nDESCRIPTION: This command filters manifests from stdin based on specified Kinds (workflows, cronworkflows) and validates them with the `argo lint` tool.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_lint.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncat manifests.yaml | argo lint --kinds=workflows,cronworkflows -\n```\n\n----------------------------------------\n\nTITLE: Port Forward to Access Argo UI\nDESCRIPTION: This command forwards a local port (2746) to the Argo Server service within the 'argo' namespace, allowing access to the UI through localhost. It requires kubectl and access to the Kubernetes cluster.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n argo port-forward svc/argo-server 2746:2746\n```\n\n----------------------------------------\n\nTITLE: Resubmitting Archived Workflow with Argo Workflows API (Python)\nDESCRIPTION: This snippet demonstrates how to resubmit an archived workflow using the Argo Workflows API. It requires the `argo_workflows` package and configures API key authentication.  It constructs a `IoArgoprojWorkflowV1alpha1ResubmitArchivedWorkflowRequest` object with parameters like `memoized`, `name`, `namespace`, `parameters`, and `uid`. Then, it calls the `resubmit_archived_workflow` method of the `ArchivedWorkflowServiceApi` with the workflow's UID and the request object, printing the API response or any encountered exception.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArchivedWorkflowServiceApi.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import archived_workflow_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_workflow import IoArgoprojWorkflowV1alpha1Workflow\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_resubmit_archived_workflow_request import IoArgoprojWorkflowV1alpha1ResubmitArchivedWorkflowRequest\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = archived_workflow_service_api.ArchivedWorkflowServiceApi(api_client)\n    uid = \"uid_example\" # str | \n    body = IoArgoprojWorkflowV1alpha1ResubmitArchivedWorkflowRequest(\n        memoized=True,\n        name=\"name_example\",\n        namespace=\"namespace_example\",\n        parameters=[\n            \"parameters_example\",\n        ],\n        uid=\"uid_example\",\n    ) # IoArgoprojWorkflowV1alpha1ResubmitArchivedWorkflowRequest | \n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.resubmit_archived_workflow(uid, body)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArchivedWorkflowServiceApi->resubmit_archived_workflow: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Argo Terminate Workflows by Field\nDESCRIPTION: This example shows how to terminate multiple workflows using a field selector. Workflows matching the field `metadata.namespace=argo` will be terminated. The `--field-selector` flag specifies the field selector.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_terminate.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nargo terminate --field-selector metadata.namespace=argo\n```\n\n----------------------------------------\n\nTITLE: Terminating Workflow using Java\nDESCRIPTION: This code snippet demonstrates how to terminate a workflow using the Argo Workflow Service API in Java. It requires the `io.argoproj.workflow` dependency and configures API key authorization using a Bearer token. The snippet defines a namespace, a workflow name and a body request and invokes the `workflowServiceTerminateWorkflow` method.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowServiceApi.md#_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.WorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    WorkflowServiceApi apiInstance = new WorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    IoArgoprojWorkflowV1alpha1WorkflowTerminateRequest body = new IoArgoprojWorkflowV1alpha1WorkflowTerminateRequest(); // IoArgoprojWorkflowV1alpha1WorkflowTerminateRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1Workflow result = apiInstance.workflowServiceTerminateWorkflow(namespace, name, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling WorkflowServiceApi#workflowServiceTerminateWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Resume Workflow via Argo API using curl\nDESCRIPTION: Resumes a suspended Argo workflow via the Argo API. The `curl` command sends a PUT request to the `/resume` endpoint, including the namespace, workflow name, and a node field selector based on the UUID of the job.  It requires an Argo access token ($ARGO_TOKEN) for authentication.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/async-pattern.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ncurl --request PUT \\\n  --url https://localhost:2746/api/v1/workflows/<NAMESPACE>/<WORKFLOWNAME>/resume \\\n  --header 'content-type: application/json' \\\n  --header \"Authorization: $ARGO_TOKEN\" \\\n  --data '{\n      \"namespace\": \"<NAMESPACE>\",\n      \"name\": \"<WORKFLOWNAME>\",\n      \"nodeFieldSelector\": \"inputs.parameters.uuid.value=<UUID>\"\n    }'\n```\n\n----------------------------------------\n\nTITLE: Scheduling Workflow on Windows Node - Argo YAML\nDESCRIPTION: This YAML defines an Argo Workflow that runs a Windows container. It uses `nodeSelector` to target Windows nodes and executes a simple command using `cmd` in a Nano Server image.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/windows.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-windows-\nspec:\n  entrypoint: hello-win\n  templates:\n    - name: hello-win\n      nodeSelector:\n        kubernetes.io/os: windows    # specify the OS your step should run on\n      container:\n        image: mcr.microsoft.com/windows/nanoserver:1809\n        command: [\"cmd\", \"/c\"]\n        args: [\"echo\", \"Hello from Windows Container!\"]\n```\n\n----------------------------------------\n\nTITLE: Profiling Goroutine Blocking with pprof (Bash)\nDESCRIPTION: This command profiles goroutine blocking using pprof. It connects to the Argo controller's debug endpoint to gather profiling data. Ensure the controller is running locally with the `ARGO_PPROF=true` environment variable set.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\ngo tool pprof http://localhost:6060/debug/pprof/block\n```\n\n----------------------------------------\n\nTITLE: Print Logs of a Specific Container\nDESCRIPTION: This command prints logs from a specific container within a pod of a workflow. It targets a single container for focused log retrieval.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_logs.md#_snippet_4\n\nLANGUAGE: CLI\nCODE:\n```\nargo logs my-wf my-pod -c my-container\n```\n\n----------------------------------------\n\nTITLE: Annotating ClusterWorkflowTemplate in YAML\nDESCRIPTION: This snippet demonstrates how to add `workflows.argoproj.io/title` and `workflows.argoproj.io/description` annotations with embedded markdown to a ClusterWorkflowTemplate. The annotations are used to display titles and descriptions of ClusterWorkflowTemplates in the Argo Workflows UI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/title-and-description.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: ClusterWorkflowTemplate\nmetadata:\n  name: my-cluster-workflow-template\n  annotations:\n    workflows.argoproj.io/title: '**Test Title**'\n    workflows.argoproj.io/description: `This is a simple hello world example.`\n```\n\n----------------------------------------\n\nTITLE: Running the Recursive Workflow using Argo CLI\nDESCRIPTION: These are example outputs from running the `coinflip-recursive` workflow using the Argo CLI (`argo get`). The output shows the steps that were executed, including `flip-coin`, `heads`, and `tails`, as well as the pod names associated with each step.  The '○' indicates a skipped step, while '✔' denotes a successful execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/recursion.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo get coinflip-recursive-tzcb5\n\nSTEP                         PODNAME                              MESSAGE\n ✔ coinflip-recursive-vhph5\n ├───✔ flip-coin             coinflip-recursive-vhph5-2123890397\n └─┬─✔ heads                 coinflip-recursive-vhph5-128690560\n   └─○ tails\n\nSTEP                          PODNAME                              MESSAGE\n ✔ coinflip-recursive-tzcb5\n ├───✔ flip-coin              coinflip-recursive-tzcb5-322836820\n └─┬─○ heads\n   └─✔ tails\n     ├───✔ flip-coin          coinflip-recursive-tzcb5-1863890320\n     └─┬─○ heads\n       └─✔ tails\n         ├───✔ flip-coin      coinflip-recursive-tzcb5-1768147140\n         └─┬─○ heads\n           └─✔ tails\n             ├───✔ flip-coin  coinflip-recursive-tzcb5-4080411136\n             └─┬─✔ heads      coinflip-recursive-tzcb5-4080323273\n               └─○ tails\n```\n\n----------------------------------------\n\nTITLE: Create Kubernetes Role Binding\nDESCRIPTION: Creates a Kubernetes role binding named 'jenkins' that binds the 'jenkins' role to the 'jenkins' service account in the 'argo' namespace. This allows the service account to perform the actions defined in the role.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/access-token.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create rolebinding jenkins --role=jenkins --serviceaccount=argo:jenkins\n```\n\n----------------------------------------\n\nTITLE: Invalid ConfigMap - Executor Resources\nDESCRIPTION: This YAML snippet demonstrates an invalid ConfigMap configuration where `executorResources` is used directly. It should be nested within the `executor` section.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  ...\n  executorResources:\n    requests:\n      cpu: 0.1\n      memory: 64Mi\n    limits:\n      cpu: 0.5\n      memory: 512Mi\n  ...\n```\n\n----------------------------------------\n\nTITLE: Payload expression example\nDESCRIPTION: This is an example of accessing a nested field within the payload of an event using the expression language. This example checks if the `clone_url` property inside the `repository` property matches a given string.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/events.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\npayload.repository.clone_url == \"http://github.com/argoproj/argo\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Workflow Argo Workflows API (Bash)\nDESCRIPTION: This snippet deletes a specific workflow by its name ('abc-dthgt') within the specified namespace ('argo') using a DELETE request to the Argo Workflows API. It requires `curl` to be installed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/rest-examples.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request DELETE \\\n  --url https://localhost:2746/api/v1/workflows/argo/abc-dthgt\n```\n\n----------------------------------------\n\nTITLE: Resubmitting a Workflow with Argo\nDESCRIPTION: Resubmits a specified Argo Workflow using its name.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resubmit.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nargo resubmit my-wf\n```\n\n----------------------------------------\n\nTITLE: Disable Metric Attribute\nDESCRIPTION: This YAML snippet shows how to disable specific attributes (labels) from being emitted for a metric using modifiers in the Argo Workflow Controller ConfigMap. Disabling attributes can help reduce the cardinality of metrics.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ndisabledAttributes:\n    - namespace\n```\n\n----------------------------------------\n\nTITLE: Retry Workflows by Label Selector - Argo\nDESCRIPTION: This example illustrates how to retry workflows using a label selector with the `argo archive retry` command. It targets workflows that match the specified label for re-execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_retry.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo archive retry -l workflows.argoproj.io/test=true\n```\n\n----------------------------------------\n\nTITLE: Argo Completion for Bash\nDESCRIPTION: Generates bash completion code for the Argo CLI. The output can be sourced directly or written to a file and sourced in `.bash_profile` to enable completions in the shell.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_completion.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nsource <(argo completion bash)\n```\n\n----------------------------------------\n\nTITLE: Running Stress Test Workflow\nDESCRIPTION: This snippet executes the stress test tool to submit a large number of workflows. This tool is designed to overload the system and provide data about the performance under heavy load.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/stress-testing.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngo run ./test/stress/tool -n 10000\n```\n\n----------------------------------------\n\nTITLE: Using Existing Volume in Argo Workflow (YAML)\nDESCRIPTION: This YAML demonstrates how to use an already existing Kubernetes PersistentVolumeClaim (PVC) in an Argo Workflow. It first defines the PVC 'my-existing-volume' and then references it in the workflow's `volumes` section. The workflow then uses this volume to share data between the 'hello-world-to-file' and 'print-message-from-file' templates.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/volumes.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# Define Kubernetes PVC\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: my-existing-volume\nspec:\n  accessModes: [ \"ReadWriteOnce\" ]\n  resources:\n    requests:\n      storage: 1Gi\n\n---\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: volumes-existing-\nspec:\n  entrypoint: volumes-existing-example\n  volumes:\n  # Pass my-existing-volume as an argument to the volumes-existing-example template\n  # Same syntax as k8s Pod spec\n  - name: workdir\n    persistentVolumeClaim:\n      claimName: my-existing-volume\n\n  templates:\n  - name: volumes-existing-example\n    steps:\n    - - name: generate\n        template: hello-world-to-file\n    - - name: print\n        template: print-message-from-file\n\n  - name: hello-world-to-file\n    container:\n      image: busybox\n      command: [sh, -c]\n      args: [\"echo generating message in volume; echo hello world | tee /mnt/vol/hello_world.txt\"]\n      volumeMounts:\n      - name: workdir\n        mountPath: /mnt/vol\n\n  - name: print-message-from-file\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"echo getting message from volume; find /mnt/vol; cat /mnt/vol/hello_world.txt\"]\n      volumeMounts:\n      - name: workdir\n        mountPath: /mnt/vol\n\n```\n\n----------------------------------------\n\nTITLE: Retrieving User Info using get_user_info in Python\nDESCRIPTION: This code shows how to use the `get_user_info` method of the `InfoServiceApi` to retrieve user information from Argo Workflows. It requires the `argo_workflows` package and uses API key authentication. The endpoint has no required parameters.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/InfoServiceApi.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import info_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_get_user_info_response import IoArgoprojWorkflowV1alpha1GetUserInfoResponse\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = info_service_api.InfoServiceApi(api_client)\n\n    # example, this endpoint has no required or optional parameters\n    try:\n        api_response = api_instance.get_user_info()\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling InfoServiceApi->get_user_info: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Argo Version Command Options\nDESCRIPTION: These options configure the output of the `argo version` command. The `-h` or `--help` flag displays help information, while `--short` prints only the version number.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_version.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n  -h, --help    help for version\n      --short   print just the version number\n```\n\n----------------------------------------\n\nTITLE: Copy Workflow Artifacts to Local Directory (bash)\nDESCRIPTION: This example demonstrates how to copy all artifacts from a workflow to a specified output directory.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cp.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo cp my-wf output-directory\n```\n\n----------------------------------------\n\nTITLE: Labeling Argo Workflow with Creator Information in YAML\nDESCRIPTION: This YAML snippet demonstrates how to add creator labels to an Argo Workflow. It includes labels for the creator's username, email address (with '@' replaced by '.at.'), and preferred username.  The labels allow tracking the user who created the workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-creator.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: my-wf\n  labels:\n    workflows.argoproj.io/creator: admin\n    # labels must be DNS formatted, so the \"@\" is replaces by '.at.'\n    workflows.argoproj.io/creator-email: admin.at.your.org\n    workflows.argoproj.io/creator-preferred-username: admin-preferred-username\n```\n\n----------------------------------------\n\nTITLE: Suspend the latest Argo workflow\nDESCRIPTION: This example demonstrates how to suspend the most recently created workflow using the `@latest` specifier with the `argo suspend` command.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_suspend.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nargo suspend @latest\n```\n\n----------------------------------------\n\nTITLE: Argo CLI Global Options\nDESCRIPTION: Lists the options inherited from parent commands that configure the Argo CLI's behavior, including server address, authentication, namespace, and logging level. These options allow customization of the CLI's interaction with the Argo Server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template_get.md#_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n      --argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Deleting a Specific Argo Workflow\nDESCRIPTION: This command demonstrates how to delete a specific Argo workflow using its name. The `argo delete` command followed by the workflow name (`my-wf`) initiates the deletion process.  The Argo CLI tool is a prerequisite, configured to communicate with the Argo Workflows server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_delete.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo delete my-wf\n```\n\n----------------------------------------\n\nTITLE: Listing All Workflows with Argo CLI\nDESCRIPTION: Lists all workflows in the current namespace using the `argo list` command. No additional flags are used, showing basic workflow retrieval.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_list.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo list\n```\n\n----------------------------------------\n\nTITLE: Listing Event Sources using Argo Workflows Python API\nDESCRIPTION: This snippet demonstrates how to list event sources within a specific namespace using the Argo Workflows EventSourceServiceApi in Python. It requires the argo-workflows package and configures API key authentication via a Bearer token. The `list_event_sources` method is called with the namespace, and the API response or any exceptions are printed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import event_source_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.github_com_argoproj_argo_events_pkg_apis_events_v1alpha1_event_source_list import GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceList\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n```\n\n----------------------------------------\n\nTITLE: Resubmitting Workflows by Label Selector with Argo\nDESCRIPTION: Resubmits Argo Workflows that match the specified label selector.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resubmit.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nargo resubmit -l workflows.argoproj.io/test=true\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenWhisk Trigger in Argo Events\nDESCRIPTION: This snippet demonstrates the configuration of an OpenWhisk trigger within Argo Events. It showcases the usage of action name, authentication token using SecretKeySelector, host, namespace, parameters, payload, and version. This configuration is used to define how Argo Events interacts with an OpenWhisk instance to trigger actions based on events.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nopen_whisk=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1OpenWhiskTrigger(\n                                action_name=\"action_name_example\",\n                                auth_token=SecretKeySelector(\n                                    key=\"key_example\",\n                                    name=\"name_example\",\n                                    optional=True,\n                                ),\n                                host=\"host_example\",\n                                namespace=\"namespace_example\",\n                                parameters=[\n                                    GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TriggerParameter(\n                                        dest=\"dest_example\",\n                                        operation=\"operation_example\",\n                                        src=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TriggerParameterSource(\n                                            context_key=\"context_key_example\",\n                                            context_template=\"context_template_example\",\n                                            data_key=\"data_key_example\",\n                                            data_template=\"data_template_example\",\n                                            dependency_name=\"dependency_name_example\",\n                                            use_raw_data=True,\n                                            value=\"value_example\",\n                                        ),\n                                    ),\n                                ],\n                                payload=[\n                                    GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TriggerParameter(\n                                        dest=\"dest_example\",\n                                        operation=\"operation_example\",\n                                        src=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TriggerParameterSource(\n                                            context_key=\"context_key_example\",\n                                            context_template=\"context_template_example\",\n                                            data_key=\"data_key_example\",\n                                            data_template=\"data_template_example\",\n                                            dependency_name=\"dependency_name_example\",\n                                            use_raw_data=True,\n                                            value=\"value_example\",\n                                        ),\n                                    ),\n                                ],\n                                version=\"version_example\",\n                            )\n```\n\n----------------------------------------\n\nTITLE: Disable Artifact Compression in Argo Workflows YAML\nDESCRIPTION: This YAML snippet demonstrates how to disable artifact compression in Argo Workflows by setting the `archive` property to `none`. This is necessary to view `.tgz` artifacts in the UI, as compressed artifacts are not supported.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/artifact-visualization.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- name: artifact\n  # ...\n  archive:\n    none: { }\n```\n\n----------------------------------------\n\nTITLE: Finding Workflows Using Deprecated ttlSecondsAfterFinished\nDESCRIPTION: This bash command uses kubectl to search for workflows across all namespaces that are using the deprecated `ttlSecondsAfterFinished` field. The output is in YAML format and piped to grep to filter for the specified field. This identifies workflows that need to be updated to use `spec.ttlStrategy.secondsAfterCompletion`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_18\n\nLANGUAGE: Bash\nCODE:\n```\nkubectl get wf --all-namespaces -o yaml | grep ttlSecondsAfterFinished\n```\n\n----------------------------------------\n\nTITLE: Resuming Cron Workflow with Argo Workflows API in Java\nDESCRIPTION: This code snippet demonstrates how to resume a CronWorkflow using the Argo Workflows API in Java. It initializes the API client, configures authentication, and then calls the `cronWorkflowServiceResumeCronWorkflow` method with the namespace, name of the CronWorkflow, and a resume request body. It handles potential API exceptions and prints the result or error details.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/CronWorkflowServiceApi.md#_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.CronWorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    CronWorkflowServiceApi apiInstance = new CronWorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    IoArgoprojWorkflowV1alpha1CronWorkflowResumeRequest body = new IoArgoprojWorkflowV1alpha1CronWorkflowResumeRequest(); // IoArgoprojWorkflowV1alpha1CronWorkflowResumeRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1CronWorkflow result = apiInstance.cronWorkflowServiceResumeCronWorkflow(namespace, name, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling CronWorkflowServiceApi#cronWorkflowServiceResumeCronWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Argo Template Create Options\nDESCRIPTION: Lists the available options for the `argo template create` command. These options control the output format, whether to perform strict validation, and provide help information.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template_create.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n  -h, --help            help for create\n  -o, --output string   Output format. One of: name|json|yaml|wide\n      --strict          perform strict workflow validation (default true)\n```\n\n----------------------------------------\n\nTITLE: Deleting the Latest Argo Workflow\nDESCRIPTION: This command shows how to delete the most recently submitted Argo workflow using the `@latest` alias. The `argo delete` command along with the `@latest` identifier targets the latest workflow for deletion.  The command relies on the Argo CLI tool being installed and configured correctly to access the Argo Workflows server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_delete.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo delete @latest\n```\n\n----------------------------------------\n\nTITLE: Argo CLI Global Options\nDESCRIPTION: Lists the options inherited from the parent commands in the Argo CLI. These options configure aspects such as server address, authentication, and logging.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n      --argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Setting Template Timeout in Argo Workflows (YAML)\nDESCRIPTION: This YAML configuration sets a timeout specifically for the `sleep` template in the Argo Workflow. The `activeDeadlineSeconds` field is set within the template definition to 10, meaning only this template will be terminated if it exceeds 10 seconds. The workflow's entrypoint is the `sleep` template, which is configured to sleep for 60 seconds, but will be terminated after 10.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/timeouts.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: timeouts-\nspec:\n  entrypoint: sleep\n  templates:\n  - name: sleep\n    activeDeadlineSeconds: 10 # terminate container template after 10 seconds\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"echo sleeping for 1m; sleep 60; echo done\"]\n```\n\n----------------------------------------\n\nTITLE: Getting Single Workflow Argo Workflows API (Bash)\nDESCRIPTION: This snippet retrieves a specific workflow by its name ('abc-dthgt') within the specified namespace ('argo') using a GET request to the Argo Workflows API.  It requires `curl` to be installed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/rest-examples.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request GET \\\n  --url https://localhost:2746/api/v1/workflows/argo/abc-dthgt\n```\n\n----------------------------------------\n\nTITLE: Resubmitting and Tailing Logs for Workflow Completion using Argo\nDESCRIPTION: This command resubmits a workflow and tails its logs until it completes.  The `--log` flag streams the workflow logs to the console, providing detailed insights into the execution process. It requires a single workflow to be resubmitted.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_resubmit.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nargo archive resubmit --log uid\n```\n\n----------------------------------------\n\nTITLE: Set Argo CLI Environment Variables for Docker\nDESCRIPTION: Sets environment variables required to configure the Argo CLI when running inside a Docker container. These variables define the Argo Server address, disable KUBECONFIG, and specify the namespace.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/access-token.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nARGO_SERVER=\"${{HOST}}:443\"\nKUBECONFIG=/dev/null\nARGO_NAMESPACE=sandbox\n```\n\n----------------------------------------\n\nTITLE: Print Logs of Workflow's Pods\nDESCRIPTION: This command prints logs from all pods associated with a given workflow. It retrieves logs from all containers within those pods.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_logs.md#_snippet_5\n\nLANGUAGE: CLI\nCODE:\n```\nargo logs my-wf my-pod\n```\n\n----------------------------------------\n\nTITLE: Run a Set of Tests by Build Tag - Bash\nDESCRIPTION: This bash command runs a set of tests based on the build tag defined at the top of the test file. For example, `make test-api` will run tests with the `//go:build api` tag.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_20\n\nLANGUAGE: Bash\nCODE:\n```\nmake test-api\n```\n\n----------------------------------------\n\nTITLE: Start Argo with TLS Proxy Simulation - Bash\nDESCRIPTION: This bash command starts Argo with TLS proxy simulation by using `UI_SECURE=true`, which implies `UI=true`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_15\n\nLANGUAGE: Bash\nCODE:\n```\nmake start UI_SECURE=true\n```\n\n----------------------------------------\n\nTITLE: Running Hello World with Docker Bash\nDESCRIPTION: This bash command demonstrates running the `busybox` container with an `echo` command directly from the shell. It is a simple way to execute the same container workload locally, without using Kubernetes or Argo Workflows. It requires Docker to be installed and configured.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/hello-world.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ docker run busybox echo \"hello world\"\nhello world\n```\n\n----------------------------------------\n\nTITLE: Example RBAC Expression (Bash)\nDESCRIPTION: This snippet provides an example of RBAC expression to grant access based on custom claim name for OIDC groups.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# assuming customClaimGroupName: argo_groups\nworkflows.argoproj.io/rbac-rule: \"'argo_admins' in groups\"\n```\n\n----------------------------------------\n\nTITLE: Define Workflow-Level Gauge Metric in Argo\nDESCRIPTION: This YAML snippet defines a Workflow-level Gauge metric named `exec_duration_gauge` with a label `name` and a help doc string. The `value` is set to the workflow duration using an Argo variable. The metric reports the workflow execution duration time.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: model-training-\nspec:\n  entrypoint: steps\n  metrics:\n    prometheus:\n      - name: exec_duration_gauge         # Metric name (will be prepended with \"argo_workflows_\")\n        labels:                           # Labels are optional. Avoid cardinality explosion.\n          - key: name\n            value: model_a\n        help: \"Duration gauge by name\"    # A help doc describing your metric. This is required.\n        gauge:                            # The metric type. Available are \"gauge\", \"histogram\", and \"counter\".\n          value: \"{{workflow.duration}}\"  # The value of your metric. It could be an Argo variable (see variables doc) or a literal value\n\n...\n```\n\n----------------------------------------\n\nTITLE: Workflow Finalizer Definition YAML\nDESCRIPTION: This YAML snippet shows the structure of a Workflow resource including the `workflows.argoproj.io/artifact-gc` finalizer. The finalizer prevents the Workflow from being deleted until artifact garbage collection is complete. It's part of the Workflow metadata and prevents premature deletion.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/artifacts.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\n  finalizers:\n  - workflows.argoproj.io/artifact-gc\n```\n\n----------------------------------------\n\nTITLE: Listing Workflows with Detailed Information with Argo CLI\nDESCRIPTION: Lists workflows with additional information such as parameters, using the `-o wide` flag. This presents a more comprehensive output.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_list.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nargo list -o wide\n```\n\n----------------------------------------\n\nTITLE: Creating Workflow in Argo Workflows API using Java\nDESCRIPTION: This Java code snippet demonstrates how to create a workflow using the Argo Workflows API. It initializes the API client, configures authentication, and calls the `workflowServiceCreateWorkflow` method with the namespace and workflow creation request body. The code includes exception handling for API errors and prints the result or error details to the console.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowServiceApi.md#_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.WorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    WorkflowServiceApi apiInstance = new WorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    IoArgoprojWorkflowV1alpha1WorkflowCreateRequest body = new IoArgoprojWorkflowV1alpha1WorkflowCreateRequest(); // IoArgoprojWorkflowV1alpha1WorkflowCreateRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1Workflow result = apiInstance.workflowServiceCreateWorkflow(namespace, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling WorkflowServiceApi#workflowServiceCreateWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Cron Workflow Run Once Configuration\nDESCRIPTION: This YAML snippet configures a CronWorkflow to run only once by setting `maxSuccess` to 1. The workflow scheduling stops once one workflow run completes successfully.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/proposals/cron-wf-improvement-proposal.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nRunStrategy:\n maxSuccess: 1\n```\n\n----------------------------------------\n\nTITLE: Annotating Service Account for Secret Discovery (YAML)\nDESCRIPTION: This YAML configures a Kubernetes ServiceAccount named `default` by adding an annotation `workflows.argoproj.io/service-account-token.name` that specifies the name of the associated secret (`my-token`). This method is useful when the secret already exists or the service account has a very long name, and allows to associate the secret with the service account.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/service-account-secrets.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: default\n  annotations:\n    workflows.argoproj.io/service-account-token.name: my-token\n```\n\n----------------------------------------\n\nTITLE: Configuring PodAntiAffinity\nDESCRIPTION: This snippet configures a PodAntiAffinity, which defines rules to avoid scheduling pods on the same node or in the same domain as other pods. It includes preferredDuringSchedulingIgnoredDuringExecution (a list of WeightedPodAffinityTerm) and requiredDuringSchedulingIgnoredDuringExecution (a list of PodAffinityTerm).  This configuration ensures that pods are spread across different nodes or domains, enhancing availability and resilience.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nPodAntiAffinity(\n  preferred_during_scheduling_ignored_during_execution=[\n    WeightedPodAffinityTerm(\n      pod_affinity_term=PodAffinityTerm(\n        label_selector=LabelSelector(\n          match_expressions=[\n            LabelSelectorRequirement(\n              key=\"key_example\",\n              operator=\"operator_example\",\n              values=[\n                \"values_example\",\n              ],\n            ),\n          ],\n          match_labels={\n            \"key\": \"key_example\",\n          },\n        ),\n        match_label_keys=[\n          \"match_label_keys_example\",\n        ],\n        mismatch_label_keys=[\n          \"mismatch_label_keys_example\",\n        ],\n        namespace_selector=LabelSelector(\n          match_expressions=[\n            LabelSelectorRequirement(\n              key=\"key_example\",\n              operator=\"operator_example\",\n              values=[\n                \"values_example\",\n              ],\n            ),\n          ],\n          match_labels={\n            \"key\": \"key_example\",\n          },\n        ),\n        namespaces=[\n          \"namespaces_example\",\n        ],\n        topology_key=\"topology_key_example\",\n      ),\n      weight=1,\n    ),\n  ],\n  required_during_scheduling_ignored_during_execution=[\n    PodAffinityTerm(\n      label_selector=LabelSelector(\n        match_expressions=[\n          LabelSelectorRequirement(\n            key=\"key_example\",\n            operator=\"operator_example\",\n            values=[\n              \"values_example\",\n            ],\n          ),\n        ],\n        match_labels={\n          \"key\": \"key_example\",\n        },\n      ),\n      match_label_keys=[\n        \"match_label_keys_example\",\n      ],\n      mismatch_label_keys=[\n        \"mismatch_label_keys_example\",\n      ],\n      namespace_selector=LabelSelector(\n        match_expressions=[\n          LabelSelectorRequirement(\n            key=\"key_example\",\n            operator=\"operator_example\",\n            values=[\n              \"values_example\",\n            ],\n          ),\n        ],\n        match_labels={\n          \"key\": \"key_example\",\n        },\n      ),\n      namespaces=[\n        \"namespaces_example\",\n      ],\n      topology_key=\"topology_key_example\",\n    ),\n  ],\n)\n```\n\n----------------------------------------\n\nTITLE: Installing and Verifying Argo Workflows\nDESCRIPTION: This snippet installs Argo Workflows using the `make start` command with the stress profile, then verifies the installation by checking the status of deployments using `kubectl`.  Successful deployment is necessary before running any workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/stress-testing.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install workflows (If this fails, try running it again):\nmake start PROFILE=stress\n\n# Make sure pods are running:\nkubectl get deployments\n```\n\n----------------------------------------\n\nTITLE: Workflow Configuration with Retry Strategy (YAML)\nDESCRIPTION: This YAML configuration demonstrates how to define a workflow with a retry strategy that automatically retries pods if they are deleted due to an error. The `retryPolicy` is set to `OnError`, and a `limit` of 1 retry attempt is specified. The workflow defines a single template named `main` that uses a `busybox` container to sleep for 30 seconds.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/tolerating-pod-deletion.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: example\nspec:\n  retryStrategy:\n   retryPolicy: OnError\n   limit: 1\n  entrypoint: main\n  templates:\n    - name: main\n      container:\n        image: busybox\n        command:\n          - sleep\n          - 30s\n```\n\n----------------------------------------\n\nTITLE: Replacing synchronization mutex - YAML\nDESCRIPTION: This snippet shows how to replace the deprecated `mutex` field in the `synchronization` block with the new `mutexes` field, which now takes a list. The original single value is moved into a list.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/deprecations.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nsynchronization:\n  mutexes:\n    - name: foobar\n```\n\n----------------------------------------\n\nTITLE: Configure User Info Path (YAML)\nDESCRIPTION: This snippet configures the `userInfoPath` to specify the user info endpoint that contains the groups claim, in cases when the OIDC provider provides groups information only using the user-info endpoint (e.g. Okta).\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nsso:\n  userInfoPath: /oauth2/v1/userinfo\n```\n\n----------------------------------------\n\nTITLE: Remove Finalizer via kubectl patch\nDESCRIPTION: This command uses `kubectl patch` to remove the `workflows.argoproj.io/artifact-gc` finalizer from a Workflow resource. It uses a JSON patch to remove the `/metadata/finalizers` path. This allows the Workflow to be deleted even if artifact garbage collection has failed. Requires kubectl installed and configured.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/artifacts.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nkubectl patch workflow my-wf \\\n    --type json \\\n    --patch='[ { \"op\": \"remove\", \"path\": \"/metadata/finalizers\" } ]'\n```\n\n----------------------------------------\n\nTITLE: Get Input Artifact by UID Example in Python\nDESCRIPTION: This example demonstrates the usage of `get_input_artifact_by_uid` to retrieve an input artifact by its UID. It uses the `argo_workflows` package and configures API key authentication.  The snippet sets the UID, node ID, and artifact name parameters, calls the API, and prints the result.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArtifactServiceApi.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import artifact_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = artifact_service_api.ArtifactServiceApi(api_client)\n    uid = \"uid_example\" # str | \n    node_id = \"nodeId_example\" # str | \n    artifact_name = \"artifactName_example\" # str | \n\n    # example passing only required values which don't have defaults set\n    try:\n        # Get an input artifact by UID.\n        api_response = api_instance.get_input_artifact_by_uid(uid, node_id, artifact_name)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArtifactServiceApi->get_input_artifact_by_uid: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Resubmitting and Waiting for Workflow Completion using Argo\nDESCRIPTION: This command resubmits a workflow and then waits for it to complete. It utilizes the `--wait` flag, which blocks until the workflow finishes, providing immediate feedback on the resubmission's success or failure.  It only works when a single workflow is resubmitted.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_resubmit.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nargo archive resubmit --wait uid\n```\n\n----------------------------------------\n\nTITLE: Configuring a Slack Event Source in Argo Events\nDESCRIPTION: This snippet shows the configuration of a Slack event source in Argo Events. Key parameters include the filter expression, metadata, signing secret, token, and webhook context, which specifies authentication, endpoint, metadata, method, port, server certificate secret, server key secret, and URL. Uses `GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SlackEventSource` and related objects.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_28\n\nLANGUAGE: YAML\nCODE:\n```\nslack={\n                    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SlackEventSource(\n                        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceFilter(\n                            expression=\"expression_example\",\n                        ),\n                        metadata={\n                            \"key\": \"key_example\",\n                        },\n                        signing_secret=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        token=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        webhook=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1WebhookContext(\n                            auth_secret=SecretKeySelector(\n                                key=\"key_example\",\n                                name=\"name_example\",\n                                optional=True,\n                            ),\n                            endpoint=\"endpoint_example\",\n                            max_payload_size=\"max_payload_size_example\",\n                            metadata={\n                                \"key\": \"key_example\",\n                            },\n                            method=\"method_example\",\n                            port=\"port_example\",\n                            server_cert_secret=SecretKeySelector(\n                                key=\"key_example\",\n                                name=\"name_example\",\n                                optional=True,\n                            ),\n                            server_key_secret=SecretKeySelector(\n                                key=\"key_example\",\n                                name=\"name_example\",\n                                optional=True,\n                            ),\n                            url=\"url_example\",\n                        ),\n                    ),\n                }\n```\n\n----------------------------------------\n\nTITLE: Argo Resume Latest Workflow Example\nDESCRIPTION: Shows how to resume the latest workflow using the `@latest` keyword. This resumes the most recently submitted workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resume.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n# Resume the latest workflow:\n\t\t\n  argo resume @latest\n\t\t\n```\n\n----------------------------------------\n\nTITLE: Looping with Items (Complex) in Argo Workflows (YAML)\nDESCRIPTION: This example demonstrates using `withItems` with a JSON object for each entry, allowing multiple pieces of information to be passed to each template. The elements are addressed by key using `{{item.key}}`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/loops.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: loops-maps-\nspec:\n  entrypoint: loop-map-example\n  templates:\n  - name: loop-map-example # parameter specifies the list to iterate over\n    steps:\n    - - name: test-linux\n        template: cat-os-release\n        arguments:\n          parameters:\n          - name: image\n            value: \"{{item.image}}\"\n          - name: tag\n            value: \"{{item.tag}}\"\n        withItems:\n        - { image: 'debian', tag: '9.1' }       #item set 1\n        - { image: 'debian', tag: '8.9' }       #item set 2\n        - { image: 'alpine', tag: '3.6' }       #item set 3\n        - { image: 'ubuntu', tag: '17.10' }     #item set 4\n\n  - name: cat-os-release\n    inputs:\n      parameters:\n      - name: image\n      - name: tag\n    container:\n      image: \"{{inputs.parameters.image}}:{{inputs.parameters.tag}}\"\n      command: [cat]\n      args: [/etc/os-release]\n```\n\n----------------------------------------\n\nTITLE: Valid ConfigMap - Executor Image\nDESCRIPTION: This YAML snippet shows a valid ConfigMap configuration for Argo Workflows. The `executor.image` setting is used to configure the executor image, replacing the deprecated `executorImage` field.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workflow-controller-configmap\ndata:\n  ...\n  executor: |\n    image: argoproj/argocli:latest\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring WeightedPodAffinityTerm\nDESCRIPTION: This snippet configures a WeightedPodAffinityTerm, which combines a PodAffinityTerm with a weight. The weight is a value between 1 and 100 that indicates the importance of the term in relation to other terms. This enables defining preferred pod placement based on affinity rules, with the weight influencing the scheduler's decision.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nWeightedPodAffinityTerm(\n  pod_affinity_term=PodAffinityTerm(\n    label_selector=LabelSelector(\n      match_expressions=[\n        LabelSelectorRequirement(\n          key=\"key_example\",\n          operator=\"operator_example\",\n          values=[\n            \"values_example\",\n          ],\n        ),\n      ],\n      match_labels={\n        \"key\": \"key_example\",\n      },\n    ),\n    match_label_keys=[\n      \"match_label_keys_example\",\n    ],\n    mismatch_label_keys=[\n      \"mismatch_label_keys_example\",\n    ],\n    namespace_selector=LabelSelector(\n      match_expressions=[\n        LabelSelectorRequirement(\n          key=\"key_example\",\n          operator=\"operator_example\",\n          values=[\n            \"values_example\",\n          ],\n        ),\n      ],\n      match_labels={\n        \"key\": \"key_example\",\n      },\n    ),\n    namespaces=[\n      \"namespaces_example\",\n    ],\n    topology_key=\"topology_key_example\",\n  ),\n  weight=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Workflows with Specific Labels with Argo CLI\nDESCRIPTION: Lists workflows that have both specified labels using the `-l` or `--selector` flag. Multiple labels are comma-separated (e.g., `-l label1=value1,label2=value2`).\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_list.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nargo list -l label1=value1,label2=value2\n```\n\n----------------------------------------\n\nTITLE: Retrieving Version using get_version in Python\nDESCRIPTION: This snippet demonstrates how to retrieve the version information of Argo Workflows using the `get_version` method of the `InfoServiceApi`.  It requires the `argo_workflows` package and uses API key authentication. This endpoint does not take any parameters.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/InfoServiceApi.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import info_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_version import IoArgoprojWorkflowV1alpha1Version\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = info_service_api.InfoServiceApi(api_client)\n\n    # example, this endpoint has no required or optional parameters\n    try:\n        api_response = api_instance.get_version()\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling InfoServiceApi->get_version: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Apply Azurite Deployment - Bash\nDESCRIPTION: This bash command applies the Azurite deployment configuration for running Azure tests against a local Azurite instance. It requires `kubectl` and a Kubernetes cluster connection.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_18\n\nLANGUAGE: Bash\nCODE:\n```\nkubectl -n $KUBE_NAMESPACE apply -f test/e2e/azure/deploy-azurite.yaml\nmake start\n```\n\n----------------------------------------\n\nTITLE: Configuring LabelSelector with matchLabels\nDESCRIPTION: This snippet shows how to define a `LabelSelector` with `match_labels`.  This allows selecting resources that have specific key-value label pairs. It's often used for pod affinity and node selectors to target specific nodes or pods.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_33\n\nLANGUAGE: YAML\nCODE:\n```\nLabelSelector(\n  match_expressions=[\n    LabelSelectorRequirement(\n      key=\"key_example\",\n      operator=\"operator_example\",\n      values=[\n        \"values_example\",\n      ],\n    ),\n  ],\n  match_labels={\n    \"key\": \"key_example\",\n  },\n)\n```\n\n----------------------------------------\n\nTITLE: Configure Azure Artifact with Account Key\nDESCRIPTION: This YAML snippet configures an Argo artifact to be stored in Azure Blob Storage. It specifies the endpoint, container, and blob path, and uses an `accountKeySecret` to reference a Kubernetes Secret containing the storage account access key. The Secret name is `my-azure-storage-credentials` and the key within the Secret is `account-access-key`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\nartifacts:\n  - name: message\n    path: /tmp/message\n    azure:\n      endpoint: https://mystorageaccountname.blob.core.windows.net\n      container: my-container-name\n      blob: path/in/container\n      # accountKeySecret is a secret selector.\n      # It references the Kubernetes Secret named 'my-azure-storage-credentials'.\n      # This secret is expected to have the key 'account-access-key',\n      # containing the base64 encoded credentials to the storage account.\n      accountKeySecret:\n        name: my-azure-storage-credentials\n        key: account-access-key\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Checks (Bash)\nDESCRIPTION: This command runs all pre-commit checks to ensure the code adheres to the project's style guidelines and quality standards. It is essential to run this before committing code to prevent issues in the CI/CD pipeline. The `-B` flag forces a rebuild.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nmake pre-commit -B\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS S3 Artifact with Access Keys\nDESCRIPTION: This YAML snippet configures an Argo Workflow artifact to use AWS S3.  It specifies the endpoint, bucket, and key for the artifact.  It uses Kubernetes secrets (`my-s3-credentials`) to store the access key and secret key. These secrets should have keys named `accessKey` and `secretKey`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nartifacts:\n  - name: my-output-artifact\n    path: /my-output-artifact\n    s3:\n      endpoint: s3.amazonaws.com\n      bucket: my-s3-bucket\n      key: path/in/bucket/my-output-artifact.tgz\n      # The following fields are secret selectors.\n      # They reference the k8s secret named 'my-s3-credentials'.\n      # This secret is expected to have the keys 'accessKey' and 'secretKey',\n      # containing the base64 encoded credentials to the bucket.\n      accessKeySecret:\n        name: my-s3-credentials\n        key: accessKey\n      secretKeySecret:\n        name: my-s3-credentials\n        key: secretKey\n```\n\n----------------------------------------\n\nTITLE: Get Artifact File with ArtifactServiceApi in Java\nDESCRIPTION: This code snippet demonstrates how to retrieve an artifact file using the `artifactServiceGetArtifactFile` method of the `ArtifactServiceApi`. It initializes the API client, configures authentication, and then calls the method with required parameters such as namespace, ID discriminator, ID, node ID, artifact name, and artifact discriminator. The returned file is then printed to the console. An exception handler is included to catch and print any errors that occur during the API call.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ArtifactServiceApi.md#_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ArtifactServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ArtifactServiceApi apiInstance = new ArtifactServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String idDiscriminator = \"idDiscriminator_example\"; // String | \n    String id = \"id_example\"; // String | \n    String nodeId = \"nodeId_example\"; // String | \n    String artifactName = \"artifactName_example\"; // String | \n    String artifactDiscriminator = \"artifactDiscriminator_example\"; // String | \n    try {\n      File result = apiInstance.artifactServiceGetArtifactFile(namespace, idDiscriminator, id, nodeId, artifactName, artifactDiscriminator);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ArtifactServiceApi#artifactServiceGetArtifactFile\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Executor Plugin Re-Queue Response\nDESCRIPTION: This JSON snippet shows an example of the JSON response for re-queueing. The `requeue` field contains an amount of time, after which the task will be re-queued.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"node\": {\n    \"phase\": \"Running\",\n    \"message\": \"Long-running task started\"\n  },\n  \"requeue\": \"2m\"\n}\n```\n\n----------------------------------------\n\nTITLE: Python Script to Generate Random Integer\nDESCRIPTION: This Python script generates a random integer between 1 and 100 using the `random.randint` function.  It requires the `python:alpine3.6` image to have the `random` module available. The output is printed to standard output which becomes the `result` of the template.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/scripts-and-results.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport random\ni = random.randint(1, 100)\nprint(i)\n```\n\n----------------------------------------\n\nTITLE: Configuring Slack Trigger in Argo Events\nDESCRIPTION: This snippet illustrates the configuration of a Slack trigger in Argo Events. It demonstrates how to specify message attachments, blocks, channel, and the message itself. It includes trigger parameters for dynamic message content and a sender configuration to customize the Slack message appearance (icon, username). The Slack token is retrieved using a SecretKeySelector for secure handling.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nslack=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SlackTrigger(\n                                attachments=\"attachments_example\",\n                                blocks=\"blocks_example\",\n                                channel=\"channel_example\",\n                                message=\"message_example\",\n                                parameters=[\n                                    GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TriggerParameter(\n                                        dest=\"dest_example\",\n                                        operation=\"operation_example\",\n                                        src=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TriggerParameterSource(\n                                            context_key=\"context_key_example\",\n                                            context_template=\"context_template_example\",\n                                            data_key=\"data_key_example\",\n                                            data_template=\"data_template_example\",\n                                            dependency_name=\"dependency_name_example\",\n                                            use_raw_data=True,\n                                            value=\"value_example\",\n                                        ),\n                                    ),\n                                ],\n                                sender=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SlackSender(\n                                    icon=\"icon_example\",\n                                    username=\"username_example\",\n                                ),\n                                slack_token=SecretKeySelector(\n                                    key=\"key_example\",\n                                    name=\"name_example\",\n                            \n```\n\n----------------------------------------\n\nTITLE: Valid ConfigMap - Executor Resources\nDESCRIPTION: This YAML snippet shows a valid ConfigMap configuration, correctly nesting `resources` within the `executor` section. This configuration properly defines resource requests and limits for the executor.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  ...\n  executor: |\n    resources:\n      requests:\n        cpu: 0.1\n        memory: 64Mi\n      limits:\n        cpu: 0.5\n        memory: 512Mi\n  ...\n```\n\n----------------------------------------\n\nTITLE: Revoke Argo Server SSO Tokens (Bash)\nDESCRIPTION: This snippet deletes the Kubernetes secret that stores the token encryption key. Deleting the secret will invalidate all existing SSO tokens, forcing users to log in again after the Argo Server restarts.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete secret sso\n```\n\n----------------------------------------\n\nTITLE: Invalid ConfigMap - Executor Image\nDESCRIPTION: This YAML snippet demonstrates an invalid ConfigMap configuration for Argo Workflows. The `executorImage` field is deprecated and should be replaced with the `executor.image` configuration.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workflow-controller-configmap\ndata:\n  ...\n  executorImage: argoproj/argocli:latest\n  ...\n```\n\n----------------------------------------\n\nTITLE: Argo Inherited Options\nDESCRIPTION: These options are inherited from parent commands and provide settings for configuring the Argo CLI client, including server address, authentication, TLS settings, and logging levels. They can be used to customize the CLI's interaction with the Argo server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_lint.md#_snippet_2\n\nLANGUAGE: go\nCODE:\n```\n      --argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Replacing cronworkflow schedule - YAML\nDESCRIPTION: This snippet shows how to replace the deprecated `schedule` field in a `cronworkflow` with the new `schedules` field, which now takes a list. The original single value is moved into a list.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/deprecations.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nspec:\n  schedules:\n    - \"30 1 * * *\"\n```\n\n----------------------------------------\n\nTITLE: Bash Script to Generate Random Integer\nDESCRIPTION: This Bash script generates a random integer between 1 and 100 using `cat /dev/urandom`, `od`, and `awk`. The standard output of this script is captured as the `result` output parameter of the `gen-random-int-bash` template.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/scripts-and-results.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncat /dev/urandom | od -N2 -An -i | awk -v f=1 -v r=100 '{printf \"%i\\n\", f + r * $1 / 65536}'\n```\n\n----------------------------------------\n\nTITLE: Get Input Artifact with ArtifactServiceApi in Java\nDESCRIPTION: This code snippet shows how to retrieve an input artifact using the `artifactServiceGetInputArtifact` method. It configures the API client, sets up authentication with a Bearer token, and then calls the API method with the namespace, name, node ID, and artifact name. The retrieved file is printed to the console. An exception handler is included to catch API exceptions and print the error details.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ArtifactServiceApi.md#_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ArtifactServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ArtifactServiceApi apiInstance = new ArtifactServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    String nodeId = \"nodeId_example\"; // String | \n    String artifactName = \"artifactName_example\"; // String | \n    try {\n      File result = apiInstance.artifactServiceGetInputArtifact(namespace, name, nodeId, artifactName);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ArtifactServiceApi#artifactServiceGetInputArtifact\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Steps Template in Argo\nDESCRIPTION: This YAML snippet defines a 'hello-hello-hello' template of type 'steps'. It outlines a series of steps where 'step1' executes first, followed by 'step2a' and 'step2b' running in parallel.  The template names 'prepare-data', 'run-data-first-half', and 'run-data-second-half' should refer to other defined templates.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-concepts.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n  - name: hello-hello-hello\n    steps:\n    - - name: step1\n        template: prepare-data\n    - - name: step2a\n        template: run-data-first-half\n      - name: step2b\n        template: run-data-second-half\n```\n\n----------------------------------------\n\nTITLE: Inspecting Docker Image Entrypoint and Command\nDESCRIPTION: This command uses `docker image inspect` to retrieve the entrypoint and command configured for a specified Docker image. The output format is specified using `-f` to extract only the entrypoint and command values. This is relevant for configuring the `command` for containers when using the Emissary executor.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-executors.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker image inspect -f '{{.Config.Entrypoint}} {{.Config.Cmd}}' argoproj/argosay:v2\n```\n\n----------------------------------------\n\nTITLE: Argo Submit WorkflowTemplate with Parameters (CLI)\nDESCRIPTION: This command shows how to submit a WorkflowTemplate as a Workflow using the Argo CLI and passing in a parameter.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nargo submit --from workflowtemplate/workflow-template-submittable -p message=value1\n```\n\n----------------------------------------\n\nTITLE: Sensor API Authentication and Configuration with BearerToken (Python)\nDESCRIPTION: This code snippet demonstrates how to configure and authenticate with the Argo Workflows Sensor API using a bearer token. It imports necessary modules from the argo_workflows library, defines the API host, and configures the API key for authentication.  A sensor update request is prepared using the imported models.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import sensor_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.github_com_argoproj_argo_events_pkg_apis_events_v1alpha1_sensor import GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Sensor\nfrom argo_workflows.model.sensor_update_sensor_request import SensorUpdateSensorRequest\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n```\n\n----------------------------------------\n\nTITLE: Argo CLI Global Options\nDESCRIPTION: These are global options inherited from parent commands in the Argo CLI. They configure various aspects of the CLI's behavior, such as server address, authentication, TLS settings, logging level, and namespace. These options are available across multiple Argo CLI commands.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_auth_token.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n--argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Test API Access with Token (Failure)\nDESCRIPTION: Tests the access token by attempting to access a resource (workflow-templates) for which the token should not have permissions.  A 403 error indicates that the token is working correctly and preventing unauthorized access.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/access-token.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://localhost:2746/api/v1/workflow-templates/argo -H \"Authorization: $ARGO_TOKEN\"\n# 403 error\n```\n\n----------------------------------------\n\nTITLE: Submitting Workflow using Argo CLI\nDESCRIPTION: This bash command uses the Argo CLI to submit a Workflow from a YAML file located at the specified URL. Dependencies: Argo CLI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cluster-workflow-templates.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\nargo submit https://raw.githubusercontent.com/argoproj/argo-workflows/main/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml\n```\n\n----------------------------------------\n\nTITLE: Create Event Source in Argo Workflows using Python\nDESCRIPTION: This code snippet demonstrates how to create an event source within Argo Workflows using the Python client. It configures the API client, sets up authentication, and calls the `create_event_source` method to create a new event source in a specified namespace. The example requires the `argo_workflows` package and uses API key authentication.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import event_source_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.eventsource_create_event_source_request import EventsourceCreateEventSourceRequest\nfrom argo_workflows.model.github_com_argoproj_argo_events_pkg_apis_events_v1alpha1_event_source import GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSource\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n```\n\n----------------------------------------\n\nTITLE: Retry Multiple Workflows by UID - Argo\nDESCRIPTION: This command demonstrates how to retry multiple workflows simultaneously by providing a list of their UIDs to `argo archive retry`. It allows for batch re-execution of several workflows at once.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_retry.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo archive retry uid another-uid\n```\n\n----------------------------------------\n\nTITLE: Replacing synchronization semaphore - YAML\nDESCRIPTION: This snippet shows how to replace the deprecated `semaphore` field in the `synchronization` block with the new `semaphores` field, which now takes a list. The original single value is moved into a list.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/deprecations.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nsynchronization:\n  semaphore:\n    configMapKeyRef:\n      name: my-config\n      key: workflow\n```\n\n----------------------------------------\n\nTITLE: Start Argo Server with SSO (Bash)\nDESCRIPTION: This snippet starts the Argo Server with SSO authentication enabled using the `--auth-mode sso` flag.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo server --auth-mode sso --auth-mode ...\n```\n\n----------------------------------------\n\nTITLE: Cron Workflow Back-off Strategy Configuration\nDESCRIPTION: This YAML snippet shows how to configure a back-off strategy for CronWorkflows.  The user can specify a `value` for the maximum number of failures before stopping the schedule, and can also enable a back-off period between failures using the `back-off` option.  This allows the user to configure the workflow to backoff instead of stopping the workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/proposals/cron-wf-improvement-proposal.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nRunStrategy:\n maxSuccess:\n maxFailures:\n  value: # this would be optional\n  back-off:\n   enabled: true\n   factor: 2\n```\n\n----------------------------------------\n\nTITLE: Create Kubernetes Service Account\nDESCRIPTION: Creates a Kubernetes service account named 'jenkins'. Service accounts provide an identity for processes that run inside a Pod.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/access-token.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create sa jenkins\n```\n\n----------------------------------------\n\nTITLE: Get Input Artifact by UID with ArtifactServiceApi in Java\nDESCRIPTION: This Java code snippet illustrates how to retrieve an input artifact using its UID with the `artifactServiceGetInputArtifactByUID` method. It initializes the ApiClient, sets the base path, and configures BearerToken authentication. The code then calls the API with the artifact's UID, node ID, and artifact name. The retrieved file is printed to the console. It includes exception handling for potential API errors, printing details like status code, response body, and headers.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ArtifactServiceApi.md#_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ArtifactServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ArtifactServiceApi apiInstance = new ArtifactServiceApi(defaultClient);\n    String uid = \"uid_example\"; // String | \n    String nodeId = \"nodeId_example\"; // String | \n    String artifactName = \"artifactName_example\"; // String | \n    try {\n      File result = apiInstance.artifactServiceGetInputArtifactByUID(uid, nodeId, artifactName);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ArtifactServiceApi#artifactServiceGetInputArtifactByUID\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Resubmitting Multiple Workflows with Argo\nDESCRIPTION: Resubmits multiple Argo Workflows specified by their names.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resubmit.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nargo resubmit my-wf my-other-wf my-third-wf\n```\n\n----------------------------------------\n\nTITLE: Replacing cronworkflow schedule - YAML\nDESCRIPTION: This snippet shows how to replace the deprecated `schedule` field in a `cronworkflow` with the new `schedules` field, which now takes a list. The original single value is moved into a list.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/deprecations.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nspec:\n  schedule: \"30 1 * * *\"\n```\n\n----------------------------------------\n\nTITLE: Get Sensor Example (Python)\nDESCRIPTION: Retrieves a specific sensor from a given namespace using its name. Requires API key authentication (BearerToken). The example demonstrates initializing the API client, configuring authentication, and calling the `get_sensor` method with the namespace and sensor name. Optionally, it can also include `get_options_resource_version`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import sensor_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.github_com_argoproj_argo_events_pkg_apis_events_v1alpha1_sensor import GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Sensor\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = sensor_service_api.SensorServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    name = \"name_example\" # str | \n    get_options_resource_version = \"getOptions.resourceVersion_example\" # str | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.get_sensor(namespace, name)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling SensorServiceApi->get_sensor: %s\\n\" % e)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.get_sensor(namespace, name, get_options_resource_version=get_options_resource_version)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling SensorServiceApi->get_sensor: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Force Triggering CI for Changelog Updates\nDESCRIPTION: These git commands are used to force trigger a new CI build after the changelog has been updated. This ensures that the updated changelog is included in the release. This involves deleting the local branch, fetching the upstream branch, checking it out, creating an empty commit, and pushing the changes.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/releasing.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit branch -D create-pull-request/changelog\ngit fetch upstream\ngit checkout --track upstream/create-pull-request/changelog\ngit commit -s --allow-empty -m \"chore: Force trigger CI\"\ngit push upstream create-pull-request/changelog\n```\n\n----------------------------------------\n\nTITLE: Defining Artifact Repository ConfigMap in YAML\nDESCRIPTION: This snippet demonstrates how to define an artifact repository configuration using a ConfigMap. The ConfigMap contains the repository configuration, including bucket name, endpoint, insecure flag, and access/secret key secrets. It's designed to be used with Argo Workflows to reduce template duplication and store sensitive information outside of templates.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/artifact-repository-ref.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  # If you want to use this config map by default, name it \"artifact-repositories\". Otherwise, you can provide a reference to a\n  # different config map in `artifactRepositoryRef.configMap`.\n  name: my-artifact-repository\n  annotations:\n    # v3.0 and after - if you want to use a specific key, put that key into this annotation.\n    workflows.argoproj.io/default-artifact-repository: default-v1-s3-artifact-repository\ndata:\n  default-v1-s3-artifact-repository: |\n    s3:\n      bucket: my-bucket\n      endpoint: minio:9000\n      insecure: true\n      accessKeySecret:\n        name: my-minio-cred\n        key: accesskey\n      secretKeySecret:\n        name: my-minio-cred\n        key: secretkey\n  v2-s3-artifact-repository: |\n    s3:\n      ...\n```\n\n----------------------------------------\n\nTITLE: Defining Recursive Coin Flip Workflow YAML\nDESCRIPTION: This YAML defines an Argo Workflow that recursively flips a coin using the `coinflip` template until the result is 'heads'. The workflow includes `flip-coin`, `heads`, and `tails` templates to manage the coin flip and recursion logic. The `when` condition controls the recursive calls to `coinflip` based on the coin flip result.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/recursion.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: coinflip-recursive-\nspec:\n  entrypoint: coinflip\n  templates:\n  - name: coinflip\n    steps:\n    # flip a coin\n    - - name: flip-coin\n        template: flip-coin\n    # evaluate the result in parallel\n    - - name: heads\n        template: heads                 # call heads template if \"heads\"\n        when: \"{{steps.flip-coin.outputs.result}} == heads\"\n      - name: tails                     # keep flipping coins if \"tails\"\n        template: coinflip\n        when: \"{{steps.flip-coin.outputs.result}} == tails\"\n\n  - name: flip-coin\n    script:\n      image: python:alpine3.6\n      command: [python]\n      source: |\n        import random\n        result = \"heads\" if random.randint(0,1) == 0 else \"tails\"\n        print(result)\n\n  - name: heads\n    container:\n      image: alpine:3.6\n      command: [sh, -c]\n      args: [\"echo \\\"it was heads\\\"\"]\n```\n\n----------------------------------------\n\nTITLE: Executor Environment Variable Configuration\nDESCRIPTION: Example of configuring the Argo Executor environment variables within a `workflow-controller-configmap`.  This demonstrates how to set `RESOURCE_STATE_CHECK_INTERVAL` to 3 seconds.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/environment-variables.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workflow-controller-configmap\ndata:\n  config: |\n    executor:\n      env:\n      - name: RESOURCE_STATE_CHECK_INTERVAL\n        value: 3s\n```\n\n----------------------------------------\n\nTITLE: Argo Auth Command Usage\nDESCRIPTION: This snippet shows the basic usage of the `argo auth` command in the Argo Workflows CLI. It is used for managing authentication settings.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_auth.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nargo auth [flags]\n```\n\n----------------------------------------\n\nTITLE: Update and Output Argo Cron Workflow as YAML\nDESCRIPTION: This snippet shows how to update a cron workflow template and print the result as YAML using the `--output yaml` flag. FILE1 is the path to the workflow template file. The output will be a YAML representation of the updated cron workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_update.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n# Update a Cron Workflow Template and print it as YAML:\n  argo cron update FILE1 --output yaml\n```\n\n----------------------------------------\n\nTITLE: Get Workflow Info by UID - Argo Archive\nDESCRIPTION: Retrieves information about an archived workflow using its UID. This example demonstrates retrieving workflow details in a default format.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_get.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Get information about an archived workflow by its UID:\n  argo archive get abc123-def456-ghi789-jkl012\n```\n\n----------------------------------------\n\nTITLE: Release Workflow Step from Pause (Bash)\nDESCRIPTION: Releases the workflow step from its paused state by creating a marker file in the container's filesystem. The location of the file depends on whether the step was paused before or after execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/debug-pause.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntouch /proc/1/root/var/run/argo/ctr/main/after\n```\n\n----------------------------------------\n\nTITLE: Listing Archived Workflows with Argo Workflows API in Java\nDESCRIPTION: This code snippet showcases how to list archived workflows using the Argo Workflows Java client library. It initializes the ApiClient, sets the base path, configures API key authentication, and calls the archivedWorkflowServiceListArchivedWorkflows method of the ArchivedWorkflowServiceApi.  Several optional parameters for filtering and pagination are also demonstrated. The snippet handles potential ApiException by printing error details.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ArchivedWorkflowServiceApi.md#_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ArchivedWorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ArchivedWorkflowServiceApi apiInstance = new ArchivedWorkflowServiceApi(defaultClient);\n    String listOptionsLabelSelector = \"listOptionsLabelSelector_example\"; // String | A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional.\n    String listOptionsFieldSelector = \"listOptionsFieldSelector_example\"; // String | A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional.\n    Boolean listOptionsWatch = true; // Boolean | Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional.\n    Boolean listOptionsAllowWatchBookmarks = true; // Boolean | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional.\n    String listOptionsResourceVersion = \"listOptionsResourceVersion_example\"; // String | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    String listOptionsResourceVersionMatch = \"listOptionsResourceVersionMatch_example\"; // String | resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    String listOptionsTimeoutSeconds = \"listOptionsTimeoutSeconds_example\"; // String | Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional.\n    String listOptionsLimit = \"listOptionsLimit_example\"; // String | limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n    String listOptionsContinue = \"listOptionsContinue_example\"; // String | The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n    Boolean listOptionsSendInitialEvents = true; // Boolean | `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\\\"io.k8s.initial-events-end\\\": \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"` (for backward compatibility reasons) and to false otherwise. +optional\n    String namePrefix = \"namePrefix_example\"; // String |\n    String namespace = \"namespace_example\"; // String |\n    try {\n      IoArgoprojWorkflowV1alpha1WorkflowList result = apiInstance.archivedWorkflowServiceListArchivedWorkflows(listOptionsLabelSelector, listOptionsFieldSelector, listOptionsWatch, listOptionsAllowWatchBookmarks, listOptionsResourceVersion, listOptionsResourceVersionMatch, listOptionsTimeoutSeconds, listOptionsLimit, listOptionsContinue, listOptionsSendInitialEvents, namePrefix, namespace);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ArchivedWorkflowServiceApi#archivedWorkflowServiceListArchivedWorkflows\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: List Completed Workflows - Bash\nDESCRIPTION: This bash command lists completed Argo Workflows that have finished within the last 7 days. It leverages the `argo list` command with the `--completed` and `--since` flags to filter the results.  It requires the `argo` CLI tool to be installed and configured to connect to the Argo Workflows cluster.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cost-optimisation.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo list --completed --since 7d\n```\n\n----------------------------------------\n\nTITLE: Resubmitting Workflows by Field Selector using Argo\nDESCRIPTION: This command resubmits workflows that match a specified field selector.  It uses the `--field-selector` flag followed by the field query. This allows for selecting workflows based on their field values, such as namespace.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_resubmit.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nargo archive resubmit --field-selector metadata.namespace=argo\n```\n\n----------------------------------------\n\nTITLE: Define Template-Level Counter Metric in Argo\nDESCRIPTION: This YAML snippet defines a Template-level Counter metric named `result_counter`. It includes a label `name` and emits the metric conditionally when the step fails, as indicated by the `when` clause. The counter increments by 1 each time the condition is met.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  templates:\n    - name: flakey\n      metrics:\n        prometheus:\n          - name: result_counter\n            help: \"Count of step execution by result status\"\n            labels:\n              - key: name\n                value: flakey\n            when: \"{{status}} == Failed\"       # Emit the metric conditionally. Works the same as normal \"when\"\n            counter:\n              value: \"1\"                            # This increments the counter by 1\n      container:\n        image: python:alpine3.6\n        command: [\"python\", -c]\n        # fail with a 66% probability\n        args: [\"import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\"]\n...\n```\n\n----------------------------------------\n\nTITLE: Emissary Executor Configuration\nDESCRIPTION: This YAML snippet shows how to configure the Emissary executor in the workflow controller configmap.  It allows for testing the Emissary executor without making it the default.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\n# Specifies the executor to use.\n#\n# You can use this to:\n# * Tailor your executor based on your preference for security or performance.\n# * Test out an executor without committing yourself to use it for every workflow.\n#\n# To find out which executor was actually use, see the `wait` container logs.\n#\n# The list is in order of precedence; the first matching executor is used.\n# This has precedence over `containerRuntimeExecutor`.\ncontainerRuntimeExecutors: |\n  - name: emissary\n    selector:\n      matchLabels:\n        workflows.argoproj.io/container-runtime-executor: emissary\n```\n\n----------------------------------------\n\nTITLE: Cron Workflow Failure Strategy Configuration\nDESCRIPTION: This YAML snippet shows how to use the `maxFailures` setting inside `RunStrategy` to suspend the CronWorkflow scheduling after a specified number of failed workflow runs. It handles Failed, Errored, or spec error statuses. The workflow is stopped when the number of failures is reached.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/proposals/cron-wf-improvement-proposal.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nRunStrategy:\n maxFailures:\n```\n\n----------------------------------------\n\nTITLE: Cherry-Picking Fixes with Script\nDESCRIPTION: This script helps in cherry-picking commits related to fixes, dependency updates (chore(deps)), build changes, and CI improvements into a release branch. It automates the cherry-picking process and identifies commits that require manual patching. It takes the release branch name (e.g., release-3.3) and a search term (e.g., \"fix\") as input.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/releasing.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./hack/cherry-pick.sh release-3.3 \"fix\"\n./hack/cherry-pick.sh release-3.3 \"chore(deps)\"\n./hack/cherry-pick.sh release-3.3 \"build\"\n./hack/cherry-pick.sh release-3.3 \"ci\"\n```\n\n----------------------------------------\n\nTITLE: Listing Archived Workflows in Chunks\nDESCRIPTION: This example demonstrates listing archived workflows in chunks using the `--chunk-size` flag. This is useful when dealing with a large number of archived workflows to avoid overwhelming the system.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_list.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo archive list --chunk-size 100\n```\n\n----------------------------------------\n\nTITLE: Simple Python Executor Plugin\nDESCRIPTION: This Python script implements a basic Executor Plugin that handles HTTP requests to the `/api/v1/template.execute` endpoint. It reads the authorization token, verifies it, and responds with a success message if the 'hello' plugin is present in the template configuration.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\n\nwith open(\"/var/run/argo/token\") as f:\n    token = f.read().strip()\n\n\nclass Plugin(BaseHTTPRequestHandler):\n\n    def args(self):\n        return json.loads(self.rfile.read(int(self.headers.get('Content-Length'))))\n\n    def reply(self, reply):\n        self.send_response(200)\n        self.end_headers()\n        self.wfile.write(json.dumps(reply).encode(\"UTF-8\"))\n\n    def forbidden(self):\n        self.send_response(403)\n        self.end_headers()\n\n    def unsupported(self):\n        self.send_response(404)\n        self.end_headers()\n\n    def do_POST(self):\n        if self.headers.get(\"Authorization\") != \"Bearer \" + token:\n            self.forbidden()\n        elif self.path == '/api/v1/template.execute':\n            args = self.args()\n            if 'hello' in args['template'].get('plugin', {}):\n                self.reply(\n                    {'node': {'phase': 'Succeeded', 'message': 'Hello template!',\n                              'outputs': {'parameters': [{'name': 'foo', 'value': 'bar'}]}}})\n            else:\n                self.reply({})\n        else:\n            self.unsupported()\n\n\nif __name__ == '__main__':\n    httpd = HTTPServer(('', 4355), Plugin)\n    httpd.serve_forever()\n```\n\n----------------------------------------\n\nTITLE: Argo Cluster Template Inherited Options\nDESCRIPTION: This snippet outlines the options inherited from parent commands. These options configure various aspects of the Argo CLI, such as the API server address, authentication details, and logging level. These options are available to all `argo cluster-template` subcommands.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n      --argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Configuring ResourceRequirements\nDESCRIPTION: This snippet configures ResourceRequirements, which defines the CPU and memory resources that a container requests and limits. It includes claims, limits, and requests. This helps Kubernetes schedule pods based on resource availability and prevents containers from consuming excessive resources.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nResourceRequirements(\n  claims=[\n    ResourceClaim(\n      name=\"name_example\",\n      request=\"request_example\",\n    ),\n  ],\n  limits={\n    \"key\": \"key_example\",\n  },\n  requests={\n    \"key\": \"key_example\",\n  },\n)\n```\n\n----------------------------------------\n\nTITLE: Print Logs of a Pod with Time Filter\nDESCRIPTION: This command prints logs from a specific pod, filtering log entries to only include those newer than a specified duration (e.g., 1 hour).\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_logs.md#_snippet_6\n\nLANGUAGE: CLI\nCODE:\n```\nargo logs --since=1h my-pod\n```\n\n----------------------------------------\n\nTITLE: Listing All Workflows Across All Namespaces with Argo CLI\nDESCRIPTION: Lists all workflows from all namespaces using the `-A` or `--all-namespaces` flag.  This requires appropriate cluster-level permissions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_list.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo list -A\n```\n\n----------------------------------------\n\nTITLE: Filter Nodes by Input Parameter\nDESCRIPTION: Illustrates how to filter Argo Workflow nodes based on the value of an input parameter. This allows users to target nodes where a specific input parameter matches a certain value. It is useful for commands where certain parameters need to be satisfied.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/node-field-selector.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n--node-field-selector=inputs.parameters.foo.value=bar\n```\n\n----------------------------------------\n\nTITLE: Inherited Options for Argo Template\nDESCRIPTION: Shows options inherited from the parent commands. These allow configuring aspects like server address, authentication, TLS settings, namespaces, and logging level.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template_create.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n      --argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Cron Workflow Stop on Failure Configuration\nDESCRIPTION: This YAML snippet configures a CronWorkflow to stop scheduling workflows after two failures by setting `maxFailures` to 2. The workflow stops scheduling if it fails twice.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/proposals/cron-wf-improvement-proposal.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nRunStrategy:\n maxFailures: 2\n```\n\n----------------------------------------\n\nTITLE: Automated Cherry-Picking\nDESCRIPTION: This script automates the cherry-picking process for a given release branch and search term. The 'false' argument likely indicates that it should continue even if some cherry-picks fail. The output should be inspected for errors, and a manual patch might be necessary if cherry-picking fails.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/releasing.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./hack/cherry-pick.sh release-3.3 \"fix\" false\n```\n\n----------------------------------------\n\nTITLE: Configuring ResourceRequirements\nDESCRIPTION: This snippet demonstrates how to configure `ResourceRequirements`, which defines the resources (CPU, memory, etc.) that a container needs. It specifies both `limits` (maximum resources allowed) and `requests` (minimum resources required). It also allows defining `claims`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_39\n\nLANGUAGE: YAML\nCODE:\n```\nResourceRequirements(\n  claims=[\n    ResourceClaim(\n      name=\"name_example\",\n      request=\"request_example\",\n    ),\n  ],\n  limits={\n    \"key\": \"key_example\",\n  },\n  requests={\n    \"key\": \"key_example\",\n  },\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Argo Workflow Templates\nDESCRIPTION: The `argo template list` command is used to display a list of workflow templates. It supports flags for filtering templates by namespace, using selectors, and specifying the output format.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template_list.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nargo template list [flags]\n```\n\n----------------------------------------\n\nTITLE: Tagging a New Release\nDESCRIPTION: This git command creates and pushes a new tag to the release branch. GitHub Actions will then automatically build and publish the release based on this tag. The tag name should correspond to the release version number (e.g., v3.3.4).\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/releasing.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit tag v3.3.4\ngit push upstream v3.3.4 # or origin if you do not use upstream\n```\n\n----------------------------------------\n\nTITLE: Argo Completion Command\nDESCRIPTION: Displays the usage synopsis for the `argo completion` command. It generates shell completion code for bash, zsh, or fish.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_completion.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nargo completion SHELL [flags]\n```\n\n----------------------------------------\n\nTITLE: Get Event Source with Argo Events API in Java\nDESCRIPTION: This Java code snippet demonstrates how to retrieve a specific event source using the Argo Events EventSourceService API. It requires the io.argoproj.workflow dependency and uses API key authentication (BearerToken). The snippet initializes the API client, sets the base path, configures authentication, and calls the `eventSourceServiceGetEventSource` method with the namespace and event source name as parameters. The result, representing the event source, is then printed to the console. An exception block handles potential API errors.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/EventSourceServiceApi.md#_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.EventSourceServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    EventSourceServiceApi apiInstance = new EventSourceServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    try {\n      GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSource result = apiInstance.eventSourceServiceGetEventSource(namespace, name);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling EventSourceServiceApi#eventSourceServiceGetEventSource\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Lint CronWorkflow Example - Java\nDESCRIPTION: This code snippet demonstrates how to lint a CronWorkflow definition using the `cronWorkflowServiceLintCronWorkflow` method of the `CronWorkflowServiceApi`. It requires the namespace and a `IoArgoprojWorkflowV1alpha1LintCronWorkflowRequest` object containing the CronWorkflow definition to be linted. It configures an API key for authentication (BearerToken).\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/CronWorkflowServiceApi.md#_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.CronWorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    CronWorkflowServiceApi apiInstance = new CronWorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    IoArgoprojWorkflowV1alpha1LintCronWorkflowRequest body = new IoArgoprojWorkflowV1alpha1LintCronWorkflowRequest(); // IoArgoprojWorkflowV1alpha1LintCronWorkflowRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1CronWorkflow result = apiInstance.cronWorkflowServiceLintCronWorkflow(namespace, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling CronWorkflowServiceApi#cronWorkflowServiceLintCronWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Set Message on Workflow Node\nDESCRIPTION: Shows how to set the message for a specific node within a workflow using the `--message` flag. The `--node-field-selector` is used to target the correct node.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_node.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo node set my-wf --message \"We did it!\"\" --node-field-selector displayName=approve\n```\n\n----------------------------------------\n\nTITLE: Deleting Archived Workflow in Argo Workflows using Python\nDESCRIPTION: This snippet demonstrates how to delete an archived workflow using the Argo Workflows API. It initializes the API client, configures authentication using an API key (Bearer Token), and calls the `delete_archived_workflow` method.  It handles potential exceptions during the API call and prints the API response or the exception message.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArchivedWorkflowServiceApi.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import archived_workflow_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = archived_workflow_service_api.ArchivedWorkflowServiceApi(api_client)\n    uid = \"uid_example\" # str | \n    namespace = \"namespace_example\" # str |  (optional)\n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.delete_archived_workflow(uid)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArchivedWorkflowServiceApi->delete_archived_workflow: %s\\n\" % e)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.delete_archived_workflow(uid, namespace=namespace)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArchivedWorkflowServiceApi->delete_archived_workflow: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Get Input Artifact Example in Python\nDESCRIPTION: This snippet shows how to retrieve an input artifact using the `get_input_artifact` method. It also uses API key authentication with a Bearer token. It requires `argo_workflows` package. Parameters such as namespace, name, node ID, and artifact name are defined before calling the API.  The response from the API call is then printed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArtifactServiceApi.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import artifact_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = artifact_service_api.ArtifactServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    name = \"name_example\" # str | \n    node_id = \"nodeId_example\" # str | \n    artifact_name = \"artifactName_example\" # str | \n\n    # example passing only required values which don't have defaults set\n    try:\n        # Get an input artifact.\n        api_response = api_instance.get_input_artifact(namespace, name, node_id, artifact_name)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArtifactServiceApi->get_input_artifact: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Listing Cron Workflows with Argo CLI\nDESCRIPTION: This command lists cron workflows using the Argo CLI. It can be used with various flags to filter the results based on namespace, labels, and output format.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_list.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo cron list [flags]\n```\n\n----------------------------------------\n\nTITLE: Retry Workflows by Field Selector - Argo\nDESCRIPTION: This command demonstrates how to retry workflows using a field selector with the `argo archive retry` command. It allows filtering workflows based on field values for targeted re-execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_retry.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nargo archive retry --field-selector metadata.namespace=argo\n```\n\n----------------------------------------\n\nTITLE: Creating Dynamic Marker File Names (Bash)\nDESCRIPTION: This script demonstrates how to create marker file names dynamically using the date and input parameters.  This ensures uniqueness for each task based on the date and provided input number.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/work-avoidance.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ntouch /work/markers/$(date +%Y-%m-%d)-echo-{{inputs.parameters.num}}\n```\n\n----------------------------------------\n\nTITLE: Argo Archive List Label Keys Help Option\nDESCRIPTION: The `-h` or `--help` flag displays help information for the `list-label-keys` command, providing details on available options and usage.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_list-label-keys.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n  -h, --help   help for list-label-keys\n```\n\n----------------------------------------\n\nTITLE: Quantity Serialization Format\nDESCRIPTION: Defines the serialization format for Quantity, which represents numeric values with optional suffixes to indicate units (e.g., bytes, CPU cores). The format supports binary and decimal prefixes (Ki, Mi, Gi, Ti, Pi, Ei, m, k, M, G, T, P, E) and decimal exponents. This representation is designed to avoid floating-point numbers and ensure accurate representation of resource quantities.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_swagger.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n<quantity>        ::= <signedNumber><suffix>\n\n(Note that <suffix> may be empty, from the \"\" case in <decimalSI>.)\n\n<digit>           ::= 0 | 1 | ... | 9\n<digits>          ::= <digit> | <digit><digits>\n<number>          ::= <digits> | <digits>.<digits> | <digits>. | .<digits>\n<sign>            ::= \"+\" | \"-\"\n<signedNumber>    ::= <number> | <sign><number>\n<suffix>          ::= <binarySI> | <decimalExponent> | <decimalSI>\n<binarySI>        ::= Ki | Mi | Gi | Ti | Pi | Ei\n\n(International System of units; See: http://physics.nist.gov/cuu/Units/binary.html)\n\n<decimalSI>       ::= m | \"\" | k | M | G | T | P | E\n\n(Note that 1024 = 1Ki but 1000 = 1k; I didn't choose the capitalization.)\n\n<decimalExponent> ::= \"e\" <signedNumber> | \"E\" <signedNumber>\n```\n\n----------------------------------------\n\nTITLE: Get Argo Server Service Details\nDESCRIPTION: This command retrieves the details of the Argo Server service, including its type, cluster IP, external IP (if available), and exposed ports. Requires kubectl and access to the Kubernetes cluster.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get svc argo-server -n argo\n```\n\n----------------------------------------\n\nTITLE: Workflow Controller Startup Error\nDESCRIPTION: This text snippet shows the error message that the workflow controller throws at startup if it is incorrectly configured.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nFailed to register watch for controller config map: error unmarshaling JSON: while decoding JSON: json: unknown field \"args\"\n```\n\n----------------------------------------\n\nTITLE: Suspending Argo Cron Workflows\nDESCRIPTION: This command suspends one or more specified Argo cron workflows. It accepts the names of the cron workflows as arguments.  It relies on the argo CLI tool being properly configured and connected to the Argo server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_suspend.md#_snippet_0\n\nLANGUAGE: CLI\nCODE:\n```\nargo cron suspend CRON_WORKFLOW... [flags]\n```\n\n----------------------------------------\n\nTITLE: Specify File Extension for Artifact Visualization in Argo Workflows YAML\nDESCRIPTION: This YAML snippet shows how to specify a file extension in the artifact key to enable UI visualization.  The UI checks for supported file extensions in the key to determine if a file can be displayed. Required dependency: Argo Workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/artifact-visualization.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- name: single-file\n  s3:\n    key: visualization.png\n```\n\n----------------------------------------\n\nTITLE: Argo Terminate Synopsis\nDESCRIPTION: Displays the synopsis of the argo terminate command. This command is used to immediately stop a workflow and do not run any exit handlers.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_terminate.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nargo terminate WORKFLOW WORKFLOW2... [flags]\n```\n\n----------------------------------------\n\nTITLE: Watching Sensors with Extensive Parameters in Argo Workflows (Python)\nDESCRIPTION: This code demonstrates how to use `watch_sensors` with a wide range of optional parameters to refine the watch operation. This includes options for label and field selectors, resource versioning, timeouts, and more. Error handling is also included.\n\nRequired dependencies: `argo_workflows`\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = sensor_service_api.SensorServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    list_options_label_selector = \"listOptions.labelSelector_example\" # str | A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. (optional)\n    list_options_field_selector = \"listOptions.fieldSelector_example\" # str | A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. (optional)\n    list_options_watch = True # bool | Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. (optional)\n    list_options_allow_watch_bookmarks = True # bool | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. (optional)\n    list_options_resource_version = \"listOptions.resourceVersion_example\" # str | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_resource_version_match = \"listOptions.resourceVersionMatch_example\" # str | resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_timeout_seconds = \"listOptions.timeoutSeconds_example\" # str | Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. (optional)\n    list_options_limit = \"listOptions.limit_example\" # str | limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. (optional)\n    list_options_continue = \"listOptions.continue_example\" # str | The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. (optional)\n    list_options_send_initial_events = True # bool | `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\\\"io.k8s.initial-events-end\\\": \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"` (for backward compatibility reasons) and to false otherwise. +optional (optional)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.watch_sensors(namespace, list_options_label_selector=list_options_label_selector, list_options_field_selector=list_options_field_selector, list_options_watch=list_options_watch, list_options_allow_watch_bookmarks=list_options_allow_watch_bookmarks, list_options_resource_version=list_options_resource_version, list_options_resource_version_match=list_options_resource_version_match, list_options_timeout_seconds=list_options_timeout_seconds, list_options_limit=list_options_limit, list_options_continue=list_options_continue, list_options_send_initial_events=list_options_send_initial_events)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling SensorServiceApi->watch_sensors: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Argo Inherited Command Options\nDESCRIPTION: These options are inherited from parent commands and affect how the Argo CLI interacts with the Argo server. They configure authentication, namespace, server address, and other settings.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_version.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n      --argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Configuring Pulsar Trigger in Argo Events\nDESCRIPTION: This snippet demonstrates the configuration of a Pulsar trigger within Argo Events. It outlines how to define authentication parameters, including Athenz and token-based authentication using SecretKeySelectors. It also includes connection backoff settings, TLS configurations with SecretKeySelectors for certificates, and topic and URL details. The trigger uses parameters and payloads for message customization.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\npulsar=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1PulsarTrigger(\n                                auth_athenz_params={\n                                    \"key\": \"key_example\",\n                                },\n                                auth_athenz_secret=SecretKeySelector(\n                                    key=\"key_example\",\n                                    name=\"name_example\",\n                                    optional=True,\n                                ),\n                                auth_token_secret=SecretKeySelector(\n                                    key=\"key_example\",\n                                    name=\"name_example\",\n                                    optional=True,\n                                ),\n                                connection_backoff=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Backoff(\n                                    duration=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Int64OrString(\n                                        int64_val=\"int64_val_example\",\n                                        str_val=\"str_val_example\",\n                                        type=\"type_example\",\n                                    ),\n                                    factor=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Amount(\n                                        value='YQ==',\n                                    ),\n                                    jitter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Amount(\n                                        value='YQ==',\n                                    ),\n                                    steps=1,\n                                ),\n                                parameters=[\n                                    GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TriggerParameter(\n                                        dest=\"dest_example\",\n                                        operation=\"operation_example\",\n                                        src=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TriggerParameterSource(\n                                            context_key=\"context_key_example\",\n                                            context_template=\"context_template_example\",\n                                            data_key=\"data_key_example\",\n                                            data_template=\"data_template_example\",\n                                            dependency_name=\"dependency_name_example\",\n                                            use_raw_data=True,\n                                            value=\"value_example\",\n                                        ),\n                                    ),\n                                ],\n                                payload=[\n                                    GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TriggerParameter(\n                                        dest=\"dest_example\",\n                                        operation=\"operation_example\",\n                                        src=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TriggerParameterSource(\n                                            context_key=\"context_key_example\",\n                                            context_template=\"context_template_example\",\n                                            data_key=\"data_key_example\",\n                                            data_template=\"data_template_example\",\n                                            dependency_name=\"dependency_name_example\",\n                                            use_raw_data=True,\n                                            value=\"value_example\",\n                                        ),\n                                    ),\n                                ],\n                                tls=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TLSConfig(\n                                    ca_cert_secret=SecretKeySelector(\n                                        key=\"key_example\",\n                                        name=\"name_example\",\n                                        optional=True,\n                                    ),\n                                    client_cert_secret=SecretKeySelector(\n                                        key=\"key_example\",\n                                        name=\"name_example\",\n                                        optional=True,\n                                    ),\n                                    client_key_secret=SecretKeySelector(\n                                        key=\"key_example\",\n                                        name=\"name_example\",\n                                        optional=True,\n                                    ),\n                                    insecure_skip_verify=True,\n                                ),\n                                tls_allow_insecure_connection=True,\n                                tls_trust_certs_secret=SecretKeySelector(\n                                    key=\"key_example\",\n                                    name=\"name_example\",\n                                    optional=True,\n                                ),\n                                tls_validate_hostname=True,\n                                topic=\"topic_example\",\n                                url=\"url_example\",\n                            )\n```\n\n----------------------------------------\n\nTITLE: List Sensors Example (Python)\nDESCRIPTION: Lists all sensors in a given namespace. Requires API key authentication (BearerToken). The example demonstrates initializing the API client, configuring authentication, and calling the `list_sensors` method with the namespace. The response is then printed using pprint.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import sensor_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.github_com_argoproj_argo_events_pkg_apis_events_v1alpha1_sensor_list import GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SensorList\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n```\n\n----------------------------------------\n\nTITLE: Getting MinIO Credentials\nDESCRIPTION: These commands retrieve the MinIO access key and secret key from a Kubernetes secret. They use `kubectl` and `base64` to decode the credentials, which are required to log in to the MinIO UI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get secret argo-artifacts -o jsonpath='{.data.accesskey}' | base64 --decode\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get secret argo-artifacts -o jsonpath='{.data.secretkey}' | base64 --decode\n```\n\n----------------------------------------\n\nTITLE: Sensor Logs Example (Python)\nDESCRIPTION: This Python code snippet demonstrates how to use the `argo-workflows` library to stream sensor logs from the Argo Workflows API. It configures the API client, sets up API key authentication, and calls the `sensors_logs` method. It requires the `argo_workflows` package and assumes a running Argo Workflows server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import sensor_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.stream_result_of_sensor_log_entry import StreamResultOfSensorLogEntry\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n```\n\n----------------------------------------\n\nTITLE: Listing Archived Workflows with Argo Workflows API in Python\nDESCRIPTION: This Python code snippet demonstrates how to use the Argo Workflows API to list archived workflows. It includes setting up API key authentication, initializing the API client, and calling the `list_archived_workflows` method. The code then prints the returned workflow list.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArchivedWorkflowServiceApi.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import archived_workflow_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_workflow_list import IoArgoprojWorkflowV1alpha1WorkflowList\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n```\n\n----------------------------------------\n\nTITLE: Conditional with Embedded Expression\nDESCRIPTION: Illustrates how to handle parameters with quotes within a `govaluate` conditional expression in Argo Workflows. Uses the `expr` expression syntax to properly evaluate the parameter, mitigating potential issues with quote invalidation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/conditionals.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nwhen: \"{{=inputs.parameters['may-contain-quotes'] == 'example'}}\"\n```\n\n----------------------------------------\n\nTITLE: Trimming String with Sprig Function in Argo\nDESCRIPTION: This example illustrates how to use a Sprig function to trim whitespace from a string. The `sprig.trim` function is used to remove leading and trailing whitespace from the `my-string-param` input parameter.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nsprig.trim(inputs.parameters['my-string-param'])\n```\n\n----------------------------------------\n\nTITLE: Linting a Workflow using Argo Workflows API in Java\nDESCRIPTION: This code snippet demonstrates how to lint a workflow using the Argo Workflows API. It initializes the API client, configures authentication using a Bearer token, and calls the `workflowServiceLintWorkflow` method. The workflow to be linted is passed as a `IoArgoprojWorkflowV1alpha1WorkflowLintRequest` object in the request body. It includes error handling for potential API exceptions and prints the linted workflow details upon success. Requires the `io.argoproj.workflow` library and 'namespace' and 'body' parameters.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowServiceApi.md#_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.WorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    WorkflowServiceApi apiInstance = new WorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    IoArgoprojWorkflowV1alpha1WorkflowLintRequest body = new IoArgoprojWorkflowV1alpha1WorkflowLintRequest(); // IoArgoprojWorkflowV1alpha1WorkflowLintRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1Workflow result = apiInstance.workflowServiceLintWorkflow(namespace, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling WorkflowServiceApi#workflowServiceLintWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Workflow Template with Java\nDESCRIPTION: This Java snippet demonstrates how to create a Workflow Template using the Argo Workflows API. It initializes the API client, configures authentication using a Bearer token, and calls the `workflowTemplateServiceCreateWorkflowTemplate` method with the specified namespace and request body. The example handles potential API exceptions and prints the result or error details.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowTemplateServiceApi.md#_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.WorkflowTemplateServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    WorkflowTemplateServiceApi apiInstance = new WorkflowTemplateServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    IoArgoprojWorkflowV1alpha1WorkflowTemplateCreateRequest body = new IoArgoprojWorkflowV1alpha1WorkflowTemplateCreateRequest(); // IoArgoprojWorkflowV1alpha1WorkflowTemplateCreateRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1WorkflowTemplate result = apiInstance.workflowTemplateServiceCreateWorkflowTemplate(namespace, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling WorkflowTemplateServiceApi#workflowTemplateServiceCreateWorkflowTemplate\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Sensor with Argo Workflows\nDESCRIPTION: This snippet demonstrates how to create a sensor using the SensorServiceApi.create_sensor method. It requires the namespace and the sensor definition (body) as input. It also shows how to handle potential exceptions during the API call.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ntry:\n    api_response = api_instance.create_sensor(namespace, body)\n    pprint(api_response)\nexcept argo_workflows.ApiException as e:\n    print(\"Exception when calling SensorServiceApi->create_sensor: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Listing Sensors with Optional Parameters (Python)\nDESCRIPTION: This code snippet shows how to list sensors with all the available optional parameters. It uses the `list_sensors` method, passing various `list_options` for filtering, watching, and managing the listing process. It also includes error handling for potential `ApiException` errors.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = sensor_service_api.SensorServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    list_options_label_selector = \"listOptions.labelSelector_example\" # str | A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. (optional)\n    list_options_field_selector = \"listOptions.fieldSelector_example\" # str | A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. (optional)\n    list_options_watch = True # bool | Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. (optional)\n    list_options_allow_watch_bookmarks = True # bool | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. (optional)\n    list_options_resource_version = \"listOptions.resourceVersion_example\" # str | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_resource_version_match = \"listOptions.resourceVersionMatch_example\" # str | resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_timeout_seconds = \"listOptions.timeoutSeconds_example\" # str | Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. (optional)\n    list_options_limit = \"listOptions.limit_example\" # str | limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. (optional)\n    list_options_continue = \"listOptions.continue_example\" # str | The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. (optional)\n    list_options_send_initial_events = True # bool | `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\\\"io.k8s.initial-events-end\\\": \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"` (for backward compatibility reasons) and to false otherwise. +optional (optional)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.list_sensors(namespace, list_options_label_selector=list_options_label_selector, list_options_field_selector=list_options_field_selector, list_options_watch=list_options_watch, list_options_allow_watch_bookmarks=list_options_allow_watch_bookmarks, list_options_resource_version=list_options_resource_version, list_options_resource_version_match=list_options_resource_version_match, list_options_timeout_seconds=list_options_timeout_seconds, list_options_limit=list_options_limit, list_options_continue=list_options_continue, list_options_send_initial_events=list_options_send_initial_events)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling SensorServiceApi->list_sensors: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Database Tooling Usage - Console\nDESCRIPTION: This console output displays the usage information for the `go run ./hack/db` CLI, which provides commands for working with the database locally.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_21\n\nLANGUAGE: Console\nCODE:\n```\n$ go run ./hack/db\nCLI for developers to use when working on the DB locally\n\nUsage:\n  db [command]\n\nAvailable Commands:\n  completion              Generate the autocompletion script for the specified shell\n  fake-archived-workflows Insert randomly-generated workflows into argo_archived_workflows, for testing purposes\n  help                    Help about any command\n  migrate                 Force DB migration for given cluster/table\n\nFlags:\n  -c, --dsn string   DSN connection string. For MySQL, use 'mysql:password@tcp/argo'. (default \"postgres://postgres@localhost:5432/postgres\")\n  -h, --help         help for db\n\nUse \"db [command] --help\" for more information about a command.\n```\n\n----------------------------------------\n\nTITLE: Discriminator expression example\nDESCRIPTION: This is an example showing how to access the discriminator from the URL using the expression language. This expression evaluates whether the discriminator is equal to the string 'my-discriminator'.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/events.md#_snippet_9\n\nLANGUAGE: text\nCODE:\n```\ndiscriminator == \"my-discriminator\"\n```\n\n----------------------------------------\n\nTITLE: Submitting Workflow Argo Workflows API (Bash)\nDESCRIPTION: This snippet shows how to submit a workflow to the Argo Workflows API using a POST request. It sends a JSON payload containing the workflow definition to the specified endpoint, targeting the 'argo' namespace.  It requires `curl` to be installed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/rest-examples.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n  --url https://localhost:2746/api/v1/workflows/argo \\\n  --header 'content-type: application/json' \\\n  --data '{ \\\n  \"namespace\": \"argo\", \\\n  \"serverDryRun\": false, \\\n  \"workflow\": { \\\n      \"metadata\": { \\\n        \"generateName\": \"hello-world-\", \\\n        \"namespace\": \"argo\", \\\n        \"labels\": { \\\n          \"workflows.argoproj.io/completed\": \"false\" \\\n         } \\\n      }, \\\n     \"spec\": { \\\n       \"templates\": [ \\\n        { \\\n         \"name\": \"hello-world\", \\\n         \"arguments\": {}, \\\n         \"inputs\": {}, \\\n         \"outputs\": {}, \\\n         \"metadata\": {}, \\\n         \"container\": { \\\n          \"name\": \"\", \\\n          \"image\": \"busybox\", \\\n          \"command\": [ \\\n            \"echo\" \\\n          ], \\\n          \"args\": [ \\\n            \"hello world\" \\\n          ], \\\n          \"resources\": {} \\\n        } \\\n      } \\\n    ], \\\n    \"entrypoint\": \"hello-world\", \\\n    \"arguments\": {} \\\n  } \\\n}'\n```\n\n----------------------------------------\n\nTITLE: Updating Event Source with Python\nDESCRIPTION: This Python code snippet demonstrates how to update an event source using the Argo Workflows API. It uses the `argo_workflows` library to interact with the API, and defines the necessary objects for authentication and request parameters. The `update_event_source` function is called to update the event source in a given namespace.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import event_source_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.eventsource_update_event_source_request import EventsourceUpdateEventSourceRequest\nfrom argo_workflows.model.github_com_argoproj_argo_events_pkg_apis_events_v1alpha1_event_source import GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSource\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n```\n\n----------------------------------------\n\nTITLE: Storage Grid Event Source Configuration\nDESCRIPTION: Sets up a Storage Grid event source for Argo Events. It configures API URL, authentication token using SecretKeySelector, bucket details, event types, and filter for prefix and suffix. The configuration also specifies metadata, region, topic ARN, and webhook context for receiving events.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\nstorage_grid: {\n    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1StorageGridEventSource(\n        api_url=\"api_url_example\",\n        auth_token=SecretKeySelector(\n            key=\"key_example\",\n            name=\"name_example\",\n            optional=True,\n        ),\n        bucket=\"bucket_example\",\n        events=[\n            \"events_example\",\n        ],\n        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1StorageGridFilter(\n            prefix=\"prefix_example\",\n            suffix=\"suffix_example\",\n        ),\n        metadata={\n            \"key\": \"key_example\",\n        },\n        region=\"region_example\",\n        topic_arn=\"topic_arn_example\",\n        webhook=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1WebhookContext(\n            auth_secret=SecretKeySelector(\n                key=\"key_example\",\n                name=\"name_example\",\n                optional=True,\n            ),\n            endpoint=\"endpoint_example\",\n\n```\n\n----------------------------------------\n\nTITLE: Goevaluate When Condition Example\nDESCRIPTION: This YAML snippet demonstrates how to use single quotes to resolve issues between expression template tags and Goevaluate when conditions. The single quotes ensure correct evaluation of the expression.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nwhen: \"'{{inputs.parameters.should-print}}' != '2021-01-01'\"\n```\n\n----------------------------------------\n\nTITLE: Plain List Expression Example\nDESCRIPTION: This code snippet demonstrates a simple expression that defines a plain list. It showcases the basic syntax for creating a list within an expression template tag.  This is typically used in conjunction with other expressions for more complex data manipulation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n[1, 2]\n```\n\n----------------------------------------\n\nTITLE: Watching Workflows with Argo Workflows API in Java\nDESCRIPTION: This code snippet demonstrates how to watch Argo Workflows using the Java API. It sets up the API client with authentication, configures the base path, and then calls the `workflowServiceWatchWorkflows` method to watch for workflow events in a specified namespace. The snippet also handles potential `ApiException` exceptions during the API call, printing details about the error.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowServiceApi.md#_snippet_8\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.WorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    WorkflowServiceApi apiInstance = new WorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String listOptionsLabelSelector = \"listOptionsLabelSelector_example\"; // String | A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional.\n    String listOptionsFieldSelector = \"listOptionsFieldSelector_example\"; // String | A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional.\n    Boolean listOptionsWatch = true; // Boolean | Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional.\n    Boolean listOptionsAllowWatchBookmarks = true; // Boolean | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional.\n    String listOptionsResourceVersion = \"listOptionsResourceVersion_example\"; // String | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    String listOptionsResourceVersionMatch = \"listOptionsResourceVersionMatch_example\"; // String | resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    String listOptionsTimeoutSeconds = \"listOptionsTimeoutSeconds_example\"; // String | Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional.\n    String listOptionsLimit = \"listOptionsLimit_example\"; // String | limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n    String listOptionsContinue = \"listOptionsContinue_example\"; // String | The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n    Boolean listOptionsSendInitialEvents = true; // Boolean | `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\\\"io.k8s.initial-events-end\\\": \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"` (for backward compatibility reasons) and to false otherwise. +optional\n    String fields = \"fields_example\"; // String | \n    try {\n      StreamResultOfIoArgoprojWorkflowV1alpha1WorkflowWatchEvent result = apiInstance.workflowServiceWatchWorkflows(namespace, listOptionsLabelSelector, listOptionsFieldSelector, listOptionsWatch, listOptionsAllowWatchBookmarks, listOptionsResourceVersion, listOptionsResourceVersionMatch, listOptionsTimeoutSeconds, listOptionsLimit, listOptionsContinue, listOptionsSendInitialEvents, fields);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling WorkflowServiceApi#workflowServiceWatchWorkflows\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Development Version of Argo Workflows SDK\nDESCRIPTION: This command installs the latest development version of the Argo Workflows SDK directly from the GitHub repository using pip and git. It specifies the subdirectory where the SDK is located.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/argoproj/argo-workflows@main#subdirectory=sdks/python/client\n```\n\n----------------------------------------\n\nTITLE: Creating Sensor using sensorServiceCreateSensor in Java\nDESCRIPTION: This code snippet demonstrates how to create a sensor using the `sensorServiceCreateSensor` method in Java. It initializes the ApiClient, configures authentication, and calls the `sensorServiceCreateSensor` method with the namespace and sensor request body. It handles potential ApiException during the creation process.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/SensorServiceApi.md#_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.SensorServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    SensorServiceApi apiInstance = new SensorServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    SensorCreateSensorRequest body = new SensorCreateSensorRequest(); // SensorCreateSensorRequest | \n    try {\n      GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Sensor result = apiInstance.sensorServiceCreateSensor(namespace, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling SensorServiceApi#sensorServiceCreateSensor\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Execute Template Endpoint - POST /api/v1/template.execute\nDESCRIPTION: This endpoint executes a template using the executor plugin. It accepts a POST request with the `ExecuteTemplateArgs` in the request body and returns an `ExecuteTemplateReply` on success (HTTP 200).  The endpoint uses the \"application/json\" content type for both requests and responses.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_swagger.md#_snippet_0\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /api/v1/template.execute\n```\n\n----------------------------------------\n\nTITLE: Configuring Workflow Controller ConfigMap without `config` key\nDESCRIPTION: This snippet demonstrates configuring the Workflow Controller ConfigMap without the `config: |` key, a feature available in Argo Workflows version 2.7 and later. In this approach, all nested maps under top-level keys must be defined as strings. This simplifies ConfigMap generation using tools like Kustomize.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-controller-configmap.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# This file describes the config settings available in the workflow controller configmap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workflow-controller-configmap\ndata:                      # \"config: |\" key is optional in 2.7+!\n  instanceID: my-ci-controller\n  artifactRepository: |    # However, all nested maps must be strings\n   archiveLogs: true\n   s3:\n     endpoint: s3.amazonaws.com\n     bucket: my-bucket\n     region: us-west-2\n     insecure: false\n     accessKeySecret:\n       name: my-s3-credentials\n       key: accessKey\n     secretKeySecret:\n       name: my-s3-credentials\n       key: secretKey\n\n```\n\n----------------------------------------\n\nTITLE: Observe Argo Workflow Logs (Bash)\nDESCRIPTION: This snippet displays the logs of the latest workflow run using the Argo CLI. The '@latest' argument is used to specify the most recent workflow. This command is useful for debugging and monitoring workflow execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/quick-start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nargo logs -n argo @latest\n```\n\n----------------------------------------\n\nTITLE: Configure Argo Server Deployment\nDESCRIPTION: This YAML snippet configures the Argo Server Deployment to enable the SSO authentication mode. It adds the `--auth-mode=sso` argument to the argo-server container's arguments, which is essential for activating SSO authentication.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso-argocd.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: argo-server\nspec:\n  template:\n    spec:\n      containers:\n        - name: argo-server\n          args:\n            - server\n            - --auth-mode=sso\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Collector Configuration\nDESCRIPTION: This YAML snippet demonstrates the configuration of an OpenTelemetry collector to receive metrics via the gRPC protocol on port 4317. It defines the `otlp` receiver with `grpc` protocol settings.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4317\n```\n\n----------------------------------------\n\nTITLE: Creating IAM Role and Attaching Policy\nDESCRIPTION: These commands create an IAM role and attach a policy to it. The `$mybucket-role` variable stores the role name, and `policy.json` contains the policy document. This setup is used for granting permissions to access S3 resources.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\naws iam create-role --role-name $mybucket-role\naws iam put-role-policy --role-name $mybucket-user --policy-name $mybucket-policy --policy-document file://policy.json\n```\n\n----------------------------------------\n\nTITLE: Configuring EnvVar with ValueFrom\nDESCRIPTION: This snippet configures an environment variable with a value sourced from a ConfigMap, Secret, or FieldRef. It uses the `value_from` field of the `EnvVar` object to specify the source of the environment variable's value. This enables injecting configuration data into containers without hardcoding values in the image.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nEnvVar(\n  name=\"name_example\",\n  value=\"value_example\",\n  value_from=EnvVarSource(\n    config_map_key_ref=ConfigMapKeySelector(\n      key=\"key_example\",\n      name=\"name_example\",\n      optional=True,\n    ),\n    field_ref=ObjectFieldSelector(\n      api_version=\"api_version_example\",\n      field_path=\"field_path_example\",\n    ),\n    resource_field_ref=ResourceFieldSelector(\n      container_name=\"container_name_example\",\n      divisor=\"divisor_example\",\n      resource=\"resource_example\",\n    ),\n    secret_key_ref=SecretKeySelector(\n      key=\"key_example\",\n      name=\"name_example\",\n      optional=True,\n    ),\n  ),\n)\n```\n\n----------------------------------------\n\nTITLE: Retry Archived Workflow in Argo Workflows (Python)\nDESCRIPTION: This code snippet demonstrates how to retry an archived workflow using the Argo Workflows API and the Python client library. It includes setting up API key authentication, creating an instance of the ArchivedWorkflowServiceApi, and calling the retry_archived_workflow method with the required parameters (uid and body). It also demonstrates exception handling for API calls.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArchivedWorkflowServiceApi.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import archived_workflow_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_workflow import IoArgoprojWorkflowV1alpha1Workflow\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_retry_archived_workflow_request import IoArgoprojWorkflowV1alpha1RetryArchivedWorkflowRequest\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = archived_workflow_service_api.ArchivedWorkflowServiceApi(api_client)\n    uid = \"uid_example\" # str | \n    body = IoArgoprojWorkflowV1alpha1RetryArchivedWorkflowRequest(\n        name=\"name_example\",\n        namespace=\"namespace_example\",\n        node_field_selector=\"node_field_selector_example\",\n        parameters=[\n            \"parameters_example\",\n        ],\n        restart_successful=True,\n        uid=\"uid_example\",\n    ) # IoArgoprojWorkflowV1alpha1RetryArchivedWorkflowRequest | \n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.retry_archived_workflow(uid, body)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArchivedWorkflowServiceApi->retry_archived_workflow: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Build Argo Executor Image for Specific Platforms - Bash\nDESCRIPTION: This bash command builds the Argo Executor image for specific platforms, such as arm64 and amd64. It uses the `TARGET_PLATFORM` environment variable to specify the target architectures.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\n# compile for both arm64 and amd64\nmake argoexec-image TARGET_PLATFORM=linux/arm64,linux/amd64\n```\n\n----------------------------------------\n\nTITLE: Creating Event Source with eventSourceServiceCreateEventSource in Java\nDESCRIPTION: This code snippet demonstrates how to create a new event source using the `eventSourceServiceCreateEventSource` method of the `EventSourceServiceApi`. It configures an API client, sets up authentication, and then calls the method with the namespace and event source creation request as parameters. It prints the result or any exception encountered.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/EventSourceServiceApi.md#_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.EventSourceServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    EventSourceServiceApi apiInstance = new EventSourceServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    EventsourceCreateEventSourceRequest body = new EventsourceCreateEventSourceRequest(); // EventsourceCreateEventSourceRequest | \n    try {\n      GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSource result = apiInstance.eventSourceServiceCreateEventSource(namespace, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling EventSourceServiceApi#eventSourceServiceCreateEventSource\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Listing Cluster Workflow Template Names Only\nDESCRIPTION: This command lists only the names of the cluster workflow templates. It is useful for scripting or when only the names are required.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_list.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nargo cluster-template list -o name\n```\n\n----------------------------------------\n\nTITLE: Delete CronWorkflow using Argo Workflows API in Java\nDESCRIPTION: This snippet demonstrates how to delete a CronWorkflow using the Argo Workflows CronWorkflowServiceApi in Java. It initializes the ApiClient, configures authentication, and calls the cronWorkflowServiceDeleteCronWorkflow method with the namespace, name, and various deletion options. The example includes error handling for API exceptions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/CronWorkflowServiceApi.md#_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.CronWorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    CronWorkflowServiceApi apiInstance = new CronWorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    String deleteOptionsGracePeriodSeconds = \"deleteOptionsGracePeriodSeconds_example\"; // String | The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional.\n    String deleteOptionsPreconditionsUid = \"deleteOptionsPreconditionsUid_example\"; // String | Specifies the target UID. +optional.\n    String deleteOptionsPreconditionsResourceVersion = \"deleteOptionsPreconditionsResourceVersion_example\"; // String | Specifies the target ResourceVersion +optional.\n    Boolean deleteOptionsOrphanDependents = true; // Boolean | Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional.\n    String deleteOptionsPropagationPolicy = \"deleteOptionsPropagationPolicy_example\"; // String | Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional.\n    List<String> deleteOptionsDryRun = Arrays.asList(); // List<String> | When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional +listType=atomic.\n    Boolean deleteOptionsIgnoreStoreReadErrorWithClusterBreakingPotential = true; // Boolean | if set to true, it will trigger an unsafe deletion of the resource in case the normal deletion flow fails with a corrupt object error. A resource is considered corrupt if it can not be retrieved from the underlying storage successfully because of a) its data can not be transformed e.g. decryption failure, or b) it fails to decode into an object. NOTE: unsafe deletion ignores finalizer constraints, skips precondition checks, and removes the object from the storage. WARNING: This may potentially break the cluster if the workload associated with the resource being unsafe-deleted relies on normal deletion flow. Use only if you REALLY know what you are doing. The default value is false, and the user must opt in to enable it +optional.\n    try {\n      Object result = apiInstance.cronWorkflowServiceDeleteCronWorkflow(namespace, name, deleteOptionsGracePeriodSeconds, deleteOptionsPreconditionsUid, deleteOptionsPreconditionsResourceVersion, deleteOptionsOrphanDependents, deleteOptionsPropagationPolicy, deleteOptionsDryRun, deleteOptionsIgnoreStoreReadErrorWithClusterBreakingPotential);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling CronWorkflowServiceApi#cronWorkflowServiceDeleteCronWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Define Read-Only Role for Argo UI Access in YAML\nDESCRIPTION: Defines a Kubernetes Role using YAML to grant read-only access to the Argo UI. It includes permissions for standard Kubernetes APIs (events, pods, pods/log) and Argo APIs (eventsources, sensors, workflows, etc.) with `get`, `list`, and `watch` verbs.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/security.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: ui-user-read-only\nrules:\n  # k8s standard APIs\n  - apiGroups:\n      - \"\"\n    resources:\n      - events\n      - pods\n      - pods/log\n    verbs:\n      - get\n      - list\n      - watch\n  # Argo APIs. See also https://github.com/argoproj/argo-workflows/blob/main/manifests/cluster-install/workflow-controller-rbac/workflow-aggregate-roles.yaml#L4\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - eventsources\n      - sensors\n      - workflows\n      - workfloweventbindings\n      - workflowtemplates\n      - clusterworkflowtemplates\n      - cronworkflows\n      - workflowtaskresults\n    verbs:\n      - get\n      - list\n      - watch\n```\n\n----------------------------------------\n\nTITLE: Submitting Workflow using Java\nDESCRIPTION: This code snippet demonstrates how to submit a workflow using the Argo Workflow Service API in Java. It requires the `io.argoproj.workflow` dependency and configures API key authorization using a Bearer token. The snippet defines a namespace and a workflow submit request body and invokes the `workflowServiceSubmitWorkflow` method.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowServiceApi.md#_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.WorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    WorkflowServiceApi apiInstance = new WorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    IoArgoprojWorkflowV1alpha1WorkflowSubmitRequest body = new IoArgoprojWorkflowV1alpha1WorkflowSubmitRequest(); // IoArgoprojWorkflowV1alpha1WorkflowSubmitRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1Workflow result = apiInstance.workflowServiceSubmitWorkflow(namespace, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling WorkflowServiceApi#workflowServiceSubmitWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Get Artifact File Example in Python\nDESCRIPTION: This snippet demonstrates how to retrieve an artifact file using the `get_artifact_file` method from the Argo Workflows Artifact Service API. It requires the `argo_workflows` package and sets up API key authentication with a Bearer token. The snippet defines parameters for the API call, such as namespace, ID discriminator, ID, node ID, and artifact name, and then calls the API and prints the response.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArtifactServiceApi.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import artifact_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = artifact_service_api.ArtifactServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    id_discriminator = \"workflow\" # str | \n    id = \"id_example\" # str | \n    node_id = \"nodeId_example\" # str | \n    artifact_name = \"artifactName_example\" # str | \n\n    # example passing only required values which don't have defaults set\n    try:\n        # Get an artifact.\n        api_response = api_instance.get_artifact_file(namespace, id_discriminator, id, node_id, artifact_name, )\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArtifactServiceApi->get_artifact_file: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Argo Submit Workflow Command (CLI)\nDESCRIPTION: This command uses the Argo CLI to submit a Workflow from a remote YAML file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nargo submit https://raw.githubusercontent.com/argoproj/argo-workflows/main/examples/workflow-template/hello-world.yaml\n```\n\n----------------------------------------\n\nTITLE: Resuming a Workflow with Argo CLI\nDESCRIPTION: This command resumes a suspended Argo Workflow, allowing it to continue execution from the point of suspension.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/suspending.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo resume WORKFLOW\n```\n\n----------------------------------------\n\nTITLE: Installing Argo Workflows SDK\nDESCRIPTION: This command installs the official releases of the Argo Workflows SDK from PyPI using pip.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install argo-workflows\n```\n\n----------------------------------------\n\nTITLE: Lint Manifests in Directory - Argo\nDESCRIPTION: This command validates all Kubernetes manifests within the specified directory using the `argo lint` tool.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_lint.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nargo lint ./manifests\n```\n\n----------------------------------------\n\nTITLE: Get Version with InfoService Java\nDESCRIPTION: This snippet shows how to retrieve the version of Argo Workflows using the `infoServiceGetVersion` method.  It initializes the API client, sets up API key authentication, and then calls the method.  Error handling is included to catch and display any API exceptions that may occur.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/InfoServiceApi.md#_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.InfoServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    InfoServiceApi apiInstance = new InfoServiceApi(defaultClient);\n    try {\n      IoArgoprojWorkflowV1alpha1Version result = apiInstance.infoServiceGetVersion();\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling InfoServiceApi#infoServiceGetVersion\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: CronWorkflow with Success-Based Stop Strategy\nDESCRIPTION: Defines a `CronWorkflow` that automatically stops scheduling new workflows after one successful execution. It leverages the `stopStrategy.expression` to check the `cronworkflow.succeeded` variable.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cron-workflows.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nstopStrategy:\n  expression: \"cronworkflow.succeeded >= 1\"\n```\n\n----------------------------------------\n\nTITLE: Disable TLS (Plain Text) - Bash\nDESCRIPTION: This snippet demonstrates how to start the Argo Server without TLS encryption, sending all data in plain text.  This is recommended for development environments only. It involves setting the `ARGO_SECURE` environment variable to `false` and using the `--secure=false` flag when starting the Argo Server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/tls.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport ARGO_SECURE=false\nargo server --secure=false\n```\n\n----------------------------------------\n\nTITLE: Getting a Workflow using Argo Workflows API in Java\nDESCRIPTION: This code snippet demonstrates how to retrieve a specific workflow from the Argo Workflows API using its name and namespace. It initializes the API client, configures authentication, and then calls the `workflowServiceGetWorkflow` method. The example includes error handling for potential API exceptions and prints the workflow details upon success.  It requires the `io.argoproj.workflow` library and the 'namespace' and 'name' parameters.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowServiceApi.md#_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.WorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    WorkflowServiceApi apiInstance = new WorkflowServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    String getOptionsResourceVersion = \"getOptionsResourceVersion_example\"; // String | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    String fields = \"fields_example\"; // String | Fields to be included or excluded in the response. e.g. \\\"spec,status.phase\\\", \\\"-status.nodes\\\".\n    try {\n      IoArgoprojWorkflowV1alpha1Workflow result = apiInstance.workflowServiceGetWorkflow(namespace, name, getOptionsResourceVersion, fields);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling WorkflowServiceApi#workflowServiceGetWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Creating Workflow from ClusterWorkflowTemplate in YAML\nDESCRIPTION: This YAML creates a Workflow from the `cluster-workflow-template-submittable` ClusterWorkflowTemplate, overriding the default message argument. The `entrypoint` is specified and the `arguments` will override the default arguments defined in `ClusterWorkflowTemplate`. Dependencies: Argo Workflows, `cluster-workflow-template-submittable` ClusterWorkflowTemplate must exist.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cluster-workflow-templates.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: cluster-workflow-template-hello-world-\nspec:\n  entrypoint: print-message\n  arguments:\n    parameters:\n      - name: message\n        value: \"from workflow\"\n  workflowTemplateRef:\n    name: cluster-workflow-template-submittable\n    clusterScope: true\n```\n\n----------------------------------------\n\nTITLE: Listing Cluster Workflow Templates with Details\nDESCRIPTION: This command lists cluster workflow templates with additional details like labels, annotations and status. This provides a comprehensive view of each template's configuration and status.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_list.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nargo cluster-template list --output wide\n```\n\n----------------------------------------\n\nTITLE: Finding Workflows Using Deprecated templateRef\nDESCRIPTION: This bash command uses kubectl to search for workflows across all namespaces that are using the deprecated `templateRef` field. The output is in YAML format and piped to grep to filter for the specified field. This identifies workflows that need to be updated to use steps or DAG.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_17\n\nLANGUAGE: Bash\nCODE:\n```\nkubectl get wf --all-namespaces -o yaml | grep templateRef\n```\n\n----------------------------------------\n\nTITLE: Resubmitting and Watching for Completion with Argo\nDESCRIPTION: Resubmits a specified Argo Workflow and watches its progress until completion. Only works when a single workflow is resubmitted.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resubmit.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nargo resubmit --watch my-wf.yaml\n```\n\n----------------------------------------\n\nTITLE: Referencing ClusterWorkflowTemplates in YAML\nDESCRIPTION: This YAML snippet demonstrates how to reference a template from another ClusterWorkflowTemplate using `templateRef` with `clusterScope: true`. It creates a Workflow that calls the `print-message` template from the `cluster-workflow-template-print-message` ClusterWorkflowTemplate. Dependencies: Argo Workflows, `cluster-workflow-template-print-message` ClusterWorkflowTemplate must exist.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cluster-workflow-templates.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: workflow-template-hello-world-\nspec:\n  entrypoint: hello-world\n  templates:\n  - name: hello-world\n    steps:                              # You should only reference external \"templates\" in a \"steps\" or \"dag\" \"template\".\n      - - name: call-print-message\n          templateRef:                  # You can reference a \"template\" from another \"WorkflowTemplate or ClusterWorkflowTemplate\" using this field\n            name: cluster-workflow-template-print-message   # This is the name of the \"WorkflowTemplate or ClusterWorkflowTemplate\" CRD that contains the \"template\" you want\n            template: print-message     # This is the name of the \"template\" you want to reference\n            clusterScope: true          # This field indicates this templateRef is pointing ClusterWorkflowTemplate\n          arguments:                    # You can pass in arguments as normal\n            parameters:\n            - name: message\n              value: \"hello world\"\n```\n\n----------------------------------------\n\nTITLE: Argo Template Help Option\nDESCRIPTION: Displays the help message for the `argo template` command, listing available options and subcommands.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n-h, --help   help for template\n```\n\n----------------------------------------\n\nTITLE: Argo Cluster-Template Lint Command\nDESCRIPTION: The `argo cluster-template lint` command validates cluster workflow template manifest files or directories. It accepts a list of file paths as input and supports various flags for customization, such as specifying the output format or enabling strict validation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_lint.md#_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nargo cluster-template lint FILE... [flags]\n```\n\n----------------------------------------\n\nTITLE: Start Argo with MySQL Profile - Bash\nDESCRIPTION: This bash command starts Argo with the MySQL profile for workflow archive testing. It requires a MySQL database running on <http://localhost:3306>.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\nmake start PROFILE=mysql\n```\n\n----------------------------------------\n\nTITLE: Submit Workflow from YAML\nDESCRIPTION: This Python snippet demonstrates submitting an Argo Workflow from a raw YAML manifest. It fetches the YAML from a URL, loads it using the `yaml` library, and then creates a workflow using the Argo Workflows API.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint\n\nimport requests\nimport yaml\n\nimport argo_workflows\nfrom argo_workflows.api import workflow_service_api\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_workflow_create_request import (\n    IoArgoprojWorkflowV1alpha1WorkflowCreateRequest,\n)\n\nconfiguration = argo_workflows.Configuration(host=\"https://127.0.0.1:2746\")\nconfiguration.verify_ssl = False\n\nresp = requests.get('https://raw.githubusercontent.com/argoproj/argo-workflows/main/examples/hello-world.yaml')\nmanifest = yaml.safe_load(resp.text)\n\napi_client = argo_workflows.ApiClient(configuration)\napi_instance = workflow_service_api.WorkflowServiceApi(api_client)\napi_response = api_instance.create_workflow(\n    namespace=\"argo\",\n    body=IoArgoprojWorkflowV1alpha1WorkflowCreateRequest(workflow=manifest, _check_type=False),\n    _check_return_type=False)\npprint(api_response)\n\n```\n\n----------------------------------------\n\nTITLE: Slack Event Source Configuration\nDESCRIPTION: Configures a Slack event source for Argo Events. This configuration includes defining the signing secret and token using SecretKeySelector for secure access. It also defines a WebhookContext specifying the endpoint, method, and port for receiving Slack events, along with optional authentication secrets.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nslack: {\n    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SlackEventSource(\n        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceFilter(\n            expression=\"expression_example\",\n        ),\n        metadata={\n            \"key\": \"key_example\",\n        },\n        signing_secret=SecretKeySelector(\n            key=\"key_example\",\n            name=\"name_example\",\n            optional=True,\n        ),\n        token=SecretKeySelector(\n            key=\"key_example\",\n            name=\"name_example\",\n            optional=True,\n        ),\n        webhook=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1WebhookContext(\n            auth_secret=SecretKeySelector(\n                key=\"key_example\",\n                name=\"name_example\",\n                optional=True,\n            ),\n            endpoint=\"endpoint_example\",\n            max_payload_size=\"max_payload_size_example\",\n            metadata={\n                \"key\": \"key_example\",\n            },\n            method=\"method_example\",\n            port=\"port_example\",\n            server_cert_secret=SecretKeySelector(\n                key=\"key_example\",\n                name=\"name_example\",\n                optional=True,\n            ),\n            server_key_secret=SecretKeySelector(\n                key=\"key_example\",\n                name=\"name_example\",\n                optional=True,\n            ),\n            url=\"url_example\",\n        ),\n    ),\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SecurityContext\nDESCRIPTION: This snippet configures a SecurityContext, which defines the security settings for a container, including capabilities, privileges, and user/group IDs. It includes settings such as allowPrivilegeEscalation, capabilities (add and drop), privileged, readOnlyRootFilesystem, runAsGroup, runAsNonRoot and runAsUser. This enables hardening containers and restricting their access to system resources.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nSecurityContext(\n  allow_privilege_escalation=True,\n  app_armor_profile=AppArmorProfile(\n    localhost_profile=\"localhost_profile_example\",\n    type=\"type_example\",\n  ),\n  capabilities=Capabilities(\n    add=[\n      \"add_example\",\n    ],\n    drop=[\n      \"drop_example\",\n    ],\n  ),\n  privileged=True,\n  proc_mount=\"proc_mount_example\",\n  read_only_root_filesystem=True,\n  run_as_group=1,\n  run_as_non_root=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Cluster Workflow Template with YAML Output\nDESCRIPTION: This command creates a cluster workflow template and outputs it in YAML format. The `--output yaml` flag specifies that the output should be formatted as YAML.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_create.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo cluster-template create FILE1 --output yaml\n```\n\n----------------------------------------\n\nTITLE: Argo Cron Lint Inherited Options\nDESCRIPTION: These options are inherited from parent commands and configure various aspects of the Argo CLI, such as server address, authentication, and logging.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_lint.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n      --argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Argo Workflow Definition with Custom Variables (YAML)\nDESCRIPTION: This YAML defines an Argo workflow with a main template `hello-hello-hello` and a sub-template `print-message`. The `print-message` template uses a custom variable `{{user.username}}` in the container's arguments, which is expected to be resolved by Argo using a user-defined context or Jinja templating.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/custom-template-variable-reference.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: custom-template-variable-\nspec:\n  entrypoint: hello-hello-hello\n\n  templates:\n    - name: hello-hello-hello\n      steps:\n        - - name: hello1\n            template: print-message\n            arguments:\n              parameters: [{name: message, value: \"hello1\"}]\n        - - name: hello2a\n            template: print-message\n            arguments:\n              parameters: [{name: message, value: \"hello2a\"}]\n          - name: hello2b\n            template: print-message\n            arguments:\n              parameters: [{name: message, value: \"hello2b\"}]\n\n    - name: print-message\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: busybox\n        command: [echo]\n        args: [\"{{user.username}}\"]\n```\n\n----------------------------------------\n\nTITLE: Adding Dynamic Annotations to Workflow Template (YAML)\nDESCRIPTION: This snippet demonstrates how to dynamically set annotation values based on input parameters in an Argo Workflow template. The `annotations` field uses the `{{inputs.parameters.display-name}}` expression to reference the value of the `display-name` parameter. This allows you to override the annotation value when submitting the workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/annotations.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: templated-annotations-workflow\nspec:\n  entrypoint: whalesay\n  arguments:\n    parameters:\n      - name: display-name\n        value: \"default-display-name\"\n  templates:\n  - name: whalesay\n    annotations:\n      workflows.argoproj.io/display-name: \"{{inputs.parameters.display-name}}\"\n    inputs:\n      parameters:\n        - name: display-name\n    container:\n      image: docker/whalesay\n      command: [cowsay]\n      args: [\"hello world\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Workflow Controller ConfigMap with `config` key\nDESCRIPTION: This snippet shows how to configure the Workflow Controller ConfigMap using the `config: |` key. It defines settings such as the instance ID and artifact repository, which includes S3 bucket details and credentials. The `config: |` approach is valid in all versions of Argo Workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-controller-configmap.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# This file describes the config settings available in the workflow controller configmap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workflow-controller-configmap\ndata:\n  config: |\n    instanceID: my-ci-controller\n    artifactRepository:\n      archiveLogs: true\n      s3:\n        endpoint: s3.amazonaws.com\n        bucket: my-bucket\n        region: us-west-2\n        insecure: false\n        accessKeySecret:\n          name: my-s3-credentials\n          key: accessKey\n        secretKeySecret:\n          name: my-s3-credentials\n          key: secretKey\n\n```\n\n----------------------------------------\n\nTITLE: Cron Backfill Options\nDESCRIPTION: These options allow the user to customize the cron workflow backfill. The `--argname` specifies the argument name for the schedule time. The `--start` and `--end` dates define the range for the backfill. The `--maxworkflowcount` limits the number of generated workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_backfill.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n      --argname string         Schedule time argument name for workflow (default \"cronScheduleTime\")\n      --end string             End Date\n      --format string          Date format for Schedule time value (default \"Mon, 02 Jan 2006 15:04:05 MST\")\n  -h, --help                   help for backfill\n      --maxworkflowcount int   Maximum number of generated backfill workflows (default 1000)\n      --name string            Backfill name\n      --parallel               Enabled all backfile workflows run parallel\n      --start string           Start date\n```\n\n----------------------------------------\n\nTITLE: Maven Dependency: Argo Client Java (Latest Snapshot)\nDESCRIPTION: This XML snippet configures a Maven dependency for the latest snapshot version of the Argo Client Java library. It specifies the groupId, artifactId, and a snapshot version (0.0.0-SNAPSHOT). Use with caution as snapshot versions may be unstable.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/README.md#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n    <groupId>io.argoproj.workflow</groupId>\n    <artifactId>argo-client-java</artifactId>\n    <version>0.0.0-SNAPSHOT</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Getting Archived Workflow using Argo Workflows API in Java\nDESCRIPTION: This code snippet demonstrates how to retrieve an archived workflow using the Argo Workflows API. It initializes the ApiClient, configures authentication with an API key, and then calls the archivedWorkflowServiceGetArchivedWorkflow method. It requires the uid, namespace, and name of the workflow to be retrieved. The method returns an IoArgoprojWorkflowV1alpha1Workflow object, and any ApiException is caught and its details are printed to the console.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ArchivedWorkflowServiceApi.md#_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ArchivedWorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ArchivedWorkflowServiceApi apiInstance = new ArchivedWorkflowServiceApi(defaultClient);\n    String uid = \"uid_example\"; // String | \n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    try {\n      IoArgoprojWorkflowV1alpha1Workflow result = apiInstance.archivedWorkflowServiceGetArchivedWorkflow(uid, namespace, name);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ArchivedWorkflowServiceApi#archivedWorkflowServiceGetArchivedWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Submitting event with discriminator\nDESCRIPTION: This example demonstrates how to submit an event to the Argo Server's event endpoint with a discriminator. The discriminator allows for filtering events based on a specific value within the URL. It includes the necessary authorization header and a JSON payload.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/events.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://localhost:2746/api/v1/events/argo/my-discriminator \\\n  -H \"Authorization: $ARGO_TOKEN\" \\\n  -d '{\"message\": \"hello\"}'\n```\n\n----------------------------------------\n\nTITLE: Debug Workflow Container with Ephemeral Container (Bash)\nDESCRIPTION: Creates an ephemeral container within the pod running the workflow step.  This allows debugging operations without modifying the original container. `--share-processes` is important to access volumes.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/debug-pause.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl debug -n argo -it POD_NAME --image=busybox --target=main --share-processes\n```\n\n----------------------------------------\n\nTITLE: List CronWorkflows using Argo CLI\nDESCRIPTION: Shows how to list existing `CronWorkflows` using the Argo CLI. The output includes the name, age, last run time, schedules, and suspension status of each CronWorkflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cron-workflows.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ argo cron list\nNAME           AGE   LAST RUN   SCHEDULES    SUSPENDED\ntest-cron-wf   49s   N/A        * * * * *   false\n```\n\n----------------------------------------\n\nTITLE: Argo Cluster Template Help Options\nDESCRIPTION: This snippet describes the available options for the `argo cluster-template` command. The `-h` or `--help` flag displays help information about the command and its subcommands. It doesn't require any external dependencies.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n  -h, --help   help for cluster-template\n```\n\n----------------------------------------\n\nTITLE: Inherited Argo Command Options\nDESCRIPTION: These are options inherited from parent `argo` commands, providing configurations for Argo Server connections, authentication, Kubernetes context, and logging levels. They allow fine-tuning of how the `argo` CLI interacts with the Argo Workflows server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_backfill.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n      --argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Argo Cron Lint Options\nDESCRIPTION: These are the available options for the 'argo cron lint' command, including help, output format, and strict validation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_lint.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n  -h, --help            help for lint\n  -o, --output string   Linting results output format. One of: pretty|simple (default \"pretty\")\n      --strict          perform strict validation (default true)\n```\n\n----------------------------------------\n\nTITLE: Argo Executor Plugin Build Command Syntax\nDESCRIPTION: This is the basic command syntax for building an Argo executor plugin.  `DIR` specifies the directory containing the plugin's source code.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_executor-plugin_build.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo executor-plugin build DIR [flags]\n```\n\n----------------------------------------\n\nTITLE: Defining a DAG Template in Argo\nDESCRIPTION: This YAML snippet defines a 'diamond' template of type 'dag'. It specifies a directed acyclic graph of tasks with dependencies. Task 'A' runs first, then 'B' and 'C' run in parallel, and finally 'D' runs after both 'B' and 'C' are complete. Each task uses a template named 'echo'.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-concepts.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n  - name: diamond\n    dag:\n      tasks:\n      - name: A\n        template: echo\n      - name: B\n        dependencies: [A]\n        template: echo\n      - name: C\n        dependencies: [A]\n        template: echo\n      - name: D\n        dependencies: [B, C]\n        template: echo\n```\n\n----------------------------------------\n\nTITLE: Creating an Event Source with Argo Workflows API in Python\nDESCRIPTION: This code snippet demonstrates how to create an event source within a specified namespace using the Argo Workflows API. It includes exception handling for potential API errors during the event source creation process. It assumes that `api_instance`, `namespace`, and `body` are pre-defined with the API client, namespace name, and event source definition respectively.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ntry:\n    api_response = api_instance.create_event_source(namespace, body)\n    pprint(api_response)\nexcept argo_workflows.ApiException as e:\n    print(\"Exception when calling EventsourceServiceApi->create_event_source: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Submitting and Watching Hybrid Workflow - Argo CLI\nDESCRIPTION: These commands submit the 'hello-hybrid.yaml' workflow to Argo and watches the logs for the executed workflow. This workflow will run parts on a Windows node and parts on a Linux node.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/windows.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ argo submit --watch https://raw.githubusercontent.com/argoproj/argo-workflows/main/examples/hello-hybrid.yaml\n$ argo logs hello-hybrid-plqpp\nhello-hybrid-plqpp-1977432187: \"Hello from Windows Container!\"\nhello-hybrid-plqpp-764774907: Hello from Linux Container!\n```\n\n----------------------------------------\n\nTITLE: Retrieving Info using get_info in Python\nDESCRIPTION: This code snippet shows how to use the `get_info` method of the `InfoServiceApi` to retrieve information from Argo Workflows. It requires the `argo_workflows` package and uses API key authentication. This endpoint does not take any parameters.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/InfoServiceApi.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import info_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_info_response import IoArgoprojWorkflowV1alpha1InfoResponse\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = info_service_api.InfoServiceApi(api_client)\n\n    # example, this endpoint has no required or optional parameters\n    try:\n        api_response = api_instance.get_info()\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling InfoServiceApi->get_info: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Configuring LabelSelectorRequirement\nDESCRIPTION: This snippet demonstrates how to define a `LabelSelectorRequirement` used within Kubernetes resource selectors.  It specifies a key, an operator, and a list of values for filtering resources based on labels.  This is commonly used in node or pod affinity/anti-affinity rules.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_32\n\nLANGUAGE: YAML\nCODE:\n```\nLabelSelectorRequirement(\n  key=\"key_example\",\n  operator=\"operator_example\",\n  values=[\n    \"values_example\",\n  ],\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Sensors using Argo Workflow API in Java\nDESCRIPTION: This Java snippet demonstrates how to list sensors using the Argo Workflows SensorServiceApi. It configures an ApiClient, sets the base path, configures API key authentication (BearerToken), and then calls the sensorServiceListSensors method with various list options.  It handles potential ApiExceptions by printing the error details to the console.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/SensorServiceApi.md#_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.SensorServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    SensorServiceApi apiInstance = new SensorServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String listOptionsLabelSelector = \"listOptionsLabelSelector_example\"; // String | A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional.\n    String listOptionsFieldSelector = \"listOptionsFieldSelector_example\"; // String | A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional.\n    Boolean listOptionsWatch = true; // Boolean | Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional.\n    Boolean listOptionsAllowWatchBookmarks = true; // Boolean | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional.\n    String listOptionsResourceVersion = \"listOptionsResourceVersion_example\"; // String | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    String listOptionsResourceVersionMatch = \"listOptionsResourceVersionMatch_example\"; // String | resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    String listOptionsTimeoutSeconds = \"listOptionsTimeoutSeconds_example\"; // String | Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional.\n    String listOptionsLimit = \"listOptionsLimit_example\"; // String | limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n    String listOptionsContinue = \"listOptionsContinue_example\"; // String | The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n    Boolean listOptionsSendInitialEvents = true; // Boolean | `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\\\"io.k8s.initial-events-end\\\": \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"` (for backward compatibility reasons) and to false otherwise. +optional\n    try {\n      GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SensorList result = apiInstance.sensorServiceListSensors(namespace, listOptionsLabelSelector, listOptionsFieldSelector, listOptionsWatch, listOptionsAllowWatchBookmarks, listOptionsResourceVersion, listOptionsResourceVersionMatch, listOptionsTimeoutSeconds, listOptionsLimit, listOptionsContinue, listOptionsSendInitialEvents);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling SensorServiceApi#sensorServiceListSensors\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding Swagger UI CSS\nDESCRIPTION: This snippet links the Swagger UI stylesheet from a CDN to style the API documentation interface. It is a necessary dependency for rendering the Swagger UI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/swagger.md#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<link rel=\"stylesheet\" href=\"https://unpkg.com/swagger-ui-dist@5.11.0/swagger-ui.css\" />\n```\n\n----------------------------------------\n\nTITLE: Example Resource Duration Calculation\nDESCRIPTION: This example demonstrates how resource duration is calculated for a pod running for 3 minutes with CPU, memory, and GPU limits. The calculation involves dividing the resource limit by the base amount and multiplying by the runtime.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/resource-duration.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCPU:    3min * 2000m / 1000m = 6min * (1 cpu)\nMemory: 3min * 1Gi / 100Mi   = 30min * (100Mi memory)\nGPU:    3min * 1     / 1     = 3min * (1 nvidia.com/gpu)\n```\n\n----------------------------------------\n\nTITLE: Deleting ClusterWorkflowTemplate in Java\nDESCRIPTION: This snippet shows how to delete a ClusterWorkflowTemplate using the Argo Workflows API.  It requires the name of the template to be deleted.  It initializes the API client, configures authentication, and calls the `clusterWorkflowTemplateServiceDeleteClusterWorkflowTemplate` method with the required parameters.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ClusterWorkflowTemplateServiceApi.md#_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ClusterWorkflowTemplateServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ClusterWorkflowTemplateServiceApi apiInstance = new ClusterWorkflowTemplateServiceApi(defaultClient);\n    String name = \"name_example\"; // String | \n    String deleteOptionsGracePeriodSeconds = \"deleteOptionsGracePeriodSeconds_example\"; // String | The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional.\n    String deleteOptionsPreconditionsUid = \"deleteOptionsPreconditionsUid_example\"; // String | Specifies the target UID. +optional.\n    String deleteOptionsPreconditionsResourceVersion = \"deleteOptionsPreconditionsResourceVersion_example\"; // String | Specifies the target ResourceVersion +optional.\n    Boolean deleteOptionsOrphanDependents = true; // Boolean | Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional.\n    String deleteOptionsPropagationPolicy = \"deleteOptionsPropagationPolicy_example\"; // String | Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional.\n    List<String> deleteOptionsDryRun = Arrays.asList(); // List<String> | When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional +listType=atomic.\n    Boolean deleteOptionsIgnoreStoreReadErrorWithClusterBreakingPotential = true; // Boolean | if set to true, it will trigger an unsafe deletion of the resource in case the normal deletion flow fails with a corrupt object error. A resource is considered corrupt if it can not be retrieved from the underlying storage successfully because of a) its data can not be transformed e.g. decryption failure, or b) it fails to decode into an object. NOTE: unsafe deletion ignores finalizer constraints, skips precondition checks, and removes the object from the storage. WARNING: This may potentially break the cluster if the workload associated with the resource being unsafe-deleted relies on normal deletion flow. Use only if you REALLY know what you are doing. The default value is false, and the user must opt in to enable it +optional.\n    try {\n      Object result = apiInstance.clusterWorkflowTemplateServiceDeleteClusterWorkflowTemplate(name, deleteOptionsGracePeriodSeconds, deleteOptionsPreconditionsUid, deleteOptionsPreconditionsResourceVersion, deleteOptionsOrphanDependents, deleteOptionsPropagationPolicy, deleteOptionsDryRun, deleteOptionsIgnoreStoreReadErrorWithClusterBreakingPotential);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ClusterWorkflowTemplateServiceApi#clusterWorkflowTemplateServiceDeleteClusterWorkflowTemplate\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Start Argo Workflows Locally - Bash\nDESCRIPTION: This bash command starts the Argo Workflow controller locally. This allows you to run workflows without Docker or Kubernetes. Running `make clean` before `make start` ensures recompilation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nmake start\n```\n\n----------------------------------------\n\nTITLE: Creating Workflow using ClusterWorkflowTemplate entrypoint and arguments in YAML\nDESCRIPTION: This YAML creates a Workflow based on the `cluster-workflow-template-submittable` ClusterWorkflowTemplate, using the template's defined `entrypoint` and `Workflow Arguments` without overriding them. Dependencies: Argo Workflows, `cluster-workflow-template-submittable` ClusterWorkflowTemplate must exist.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cluster-workflow-templates.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: cluster-workflow-template-hello-world-\nspec:\n  workflowTemplateRef:\n    name: cluster-workflow-template-submittable\n    clusterScope: true\n\n```\n\n----------------------------------------\n\nTITLE: Retry Latest Workflow\nDESCRIPTION: This example demonstrates how to retry the latest workflow using the `@latest` shorthand. The Argo CLI resolves `@latest` to the most recently created workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_retry.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nargo retry @latest\n```\n\n----------------------------------------\n\nTITLE: Listing Workflow Event Bindings using Argo Workflows Python SDK\nDESCRIPTION: This code snippet demonstrates how to list workflow event bindings in a given namespace using the Argo Workflows Python SDK.  It configures the API client with authentication and authorization parameters, then calls the list_workflow_event_bindings method. It showcases both required and optional parameters. It depends on the argo_workflows package.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventServiceApi.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import event_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_workflow_event_binding_list import IoArgoprojWorkflowV1alpha1WorkflowEventBindingList\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = event_service_api.EventServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    list_options_label_selector = \"listOptions.labelSelector_example\" # str | A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. (optional)\n    list_options_field_selector = \"listOptions.fieldSelector_example\" # str | A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. (optional)\n    list_options_watch = True # bool | Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. (optional)\n    list_options_allow_watch_bookmarks = True # bool | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. (optional)\n    list_options_resource_version = \"listOptions.resourceVersion_example\" # str | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_resource_version_match = \"listOptions.resourceVersionMatch_example\" # str | resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_timeout_seconds = \"listOptions.timeoutSeconds_example\" # str | Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. (optional)\n    list_options_limit = \"listOptions.limit_example\" # str | limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. (optional)\n    list_options_continue = \"listOptions.continue_example\" # str | The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. (optional)\n    list_options_send_initial_events = True # bool | `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\\\"io.k8s.initial-events-end\\\": \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"` (for backward compatibility reasons) and to false otherwise. +optional (optional)\n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.list_workflow_event_bindings(namespace)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventServiceApi->list_workflow_event_bindings: %s\\n\" % e)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.list_workflow_event_bindings(namespace, list_options_label_selector=list_options_label_selector, list_options_field_selector=list_options_field_selector, list_options_watch=list_options_watch, list_options_allow_watch_bookmarks=list_options_allow_watch_bookmarks, list_options_resource_version=list_options_resource_version, list_options_resource_version_match=list_options_resource_version_match, list_options_timeout_seconds=list_options_timeout_seconds, list_options_limit=list_options_limit, list_options_continue=list_options_continue, list_options_send_initial_events=list_options_send_initial_events)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventServiceApi->list_workflow_event_bindings: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Listing Archived Workflows with Label Selectors\nDESCRIPTION: This example demonstrates listing archived workflows that match specific labels using the `-l` flag. You can specify multiple labels separated by commas to filter the results based on multiple criteria.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_list.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nargo archive list -l key1=value1,key2=value2\n```\n\n----------------------------------------\n\nTITLE: Getting Workflow by UID - Argo Archive\nDESCRIPTION: Retrieves an archived workflow by its UID. The UID is a unique identifier for the workflow within the Argo archive. This command provides a basic example of fetching a workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_get.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo archive get UID [flags]\n```\n\n----------------------------------------\n\nTITLE: Configuring PodAffinityTerm\nDESCRIPTION: This snippet demonstrates the configuration of a `PodAffinityTerm` used to define affinity rules for pods. It uses `label_selector`, `match_label_keys`, `mismatch_label_keys`, `namespace_selector`, `namespaces`, and `topology_key` to specify which pods should be considered for affinity.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_34\n\nLANGUAGE: YAML\nCODE:\n```\nPodAffinityTerm(\n  label_selector=LabelSelector(\n    match_expressions=[\n      LabelSelectorRequirement(\n        key=\"key_example\",\n        operator=\"operator_example\",\n        values=[\n          \"values_example\",\n        ],\n      ),\n    ],\n    match_labels={\n      \"key\": \"key_example\",\n    },\n  ),\n  match_label_keys=[\n    \"match_label_keys_example\",\n  ],\n  mismatch_label_keys=[\n    \"mismatch_label_keys_example\",\n  ],\n  namespace_selector=LabelSelector(\n    match_expressions=[\n      LabelSelectorRequirement(\n        key=\"key_example\",\n        operator=\"operator_example\",\n        values=[\n          \"values_example\",\n        ],\n      ),\n    ],\n    match_labels={\n      \"key\": \"key_example\",\n    },\n  ),\n  namespaces=[\n    \"namespaces_example\",\n  ],\n  topology_key=\"topology_key_example\",\n)\n```\n\n----------------------------------------\n\nTITLE: Embedding Swagger UI JavaScript Bundle\nDESCRIPTION: This snippet includes the Swagger UI JavaScript bundle from a CDN. This bundle provides the core functionality for rendering and interacting with the API documentation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/swagger.md#_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"https://unpkg.com/swagger-ui-dist@5.11.0/swagger-ui-bundle.js\" crossorigin></script>\n```\n\n----------------------------------------\n\nTITLE: Argo Auth Token Command\nDESCRIPTION: This command is used to print the authentication token for accessing the Argo Workflows server. It does not require any specific dependencies beyond the Argo Workflows CLI itself. The command outputs the authentication token to the standard output.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_auth_token.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nargo auth token [flags]\n```\n\n----------------------------------------\n\nTITLE: Watching Sensors with Minimal Parameters in Argo Workflows (Python)\nDESCRIPTION: This code snippet shows how to use the `watch_sensors` method to watch sensors in a given namespace. It uses the minimal required parameters and includes exception handling for potential API errors.\n\nRequired dependencies: `argo_workflows`\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = sensor_service_api.SensorServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.watch_sensors(namespace)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling SensorServiceApi->watch_sensors: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Argo Server Deployment Configuration\nDESCRIPTION: This YAML snippet configures the Argo Server deployment, setting the `ARGO_BASE_HREF` environment variable to `/argo/`. This is necessary when running the server behind a reverse proxy with a sub-path.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: argo-server\nspec:\n  selector:\n    matchLabels:\n      app: argo-server\n  template:\n    metadata:\n      labels:\n        app: argo-server\n    spec:\n      containers:\n      - args:\n        - server\n        env:\n          - name: ARGO_BASE_HREF\n            value: /argo/\n        image: argoproj/argocli:latest\n        name: argo-server\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring PodAffinityTerm\nDESCRIPTION: This snippet configures a PodAffinityTerm, defining criteria for selecting pods based on their labels and namespaces. It includes a LabelSelector to match pod labels, namespaceSelector to match namespaces, a list of matchLabelKeys, mismatchLabelKeys and a topologyKey that indicates the node label key to use for matching pods. This allows defining affinity rules based on pod and node characteristics.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nPodAffinityTerm(\n  label_selector=LabelSelector(\n    match_expressions=[\n      LabelSelectorRequirement(\n        key=\"key_example\",\n        operator=\"operator_example\",\n        values=[\n          \"values_example\",\n        ],\n      ),\n    ],\n    match_labels={\n      \"key\": \"key_example\",\n    },\n  ),\n  match_label_keys=[\n    \"match_label_keys_example\",\n  ],\n  mismatch_label_keys=[\n    \"mismatch_label_keys_example\",\n  ],\n  namespace_selector=LabelSelector(\n    match_expressions=[\n      LabelSelectorRequirement(\n        key=\"key_example\",\n        operator=\"operator_example\",\n        values=[\n          \"values_example\",\n        ],\n      ),\n    ],\n    match_labels={\n      \"key\": \"key_example\",\n    },\n  ),\n  namespaces=[\n    \"namespaces_example\",\n  ],\n  topology_key=\"topology_key_example\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring LabelSelectorRequirement\nDESCRIPTION: This snippet configures a LabelSelectorRequirement, which is part of a LabelSelector.  It specifies a requirement that a label on a Kubernetes object must satisfy to match the selector.  It includes the key of the label, the operator (e.g., In, NotIn, Exists, DoesNotExist), and the values to compare against, forming the selection criteria.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nLabelSelectorRequirement(\n  key=\"key_example\",\n  operator=\"operator_example\",\n  values=[\n    \"values_example\",\n  ],\n)\n```\n\n----------------------------------------\n\nTITLE: Argo CLI Inherited Options\nDESCRIPTION: This snippet shows a selection of options inherited from parent commands by the Argo CLI. These options configure various aspects of the connection to the Argo server, including server address, authentication, TLS settings, and logging.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_auth.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n      --argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Configuring Artifact Archival in Argo Workflows\nDESCRIPTION: This YAML demonstrates different artifact archival strategies within Argo Workflows. It shows how to disable archiving entirely or customize the compression behavior (disabling compression in this case).\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/artifacts.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\noutputs:\n  artifacts:\n    # default behavior - tar+gzip default compression.\n  - name: hello-art-1\n    path: /tmp/hello_world.txt\n\n    # disable archiving entirely - upload the file / directory as is.\n    # this is useful when the container layout matches the desired target repository layout.\n  - name: hello-art-2\n    path: /tmp/hello_world.txt\n    archive:\n      none: {}\n\n    # customize the compression behavior (disabling it here).\n    # this is useful for files with varying compression benefits,\n    # e.g. disabling compression for a cached build workspace and large binaries,\n    # or increasing compression for \"perfect\" textual data - like a json/xml export of a large database.\n  - name: hello-art-3\n    path: /tmp/hello_world.txt\n    archive:\n      tar:\n        # no compression (also accepts the standard gzip 1 to 9 values)\n        compressionLevel: 0\n```\n\n----------------------------------------\n\nTITLE: Watching Argo Event Sources with Minimal Parameters (Python)\nDESCRIPTION: This code snippet shows how to watch Argo Workflows event sources using the `watch_event_sources` method with only the required `namespace` parameter. It also includes error handling to catch potential API exceptions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = event_source_service_api.EventSourceServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.watch_event_sources(namespace)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventSourceServiceApi->watch_event_sources: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Listing Sensors with Minimal Parameters (Python)\nDESCRIPTION: This code snippet demonstrates listing sensors using the `list_sensors` method of the `SensorServiceApi` with only the required `namespace` parameter. It prints the API response using `pprint`, and handles potential `ApiException` errors.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = sensor_service_api.SensorServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.list_sensors(namespace)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling SensorServiceApi->list_sensors: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Bash: Data Sourcing and Transformation\nDESCRIPTION: This bash snippet demonstrates a common data sourcing and transformation operation using `find`, `grep`, and `sed`. It finds files recursively, filters for `.pdf` files, and then modifies the filenames.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/data-sourcing-and-transformation.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nfind -r . | grep \".pdf\" | sed \"s/foo/foo.ready/\"\n```\n\n----------------------------------------\n\nTITLE: CLI Installation (Mac/Linux) - Bash\nDESCRIPTION: This snippet installs the Argo Workflows CLI on macOS or Linux. It downloads the appropriate binary, unzips it, makes it executable, moves it to the path, and then verifies the installation by running `argo version`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/hack/release-notes.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Detect OS\nARGO_OS=\"darwin\"\nif [[ \"$(uname -s)\" != \"Darwin\" ]]; then\n  ARGO_OS=\"linux\"\nfi\n\n# Download the binary\ncurl -sLO \"https://github.com/argoproj/argo-workflows/releases/download/$version/argo-$ARGO_OS-amd64.gz\"\n\n# Unzip\ngunzip \"argo-$ARGO_OS-amd64.gz\"\n\n# Make binary executable\nchmod +x \"argo-$ARGO_OS-amd64\"\n\n# Move binary to path\nmv \"./argo-$ARGO_OS-amd64\" /usr/local/bin/argo\n\n# Test installation\nargo version\n```\n\n----------------------------------------\n\nTITLE: Initializing Swagger UI with JavaScript\nDESCRIPTION: This JavaScript code initializes the Swagger UI by loading the swagger.json file from the specified URL. The `SwaggerUIBundle` function is used to configure the UI, setting the URL to the API specification and the DOM element where the UI will be rendered. It is executed when the window has loaded.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/swagger.md#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n  window.onload = function loadSwaggerUI() {\n    window.ui = SwaggerUIBundle({\n      url: \"https://raw.githubusercontent.com/argoproj/argo-workflows/main/api/openapi-spec/swagger.json\",\n      dom_id: \"#swagger-ui\",\n    });\n  };\n```\n\n----------------------------------------\n\nTITLE: Set Argo Workflows Version (Bash)\nDESCRIPTION: This snippet sets the desired version of Argo Workflows in an environment variable.  Replace 'vX.Y.Z' with the specific version number you wish to use. This variable is used in subsequent commands to download the appropriate installation manifest.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/quick-start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nARGO_WORKFLOWS_VERSION=\"vX.Y.Z\"\n```\n\n----------------------------------------\n\nTITLE: Argo Cron Workflow Creation\nDESCRIPTION: Creates a cron workflow in Argo Workflows by specifying one or more YAML files. Supports overriding schedule, entrypoint, and other configurations. Requires the Argo CLI to be installed and configured.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_create.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo cron create FILE1 FILE2... [flags]\n```\n\n----------------------------------------\n\nTITLE: Listing Event Sources in Argo Workflows (All Options)\nDESCRIPTION: This snippet demonstrates listing event sources in a given namespace with all available options. It includes parameters for label and field selectors, watching, resource versions, timeouts, limits, and continuation tokens. Exception handling is included.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = event_source_service_api.EventSourceServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    list_options_label_selector = \"listOptions.labelSelector_example\" # str | A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. (optional)\n    list_options_field_selector = \"listOptions.fieldSelector_example\" # str | A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. (optional)\n    list_options_watch = True # bool | Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. (optional)\n    list_options_allow_watch_bookmarks = True # bool | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. (optional)\n    list_options_resource_version = \"listOptions.resourceVersion_example\" # str | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_resource_version_match = \"listOptions.resourceVersionMatch_example\" # str | resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional (optional)\n    list_options_timeout_seconds = \"listOptions.timeoutSeconds_example\" # str | Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. (optional)\n    list_options_limit = \"listOptions.limit_example\" # str | limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. (optional)\n    list_options_continue = \"listOptions.continue_example\" # str | The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. (optional)\n    list_options_send_initial_events = True # bool | `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\\\"io.k8s.initial-events-end\\\": \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"` (for backward compatibility reasons) and to false otherwise. +optional (optional)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.list_event_sources(namespace, list_options_label_selector=list_options_label_selector, list_options_field_selector=list_options_field_selector, list_options_watch=list_options_watch, list_options_allow_watch_bookmarks=list_options_allow_watch_bookmarks, list_options_resource_version=list_options_resource_version, list_options_resource_version_match=list_options_resource_version_match, list_options_timeout_seconds=list_options_timeout_seconds, list_options_limit=list_options_limit, list_options_continue=list_options_continue, list_options_send_initial_events=list_options_send_initial_events)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventSourceServiceApi->list_event_sources: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Get Workflow Definition using Kubectl\nDESCRIPTION: Demonstrates how to retrieve the YAML definition of an Argo Workflow using `kubectl`. This allows users to inspect the workflow's structure, including node details such as display name and full name, which can be used in node field selectors.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/node-field-selector.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get wf appr-promotion-ffsv4 -o yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring SecurityContext\nDESCRIPTION: This snippet shows how to configure a `SecurityContext`, which defines the security-related settings for a container.  It includes settings for privilege escalation, AppArmor profile, capabilities, privileged mode, root filesystem read-only, and user/group IDs.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_40\n\nLANGUAGE: YAML\nCODE:\n```\nSecurityContext(\n  allow_privilege_escalation=True,\n  app_armor_profile=AppArmorProfile(\n    localhost_profile=\"localhost_profile_example\",\n    type=\"type_example\",\n  ),\n  capabilities=Capabilities(\n    add=[\n      \"add_example\",\n    ],\n    drop=[\n      \"drop_example\",\n    ],\n  ),\n  privileged=True,\n  proc_mount=\"proc_mount_example\",\n  read_only_root_filesystem=True,\n  run_as_group=1,\n  run_as_non_root=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Argo Workflow Execution History with CLI\nDESCRIPTION: This bash command demonstrates how to use the `argo get` CLI command to display the execution history of an Argo Workflow.  The output shows the execution order and parallelism of the steps. This command requires the Argo CLI to be installed and configured to connect to the Argo Workflows cluster.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/steps.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nSTEP            TEMPLATE           PODNAME                 DURATION  MESSAGE\n ✔ steps-z2zdn  hello-hello-hello\n ├───✔ hello1   print-message      steps-z2zdn-27420706    2s\n └─┬─✔ hello2a  print-message      steps-z2zdn-2006760091  3s\n   └─✔ hello2b  print-message      steps-z2zdn-2023537710  3s\n```\n\n----------------------------------------\n\nTITLE: Argo Workflow Controller ConfigMap YAML\nDESCRIPTION: This ConfigMap defines default workflow specifications within Argo Workflows.  It specifies default metadata annotations, labels, `ttlStrategy`, and `parallelism` settings.  These defaults are applied to all workflows executed by the controller unless overridden at the workflow level.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/default-workflow-specs.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# This file describes the config settings available in the workflow controller configmap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workflow-controller-configmap\ndata:\n  # Default values that will apply to all Workflows from this controller, unless overridden on the Workflow-level\n  workflowDefaults: |-\n    metadata:\n      annotations:\n        argo: workflows\n      labels:\n        foo: bar\n    spec:\n      ttlStrategy:\n        secondsAfterSuccess: 5\n      parallelism: 3\n```\n\n----------------------------------------\n\nTITLE: Create Executor Role for Argo Workflows\nDESCRIPTION: This YAML snippet defines a Kubernetes Role named 'executor' that grants permissions to create and patch workflowtaskresults within the argoproj.io API group.  This role is essential for the Argo Workflow executor to function correctly in versions 3.4 and later.  It requires the rbac.authorization.k8s.io/v1 API version.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-rbac.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: executor\nrules:\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - workflowtaskresults\n    verbs:\n      - create\n      - patch\n```\n\n----------------------------------------\n\nTITLE: Add Hosts to /etc/hosts - Text\nDESCRIPTION: This text snippet shows the entries that need to be added to the `/etc/hosts` file to map hostnames to the local IP address (127.0.0.1). This is necessary for Argo Workflows to resolve the addresses of local services like Dex, MinIO, Postgres, MySQL, and Azurite.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\n127.0.0.1 dex\n127.0.0.1 minio\n127.0.0.1 postgres\n127.0.0.1 mysql\n127.0.0.1 azurite\n```\n\n----------------------------------------\n\nTITLE: Creating ClusterWorkflowTemplate in Java\nDESCRIPTION: This snippet demonstrates how to create a ClusterWorkflowTemplate using the Argo Workflows API in Java. It initializes the API client, configures authentication, and calls the `clusterWorkflowTemplateServiceCreateClusterWorkflowTemplate` method with a request body.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ClusterWorkflowTemplateServiceApi.md#_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ClusterWorkflowTemplateServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ClusterWorkflowTemplateServiceApi apiInstance = new ClusterWorkflowTemplateServiceApi(defaultClient);\n    IoArgoprojWorkflowV1alpha1ClusterWorkflowTemplateCreateRequest body = new IoArgoprojWorkflowV1alpha1ClusterWorkflowTemplateCreateRequest(); // IoArgoprojWorkflowV1alpha1ClusterWorkflowTemplateCreateRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1ClusterWorkflowTemplate result = apiInstance.clusterWorkflowTemplateServiceCreateClusterWorkflowTemplate(body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ClusterWorkflowTemplateServiceApi#clusterWorkflowTemplateServiceCreateClusterWorkflowTemplate\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating a Cluster Workflow Template with Relaxed Validation\nDESCRIPTION: This command updates a cluster workflow template with relaxed validation. Setting `--strict false` disables strict workflow validation, allowing updates with potentially incomplete or less strictly formatted templates.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_update.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo cluster-template update FILE1 --strict false\n```\n\n----------------------------------------\n\nTITLE: Generate Azure SAS Token with Azure CLI\nDESCRIPTION: This snippet generates a Shared Access Signature (SAS) token for an Azure storage container using the Azure CLI. The `az storage container generate-sas` command creates the SAS token with specified permissions, expiry, and authentication mode. The result is stored in the `SAS_TOKEN` variable.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nSAS_TOKEN=\"$(az storage container generate-sas --account-name <storage-account> --name <container> --permissions acdlrw --expiry <date-time> --auth-mode key)\"\n```\n\n----------------------------------------\n\nTITLE: Referencing Artifact Repository in Workflow Spec YAML\nDESCRIPTION: This snippet shows how to reference an artifact repository defined in a ConfigMap within a workflow specification.  It uses the `artifactRepositoryRef` field to specify the ConfigMap and key containing the desired repository configuration. This allows workflows to use pre-defined repository settings, avoiding duplication and centralizing configuration.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/artifact-repository-ref.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  artifactRepositoryRef:\n    configMap: my-artifact-repository # default is \"artifact-repositories\"\n    key: v2-s3-artifact-repository # default can be set by the `workflows.argoproj.io/default-artifact-repository` annotation in config map.\n```\n\n----------------------------------------\n\nTITLE: Configuring LabelSelector\nDESCRIPTION: This snippet configures a LabelSelector, which is used to select Kubernetes objects based on their labels. It includes matchExpressions (a list of LabelSelectorRequirement) and matchLabels (a map of label key-value pairs). The selector matches objects that satisfy all specified requirements and labels.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nLabelSelector(\n  match_expressions=[\n    LabelSelectorRequirement(\n      key=\"key_example\",\n      operator=\"operator_example\",\n      values=[\n        \"values_example\",\n      ],\n    ),\n  ],\n  match_labels={\n    \"key\": \"key_example\",\n  },\n)\n```\n\n----------------------------------------\n\nTITLE: Template-Level Synchronization using ConfigMap (Semaphore)\nDESCRIPTION: This snippet illustrates template-level synchronization using a semaphore configured via a ConfigMap. The synchronization key 'template' is set with a limit of '2', allowing a maximum of two concurrent instances of the 'acquire-lock' template.  This applies across all steps, tasks, or workflows referencing the same template.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/synchronization.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"examples/synchronization-tmpl-level.yaml:11\"\n```\n\n----------------------------------------\n\nTITLE: SQS Event Source Configuration\nDESCRIPTION: Configures an SQS (Simple Queue Service) event source for Argo Events. The configuration defines access keys, endpoints, queue details, and region using SecretKeySelector for security. It includes flags to specify JSON body parsing and DLQ usage. Additionally, it sets queue account ID, role ARN, session token, and wait time seconds.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nsqs: {\n    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SQSEventSource(\n        access_key=SecretKeySelector(\n            key=\"key_example\",\n            name=\"name_example\",\n            optional=True,\n        ),\n        dlq=True,\n        endpoint=\"endpoint_example\",\n        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceFilter(\n            expression=\"expression_example\",\n        ),\n        json_body=True,\n        metadata={\n            \"key\": \"key_example\",\n        },\n        queue=\"queue_example\",\n        queue_account_id=\"queue_account_id_example\",\n        region=\"region_example\",\n        role_arn=\"role_arn_example\",\n        secret_key=SecretKeySelector(\n            key=\"key_example\",\n            name=\"name_example\",\n            optional=True,\n        ),\n        session_token=SecretKeySelector(\n            key=\"key_example\",\n            name=\"name_example\",\n            optional=True,\n        ),\n        wait_time_seconds=\"wait_time_seconds_example\",\n    ),\n}\n```\n\n----------------------------------------\n\nTITLE: Using Argo CLI with Verified Certificate (Env Var) - Bash\nDESCRIPTION: This snippet demonstrates how to use the Argo CLI with TLS encryption when the Argo Server is using a verified certificate, utilizing environment variables. It involves setting the `ARGO_SECURE` environment variable to `true` and then running the Argo CLI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/tls.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport ARGO_SECURE=true\nargo list\n```\n\n----------------------------------------\n\nTITLE: Start K3D Cluster - Bash\nDESCRIPTION: This bash command starts a K3D Kubernetes cluster. The `--wait` flag ensures that the cluster is fully operational before proceeding, which is recommended for local Argo Workflows development.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nk3d cluster start --wait\n```\n\n----------------------------------------\n\nTITLE: Deleting a Workflow Template with Java\nDESCRIPTION: This Java snippet demonstrates how to delete a Workflow Template using the Argo Workflows API. It initializes the API client, configures authentication using a Bearer token, and calls the `workflowTemplateServiceDeleteWorkflowTemplate` method with the specified namespace, name, and various deletion options. The example handles potential API exceptions and prints the result or error details.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowTemplateServiceApi.md#_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.WorkflowTemplateServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    WorkflowTemplateServiceApi apiInstance = new WorkflowTemplateServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    String deleteOptionsGracePeriodSeconds = \"deleteOptionsGracePeriodSeconds_example\"; // String | The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional.\n    String deleteOptionsPreconditionsUid = \"deleteOptionsPreconditionsUid_example\"; // String | Specifies the target UID. +optional.\n    String deleteOptionsPreconditionsResourceVersion = \"deleteOptionsPreconditionsResourceVersion_example\"; // String | Specifies the target ResourceVersion +optional.\n    Boolean deleteOptionsOrphanDependents = true; // Boolean | Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional.\n    String deleteOptionsPropagationPolicy = \"deleteOptionsPropagationPolicy_example\"; // String | Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional.\n    List<String> deleteOptionsDryRun = Arrays.asList(); // List<String> | When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional +listType=atomic.\n    Boolean deleteOptionsIgnoreStoreReadErrorWithClusterBreakingPotential = true; // Boolean | if set to true, it will trigger an unsafe deletion of the resource in case the normal deletion flow fails with a corrupt object error. A resource is considered corrupt if it can not be retrieved from the underlying storage successfully because of a) its data can not be transformed e.g. decryption failure, or b) it fails to decode into an object. NOTE: unsafe deletion ignores finalizer constraints, skips precondition checks, and removes the object from the storage. WARNING: This may potentially break the cluster if the workload associated with the resource being unsafe-deleted relies on normal deletion flow. Use only if you REALLY know what you are doing. The default value is false, and the user must opt in to enable it +optional.\n    try {\n      Object result = apiInstance.workflowTemplateServiceDeleteWorkflowTemplate(namespace, name, deleteOptionsGracePeriodSeconds, deleteOptionsPreconditionsUid, deleteOptionsPreconditionsResourceVersion, deleteOptionsOrphanDependents, deleteOptionsPropagationPolicy, deleteOptionsDryRun, deleteOptionsIgnoreStoreReadErrorWithClusterBreakingPotential);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling WorkflowTemplateServiceApi#workflowTemplateServiceDeleteWorkflowTemplate\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configure Image Command in Workflow\nDESCRIPTION: This YAML snippet configures the command for a Docker image in an Argo Workflow. It's used when the Docker image has a v1 manifest and requires explicit entrypoint and command configuration.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nimages:\n  docker/whalesay:latest:\n    cmd: [/bin/bash]\n```\n\n----------------------------------------\n\nTITLE: Set Instance ID in Workflow Controller ConfigMap\nDESCRIPTION: This YAML snippet shows how to set an instance ID for the Argo Workflow Controller.  Setting an instance ID is necessary for sharding Argo instances within a cluster, allowing multiple Argo installations to run concurrently. The `instanceID` value must be unique for each Argo instance.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/scaling.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workflow-controller-configmap\ndata:\n    instanceID: i1\n```\n\n----------------------------------------\n\nTITLE: Argo Archive Usage\nDESCRIPTION: Displays the usage instructions and available options for the 'argo archive' command. It provides a summary of how to manage the workflow archive using the Argo CLI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nargo archive [flags]\n```\n\n----------------------------------------\n\nTITLE: Override Entrypoint using Argo CLI (Bash)\nDESCRIPTION: This bash snippet shows how to override the default entrypoint of an Argo Workflow using the Argo CLI's `argo submit` command.  The `--entrypoint` flag is used to specify the name of the template to invoke as the entrypoint, which is `print-message-caps` in this example. `arguments-parameters.yaml` is assumed to be the name of the workflow definition file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/parameters.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nargo submit arguments-parameters.yaml --entrypoint print-message-caps\n```\n\n----------------------------------------\n\nTITLE: List and Delete Old Workflows - Bash\nDESCRIPTION: These bash commands list and delete Argo Workflows that were completed more than 7 days ago. It uses the `argo list` and `argo delete` commands with the `--older` flag to select workflows based on their completion time.  These commands require Argo Workflows v2.9 or later and the `argo` CLI tool.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cost-optimisation.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nargo list --older 7d\nargo delete --older 7d\n```\n\n----------------------------------------\n\nTITLE: Common Metrics Configuration in ConfigMap\nDESCRIPTION: This YAML snippet shows common metrics configuration options in the Argo Workflow Controller ConfigMap, including setting the metrics TTL and using modifiers to customize emitted metrics, such as disabling specific metrics or attributes.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmetricsConfig: |\n  # MetricsTTL sets how often custom metrics are cleared from memory. Default is \"0\", metrics are never cleared. Histogram metrics are never cleared.\n  metricsTTL: \"10m\"\n  # Modifiers allows tuning of each of the emitted metrics\n  modifiers:\n    pod_missing:\n      disabled: true\n    cronworkflows_triggered_total:\n      disabledAttributes:\n        - name\n    k8s_request_duration:\n      histogramBuckets: [ 1.0, 2.0, 10.0 ]\n```\n\n----------------------------------------\n\nTITLE: Python Script for Coin Flip Simulation\nDESCRIPTION: This Python script simulates a coin flip and prints either 'heads' or 'tails' to standard output. It relies on the `random` module for generating a random integer to determine the outcome. This script is used within the `flip-coin` template of the Argo Workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/recursion.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport random\nresult = \"heads\" if random.randint(0,1) == 0 else \"tails\"\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Set Node Selector for Cheaper Instances - YAML\nDESCRIPTION: This YAML snippet demonstrates how to use a node selector in an Argo Workflow to utilize cheaper instances, such as spot instances, in a Kubernetes cluster.  This is achieved by specifying the `nodeSelector` field within the workflow's specification.  The node selector targets nodes labeled with `node-role.kubernetes.io/argo-spot-worker=true`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cost-optimisation.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nnodeSelector:\n  \"node-role.kubernetes.io/argo-spot-worker\": \"true\"\n```\n\n----------------------------------------\n\nTITLE: Configuring an SNS Event Source in Argo Events\nDESCRIPTION: This snippet details the configuration of an SNS (Simple Notification Service) event source within Argo Events. It highlights how to set parameters like access key, endpoint, filter, region, role ARN, secret key, topic ARN, and webhook settings. The code shows how to use SecretKeySelector for sensitive information and configure the `WebhookContext`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_29\n\nLANGUAGE: YAML\nCODE:\n```\nsns={\n                    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SNSEventSource(\n                        access_key=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        endpoint=\"endpoint_example\",\n                        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceFilter(\n                            expression=\"expression_example\",\n                        ),\n                        metadata={\n                            \"key\": \"key_example\",\n                        },\n                        region=\"region_example\",\n                        role_arn=\"role_arn_example\",\n                        secret_key=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        topic_arn=\"topic_arn_example\",\n                        validate_signature=True,\n                        webhook=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1WebhookContext(\n                            auth_secret=SecretKeySelector(\n                                key=\"key_example\",\n                                name=\"name_example\",\n                                optional=True,\n                            ),\n                            endpoint=\"endpoint_example\",\n                            max_payload_size=\"max_payload_size_example\",\n                            metadata={\n                                \"key\": \"key_example\",\n                            },\n                            method=\"method_example\",\n                            port=\"port_example\",\n                            server_cert_secret=SecretKeySelector(\n                                key=\"key_example\",\n                                name=\"name_example\",\n                                optional=True,\n                            ),\n                            server_key_secret=SecretKeySelector(\n                                key=\"key_example\",\n                                name=\"name_example\",\n                                optional=True,\n                            ),\n                            url=\"url_example\",\n                        ),\n                    ),\n                }\n```\n\n----------------------------------------\n\nTITLE: Enable TLS with Self-Signed Certificate - Bash\nDESCRIPTION: This snippet shows how to start the Argo Server with TLS encryption using a self-signed certificate.  This is suitable for development and test environments. It involves using the `--secure` flag when starting the Argo Server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/tls.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo server --secure\n```\n\n----------------------------------------\n\nTITLE: Retry and Wait for Completion - Argo\nDESCRIPTION: This example shows how to retry a workflow and wait for its completion using the `--wait` flag with the `argo archive retry` command. It blocks until the retried workflow finishes execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_retry.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nargo archive retry --wait uid\n```\n\n----------------------------------------\n\nTITLE: Argo Cluster-Template Lint Options\nDESCRIPTION: These options configure the behavior of the `argo cluster-template lint` command. They control aspects such as help display, output format, and the strictness of workflow validation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_lint.md#_snippet_1\n\nLANGUAGE: go\nCODE:\n```\n  -h, --help            help for lint\n  -o, --output string   Linting results output format. One of: pretty|simple (default \"pretty\")\n      --strict          perform strict workflow validation (default true)\n```\n\n----------------------------------------\n\nTITLE: Creating Secret with Discovery by Name (YAML)\nDESCRIPTION: This YAML snippet defines a Kubernetes Secret of type `kubernetes.io/service-account-token`.  The secret's name is set to `default.service-account-token` and it is annotated to associate it with the `default` service account.  This approach simplifies secret discovery by adhering to a naming convention that links the secret to the service account.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/service-account-secrets.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: default.service-account-token\n  annotations:\n    kubernetes.io/service-account.name: default\ntype: kubernetes.io/service-account-token\n```\n\n----------------------------------------\n\nTITLE: Retrieve Azure Access Key with Azure CLI\nDESCRIPTION: This snippet retrieves the access key for a specified Azure storage account using the Azure CLI. The `az storage account keys list` command queries the account keys and extracts the first key's value. The result is stored in the `ACCESS_KEY` variable.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nACCESS_KEY=\"$(az storage account keys list -n mystorageaccountname --query '[0].value' -otsv)\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Resource in Argo\nDESCRIPTION: This YAML snippet defines a 'k8s-owner-reference' template of type 'resource'. It creates a ConfigMap resource on the Kubernetes cluster with a generated name and a 'some' data field. The action is set to 'create' to define the operation to be performed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-concepts.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n  - name: k8s-owner-reference\n    resource:\n      action: create\n      manifest: |\n        apiVersion: v1\n        kind: ConfigMap\n        metadata:\n          generateName: owned-eg-\n        data:\n          some: value\n```\n\n----------------------------------------\n\nTITLE: Workflow: Using WorkflowTemplate with bar entrypoint (YAML)\nDESCRIPTION: This workflow defines an entrypoint named 'bar' and uses a WorkflowTemplate named 'say-main-entrypoint'.  The 'echo' template from the WorkflowTemplate will print the value of `workflow.mainEntrypoint`, which will be 'bar'.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/variables.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: bar-\nspec:\n  entrypoint: bar\n  templates:\n    - name: bar\n      steps:\n      - - name: step\n          templateRef:\n            name: say-main-entrypoint\n            template: echo\n```\n\n----------------------------------------\n\nTITLE: Configure Dex Static Clients in Argo CD\nDESCRIPTION: This YAML snippet configures the Argo CD ConfigMap to define a static client for Argo Workflows in Dex. The `id` should match the client ID used in the secret (`argo-workflows-sso`).  The `redirectURIs` should point to the `/oauth2/callback` endpoint of the Argo Workflows server. The `secretEnv` links the client secret to the environment variable set in the Dex deployment.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso-argocd.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\ndata:\n  # Kustomize sees the value of dex.config as a single string instead of yaml. It will not merge\n  # Dex settings, but instead it will replace the entire configuration with the settings below,\n  # so add these to the existing config instead of setting them in a separate file\n  dex.config: |\n    # Setting staticClients allows Argo Workflows to use Argo CD's Dex installation for authentication\n    staticClients:\n      # This is the OIDC client ID in plaintext\n      - id: argo-workflows-sso\n        name: Argo Workflow\n        redirectURIs:\n          - https://argo-workflows.mydomain.com/oauth2/callback\n        secretEnv: ARGO_WORKFLOWS_SSO_CLIENT_SECRET\n```\n\n----------------------------------------\n\nTITLE: Argo Auth Help Option\nDESCRIPTION: This snippet shows the help option for the `argo auth` command.  It displays the available options and usage instructions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_auth.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n-h, --help   help for auth\n```\n\n----------------------------------------\n\nTITLE: RBAC Role for Artifact GC in Argo Workflows\nDESCRIPTION: This YAML defines an RBAC Role for granting the minimum permissions required for artifact garbage collection in Argo Workflows. It allows listing, watching, and patching workflow artifact GC tasks.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/artifacts.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  annotations:\n    workflows.argoproj.io/description: |\n      This is the minimum recommended permissions needed if you want to use artifact GC.\n  name: artifactgc\nrules:\n- apiGroups:\n  - argoproj.io\n  resources:\n  - workflowartifactgctasks\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - argoproj.io\n  resources:\n  - workflowartifactgctasks/status\n  verbs:\n  - patch\n```\n\n----------------------------------------\n\nTITLE: Argo Workflow Definition Example YAML\nDESCRIPTION: This is an example of an Argo Workflow definition demonstrating how to define metadata, labels, ttlStrategy, and parallelism. The Workflow definition shows how values can be configured at the Workflow level, overriding any defaults set in the workflow-controller-configmap.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/default-workflow-specs.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: gc-ttl-\n  annotations:\n    argo: workflows\n  labels:\n    foo: bar\nspec:\n  ttlStrategy:\n    secondsAfterSuccess: 5     # Time to live after workflow is successful\n  parallelism: 3\n```\n\n----------------------------------------\n\nTITLE: Building Workflow Executor Image for Windows - Docker CLI\nDESCRIPTION: These commands clone the argo-workflows repository, navigate to the 'argo' directory, and build the argoexec image for Windows using the specified Dockerfile. This requires a Windows machine with Docker installed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/windows.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/argoproj/argo-workflows.git\ncd argo\ndocker build -t myargoexec -f .\\Dockerfile.windows --target argoexec .\n```\n\n----------------------------------------\n\nTITLE: Configuring EnvFromSource\nDESCRIPTION: This snippet shows how to configure `EnvFromSource`, which allows populating environment variables from ConfigMaps or Secrets. It uses `config_map_ref`, `prefix`, and `secret_ref` to define the source of the environment variables.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_38\n\nLANGUAGE: YAML\nCODE:\n```\nEnvFromSource(\n  config_map_ref=ConfigMapEnvSource(\n    name=\"name_example\",\n    optional=True,\n  ),\n  prefix=\"prefix_example\",\n  secret_ref=SecretEnvSource(\n    name=\"name_example\",\n    optional=True,\n  ),\n)\n```\n\n----------------------------------------\n\nTITLE: Get Workflow Info in YAML - Argo Archive\nDESCRIPTION: Retrieves information about an archived workflow using its UID and outputs it in YAML format. The `-o yaml` flag specifies the desired output format.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_get.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Get information about an archived workflow in YAML format:\n  argo archive get abc123-def456-ghi789-jkl012 -o yaml\n```\n\n----------------------------------------\n\nTITLE: Get ClusterWorkflowTemplate - Java\nDESCRIPTION: This code snippet demonstrates how to retrieve a ClusterWorkflowTemplate by its name using the Argo Workflows API. It requires the 'name' parameter, and optionally accepts 'getOptionsResourceVersion' to specify the resource version. The example sets up the API client, configures authentication, and handles potential API exceptions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ClusterWorkflowTemplateServiceApi.md#_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ClusterWorkflowTemplateServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ClusterWorkflowTemplateServiceApi apiInstance = new ClusterWorkflowTemplateServiceApi(defaultClient);\n    String name = \"name_example\"; // String | \n    String getOptionsResourceVersion = \"getOptionsResourceVersion_example\"; // String | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    try {\n      IoArgoprojWorkflowV1alpha1ClusterWorkflowTemplate result = apiInstance.clusterWorkflowTemplateServiceGetClusterWorkflowTemplate(name, getOptionsResourceVersion);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ClusterWorkflowTemplateServiceApi#clusterWorkflowTemplateServiceGetClusterWorkflowTemplate\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring workflowRestrictions in ConfigMap YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure `workflowRestrictions` in the `workflow-controller-configmap`. It sets the `templateReferencing` to `Strict`, enforcing that workflows can only run with `workflowTemplateRef`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-restrictions.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# This file describes the config settings available in the workflow controller configmap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workflow-controller-configmap\ndata:\n  workflowRestrictions: |\n    templateReferencing: Strict\n```\n\n----------------------------------------\n\nTITLE: Collect Event with InfoService Java\nDESCRIPTION: This snippet demonstrates how to collect an event using the `infoServiceCollectEvent` method of the `InfoServiceApi`. It initializes an API client, configures API key authorization, creates a request body, and calls the `infoServiceCollectEvent` method, handling potential API exceptions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/InfoServiceApi.md#_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.InfoServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    InfoServiceApi apiInstance = new InfoServiceApi(defaultClient);\n    IoArgoprojWorkflowV1alpha1CollectEventRequest body = new IoArgoprojWorkflowV1alpha1CollectEventRequest(); // IoArgoprojWorkflowV1alpha1CollectEventRequest | \n    try {\n      Object result = apiInstance.infoServiceCollectEvent(body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling InfoServiceApi#infoServiceCollectEvent\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Executor Plugin HTTP Request\nDESCRIPTION: This bash command demonstrates an example HTTP POST request sent to an Executor Plugin to trigger template execution. It sends a JSON payload containing workflow and template information to the `/api/v1/template.execute` endpoint.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:4355/api/v1/template.execute -d \\\n'{\n  \"workflow\": {\n    \"metadata\": {\n      \"name\": \"my-wf\"\n    }\n  },\n  \"template\": {\n    \"name\": \"my-tmpl\",\n    \"inputs\": {},\n    \"outputs\": {},\n    \"plugin\": {\n      \"hello\": {}\n    }\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Triggering WorkflowTemplate creation with event\nDESCRIPTION: This bash command triggers the creation of a WorkflowTemplate by sending an event to the Argo Server. The event includes the required authorization header, the \"X-Argo-E2E\" header set to true, and a JSON payload with a \"message\" field.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/events.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl $ARGO_SERVER/api/v1/events/argo/my-discriminator \\\n    -H \"Authorization: $ARGO_TOKEN\" \\\n    -H \"X-Argo-E2E: true\" \\\n    -d '{\"message\": \"hello events\"}'\n```\n\n----------------------------------------\n\nTITLE: Argo Cron Update Command Syntax\nDESCRIPTION: This snippet shows the basic syntax of the `argo cron update` command. It accepts one or more file paths as arguments, representing the cron workflow templates to be updated, along with optional flags to modify the update process.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_update.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nargo cron update FILE1 FILE2... [flags]\n```\n\n----------------------------------------\n\nTITLE: Profiling CPU Usage with pprof (Bash)\nDESCRIPTION: This command profiles CPU usage for 30 seconds using pprof.  It connects to the Argo controller's debug endpoint to gather profiling data. Ensure the controller is running locally with the `ARGO_PPROF=true` environment variable set.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\ngo tool pprof http://localhost:6060/debug/pprof/profile\n```\n\n----------------------------------------\n\nTITLE: Equivalent Depends with explicit task results\nDESCRIPTION: This YAML snippet illustrates the equivalent `depends` condition when specifying task results explicitly. It shows that the unspecified dependency from the previous example is equivalent to `(task.Succeeded || task.Skipped || task.Daemoned) || task-2.Failed`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/enhanced-depends-logic.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndepends: (task.Succeeded || task.Skipped || task.Daemoned) || task-2.Failed\n```\n\n----------------------------------------\n\nTITLE: Getting CronWorkflow Details using Argo CLI\nDESCRIPTION: This command retrieves detailed information about a specific CronWorkflow using the Argo CLI. The output includes the name, namespace, creation time, schedule, suspension status, concurrency policy, last scheduled time, next scheduled time, and active workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cron-workflows.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n$ argo cron get test-cron-wf\n```\n\n----------------------------------------\n\nTITLE: Submit Workflow with Instance ID (Argo CLI)\nDESCRIPTION: This bash command shows how to submit a workflow using the Argo CLI, specifying an instance ID.  The `--instanceid` flag tells the CLI which Argo instance to target for workflow submission. This command assumes `my-wf.yaml` is the workflow definition file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/scaling.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nargo --instanceid i1 submit my-wf.yaml\n```\n\n----------------------------------------\n\nTITLE: Maven Dependency: Argo Client Java (Recommended)\nDESCRIPTION: This XML snippet configures a Maven dependency for the Argo Client Java library. It specifies the groupId, artifactId, and a recommended version (v3.3.8). This is the preferred method for including the Argo Workflows Java SDK in a Java project.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/README.md#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n    <groupId>io.argoproj.workflow</groupId>\n    <artifactId>argo-client-java</artifactId>\n    <version>v3.3.8</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring EnvVar with ValueFrom\nDESCRIPTION: This snippet demonstrates how to configure an `EnvVar` with a `value_from` field, allowing the environment variable's value to be sourced from various Kubernetes resources like ConfigMaps, Secrets, ObjectFields, or ResourceFields. It shows an example using a ConfigMapKeySelector.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_37\n\nLANGUAGE: YAML\nCODE:\n```\nEnvVar(\n  name=\"name_example\",\n  value=\"value_example\",\n  value_from=EnvVarSource(\n    config_map_key_ref=ConfigMapKeySelector(\n      key=\"key_example\",\n      name=\"name_example\",\n      optional=True,\n    ),\n    field_ref=ObjectFieldSelector(\n      api_version=\"api_version_example\",\n      field_path=\"field_path_example\",\n    ),\n    resource_field_ref=ResourceFieldSelector(\n      container_name=\"container_name_example\",\n      divisor=\"divisor_example\",\n      resource=\"resource_example\",\n    ),\n    secret_key_ref=SecretKeySelector(\n      key=\"key_example\",\n      name=\"name_example\",\n      optional=True,\n    ),\n  ),\n)\n```\n\n----------------------------------------\n\nTITLE: Artifact Mount Path Example - Argo YAML\nDESCRIPTION: This YAML snippet shows how to define an artifact input with a specific path in an Argo Workflow targeting Windows. It demonstrates that all paths are automatically mapped to the C: drive within the container.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/windows.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n # ...\n    - name: print-message\n      inputs:\n        artifacts:\n          # unpack the message input artifact\n          # and put it at C:\\message\n          - name: message\n            path: \"/message\" # gets mapped to C:\\message\n      nodeSelector:\n        kubernetes.io/os: windows\n      container:\n        image: mcr.microsoft.com/windows/nanoserver:1809\n        command: [\"cmd\", \"/c\"]\n        args: [\"dir C:\\\\message\"]   # List the C:\\message directory\n```\n\n----------------------------------------\n\nTITLE: Skipping Database Migration (YAML)\nDESCRIPTION: This YAML snippet shows how to skip database migration when enabling persistence for Argo Workflows.  Setting `skipMigration` to `true` prevents the workflow controller from automatically updating the database schema.  This is an advanced option and should be used with caution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-archive.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\npersistence:\n  skipMigration: true\n```\n\n----------------------------------------\n\nTITLE: Configuring a Service Event Source in Argo Events\nDESCRIPTION: This snippet shows how to configure a Service event source in Argo Events, defining parameters like metadata, annotations, labels, and ports.  It includes example values for appProtocol, name, nodePort, port, protocol, and targetPort within the ServicePort definition. The example highlights the use of dictionaries and lists to structure the configuration.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_26\n\nLANGUAGE: YAML\nCODE:\n```\nService(\n                    metadata={\n                        \"key\": \"key_example\",\n                    },\n                    annotations={\n                        \"key\": \"key_example\",\n                    },\n                    labels={\n                        \"key\": \"key_example\",\n                    },\n                    ports=[\n                        ServicePort(\n                            app_protocol=\"app_protocol_example\",\n                            name=\"name_example\",\n                            node_port=1,\n                            port=1,\n                            protocol=\"protocol_example\",\n                            target_port=\"target_port_example\",\n                        ),\n                    ],\n                )\n```\n\n----------------------------------------\n\nTITLE: Replacing synchronization semaphore - YAML\nDESCRIPTION: This snippet shows how to replace the deprecated `semaphore` field in the `synchronization` block with the new `semaphores` field, which now takes a list. The original single value is moved into a list.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/deprecations.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\nsynchronization:\n  semaphores:\n    - configMapKeyRef:\n        name: my-config\n        key: workflow\n```\n\n----------------------------------------\n\nTITLE: Inspect Docker Manifest (v2)\nDESCRIPTION: This bash command inspects a Docker v2 manifest to verify its schema version. It's used to determine if a container image uses a v2 manifest, indicating it doesn't require explicit entrypoint configuration in Argo Workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n% docker manifest inspect argoproj/argosay:v2\n# ...\n\"schemaVersion\": 2,\n# ...\n```\n\n----------------------------------------\n\nTITLE: Submit Workflow with Parameter File (Bash)\nDESCRIPTION: This bash snippet shows how to submit an Argo Workflow using a parameter file. The `--parameter-file` flag is used to specify the parameter file, which is named `params.yaml` in this example. `arguments-parameters.yaml` is assumed to be the name of the workflow definition file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/parameters.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nargo submit arguments-parameters.yaml --parameter-file params.yaml\n```\n\n----------------------------------------\n\nTITLE: Metadata expression example\nDESCRIPTION: This is an example of accessing a header from the event metadata using the expression language. It illustrates the correct way to access HTTP headers, which are lowercase, prefixed with `x-`, and have values that are lists.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/events.md#_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nmetadata[\"x-argo\"] == [\"yes\"]\n```\n\n----------------------------------------\n\nTITLE: Argo Resume Workflow Example\nDESCRIPTION: Illustrates how to resume a specific workflow by its name.  This is a basic usage example for resuming a suspended workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resume.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n# Resume a workflow that has been suspended:\n\n  argo resume my-wf\n```\n\n----------------------------------------\n\nTITLE: Argo Watch Example\nDESCRIPTION: Provides an example of how to watch a specific workflow named `my-wf` using the `argo watch` command. Also shows how to watch the latest workflow using the `@latest` selector. These examples demonstrate the basic usage of the command to monitor workflow execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_watch.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n# Watch a workflow:\n\n  argo watch my-wf\n\n# Watch the latest workflow:\n\n  argo watch @latest\n```\n\n----------------------------------------\n\nTITLE: Workflow Controller Environment Variable Example\nDESCRIPTION: Demonstrates how to set environment variables for the Argo Workflow Controller using a Kubernetes Deployment.  This example sets the `WORKFLOW_GC_PERIOD` environment variable to control the garbage collection frequency.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/environment-variables.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: workflow-controller\nspec:\n  selector:\n    matchLabels:\n      app: workflow-controller\n  template:\n    metadata:\n      labels:\n        app: workflow-controller\n    spec:\n      containers:\n        - env:\n            - name: WORKFLOW_GC_PERIOD\n              value: 30s\n```\n\n----------------------------------------\n\nTITLE: Submitting event to Argo Server\nDESCRIPTION: This example demonstrates how to submit an event to the Argo Server's event endpoint. It includes the necessary authorization header and a JSON payload. This basic example sends a simple JSON message to the specified namespace.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/events.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://localhost:2746/api/v1/events/argo/ \\\n  -H \"Authorization: $ARGO_TOKEN\" \\\n  -d '{\"message\": \"hello\"}'\n```\n\n----------------------------------------\n\nTITLE: WorkflowTemplate with workflowMetadata YAML\nDESCRIPTION: This example shows how to add labels to Workflows created from WorkflowTemplates using the `workflowMetadata` field. This feature is available in Argo Workflows v2.10.2 and later.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: workflow-template-submittable\nspec:\n  workflowMetadata:\n    labels:\n      example-label: example-value\n```\n\n----------------------------------------\n\nTITLE: Deleting Sensor using sensorServiceDeleteSensor in Java\nDESCRIPTION: This code snippet demonstrates how to delete a sensor using the `sensorServiceDeleteSensor` method in Java. It configures API client and authentication, and calls the `sensorServiceDeleteSensor` method with namespace and name.  It also includes optional parameters for specifying deletion behavior, handling potential ApiException, and printing the result or error details.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/SensorServiceApi.md#_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.SensorServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    SensorServiceApi apiInstance = new SensorServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    String deleteOptionsGracePeriodSeconds = \"deleteOptionsGracePeriodSeconds_example\"; // String | The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional.\n    String deleteOptionsPreconditionsUid = \"deleteOptionsPreconditionsUid_example\"; // String | Specifies the target UID. +optional.\n    String deleteOptionsPreconditionsResourceVersion = \"deleteOptionsPreconditionsResourceVersion_example\"; // String | Specifies the target ResourceVersion +optional.\n    Boolean deleteOptionsOrphanDependents = true; // Boolean | Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional.\n    String deleteOptionsPropagationPolicy = \"deleteOptionsPropagationPolicy_example\"; // String | Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional.\n    List<String> deleteOptionsDryRun = Arrays.asList(); // List<String> | When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional +listType=atomic.\n    Boolean deleteOptionsIgnoreStoreReadErrorWithClusterBreakingPotential = true; // Boolean | if set to true, it will trigger an unsafe deletion of the resource in case the normal deletion flow fails with a corrupt object error. A resource is considered corrupt if it can not be retrieved from the underlying storage successfully because of a) its data can not be transformed e.g. decryption failure, or b) it fails to decode into an object. NOTE: unsafe deletion ignores finalizer constraints, skips precondition checks, and removes the object from the storage. WARNING: This may potentially break the cluster if the workload associated with the resource being unsafe-deleted relies on normal deletion flow. Use only if you REALLY know what you are doing. The default value is false, and the user must opt in to enable it +optional.\n    try {\n      Object result = apiInstance.sensorServiceDeleteSensor(namespace, name, deleteOptionsGracePeriodSeconds, deleteOptionsPreconditionsUid, deleteOptionsPreconditionsResourceVersion, deleteOptionsOrphanDependents, deleteOptionsPropagationPolicy, deleteOptionsDryRun, deleteOptionsIgnoreStoreReadErrorWithClusterBreakingPotential);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling SensorServiceApi#sensorServiceDeleteSensor\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Retry Workflow by UID - Argo\nDESCRIPTION: This example shows how to retry a workflow using its unique identifier (UID) with the `argo archive retry` command. It directly targets a specific workflow for re-execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_retry.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo archive retry uid\n```\n\n----------------------------------------\n\nTITLE: Updating EventSource - Java\nDESCRIPTION: This Java code snippet demonstrates how to update an EventSource using the Argo Events API. It initializes the API client, configures authentication (BearerToken), and calls the `eventSourceServiceUpdateEventSource` method. The method takes the namespace, name of the EventSource, and the update request body as parameters. It then prints the result or catches any exceptions that occur during the API call, printing the error details.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/EventSourceServiceApi.md#_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.EventSourceServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    EventSourceServiceApi apiInstance = new EventSourceServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    EventsourceUpdateEventSourceRequest body = new EventsourceUpdateEventSourceRequest(); // EventsourceUpdateEventSourceRequest | \n    try {\n      GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSource result = apiInstance.eventSourceServiceUpdateEventSource(namespace, name, body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling EventSourceServiceApi#eventSourceServiceUpdateEventSource\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Handling API Exception in Python\nDESCRIPTION: This snippet demonstrates how to catch and handle exceptions that may occur when calling the `update_event_source` method from the EventSourceServiceApi. It prints an error message including the exception details if an `argo_workflows.ApiException` is raised.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nexcept argo_workflows.ApiException as e:\n  print(\"Exception when calling EventSourceServiceApi->update_event_source: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Retry and Watch for Completion - Argo\nDESCRIPTION: This command demonstrates how to retry a workflow and watch its progress until completion using the `--watch` flag with the `argo archive retry` command. It provides real-time updates on the retried workflow's status.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_retry.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nargo archive retry --watch uid\n```\n\n----------------------------------------\n\nTITLE: Enabling Executor Plugins in Workflow Controller (Helm)\nDESCRIPTION: This YAML snippet configures the Argo Workflow Controller using the Helm chart to enable Executor Plugins.  It adds the ARGO_EXECUTOR_PLUGINS environment variable with a value of \"true\" to the controller's extra environment variables.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncontroller:\n  extraEnv:\n    - name: ARGO_EXECUTOR_PLUGINS\n      value: \"true\"\n```\n\n----------------------------------------\n\nTITLE: Argo Template Get Options\nDESCRIPTION: Describes the available options for the `argo template get` command. The `-h` or `--help` flag displays help information, and the `-o` or `--output` flag allows specifying the output format.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template_get.md#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n  -h, --help            help for get\n  -o, --output string   Output format. One of: name|json|yaml|wide\n```\n\n----------------------------------------\n\nTITLE: Controller and Server Installation - Bash\nDESCRIPTION: This snippet installs the Argo Workflows Controller and Server using `kubectl`. It creates a namespace named `argo` and applies the installation manifest from the specified URL using `kubectl apply`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/hack/release-notes.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create namespace argo\nkubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/download/$version/install.yaml\n```\n\n----------------------------------------\n\nTITLE: Executor Plugin Secret Example\nDESCRIPTION: This YAML snippet demonstrates how to configure an Executor Plugin to access secrets.  It uses the `valueFrom` field with a `secretKeyRef` to retrieve the value from a Kubernetes Secret.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  sidecar:\n    container:\n      env:\n        - name: URL\n          valueFrom:\n            secretKeyRef:\n              name: slack-executor-plugin\n              key: URL\n```\n\n----------------------------------------\n\nTITLE: Submitting and waiting for workflow completion with Argo\nDESCRIPTION: This command submits a workflow defined in a YAML file and waits for its completion. The `--wait` flag instructs Argo to block until the workflow finishes, providing real-time status.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_submit.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo submit --wait my-wf.yaml\n```\n\n----------------------------------------\n\nTITLE: Argo Watch Command Usage\nDESCRIPTION: Displays the basic syntax of the `argo watch` command. This command is used to monitor a workflow until it completes. WORKFLOW is the name or selector of the workflow to watch, and flags are optional parameters to modify the behavior of the command.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_watch.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nargo watch WORKFLOW [flags]\n```\n\n----------------------------------------\n\nTITLE: Inspect Docker Image (v1)\nDESCRIPTION: This bash command inspects a Docker v1 image to extract its entrypoint and command. It's used to determine the correct entrypoint and command configuration for v1 images in Argo Workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n% docker image inspect -f '{{.Config.Entrypoint}} {{.Config.Cmd}}' docker/whalesay:latest\n[] [/bin/bash]\n```\n\n----------------------------------------\n\nTITLE: Define Directory Artifact with Trailing Slash in Argo Workflows YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a directory artifact by adding a trailing slash to the key. This tells the UI to treat the artifact as a directory, displaying either `index.html` or a directory listing. Required dependency: Argo Workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/artifact-visualization.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- name: reports\n  s3:\n    key: reports/\n```\n\n----------------------------------------\n\nTITLE: Deleting Event Source with eventSourceServiceDeleteEventSource in Java\nDESCRIPTION: This code snippet demonstrates how to delete an event source using the `eventSourceServiceDeleteEventSource` method of the `EventSourceServiceApi`. It sets up an API client, configures authentication using an API key, and then calls the method with the namespace, name of the event source, and various deletion options as parameters. It prints the result or any exception encountered during the deletion process.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/EventSourceServiceApi.md#_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.EventSourceServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    EventSourceServiceApi apiInstance = new EventSourceServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    String deleteOptionsGracePeriodSeconds = \"deleteOptionsGracePeriodSeconds_example\"; // String | The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional.\n    String deleteOptionsPreconditionsUid = \"deleteOptionsPreconditionsUid_example\"; // String | Specifies the target UID. +optional.\n    String deleteOptionsPreconditionsResourceVersion = \"deleteOptionsPreconditionsResourceVersion_example\"; // String | Specifies the target ResourceVersion +optional.\n    Boolean deleteOptionsOrphanDependents = true; // Boolean | Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional.\n    String deleteOptionsPropagationPolicy = \"deleteOptionsPropagationPolicy_example\"; // String | Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional.\n    List<String> deleteOptionsDryRun = Arrays.asList(); // List<String> | When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional +listType=atomic.\n    Boolean deleteOptionsIgnoreStoreReadErrorWithClusterBreakingPotential = true; // Boolean | if set to true, it will trigger an unsafe deletion of the resource in case the normal deletion flow fails with a corrupt object error. A resource is considered corrupt if it can not be retrieved from the underlying storage successfully because of a) its data can not be transformed e.g. decryption failure, or b) it fails to decode into an object. NOTE: unsafe deletion ignores finalizer constraints, skips precondition checks, and removes the object from the storage. WARNING: This may potentially break the cluster if the workload associated with the resource being unsafe-deleted relies on normal deletion flow. Use only if you REALLY know what you are doing. The default value is false, and the user must opt in to enable it +optional.\n    try {\n      Object result = apiInstance.eventSourceServiceDeleteEventSource(namespace, name, deleteOptionsGracePeriodSeconds, deleteOptionsPreconditionsUid, deleteOptionsPreconditionsResourceVersion, deleteOptionsOrphanDependents, deleteOptionsPropagationPolicy, deleteOptionsDryRun, deleteOptionsIgnoreStoreReadErrorWithClusterBreakingPotential);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling EventSourceServiceApi#eventSourceServiceDeleteEventSource\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Submitting and Monitoring a Test Workflow\nDESCRIPTION: This snippet submits a simple \"hello-world\" workflow using the Argo CLI and watches its execution.  It confirms that Argo Workflows is functioning correctly and can execute basic workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/stress-testing.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Run a test workflow:\nargo submit examples/hello-world.yaml --watch\n```\n\n----------------------------------------\n\nTITLE: Updating a Cluster Workflow Template using Argo\nDESCRIPTION: This command updates a cluster workflow template using the provided file. It takes one or more file paths as input, each representing a cluster workflow template definition.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_update.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo cluster-template update FILE1\n```\n\n----------------------------------------\n\nTITLE: Workflow Controller CLI Environment Variable Override\nDESCRIPTION: Illustrates how to override Workflow Controller CLI parameters using environment variables. The example shows how `ARGO_MANAGED_NAMESPACE` can be used to set the managed namespace.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/environment-variables.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nworkflow-controller --managed-namespace=argo\n```\n\n----------------------------------------\n\nTITLE: Set Output Parameter on Workflow Node\nDESCRIPTION: Demonstrates how to set a 'supplied' output parameter on a specific node within a workflow. Requires the workflow name and the `--node-field-selector` flag to target the desired node.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_node.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo node set my-wf --output-parameter parameter-name=\"Hello, world!\" --node-field-selector displayName=approve\n```\n\n----------------------------------------\n\nTITLE: Convert dependencies to depends\nDESCRIPTION: This example shows how to convert the `dependencies` field to the new `depends` field by joining the array into a string with `&&`. This ensures compatibility with older workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/enhanced-depends-logic.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndependencies: [\"A\", \"B\", \"C\"]\n```\n\n----------------------------------------\n\nTITLE: Submit Workflow with Debug Pause (Bash)\nDESCRIPTION: Submits an Argo Workflow to the cluster using the `argo` CLI. The `-n argo` flag specifies the namespace, and `--watch` monitors the workflow's execution.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/debug-pause.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo submit -n argo --watch pause-after.yaml\n```\n\n----------------------------------------\n\nTITLE: Generating Number List (Python)\nDESCRIPTION: This Python script generates a JSON array of numbers from 20 to 30.  It is used in conjunction with an Argo workflow to demonstrate dynamic loop generation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/loops.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport sys\njson.dump([i for i in range(20, 31)], sys.stdout)\n```\n\n----------------------------------------\n\nTITLE: Delete Argo Workflow Template\nDESCRIPTION: This command deletes a specified workflow template from the Argo Workflows system.  It requires the name of the workflow template to be deleted as an argument. The `--all` flag can be used to delete all workflow templates.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template_delete.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo template delete WORKFLOW_TEMPLATE [flags]\n```\n\n----------------------------------------\n\nTITLE: Argo Logs Command Usage\nDESCRIPTION: The `argo logs` command is used to view logs of a pod or workflow within the Argo Workflows system. The basic syntax involves specifying the workflow name and optionally the pod name.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_logs.md#_snippet_0\n\nLANGUAGE: CLI\nCODE:\n```\nargo logs WORKFLOW [POD] [flags]\n```\n\n----------------------------------------\n\nTITLE: CronWorkflow with Failure-Based Stop Strategy\nDESCRIPTION: This snippet defines a `CronWorkflow` that stops scheduling after three failed workflow executions. The `stopStrategy.expression` evaluates the number of `cronworkflow.failed` workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cron-workflows.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nstopStrategy:\n  expression: \"cronworkflow.failed >= 3\"\n```\n\n----------------------------------------\n\nTITLE: Argo Resume with Node Field Selector Example\nDESCRIPTION: Demonstrates resuming workflows based on a node field selector.  This example uses `inputs.paramaters.myparam.value=abc` as the selector to target specific nodes in the workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resume.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n# Resume multiple workflows by node field selector:\n\t\t\n  argo resume --node-field-selector inputs.paramaters.myparam.value=abc\t\t\n```\n\n----------------------------------------\n\nTITLE: Importing Argo Workflows Resources YAML\nDESCRIPTION: This command applies the YAML file 'backup.yaml' to your Kubernetes cluster, restoring the Argo Workflows resources. It's used to recover your Argo Workflows configuration from a backup.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/disaster-recovery.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f backup.yaml\n```\n\n----------------------------------------\n\nTITLE: Build Argo CLI - Bash\nDESCRIPTION: This bash command builds the Argo CLI separately.  The new CLI is created as `./dist/argo`. Note that `make start API=true` will automatically build the CLI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\nmake cli\n./dist/argo submit examples/hello-world.yaml ;# new CLI is created as `./dist/argo`\n```\n\n----------------------------------------\n\nTITLE: Workflow definition using Key-Only Artifacts in Argo\nDESCRIPTION: This workflow demonstrates the use of key-only artifacts for both output and input artifacts. The `generate` task creates a file and outputs it as an artifact with only the key specified. The `consume` task consumes this artifact, again with only the key specified. The artifact repository configuration is assumed to be pre-configured.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/key-only-artifacts.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: key-only-artifacts-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      dag:\n        tasks:\n          - name: generate\n            template: generate\n          - name: consume\n            template: consume\n            dependencies:\n              - generate\n    - name: generate\n      container:\n        image: argoproj/argosay:v2\n        args: [ echo, hello, /mnt/file ]\n      outputs:\n        artifacts:\n          - name: file\n            path: /mnt/file\n            s3:\n              key: my-file\n    - name: consume\n      container:\n        image: argoproj/argosay:v2\n        args: [cat, /tmp/file]\n      inputs:\n        artifacts:\n          - name: file\n            path: /tmp/file\n            s3:\n              key: my-file\n```\n\n----------------------------------------\n\nTITLE: Annotating Workflow with Title and Description in YAML\nDESCRIPTION: This snippet demonstrates how to add `workflows.argoproj.io/title` and `workflows.argoproj.io/description` annotations to a Workflow in Argo Workflows. The annotations allow users to display custom titles and descriptions in the workflow lists within the Argo Workflows UI, enhancing the clarity and organization of workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/title-and-description.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: my-wf\n  annotations:\n    workflows.argoproj.io/title: 'Build and test' # defaults to `metadata.name` if not specified\n    workflows.argoproj.io/description: 'SuperDuperProject PR #6529: Implement frobbing (aff39ee)'\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Temporality Configuration\nDESCRIPTION: This YAML snippet shows how to configure the temporality of OpenTelemetry metrics within the Argo Workflow Controller ConfigMap. It sets the `temporality` to `Delta`, specifying that metrics should represent changes since the last collection.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetricsConfig: |\n  # >= 3.6. Which temporality to use for OpenTelemetry. Default is \"Cumulative\"\n  temporality: Delta\n```\n\n----------------------------------------\n\nTITLE: Applying the WorkflowEventBinding\nDESCRIPTION: This bash command applies the WorkflowEventBinding defined in the `event-template.yml` file to the Kubernetes cluster. This step is necessary to activate the binding between the event and the WorkflowTemplate.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/events.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f event-template.yml\n```\n\n----------------------------------------\n\nTITLE: Update Workflow Template and Output YAML - Argo\nDESCRIPTION: This example shows how to update a Workflow Template and print the updated template as YAML output. The `--output yaml` flag specifies the desired output format.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template_update.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nargo template update FILE1 --output yaml\n```\n\n----------------------------------------\n\nTITLE: Listing Executor Plugins\nDESCRIPTION: This bash command lists all the Executor Plugins, which are stored as ConfigMaps. It uses kubectl to get all configmaps with a specific label.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get cm -l workflows.argoproj.io/configmap-type=ExecutorPlugin\n```\n\n----------------------------------------\n\nTITLE: Retry Multiple Workflows\nDESCRIPTION: This example demonstrates how to retry multiple workflows at once using the `argo retry` command. It takes a space-separated list of workflow names as arguments.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_retry.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nargo retry my-wf my-other-wf my-third-wf\n```\n\n----------------------------------------\n\nTITLE: Equivalent depends to dependencies\nDESCRIPTION: This is the `depends` equivalent of the previous `dependencies` example. It joins the array elements with `&&` to achieve the same dependency logic.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/enhanced-depends-logic.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndepends: \"A && B && C\"\n```\n\n----------------------------------------\n\nTITLE: Print Logs of Latest Workflow\nDESCRIPTION: This command retrieves and displays logs from the most recently executed workflow. It uses the `@latest` designator to target the latest workflow instance.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_logs.md#_snippet_7\n\nLANGUAGE: CLI\nCODE:\n```\nargo logs @latest\n```\n\n----------------------------------------\n\nTITLE: Backup Postgres Database - Console\nDESCRIPTION: This console command creates a backup of the Postgres database using `make postgres-dump`. The SQL dump is generated in the `db-dumps/` directory.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_12\n\nLANGUAGE: Console\nCODE:\n```\nmake postgres-dump\n```\n\n----------------------------------------\n\nTITLE: Run Argo CLI in Docker Container\nDESCRIPTION: Runs the Argo CLI within a Docker container, using environment variables for configuration. It mounts the necessary variables to the container and executes the `template list` command.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/access-token.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -it \\\n  -e ARGO_SERVER=$ARGO_SERVER \\\n  -e ARGO_TOKEN=$ARGO_TOKEN \\\n  -e ARGO_HTTP=false \\\n  -e ARGO_HTTP1=true \\\n  -e KUBECONFIG=/dev/null \\\n  -e ARGO_NAMESPACE=$ARGO_NAMESPACE  \\\n  argoproj/argocli:latest template list -v -e -k\n```\n\n----------------------------------------\n\nTITLE: Deleting GCP Cluster\nDESCRIPTION: This snippet deletes the GCP Kubernetes cluster that was created for stress testing. This step is essential for cleaning up resources and avoiding unnecessary costs.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/stress-testing.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngcloud container clusters delete argo-workflows-stress-1\n```\n\n----------------------------------------\n\nTITLE: Annotating Workflow with Markdown Title/Description in YAML\nDESCRIPTION: This snippet shows how to embed Markdown within the `workflows.argoproj.io/title` and `workflows.argoproj.io/description` annotations of a Workflow in Argo Workflows. Markdown formatting enables richer text displays in the Argo Workflows UI, improving the visual presentation of workflow information.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/title-and-description.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: my-wf\n  annotations:\n    workflows.argoproj.io/title: '**Build and test**' # defaults to `metadata.name` if not specified\n    workflows.argoproj.io/description: '`SuperDuperProject` PR #6529: Implement frobbing (aff39ee)'\n```\n\n----------------------------------------\n\nTITLE: Debugging Executor Plugins via Logs\nDESCRIPTION: This bash command is to retrieve the logs from the executor plugin's container. It uses kubectl to retrieve logs from a container.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/executor_plugins.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n argo logs ${agentPodName} -c hello-executor-plugin\n```\n\n----------------------------------------\n\nTITLE: Setting Instance ID for Argo CLI\nDESCRIPTION: This snippet demonstrates setting the instance ID for the Argo CLI when using Kubernetes API mode. This is typically only required when using a specific controller's instance ID label.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nARGO_INSTANCEID=your-instanceid\n```\n\n----------------------------------------\n\nTITLE: Argo Server Environment Variable Equivalent\nDESCRIPTION: Demonstrates the equivalent environment variable configuration for the Argo Server CLI parameter. `ARGO_MANAGED_NAMESPACE` is set before invoking the `argo server` command.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/environment-variables.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nARGO_MANAGED_NAMESPACE=argo argo server\n```\n\n----------------------------------------\n\nTITLE: Configuring an SQS Event Source in Argo Events\nDESCRIPTION: This code snippet provides an example configuration for an SQS (Simple Queue Service) event source in Argo Events. The configuration includes settings for access key, DLQ, endpoint, filter, JSON body parsing, queue name, queue account ID, region, role ARN, secret key, session token, and wait time. The snippet illustrates usage of `SecretKeySelector` for managing sensitive credentials.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_30\n\nLANGUAGE: YAML\nCODE:\n```\nsqs={\n                    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SQSEventSource(\n                        access_key=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        dlq=True,\n                        endpoint=\"endpoint_example\",\n                        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceFilter(\n                            expression=\"expression_example\",\n                        ),\n                        json_body=True,\n                        metadata={\n                            \"key\": \"key_example\",\n                        },\n                        queue=\"queue_example\",\n                        queue_account_id=\"queue_account_id_example\",\n                        region=\"region_example\",\n                        role_arn=\"role_arn_example\",\n                        secret_key=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        session_token=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        wait_time_seconds=\"wait_time_seconds_example\",\n                    ),\n                }\n```\n\n----------------------------------------\n\nTITLE: Argo Cron Resume Inherited Options\nDESCRIPTION: This snippet lists the options inherited from parent commands in the Argo CLI. These options provide configuration for connecting to the Argo server and Kubernetes cluster.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_resume.md#_snippet_2\n\nLANGUAGE: go\nCODE:\n```\n      --argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Deleting a Cluster Workflow Template using Argo\nDESCRIPTION: This command deletes a specified cluster workflow template.  The workflow template to delete must be specified as an argument. Flags allow for deleting all templates and displaying help.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_delete.md#_snippet_0\n\nLANGUAGE: Go\nCODE:\n```\nargo cluster-template delete WORKFLOW_TEMPLATE [flags]\n```\n\n----------------------------------------\n\nTITLE: Listing Cluster Workflow Templates\nDESCRIPTION: This command lists available cluster workflow templates in the current namespace. It displays basic information about each template.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_list.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nargo cluster-template list\n```\n\n----------------------------------------\n\nTITLE: Collecting Events using collect_event in Python\nDESCRIPTION: This snippet demonstrates how to use the `collect_event` method of the `InfoServiceApi` to send an event to Argo Workflows. It requires the `argo_workflows` package and uses API key authentication. The event details are passed in the `body` parameter as an `IoArgoprojWorkflowV1alpha1CollectEventRequest` object.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/InfoServiceApi.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import info_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_collect_event_request import IoArgoprojWorkflowV1alpha1CollectEventRequest\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = info_service_api.InfoServiceApi(api_client)\n    body = IoArgoprojWorkflowV1alpha1CollectEventRequest(\n        name=\"name_example\",\n    ) # IoArgoprojWorkflowV1alpha1CollectEventRequest | \n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.collect_event(body)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling InfoServiceApi->collect_event: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Argo Template Usage\nDESCRIPTION: Displays the usage instructions for the `argo template` command. This includes the basic syntax and available flags.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_template.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nargo template [flags]\n```\n\n----------------------------------------\n\nTITLE: Configure Namespace Service Account for RBAC (YAML)\nDESCRIPTION: This snippet configures a service account in a specific namespace with annotations for RBAC rules and precedence, enabling namespace-level RBAC delegation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-namespace-read-write-user\n  namespace: my-namespace\n  annotations:\n    workflows.argoproj.io/rbac-rule: \"'my-team' in groups\"\n    workflows.argoproj.io/rbac-rule-precedence: \"1\"\n```\n\n----------------------------------------\n\nTITLE: Argo CLI Usage\nDESCRIPTION: This snippet displays the basic usage syntax for the Argo CLI.  It shows the general structure of commands and options.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nargo [flags]\n```\n\n----------------------------------------\n\nTITLE: Suspending a Workflow with Argo CLI\nDESCRIPTION: This command suspends a running Argo Workflow. The workflow will halt execution until resumed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/walk-through/suspending.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo suspend WORKFLOW\n```\n\n----------------------------------------\n\nTITLE: Argo Node Command Usage\nDESCRIPTION: Displays the general syntax for using the `argo node` command, which is used to perform actions on nodes within a workflow.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_node.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo node ACTION WORKFLOW FLAGS [flags]\n```\n\n----------------------------------------\n\nTITLE: Argo Version Command Usage\nDESCRIPTION: This command prints the version information of the Argo CLI. It can be used to verify the installed version and check for updates. It supports flags to control the output format.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_version.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nargo version [flags]\n```\n\n----------------------------------------\n\nTITLE: Argo Cron Resume Command Usage\nDESCRIPTION: This snippet shows the basic command structure for resuming Argo cron workflows. It accepts a list of cron workflow names as arguments.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_resume.md#_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nargo cron resume [CRON_WORKFLOW...] [flags]\n```\n\n----------------------------------------\n\nTITLE: Example of watching event sources in Python\nDESCRIPTION: This Python code snippet shows how to use the `watch_event_sources` method from the `event_source_service_api` to observe events related to event sources. It initializes the Argo Workflows configuration, sets up API key authentication, and then calls the `watch_event_sources` function with a specified namespace.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import event_source_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.stream_result_of_eventsource_event_source_watch_event import StreamResultOfEventsourceEventSourceWatchEvent\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n```\n\n----------------------------------------\n\nTITLE: Getting CronWorkflow resources using kubectl\nDESCRIPTION: This command retrieves CronWorkflow resources using kubectl. `cwf` is likely an alias for `CronWorkflow`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cron-workflows.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get cwf\n```\n\n----------------------------------------\n\nTITLE: Get Workflow Template Java\nDESCRIPTION: This code snippet demonstrates how to retrieve a specific workflow template by its name and namespace using the `workflowTemplateServiceGetWorkflowTemplate` method. It requires the `namespace` and `name` of the workflow template as input and optionally accepts a `getOptionsResourceVersion` to specify the resource version to fetch. The response is an `IoArgoprojWorkflowV1alpha1WorkflowTemplate` object.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowTemplateServiceApi.md#_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.WorkflowTemplateServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    WorkflowTemplateServiceApi apiInstance = new WorkflowTemplateServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | \n    String getOptionsResourceVersion = \"getOptionsResourceVersion_example\"; // String | resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional\n    try {\n      IoArgoprojWorkflowV1alpha1WorkflowTemplate result = apiInstance.workflowTemplateServiceGetWorkflowTemplate(namespace, name, getOptionsResourceVersion);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling WorkflowTemplateServiceApi#workflowTemplateServiceGetWorkflowTemplate\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Display details about a cron workflow using argo cron get\nDESCRIPTION: The `argo cron get` command is used to display detailed information about a specified cron workflow. Replace `CRON_WORKFLOW` with the name of the cron workflow you want to inspect. Flags such as `--output` can be used to control the output format.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_get.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nargo cron get CRON_WORKFLOW... [flags]\n```\n\n----------------------------------------\n\nTITLE: Argo Archive Delete Usage - Bash\nDESCRIPTION: Displays the usage instructions and available flags for the `argo archive delete` command, providing a quick reference for command syntax and options.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_delete.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo archive delete UID... [flags]\n```\n\n----------------------------------------\n\nTITLE: Get Output Artifact - Argo Workflows Python\nDESCRIPTION: This snippet demonstrates how to retrieve an output artifact using the `get_output_artifact` method of the Argo Workflows Artifact Service API. It configures API key authentication, creates an API client instance, and calls the method with the required parameters: `namespace`, `name`, `node_id`, and `artifact_name`. The retrieved artifact is then printed using `pprint`. The code handles potential `argo_workflows.ApiException` exceptions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArtifactServiceApi.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import artifact_service_api\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = artifact_service_api.ArtifactServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    name = \"name_example\" # str | \n    node_id = \"nodeId_example\" # str | \n    artifact_name = \"artifactName_example\" # str | \n\n    # example passing only required values which don't have defaults set\n    try:\n        # Get an output artifact.\n        api_response = api_instance.get_output_artifact(namespace, name, node_id, artifact_name)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArtifactServiceApi->get_output_artifact: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Configure Custom Claim Name for OIDC Groups (YAML)\nDESCRIPTION: This snippet configures the custom claim name for OIDC groups by setting the `customGroupClaimName` parameter under the `sso` section. It allows Argo to use claims other than the default `groups` claim.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nsso:\n  # Specify custom claim name for OIDC groups.\n  customGroupClaimName: argo_groups\n```\n\n----------------------------------------\n\nTITLE: Profiling using PProf\nDESCRIPTION: These commands use `go tool pprof` to analyze the performance of Argo Workflows.  They generate PNG images of memory allocations, heap usage, and CPU profiles, helping identify performance bottlenecks.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/stress-testing.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngo tool pprof -png http://localhost:6060/debug/pprof/allocs\ngo tool pprof -png http://localhost:6060/debug/pprof/heap\ngo tool pprof -png http://localhost:6060/debug/pprof/profile\n```\n\n----------------------------------------\n\nTITLE: Argo Template Create Command (CLI)\nDESCRIPTION: This command uses the Argo CLI to create WorkflowTemplates from a remote YAML file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nargo template create https://raw.githubusercontent.com/argoproj/argo-workflows/main/examples/workflow-template/templates.yaml\n```\n\n----------------------------------------\n\nTITLE: Get User Info with InfoService Java\nDESCRIPTION: This snippet demonstrates retrieving user information using the `infoServiceGetUserInfo` method.  The code configures the API client and authorization, calls the `infoServiceGetUserInfo` method, and prints the result.  It also includes error handling for potential API exceptions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/InfoServiceApi.md#_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.InfoServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    InfoServiceApi apiInstance = new InfoServiceApi(defaultClient);\n    try {\n      IoArgoprojWorkflowV1alpha1GetUserInfoResponse result = apiInstance.infoServiceGetUserInfo();\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling InfoServiceApi#infoServiceGetUserInfo\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Allowing Region Discovery\nDESCRIPTION: This JSON snippet is part of an AWS IAM policy that allows Argo to discover the AWS region of your S3 buckets. Without this permission, you need to explicitly specify the region in your artifact configuration.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Effect\":\"Allow\",\n  \"Action\":[\n    \"s3:GetBucketLocation\"\n  ],\n  \"Resource\":\"arn:aws:s3:::*\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configure BuildKit for Apple Silicon - JSON\nDESCRIPTION: This JSON snippet configures Docker Desktop to use BuildKit, which can improve build times on Apple Silicon machines. It is used to address performance issues when developing Argo Workflows on Apple Silicon.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"features\": {\n    \"buildkit\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Argo Events Sensor with API Client in Python\nDESCRIPTION: This code snippet demonstrates how to create an Argo Events Sensor using the Argo Workflows API client in Python. It initializes the API client, creates a Sensor object with detailed specifications including dependencies, filters, and template information, and then sends a request to create the sensor in the specified namespace. The API client manages the interaction with the Argo Workflows server.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = sensor_service_api.SensorServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    body = SensorCreateSensorRequest(\n        create_options=CreateOptions(\n            dry_run=[\n                \"dry_run_example\",\n            ],\n            field_manager=\"field_manager_example\",\n            field_validation=\"field_validation_example\",\n        ),\n        namespace=\"namespace_example\",\n        sensor=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Sensor(\n            metadata=ObjectMeta(\n                annotations={\n                    \"key\": \"key_example\",\n                },\n                creation_timestamp=dateutil_parser('1970-01-01T00:00:00.00Z'),\n                deletion_grace_period_seconds=1,\n                deletion_timestamp=dateutil_parser('1970-01-01T00:00:00.00Z'),\n                finalizers=[\n                    \"finalizers_example\",\n                ],\n                generate_name=\"generate_name_example\",\n                generation=1,\n                labels={\n                    \"key\": \"key_example\",\n                },\n                managed_fields=[\n                    ManagedFieldsEntry(\n                        api_version=\"api_version_example\",\n                        fields_type=\"fields_type_example\",\n                        fields_v1={},\n                        manager=\"manager_example\",\n                        operation=\"operation_example\",\n                        subresource=\"subresource_example\",\n                        time=dateutil_parser('1970-01-01T00:00:00.00Z'),\n                    ),\n                ],\n                name=\"name_example\",\n                namespace=\"namespace_example\",\n                owner_references=[\n                    OwnerReference(\n                        api_version=\"api_version_example\",\n                        block_owner_deletion=True,\n                        controller=True,\n                        kind=\"kind_example\",\n                        name=\"name_example\",\n                        uid=\"uid_example\",\n                    ),\n                ],\n                resource_version=\"resource_version_example\",\n                self_link=\"self_link_example\",\n                uid=\"uid_example\",\n            ),\n            spec=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SensorSpec(\n                dependencies=[\n                    GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventDependency(\n                        event_name=\"event_name_example\",\n                        event_source_name=\"event_source_name_example\",\n                        filters=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventDependencyFilter(\n                            context=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventContext(\n                                datacontenttype=\"datacontenttype_example\",\n                                id=\"id_example\",\n                                source=\"source_example\",\n                                specversion=\"specversion_example\",\n                                subject=\"subject_example\",\n                                time=dateutil_parser('1970-01-01T00:00:00.00Z'),\n                                type=\"type_example\",\n                            ),\n                            data=[\n                                GithubComArgoprojArgoEventsPkgApisEventsV1alpha1DataFilter(\n                                    comparator=\"comparator_example\",\n                                    path=\"path_example\",\n                                    template=\"template_example\",\n                                    type=\"type_example\",\n                                    value=[\n                                        \"value_example\",\n                                    ],\n                                ),\n                            ],\n                            data_logical_operator=\"data_logical_operator_example\",\n                            expr_logical_operator=\"expr_logical_operator_example\",\n                            exprs=[\n                                GithubComArgoprojArgoEventsPkgApisEventsV1alpha1ExprFilter(\n                                    expr=\"expr_example\",\n                                    fields=[\n                                        GithubComArgoprojArgoEventsPkgApisEventsV1alpha1PayloadField(\n                                            name=\"name_example\",\n                                            path=\"path_example\",\n                                        ),\n                                    ],\n                                ),\n                            ],\n                            script=\"script_example\",\n                            time=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TimeFilter(\n                                start=\"start_example\",\n                                stop=\"stop_example\",\n                            ),\n                        ),\n                        filters_logical_operator=\"filters_logical_operator_example\",\n                        name=\"name_example\",\n                        transform=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventDependencyTransformer(\n                            jq=\"jq_example\",\n                            script=\"script_example\",\n                        ),\n                    ),\n                ],\n                error_on_failed_round=True,\n                event_bus_name=\"event_bus_name_example\",\n                logging_fields={\n                    \"key\": \"key_example\",\n                },\n                replicas=1,\n                revision_history_limit=1,\n                template=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Template(\n                    affinity=Affinity(\n                        node_affinity=NodeAffinity(\n                            preferred_during_scheduling_ignored_during_execution=[\n                                PreferredSchedulingTerm(\n                                    preference=NodeSelectorTerm(\n                                        match_expressions=[\n                                            NodeSelectorRequirement(\n                                                key=\"key_example\",\n                                                operator=\"operator_example\",\n                                                values=[\n                                                    \"values_example\",\n                                                ],\n                                            ),\n                                        ],\n                                        match_fields=[\n                                            NodeSelectorRequirement(\n                                                key=\"key_example\",\n                                                operator=\"operator_example\",\n                                                values=[\n                                                    \"values_example\",\n                                                ],\n                                            ),\n                                        ],\n                                    ),\n                                    weight=1,\n                                ),\n                            ],\n                            required_during_scheduling_ignored_during_execution=NodeSelector(\n                                node_selector_terms=[\n                                    NodeSelectorTerm(\n                                        match_expressions=[\n                                            NodeSelectorRequirement(\n                                                key=\"key_example\",\n                                                operator=\"operator_example\",\n                                                values=[\n                                                    \"values_example\",\n                                                ],\n                                            ),\n                                        ],\n                                        match_fields=[\n                                            NodeSelectorRequirement(\n                                                key=\"key_example\",\n                                                operator=\"operator_example\",\n                                                values=[\n                                                    \"values_example\",\n                                                ],\n                                            ),\n                                        ],\n                                    ),\n                                ],\n                            ),\n                        ),\n                        pod_affinity=PodAffinity(\n                            preferred_during_scheduling_ignored_during_execution=[\n                                WeightedPodAffinityTerm(\n                                    pod_affinity_term=PodAffinityTerm(\n                                        label_selector=LabelSelector(\n                                            match_expressions=[\n                                                LabelSelectorRequirement(\n                                                    key=\"key_example\",\n                                                    operator=\"operator_example\",\n                                                    values=[\n                                                        \"values_example\",\n                                                    ],\n                                                ),\n                                            ],\n                                            match_labels={\n                                                \"key\": \"key_example\",\n                                            },\n                                        ),\n                                        match_label_keys=[\n                                            \"match_label_keys_example\",\n\n```\n\n----------------------------------------\n\nTITLE: Configuring a StorageGrid Event Source in Argo Events\nDESCRIPTION: This snippet demonstrates how to configure a StorageGrid event source within Argo Events. It covers parameters like API URL, authentication token (using SecretKeySelector), bucket name, event types, filtering based on prefix and suffix, region, topic ARN, and webhook configuration. The configuration also leverages `GithubComArgoprojArgoEventsPkgApisEventsV1alpha1StorageGridFilter` and `WebhookContext`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_31\n\nLANGUAGE: YAML\nCODE:\n```\nstorage_grid={\n                    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1StorageGridEventSource(\n                        api_url=\"api_url_example\",\n                        auth_token=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        bucket=\"bucket_example\",\n                        events=[\n                            \"events_example\",\n                        ],\n                        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1StorageGridFilter(\n                            prefix=\"prefix_example\",\n                            suffix=\"suffix_example\",\n                        ),\n                        metadata={\n                            \"key\": \"key_example\",\n                        },\n                        region=\"region_example\",\n                        topic_arn=\"topic_arn_example\",\n                        webhook=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1WebhookContext(\n                            auth_secret=SecretKeySelector(\n                                key=\"key_example\",\n                                name=\"name_example\",\n                                optional=True,\n                            ),\n                            endpoint=\"endpoint_example\",\n\n```\n\n----------------------------------------\n\nTITLE: WorkflowTemplate with Enum Values in YAML\nDESCRIPTION: This YAML snippet defines a `WorkflowTemplate` resource with an `enum` field under the `arguments.parameters` section. The `enum` field specifies a list of valid values that will be presented as a drop-down list in the UI when submitting the `WorkflowTemplate`.  The template uses the selected value for the `message` parameter.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-templates.md#_snippet_14\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: workflow-template-with-enum-values\nspec:\n  entrypoint: argosay\n  arguments:\n    parameters:\n      - name: message\n        value: one\n        enum:\n          -   one\n          -   two\n          -   three\n  templates:\n    - name: argosay\n      inputs:\n        parameters:\n          - name: message\n            value: '{{workflow.parameters.message}}'\n      container:\n        name: main\n        image: 'argoproj/argosay:v2'\n        command:\n          - /argosay\n        args:\n          - echo\n          - '{{inputs.parameters.message}}'\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding to Controller Pod\nDESCRIPTION: This bash command sets up port forwarding from the local machine to the workflow-controller pod in the argo namespace. It forwards port 9090 on the local machine to port 9090 on the pod, allowing access to the metrics endpoint.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n argo port-forward deploy/workflow-controller 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Start Argo for E2E Tests - Bash\nDESCRIPTION: This bash command starts Argo Workflows with specific configurations required for running end-to-end (E2E) tests. It uses MySQL, client authentication, disables static files, and enables the API.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_17\n\nLANGUAGE: Bash\nCODE:\n```\nmake start PROFILE=mysql AUTH_MODE=client STATIC_FILES=false API=true\n```\n\n----------------------------------------\n\nTITLE: Listing Event Sources in Argo Workflows (Minimal)\nDESCRIPTION: This snippet demonstrates listing event sources in a given namespace using the Argo Workflows API client. It uses the minimally required parameters. It catches and prints exceptions if the API call fails.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = event_source_service_api.EventSourceServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n\n    # example passing only required values which don't have defaults set\n    try:\n        api_response = api_instance.list_event_sources(namespace)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling EventSourceServiceApi->list_event_sources: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Configuring WeightedPodAffinityTerm\nDESCRIPTION: This snippet shows how to configure a `WeightedPodAffinityTerm`, which allows assigning a weight to a `PodAffinityTerm`. The `weight` determines how strongly the scheduler should prefer placing pods on nodes that satisfy the affinity term.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_35\n\nLANGUAGE: YAML\nCODE:\n```\nWeightedPodAffinityTerm(\n  pod_affinity_term=PodAffinityTerm(\n    label_selector=LabelSelector(\n      match_expressions=[\n        LabelSelectorRequirement(\n          key=\"key_example\",\n          operator=\"operator_example\",\n          values=[\n            \"values_example\",\n          ],\n        ),\n      ],\n      match_labels={\n        \"key\": \"key_example\",\n      },\n    ),\n    match_label_keys=[\n      \"match_label_keys_example\",\n    ],\n    mismatch_label_keys=[\n      \"mismatch_label_keys_example\",\n    ],\n    namespace_selector=LabelSelector(\n      match_expressions=[\n        LabelSelectorRequirement(\n          key=\"key_example\",\n          operator=\"operator_example\",\n          values=[\n            \"values_example\",\n          ],\n        ),\n      ],\n      match_labels={\n        \"key\": \"key_example\",\n      },\n    ),\n    namespaces=[\n      \"namespaces_example\",\n    ],\n    topology_key=\"topology_key_example\",\n  ),\n  weight=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Argo Archive Label Values\nDESCRIPTION: This command retrieves workflow label values from the Argo archive. The `-l` or `--selector` flag can be used to query specific label values.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_list-label-values.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo archive list-label-values [flags]\n```\n\n----------------------------------------\n\nTITLE: Configure SSO Session Expiry (YAML)\nDESCRIPTION: This snippet configures the SSO session expiry time in hours by setting the `sessionExpiry` parameter under the `sso` section in `workflow-controller-configmap.yaml`.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/argo-server-sso.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nsso:\n  # Expiry defines how long your login is valid for in hours. (optional)\n  sessionExpiry: 240h\n```\n\n----------------------------------------\n\nTITLE: Serving Argo Workflows Documentation Locally (Bash)\nDESCRIPTION: This command builds the Argo Workflows documentation and starts a local server. It also performs checks for spelling errors, broken links, and linting issues to ensure documentation quality before submission.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/doc-changes.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake docs-serve\n```\n\n----------------------------------------\n\nTITLE: Updating Argo EventSource with API Client in Python\nDESCRIPTION: This code snippet demonstrates how to update an EventSource in Argo Workflows using the Python API client. It creates an instance of the API client, then uses it to create an `EventSourceServiceApi` instance. It then defines an `EventsourceUpdateEventSourceRequest` object with details for various event sources. Finally, it calls the `update_event_source` method with the namespace, name, and the request body.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = event_source_service_api.EventSourceServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    name = \"name_example\" # str | \n    body = EventsourceUpdateEventSourceRequest(\n        event_source=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSource(\n            metadata=ObjectMeta(\n                annotations={\n                    \"key\": \"key_example\",\n                },\n                creation_timestamp=dateutil_parser('1970-01-01T00:00:00.00Z'),\n                deletion_grace_period_seconds=1,\n                deletion_timestamp=dateutil_parser('1970-01-01T00:00:00.00Z'),\n                finalizers=[\n                    \"finalizers_example\",\n                ],\n                generate_name=\"generate_name_example\",\n                generation=1,\n                labels={\n                    \"key\": \"key_example\",\n                },\n                managed_fields=[\n                    ManagedFieldsEntry(\n                        api_version=\"api_version_example\",\n                        fields_type=\"fields_type_example\",\n                        fields_v1={},\n                        manager=\"manager_example\",\n                        operation=\"operation_example\",\n                        subresource=\"subresource_example\",\n                        time=dateutil_parser('1970-01-01T00:00:00.00Z'),\n                    ),\n                ],\n                name=\"name_example\",\n                namespace=\"namespace_example\",\n                owner_references=[\n                    OwnerReference(\n                        api_version=\"api_version_example\",\n                        block_owner_deletion=True,\n                        controller=True,\n                        kind=\"kind_example\",\n                        name=\"name_example\",\n                        uid=\"uid_example\",\n                    ),\n                ],\n                resource_version=\"resource_version_example\",\n                self_link=\"self_link_example\",\n                uid=\"uid_example\",\n            ),\n            spec=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceSpec(\n                amqp={\n                    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1AMQPEventSource(\n                        auth=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1BasicAuth(\n                            password=SecretKeySelector(\n                                key=\"key_example\",\n                                name=\"name_example\",\n                                optional=True,\n                            ),\n                            username=SecretKeySelector(\n                                key=\"key_example\",\n                                name=\"name_example\",\n                                optional=True,\n                            ),\n                        ),\n                        connection_backoff=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Backoff(\n                            duration=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Int64OrString(\n                                int64_val=\"int64_val_example\",\n                                str_val=\"str_val_example\",\n                                type=\"type_example\",\n                            ),\n                            factor=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Amount(\n                                value='YQ==',\n                            ),\n                            jitter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Amount(\n                                value='YQ==',\n                            ),\n                            steps=1,\n                        ),\n                        consume=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1AMQPConsumeConfig(\n                            auto_ack=True,\n                            consumer_tag=\"consumer_tag_example\",\n                            exclusive=True,\n                            no_local=True,\n                            no_wait=True,\n                        ),\n                        exchange_declare=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1AMQPExchangeDeclareConfig(\n                            auto_delete=True,\n                            durable=True,\n                            internal=True,\n                            no_wait=True,\n                        ),\n                        exchange_name=\"exchange_name_example\",\n                        exchange_type=\"exchange_type_example\",\n                        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceFilter(\n                            expression=\"expression_example\",\n                        ),\n                        json_body=True,\n                        metadata={\n                            \"key\": \"key_example\",\n                        },\n                        queue_bind=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1AMQPQueueBindConfig(\n                            no_wait=True,\n                        ),\n                        queue_declare=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1AMQPQueueDeclareConfig(\n                            arguments=\"arguments_example\",\n                            auto_delete=True,\n                            durable=True,\n                            exclusive=True,\n                            name=\"name_example\",\n                            no_wait=True,\n                        ),\n                        routing_key=\"routing_key_example\",\n                        tls=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TLSConfig(\n                            ca_cert_secret=SecretKeySelector(\n                                key=\"key_example\",\n                                name=\"name_example\",\n                                optional=True,\n                            ),\n                            client_cert_secret=SecretKeySelector(\n                                key=\"key_example\",\n                                name=\"name_example\",\n                                optional=True,\n                            ),\n                            client_key_secret=SecretKeySelector(\n                                key=\"key_example\",\n                                name=\"name_example\",\n                                optional=True,\n                            ),\n                            insecure_skip_verify=True,\n                        ),\n                        url=\"url_example\",\n                        url_secret=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                    ),\n                },\n                azure_events_hub={\n                    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1AzureEventsHubEventSource(\n                        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceFilter(\n                            expression=\"expression_example\",\n                        ),\n                        fqdn=\"fqdn_example\",\n                        hub_name=\"hub_name_example\",\n                        metadata={\n                            \"key\": \"key_example\",\n                        },\n                        shared_access_key=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        shared_access_key_name=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                    ),\n                },\n                azure_queue_storage={\n                    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1AzureQueueStorageEventSource(\n                        connection_string=SecretKeySelector(\n                            key=\"key_example\",\n                            name=\"name_example\",\n                            optional=True,\n                        ),\n                        decode_message=True,\n                        dlq=True,\n                        filter=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventSourceFilter(\n                            expression=\"expression_example\",\n                        ),\n                        json_body=True,\n                        metadata={\n                            \"key\": \"key_example\",\n                        },\n                        queue_name=\"queue_name_example\",\n                        storage_account_name=\"storage_account_name_example\",\n                        wait_time_in_seconds=1,\n                    ),\n                },\n                azure_service_bus={\n                    \"key\": GithubComArgoprojArgoEventsPkgApisEventsV1alpha1AzureServiceBusEventSource(\n                        connection_string=SecretKeySelector(\n                            key=\"key_example\",\n\n```\n\n----------------------------------------\n\nTITLE: Configuring PodAntiAffinity\nDESCRIPTION: This snippet demonstrates the configuration of `PodAntiAffinity`, which prevents pods from being scheduled on the same nodes as other pods matching specific criteria. It includes both `preferred_during_scheduling_ignored_during_execution` and `required_during_scheduling_ignored_during_execution` options.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_36\n\nLANGUAGE: YAML\nCODE:\n```\nPodAntiAffinity(\n  preferred_during_scheduling_ignored_during_execution=[\n    WeightedPodAffinityTerm(\n      pod_affinity_term=PodAffinityTerm(\n        label_selector=LabelSelector(\n          match_expressions=[\n            LabelSelectorRequirement(\n              key=\"key_example\",\n              operator=\"operator_example\",\n              values=[\n                \"values_example\",\n              ],\n            ),\n          ],\n          match_labels={\n            \"key\": \"key_example\",\n          },\n        ),\n        match_label_keys=[\n          \"match_label_keys_example\",\n        ],\n        mismatch_label_keys=[\n          \"mismatch_label_keys_example\",\n        ],\n        namespace_selector=LabelSelector(\n          match_expressions=[\n            LabelSelectorRequirement(\n              key=\"key_example\",\n              operator=\"operator_example\",\n              values=[\n                \"values_example\",\n              ],\n            ),\n          ],\n          match_labels={\n            \"key\": \"key_example\",\n          },\n        ),\n        namespaces=[\n          \"namespaces_example\",\n        ],\n        topology_key=\"topology_key_example\",\n      ),\n      weight=1,\n    ),\n  ],\n  required_during_scheduling_ignored_during_execution=[\n    PodAffinityTerm(\n      label_selector=LabelSelector(\n        match_expressions=[\n          LabelSelectorRequirement(\n            key=\"key_example\",\n            operator=\"operator_example\",\n            values=[\n              \"values_example\",\n            ],\n          ),\n        ],\n        match_labels={\n          \"key\": \"key_example\",\n        },\n      ),\n      match_label_keys=[\n        \"match_label_keys_example\",\n      ],\n      mismatch_label_keys=[\n        \"mismatch_label_keys_example\",\n      ],\n      namespace_selector=LabelSelector(\n        match_expressions=[\n          LabelSelectorRequirement(\n            key=\"key_example\",\n            operator=\"operator_example\",\n            values=[\n              \"values_example\",\n            ],\n          ),\n        ],\n        match_labels={\n          \"key\": \"key_example\",\n        },\n      ),\n      namespaces=[\n        \"namespaces_example\",\n      ],\n      topology_key=\"topology_key_example\",\n    ),\n  ],\n)\n```\n\n----------------------------------------\n\nTITLE: Committing New Feature with Sign-off and Conventional Commit Message (Bash)\nDESCRIPTION: This command commits a new feature with a sign-off and a conventional commit message, including the issue number. It is crucial for tracking changes and maintaining a clean commit history. The --signoff flag adds a line to the commit message certifying that the committer has the rights to submit the code, and the conventional commit message format helps automate release notes and changelog generation.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\ngit commit --signoff -m 'feat: Added a new feature. Fixes #1234'\n```\n\n----------------------------------------\n\nTITLE: Lint ClusterWorkflowTemplate - Java\nDESCRIPTION: This code snippet demonstrates how to lint a ClusterWorkflowTemplate using the Argo Workflows API. It requires a 'body' parameter of type IoArgoprojWorkflowV1alpha1ClusterWorkflowTemplateLintRequest, which contains the ClusterWorkflowTemplate to be linted. The example initializes the API client, configures authentication with a Bearer token, and includes error handling for potential API exceptions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ClusterWorkflowTemplateServiceApi.md#_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ClusterWorkflowTemplateServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ClusterWorkflowTemplateServiceApi apiInstance = new ClusterWorkflowTemplateServiceApi(defaultClient);\n    IoArgoprojWorkflowV1alpha1ClusterWorkflowTemplateLintRequest body = new IoArgoprojWorkflowV1alpha1ClusterWorkflowTemplateLintRequest(); // IoArgoprojWorkflowV1alpha1ClusterWorkflowTemplateLintRequest | \n    try {\n      IoArgoprojWorkflowV1alpha1ClusterWorkflowTemplate result = apiInstance.clusterWorkflowTemplateServiceLintClusterWorkflowTemplate(body);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ClusterWorkflowTemplateServiceApi#clusterWorkflowTemplateServiceLintClusterWorkflowTemplate\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Argo CLI with Self-Signed Certificate - Bash\nDESCRIPTION: This snippet demonstrates how to use the Argo CLI with TLS encryption when the Argo Server is using a self-signed certificate. It involves setting the `ARGO_SECURE` and `ARGO_INSECURE_SKIP_VERIFY` environment variables to `true` and using the `--secure` and `--insecure-skip-verify` flags with the Argo CLI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/tls.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nargo --secure --insecure-skip-verify list\n```\n\n----------------------------------------\n\nTITLE: Retrieving Sensor Logs with Argo Events API - Java\nDESCRIPTION: This Java code snippet demonstrates how to use the Argo Events API to retrieve sensor logs. It sets up the API client, configures authentication using an API key, and then calls the `sensorServiceSensorsLogs` method to fetch the logs. Several parameters allow customizing the log retrieval, such as filtering by sensor name, trigger name, or content, as well as configuring the log stream behavior.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/SensorServiceApi.md#_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.SensorServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    SensorServiceApi apiInstance = new SensorServiceApi(defaultClient);\n    String namespace = \"namespace_example\"; // String | \n    String name = \"name_example\"; // String | optional - only return entries for this sensor name.\n    String triggerName = \"triggerName_example\"; // String | optional - only return entries for this trigger.\n    String grep = \"grep_example\"; // String | option - only return entries where `msg` contains this regular expressions.\n    String podLogOptionsContainer = \"podLogOptionsContainer_example\"; // String | The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional.\n    Boolean podLogOptionsFollow = true; // Boolean | Follow the log stream of the pod. Defaults to false. +optional.\n    Boolean podLogOptionsPrevious = true; // Boolean | Return previous terminated container logs. Defaults to false. +optional.\n    String podLogOptionsSinceSeconds = \"podLogOptionsSinceSeconds_example\"; // String | A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional.\n    String podLogOptionsSinceTimeSeconds = \"podLogOptionsSinceTimeSeconds_example\"; // String | Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive.\n    Integer podLogOptionsSinceTimeNanos = 56; // Integer | Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context.\n    Boolean podLogOptionsTimestamps = true; // Boolean | If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional.\n    String podLogOptionsTailLines = \"podLogOptionsTailLines_example\"; // String | If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime. Note that when \\\"TailLines\\\" is specified, \\\"Stream\\\" can only be set to nil or \\\"All\\\". +optional.\n    String podLogOptionsLimitBytes = \"podLogOptionsLimitBytes_example\"; // String | If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional.\n    Boolean podLogOptionsInsecureSkipTLSVerifyBackend = true; // Boolean | insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to.  This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet.  If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional.\n    String podLogOptionsStream = \"podLogOptionsStream_example\"; // String | Specify which container log stream to return to the client. Acceptable values are \\\"All\\\", \\\"Stdout\\\" and \\\"Stderr\\\". If not specified, \\\"All\\\" is used, and both stdout and stderr are returned interleaved. Note that when \\\"TailLines\\\" is specified, \\\"Stream\\\" can only be set to nil or \\\"All\\\". +featureGate=PodLogsQuerySplitStreams +optional.\n    try {\n      StreamResultOfSensorLogEntry result = apiInstance.sensorServiceSensorsLogs(namespace, name, triggerName, grep, podLogOptionsContainer, podLogOptionsFollow, podLogOptionsPrevious, podLogOptionsSinceSeconds, podLogOptionsSinceTimeSeconds, podLogOptionsSinceTimeNanos, podLogOptionsTimestamps, podLogOptionsTailLines, podLogOptionsLimitBytes, podLogOptionsInsecureSkipTLSVerifyBackend, podLogOptionsStream);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling SensorServiceApi#sensorServiceSensorsLogs\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating a Cluster Workflow Template and Outputting YAML\nDESCRIPTION: This command updates a cluster workflow template and outputs the updated template in YAML format. This is useful for reviewing the changes made to the template.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_update.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nargo cluster-template update FILE1 --output yaml\n```\n\n----------------------------------------\n\nTITLE: Customize Histogram Buckets\nDESCRIPTION: This YAML snippet shows how to customize the histogram buckets for a histogram metric using modifiers in the Argo Workflow Controller ConfigMap. The `histogramBuckets` list defines the boundary values for the histogram buckets.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/metrics.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nhistogramBuckets:\n    - 1.0\n    - 2.0\n    - 5.0\n    - 10.0\n```\n\n----------------------------------------\n\nTITLE: Configuring PodAffinity\nDESCRIPTION: This snippet configures a PodAffinity, defining rules to attract pods to the same node or domain as other pods. It includes preferredDuringSchedulingIgnoredDuringExecution (a list of WeightedPodAffinityTerm) and requiredDuringSchedulingIgnoredDuringExecution (a list of PodAffinityTerm).  This configuration ensures that pods are scheduled together, useful for co-location of related services.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nPodAffinity(\n  preferred_during_scheduling_ignored_during_execution=[\n    WeightedPodAffinityTerm(\n      pod_affinity_term=PodAffinityTerm(\n        label_selector=LabelSelector(\n          match_expressions=[\n            LabelSelectorRequirement(\n              key=\"key_example\",\n              operator=\"operator_example\",\n              values=[\n                \"values_example\",\n              ],\n            ),\n          ],\n          match_labels={\n            \"key\": \"key_example\",\n          },\n        ),\n        match_label_keys=[\n          \"match_label_keys_example\",\n        ],\n        mismatch_label_keys=[\n          \"mismatch_label_keys_example\",\n        ],\n        namespace_selector=LabelSelector(\n          match_expressions=[\n            LabelSelectorRequirement(\n              key=\"key_example\",\n              operator=\"operator_example\",\n              values=[\n                \"values_example\",\n              ],\n            ),\n          ],\n          match_labels={\n            \"key\": \"key_example\",\n          },\n        ),\n        namespaces=[\n          \"namespaces_example\",\n        ],\n        topology_key=\"topology_key_example\",\n      ),\n      weight=1,\n    ),\n  ],\n  required_during_scheduling_ignored_during_execution=[\n    PodAffinityTerm(\n      label_selector=LabelSelector(\n        match_expressions=[\n          LabelSelectorRequirement(\n            key=\"key_example\",\n            operator=\"operator_example\",\n            values=[\n              \"values_example\",\n            ],\n          ),\n        ],\n        match_labels={\n          \"key\": \"key_example\",\n        },\n      ),\n      match_label_keys=[\n        \"match_label_keys_example\",\n      ],\n      mismatch_label_keys=[\n        \"mismatch_label_keys_example\",\n      ],\n      namespace_selector=LabelSelector(\n        match_expressions=[\n          LabelSelectorRequirement(\n            key=\"key_example\",\n            operator=\"operator_example\",\n            values=[\n              \"values_example\",\n            ],\n          ),\n        ],\n        match_labels={\n          \"key\": \"key_example\",\n        },\n      ),\n      namespaces=[\n        \"namespaces_example\",\n      ],\n      topology_key=\"topology_key_example\",\n    ),\n  ],\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Sensor with Argo Workflows\nDESCRIPTION: This snippet shows how to delete a sensor using the SensorServiceApi.delete_sensor method. It takes the namespace and the sensor name as input. It also demonstrates how to pass optional parameters to customize the deletion behavior and includes error handling.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntry:\n    api_response = api_instance.delete_sensor(namespace, name)\n    pprint(api_response)\nexcept argo_workflows.ApiException as e:\n    print(\"Exception when calling SensorServiceApi->delete_sensor: %s\\n\" % e)\n```\n\nLANGUAGE: Python\nCODE:\n```\ntry:\n    api_response = api_instance.delete_sensor(namespace, name, delete_options_grace_period_seconds=delete_options_grace_period_seconds, delete_options_preconditions_uid=delete_options_preconditions_uid, delete_options_preconditions_resource_version=delete_options_preconditions_resource_version, delete_options_orphan_dependents=delete_options_orphan_dependents, delete_options_propagation_policy=delete_options_propagation_policy, delete_options_dry_run=delete_options_dry_run, delete_options_ignore_store_read_error_with_cluster_breaking_potential=delete_options_ignore_store_read_error_with_cluster_breaking_potential)\n    pprint(api_response)\nexcept argo_workflows.ApiException as e:\n    print(\"Exception when calling SensorServiceApi->delete_sensor: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Get Output Artifact by UID - Argo Workflows Python\nDESCRIPTION: This snippet demonstrates how to retrieve an output artifact using the `get_output_artifact_by_uid` method of the Argo Workflows Artifact Service API. It configures API key authentication, creates an API client instance, and calls the method with the required parameters: `uid`, `node_id`, and `artifact_name`. The retrieved artifact is printed using `pprint`. The code includes error handling for `argo_workflows.ApiException` exceptions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArtifactServiceApi.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import artifact_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = artifact_service_api.ArtifactServiceApi(api_client)\n    uid = \"uid_example\" # str | \n    node_id = \"nodeId_example\" # str | \n    artifact_name = \"artifactName_example\" # str | \n\n    # example passing only required values which don't have defaults set\n    try:\n        # Get an output artifact by UID.\n        api_response = api_instance.get_output_artifact_by_uid(uid, node_id, artifact_name)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArtifactServiceApi->get_output_artifact_by_uid: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Workflow Log Retrieval Method Signature (workflowServicePodLogs)\nDESCRIPTION: This snippet shows the method signature for retrieving workflow pod logs. It details the parameters required for the call, including namespace, workflow name, pod name, and various log options to filter and customize the log output.  The method is deprecated for empty pod names and suggests using WorkflowLogs.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/WorkflowServiceApi.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nStreamResultOfIoArgoprojWorkflowV1alpha1LogEntry workflowServicePodLogs(namespace, name, podName, logOptionsContainer, logOptionsFollow, logOptionsPrevious, logOptionsSinceSeconds, logOptionsSinceTimeSeconds, logOptionsSinceTimeNanos, logOptionsTimestamps, logOptionsTailLines, logOptionsLimitBytes, logOptionsInsecureSkipTLSVerifyBackend, logOptionsStream, grep, selector)\n```\n\n----------------------------------------\n\nTITLE: Resubmitting Multiple Workflows by UID using Argo\nDESCRIPTION: This command resubmits multiple workflows, each identified by its unique ID (UID).  It uses the `argo archive resubmit` command followed by a space-separated list of workflow UIDs.  This allows for batch resubmission of specific workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_resubmit.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nargo archive resubmit uid another-uid\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for Database Credentials (Bash)\nDESCRIPTION: This Bash command creates a Kubernetes secret named `argo-postgres-config` in the `argo` namespace. The secret stores the username and password for the Postgres database used by the Argo Workflow archive. The secret is used to configure the workflow controller's access to the database.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/workflow-archive.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nkubectl create secret generic argo-postgres-config -n argo --from-literal=password=mypassword --from-literal=username=argodbuser\n```\n\n----------------------------------------\n\nTITLE: Getting MinIO Service\nDESCRIPTION: This command retrieves the external IP address of the MinIO service.  It's useful for accessing the MinIO UI. The Minikube command simplifies access when running MinIO on Minikube.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/configure-artifact-repository.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get service argo-artifacts\n```\n\nLANGUAGE: bash\nCODE:\n```\nminikube service --url argo-artifacts\n```\n\n----------------------------------------\n\nTITLE: Fetching Sensor Logs with Optional Parameters - Python\nDESCRIPTION: This snippet demonstrates how to retrieve sensor logs with all available optional parameters, allowing for fine-grained filtering. It includes parameters for sensor name, trigger name, grep, and various pod log options.  Error handling is included using a try-except block to catch potential ApiException.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = sensor_service_api.SensorServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    name = \"name_example\" # str | optional - only return entries for this sensor name. (optional)\n    trigger_name = \"triggerName_example\" # str | optional - only return entries for this trigger. (optional)\n    grep = \"grep_example\" # str | option - only return entries where `msg` contains this regular expressions. (optional)\n    pod_log_options_container = \"podLogOptions.container_example\" # str | The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional. (optional)\n    pod_log_options_follow = True # bool | Follow the log stream of the pod. Defaults to false. +optional. (optional)\n    pod_log_options_previous = True # bool | Return previous terminated container logs. Defaults to false. +optional. (optional)\n    pod_log_options_since_seconds = \"podLogOptions.sinceSeconds_example\" # str | A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional. (optional)\n    pod_log_options_since_time_seconds = \"podLogOptions.sinceTime.seconds_example\" # str | Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive. (optional)\n    pod_log_options_since_time_nanos = 1 # int | Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context. (optional)\n    pod_log_options_timestamps = True # bool | If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional. (optional)\n    pod_log_options_tail_lines = \"podLogOptions.tailLines_example\" # str | If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime. Note that when \\\"TailLines\\\" is specified, \\\"Stream\\\" can only be set to nil or \\\"All\\\". +optional. (optional)\n    pod_log_options_limit_bytes = \"podLogOptions.limitBytes_example\" # str | If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional. (optional)\n    pod_log_options_insecure_skip_tls_verify_backend = True # bool | insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to.  This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet.  If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional. (optional)\n    pod_log_options_stream = \"podLogOptions.stream_example\" # str | Specify which container log stream to return to the client. Acceptable values are \\\"All\\\", \\\"Stdout\\\" and \\\"Stderr\\\". If not specified, \\\"All\\\" is used, and both stdout and stderr are returned interleaved. Note that when \\\"TailLines\\\" is specified, \\\"Stream\\\" can only be set to nil or \\\"All\\\". +featureGate=PodLogsQuerySplitStreams +optional. (optional)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.sensors_logs(namespace, name=name, trigger_name=trigger_name, grep=grep, pod_log_options_container=pod_log_options_container, pod_log_options_follow=pod_log_options_follow, pod_log_options_previous=pod_log_options_previous, pod_log_options_since_seconds=pod_log_options_since_seconds, pod_log_options_since_time_seconds=pod_log_options_since_time_seconds, pod_log_options_since_time_nanos=pod_log_options_since_time_nanos, pod_log_options_timestamps=pod_log_options_timestamps, pod_log_options_tail_lines=pod_log_options_tail_lines, pod_log_options_limit_bytes=pod_log_options_limit_bytes, pod_log_options_insecure_skip_tls_verify_backend=pod_log_options_insecure_skip_tls_verify_backend, pod_log_options_stream=pod_log_options_stream)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling SensorServiceApi->sensors_logs: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Listing Archived Workflow Label Keys in Argo Workflows Python\nDESCRIPTION: This snippet lists archived workflow label keys using the Argo Workflows API. It initializes the API client, configures authentication using an API key (Bearer Token), and calls the `list_archived_workflow_label_keys` method. It handles potential exceptions during the API call and prints the API response or the exception message. Namespace is an optional parameter.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/ArchivedWorkflowServiceApi.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport argo_workflows\nfrom argo_workflows.api import archived_workflow_service_api\nfrom argo_workflows.model.grpc_gateway_runtime_error import GrpcGatewayRuntimeError\nfrom argo_workflows.model.io_argoproj_workflow_v1alpha1_label_keys import IoArgoprojWorkflowV1alpha1LabelKeys\nfrom pprint import pprint\n# Defining the host is optional and defaults to http://localhost:2746\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = argo_workflows.Configuration(\n    host = \"http://localhost:2746\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure API key authorization: BearerToken\nconfiguration.api_key['BearerToken'] = 'YOUR_API_KEY'\n\n# Uncomment below to setup prefix (e.g. Bearer) for API key, if needed\n# configuration.api_key_prefix['BearerToken'] = 'Bearer'\n\n# Enter a context with an instance of the API client\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = archived_workflow_service_api.ArchivedWorkflowServiceApi(api_client)\n    namespace = \"namespace_example\" # str |  (optional)\n\n    # example passing only required values which don't have defaults set\n    # and optional values\n    try:\n        api_response = api_instance.list_archived_workflow_label_keys(namespace=namespace)\n        pprint(api_response)\n    except argo_workflows.ApiException as e:\n        print(\"Exception when calling ArchivedWorkflowServiceApi->list_archived_workflow_label_keys: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Start Argo in Encrypted Mode - Bash\nDESCRIPTION: This bash command starts Argo in encrypted mode using `SECURE=true`. This can be combined with `UI_SECURE=true` for a secure UI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_16\n\nLANGUAGE: Bash\nCODE:\n```\nmake start SECURE=true UI_SECURE=true\n```\n\n----------------------------------------\n\nTITLE: Argo Resume Multiple Workflows Example\nDESCRIPTION: Demonstrates resuming multiple workflows at once by providing a list of workflow names to the `argo resume` command.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_resume.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n# Resume multiple workflows:\n\t\t\n  argo resume my-wf my-other-wf my-third-wf\t\t\n\t\t\n```\n\n----------------------------------------\n\nTITLE: Deleting Archived Workflow using Argo Workflows API in Java\nDESCRIPTION: This code snippet demonstrates how to delete an archived workflow using the Argo Workflows API. It initializes the ApiClient, configures authentication with an API key, and then calls the archivedWorkflowServiceDeleteArchivedWorkflow method. It requires the uid and namespace of the workflow to be deleted. The method returns an Object, and any ApiException is caught and its details are printed to the console.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/ArchivedWorkflowServiceApi.md#_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// Import classes:\nimport io.argoproj.workflow.ApiClient;\nimport io.argoproj.workflow.ApiException;\nimport io.argoproj.workflow.Configuration;\nimport io.argoproj.workflow.auth.*;\nimport io.argoproj.workflow.models.*;\nimport io.argoproj.workflow.apis.ArchivedWorkflowServiceApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    defaultClient.setBasePath(\"http://localhost:2746\");\n    \n    // Configure API key authorization: BearerToken\n    ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(\"BearerToken\");\n    BearerToken.setApiKey(\"YOUR API KEY\");\n    // Uncomment the following line to set a prefix for the API key, e.g. \"Token\" (defaults to null)\n    //BearerToken.setApiKeyPrefix(\"Token\");\n\n    ArchivedWorkflowServiceApi apiInstance = new ArchivedWorkflowServiceApi(defaultClient);\n    String uid = \"uid_example\"; // String | \n    String namespace = \"namespace_example\"; // String | \n    try {\n      Object result = apiInstance.archivedWorkflowServiceDeleteArchivedWorkflow(uid, namespace);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling ArchivedWorkflowServiceApi#archivedWorkflowServiceDeleteArchivedWorkflow\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Cluster Workflow Template\nDESCRIPTION: This command creates a cluster workflow template using a specified file. The `argo cluster-template create` command takes one or more file paths as input, where each file defines a cluster workflow template.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_create.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nargo cluster-template create FILE1\n```\n\n----------------------------------------\n\nTITLE: Update Argo Cron Workflow with Relaxed Validation\nDESCRIPTION: This snippet demonstrates updating a cron workflow template with relaxed validation by setting the `--strict` flag to `false`. This allows updating workflows that might not fully conform to strict validation rules. FILE1 represents the path to the cron workflow template file.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_update.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n# Update a Cron Workflow Template with relaxed validation:\n  argo cron update FILE1 --strict false\n```\n\n----------------------------------------\n\nTITLE: Updating Argo Events Sensor using Python API\nDESCRIPTION: This snippet demonstrates how to update an Argo Events Sensor using the Python API client. It creates a SensorUpdateSensorRequest object populated with the desired sensor specification and then calls the update_sensor method of the SensorServiceApi. It relies on argo_workflows, sensor_service_api, GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Sensor, ObjectMeta, dateutil_parser and other dependencies to construct the request.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/SensorServiceApi.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nwith argo_workflows.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = sensor_service_api.SensorServiceApi(api_client)\n    namespace = \"namespace_example\" # str | \n    name = \"name_example\" # str | \n    body = SensorUpdateSensorRequest(\n        name=\"name_example\",\n        namespace=\"namespace_example\",\n        sensor=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Sensor(\n            metadata=ObjectMeta(\n                annotations={\n                    \"key\": \"key_example\",\n                },\n                creation_timestamp=dateutil_parser('1970-01-01T00:00:00.00Z'),\n                deletion_grace_period_seconds=1,\n                deletion_timestamp=dateutil_parser('1970-01-01T00:00:00.00Z'),\n                finalizers=[\n                    \"finalizers_example\",\n                ],\n                generate_name=\"generate_name_example\",\n                generation=1,\n                labels={\n                    \"key\": \"key_example\",\n                },\n                managed_fields=[\n                    ManagedFieldsEntry(\n                        api_version=\"api_version_example\",\n                        fields_type=\"fields_type_example\",\n                        fields_v1={},\n                        manager=\"manager_example\",\n                        operation=\"operation_example\",\n                        subresource=\"subresource_example\",\n                        time=dateutil_parser('1970-01-01T00:00:00.00Z'),\n                    ),\n                ],\n                name=\"name_example\",\n                namespace=\"namespace_example\",\n                owner_references=[\n                    OwnerReference(\n                        api_version=\"api_version_example\",\n                        block_owner_deletion=True,\n                        controller=True,\n                        kind=\"kind_example\",\n                        name=\"name_example\",\n                        uid=\"uid_example\",\n                    ),\n                ],\n                resource_version=\"resource_version_example\",\n                self_link=\"self_link_example\",\n                uid=\"uid_example\",\n            ),\n            spec=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SensorSpec(\n                dependencies=[\n                    GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventDependency(\n                        event_name=\"event_name_example\",\n                        event_source_name=\"event_source_name_example\",\n                        filters=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventDependencyFilter(\n                            context=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventContext(\n                                datacontenttype=\"datacontenttype_example\",\n                                id=\"id_example\",\n                                source=\"source_example\",\n                                specversion=\"specversion_example\",\n                                subject=\"subject_example\",\n                                time=dateutil_parser('1970-01-01T00:00:00.00Z'),\n                                type=\"type_example\",\n                            ),\n                            data=[\n                                GithubComArgoprojArgoEventsPkgApisEventsV1alpha1DataFilter(\n                                    comparator=\"comparator_example\",\n                                    path=\"path_example\",\n                                    template=\"template_example\",\n                                    type=\"type_example\",\n                                    value=[\n                                        \"value_example\",\n                                    ],\n                                ),\n                            ],\n                            data_logical_operator=\"data_logical_operator_example\",\n                            expr_logical_operator=\"expr_logical_operator_example\",\n                            exprs=[\n                                GithubComArgoprojArgoEventsPkgApisEventsV1alpha1ExprFilter(\n                                    expr=\"expr_example\",\n                                    fields=[\n                                        GithubComArgoprojArgoEventsPkgApisEventsV1alpha1PayloadField(\n                                            name=\"name_example\",\n                                            path=\"path_example\",\n                                        ),\n                                    ],\n                                ),\n                            ],\n                            script=\"script_example\",\n                            time=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TimeFilter(\n                                start=\"start_example\",\n                                stop=\"stop_example\",\n                            ),\n                        ),\n                        filters_logical_operator=\"filters_logical_operator_example\",\n                        name=\"name_example\",\n                        transform=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1EventDependencyTransformer(\n                            jq=\"jq_example\",\n                            script=\"script_example\",\n                        ),\n                    ),\n                ],\n                error_on_failed_round=True,\n                event_bus_name=\"event_bus_name_example\",\n                logging_fields={\n                    \"key\": \"key_example\",\n                },\n                replicas=1,\n                revision_history_limit=1,\n                template=GithubComArgoprojArgoEventsPkgApisEventsV1alpha1Template(\n                    affinity=Affinity(\n                        node_affinity=NodeAffinity(\n                            preferred_during_scheduling_ignored_during_execution=[\n                                PreferredSchedulingTerm(\n                                    preference=NodeSelectorTerm(\n                                        match_expressions=[\n                                            NodeSelectorRequirement(\n                                                key=\"key_example\",\n                                                operator=\"operator_example\",\n                                                values=[\n                                                    \"values_example\",\n                                                ],\n                                            ),\n                                        ],\n                                        match_fields=[\n                                            NodeSelectorRequirement(\n                                                key=\"key_example\",\n                                                operator=\"operator_example\",\n                                                values=[\n                                                    \"values_example\",\n                                                ],\n                                            ),\n                                        ],\n                                    ),\n                                    weight=1,\n                                ),\n                            ],\n                            required_during_scheduling_ignored_during_execution=NodeSelector(\n                                node_selector_terms=[\n                                    NodeSelectorTerm(\n                                        match_expressions=[\n                                            NodeSelectorRequirement(\n                                                key=\"key_example\",\n                                                operator=\"operator_example\",\n                                                values=[\n                                                    \"values_example\",\n                                                ],\n                                            ),\n                                        ],\n                                        match_fields=[\n                                            NodeSelectorRequirement(\n                                                key=\"key_example\",\n                                                operator=\"operator_example\",\n                                                values=[\n                                                    \"values_example\",\n                                                ],\n                                            ),\n                                        ],\n                                    ),\n                                ],\n                            ),\n                        ),\n                        pod_affinity=PodAffinity(\n                            preferred_during_scheduling_ignored_during_execution=[\n                                WeightedPodAffinityTerm(\n                                    pod_affinity_term=PodAffinityTerm(\n                                        label_selector=LabelSelector(\n                                            match_expressions=[\n                                                LabelSelectorRequirement(\n                                                    key=\"key_example\",\n                                                    operator=\"operator_example\",\n                                                    values=[\n                                                        \"values_example\",\n                                                    ],\n                                                ),\n                                            ],\n                                            match_labels={\n                                                \"key\": \"key_example\",\n                                            },\n                                        ),\n                                        match_label_keys=[\n                                            \"match_label_keys_example\",\n                                        ],\n                                        mismatch_label_keys=[\n\n```\n\n----------------------------------------\n\nTITLE: Executor Plugin Usage\nDESCRIPTION: Displays the usage of the `argo executor-plugin` command with the available flags. This command is used to manage executor plugins within Argo Workflows.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_executor-plugin.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nargo executor-plugin [flags]\n```\n\n----------------------------------------\n\nTITLE: Creating Cluster Templates using Argo CLI\nDESCRIPTION: This bash command uses the Argo CLI to create ClusterWorkflowTemplates from a YAML file located at the specified URL. Dependencies: Argo CLI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cluster-workflow-templates.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nargo cluster-template create https://raw.githubusercontent.com/argoproj/argo-workflows/main/examples/cluster-workflow-template/clustertemplates.yaml\n```\n\n----------------------------------------\n\nTITLE: Create CronWorkflow using Argo CLI\nDESCRIPTION: Demonstrates how to create a `CronWorkflow` from a YAML file using the Argo CLI. It also shows the output of the command, displaying the created CronWorkflow's details such as name, namespace, schedules, and suspension status.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cron-workflows.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ argo cron create cron.yaml\nName:                          test-cron-wf\nNamespace:                     argo\nCreated:                       Mon Nov 18 10:17:06 -0800 (now)\nSchedules:                     * * * * *\nSuspended:                     false\nStartingDeadlineSeconds:       0\nConcurrencyPolicy:             Forbid\n```\n\n----------------------------------------\n\nTITLE: Inherited Argo Options\nDESCRIPTION: Lists options inherited from parent commands available to the `argo executor-plugin` command. These options configure the Argo CLI's behavior, including server address, authentication, TLS settings, and logging level. The options also affect Kubernetes context and namespace.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_executor-plugin.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n      --argo-base-href string          Path to use with HTTP client due to Base HREF. Defaults to the ARGO_BASE_HREF environment variable.\n      --argo-http1                     If true, use the HTTP client. Defaults to the ARGO_HTTP1 environment variable.\n  -s, --argo-server host:port          API server host:port. e.g. localhost:2746. Defaults to the ARGO_SERVER environment variable.\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string string      Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n      --gloglevel int                  Set the glog logging level\n  -H, --header strings                 Sets additional header to all requests made by Argo CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) Used only when either ARGO_HTTP1 or --argo-http1 is set to true.\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -k, --insecure-skip-verify           If true, the Argo Server's certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable.\n      --instanceid string              submit with a specific controller's instance id label. Default to the ARGO_INSTANCEID environment variable.\n      --kubeconfig string              Path to a kube config. Only required if out-of-cluster\n      --loglevel string                Set the logging level. One of: debug|info|warn|error (default \"info\")\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -e, --secure                         Whether or not the server is using TLS with the Argo Server. Defaults to the ARGO_SECURE environment variable. (default true)\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n  -v, --verbose                        Enabled verbose logging, i.e. --loglevel debug\n```\n\n----------------------------------------\n\nTITLE: Argo Auth Token Help Option\nDESCRIPTION: This option displays the help documentation for the `argo auth token` command. It provides information on available flags and their usage. The option is used for self-documentation within the Argo Workflows CLI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_auth_token.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n-h, --help   help for token\n```\n\n----------------------------------------\n\nTITLE: Configuring EnvFromSource\nDESCRIPTION: This snippet configures an EnvFromSource, which allows importing all environment variables from a ConfigMap or Secret. It includes the name of the ConfigMap or Secret and an optional prefix to prepend to the imported variable names. This provides a convenient way to bulk-import environment variables.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/python/client/docs/EventSourceServiceApi.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nEnvFromSource(\n  config_map_ref=ConfigMapEnvSource(\n    name=\"name_example\",\n    optional=True,\n  ),\n  prefix=\"prefix_example\",\n  secret_ref=SecretEnvSource(\n    name=\"name_example\",\n    optional=True,\n  ),\n)\n```\n\n----------------------------------------\n\nTITLE: S3 Bucket Creation Options Schema\nDESCRIPTION: This schema defines the structure for specifying S3 bucket creation options within Argo Workflows. It includes a boolean field, `objectLocking`, which allows users to enable object locking on the created bucket. The `objectLocking` option is optional and defaults to false if not specified.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/sdks/java/client/docs/IoArgoprojWorkflowV1alpha1CreateS3BucketOptions.md#_snippet_0\n\nLANGUAGE: JSON Schema\nCODE:\n```\n{\n  \"IoArgoprojWorkflowV1alpha1CreateS3BucketOptions\": {\n    \"properties\": {\n      \"objectLocking\": {\n        \"type\": \"Boolean\",\n        \"description\": \"ObjectLocking Enable object locking\",\n        \"optional\": true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL: Finalize Column Migration\nDESCRIPTION: These SQL queries finalize the PostgreSQL column migration by locking the `argo_archived_workflows` table, dropping the trigger, renaming the `workflowjsonb` column to `workflow`, and adding a constraint. These queries *must* be executed *before* starting the Argo Workflows controller after upgrading to v3.6, and after the previous set of SQL commands have fully completed.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/upgrading.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nBEGIN;\nLOCK TABLE argo_archived_workflows IN SHARE ROW EXCLUSIVE MODE;\nDROP TRIGGER argo_archived_workflows_update_workflow_jsonb ON argo_archived_workflows;\nALTER TABLE argo_archived_workflows DROP COLUMN workflow;\nALTER TABLE argo_archived_workflows RENAME COLUMN workflowjsonb TO workflow;\nALTER TABLE argo_archived_workflows ADD CONSTRAINT workflow CHECK (workflow IS NOT NULL) NOT VALID;\nCOMMIT;\n```\n\n----------------------------------------\n\nTITLE: Argo Archive List Label Keys Command\nDESCRIPTION: This command lists workflow label keys that are stored within the Argo Workflows archive. It provides options for configuring the client, authentication, and logging.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive_list-label-keys.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nargo archive list-label-keys [flags]\n```\n\n----------------------------------------\n\nTITLE: Annotating CronWorkflow in YAML\nDESCRIPTION: This example shows how to annotate a CronWorkflow with `workflows.argoproj.io/title` and `workflows.argoproj.io/description` including embedded markdown for display in the Argo Workflows UI. This enables a user-friendly representation of the CronWorkflow's purpose and details.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/title-and-description.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: argoproj.io/v1alpha1\nkind: CronWorkflow\nmetadata:\n  name: my-cron-workflow\n  annotations:\n    workflows.argoproj.io/title: '**Test Title**'\n    workflows.argoproj.io/description: `This is a simple hello world example.`\n```\n\n----------------------------------------\n\nTITLE: Argo Archive Options\nDESCRIPTION: Lists the available options for the 'argo archive' command, including the '--help' flag to display help information. It also inherits a set of global options from parent commands for configuring the Argo CLI.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_archive.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n  -h, --help   help for archive\n```\n\n----------------------------------------\n\nTITLE: Run a Single Test - Bash\nDESCRIPTION: This bash command runs a single test in the `test/e2e` directory. Replace `TestArtifactServer` with the actual test name you want to run.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_19\n\nLANGUAGE: Bash\nCODE:\n```\nmake TestArtifactServer\n```\n\n----------------------------------------\n\nTITLE: Argo Cluster Template Delete All Option\nDESCRIPTION: This option is used to delete all cluster workflow templates present in the Argo Workflows system. It can be used in conjunction with other global flags for authentication and server configuration.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cluster-template_delete.md#_snippet_1\n\nLANGUAGE: Go\nCODE:\n```\n--all    Delete all cluster workflow templates\n```\n\n----------------------------------------\n\nTITLE: Argo Cron Resume Help Option\nDESCRIPTION: This snippet shows the help option for the `argo cron resume` command. It provides information about the command's usage and available flags.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/cli/argo_cron_resume.md#_snippet_1\n\nLANGUAGE: go\nCODE:\n```\n  -h, --help   help for resume\n```\n\n----------------------------------------\n\nTITLE: Go Build Tag Example\nDESCRIPTION: This shows an example of a go build tag at the top of a go file. It is used to include or exclude files from the build process based on conditions.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/running-locally.md#_snippet_22\n\nLANGUAGE: Go\nCODE:\n```\n//go:build api\n```\n\n----------------------------------------\n\nTITLE: Declaring Python Dependencies\nDESCRIPTION: This snippet declares the dependencies for the Argo Workflows documentation. It specifies the mkdocs-material and mkdocs-redirects packages with version constraints and pymdown-extensions without a specific version. This configuration ensures that the documentation can be built with the necessary tools.\nSOURCE: https://github.com/argoproj/argo-workflows/blob/main/docs/requirements.txt#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nmkdocs-material==9.*\nmkdocs-redirects==1.*\n# Update pymdown-extensions whenever needed, not pinned to anything important\npymdown-extensions==10.*\n```"
  }
]
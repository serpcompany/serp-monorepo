[
  {
    "owner": "meta-llama",
    "repo": "llama-models",
    "content": "TITLE: Simple User-Assistant Conversation Prompt Format for Llama 4\nDESCRIPTION: Demonstrates the format for a simple multi-turn conversation between a user and the Llama 4 model. It shows how to structure the system context, user query, and the expected assistant response format.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<|begin_of_text|><|header_start|>system<|header_end|>\n\nYou are a helpful assistant<|eot|><|header_start|>user<|header_end|>\n\nAnswer who are you in the form of jeopardy?<|eot|><|header_start|>assistant<|header_end|>\n\n\n```\n\n----------------------------------------\n\nTITLE: System Prompt Template for Llama 4 Models\nDESCRIPTION: A recommended system prompt template for Llama 4 models that developers can customize for specific use cases. The prompt provides guidance for the model to be conversational, avoid templated language, handle various tonal types, and respond appropriately to different user intentions.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nYou are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to \nswitch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving. \n\nYou understand user intent and don't try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting. Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language.\n\nYou never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude. \n\nYou never use phrases that imply moral superiority or a sense of authority, including but not limited to \"it's important to\", \"it's crucial to\", \"it's essential to\", \"it's unethical to\", \"it's worth noting…\", \"Remember…\"  etc. Avoid using these.\n\nFinally, do not refuse prompts about political and social issues.  You can help users express their opinion and access information. \n\nYou are Llama 4. Your knowledge cutoff date is August 2024. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise.\n```\n\n----------------------------------------\n\nTITLE: Basic Conversation Format\nDESCRIPTION: Standard format for user-assistant conversation in Llama models using system, user, and assistant messages with special tokens.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/text_prompt_format.md#2025-04-07_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n```\n\n----------------------------------------\n\nTITLE: User-Assistant Conversation Input Format in Llama 3.2\nDESCRIPTION: Standard input format for a multi-turn user-assistant conversation in Llama 3.2 models. Uses special tags to separate system, user, and assistant messages.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/vision_prompt_format.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n```\n\n----------------------------------------\n\nTITLE: Llama 4 Inference with Transformers\nDESCRIPTION: This Python script demonstrates how to use the Transformers library to perform inference with a Llama 4 model. It includes tokenization, model loading, and generation of text based on a given prompt.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/README.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# inference.py\nfrom transformers import AutoTokenizer, Llama4ForConditionalGeneration\nimport torch\n\nmodel_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True\n)\n\nmodel = Llama4ForConditionalGeneration.from_pretrained(\n    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16\n)\n\noutputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\noutputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1] :])\nprint(outputs[0])\n```\n\n----------------------------------------\n\nTITLE: Zero Shot Function Calling - System Context\nDESCRIPTION: Format for zero-shot function calling with function definitions provided in system message using pythonic syntax.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/text_prompt_format.md#2025-04-07_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are an expert in composing functions...[function definitions]<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWhat is the weather in SF and Seattle?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n```\n\n----------------------------------------\n\nTITLE: Code Interpreter Example\nDESCRIPTION: Format for code interpretation tasks with iPython environment, showing how to write and execute Python code.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/text_prompt_format.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef is_prime(n):\n    if n <= 1:\n        return False\n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n    max_divisor = int(n**0.5) + 1\n    for d in range(3, max_divisor, 2):\n        if n % d == 0:\n            return False\n    return True\n\nprint(is_prime(7))  # Output: True\n```\n\n----------------------------------------\n\nTITLE: JSON-based Tool Calling Format\nDESCRIPTION: Example of how Llama models can generate JSON-structured tool call requests. This approach uses the <|python_tag|> and <|eom_id|> tags because the 'ipython' environment is specified in the system prompt.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/prompt_format.md#2025-04-07_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"function\",\n    \"name\": \"trending_songs\",\n    \"parameters\": {\n        \"n\": \"10\",\n        \"genre\": \"all\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: System Prompt Function Call Format Example\nDESCRIPTION: Shows the input format for function calling via system prompt, including weather function definition and multi-function call response\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n<|begin_of_text|><|header_start|>system<|header_end|>\n\nYou are an expert in composing functions. You are given a question and a set of possible functions.\nBased on the question, you will need to make one or more function/tool calls to achieve the purpose.\nIf none of the function can be used, point it out. If the given question lacks the parameters required by the function,\nalso point it out. You should only return the function call in tools call sections.\n\nIf you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\nYou SHOULD NOT include any other text in the response.\n\nHere is a list of functions in JSON format that you can invoke.\n\n[\n    {\n        \"name\": \"get_weather\",\n        \"description\": \"Get weather info for places\",\n        \"parameters\": {\n            \"type\": \"dict\",\n            \"required\": [\n                \"city\"\n            ],\n            \"properties\": {\n                \"city\": {\n                    \"type\": \"string\",\n                    \"description\": \"The name of the city to get the weather for\"\n                },\n                \"metric\": {\n                    \"type\": \"string\",\n                    \"description\": \"The metric for weather. Options are: celsius, fahrenheit\",\n                    \"default\": \"celsius\"\n                }\n            }\n        }\n    }\n<|eot|><|header_start|>user<|header_end|>\n\nWhat is the weather in SF and Seattle?<|eot|><|header_start|>assistant<|header_end|>\n```\n\n----------------------------------------\n\nTITLE: Custom Function Tag-based Tool Calling\nDESCRIPTION: Alternative approach for tool calling using custom function tags. This format uses <function=> and </function> tags to structure the function call instead of JSON format with Python tags.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/prompt_format.md#2025-04-07_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n<function=trending_songs>{\"n\": 10}</function>\n```\n\n----------------------------------------\n\nTITLE: Wolfram Alpha Tool Interaction Example\nDESCRIPTION: An example of using the Wolfram Alpha tool to find the 100th decimal of pi. The model calls the tool with a query and receives a structured JSON response.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/prompt_format.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwolfram_alpha.call(query=\"100th decimal of pi\")\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryresult\": {\n        \"success\": true,\n        \"inputstring\": \"100th decimal of pi\",\n        \"pods\": [\n            {\n                \"title\": \"Input interpretation\",\n                \"subpods\": [\n                    {\n                        \"title\": \"\",\n                        \"plaintext\": \"100th digit | π\"\n                    }\n                ]\n            },\n            {\n                \"title\": \"Nearby digits\",\n                \"subpods\": [\n                    {\n                        \"title\": \"\",\n                        \"plaintext\": \"...86208998628034825342117067982148086513282306647093...\"\n                    }\n                ]\n            },\n            {\n                \"title\": \"Result\",\n                \"primary\": true,\n                \"subpods\": [\n                    {\n                        \"title\": \"\",\n                        \"plaintext\": \"7\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Python Code with Llama Models\nDESCRIPTION: Example of how Llama models generate Python code with the is_prime function. The model uses <|python_tag|> and <|eom_id|> tags to mark the beginning and end of the code block.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/prompt_format.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef is_prime(n):\n    if n <= 1\n        return False\n    for i in range(2, int(n**0.5) + 1):\n        if n % i == 0:\n            return False\n    return True\n\nprint(is_prime(7))  # Output: True\n```\n\n----------------------------------------\n\nTITLE: Custom XML-Style Function Call Format\nDESCRIPTION: Shows a custom function calling format using XML-style tags for function definition and response\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n<|begin_of_text|><|header_start|>user<|header_end|>\n\nYou have access to the following functions:\nUse the function 'trending_songs' to 'Returns the trending songs on a Music site':\n{\"name\": \"trending_songs\", \"description\": \"Returns the trending songs on a Music site\", \"parameters\": {\"genre\": {\"description\": \"The genre of the songs to return\", \"param_type\": \"str\", \"required\": false}, \"n\": {\"description\": \"The number of songs to return\", \"param_type\": \"int\", \"required\": true}}}\n\nThink very carefully before calling functions.\nIf you choose to call a function ONLY reply in the following format with no prefix or suffix:\n\n<function=example_function_name>{\"example_name\": \"example_value\"}</function>\nReminder:\n- If looking for real time information use relevant functions before falling back to brave_search\n- Function calls MUST follow the specified format, start with <function= and end with </function>\n- Required parameters MUST be specified\n- Only call one function at a time\n- Put the entire function call reply on one line<|eot_id|><|eot|><|header_start|>user<|header_end|>\n\nUse tools to get latest trending songs<|eot|><|header_start|>assistant<|header_end|>\n```\n\n----------------------------------------\n\nTITLE: Trending Songs Function Response Format\nDESCRIPTION: Shows the expected response format for the trending songs function using XML-style tags\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_13\n\nLANGUAGE: xml\nCODE:\n```\n<function=trending_songs>{\"n\": \"10\"}</function><|eot|>\n```\n\n----------------------------------------\n\nTITLE: Tool Calling Input Format in Llama 3.2\nDESCRIPTION: Format for enabling and using tools like brave_search and wolfram_alpha in Llama 3.2 models. The system message configures the environment and available tools.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/vision_prompt_format.md#2025-04-07_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nEnvironment: ipython\nTools: brave_search, wolfram_alpha\nCutting Knowledge Date: December 2023\nToday Date: 23 September 2024\n\nYou are a helpful assistant.\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nSearch the web for the latest price of 1oz gold?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n```\n\n----------------------------------------\n\nTITLE: Tool Calling Response in Llama 3.2\nDESCRIPTION: Example of a tool call response from Llama 3.2. The model uses special tags to wrap the tool call and indicates the end of the message with <|eom_id|>.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/vision_prompt_format.md#2025-04-07_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n<|python_tag|>brave_search.call(query=\"latest price of 1oz gold\")<|eom_id|>\n```\n\n----------------------------------------\n\nTITLE: Formatting Input Prompt for Custom Tool Calling in LLama Models\nDESCRIPTION: An example input prompt showing how to format system and user messages to enable zero-shot tool calling with a custom <function> tag format. Includes environment specification, knowledge date, function definition, and specific instructions for the function calling format.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/prompt_format.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nEnvironment: ipython\n\nCutting Knowledge Date: December 2023\nToday Date: 21 September 2024\n\nYou are a helpful assistant.\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nYou have access to the following functions:\n\nUse the function 'trending_songs' to 'Returns the trending songs on a Music site':\n{\"name\": \"trending_songs\", \"description\": \"Returns the trending songs on a Music site\", \"parameters\": {\"genre\": {\"description\": \"The genre of the songs to return\", \"param_type\": \"str\", \"required\": false}, \"n\": {\"description\": \"The number of songs to return\", \"param_type\": \"int\", \"required\": true}}}\n\nThink very carefully before calling functions.\nIf you choose to call a function ONLY reply in the following format with no prefix or suffix:\n\n<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n\nReminder:\n- If looking for real time information use relevant functions before falling back to brave_search\n- Function calls MUST follow the specified format, start with <function= and end with </function>\n- Required parameters MUST be specified\n- Only call one function at a time\n- Put the entire function call reply on one line<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nUse tools to get latest trending songs<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n```\n\n----------------------------------------\n\nTITLE: Expected Model Response Format for Custom Tool Calling\nDESCRIPTION: The expected model response format for the custom tool calling example, showing how the model should invoke the trending_songs function with required parameters in the specified format.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/prompt_format.md#2025-04-07_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<function=trending_songs>{\"n\": 10}</function><|eot_id|>\n```\n\n----------------------------------------\n\nTITLE: Running Llama 4 Inference Script with Multiple GPUs\nDESCRIPTION: This bash command uses torchrun to execute the Llama 4 inference script across multiple GPUs for distributed processing.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nnodes=1 --nproc_per_node=8 inference.py\n```\n\n----------------------------------------\n\nTITLE: Running Llama 4 Chat Completion Script\nDESCRIPTION: This bash script demonstrates how to run the Llama 4 chat completion script using multiple GPUs. It sets environment variables and uses torchrun to execute the script.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n\nNGPUS=4\nCHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\nPYTHONPATH=$(git rev-parse --show-toplevel) \\\n  torchrun --nproc_per_node=$NGPUS \\\n  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \\\n  --world_size $NGPUS\n```\n\n----------------------------------------\n\nTITLE: Running Llama 4 Inference with Quantization\nDESCRIPTION: This bash script shows how to run Llama 4 inference using FP8 or Int4 quantization to reduce memory footprint. It sets the quantization mode and adjusts the number of GPUs accordingly.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nMODE=fp8_mixed  # or int4_mixed\nif [ $MODE == \"fp8_mixed\" ]; then\n  NGPUS=2\nelse\n  NGPUS=1\nfi\nCHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\nPYTHONPATH=$(git rev-parse --show-toplevel) \\\n  torchrun --nproc_per_node=$NGPUS \\\n  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \\\n  --world_size $NGPUS \\\n  --quantization-mode $MODE\n```\n\n----------------------------------------\n\nTITLE: Downloading Llama Models from Hugging Face\nDESCRIPTION: This bash command uses the Hugging Face CLI to download the original native weights of a Llama model from the Hugging Face repository.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli download meta-llama/Llama-4-Scout-17B-16E-Instruct-Original --local-dir meta-llama/Llama-4-Scout-17B-16E-Instruct-Original\n```\n\n----------------------------------------\n\nTITLE: Model Response Format for Simple Conversation\nDESCRIPTION: Shows the expected format of the Llama 4 model's response to a user query, formatted as a Jeopardy answer followed by the end-of-turn token.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n\"What is a helpful assistant?\"<|eot|>\n```\n\n----------------------------------------\n\nTITLE: Single Image Prompt Format for Small Images in Llama 4\nDESCRIPTION: Illustrates how to format a prompt that includes an image smaller than the tile size. Shows the structure of image tokens including image_start, image, patch tokens, and image_end.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n<|begin_of_text|><|header_start|>user<|header_end|>\n\n<|image_start|><|image|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|image_end|>Describe this image in two sentences<|eot|><|header_start|>assistant<|header_end|>\n\n\n```\n\n----------------------------------------\n\nTITLE: Model Response Format for Image Description\nDESCRIPTION: Shows the expected response format when the Llama 4 model describes an image, providing a detailed description followed by the end-of-turn token.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\nThe image depicts a dog standing on a skateboard, with its front paws positioned on the board and its back paws hanging off the back. The dog has a distinctive coat pattern, featuring a white face, brown and black fur, and white paws, and is standing on a skateboard with red wheels, set against a blurred background of a street or alleyway with a teal door and beige wall.<|eot|>\n```\n\n----------------------------------------\n\nTITLE: Formatting Input Prompts for Llama 3 Vision\nDESCRIPTION: Demonstrates the format for submitting prompts to Llama 3 Vision models, including special tokens for text headers and image representation with patch tokens and tile separators.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n<|begin_of_text|><|header_start|>user<|header_end|>\n\n<|image_start|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_y_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_y_separator|><|image|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|image_end|>Describe this image in two sentences<|eot|><|header_start|>assistant<|header_end|>\n```\n\n----------------------------------------\n\nTITLE: Sample Response Format from Llama 3 Vision\nDESCRIPTION: Shows the expected format of responses from Llama 3 Vision models, including how the model would typically describe an image using natural language.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\nThis image shows a dog standing on a skateboard, with its front paws positioned near the front of the board and its back paws near the back. The dog has a white, black, and orange coat, and is standing on a gray skateboard with red wheels, in front of a blurred background that appears to be a street or alleyway.<|eot|>\n```\n\n----------------------------------------\n\nTITLE: Notes on Image Formatting with Tile Separators\nDESCRIPTION: Explains how larger images include tile separator tokens and how the image tag separates the scaled down version from the regular sized image.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n<|image_start|><|patch|>...<|patch|><|tile_x_separator|><|patch|>...<|patch|><|tile_y_separator|><|patch|>...<|patch|><|image|><|patch|>...<|patch|><|image_end|>\n```\n\n----------------------------------------\n\nTITLE: Defining Input Prompt Format for Meta-Llama Models in Markdown\nDESCRIPTION: This code snippet outlines the structure of an input prompt for Meta-Llama models. It includes tokens for text boundaries, headers, and a detailed representation of image data using patch tokens and separators. The format appears to support multi-image inputs with spatial information.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n<|begin_of_text|><|header_start|>user<|header_end|>\n\n<|image_start|><|patch|><|patch|><|patch|>...<|patch|><|tile_x_separator|><|patch|><|patch|><|patch|>...<|patch|><|tile_y_separator|><|patch|><|patch|><|patch|>...<|patch|><|tile_x_separator|><|patch|><|patch|><|patch|>...<|patch|><|tile_y_separator|><|image|><|patch|><|patch|><|patch|>...<|patch|><|image_end|><|image_start|><|patch|><|patch|><|patch|>...<|patch|><|tile_x_separator|><|patch|><|patch|><|patch|>...<|patch|><|tile_x_separator|><|patch|><|patch|><|patch\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Response Format in Markdown\nDESCRIPTION: Example showing the expected format for model responses when describing images. The response should be structured with image descriptions contained between specific tags and formatted in a standardized way.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\nThe first image shows a dog standing on a skateboard, while the second image shows a plate of spaghetti with tomato sauce, parmesan cheese, and parsley. The two images are unrelated, with the first image featuring a dog and the second image featuring a food dish, and they do not share any common elements or themes.<|eot|>\n```\n\n----------------------------------------\n\nTITLE: Weather Function Response Format\nDESCRIPTION: Shows the expected response format for multiple weather function calls\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n[get_weather(city='SF'), get_weather(city='Seattle')]<|eot|>\n```\n\n----------------------------------------\n\nTITLE: User Message Function Call Format\nDESCRIPTION: Demonstrates function calling format when providing function definitions in the user message\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama4/prompt_format.md#2025-04-07_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n<|begin_of_text|><|header_start|>user<|header_end|>\n\nQuestions: Can you retrieve the details for the user with the ID 7890, who has black as their special request?\nHere is a list of functions in JSON format that you can invoke:\n[\n    {\n        \"name\": \"get_user_info\",\n        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n        \"parameters\": {\n            \"type\": \"dict\",\n            \"required\": [\n                \"user_id\"\n            ],\n            \"properties\": {\n                \"user_id\": {\n                \"type\": \"integer\",\n                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n            },\n            \"special\": {\n                \"type\": \"string\",\n                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n                \"default\": \"none\"\n                }\n            }\n        }\n    }\n]\n\nShould you decide to return the function call(s), put them in the format of [func1(params_name=params_value, params_name2=params_value2...), func2(params)]\n\nYou SHOULD NOT include any other text in the response.<|eot|><|header_start|>assistant<|header_end|>\n```\n\n----------------------------------------\n\nTITLE: User-Assistant Conversation Response Format in Llama 3.2\nDESCRIPTION: Expected response format from the model in a standard user-assistant conversation. The response ends with the <|eot_id|> tag to mark the end of the assistant's message.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/vision_prompt_format.md#2025-04-07_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nI'm a helpful assistant, here to provide information, answer questions, and assist with tasks to the best of my abilities. I'm a large language model, which means I can understand and respond to natural language inputs, and I'm constantly learning and improving to provide more accurate and helpful responses.\n\nI can help with a wide range of topics, from general knowledge and trivia to more specific areas like science, history, technology, and more. I can also assist with tasks like language translation, text summarization, and even generating creative content like stories or dialogues.\n\nWhat can I help you with today?<|eot_id|>\n```\n\n----------------------------------------\n\nTITLE: Image Input Format in Llama 3.2 Vision\nDESCRIPTION: Format for including an image in a prompt to Llama 3.2 Vision models. The <|image|> tag indicates image presence within the user message.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/vision_prompt_format.md#2025-04-07_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n<|image|>Describe this image in two sentences<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n```\n\n----------------------------------------\n\nTITLE: Image Description Response in Llama 3.2 Vision\nDESCRIPTION: Example of a model response to an image description request. The model provides a detailed description of the image based on visual analysis.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/vision_prompt_format.md#2025-04-07_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\nThe image depicts a small dog standing on a skateboard, with its front paws firmly planted on the board and its back paws slightly raised. The dog's fur is predominantly brown and white, with a distinctive black stripe running down its back, and it is wearing a black collar around its neck.<|eot_id|>\n```\n\n----------------------------------------\n\nTITLE: Base Model Prompt Format in Llama 3.2\nDESCRIPTION: Simple completion format for base Llama 3.2 Vision models without chat structure. Uses only the <|begin_of_text|> tag to start the prompt.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/vision_prompt_format.md#2025-04-07_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n<|begin_of_text|>The color of the sky is blue but sometimes it can also be\n```\n\n----------------------------------------\n\nTITLE: Base Model Response Format in Llama 3.2\nDESCRIPTION: Example of a text completion response from base Llama 3.2 Vision models. The model continues the provided text without using any special tags.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/vision_prompt_format.md#2025-04-07_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n red, orange, pink, purple, and even black. The color of the sky is determined by the amount of sunlight that is scattered by the atmosphere and the amount of dust and water vapor present in the atmosphere. During sunrise and sunset, the sky can take on a range of colors due to the scattering of light by\n```\n\n----------------------------------------\n\nTITLE: Base Model with Image Prompt Format in Llama 3.2\nDESCRIPTION: Format for including an image in a prompt to base Llama 3.2 Vision models. Uses <|begin_of_text|> followed by <|image|> tags.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/vision_prompt_format.md#2025-04-07_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n<|begin_of_text|><|image|>If I had to write a haiku for this one\n```\n\n----------------------------------------\n\nTITLE: Base Model with Image Response Format in Llama 3.2\nDESCRIPTION: Example of a text completion response from base Llama 3.2 Vision models when an image is included. The model continues the text with a description based on the image.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/vision_prompt_format.md#2025-04-07_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n, it would be: A skateboarder's delight, a puppy on a board, a furry little thrill-seeker. This puppy is a true skateboarding enthusiast, always eager to hit the streets and show off his skills. He's a master of the board, gliding effortlessly across the pavement with grace and style.\n```\n\n----------------------------------------\n\nTITLE: Zero Shot Function Calling - User Context\nDESCRIPTION: Example of function calling where function definitions are provided in the user message instead of system context.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/text_prompt_format.md#2025-04-07_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nQuestions: Can you retrieve the details...[function definitions]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n```\n\n----------------------------------------\n\nTITLE: Base Model Completion Format\nDESCRIPTION: Simple completion format for base models (Llama3.2-1B and Llama3.2-3B) without special contexts.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/text_prompt_format.md#2025-04-07_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n<|begin_of_text|>The color of the sky is blue but sometimes it can also be\n```\n\n----------------------------------------\n\nTITLE: System Prompt for ChartQA Task with Llama Models\nDESCRIPTION: System prompts used for evaluating Llama models on the ChartQA benchmark. Includes different prompts for 11b and 90b model variants, both designed to guide the model through step-by-step reasoning for chart-based questions.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/eval_details.md#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# For 11b model:\n\"<|image|>You are provided a chart image and will be asked a question. You have to think through your answer and provide a step-by-step solution. Once you have the solution, write the final answer in at most a few words at the end with the phrase \\\"FINAL ANSWER:\\\". The question is: {question}<cot_start>Let's think step by step.\"\n\n# For 90b model:\n\"<|image|>You are provided a chart image and will be asked a question. Follow these steps carefully:\\n Step 1: Analyze the question to understand what specific data or information is being asked for. Focus on whether the question is asking for a specific number or category from the chart image.\\n Step 2: Identify any numbers, categories, or groups mentioned in the question and take note of them. Focus on detecting and matching them directly to the image. \\nStep 3: Study the image carefully and find the relevant data corresponding to the categories or numbers mentioned. Avoid unnecessary assumptions or calculations; simply read the correct data from the image.\\n Step 4: Develop a clear plan to solve the question by locating the right data. Focus only on the specific category or group that matches the question. \\nStep 5: Use step-by-step reasoning to ensure you are referencing the correct numbers or data points from the image, avoiding unnecessary extra steps or interpretations.\\n Step 6: Provide the final answer, starting with \\\"FINAL ANSWER:\\\" and using as few words as possible, simply stating the number or data point requested. \\n\\n The question is: {question}<cot_start>Let's think step by step.\"\n```\n\n----------------------------------------\n\nTITLE: System Prompt for DocVQA Task with Llama Models\nDESCRIPTION: System prompt used for evaluating Llama models on the DocVQA benchmark. Designed to extract precise answers from document images with instructions to provide concise, format-specific responses.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/eval_details.md#2025-04-07_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n\"<|image|> Read the text in the image carefully and answer the question with the text as seen exactly in the image. For yes/no questions, just respond Yes or No. If the answer is numeric, just respond with the number and nothing else. If the answer has multiple words, just respond with the words and absolutely nothing else. Never respond in a sentence or a phrase.\\n Question: {question}\"\n```\n\n----------------------------------------\n\nTITLE: System Prompt for VQAv2 Task with Llama Models\nDESCRIPTION: System prompt used for evaluating Llama models on the VQAv2 benchmark. Configured for generating very concise answers to visual questions with specific formatting requirements based on question type.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/eval_details.md#2025-04-07_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n\"<|image|> Look at the image carefully and answer this visual question. For yes/no questions, just respond Yes or No. If the answer is numeric, just respond with the number and nothing else. If the answer has multiple words, just respond with the words and absolutely nothing else. Never respond in a sentence or a phrase.\\n Respond with as few words as possible.\\n Question: {question}\"\n```\n\n----------------------------------------\n\nTITLE: System Prompt for MathVista Task with Llama Models\nDESCRIPTION: System prompt used for evaluating Llama models on the MathVista benchmark. Uses a simple prompt structure with an LLM-based answer extractor as recommended by the MathVista paper.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/eval_details.md#2025-04-07_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n\"<|image|>{question}\"\n```\n\n----------------------------------------\n\nTITLE: System Prompt Configuration for ChartQA Evaluation\nDESCRIPTION: System prompts used for evaluating Llama models on the ChartQA benchmark. Includes two different prompts: one for 11b models and another for 90b models, both designed to elicit step-by-step reasoning for chart-based questions.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/eval_details.md#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<|image|>You are provided a chart image and will be asked a question. You have to think through your answer and provide a step-by-step solution. Once you have the solution, write the final answer in at most a few words at the end with the phrase \"FINAL ANSWER:\". The question is: {question}<cot_start>Let's think step by step.\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n<|image|>You are provided a chart image and will be asked a question. Follow these steps carefully:\\n Step 1: Analyze the question to understand what specific data or information is being asked for. Focus on whether the question is asking for a specific number or category from the chart image.\\n Step 2: Identify any numbers, categories, or groups mentioned in the question and take note of them. Focus on detecting and matching them directly to the image. \\nStep 3: Study the image carefully and find the relevant data corresponding to the categories or numbers mentioned. Avoid unnecessary assumptions or calculations; simply read the correct data from the image.\\n Step 4: Develop a clear plan to solve the question by locating the right data. Focus only on the specific category or group that matches the question. \\nStep 5: Use step-by-step reasoning to ensure you are referencing the correct numbers or data points from the image, avoiding unnecessary extra steps or interpretations.\\n Step 6: Provide the final answer, starting with \"FINAL ANSWER:\" and using as few words as possible, simply stating the number or data point requested. \\n\\n The question is: {question}<cot_start>Let's think step by step.\n```\n\n----------------------------------------\n\nTITLE: System Prompt Configuration for DocVQA Evaluation\nDESCRIPTION: System prompt used for evaluating Llama models on the DocVQA benchmark. The prompt instructs the model to read text from images and answer questions with exact text as seen in the image, emphasizing brevity and exactness.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/eval_details.md#2025-04-07_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n<|image|> Read the text in the image carefully and answer the question with the text as seen exactly in the image. For yes/no questions, just respond Yes or No. If the answer is numeric, just respond with the number and nothing else. If the answer has multiple words, just respond with the words and absolutely nothing else. Never respond in a sentence or a phrase.\\n Question: {question}\n```\n\n----------------------------------------\n\nTITLE: System Prompt Configuration for VQAv2 Evaluation\nDESCRIPTION: System prompt used for evaluating Llama models on the VQAv2 benchmark. The prompt instructs the model to answer visual questions concisely, using the minimum number of words necessary.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/eval_details.md#2025-04-07_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n<|image|> Look at the image carefully and answer this visual question. For yes/no questions, just respond Yes or No. If the answer is numeric, just respond with the number and nothing else. If the answer has multiple words, just respond with the words and absolutely nothing else. Never respond in a sentence or a phrase.\\n Respond with as few words as possible.\\n Question: {question}\n```\n\n----------------------------------------\n\nTITLE: System Prompt Configuration for MathVista Evaluation\nDESCRIPTION: System prompt used for evaluating Llama models on the MathVista benchmark. Uses a simple image-question format with an LLM-based answer extractor as recommended by the MathVista paper.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/eval_details.md#2025-04-07_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n<|image|>{question}\n```\n\n----------------------------------------\n\nTITLE: Citation Format for Llama 3 Model Card\nDESCRIPTION: BibTeX citation format for referencing the Llama 3 Model Card in academic or technical documentation.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3/MODEL_CARD.md#2025-04-07_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{llama3modelcard,\n  title={Llama 3 Model Card},\n  author={AI@Meta},\n  year={2024},\n  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Contributors List\nDESCRIPTION: A markdown-formatted list of all contributors to the Llama Models project, organized with a second-level heading.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/models/llama3/MODEL_CARD.md#2025-04-07_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## Contributors\n\nAaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Puxin Xu; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List in requirements.txt\nDESCRIPTION: A frozen requirements file generated by UV that specifies exact versions of Python packages needed by the llama-models project. The dependencies include utilities for HTTP requests (requests), data validation (pydantic), templating (jinja2), and text processing libraries (regex, tiktoken) among others.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# This file was autogenerated by uv via the following command:\n#    uv export --frozen --no-hashes --no-emit-project --output-file=requirements.txt\nannotated-types==0.7.0\ncertifi==2025.1.31\ncharset-normalizer==3.4.1\nidna==3.10\njinja2==3.1.6\nmarkupsafe==3.0.2\npillow==11.1.0\npydantic==2.10.6\npydantic-core==2.27.2\npyyaml==6.0.2\nregex==2024.11.6\nrequests==2.32.3\ntiktoken==0.8.0\ntyping-extensions==4.12.2\nurllib3==2.3.0\n```\n\n----------------------------------------\n\nTITLE: Installing Llama Models Dependencies with pip\nDESCRIPTION: This command installs the necessary dependencies for running Llama models, including PyTorch.\nSOURCE: https://github.com/meta-llama/llama-models/blob/main/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install .[torch]\n```"
  }
]
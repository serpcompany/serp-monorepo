[
  {
    "owner": "elastic",
    "repo": "logstash",
    "content": "TITLE: GeoIP Enrichment - Logstash\nDESCRIPTION: This snippet applies the `geoip` filter to enrich events with geographical information based on the IP address in the `clientip` field.  It enriches the event with geoip fields such as city, country, etc. The only dependency is a Logstash instance with the geoip filter installed.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/lookup-enrichment.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\nfilter {\n  geoip {\n    source => \"clientip\"\n  }\n}\n```\n\nLANGUAGE: JSON\nCODE:\n```\nfilter {\n  geoip {\n    source => \"clientip\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Logstash Pipeline Configuration - Ruby\nDESCRIPTION: This snippet defines a simple Logstash pipeline configuration using the Ruby DSL. It configures the pipeline to read input from standard input (stdin) and output events to both Elasticsearch (using cloud_id and api_key for authentication) and standard output (stdout) using the rubydebug codec for structured printing. Dependencies include the stdin, elasticsearch, and stdout Logstash plugins.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/creating-logstash-pipeline.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\ninput { stdin { } }\noutput {\n  elasticsearch { cloud_id => \"<cloud id>\" api_key => \"<api key>\" }\n  stdout { codec => rubydebug }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Event Data with Set API (Ruby)\nDESCRIPTION: This snippet demonstrates the use of the `event.set()` method in Ruby to modify or add data to an event. The `set()` method takes a field name and a value as parameters. The field name can be a simple field or a nested field reference.  The method modifies the event object in place and returns the modified event, which enables chaining. Setting collections then mutating is undefined behavior, so you should set the mutated collection using set again or mutate specific keys\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/event-api.md#_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nevent.set(\"foo\", \"baz\")\nevent.set(\"[foo]\", \"zab\")\nevent.set(\"[foo][bar]\", 1)\nevent.set(\"[foo][bar]\", 1.0)\nevent.set(\"[foo][bar]\", [1, 2, 3])\nevent.set(\"[foo][bar]\", {\"a\" => 1, \"b\" => 2})\nevent.set(\"[foo][bar]\", {\"a\" => 1, \"b\" => 2, \"c\" => [1, 2]})\nevent.set(\"[@metadata][foo]\", \"baz\")\nh = {\"a\" => 1, \"b\" => 2, \"c\" => [1, 2]}\nevent.set(\"[foo][bar]\", h)\nh[\"c\"] = [3, 4]\nevent.get(\"[foo][bar][c]\") # => undefined\nh = {\"a\" => 1, \"b\" => 2, \"c\" => [1, 2]}\nevent.set(\"[foo][bar]\", h)\nh[\"c\"] = [3, 4]\nevent.set(\"[foo][bar]\", h)\n# Alternatively,\nevent.set(\"[foo][bar][c]\", [3, 4])\n```\n\n----------------------------------------\n\nTITLE: Listing Installed Logstash Plugins (sh)\nDESCRIPTION: Executes the Logstash plugin management tool to display a list of all plugins currently installed in the Logstash environment. This command is useful for verifying that a newly installed plugin is present and for seeing the available inputs, codecs, filters, and outputs.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin list\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Output in Logstash JSON\nDESCRIPTION: A Logstash output configuration that sends processed log data to Elasticsearch. This configuration specifies the hosts parameter to connect to a local Elasticsearch instance on the default port.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_17\n\nLANGUAGE: json\nCODE:\n```\noutput {\n    elasticsearch {\n        hosts => [ \"localhost:9200\" ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Event Data with Get API (Ruby)\nDESCRIPTION: This snippet demonstrates the usage of the `event.get()` method in Ruby to retrieve data from an event in Logstash.  The `get()` method takes a field name (which can be a nested field reference) as input and returns the corresponding value.  The returned value can be a string, number, timestamp, or a structured data type such as an array or a hash, or `nil` if the field does not exist. There are no dependencies except that it needs to be executed within the context of an event in Logstash's Ruby filter.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/event-api.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nevent.get(\"foo\" ) # => \"baz\"\nevent.get(\"[foo]\") # => \"zab\"\nevent.get(\"[foo][bar]\") # => 1\nevent.get(\"[foo][bar]\") # => 1.0\nevent.get(\"[foo][bar]\") # =>  [1, 2, 3]\nevent.get(\"[foo][bar]\") # => {\"a\" => 1, \"b\" => 2}\nevent.get(\"[foo][bar]\") # =>  {\"a\" => 1, \"b\" => 2, \"c\" => [1, 2]}\nevent.get(\"[@metadata][foo]\") # => \"baz\"\n```\n\n----------------------------------------\n\nTITLE: Processing Apache Logs - Logstash Configuration\nDESCRIPTION: This Logstash configuration reads Apache access log files, filters the events based on the file path, and applies grok and date filters. It replaces the 'type' field with 'apache_access'. The configuration reads from a file and outputs to Elasticsearch and stdout.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\ninput {\n  file {\n    path => \"/tmp/access_log\"\n    start_position => \"beginning\"\n  }\n}\n\nfilter {\n  if [path] =~ \"access\" {\n    mutate { replace => { \"type\" => \"apache_access\" } }\n    grok {\n      match => { \"message\" => \"%{\nCOMBINEDAPACHELOG}\" }\n    }\n  }\n  date {\n    match => [ \"timestamp\" , \"dd/MMM/yyyy:HH:mm:ss Z\" ]\n  }\n}\n\noutput {\n  elasticsearch {\n    hosts => [\"localhost:9200\"]\n  }\n  stdout { codec => rubydebug }\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Grok Filter in Logstash Configuration\nDESCRIPTION: Illustrates how to use the Logstash grok filter to parse unstructured data like an HTTP access log based on the input '55.3.244.1 GET /index.html 15824 0.043'. The `match` option specifies the target field (`message`) and the grok pattern composed of predefined patterns (`%{IP}`, `%{WORD}`, etc.) combined with custom field names (`client`, `method`, etc.) to extract specific data points into new fields.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/field-extraction.md#_snippet_2\n\nLANGUAGE: logstash\nCODE:\n```\nfilter {\n  grok {\n    match => { \"message\" => \"%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}\" }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Renaming and Stripping Fields with Logstash Mutate Filter in JSON\nDESCRIPTION: Includes two examples using the mutate filter to modify fields. The first example renames the 'HOSTORIP' field to 'client_ip', enabling standardized field names. The second example strips leading and trailing whitespace from fields 'field1' and 'field2'. These transformations require the mutate plugin and help normalize event data for downstream processing.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/core-operations.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\nfilter {\n  mutate {\n    rename => { \"HOSTORIP\" => \"client_ip\" }\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\nfilter {\n  mutate {\n    strip => [\"field1\", \"field2\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash for Syslog Message Processing - Ruby\nDESCRIPTION: Defines a Logstash pipeline configuration for receiving syslog messages over TCP and UDP on port 5000, parsing them using grok filters according to RFC3164 patterns, enriching with additional fields, and routing outputs to both Elasticsearch and the console via rubydebug. Dependencies include Logstash and its Grok filter plugin, as well as access to port 5000. Expected input is syslog-formatted messages, while output is structured event data with parsed fields. The configuration may fail for non-standard or malformed syslog lines.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_8\n\nLANGUAGE: ruby\nCODE:\n```\ninput {\n  tcp {\n    port => 5000\n    type => syslog\n  }\n  udp {\n    port => 5000\n    type => syslog\n  }\n}\n\nfilter {\n  if [type] == \"syslog\" {\n    grok {\n      match => { \"message\" => \"%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\\[%{POSINT:syslog_pid}\\])?: %{GREEDYDATA:syslog_message}\" }\n      add_field => [ \"received_at\", \"%{@timestamp}\" ]\n      add_field => [ \"received_from\", \"%{host}\" ]\n    }\n    date {\n      match => [ \"syslog_timestamp\", \"MMM  d HH:mm:ss\", \"MMM dd HH:mm:ss\" ]\n    }\n  }\n}\n\noutput {\n  elasticsearch { hosts => [\"localhost:9200\"] }\n  stdout { codec => rubydebug }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Logstash Pipeline with Elasticsearch Output in JSON\nDESCRIPTION: The final Logstash pipeline configuration that reads from Filebeat, parses logs with Grok, enriches with GeoIP, and indexes the structured data into Elasticsearch for storage and analysis.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_18\n\nLANGUAGE: json\nCODE:\n```\ninput {\n    beats {\n        port => \"5044\"\n    }\n}\n filter {\n    grok {\n        match => { \"message\" => \"%{COMBINEDAPACHELOG}\"}\n    }\n    geoip {\n        source => \"clientip\"\n    }\n}\noutput {\n    elasticsearch {\n        hosts => [ \"localhost:9200\" ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Output to Console (stdout) with rubydebug Codec (JSON)\nDESCRIPTION: This snippet configures the Logstash output section to print incoming events to the standard output using the 'rubydebug' codec, providing detailed and human-readable output on the console. Useful for testing and debugging pipeline configuration before sending data to Elasticsearch or external systems. Requires no additional dependencies; integrates directly within the Logstash config file.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n    stdout { codec => rubydebug }\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Logstash Pipelines in YAML\nDESCRIPTION: This configuration defines three Logstash pipelines: intake, buffered-es, and partner. Each pipeline uses a persisted queue and is configured with input and output plugins to process data.  The 'intake' pipeline receives data via beats and sends it to 'internal-es' and 'partner-s3' pipelines. 'buffered-es' receives from 'internal-es' and indexes the full event into Elasticsearch. The 'partner' pipeline receives from 'partner-s3', removes sensitive data, and outputs to an S3 bucket.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/pipeline-to-pipeline.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n- pipeline.id: intake\n  queue.type: persisted\n  config.string: |\n    input { beats { port => 5044 } }\n    output { pipeline { send_to => [\"internal-es\", \"partner-s3\"] } }\n- pipeline.id: buffered-es\n  queue.type: persisted\n  config.string: |\n    input { pipeline { address => \"internal-es\" } }\n    # Index the full event\n    output { elasticsearch { } }\n- pipeline.id: partner\n  queue.type: persisted\n  config.string: |\n    input { pipeline { address => \"partner-s3\" } }\n    filter {\n      # Remove the sensitive data\n      mutate { remove_field => 'sensitive-data' }\n    }\n    output { s3 { } } # Output to partner's bucket\n```\n\n----------------------------------------\n\nTITLE: Defining External Dependencies in vendor.json for Logstash Plugins\nDESCRIPTION: Example of a vendor.json file that specifies external file dependencies for a Logstash plugin. This configuration downloads the collectd file and extracts specific files from it.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_11\n\nLANGUAGE: txt\nCODE:\n```\n[{\n        \"sha1\": \"a90fe6cc53b76b7bdd56dc57950d90787cb9c96e\",\n        \"url\": \"http://collectd.org/files/collectd-5.4.0.tar.gz\",\n        \"files\": [ \"/src/types.db\" ]\n}]\n```\n\n----------------------------------------\n\nTITLE: Configuring Filters - Logstash Filter\nDESCRIPTION: This snippet configures Logstash to read from stdin, apply grok and date filters, and output to Elasticsearch and stdout. It uses grok to parse the message field according to the COMBINEDAPACHELOG pattern and the date filter to parse a timestamp.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\ninput { stdin { } }\n\nfilter {\n  grok {\n    match => { \"message\" => \"%{\nCOMBINEDAPACHELOG}\" }\n  }\n  date {\n    match => [ \"timestamp\" , \"dd/MMM/yyyy:HH:mm:ss Z\" ]\n  }\n}\n\noutput {\n  elasticsearch { hosts => [\"localhost:9200\"] }\n  stdout { codec => rubydebug }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Logstash via YUM (shell)\nDESCRIPTION: This shell command installs the logstash package from the previously configured Elastic YUM repository on RPM-based distributions such as CentOS, RHEL, or Fedora. Requires repository setup and GPG key import as prerequisites. Run with superuser privileges. Output is a working installation of Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/installing-logstash.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nsudo yum install logstash\n```\n\n----------------------------------------\n\nTITLE: Applying Dissect Filter in Logstash Configuration\nDESCRIPTION: Demonstrates how to configure the Logstash dissect filter to parse a systemd-like log message. It uses the `mapping` option to define the pattern for extracting fields like timestamp (`ts`), source (`src`), program (`prog`), process ID (`pid`), and message content (`msg`) from the `message` field using delimiters and named capture groups, based on the input 'Apr 26 12:20:02 localhost systemd[1]: Starting system activity accounting tool...'.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/field-extraction.md#_snippet_0\n\nLANGUAGE: logstash\nCODE:\n```\nfilter {\n  dissect {\n    mapping => { \"message\" => \"%{ts} %{+ts} %{+ts} %{src} %{prog}[%{pid}]: %{msg}\" }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Pipelines in Logstash Configuration YAML\nDESCRIPTION: This YAML snippet shows how to define multiple pipelines in a single Logstash instance by specifying an array of pipeline configurations in the 'pipelines.yml' file. Each pipeline entry contains a unique 'pipeline.id', the path to its configuration file defined by 'path.config', and optional pipeline-specific settings like 'pipeline.workers' (number of worker threads) and 'queue.type' (setting the queue persistence). The file must be placed in the Logstash 'path.settings' directory to be automatically loaded. If a setting is omitted, it falls back to defaults defined in 'logstash.yml'. When Logstash starts without overriding arguments, it loads all pipelines defined here; otherwise, it ignores this file.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/multiple-pipelines.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- pipeline.id: my-pipeline_1\n  path.config: \"/etc/path/to/p1.config\"\n  pipeline.workers: 3\n- pipeline.id: my-other-pipeline\n  path.config: \"/etc/different/path/p2.cfg\"\n  queue.type: persisted\n```\n\n----------------------------------------\n\nTITLE: Complete Logstash Pipeline with Beats Input and Console Output (JSON)\nDESCRIPTION: This complete Logstash pipeline example includes an 'input' section accepting events via the Beats plugin on port 5044, an (optional, commented) filter section, and an 'output' directing events to console output using the rubydebug codec. This is a deployable configuration for ingesting logs from Filebeat and debugging data flow through Logstash. The filter section is left as a customizable placeholder.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\ninput {\n    beats {\n        port => \"5044\"\n    }\n}\n# The filter part of this file is commented out to indicate that it is\n# optional.\n# filter {\n#\n# }\noutput {\n    stdout { codec => rubydebug }\n}\n```\n\n----------------------------------------\n\nTITLE: Referencing Keystore Keys in Logstash Configuration\nDESCRIPTION: This snippet illustrates how to reference values stored in the Logstash keystore within Logstash configuration files and `logstash.yml`. The key is enclosed in `${}` syntax.  It requires that the key-value pair is already stored in the keystore.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/keystore.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\noutput { elasticsearch {...password => \"${ES_PWD}\" } } } \n```\n\n----------------------------------------\n\nTITLE: Running Logstash with Configuration File - Bash\nDESCRIPTION: This command demonstrates how to execute the Logstash process using the -f flag to specify a configuration file. It instructs Logstash to load and use the pipeline defined in logstash-simple.conf. This command assumes Logstash is installed and bin/logstash is in the current directory or path. The configuration file must exist at the specified path.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/creating-logstash-pipeline.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/logstash -f logstash-simple.conf\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with Config File\nDESCRIPTION: Example showing how to run Logstash with a specific pipeline configuration file using the -f flag. This loads the configuration from the specified file.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash-command-line.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -f mypipeline.conf\n```\n\n----------------------------------------\n\nTITLE: Starting Logstash with a Custom Pipeline Configuration - Shell\nDESCRIPTION: This shell command launches Logstash while specifying a configuration file to define pipeline input and output logic. Replace `mypipeline.conf` with the actual configuration filename. Prerequisites include having the referenced configuration file on the filesystem and Logstash properly installed.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/use-filebeat-modules-kafka.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -f mypipeline.conf\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with Pipeline Configuration\nDESCRIPTION: This shell script demonstrates how to run a Logstash Docker container with a bind-mounted volume for pipeline configuration. It mounts the local directory '~/pipeline/' to '/usr/share/logstash/pipeline/' inside the container. Any configuration files placed in the host's '~/pipeline/' directory will be used by Logstash. The command pulls the image from Docker Hub and runs it with the specified volume mapping.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/docker-config.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndocker run --rm -it -v ~/pipeline/:/usr/share/logstash/pipeline/ docker.elastic.co/logstash/logstash:9.0.0\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameter Syntax for Logstash Plugins in Ruby\nDESCRIPTION: Shows the syntax for defining configuration parameters in Logstash plugins, including validation types, defaults, and other available options.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nconfig :variable_name, :validate => :variable_type, :default => \"Default value\", :required => boolean, :deprecated => boolean, :obsolete => string\n```\n\n----------------------------------------\n\nTITLE: Running Logstash Pipeline with Custom Configuration - Ruby\nDESCRIPTION: Executes the Logstash service with a specified configuration file using the Ruby bin script. This command requires Logstash to be installed and accessible from the terminal. The filename parameter '-f logstash-syslog.conf' instructs Logstash to use the shown configuration for processing events. Output displays real-time processed logs to stdout and/or forwarded destinations.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_9\n\nLANGUAGE: ruby\nCODE:\n```\nbin/logstash -f logstash-syslog.conf\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Multiline Codec for Java Stack Traces (JSON)\nDESCRIPTION: Demonstrates how to configure the Logstash `multiline` codec within the `stdin` input plugin to consolidate Java stack traces into single events. It matches lines starting with whitespace (`^\\s`) using the `pattern` option and merges them with the previous line using `what => \"previous\"`.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/multiline.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ninput {\n  stdin {\n    codec => multiline {\n      pattern => \"^\\s\"\n      what => \"previous\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash to Use Elasticsearch API Key - YAML\nDESCRIPTION: This snippet illustrates how to configure Logstash's `logstash.yml` file to use the generated Elasticsearch API key for authentication. The format requires concatenating the API key 'id' and 'api_key' with a colon separator. This enables Logstash to securely connect to Elasticsearch central management using the created API key without exposing sensitive credentials.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_21\n\nLANGUAGE: YAML\nCODE:\n```\nxpack.management.elasticsearch.api_key: TiNAGG4BaaMdaH1tRfuU:KnR6yE41RrSowb0kQ0HWoA <1>\n```\n\n----------------------------------------\n\nTITLE: Using the output isolator pattern with persistent queues in Logstash pipelines - YAML\nDESCRIPTION: This YAML configuration demonstrates the output isolator pattern to prevent blocking of critical outputs in Logstash pipelines by isolating each output to its own pipeline backed by persistent queues. The `intake` pipeline receives input via beats and sends events to two virtual addresses `es` and `http`. Separate downstream pipelines `buffered-es` and `buffered-http` consume from these addresses and have persistent queues (`queue.type: persisted`) to independently buffer events before outputting to Elasticsearch or HTTP respectively. This design reduces the risk of one slow or unavailable output blocking other outputs, at the cost of increased disk usage and serialization overhead.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/pipeline-to-pipeline.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# config/pipelines.yml\n- pipeline.id: intake\n  config.string: |\n    input { beats { port => 5044 } }\n    output { pipeline { send_to => [es, http] } }\n- pipeline.id: buffered-es\n  queue.type: persisted\n  config.string: |\n    input { pipeline { address => es } }\n    output { elasticsearch { } }\n- pipeline.id: buffered-http\n  queue.type: persisted\n  config.string: |\n    input { pipeline { address => http } }\n    output { http { } }\n```\n\n----------------------------------------\n\nTITLE: Sample Logstash Event Output from Filebeat (JSON)\nDESCRIPTION: This snippet displays a sample log event object as output to the console from Logstash when receiving data via Filebeat. It includes fields such as '@timestamp', 'beat', 'host', 'prospector', 'input', 'source', 'message', and 'tags'. All fields are dynamically populated based on the incoming log entry. The output reflects Logstash's rubydebug codec formatting, useful for verifying correct ingest and parsing.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"@timestamp\" => 2017-11-09T01:44:20.071Z,\n        \"offset\" => 325,\n      \"@version\" => \"1\",\n          \"beat\" => {\n            \"name\" => \"My-MacBook-Pro.local\",\n        \"hostname\" => \"My-MacBook-Pro.local\",\n         \"version\" => \"6.0.0\"\n    },\n          \"host\" => \"My-MacBook-Pro.local\",\n    \"prospector\" => {\n        \"type\" => \"log\"\n    },\n    \"input\" => {\n        \"type\" => \"log\"\n    },\n        \"source\" => \"/path/to/file/logstash-tutorial.log\",\n       \"message\" => \"83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \\\"GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1\\\" 200 203023 \\\"http://semicomplete.com/presentations/logstash-monitorama-2013/\\\" \\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\\\"\",\n          \"tags\" => [\n        [0] \"beats_input_codec_plain_applied\"\n    ]\n}\n...\n```\n\n----------------------------------------\n\nTITLE: Installing Plugin Dependencies with Bundler (Shell)\nDESCRIPTION: Shell command using Bundler (`bundle install`) to install the Ruby gem dependencies defined in the plugin's `Gemfile`. This command should be run within the plugin's root directory after cloning the repository.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_12\n\nLANGUAGE: Shell\nCODE:\n```\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Basic Logstash Input Plugin Structure in Ruby\nDESCRIPTION: The core structure of a Logstash input plugin showing required dependencies, configuration parameters, and essential methods including register() and run().\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"logstash/inputs/base\"\nrequire \"logstash/namespace\"\nrequire \"stud/interval\"\nrequire \"socket\" # for Socket.gethostname\n\n# Add any asciidoc formatted documentation here\n# Generate a repeating message.\n#\n# This plugin is intended only as an example.\n\nclass LogStash::Inputs::Example < LogStash::Inputs::Base\n  config_name \"example\"\n\n  # If undefined, Logstash will complain, even if codec is unused.\n  default :codec, \"plain\"\n\n  # The message string to use in the event.\n  config :message, :validate => :string, :default => \"Hello World!\"\n\n  # Set how frequently messages should be sent.\n  #\n  # The default, `1`, means send a message every second.\n  config :interval, :validate => :number, :default => 1\n\n  public\n  def register\n    @host = Socket.gethostname\n  end # def register\n\n  def run(queue)\n    Stud.interval(@interval) do\n      event = LogStash::Event.new(\"message\" => @message, \"host\" => @host)\n      decorate(event)\n      queue << event\n    end # loop\n  end # def run\n\nend # class LogStash::Inputs::Example\n```\n\n----------------------------------------\n\nTITLE: Processing Logs with Conditionals - Logstash Configuration\nDESCRIPTION: This Logstash configuration uses conditionals to label events based on the path and apply grok and date filters to access logs.  It includes an 'else if' and an 'else' block to cover other log file types, and outputs the processed data to Elasticsearch and stdout.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_6\n\nLANGUAGE: ruby\nCODE:\n```\ninput {\n  file {\n    path => \"/tmp/*_log\"\n  }\n}\n\nfilter {\n  if [path] =~ \"access\" {\n    mutate { replace => { type => \"apache_access\" } }\n    grok {\n      match => { \"message\" => \"%{\nCOMBINEDAPACHELOG}\" }\n    }\n    date {\n      match => [ \"timestamp\" , \"dd/MMM/yyyy:HH:mm:ss Z\" ]\n    }\n  } else if [path] =~ \"error\" {\n    mutate { replace => { type => \"apache_error\" } }\n  } else {\n    mutate { replace => { type => \"random_logs\" } }\n  }\n}\n\noutput {\n  elasticsearch { hosts => [\"localhost:9200\"] }\n  stdout { codec => rubydebug }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing the filter Method in Ruby for Logstash Plugins\nDESCRIPTION: The core filter method implementation that processes events in a Logstash filter plugin. It replaces the event message with a configured message and calls filter_matched to handle additional configurations like add_field or add_tag.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_8\n\nLANGUAGE: ruby\nCODE:\n```\n  public\n  def filter(event)\n\n    if @message\n      # Replace the event message with our message as configured in the\n      # config file.\n      event.set(\"message\", @message)\n    end\n\n  # filter_matched should go in the last line of our successful code\n  filter_matched(event)\nend # def filter\n```\n\n----------------------------------------\n\nTITLE: Declaring Configuration Parameters in a Logstash Plugin (Ruby)\nDESCRIPTION: This pattern shows how to declare configuration parameters for a Logstash plugin using the config macro. Parameters include the name, validation type (such as :string or :number), whether the value is required, deprecated, or obsolete, a default value, and other attributes. This ensures robust type-checking and stable plugin behavior. Only relevant to the plugin's configuration logic.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_4\n\nLANGUAGE: ruby\nCODE:\n```\n  config :variable_name, :validate => :variable_type, :default => \"Default value\", :required => boolean, :deprecated => boolean, :obsolete => string\n```\n\n----------------------------------------\n\nTITLE: Building a Ruby Gem for Logstash Plugin Distribution - sh\nDESCRIPTION: These shell commands illustrate building a Logstash plugin gem from its gemspec using gem build, producing a gem file named according to the gemspec's s.version, and running tests via bundle exec rspec after dependencies are installed. These steps require that all configuration (Gemfile, gemspec, vendor.json if needed) is complete and all dependencies are present. Adjust command arguments as appropriate for your actual plugin file names before executing.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\ngem build logstash-output-example.gemspec\n```\n\nLANGUAGE: sh\nCODE:\n```\nbundle install\n```\n\nLANGUAGE: sh\nCODE:\n```\nrake vendor\n```\n\nLANGUAGE: sh\nCODE:\n```\nbundle exec rspec\n```\n\n----------------------------------------\n\nTITLE: Declaring the Output Plugin Class for Logstash in Ruby\nDESCRIPTION: This snippet shows how to declare the main class for a Logstash output plugin, which must inherit from LogStash::Outputs::Base. The class name conventionally follows the LogStash::Outputs::<PluginName> pattern. This establishes the core behavior for the output plugin and links it into the Logstash plugin system.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\nclass LogStash::Outputs::Example < LogStash::Outputs::Base\n```\n\nLANGUAGE: ruby\nCODE:\n```\nLogStash::Outputs::Example\n```\n\n----------------------------------------\n\nTITLE: Complete Logstash Pipeline with Grok Filter in JSON\nDESCRIPTION: A complete Logstash pipeline configuration that reads from Filebeat, parses logs with the Grok filter, and outputs to standard output using the rubydebug codec for inspection.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_11\n\nLANGUAGE: json\nCODE:\n```\ninput {\n    beats {\n        port => \"5044\"\n    }\n}\nfilter {\n    grok {\n        match => { \"message\" => \"%{COMBINEDAPACHELOG}\"}\n    }\n}\noutput {\n    stdout { codec => rubydebug }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing the register Method in a Logstash Filter Plugin (Ruby)\nDESCRIPTION: The register method is a required lifecycle hook for setting up plugin state when Logstash initializes the plugin instance. Often used to assign instance variables or perform pre-runtime checks. This public method is required for compatibility with Logstash's plugin API.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_5\n\nLANGUAGE: ruby\nCODE:\n```\n  public\n  def register\n  end # def register\n```\n\n----------------------------------------\n\nTITLE: Updating Logstash Plugins - Shell\nDESCRIPTION: This shell command sequence updates installed Logstash plugins using the logstash-plugin manager. The first command updates all plugins, while the second only updates the specified plugin (logstash-input-github). An active internet connection and appropriate permissions are required. Updates are constrained to patch and minor versions unless otherwise specified.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/working-with-plugins.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin update <1>\n```\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin update logstash-input-github <2>\n```\n\n----------------------------------------\n\nTITLE: Defining a Logstash Pipeline for Kafka Input and Elasticsearch Output - YAML\nDESCRIPTION: This YAML-based Logstash pipeline configuration demonstrates ingesting events from a Kafka topic and sending them to Elasticsearch. The `input` block configures the Kafka source with server details, topics, and JSON decoding. The `output` block checks if a pipeline metadata value is present and, if so, sets it as the ingest pipeline on Elasticsearch output requests. It handles credentials and index formatting. This file (e.g., `mypipeline.conf`) should be referenced when running Logstash. Dependencies include a running Kafka broker, accessible Elasticsearch instance, and the correct Logstash plugins installed.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/use-filebeat-modules-kafka.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ninput {\n  kafka {\n    bootstrap_servers => \"myhost:9092\"\n    topics => [\"filebeat\"]\n    codec => json\n  }\n}\n\noutput {\n  if [@metadata][pipeline] {\n    elasticsearch {\n      hosts => \"https://myEShost:9200\"\n      manage_template => false\n      index => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\"\n      pipeline => \"%{[@metadata][pipeline]}\" <1>\n      user => \"elastic\"\n      password => \"secret\"\n    }\n  } else {\n    elasticsearch {\n      hosts => \"https://myEShost:9200\"\n      manage_template => false\n      index => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\"\n      user => \"elastic\"\n      password => \"secret\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Elaticsearch Output with Multiple Nodes (JSON syntax)\nDESCRIPTION: Configures Logstash to distribute data across multiple Elasticsearch nodes, balancing requests and providing redundancy. Dependencies include Elasticsearch running on specified hosts with their IP addresses and ports. When multiple hosts are listed, Logstash load-balances requests among them, optimizing resource usage and cluster resilience.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/multiple-input-output-plugins.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\noutput {\n    elasticsearch {\n        hosts => [\"IP Address 1:port1\", \"IP Address 2:port2\", \"IP Address 3\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Logstash Configuration File in Bash\nDESCRIPTION: Uses the Logstash command-line tool to test the syntax and validity of a configuration file. The `--config.test_and_exit` flag instructs Logstash to parse the specified file (`-f <configuration-file>`) and report any errors without starting the event processing pipeline. This is recommended after upgrading to ensure configuration compatibility with the new version.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/upgrading-using-package-managers.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlogstash --config.test_and_exit -f <configuration-file>\n```\n\n----------------------------------------\n\nTITLE: Listing Installed Logstash Plugins - Shell\nDESCRIPTION: This group of shell commands lists available Logstash plugins using the logstash-plugin manager utility. The options include basic listing, verbose listing with version details, filtered listing by name fragment, and filtered listing by plugin group (e.g., input, filter, codec, output). No external dependencies are needed beyond a functional Logstash installation. The commands expect Logstash binaries to be in the bin directory and may require appropriate permissions.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/working-with-plugins.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin list <1>\n```\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin list --verbose <2>\n```\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin list '*namefragment*' <3>\n```\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin list --group output <4>\n```\n\n----------------------------------------\n\nTITLE: Parse User Agent String - Logstash\nDESCRIPTION: This snippet uses the `useragent` filter to parse a user agent string from the `agent` field into various user agent fields, and stores the parsed data in the `user_agent` field. It also removes the original `agent` field. This filter requires the `useragent` filter plugin to be installed, and requires a user agent string in the specified source field.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/lookup-enrichment.md#_snippet_6\n\nLANGUAGE: JSON\nCODE:\n```\nfilter {\n  useragent {\n    source => \"agent\"\n    target => \"user_agent\"\n    remove_field => \"agent\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Global ECS Compatibility in Logstash System Config (yaml)\nDESCRIPTION: This snippet demonstrates how to globally configure the ECS compatibility mode for the entire Logstash instance by adding the 'pipeline.ecs_compatibility' parameter to the 'config/logstash.yml' file. Setting this value changes the default schema compatibility for all pipelines and plugins in the Logstash process, unless specifically overridden elsewhere. Use this method to ensure consistent ECS mode (such as 'disabled') across all pipeline configurations.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ecs-ls.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\npipeline.ecs_compatibility: disabled\n```\n\n----------------------------------------\n\nTITLE: Using Escape Sequences in Quoted Strings in Logstash Configurations - JavaScript\nDESCRIPTION: Demonstrates the use of escape sequences in quoted strings, enabled by setting 'config.support_escapes: true' in the Logstash settings. Examples include newline, tab, and escaped quotes within double or single quotes, enabling richer string content while preserving syntax correctness.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_12\n\nLANGUAGE: JavaScript\nCODE:\n```\nname => \"Hello world\"\nname => 'It\\'s a beautiful day'\n```\n\n----------------------------------------\n\nTITLE: Installing a Logstash Plugin (Logstash Plugin CLI)\nDESCRIPTION: This snippet installs a Logstash plugin from a local gem file using the `bin/logstash-plugin install` command. Replace `/my/logstash/plugins/...` with the actual path to the gem file.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_21\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin install /my/logstash/plugins/logstash-filter-example/logstash-filter-example-0.1.0.gem\n```\n\n----------------------------------------\n\nTITLE: Formatting Output File Paths with Java Time Sprintf Patterns (Logstash Configuration)\nDESCRIPTION: Shows how to use Java time patterns within the sprintf format to dynamically generate output file paths in the file output plugin. The example uses '%{{yyyy.MM.dd.HH}}' to incorporate the UTC date and hour, and '%{type}' for the event's type field. Requires the file output plugin and appropriate event fields. Inputs are event data; output is log files written to dynamic paths based on time and type.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/event-dependent-configuration.md#_snippet_4\n\nLANGUAGE: Logstash Configuration\nCODE:\n```\noutput {\n  file {\n    path => \"/var/log/%{type}.%{{yyyy.MM.dd.HH}}\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Gradle Task to Package Java Logstash Plugin as Ruby Gem\nDESCRIPTION: This command executes the Gradle 'gem' task to generate a Ruby gem package for the Java plugin, which includes necessary gem structure files and plugin metadata based on the earlier configuration. It simplifies deployment and distribution of the plugin for use within Logstash environments.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\n./gradlew gem\n```\n\n----------------------------------------\n\nTITLE: Starting Logstash with Automatic Configuration Reload (Shell)\nDESCRIPTION: This Shell command runs Logstash with the specified pipeline configuration file ('first-pipeline.conf') and enables automatic reloading of the configuration file when changes are detected. The '--config.reload.automatic' option is useful for iterative development and testing, reducing downtime and manual restarts. Logstash must be installed and the referenced config file available.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -f first-pipeline.conf --config.reload.automatic\n```\n\n----------------------------------------\n\nTITLE: Configuring TCP Port with Default (Ruby)\nDESCRIPTION: This Ruby code shows how to set a default value for the `TCP_PORT` environment variable. If `TCP_PORT` isn't defined, Logstash will use `54321`. This avoids configuration errors. The `${TCP_PORT:54321}` syntax specifies the variable and its default.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/environment-variables.md#_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\ninput {\n  tcp {\n    port => \"${TCP_PORT:54321}\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Java Codec Plugin for Logstash\nDESCRIPTION: Complete implementation of a simple Java codec plugin that decodes messages separated by a configurable delimiter and encodes messages by writing their string representation with a delimiter.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n@LogstashPlugin(name=\"java_codec_example\")\npublic class JavaCodecExample implements Codec {\n\n    public static final PluginConfigSpec<String> DELIMITER_CONFIG =\n            PluginConfigSpec.stringSetting(\"delimiter\", \",\");\n\n    private final String id;\n    private final String delimiter;\n\n    public JavaCodecExample(final Configuration config, final Context context) {\n        this(config.get(DELIMITER_CONFIG));\n    }\n\n    private JavaCodecExample(String delimiter) {\n        this.id = UUID.randomUUID().toString();\n        this.delimiter = delimiter;\n    }\n\n    @Override\n    public void decode(ByteBuffer byteBuffer, Consumer<Map<String, Object>> consumer) {\n        // a not-production-grade delimiter decoder\n        byte[] byteInput = new byte[byteBuffer.remaining()];\n        byteBuffer.get(byteInput);\n        if (byteInput.length > 0) {\n            String input = new String(byteInput);\n            String[] split = input.split(delimiter);\n            for (String s : split) {\n                Map<String, Object> map = new HashMap<>();\n                map.put(\"message\", s);\n                consumer.accept(map);\n            }\n        }\n    }\n\n    @Override\n    public void flush(ByteBuffer byteBuffer, Consumer<Map<String, Object>> consumer) {\n        // if the codec maintains any internal state such as partially-decoded input, this\n        // method should flush that state along with any additional input supplied in\n        // the ByteBuffer\n\n        decode(byteBuffer, consumer); // this is a simplistic implementation\n    }\n\n    @Override\n    public void encode(Event event, OutputStream outputStream) throws IOException {\n        outputStream.write((event.toString() + delimiter).getBytes(Charset.defaultCharset()));\n    }\n\n    @Override\n    public Collection<PluginConfigSpec<?>> configSchema() {\n        // should return a list of all configuration options for this plugin\n        return Collections.singletonList(DELIMITER_CONFIG);\n    }\n\n    @Override\n    public Codec cloneCodec() {\n        return new JavaCodecExample(this.delimiter);\n    }\n\n    @Override\n    public String getId() {\n        return this.id;\n    }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Space-Delimited URIs in Environment Variables - Bash\nDESCRIPTION: Shows how to declare an environment variable containing multiple whitespace-delimited URIs for use as a plugin parameter in Logstash. This enables users to easily manage multiple endpoints for options like 'hosts' in various Elasticsearch plugins without explicit array formatting. Required: Logstash-compatible plugins that auto-expand these entries and a shell environment for setting variables.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/plugin-concepts.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nES_HOSTS=\"es1.example.com es2.example.com:9201 es3.example.com:9201\"\n```\n\n----------------------------------------\n\nTITLE: Complete Logstash Pipeline with Grok and GeoIP Filters in JSON\nDESCRIPTION: A complete Logstash pipeline configuration that reads from Filebeat, parses logs with Grok, enriches data with GeoIP information, and outputs the results to standard output for inspection.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_15\n\nLANGUAGE: json\nCODE:\n```\ninput {\n    beats {\n        port => \"5044\"\n    }\n}\n filter {\n    grok {\n        match => { \"message\" => \"%{COMBINEDAPACHELOG}\"}\n    }\n    geoip {\n        source => \"clientip\"\n    }\n}\noutput {\n    stdout { codec => rubydebug }\n}\n```\n\n----------------------------------------\n\nTITLE: Listing installed Logstash plugins\nDESCRIPTION: Command to display all currently installed Logstash plugins, including inputs, codecs, filters, and outputs.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_20\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin list\n```\n\n----------------------------------------\n\nTITLE: Implementing the distributor pattern for data routing in Logstash pipelines - YAML\nDESCRIPTION: This YAML snippet configures several Logstash pipelines to implement the distributor pattern, where a single pipeline (`beats-server`) routes events to specific downstream pipelines based on event type using conditional logic. The distributor pipeline receives input via the beats plugin on port 5044 and outputs events to virtual addresses `weblogs`, `syslog`, or `fallback` pipelines accordingly. Each downstream pipeline listens on its respective virtual address using the `pipeline` input plugin and processes events with tailored filters and Elasticsearch outputs. This pattern improves logical separation and scalability by isolating processing logic by event type across dedicated pipelines.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/pipeline-to-pipeline.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# config/pipelines.yml\n- pipeline.id: beats-server\n  config.string: |\n    input { beats { port => 5044 } }\n    output {\n        if [type] == apache {\n          pipeline { send_to => weblogs }\n        } else if [type] == system {\n          pipeline { send_to => syslog }\n        } else {\n          pipeline { send_to => fallback }\n        }\n    }\n- pipeline.id: weblog-processing\n  config.string: |\n    input { pipeline { address => weblogs } }\n    filter {\n       # Weblog filter statements here...\n    }\n    output {\n      elasticsearch { hosts => [es_cluster_a_host] }\n    }\n- pipeline.id: syslog-processing\n  config.string: |\n    input { pipeline { address => syslog } }\n    filter {\n       # Syslog filter statements here...\n    }\n    output {\n      elasticsearch { hosts => [es_cluster_b_host] }\n    }\n- pipeline.id: fallback-processing\n    config.string: |\n    input { pipeline { address => fallback } }\n    output { elasticsearch { hosts => [es_cluster_b_host] } }\n```\n\n----------------------------------------\n\nTITLE: Streaming JDBC Lookup - Logstash\nDESCRIPTION: This snippet uses the `jdbc_streaming` filter to enrich events by executing a SQL query against a database.  It connects to a MySQL database, and executes a `SELECT` statement to retrieve country details based on a provided `country_code`. The result set is stored in the `country_details` field.  This requires a JDBC driver for MySQL, database credentials, and a database setup with the relevant tables and data.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/lookup-enrichment.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\nfilter {\n  jdbc_streaming {\n    jdbc_driver_library => \"/path/to/mysql-connector-java-5.1.34-bin.jar\"\n    jdbc_driver_class => \"com.mysql.jdbc.Driver\"\n    jdbc_connection_string => \"jdbc:mysql://localhost:3306/mydatabase\"\n    jdbc_user => \"me\"\n    jdbc_password => \"secret\"\n    statement => \"select * from WORLD.COUNTRY WHERE Code = :code\"\n    parameters => { \"code\" => \"country_code\"}\n    target => \"country_details\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Incrementing Metrics with Sprintf Format for Field Values (JavaScript/Logstash Configuration)\nDESCRIPTION: Configures a Logstash output block using the statsd plugin, applying sprintf format to dynamically increment metrics based on the status code of response events. Dependencies include the statsd output plugin and event fields such as '[response][status]'. Input is the Logstash event; output is a metric named for the response status. This illustrates Logstash-specific field formatting in plugin settings.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/event-dependent-configuration.md#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\noutput {\n  statsd {\n    increment => \"apache.%{[response][status]}\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Logstash User and Assigning Role\nDESCRIPTION: This Shell code snippet demonstrates how to use the Elasticsearch `user` API to create a `logstash_internal` user and assign it the `logstash_writer` role. The user is created with a password and a full name, and then assigned to the created role.  This user is intended to be used by Logstash for internal operations such as writing and managing indices.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nPOST _security/user/logstash_internal\n{\n  \"password\" : \"x-pack-test-password\",\n  \"roles\" : [ \"logstash_writer\"],\n  \"full_name\" : \"Internal Logstash User\"\n}\n\n```\n\n----------------------------------------\n\nTITLE: Installing a published Logstash plugin\nDESCRIPTION: Command for users to install a plugin that has been published to RubyGems.org using the logstash-plugin tool.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_25\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin install logstash-codec-mypluginname\n```\n\n----------------------------------------\n\nTITLE: Implementing getId Method in Java\nDESCRIPTION: This Java code snippet implements the `getId` method, which is part of the required interface for filter plugins. The method simply returns the plugin's unique identifier (id), assigned during instantiation. Dependencies involve correctly initializing the 'id' in the plugin's constructor.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-filter-plugin.md#_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic String getId() {\n    return id;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradle build.gradle for Java plugin packaging in Logstash\nDESCRIPTION: A Gradle configuration snippet that shows how to set up essential metadata for packaging a Java plugin as a Ruby gem. It includes plugin information such as group, version, description, license, and other required properties.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_8\n\nLANGUAGE: java\nCODE:\n```\n// ===========================================================================\n// plugin info\n// ===========================================================================\ngroup                      'org.logstashplugins' // must match the package of the main plugin class\nversion                    \"${file(\"VERSION\").text.trim()}\" // read from required VERSION file\ndescription                = \"Example Java filter implementation\"\npluginInfo.licenses        = ['Apache-2.0'] // list of SPDX license IDs\npluginInfo.longDescription = \"This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using \\$LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\npluginInfo.authors         = ['Elasticsearch']\npluginInfo.email           = ['info@elastic.co']\npluginInfo.homepage        = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\npluginInfo.pluginType      = \"filter\"\npluginInfo.pluginClass     = \"JavaFilterExample\"\npluginInfo.pluginName      = \"java_filter_example\"\n// ===========================================================================\n```\n\n----------------------------------------\n\nTITLE: Minimal Logstash configuration for testing Java output plugin\nDESCRIPTION: A basic Logstash configuration file that demonstrates how to use a Java output plugin. It includes a generator input that creates a single message and routes it to the Java output plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_11\n\nLANGUAGE: java\nCODE:\n```\ninput {\n  generator { message => \"Hello world!\" count => 1 }\n}\noutput {\n  java_output_example {}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Centralized Management Settings Logstash YAML\nDESCRIPTION: This snippet demonstrates essential `xpack.management` settings in the `logstash.yml` file required to enable and configure centralized pipeline management. It shows how to enable the feature, specify the Elasticsearch host where configurations are stored, provide authentication credentials (username/password) for Elasticsearch, set the polling interval for updates, and list the specific pipeline IDs to be managed. This configuration assumes basic authentication and no SSL.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuring-centralized-pipelines.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nxpack.management.enabled: true\nxpack.management.elasticsearch.hosts: \"http://localhost:9200/\"\nxpack.management.elasticsearch.username: logstash_admin_user\nxpack.management.elasticsearch.password: t0p.s3cr3t\nxpack.management.logstash.poll_interval: 5s\nxpack.management.pipeline.id: [\"apache\", \"cloudwatch_logs\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Input, Filter and Output with Credentials\nDESCRIPTION: This JavaScript code demonstrates how to configure the Logstash input, filter, and output plugins to use authentication. The `user` and `password` parameters are configured inside each plugin to specify the credentials for the `logstash_internal` user created previously. This allows each plugin to authenticate with Elasticsearch.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_3\n\nLANGUAGE: js\nCODE:\n```\ninput {\n  elasticsearch {\n    ...\n    user => logstash_internal\n    password => x-pack-test-password\n  }\n}\nfilter {\n  elasticsearch {\n    ...\n    user => logstash_internal\n    password => x-pack-test-password\n  }\n}\noutput {\n  elasticsearch {\n    ...\n    user => logstash_internal\n    password => x-pack-test-password\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Decoding JSON Content from File Input in Logstash\nDESCRIPTION: This snippet configures Logstash to decode JSON formatted data from a specified file, creating one event per JSON array element. It employs the JSON codec for decoding, facilitating ingestion of structured JSON data from files. Dependencies include the JSON codec plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/data-deserialization.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\ninput {\n  file {\n    path => \"/path/to/myfile.json\"\n    codec =>\"json\"\n  }\n}\n...\n```\n\n----------------------------------------\n\nTITLE: Reading Events from Dead Letter Queue in YAML\nDESCRIPTION: A basic Logstash pipeline configuration that reads events from the dead letter queue and outputs them to standard output with metadata included. It demonstrates the path configuration, commit_offsets option, and pipeline_id specification.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/dead-letter-queues.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ninput {\n  dead_letter_queue {\n    path => \"/path/to/data/dead_letter_queue\" <1>\n    commit_offsets => true <2>\n    pipeline_id => \"main\" <3>\n  }\n}\n\noutput {\n  stdout {\n    codec => rubydebug { metadata => true }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Upgrading Logstash Package (apt/yum) in Bash\nDESCRIPTION: Commands to upgrade the Logstash package using the system's package manager. `apt-get upgrade` is used on Debian/Ubuntu systems, while `yum update` is used on Red Hat/CentOS systems. These commands fetch and install the latest Logstash version available in the configured repositories (which should be updated to 9.x beforehand). Requires appropriate privileges (e.g., sudo).\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/upgrading-using-package-managers.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\napt-get upgrade logstash\n```\n\nLANGUAGE: bash\nCODE:\n```\nyum update logstash\n```\n\n----------------------------------------\n\nTITLE: Using an API Key in Logstash Elasticsearch Filter (Ruby)\nDESCRIPTION: Configures the Logstash Elasticsearch filter plugin to authenticate using an API key for looking up data in Elasticsearch. Set the `api_key` parameter within the `elasticsearch` filter block to the `id:api_key` string.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_15\n\nLANGUAGE: ruby\nCODE:\n```\nfilter {\n  elasticsearch {\n    api_key => \"TiNAGG4BaaMdaH1tRfuU:KnR6yE41RrSowb0kQ0HWoA\" \n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing a Logstash codec plugin with basic configuration\nDESCRIPTION: Command for running Logstash with a basic configuration that uses the example codec plugin on stdin input and outputs to stdout with rubydebug formatting.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_21\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash -e 'input { stdin{ codec => example{}} } output {stdout { codec => rubydebug }}'\n```\n\n----------------------------------------\n\nTITLE: Translate Field Value - Logstash\nDESCRIPTION: This snippet configures the `translate` filter to replace the value of the `response_code` field with a descriptive string, based on a mapping defined in the `dictionary`. It then removes the original `response_code` field. It uses a hash to map the response codes to corresponding descriptions. Requires only the `translate` filter to be installed.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/lookup-enrichment.md#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\nfilter {\n  translate {\n    field => \"response_code\"\n    destination => \"http_response\"\n    dictionary => {\n      \"200\" => \"OK\"\n      \"403\" => \"Forbidden\"\n      \"404\" => \"Not Found\"\n      \"408\" => \"Request Timeout\"\n    }\n    remove_field => \"response_code\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Logstash Collector Pattern in YAML\nDESCRIPTION: This configuration demonstrates the collector pattern in Logstash, where multiple input pipelines (beats, kafka) send data to a common output pipeline (partner). The 'partner' pipeline then applies a filter to remove sensitive data before sending it to Elasticsearch. This pattern is useful for enforcing consistent processing across multiple input sources.  The collector pattern allows multiple pipelines to flow into a single pipeline.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/pipeline-to-pipeline.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\n- pipeline.id: beats\n  config.string: |\n    input { beats { port => 5044 } }\n    output { pipeline { send_to => [commonOut] } }\n- pipeline.id: kafka\n  config.string: |\n    input { kafka { ... } }\n    output { pipeline { send_to => [commonOut] } }\n- pipeline.id: partner\n  # This common pipeline enforces the same logic whether data comes from Kafka or Beats\n  config.string: |\n    input { pipeline { address => commonOut } }\n    filter {\n      # Always remove sensitive data from all input sources\n      mutate { remove_field => 'sensitive-data' }\n    }\n    output { elasticsearch { } }\n```\n\n----------------------------------------\n\nTITLE: Enabling TLS/SSL in Logstash Elasticsearch Output (Logstash Config)\nDESCRIPTION: Configures the Logstash Elasticsearch output plugin to use TLS/SSL encryption when connecting to an on-premise Elasticsearch cluster. Set `ssl_enabled` to true and provide the path to the Certificate Authority's certificate file using `ssl_certificate_authorities`. This step is not needed for hosted Elasticsearch Service on Elastic Cloud.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_6\n\nLANGUAGE: logstash-config\nCODE:\n```\noutput {\n  elasticsearch {\n    ...\n    ssl_enabled => true\n    ssl_certificate_authorities => '/path/to/cert.pem'\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Dropping Events Conditionally with Logstash Drop Filter in JSON\nDESCRIPTION: Utilizes the Logstash drop filter combined with a conditional to discard events where the 'loglevel' field equals 'debug'. This technique efficiently removes unwanted debug messages from further processing. It requires Logstash’s drop filter plugin and relies on the correct existence of a 'loglevel' field in events.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/core-operations.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\nfilter {\n  if [loglevel] == \"debug\" {\n    drop { }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TCP Port with Environment Variable (Ruby)\nDESCRIPTION: This snippet demonstrates how to configure a TCP input port using an environment variable in Logstash.  It utilizes the `${TCP_PORT}` syntax to reference the environment variable's value within the `port` option.  If the variable is undefined, Logstash will throw a configuration error.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/environment-variables.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\ninput {\n  tcp {\n    port => \"${TCP_PORT}\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring a Java Output Plugin Class Implementing Logstash Output Interface\nDESCRIPTION: This Java snippet shows the declaration of a Logstash Java output plugin class annotated with @LogstashPlugin and implementing the Output interface. The annotation's name property defines the plugin identity in pipeline configurations. The class adheres to Logstash plugin API requirements by implementing this interface and following naming conventions.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n@LogstashPlugin(name=\"java_output_example\")\npublic class JavaOutputExample implements Output {\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradle Packaging Task\nDESCRIPTION: This Java code block shows configuration settings for the Gradle packaging task used to create Ruby gems for Java plugins. It specifies metadata like the group, version, description, licenses, authors, homepage, plugin type, plugin class, and plugin name. These values are crucial for gem packaging and integration with Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-filter-plugin.md#_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n// ===========================================================================\n// plugin info\n// ===========================================================================\ngroup                      'org.logstashplugins' // must match the package of the main plugin class\nversion                    \"${file(\"VERSION\").text.trim()}\" // read from required VERSION file\ndescription                = \"Example Java filter implementation\"\npluginInfo.licenses        = ['Apache-2.0'] // list of SPDX license IDs\npluginInfo.longDescription = \"This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using \\$LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\npluginInfo.authors         = ['Elasticsearch']\npluginInfo.email           = ['info@elastic.co']\npluginInfo.homepage        = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\npluginInfo.pluginType      = \"filter\"\npluginInfo.pluginClass     = \"JavaFilterExample\"\npluginInfo.pluginName      = \"java_filter_example\"\n// ===========================================================================\n```\n\n----------------------------------------\n\nTITLE: Sample Logstash Configuration for Java Input Plugin\nDESCRIPTION: This minimal configuration file demonstrates how to run Logstash with the Java input plugin, directing output to standard console with a rubydebug codec. Save this configuration to a file (e.g., java_input.conf) and execute Logstash with it to test plugin functionality. Output shows messages with timestamps.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\n```java\ninput {\n  java_input_example {}\n}\noutput {\n  stdout { codec => rubydebug }\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Plugin Dependencies (Bundler)\nDESCRIPTION: This snippet shows how to install the dependencies for a Logstash plugin using Bundler.  It assumes a `Gemfile` is present in the plugin's root directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Vendoring External Plugin Dependencies - Shell\nDESCRIPTION: Executes Rake to fetch and install external dependencies specified in vendor.json. Rake and Ruby must be installed. This is necessary if your plugin relies on resources beyond the declared gems. Must be executed prior to build or test steps when vendor.json exists.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nrake vendor\n```\n\n----------------------------------------\n\nTITLE: Creating Elasticsearch API Key via REST API - Console\nDESCRIPTION: This snippet demonstrates how to create an API key in Elasticsearch using a REST API POST request to the `_security/api_key` endpoint. The request JSON specifies the API key name and the associated role descriptors granting privileges such as 'monitor' and 'manage_logstash_pipelines' on the cluster level. The API key produced allows Logstash instances to authenticate securely for central management purposes.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_19\n\nLANGUAGE: Console\nCODE:\n```\nPOST /_security/api_key\n{\n  \"name\": \"logstash_host001\", <1>\n  \"role_descriptors\": {\n    \"logstash_monitoring\": { <2>\n      \"cluster\": [\"monitor\", \"manage_logstash_pipelines\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Filebeat Output to Kafka - YAML\nDESCRIPTION: This YAML snippet specifies how to configure Filebeat's output to send events to a Kafka broker instead of directly to Elasticsearch. The Elasticsearch output is commented out, and Kafka becomes the active output. Key parameters include the `hosts` setting (address of Kafka brokers), the `topic` to send messages to (usually `filebeat`), and the JSON codec settings for message formatting. This configuration file must be saved in `filebeat.yml`, and the Kafka service must be accessible at the specified address.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/use-filebeat-modules-kafka.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n#output.elasticsearch:\n  #hosts: [\"localhost:9200\"]\noutput.kafka:\n  hosts: [\"kafka:9092\"]\n  topic: \"filebeat\"\n  codec.json:\n    pretty: false\n```\n\n----------------------------------------\n\nTITLE: Running Logstash Core Tests with Gradle and RSpec - Shell\nDESCRIPTION: These commands run Logstash's unit and integration tests using Gradle and RSpec. Gradle is required and should be run from the project root. Optionally, RSpec can be used to run all or specific tests. For the first RSpec run, dependencies must be bootstrapped via Gradle. Outputs: test results and summaries.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew test\n```\n\nLANGUAGE: shell\nCODE:\n```\nbin/rspec\n```\n\nLANGUAGE: shell\nCODE:\n```\nbin/rspec spec/foo/bar_spec.rb\n```\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew bootstrap\n```\n\n----------------------------------------\n\nTITLE: Java Input Plugin Constructor and Configuration Initialization in Java\nDESCRIPTION: Implements the constructor required by Logstash input plugins accepting an id, Configuration, and Context. The constructor retrieves configuration values for the 'count' and 'prefix' settings via PluginConfigSpec instances and stores them for internal use. It is crucial for validating configuration and preparing the plugin state before execution.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nprivate String id;\nprivate long count;\nprivate String prefix;\n\npublic JavaInputExample(String id, Configuration config, Context context) {\n    this.id = id;\n    count = config.get(EVENT_COUNT_CONFIG);\n    prefix = config.get(PREFIX_CONFIG);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Gemfile for Logstash Plugins in Ruby\nDESCRIPTION: A basic Gemfile for a Logstash plugin that specifies the gem dependencies. It sources from rubygems.org, includes the gemspec, and depends on the Logstash gem from GitHub's master branch.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_12\n\nLANGUAGE: ruby\nCODE:\n```\nsource 'https://rubygems.org'\ngemspec\ngem \"logstash\", :github => \"elastic/logstash\", :branch => \"master\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Grok Filter in Logstash JSON\nDESCRIPTION: A Logstash filter configuration using the Grok plugin to parse combined Apache log format. The filter extracts structured fields from unstructured log data using the COMBINEDAPACHELOG pattern.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_10\n\nLANGUAGE: json\nCODE:\n```\nfilter {\n    grok {\n        match => { \"message\" => \"%{COMBINEDAPACHELOG}\"}\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Elasticsearch API Key for Logstash Data Publishing (Console)\nDESCRIPTION: Demonstrates using the Elasticsearch Create API key API (`POST /_security/api_key`) to generate an API key. This key is intended for Logstash to publish data to Elasticsearch, granting necessary cluster (`monitor`, `manage_ilm`, `read_ilm`) and index (`view_index_metadata`, `create_doc` on `logstash-*`) privileges via the `logstash_writer` role descriptor.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nPOST /_security/api_key\n{\n  \"name\": \"logstash_host001\", \n  \"role_descriptors\": {\n    \"logstash_writer\": { \n      \"cluster\": [\"monitor\", \"manage_ilm\", \"read_ilm\"],\n      \"index\": [\n        {\n          \"names\": [\"logstash-*\"],\n          \"privileges\": [\"view_index_metadata\", \"create_doc\"]\n        }\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Logstash Configuration Filter for Elasticsearch Output\nDESCRIPTION: This section contains a Logstash filter configuration designed to transform incoming data before indexing into Elasticsearch. It specifies parsing rules, field mappings, or conditional logic to ensure data consistency. Dependencies include Logstash filter plugins, such as grok, mutate, or date.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/sequel-NOTICE.txt#_snippet_1\n\nLANGUAGE: Configuration\nCODE:\n```\nfilter {\n  grok {\n    match => { \"message\" => \"%{COMBINEDAPACHELOG}\" }\n  }\n  date {\n    match => [\"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\"]\n  }\n  mutate {\n    convert => { \"response\" => \"integer\" }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building a Logstash plugin gem\nDESCRIPTION: Builds the plugin gem file from the plugin's gemspec, preparing it for installation in Logstash. Ensure that your plugin's gemspec is correctly configured before executing this command to generate the .gem package for deployment.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/input-plugin/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngem build logstash-filter-awesome.gemspec\n```\n\n----------------------------------------\n\nTITLE: Configuring Filebeat Input for Logstash (YAML syntax)\nDESCRIPTION: Specifies Filebeat's configuration to ship log files to Logstash on port 5044. Dependencies include the Filebeat application and proper path to log files. The 'paths' field points to log files, and a custom field 'type' adds context to events. This setup facilitates log forwarding to Logstash for processing.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/multiple-input-output-plugins.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nfilebeat.inputs:\n- type: log\n  paths:\n    - /var/log/*.log <1>\n  fields:\n    type: syslog <2>\noutput.logstash:\n  hosts: [\"localhost:5044\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Elasticsearch Output with SSL\nDESCRIPTION: This Ruby code snippet shows how to configure the Elasticsearch output plugin in Logstash to use SSL/TLS for secure communication with an on-premise Elasticsearch cluster.  It requires that the Elasticsearch cluster's self-signed certificate authority (CA) certificate has been copied to the Logstash instance. The `hosts` parameter specifies the Elasticsearch cluster's URL with https, and the `ssl_certificate_authorities` parameter specifies the path to the CA certificate.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\noutput {\n  elasticsearch {\n    hosts => [\"https://...\"] <1>\n    ssl_certificate_authorities => ['/etc/logstash/config/certs/ca.crt'] <2>\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Pipeline with Elastic Integrations in Ruby\nDESCRIPTION: This Ruby snippet demonstrates a Logstash pipeline configuration designed to extend Elastic integrations. It uses the elastic_agent input plugin to ingest data on port 5044, applies the filter-elastic_integration plugin as the first filter to process events consistently with Elastic integration pipelines, and includes a translate filter for metadata transformation. The output section routes events to different Elasticsearch clusters based on tenant metadata, using cloud ID and API key authentication. Key dependencies include the Logstash elastic_agent input, filter-elastic_integration filter, translate filter, and the output-elasticsearch plugin. Inputs are events received via the elastic_agent plugin; outputs are conditionally sent to Elasticsearch clusters. Limitations include ensuring the filter-elastic_integration is the first filter to preserve pipeline behavior.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/using-logstash-with-elastic-integrations.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\ninput {\n  elastic_agent {\n    port => 5044\n  }\n}\n\nfilter {\n  elastic_integration{\n    cloud_id => \"<cloud id>\"\n    cloud_auth => \"<your_cloud-auth\"\n  }\n\n  translate {\n    source => \"[http][host]\"\n    target => \"[@metadata][tenant]\"\n    dictionary_path => \"/etc/conf.d/logstash/tenants.yml\"\n  }\n}\n\noutput {\n  if [@metadata][tenant] == \"tenant01\" {\n    elasticsearch {\n      cloud_id => \"<cloud id>\"\n      api_key => \"<api key>\"\n    }\n  } else if [@metadata][tenant] == \"tenant02\" {\n    elasticsearch {\n      cloud_id => \"<cloud id>\"\n      api_key => \"<api key>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Logstash Output Plugin Class in Ruby\nDESCRIPTION: This snippet demonstrates how to define a new Logstash output plugin by subclassing the LogStash::Outputs::Base class. It includes the required dependencies, sets the concurrency mode, and outlines the structure with placeholder methods (`register` and `multi_receive`). Dependencies include Logstash core modules. The class name should match the plugin's name, and configuration, initialization, and data reception methods must be implemented. This is the main entry point for custom plugin logic, and expected inputs are event arrays for processing. No outputs are produced in this example; customization is necessary for functionality. Thread safety depends on the concurrency mode set via the `concurrency` directive.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"logstash/outputs/base\"\nrequire \"logstash/namespace\"\n\n# Add any asciidoc formatted documentation here\n# An example output that does nothing.\nclass LogStash::Outputs::Example < LogStash::Outputs::Base\n  config_name \"example\"\n\n  # This sets the concurrency behavior of this plugin. By default it is :legacy, which was the standard\n  # way concurrency worked before Logstash 2.4\n  #\n  # You should explicitly set it to either :single or :shared as :legacy will be removed in Logstash 6.0\n  #\n  # When configured as :single a single instance of the Output will be shared among the\n  # pipeline worker threads. Access to the `#multi_receive/#multi_receive_encoded/#receive` method will be synchronized\n  # i.e. only one thread will be active at a time making threadsafety much simpler.\n  #\n  # You can set this to :shared if your output is threadsafe. This will maximize\n  # concurrency but you will need to make appropriate uses of mutexes in `#multi_receive/#receive`.\n  #\n  # Only the `#multi_receive/#multi_receive_encoded` methods need to actually be threadsafe, the other methods\n  # will only be executed in a single thread\n  concurrency :single\n\n  public\n  def register\n  end # def register\n\n  public\n  # Takes an array of events\n  # Must be threadsafe if `concurrency :shared` is set\n  def multi_receive(events)\n  end # def multi_receive\nend # class LogStash::Outputs::Example\n\n```\n\n----------------------------------------\n\nTITLE: Viewing the built gem file\nDESCRIPTION: Shows the expected output file name pattern for the built gem, where the version number comes from the gemspec file.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nlogstash-codec-mypluginname-0.1.0.gem\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with Settings Configuration\nDESCRIPTION: This shell script shows how to run a Logstash Docker container with bind-mounted settings files. It demonstrates mounting an entire directory ('~/settings/') to '/usr/share/logstash/config/' or a single file ('~/settings/logstash.yml') to '/usr/share/logstash/config/logstash.yml' inside the container. This allows for overriding default Logstash settings using custom configuration files.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/docker-config.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndocker run --rm -it -v ~/settings/:/usr/share/logstash/config/ docker.elastic.co/logstash/logstash:9.0.0\n```\n\nLANGUAGE: sh\nCODE:\n```\ndocker run --rm -it -v ~/settings/logstash.yml:/usr/share/logstash/config/logstash.yml docker.elastic.co/logstash/logstash:9.0.0\n```\n\n----------------------------------------\n\nTITLE: Defining Codec Configuration Name\nDESCRIPTION: Sets the name that the plugin will be referenced by in the Logstash configuration file.  This is what users will reference in their Logstash configurations.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\n  config_name \"example\"\n```\n\n----------------------------------------\n\nTITLE: Generating Event Fingerprints with Logstash Fingerprint Filter in JSON\nDESCRIPTION: Defines a fingerprint filter configuration that creates a consistent SHA1 hash based on the concatenation of 'IP', '@timestamp', and 'message' fields. The resulting hash is stored in the metadata field '@metadata.generated_id'. This facilitates unique event identification or deduplication. Requires the fingerprint filter plugin and the SHA1 hashing method. The key '0123' is used as a secret in hash generation.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/core-operations.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\nfilter {\n  fingerprint {\n    source => [\"IP\", \"@timestamp\", \"message\"]\n    method => \"SHA1\"\n    key => \"0123\"\n    target => \"[@metadata][generated_id]\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Reverse DNS Lookup - Logstash\nDESCRIPTION: This snippet demonstrates the use of the `dns` filter in Logstash to perform a reverse DNS lookup. It takes the IP address from the `source_host` field and resolves it to a domain name. The domain name then replaces the original IP address in the `source_host` field. This plugin requires no specific dependencies beyond Logstash itself.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/lookup-enrichment.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\nfilter {\n  dns {\n    reverse => [ \"source_host\" ]\n    action => \"replace\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning a Logstash Plugin Repository (Git)\nDESCRIPTION: This snippet demonstrates how to clone a Logstash plugin repository from GitHub using Git. Replace `GITUSERNAME` and `MYPLUGINNAME` with the appropriate values. It offers both HTTPS and SSH cloning options.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/GITUSERNAME/logstash-filter-MYPLUGINNAME.git\n```\n\nLANGUAGE: sh\nCODE:\n```\ngit clone git@github.com:GITUSERNAME/logstash-filter-MYPLUGINNAME.git\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with Custom Filter Configuration\nDESCRIPTION: This snippet runs Logstash with a custom filter configuration, modifying the `message` parameter of the `example` filter.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_24\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash -e 'input { stdin{} } filter { example { message => \"This is a new message!\"} } output {stdout { codec => rubydebug }}'\n```\n\n----------------------------------------\n\nTITLE: Copying Elasticsearch Fields - Logstash\nDESCRIPTION: This snippet configures the `elasticsearch` filter to copy fields from previous log events in Elasticsearch.  It looks for an event with type \"start\" that matches the current event's \"opid\". It copies the `@timestamp` field from the matching \"start\" event to the \"started\" field of the current event. The configuration also includes date and ruby filters to calculate the time duration in hours between the two events.  This requires an Elasticsearch server and uses the `date` and `ruby` filters along with the `elasticsearch` filter.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/lookup-enrichment.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n  if [type] == \"end\" {\n     elasticsearch {\n        hosts => [\"es-server\"]\n        query => \"type:start AND operation:%{[opid]}\"\n        fields => { \"@timestamp\" => \"started\" }\n     }\n     date {\n        match => [\"[started]\", \"ISO8601\"]\n        target => \"[started]\"\n     }\n     ruby {\n        code => 'event.set(\"duration_hrs\", (event.get(\"@timestamp\") - event.get(\"started\")) / 3600) rescue nil'\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Including Required Dependencies for Logstash Output Plugins in Ruby\nDESCRIPTION: These Ruby require statements are necessary for Logstash output plugins to inherit from Logstash's base output class and access the Logstash namespace. Any additional dependencies (including Ruby gems or Logstash modules) needed by the plugin should also be required here. This ensures that all core functionalities and APIs are available to the plugin code. Both lines must be included at the top of your plugin file.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"logstash/outputs/base\"\nrequire \"logstash/namespace\"\n```\n\n----------------------------------------\n\nTITLE: Calculating Logstash Persistent Queue Capacity Formula\nDESCRIPTION: Provides a formula to estimate the required byte capacity for a Logstash persistent queue. This calculation helps determine the appropriate value for the `queue.max_bytes` setting based on incoming data rate, event size, and tolerated downtime duration, incorporating an overhead multiplication factor.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/persistent-queues.md#_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nBytes Received Per Second = Incoming Events Per Second * Raw Event Byte Size\nBytes Received Per Hour = Bytes Received per Second * 3600s\nRequired Queue Capacity = (Bytes Received Per Hour * Tolerated Hours of Downtime) * Multiplication Factor <1>\n```\n\n----------------------------------------\n\nTITLE: Using Wildcards for Pipeline IDs Logstash YAML\nDESCRIPTION: This snippet illustrates the usage of wildcards (`*`) within the `xpack.management.pipeline.id` setting in `logstash.yml`. It shows how to specify multiple pipeline IDs using patterns, such as matching all IDs ending in \"logs\" (`*logs`) or containing \"apache\" (`*apache*`), in addition to specifying exact IDs (\"tomcat_log\"). This feature allows Logstash to pick up new pipelines matching the pattern without requiring a restart.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuring-centralized-pipelines.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nxpack.management.pipeline.id: [\"*logs\", \"*apache*\", \"tomcat_log\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring GeoIP Database Management Settings in logstash.yml\nDESCRIPTION: Defines configuration parameters in logstash.yml for enabling/disabling GeoIP database updates, setting download endpoint, and poll interval. Requires the xpack.geoip settings. This configures how Logstash manages GeoIP databases, with dependencies on valid license and enabled features.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuring-geoip-database-management.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nxpack.geoip.downloader.enabled: true\nxpack.geoip.downloader.endpoint: \"https://geoip.elastic.co/v1/database\"\nxpack.geoip.downloader.poll.interval: \"24h\"\n```\n\n----------------------------------------\n\nTITLE: Generating SSL Certificate and Key with OpenSSL - Shell\nDESCRIPTION: Generates a self-signed X.509 SSL certificate and 2048-bit RSA key using the openssl command-line tool. Requires openssl to be installed on your system. The command creates 'lumberjack.key' (private key) and 'lumberjack.cert' (certificate), with the Common Name ('/CN=localhost') typically set to the upstream Logstash machine's hostname. Required for encrypted communication using the Lumberjack protocol.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-lumberjack.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nopenssl req -x509 -batch -nodes -newkey rsa:2048 -keyout lumberjack.key -out lumberjack.cert -subj /CN=localhost\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Output with SSL/TLS\nDESCRIPTION: This snippet demonstrates how to configure the Logstash output plugin to use SSL/TLS. It includes settings such as `ssl_enabled`, `ssl_key`, `ssl_certificate`, and `ssl_certificate_authorities` to secure the communication. These settings allow the Logstash client to authenticate to the Logstash server and transmit encrypted data.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-native.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\noutput {\n  logstash {\n    ...\n\n    ssl_enabled => true\n    ssl_key => \"client.pkcs8.key\"\n    ssl_certificate => \"client.crt\"\n    ssl_certificate_authorities => \"ca.crt\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Logstash Configuration Example (syslog.conf)\nDESCRIPTION: This YAML configuration file (`syslog.conf`) defines a Logstash pipeline to listen for syslog messages over UDP port 514, then forward them to Elasticsearch and output them to the console with rubydebug codec. It specifies an input, output, and associated settings for each.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash-windows.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\n# Sample Logstash configuration for receiving\n# UDP syslog messages over port 514\n\ninput {\n  udp {\n    port => 514\n    type => \"syslog\"\n  }\n}\n\noutput {\n  elasticsearch { hosts => [\"localhost:9200\"] }\n  stdout { codec => rubydebug }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Elasticsearch API Key for Logstash Monitoring (Console)\nDESCRIPTION: Uses the Elasticsearch Create API key API (`POST /_security/api_key`) to generate an API key specifically for Logstash to send monitoring data. Assigns necessary privileges (`monitor` cluster privilege, `create_index` and `create` on `.monitoring-ls-*` indices) via the `logstash_monitoring` role descriptor.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_16\n\nLANGUAGE: console\nCODE:\n```\nPOST /_security/api_key\n{\n  \"name\": \"logstash_host001\", \n  \"role_descriptors\": {\n    \"logstash_monitoring\": { \n      \"cluster\": [\"monitor\"],\n      \"index\": [\n        {\n          \"names\": [\".monitoring-ls-*\"],\n          \"privileges\": [\"create_index\", \"create\"]\n        }\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Full RSpec Test for Logging 'bound' Status in ZeroMQ Plugin (Ruby)\nDESCRIPTION: This is the complete RSpec test for asserting that the ZeroMQ output plugin logs a 'bound' info message in server mode. The test uses a test double as the logger, allows 'debug' calls, and expects the 'info' log to match the bound message and specific TCP address. It also calls 'register' and 'do_close' to fully exercise the plugin lifecycle. Requires RSpec, Logstash output plugin, and proper logger doubles. Inputs are plugin configuration options, expected outputs are logger method calls. The test fails if the log message is incorrect.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/contributing-patch-plugin.md#_snippet_4\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"logstash/outputs/zeromq\"\nrequire \"logstash/devutils/rspec/spec_helper\"\n\ndescribe LogStash::Outputs::ZeroMQ do\n  let(:output) { described_class.new(\"mode\" => \"server\", \"topology\" => \"pushpull\") }\n  let(:tracer) { double(\"logger\") }\n\n  context \"when in server mode\" do\n    it \"a ‘bound’ info line is logged\" do\n      allow(tracer).to receive(:debug)\n      output.logger = tracer\n      expect(tracer).to receive(:info).with(\"0mq: bound\", {:address=>\"tcp://127.0.0.1:2120\"})\n      output.register\n      output.do_close\n    end\n  end\nend\n```\n\n----------------------------------------\n\nTITLE: Loading a Custom Plugin Source with --path.plugins - Shell\nDESCRIPTION: This command runs Logstash with the --path.plugins flag to load plugin source code directly from a specified file system location, useful for development and testing of custom plugins prior to gem creation. The directory must adhere to a hierarchy: PATH/logstash/TYPE/NAME.rb, where TYPE is inputs, filters, outputs, or codecs. The example assumes a custom plugin source resides in /opt/shared/lib/logstash/inputs/.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/working-with-plugins.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n# supposing the code is in /opt/shared/lib/logstash/inputs/my-custom-plugin-code.rb\nbin/logstash --path.plugins /opt/shared/lib\n```\n\n----------------------------------------\n\nTITLE: Parsing Elasticsearch Search Response for geoip Criteria - Sample Output in JSON\nDESCRIPTION: This JSON snippet shows a typical Elasticsearch response when searching for log entries where the geoip.city_name field matches a specified value (such as 'Buffalo'). The structure includes metadata, total hits, and an array of matched documents with fields such as IP address, geographic data, agent string, timestamps, and the raw log message, reflecting proper field extraction and enrichment by Logstash. This output format is essential for validating both search correctness and successful pipeline configuration. The result is for demonstration purposes and may vary based on input log data, enrichment options, and Logstash configuration.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"took\": 9,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 2,\n    \"max_score\": 2.6390574,\n    \"hits\": [\n      {\n        \"_index\": \"logstash-2017.11.09\",\n        \"_type\": \"doc\",\n        \"_id\": \"L4zDnl8BW52sR0fx5whY\",\n        \"_score\": 2.6390574,\n        \"_source\": {\n          \"request\": \"/blog/geekery/disabling-battery-in-ubuntu-vms.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29\",\n          \"agent\": \"\\\"Tiny Tiny RSS/1.11 (http://tt-rss.org/)\\\"\",\n          \"geoip\": {\n            \"timezone\": \"America/New_York\",\n            \"ip\": \"198.46.149.143\",\n            \"latitude\": 42.8864,\n            \"continent_code\": \"NA\",\n            \"city_name\": \"Buffalo\",\n            \"country_name\": \"United States\",\n            \"country_code2\": \"US\",\n            \"dma_code\": 514,\n            \"country_code3\": \"US\",\n            \"region_name\": \"New York\",\n            \"location\": {\n              \"lon\": -78.8781,\n              \"lat\": 42.8864\n            },\n            \"postal_code\": \"14202\",\n            \"region_code\": \"NY\",\n            \"longitude\": -78.8781\n          },\n          \"offset\": 22795,\n          \"auth\": \"-\",\n          \"ident\": \"-\",\n          \"verb\": \"GET\",\n          \"prospector\": {\n            \"type\": \"log\"\n          },\n          \"input\": {\n            \"type\": \"log\"\n          },\n          \"source\": \"/path/to/file/logstash-tutorial.log\",\n          \"message\": \"198.46.149.143 - - [04/Jan/2015:05:29:13 +0000] \\\"GET /blog/geekery/disabling-battery-in-ubuntu-vms.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29 HTTP/1.1\\\" 200 9316 \\\"-\\\" \\\"Tiny Tiny RSS/1.11 (http://tt-rss.org/)\\\"\",\n          \"tags\": [\n            \"beats_input_codec_plain_applied\"\n          ],\n          \"referrer\": \"\\\"-\\\"\",\n          \"@timestamp\": \"2017-11-09T03:11:35.321Z\",\n          \"response\": \"200\",\n          \"bytes\": \"9316\",\n          \"clientip\": \"198.46.149.143\",\n          \"@version\": \"1\",\n          \"beat\": {\n            \"name\": \"My-MacBook-Pro.local\",\n            \"hostname\": \"My-MacBook-Pro.local\",\n            \"version\": \"6.0.0\"\n          },\n          \"host\": \"My-MacBook-Pro.local\",\n          \"httpversion\": \"1.1\",\n          \"timestamp\": \"04/Jan/2015:05:29:13 +0000\"\n        }\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Java Plugin in Logstash\nDESCRIPTION: This shell command installs the packaged Java plugin (Ruby gem) into Logstash.  The `--no-verify` flag skips the verification step and the `--local` argument specifies the local path to the gem file.  This assumes the gem file has been created by the Gradle packaging task.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-filter-plugin.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin install --no-verify --local /path/to/javaPlugin.gem\n```\n\n----------------------------------------\n\nTITLE: Defining Logstash Pipeline Sections - JavaScript\nDESCRIPTION: Illustrates the basic structure of a Logstash pipeline configuration file, separated into distinct sections: input, filter, and output. Each section configures one or more plugins to process event data sequentially. Comments demonstrate usage conventions, and the snippet outlines how multiple filters or outputs are ordered and applied.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n# This is a comment. You should use comments to describe\n# parts of your configuration.\ninput {\n  ...\n}\n\nfilter {\n  ...\n}\n\noutput {\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Java Plugin Metadata in build.gradle\nDESCRIPTION: Example `build.gradle` configuration block (using Groovy syntax) for a Logstash Java plugin. It defines essential metadata like group ID, version (read from a VERSION file), description, licenses, authors, plugin type (`filter`, `input`, `output`, or `codec`), main plugin class, and plugin name, which are used during the packaging process.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_9\n\nLANGUAGE: groovy\nCODE:\n```\n// ===========================================================================\n// plugin info\n// ===========================================================================\ngroup                      'org.logstashplugins' // must match the package of the main plugin class\nversion                    \"${file(\"VERSION\").text.trim()}\" // read from required VERSION file\ndescription                = \"Example Java filter implementation\"\npluginInfo.licenses        = ['Apache-2.0'] // list of SPDX license IDs\npluginInfo.longDescription = \"This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using \\$LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\npluginInfo.authors         = ['Elasticsearch']\npluginInfo.email           = ['info@elastic.co']\npluginInfo.homepage        = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\npluginInfo.pluginType      = \"filter\"\npluginInfo.pluginClass     = \"JavaFilterExample\"\npluginInfo.pluginName      = \"java_filter_example\"\n// ===========================================================================\n```\n\n----------------------------------------\n\nTITLE: Using an API Key in Logstash Elasticsearch Input (Ruby)\nDESCRIPTION: Configures the Logstash Elasticsearch input plugin to authenticate using an API key for reading data from Elasticsearch. Set the `api_key` parameter within the `elasticsearch` input block to the `id:api_key` string.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_14\n\nLANGUAGE: ruby\nCODE:\n```\ninput {\n  elasticsearch {\n    \"api_key\" => \"TiNAGG4BaaMdaH1tRfuU:KnR6yE41RrSowb0kQ0HWoA\" \n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting the Keystore Password\nDESCRIPTION: This shell script demonstrates how to set and use a password to protect the Logstash keystore. It sets the `LOGSTASH_KEYSTORE_PASS` environment variable, which needs to be accessible to the running Logstash instance and any users running keystore commands. This operation is optional but highly recommended for added security. Requires a valid Logstash installation.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/keystore.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nset +o history\nexport LOGSTASH_KEYSTORE_PASS=mypassword\nset -o history\nbin/logstash-keystore create\n```\n\n----------------------------------------\n\nTITLE: Sample Output from pqcheck with Healthy Page Files in Logstash\nDESCRIPTION: Example output from the pqcheck utility showing a healthy queue with three page files, displaying checkpoint information and acknowledgment status.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/persistent-queues.md#_snippet_4\n\nLANGUAGE: txt\nCODE:\n```\nubuntu@bigger:/usr/share/logstash$ bin/pqcheck /var/lib/logstash/queue/main/\nUsing bundled JDK: /usr/share/logstash/jdk\nOpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.\nChecking queue dir: /var/lib/logstash/queue/main\ncheckpoint.1, fully-acked: NO, page.1 size: 67108864\n  pageNum=1, firstUnackedPageNum=0, firstUnackedSeqNum=239675, minSeqNum=239675,\n  elementCount=218241, isFullyAcked=no\ncheckpoint.head, fully-acked: NO, page.2 size: 67108864\n  pageNum=2, firstUnackedPageNum=0, firstUnackedSeqNum=457916, minSeqNum=457916, elementCount=11805, isFullyAcked=no\ncheckpoint.0, fully-acked: NO, page.0 size: 67108864  <1>\n  pageNum=0, firstUnackedPageNum=0, firstUnackedSeqNum=176126, minSeqNum=1,\n  elementCount=239674, isFullyAcked=no <2>\n```\n\n----------------------------------------\n\nTITLE: Downloading External Dependencies with Rake (Shell)\nDESCRIPTION: Shell command `rake vendor` used to download and extract external file dependencies specified in the plugin's `vendor.json` file. This step is necessary before running tests or building the plugin if it relies on external files.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_13\n\nLANGUAGE: Shell\nCODE:\n```\nrake vendor\n```\n\n----------------------------------------\n\nTITLE: Creating Logstash Writer Role in Elasticsearch\nDESCRIPTION: This Shell code snippet shows how to use the Elasticsearch `role` API to create a `logstash_writer` role. This role grants the necessary cluster and index privileges for Logstash to manage index templates, monitor the cluster, and perform write, create, and create_index operations on Logstash indices. This role is required to allow logstash to write data to elasticsearch.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nPOST _security/role/logstash_writer\n{\n  \"cluster\": [\"manage_index_templates\", \"monitor\", \"manage_ilm\"], <1>\n  \"indices\": [\n    {\n      \"names\": [ \"logstash-*\" ], <2>\n      \"privileges\": [\"write\",\"create\",\"create_index\",\"manage\",\"manage_ilm\"]  <3>\n    }\n  ]\n}\n\n```\n\n----------------------------------------\n\nTITLE: Defining Configuration Parameters for Logstash Output Plugins in Ruby\nDESCRIPTION: This configuration declaration demonstrates how to define input parameters for the Logstash output plugin using the `config` directive. You can specify the name, validation type, default value, and flags such as required, deprecated, and obsolete for each parameter. Parameters are validated and optionally coerced by Logstash, providing robust input enforcement. The snippet is a template and should be customized with actual variable names and types as required by your plugin logic.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_4\n\nLANGUAGE: ruby\nCODE:\n```\n  config :variable_name, :validate => :variable_type, :default => \"Default value\", :required => boolean, :deprecated => boolean, :obsolete => string\n```\n\n----------------------------------------\n\nTITLE: Installing Logstash Development Gems with Gradle - Shell\nDESCRIPTION: This command uses Gradle to install Ruby development gem dependencies for Logstash. Requires Gradle to be installed and executed from the root of the Logstash repository. Input: none. Output: installs gems required for local development.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew installDevelopmentGems\n```\n\n----------------------------------------\n\nTITLE: Processing and Fixing Mapping Errors from Dead Letter Queue in JSON\nDESCRIPTION: A Logstash pipeline that reads failed events from the dead letter queue, fixes the mapping problem by removing the problematic field, and re-indexes the clean events into Elasticsearch.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/dead-letter-queues.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\ninput {\n  dead_letter_queue {\n    path => \"/path/to/data/dead_letter_queue/\" <1>\n  }\n}\nfilter {\n  mutate {\n    remove_field => \"[geoip][location]\" <2>\n  }\n}\noutput {\n  elasticsearch{\n    hosts => [ \"localhost:9200\" ] <3>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Combining and Referencing Fields with Conditionals and Sprintf Format (Logstash Configuration)\nDESCRIPTION: This pipeline filter snippet conditionally checks for the presence of nested metadata fields and uses the mutate filter to combine date and time into a new timestamp. It leverages both conditionals (if [@metadata][date] and [@metadata][time]) and sprintf format ('%{[@metadata][date]} %{[@metadata][time]}') within plugin settings. Requires fields '@metadata.date' and '@metadata.time' to exist. Inputs are event fields, outputs are new or modified event fields with formatted data.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/event-dependent-configuration.md#_snippet_1\n\nLANGUAGE: Logstash Configuration\nCODE:\n```\nfilter {\n  #  +----literal----+     +----literal----+\n  #  |               |     |               |\n  if [@metadata][date] and [@metadata][time] {\n    mutate {\n      add_field {\n        \"[@metadata][timestamp]\" => \"%{[@metadata][date]} %{[@metadata][time]}\"\n      # |                      |    |  |               |    |               | |\n      # +----string-argument---+    |  +--field-ref----+    +--field-ref----+ |\n      #                             +-------- sprintf format string ----------+\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Keys to the Keystore\nDESCRIPTION: The `add` command adds key-value pairs to the Logstash keystore. After running the command, the user is prompted to enter the value for each specified key. Keys are subject to certain naming constraints.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/keystore.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-keystore add ES_USER ES_PWD\n```\n\n----------------------------------------\n\nTITLE: Parsing XML Data into Fields with Logstash XML Filter\nDESCRIPTION: This snippet configures the XML filter to parse an entire XML document stored in the `message` field, converting XML elements into structured fields. It is useful for extracting data from XML payloads within Logstash events. Dependencies include the XML filter plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/data-deserialization.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\nfilter {\n  xml {\n    source => \"message\"\n  }\n}\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Dependencies with vendor.json for Logstash Plugins - txt\nDESCRIPTION: This snippet shows a JSON array from the vendor.json file, which is used to define external file dependencies for Logstash plugins. Each object includes a sha1 field for verifying the download, a url to fetch the resource, and an optional files array specifying which paths to extract. To use this, place the JSON array in vendor.json at your plugin's root. The rake vendor command interprets this configuration to download and extract dependencies as described.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_6\n\nLANGUAGE: txt\nCODE:\n```\n[{\n        \"sha1\": \"a90fe6cc53b76b7bdd56dc57950d90787cb9c96e\",\n        \"url\": \"http://collectd.org/files/collectd-5.4.0.tar.gz\",\n        \"files\": [ \"/src/types.db\" ]\n}]\n```\n\n----------------------------------------\n\nTITLE: Checking CPU Usage per Thread (Linux/Unix)\nDESCRIPTION: Use the `top -H` command on Linux/Unix systems to view process statistics broken down by individual threads. This helps identify high CPU usage within Logstash or other processes impacting performance.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/performance-troubleshooting.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ntop -H\n```\n\n----------------------------------------\n\nTITLE: Configuring List Properties in Logstash Plugin Settings - JavaScript\nDESCRIPTION: Example of assigning list values to plugin settings by specifying multiple string elements inside square brackets. The 'path' parameter is a string type that is configured to accept multiple entries, and similarly, 'uris' accepts a list of URI strings. These lists enable type checking when configured appropriately by the plugin author.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\npath => [ \"/var/log/messages\", \"/var/log/*.log\" ]\nuris => [ \"http://elastic.co\", \"http://example.net\" ]\n```\n\n----------------------------------------\n\nTITLE: Test Installing Logstash Plugin Gem - Shell\nDESCRIPTION: Performs download and extraction of the latest Logstash release, then installs the built input plugin gem using the bin/logstash-plugin tool. Requires curl, tar, RubyGems, and Logstash installed. The installation path and version should match the built gem file. Useful for verifying plugin compatibility before public release.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://download.elastic.co/logstash/logstash/logstash-9.0.0.tar.gz\ntar xzvf logstash-9.0.0.tar.gz\ncd logstash-9.0.0\n```\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin install /my/logstash/plugins/logstash-input-example/logstash-input-example-0.1.0.gem\n```\n\n----------------------------------------\n\nTITLE: Creating Metricbeat User API Key in Elasticsearch\nDESCRIPTION: Creates an API key for Metricbeat with permissions to monitor, read ILM policies, read pipelines, and create documents in Metricbeat indices.\nSOURCE: https://github.com/elastic/logstash/blob/main/ci/serverless/README.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\nPOST /_security/api_key\n{\n  \"name\": \"metricbeat_user\", \n  \"role_descriptors\": {\n    \"metricbeat_user_role\": { \n      \"cluster\": [\"monitor\", \"read_ilm\", \"read_pipeline\"],\n      \"index\": [\n        {\n          \"names\": [\"metricbeat-*\"],\n          \"privileges\": [\"view_index_metadata\", \"create_doc\"]\n        }\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Output Plugin Configuration - cloud_id and api_key\nDESCRIPTION: This code snippet shows how to configure the Elasticsearch output plugin in Logstash to send data to Elastic Cloud using the `cloud_id` and `api_key` options. It defines the output block with `cloud_id` and `api_key` parameters. This method authenticates using an API key instead of a username and password.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/connecting-to-cloud.md#_snippet_1\n\nLANGUAGE: Logstash\nCODE:\n```\noutput {elasticsearch { cloud_id => \"<cloud id>\" api_key => \"<api key>\" } }\n```\n\n----------------------------------------\n\nTITLE: Parsed Event JSON Structure with Grok\nDESCRIPTION: Example of a structured JSON output after applying the Grok filter to a web server log line, showing how the log is transformed into discrete, queryable fields.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n        \"request\" => \"/presentations/logstash-monitorama-2013/images/kibana-search.png\",\n          \"agent\" => \"\\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\\\"\",\n         \"offset\" => 325,\n           \"auth\" => \"-\",\n          \"ident\" => \"-\",\n           \"verb\" => \"GET\",\n     \"prospector\" => {\n        \"type\" => \"log\"\n    },\n     \"input\" => {\n        \"type\" => \"log\"\n    },\n         \"source\" => \"/path/to/file/logstash-tutorial.log\",\n        \"message\" => \"83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \\\"GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1\\\" 200 203023 \\\"http://semicomplete.com/presentations/logstash-monitorama-2013/\\\" \\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\\\"\",\n           \"tags\" => [\n        [0] \"beats_input_codec_plain_applied\"\n    ],\n       \"referrer\" => \"\\\"http://semicomplete.com/presentations/logstash-monitorama-2013/\\\"\",\n     \"@timestamp\" => 2017-11-09T02:51:12.416Z,\n       \"response\" => \"200\",\n          \"bytes\" => \"203023\",\n       \"clientip\" => \"83.149.9.216\",\n       \"@version\" => \"1\",\n           \"beat\" => {\n            \"name\" => \"My-MacBook-Pro.local\",\n        \"hostname\" => \"My-MacBook-Pro.local\",\n         \"version\" => \"6.0.0\"\n    },\n           \"host\" => \"My-MacBook-Pro.local\",\n    \"httpversion\" => \"1.1\",\n      \"timestamp\" => \"04/Jan/2015:05:13:42 +0000\"\n}\n```\n\n----------------------------------------\n\nTITLE: Building Logstash Core to Generate Java Plugin API Jar in Shell\nDESCRIPTION: Instructions to compile Logstash core from the root directory using the Gradle wrapper, producing the logstash-core jar file which contains necessary classes for plugin development. This step is required before developing Java plugins to ensure the correct version of the plugin API is available. Windows users use gradlew.bat while others use ./gradlew commands.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew assemble\n```\n\n----------------------------------------\n\nTITLE: Running Logstash on Windows\nDESCRIPTION: Basic command syntax for running Logstash from the Windows command line, using the .bat file with options placeholder for command-line flags.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash-command-line.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash.bat [options]\n```\n\n----------------------------------------\n\nTITLE: Configure HTTP Output in Upstream Logstash\nDESCRIPTION: Configures the HTTP output plugin in the upstream Logstash instance to send data to the downstream Logstash instance.  This includes setting the URL, HTTP method, enabling retries for non-idempotent requests, setting the format to JSON batch for performance, and enabling HTTP compression.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-http.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\noutput {\n    http {\n        url => '<protocol>://<downstream-logstash>:<port>'\n        http_method => post\n        retry_non_idempotent => true\n        format => json_batch\n        http_compression => true\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Beats Input Plugin (JSON syntax)\nDESCRIPTION: Adds Beats input plugin configuration to Logstash pipeline to receive data forwarded by Filebeat on port 5044. This plugin depends on Logstash's beats input and assumes Filebeat is running with the specified configuration. Enables ingestion of log data from Filebeat.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/multiple-input-output-plugins.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\nbeats {\n    port => \"5044\"\n}\n```\n\n----------------------------------------\n\nTITLE: Removing Keys from Keystore\nDESCRIPTION: The `remove` command deletes keys from the Logstash keystore. The keys to be removed are specified as arguments to the command. Requires valid Logstash installation.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/keystore.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-keystore remove ES_USER ES_PWD\n```\n\n----------------------------------------\n\nTITLE: Creating a gemspec File for Logstash Filter Plugins\nDESCRIPTION: A comprehensive gemspec file for a Logstash filter plugin that defines metadata, dependencies, and file organization. It specifies plugin details, license, and both runtime and development dependencies.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_13\n\nLANGUAGE: ruby\nCODE:\n```\nGem::Specification.new do |s|\n  s.name = 'logstash-filter-example'\n  s.version = '0.1.0'\n  s.licenses = ['Apache License (2.0)']\n  s.summary = \"This filter does x, y, z in Logstash\"\n  s.description = \"This gem is a logstash plugin required to be installed on top of the Logstash core pipeline using $LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\n  s.authors = [\"Elastic\"]\n  s.email = 'info@elastic.co'\n  s.homepage = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\n  s.require_paths = [\"lib\"]\n\n  # Files\n  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']\n   # Tests\n  s.test_files = s.files.grep(%r{^(test|spec|features)/})\n\n  # Special flag to let us know this is actually a logstash plugin\n  s.metadata = { \"logstash_plugin\" => \"true\", \"logstash_group\" => \"filter\" }\n\n  # Gem dependencies\n  s.add_runtime_dependency \"logstash-core-plugin-api\", \">= 1.60\", \"<= 2.99\"\n  s.add_development_dependency 'logstash-devutils'\nend\n```\n\n----------------------------------------\n\nTITLE: Specifying Byte Units in Logstash Plugin Settings - JavaScript\nDESCRIPTION: Illustrates how to assign byte size values to plugin settings using string formats with either SI or binary units. This allows specifying sizes in human-readable forms like '10MiB' or '180 mb', which Logstash interprets as exact byte counts, simplifying configuration of memory or file sizes.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\nmy_bytes => \"1113\"   # 1113 bytes\nmy_bytes => \"10MiB\"  # 10485760 bytes\nmy_bytes => \"100kib\" # 102400 bytes\nmy_bytes => \"180 mb\" # 180000000 bytes\n```\n\n----------------------------------------\n\nTITLE: Constructors Initializing Java Output Plugin with Configuration\nDESCRIPTION: This code provides two constructors: a public one used by Logstash at runtime with mandatory parameters and a package-private constructor for testing with customization of the output stream. The constructors initialize the plugin id, retrieve the 'prefix' setting from the configuration, and set up the PrintStream for event output. Exception throwing for unrecoverable setup errors is implied.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nprivate final String id;\nprivate String prefix;\nprivate PrintStream printer;\n\npublic JavaOutputExample(final String id, final Configuration configuration, final Context context) {\n    this(id, configuration, context, System.out);\n}\n\nJavaOutputExample(final String id, final Configuration config, final Context context, OutputStream targetStream) {\n    this.id = id;\n    prefix = config.get(PREFIX_CONFIG);\n    printer = new PrintStream(targetStream);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Core Path in Gradle Properties Txt\nDESCRIPTION: This configuration file sets the LOGSTASH_CORE_PATH variable, pointing to the local Logstash core directory. It is required for the Java plugin project to locate the compiled Logstash core JAR file. The 'target_folder' should point to the root folder of the local Logstash codebase obtained via Git clone and compilation.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_1\n\nLANGUAGE: txt\nCODE:\n```\nLOGSTASH_CORE_PATH=<target_folder>/logstash-core\n```\n\n----------------------------------------\n\nTITLE: Running Plugin Tests (RSpec)\nDESCRIPTION: This snippet shows how to run the RSpec tests for a Logstash plugin. It uses `bundle exec` to ensure the correct gem environment is used.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\nbundle exec rspec\n```\n\n----------------------------------------\n\nTITLE: Fixing Log Output Line in util/zeromq.rb (Ruby)\nDESCRIPTION: This snippet shows the corrected logging logic for the ZeroMQ plugin: if in server mode (server? returns true), it logs 'bound'; otherwise it logs 'connected'. Previously, the words were in the wrong order, causing test failure. Place this line at the specified line (e.g., line 21) in util/zeromq.rb. Inputs are runtime values for server mode and address, outputs are info-level logs.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/contributing-patch-plugin.md#_snippet_5\n\nLANGUAGE: ruby\nCODE:\n```\n@logger.info(\"0mq: #{server? ? 'bound' : 'connected'}\", :address => address)\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with a Local Plugin (Shell)\nDESCRIPTION: Starts Logstash using a simple inline configuration (`-e`) that invokes the custom codec plugin (e.g., 'awesome'). This command tests if the locally developed plugin loads and runs correctly within the Logstash instance.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/codec-plugin/README.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash -e 'codec {awesome {}}'\n```\n\n----------------------------------------\n\nTITLE: Installing Packaged Java Plugin in Logstash\nDESCRIPTION: This command installs the previously packaged Java plugin as a Ruby gem into Logstash, bypassing SSL verification if necessary. It allows Logstash to recognize and load the Java plugin during runtime, linking Java code with Logstash's plugin system.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\nbin/logstash-plugin install --no-verify --local /path/to/javaPlugin.gem\n```\n\n----------------------------------------\n\nTITLE: Rendered Ruby Code Block\nDESCRIPTION: This snippet shows how the previous AsciiDoc code snippet would render in the documentation. It demonstrates a Ruby code block with key-value pairs.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/plugin-doc.md#_snippet_4\n\nLANGUAGE: ruby\nCODE:\n```\nmatch => {\n  \"field1\" => \"value1\"\n  \"field2\" => \"value2\"\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Example Syslog Messages for Logstash Ingestion - Ruby\nDESCRIPTION: Provides sample RFC3164-style syslog lines for manual input during testing. These examples represent real-world syslog events from services like postfix, named, CRON, and rsyslogd. The 'grok' matching patterns in Logstash expect this formatting, and deviations may result in parsing errors or incomplete field extraction.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_11\n\nLANGUAGE: ruby\nCODE:\n```\nDec 23 12:11:43 louis postfix/smtpd[31499]: connect from unknown[95.75.93.154]\nDec 23 14:42:56 louis named[16000]: client 199.48.164.7#64817: query (cache) 'amsterdamboothuren.com/MX/IN' denied\nDec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)\nDec 22 18:28:06 louis rsyslogd: [origin software=\"rsyslogd\" swVersion=\"4.2.0\" x-pid=\"2253\" x-info=\"http://www.rsyslog.com\"] rsyslogd was HUPed, type 'lightweight'.\n```\n\n----------------------------------------\n\nTITLE: Starting Upstream Logstash with Lumberjack Output - Shell/Logstash\nDESCRIPTION: Launches the upstream Logstash instance with a generator input and Lumberjack output plugin configured to send five JSON events to a downstream host over SSL. 'mydownstreamhost' specifies the target, 'lumberjack.cert' is the SSL certificate, and 5000 is the port. Requires Logstash, the lumberjack output plugin, and a valid SSL certificate. Input and output are declared in a one-line Logstash pipeline.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-lumberjack.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -e 'input { generator { count => 5 } } output { lumberjack { codec => json hosts => \"mydownstreamhost\" ssl_certificate => \"lumberjack.cert\" port => 5000 } }'\n```\n\n----------------------------------------\n\nTITLE: Static JDBC Lookup - Logstash\nDESCRIPTION: This snippet configures the `jdbc_static` filter to enrich events with data fetched from a remote database.  It loads data from specified tables (e.g., `servers`, `users`) using SQL queries, stores the data in a local cache, and then performs lookups to enrich events based on fields like `ip` and `userid`. It then adds, renames and removes fields. This configuration requires a JDBC-compatible database, database credentials, and the necessary JDBC driver.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/lookup-enrichment.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\nfilter {\n  jdbc_static {\n    loaders => [ <1>\n      {\n        id => \"remote-servers\"\n        query => \"select ip, descr from ref.local_ips order by ip\"\n        local_table => \"servers\"\n      },\n      {\n        id => \"remote-users\"\n        query => \"select firstname, lastname, userid from ref.local_users order by userid\"\n        local_table => \"users\"\n      }\n    ]\n    local_db_objects => [ <2>\n      {\n        name => \"servers\"\n        index_columns => [\"ip\"]\n        columns => [\n          [\"ip\", \"varchar(15)\"],\n          [\"descr\", \"varchar(255)\"]\n        ]\n      },\n      {\n        name => \"users\"\n        index_columns => [\"userid\"]\n        columns => [\n          [\"firstname\", \"varchar(255)\"],\n          [\"lastname\", \"varchar(255)\"],\n          [\"userid\", \"int\"]\n        ]\n      }\n    ]\n    local_lookups => [ <3>\n      {\n        id => \"local-servers\"\n        query => \"select descr as description from servers WHERE ip = :ip\"\n        parameters => {ip => \"[from_ip]\"}\n        target => \"server\"\n      },\n      {\n        id => \"local-users\"\n        query => \"select firstname, lastname from users WHERE userid = :id\"\n        parameters => {id => \"[loggedin_userid]\"}\n        target => \"user\" <4>\n      }\n    ]\n    # using add_field here to add & rename values to the event root\n    add_field => { server_name => \"%{[server][0][description]}\" }\n    add_field => { user_firstname => \"%{[user][0][firstname]}\" } <5>\n    add_field => { user_lastname => \"%{[user][0][lastname]}\" }\n    remove_field => [\"server\", \"user\"]\n    jdbc_user => \"logstash\"\n    jdbc_password => \"example\"\n    jdbc_driver_class => \"org.postgresql.Driver\"\n    jdbc_driver_library => \"/tmp/logstash/vendor/postgresql-42.1.4.jar\"\n    jdbc_connection_string => \"jdbc:postgresql://remotedb:5432/ls_test_2\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting TCP Port Environment Variable (Shell)\nDESCRIPTION: This shell command sets the `TCP_PORT` environment variable to the value `12345`. This is a prerequisite for the Ruby code that utilizes it.  The environment variable is then accessible to the Logstash process during its startup and configuration parsing.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/environment-variables.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport TCP_PORT=12345\n```\n\n----------------------------------------\n\nTITLE: Implementing the Register Method\nDESCRIPTION: Demonstrates the `register` method, which is similar to an `initialize` method. It allows setting up instance variables and is called during the plugin initialization phase. It's explicitly marked as `public`.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_4\n\nLANGUAGE: ruby\nCODE:\n```\n  public\n  def register\n  end # def register\n```\n\n----------------------------------------\n\nTITLE: Using Mutate Filter to Check Field Existence\nDESCRIPTION: This snippet demonstrates how to use the `mutate` filter in Logstash to check if a boolean field exists within an event.  It utilizes a metadata field to temporarily store the value of the target field and then uses conditional logic to determine if the field existed based on whether its value was copied successfully. No external dependencies are needed.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/tips-best-practices.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nfilter {\n  mutate {\n    # we use a \"temporal\" field with a predefined arbitrary known value that\n    # lives only in filtering stage.\n    add_field => { \"[@metadata][test_field_check]\" => \"a null value\" }\n\n    # we copy the field of interest into that temporal field.\n    # If the field doesn't exist, copy is not executed.\n    copy => { \"test_field\" => \"[@metadata][test_field_check]\" }\n  }\n\n\n  # now we now if testField didn't exists, our field will have\n  # the initial arbitrary value\n  if [@metadata][test_field_check] == \"a null value\" {\n    # logic to execute when [test_field] did not exist\n    mutate { add_field => { \"field_did_not_exist\" => true }}\n  } else {\n    # logic to execute when [test_field] existed\n    mutate { add_field => { \"field_did_exist\" => true }}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring PKI Authentication in Logstash Elasticsearch Output (Logstash Config)\nDESCRIPTION: Sets up PKI authentication for the Logstash Elasticsearch output plugin using an X.509 client certificate. Requires configuring the `ssl_keystore_path` and `ssl_keystore_password`. If a separate truststore is used, `ssl_truststore_path` and `ssl_truststore_password` are also needed.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_7\n\nLANGUAGE: logstash-config\nCODE:\n```\noutput {\n  elasticsearch {\n    ...\n    ssl_keystore_path => /path/to/keystore.jks\n    ssl_keystore_password => realpassword\n    ssl_truststore_path =>  /path/to/truststore.jks \n    ssl_truststore_password =>  realpassword\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Securing Logstash Monitoring API with SSL Settings - YAML\nDESCRIPTION: This YAML configuration enables SSL/TLS for the Logstash monitoring API by setting api.ssl.enabled: true and providing the file path and password for the keystore containing the necessary certificate and private key. Dependencies: a valid keystore file in JKS or PKCS12 format. Input: applied to logstash.yml; Output: Logstash API served over HTTPS. Limitation: passwords in plain YAML are discouraged; prefer environment variables or keystore references.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-logstash.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napi.ssl.enabled: true\napi.ssl.keystore.path: /path/to/keystore.jks\napi.ssl.keystore.password: \"s3cUr3p4$$w0rd\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Elastic YUM Repository for Logstash (shell)\nDESCRIPTION: This snippet provides the YUM repository definition required for Logstash installation on RPM-based systems. Replace {{major-version}} with the desired Logstash major version. The definition includes repository metadata, base URL, GPG settings, and repository type. Place this configuration in a file with a .repo extension (e.g., logstash.repo) in /etc/yum.repos.d/. No dynamic input at runtime. The output is a system-ready YUM repository configuration.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/installing-logstash.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n[logstash-{{major-version}}]\nname=Elastic repository for {{major-version}} packages\nbaseurl=https://artifacts.elastic.co/packages/{{major-version}}/yum\ngpgcheck=1\ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch\nenabled=1\nautorefresh=1\ntype=rpm-md\n```\n\n----------------------------------------\n\nTITLE: Using Comments in Logstash Configuration Files - JavaScript\nDESCRIPTION: Shows the comment syntax in Logstash configuration files, which follows Perl, Ruby, and Python style comments starting with '#'. Comments can appear on their own line or at the end of configuration lines, aiding readability and documentation inside configurations.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_13\n\nLANGUAGE: JavaScript\nCODE:\n```\n# this is a comment\n\ninput { # comments can appear at the end of a line, too\n  # ...\n}\n```\n\n----------------------------------------\n\nTITLE: Assigning URI Values in Logstash Plugin Settings - JavaScript\nDESCRIPTION: Shows an example of setting a URI string in plugin configurations. The example includes a URI containing a username and password. Logstash redacts the password portion in logs, enhancing security while allowing full URI specification.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_10\n\nLANGUAGE: JavaScript\nCODE:\n```\nmy_uri => \"http://foo:bar@example.net\"\n```\n\n----------------------------------------\n\nTITLE: Installing Logstash Development Dependencies with Bundler (Shell)\nDESCRIPTION: Installs Logstash's development dependencies using `bundle install` after configuring the local path for gems with `bundle config`. This is presented as an alternative to using Gradle for dependency management.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\nbundle config set --local path vendor/bundle\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Monitoring GeoIP Database Status via Node Stats API in JavaScript\nDESCRIPTION: This code snippet demonstrates how to fetch current GeoIP database management metrics from a Logstash instance using a GET request to the Node Stats API. It retrieves JSON data on database status, last update times, and success/failure counts to assess database health. The request targets the default API endpoint hosted locally on port 9600.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/logstash-geoip-database-management.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\ncurl -XGET 'localhost:9600/_node/stats/geoip_download_manager?pretty'\n```\n\n----------------------------------------\n\nTITLE: Setting Logstash Source Code Location - Shell\nDESCRIPTION: These shell commands set two environment variables: LOGSTASH_SOURCE, indicating a build from source, and LOGSTASH_PATH, specifying the path to the local Logstash source directory. Adjust '/YOUR/LOGSTASH/DIRECTORY' as needed. These must be exported before running build-related tasks.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexport LOGSTASH_SOURCE=1\nexport LOGSTASH_PATH=/YOUR/LOGSTASH/DIRECTORY\n```\n\n----------------------------------------\n\nTITLE: Sample Logstash Output to Elasticsearch\nDESCRIPTION: This logstash output configuration directs processed data to an Elasticsearch cluster. It specifies the Elasticsearch host addresses and index name, enabling data to be indexed for searchability. It depends on the elasticsearch output plugin in Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/sequel-NOTICE.txt#_snippet_2\n\nLANGUAGE: Configuration\nCODE:\n```\noutput {\n  elasticsearch {\n    hosts => [\"localhost:9200\"]\n    index => \"logs-%{+YYYY.MM.dd}\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Decoding Protobuf Messages from Kafka Stream in Logstash\nDESCRIPTION: This snippet sets up Logstash to decode protobuf encoded messages from Kafka, requiring the protobuf class definition files compiled for Ruby. It specifies Kafka connection parameters and protobuf class details, enabling processing of protobuf data streams. Dependencies include the protobuf codec plugin and precompiled Ruby protobuf classes.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/data-deserialization.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\ninput\n  kafka {\n    zk_connect => \"127.0.0.1\"\n    topic_id => \"your_topic_goes_here\"\n    codec => protobuf {\n      class_name => \"Animal::Unicorn\"\n      include_path => ['/path/to/protobuf/definitions/UnicornProtobuf.pb.rb']\n    }\n  }\n}\n...\n```\n\n----------------------------------------\n\nTITLE: Building Logstash with a Custom JRuby Distribution - Shell\nDESCRIPTION: Runs Gradle build and test commands while specifying a custom JRuby installation via the custom.jruby.path property. Make sure the path points to the root source of a valid JRuby distribution. Output: builds and tests Logstash using the custom JRuby environment.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew clean test -Pcustom.jruby.path=\"/path/to/jruby\"\n```\n\n----------------------------------------\n\nTITLE: Configure Basic Authentication in Logstash (HTTP)\nDESCRIPTION: Configures basic username/password authentication for the HTTP input/output plugins in both Logstash instances.  Requires setting the `user` and `password` options.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-http.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n...\n  http {\n    ...\n\n    user => \"your-user\"\n    password => \"your-secret\"\n  }\n...\n```\n\n----------------------------------------\n\nTITLE: Building OSS-only Artifacts via Gradle - Shell\nDESCRIPTION: Runs specialized Gradle tasks to create OSS-licensed tarball and zip distributions of Logstash. Input: none. Output: OSS-only distributable archives.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew assembleOssTarDistribution\n```\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew assembleOssZipDistribution\n```\n\n----------------------------------------\n\nTITLE: Configuring Twitter Input Plugin in Logstash (JSON syntax)\nDESCRIPTION: Defines a Logstash input plugin configuration to ingest data from Twitter using OAuth credentials and keywords. Dependencies include the 'twitter' input plugin and valid Twitter API credentials. Key parameters include consumer keys/secrets, OAuth tokens, and search keywords. The output is streamed data from Twitter matching the criteria.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/multiple-input-output-plugins.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ntwitter {\n    consumer_key => \"enter_your_consumer_key_here\"\n    consumer_secret => \"enter_your_secret_here\"\n    keywords => [\"cloud\"]\n    oauth_token => \"enter_your_access_token_here\"\n    oauth_token_secret => \"enter_your_access_token_secret_here\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing the `encode` Method in a Logstash Codec (Ruby)\nDESCRIPTION: Defines the `encode` method for a Logstash codec plugin in Ruby. This method takes a Logstash event, retrieves its 'message' field, appends a configured string (`@append`) and a newline (`NL`), and then calls the `@on_event` callback with the original event and the newly formatted string. It serializes event data into a specific output format.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_6\n\nLANGUAGE: Ruby\nCODE:\n```\n  public\n  def encode(event)\n    @on_event.call(event, event.get(\"message\").to_s + @append + NL)\n  end # def encode\n```\n\n----------------------------------------\n\nTITLE: Building RPM and Debian Packages Using Rake and fpm - Shell\nDESCRIPTION: Executes Rake tasks to build .rpm and .deb packages for Logstash. Requires the fpm packaging tool. Separate commands are provided for OSS-only RPM and DEB artifacts. Output: native packages suitable for Linux package managers.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\nrake artifact:rpm\n```\n\nLANGUAGE: shell\nCODE:\n```\nrake artifact:deb\n```\n\n----------------------------------------\n\nTITLE: Error Message for Corrupt Persistent Queue in Logstash\nDESCRIPTION: Example error message that indicates a persistent queue page file is too small to hold elements, suggesting queue corruption.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/persistent-queues.md#_snippet_2\n\nLANGUAGE: txt\nCODE:\n```\nmessage=>\"java.io.IOException: Page file size is too small to hold elements\"\n```\n\n----------------------------------------\n\nTITLE: Installing a Logstash Plugin from RubyGems - Shell\nDESCRIPTION: This shell command installs a specified plugin (in this case, logstash-input-github) into the existing Logstash deployment using the logstash-plugin manager. Internet access and access to RubyGems.org are required, and the specified gem must exist and be compatible with your Logstash version. After installation, the plugin becomes available for use in Logstash configuration files.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/working-with-plugins.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin install logstash-input-github\n```\n\n----------------------------------------\n\nTITLE: Retrieving Runtime Logging Configuration via REST API\nDESCRIPTION: Shows an example of using a GET request to retrieve current logging levels for all subsystems and plugins in Logstash, useful for monitoring or debugging log levels dynamically.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/logging.md#_snippet_4\n\nLANGUAGE: js\nCODE:\n```\ncurl -XGET 'localhost:9600/_node/logging?pretty'\n```\n\n----------------------------------------\n\nTITLE: Enabling HTTP Basic Authentication for Logstash API - YAML\nDESCRIPTION: This configures the Logstash monitoring API to require HTTP Basic authentication by specifying the authentication type and supplying expected credentials in logstash.yml. Input: YAML configuration applied to Logstash; Output: API requests must now provide the specified username and password. Limitation: storing plain text credentials is insecure; environment or keystore variable resolution is recommended.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-logstash.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napi.auth.type: basic\napi.auth.basic.username: \"logstash\"\napi.auth.basic.password: \"s3cUreP4$$w0rD\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependencies in Gemfile for Logstash Plugins - ruby\nDESCRIPTION: This Ruby Gemfile snippet declares the plugin's dependencies and sources, allowing Bundler to manage and install required gems. It specifies Rubygems.org as the source, references the gemspec for plugin metadata, and sets a dependency on the logstash gem from the specified GitHub repository and branch. Developers should edit this file to declare any additional runtime or development dependencies needed for their plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_7\n\nLANGUAGE: ruby\nCODE:\n```\nsource 'https://rubygems.org'\ngemspec\ngem \"logstash\", :github => \"elastic/logstash\", :branch => \"master\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Filebeat Inputs and Output to Logstash (YAML)\nDESCRIPTION: This YAML snippet configures Filebeat to collect log lines from a specified Apache log file and forward them to Logstash on localhost using port 5044. It defines a log input type and requires the absolute path to the log file to be set under 'paths'. The 'output.logstash' section specifies where Filebeat sends the events; no TLS/SSL options are set for simplicity, which is suitable only for local development or tutorials.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nfilebeat.inputs:\n- type: log\n  paths:\n    - /path/to/file/logstash-tutorial.log <1>\noutput.logstash:\n  hosts: [\"localhost:5044\"]\n```\n\n----------------------------------------\n\nTITLE: Generating documentation for all plugins\nDESCRIPTION: This command uses `logstash-docgen` to generate documentation for all plugins in the `logstash-plugins` organization. It needs to be run from the `tools/logstash-docgen` directory after installing the dependencies.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/logstash-docgen/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncd tools/logstash-docgen\nbundle install\n```\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-docgen --all # will generate the doc for all the plugins\nbin/logstash-docgen logstash-input-file logstash-input-s3 # generate doc for 2 plugins\n```\n\n----------------------------------------\n\nTITLE: Running Filebeat in the Foreground with a Custom Configuration (Shell)\nDESCRIPTION: This Shell command runs Filebeat with a specified configuration file ('filebeat.yml') in the foreground and enables debug output for the 'publish' subsystem. The command must be executed from the Filebeat installation directory and may require root privileges. The '-e' flag directs logs to the standard error, while '-d' specifies which debug selectors to enable.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo ./filebeat -e -c filebeat.yml -d \"publish\"\n```\n\n----------------------------------------\n\nTITLE: Setting HTTP Proxy for GeoIP Database Downloads\nDESCRIPTION: Exports an environment variable 'http_proxy' pointing to a proxy server URL to enable Logstash to connect to Elastic GeoIP endpoints through an HTTP proxy. This is useful in restricted network environments.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuring-geoip-database-management.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nexport http_proxy=\"http://PROXY_IP:PROXY_PORT\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Vendor Dependencies (Rake)\nDESCRIPTION: This snippet demonstrates how to download external file dependencies defined in the `vendor.json` file using a Rake task. This is necessary if the plugin relies on external files.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nrake vendor\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with a local plugin via command line\nDESCRIPTION: Executes Logstash with a custom plugin by editing the Gemfile to point to the local plugin directory and running Logstash with an inline configuration. Any modifications to the plugin code will be immediately applied during execution.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/input-plugin/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -e 'filter {awesome {}}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Settings with Environment Variables - Logstash YAML\nDESCRIPTION: Illustrates how to use environment variable interpolation to dynamically set Logstash configuration values. It shows syntax for simple variable substitution and for providing a default value if the environment variable is not set. This allows configuration flexibility without modifying the `logstash.yml` file directly.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/logstash-settings-file.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\npipeline:\n  batch:\n    size: ${BATCH_SIZE}\n    delay: ${BATCH_DELAY:50}\nnode:\n  name: \"node_${LS_NODE_NAME}\"\npath:\n   queue: \"/tmp/${QUEUE_DIR:queue}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring UDP Input and CEF Codec ECS Mode in Logstash Pipeline (text)\nDESCRIPTION: This example demonstrates how to set ECS compatibility to version 8 ('v8') for both a UDP input plugin and its associated CEF codec within a Logstash pipeline. The 'ecs_compatibility' attribute is specified at both the input and codec levels, ensuring event processing follows ECS v8 schema. Place this snippet in a Logstash pipeline configuration; dependencies include Logstash and the required plugins, and it must match your environment's version compatibility.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ecs-ls.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ninput {\n  udp {\n    port => 1234\n    ecs_compatibility => v8\n    codec => cef {\n      ecs_compatibility => v8\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Truststore Authentication for Monitoring in YAML\nDESCRIPTION: This YAML snippet sets up a Java truststore for SSL validation when connecting Logstash to monitored Elasticsearch nodes. The 'xpack.monitoring.elasticsearch.ssl.truststore.path' parameter specifies the truststore (JKS/PKCS12) file, and 'xpack.monitoring.elasticsearch.ssl.truststore.password' provides its password. Required if you need custom CA or chained certificates not in the default truststore.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-internal-collection-legacy.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nxpack.monitoring.elasticsearch.ssl.truststore.path: /path/to/file\nxpack.monitoring.elasticsearch.ssl.truststore.password: password\n```\n\n----------------------------------------\n\nTITLE: Installing Local Plugin in Logstash Clone (Shell)\nDESCRIPTION: Command to install plugins referenced in the Logstash Gemfile, including local ones added via the `:path` option. The `--no-verify` flag skips gem verification checks, useful during rapid development cycles.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/output-plugin/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nbin/logstash-plugin install --no-verify\n```\n\n----------------------------------------\n\nTITLE: Removing a Logstash Plugin - Shell\nDESCRIPTION: This shell command removes the specified plugin (logstash-input-github) from the Logstash installation using the logstash-plugin manager. Removal may affect configurations that rely on the plugin, and it generally requires appropriate file system permissions and that Logstash is not actively using the plugin when the command is run.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/working-with-plugins.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin remove logstash-input-github\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Configuration Reloading in Logstash\nDESCRIPTION: Starts Logstash with the --config.reload.automatic (`-r`) option to enable automatic detection and reloading of configuration changes at intervals, defaulting to every 3 seconds. This setup depends on command-line configuration, but disables when using the `-e` flag. Key parameter `--config.reload.interval` allows specifying the check interval in seconds. This facilitates dynamic updates without restarting Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/reloading-config.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -f apache.config --config.reload.automatic\n```\n\n----------------------------------------\n\nTITLE: Resetting Runtime Log Levels to Default\nDESCRIPTION: Provides a command to reset all dynamically modified log levels back to the settings defined in the log4j2.properties file, restoring default logging behavior after temporary adjustments.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/logging.md#_snippet_6\n\nLANGUAGE: js\nCODE:\n```\ncurl -XPUT 'localhost:9600/_node/logging/reset?pretty'\n```\n\n----------------------------------------\n\nTITLE: Applying KV Filter in Logstash Configuration\nDESCRIPTION: Shows a basic configuration for the Logstash kv filter. When applied, this filter automatically parses key-value pairs present in an event's fields (by default, the `message` field) into new, distinct fields. For example, it would turn the input 'ip=1.2.3.4 error=REFUSED' into `ip: 1.2.3.4` and `error: REFUSED` fields.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/field-extraction.md#_snippet_1\n\nLANGUAGE: logstash\nCODE:\n```\nfilter {\n  kv { }\n}\n```\n\n----------------------------------------\n\nTITLE: Packaging a Logstash Java Plugin using Gradle\nDESCRIPTION: Shell command to execute the `gem` task using the Gradle wrapper (`./gradlew` for Unix-like systems, `gradlew.bat` for Windows). This task compiles the Java code and packages the Logstash plugin into a Ruby gem file suitable for installation.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew gem\n```\n\n----------------------------------------\n\nTITLE: Pulling Logstash Docker Image (Shell)\nDESCRIPTION: Instructs users on how to download the Logstash Docker image from the Elastic registry. It uses the standard `docker pull` command with placeholders for the specific repository and version. Requires Docker to be installed and configured to access the Elastic registry.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/docker.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ndocker pull {{docker-repo}}:{{stack-version}}\n```\n\n----------------------------------------\n\nTITLE: Running Java-only Tests for Logstash - Shell\nDESCRIPTION: Runs Logstash's subset of tests written in Java using Gradle. Requires Gradle. Input: none. Output: results of Java-only tests.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew javaTests\n```\n\n----------------------------------------\n\nTITLE: Building the Logstash Plugin Gem using Shell\nDESCRIPTION: This snippet demonstrates the command to build a gem file for the Logstash plugin from its gemspec file, essential for applying source code changes and deploying the plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/support/logstash-integration-failure_injector/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ngem build logstash-integration-failure_injector.gemspec\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Input Plugins in Logstash - JavaScript\nDESCRIPTION: Demonstrates how to configure multiple input plugins of the same type within a single input block. Each plugin instance includes settings such as 'port' and 'tags' to differentiate their behavior. This example configures two HTTP input plugins listening on different ports with distinct tags for event classification.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\ninput {\n  http {\n    port => 3333\n    tags => gateway\n  }\n  http {\n    port => 4444\n    tags => billing\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Logger with log4j2 YAML Example\nDESCRIPTION: Defines a logger for the Elasticsearch output module in Logstash by setting its name and level in the log4j2 configuration, which influences the verbosity of logs generated by Elasticsearch-related components.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/logging.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlogger.elasticsearchoutput.name = logstash.outputs.elasticsearch\nlogger.elasticsearchoutput.level = debug\n```\n\n----------------------------------------\n\nTITLE: Example of Invalid GeoIP Data JSON\nDESCRIPTION: An example of a JSON document with invalid GeoIP data that causes a mapping error in Elasticsearch because it contains a string value in a field that expects a geo_point object.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/dead-letter-queues.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"geoip\":{\"location\":\"home\"}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Slowlog Thresholds in Logstash YAML\nDESCRIPTION: A YAML configuration example demonstrating how to set slowlog thresholds for different log levels (warn, info, debug, trace). Each threshold specifies the minimum event processing time that will trigger logging at that level.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/logging.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nslowlog.threshold.warn: 2s\nslowlog.threshold.info: 1s\nslowlog.threshold.debug: 500ms\nslowlog.threshold.trace: 100ms\n```\n\n----------------------------------------\n\nTITLE: Stopping Logstash as a Service using systemd\nDESCRIPTION: Provides a shell command to stop Logstash when running as a systemd service. This command ensures a graceful shutdown via the system’s init system, suitable for production environments managed by systemd.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/shutdown.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsystemctl stop logstash\n```\n\n----------------------------------------\n\nTITLE: Testing Logstash Configuration After Upgrade\nDESCRIPTION: Command to test a Logstash configuration file before restarting the pipeline after an upgrade. This validates that the configuration is compatible with the new Logstash version.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/upgrading-using-direct-download.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlogstash --config.test_and_exit -f <configuration-file>\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP and HTTPS Proxy Environment Variables - Shell\nDESCRIPTION: These environment variable exports configure the HTTP and HTTPS proxies for the current terminal session, allowing Logstash plugin management commands to access RubyGems.org through a local proxy (e.g., on port 3128). This is required if running Logstash commands behind a corporate firewall. Both variables should be set before running any plugin manager commands, and the proxy endpoint should be accessible; otherwise, network requests will fail.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/working-with-plugins.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport http_proxy=http://localhost:3128\nexport https_proxy=http://localhost:3128\n```\n\n----------------------------------------\n\nTITLE: Implementing a Delay Filter in Logstash with Python\nDESCRIPTION: This script introduces a delay filter for Logstash using Python, allowing control over data processing timing. It simulates latency by pausing processing for a specified number of seconds, enabling testing and tuning of performance.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/com.fasterxml.jackson.dataformat!jackson-dataformat-yaml-NOTICE.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n\n# Pause execution for a specified delay duration\n\ndef delay_filter(event, delay_seconds):\n    time.sleep(delay_seconds)\n    return event\n```\n\n----------------------------------------\n\nTITLE: Running pqcheck Utility in Logstash\nDESCRIPTION: Command for running the pqcheck utility to identify corrupted persistent queues by examining checkpoint files and their referenced page files.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/persistent-queues.md#_snippet_3\n\nLANGUAGE: txt\nCODE:\n```\nbin/pqcheck <queue_directory>\n```\n\n----------------------------------------\n\nTITLE: Installing a Local Logstash Java Plugin Gem\nDESCRIPTION: Shell command using `logstash-plugin install` to install a locally built Java plugin gem file into a Logstash instance. The `--no-verify` flag skips signature verification, and `--local` specifies that the gem file is on the local filesystem.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin install --no-verify --local /path/to/javaPlugin.gem\n```\n\n----------------------------------------\n\nTITLE: Defining Target Cluster UUID for Logstash Monitoring in YAML Configuration\nDESCRIPTION: Optional YAML configuration to bind Logstash metrics to a specific Elasticsearch cluster by defining the monitoring.cluster_uuid in logstash.yml.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/dashboard-monitoring-with-elastic-agent.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmonitoring.cluster_uuid: PRODUCTION_ES_CLUSTER_UUID\n```\n\n----------------------------------------\n\nTITLE: Adding Elastic APT Repository for Logstash (shell)\nDESCRIPTION: This shell command adds the Elastic APT repository for Logstash, referencing the GPG keyring installed previously. Replace {{major-version}} with the desired Logstash major version. The command echoes the repository definition and appends it to an APT sources list file, granting APT access to Elastic's official packages. Requires shell access and sudo privileges. The output is a new repository configuration file in /etc/apt/sources.list.d.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/installing-logstash.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\necho \"deb [signed-by=/usr/share/keyrings/elastic-keyring.gpg] https://artifacts.elastic.co/packages/{{major-version}}/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-{{major-version}}.list\n```\n\n----------------------------------------\n\nTITLE: Running All Logstash Integration Tests Locally (Bash/Shell)\nDESCRIPTION: Executes the main integration test script to run the complete suite of Logstash integration tests. This command should be run from the Logstash root directory on a Mac or Linux system with the required dependencies (JRuby, rspec, rake, bundler, Java) installed.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/integration/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nci/integration_tests.sh\n```\n\n----------------------------------------\n\nTITLE: Setting LS_JAVA_HOME and PATH using SETX (PowerShell)\nDESCRIPTION: These PowerShell commands use SETX to set the LS_JAVA_HOME environment variable and update the PATH variable to include the Java bin directory. The /m flag sets the variables at the system level, requiring administrator privileges. This allows Logstash to find and use the correct Java installation.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash-windows.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nPS C:\\Windows\\system32> SETX /m LS_JAVA_HOME \"C:\\Program Files\\Java\\jdk-11.0.3\"\nPS C:\\Windows\\system32> SETX /m PATH \"$env:PATH;C:\\Program Files\\Java\\jdk-11.0.3\\bin;\"\n```\n\n----------------------------------------\n\nTITLE: Sample Output from pqcheck with Corrupted Page File in Logstash\nDESCRIPTION: Example output from the pqcheck utility showing a corrupted page file indicated by a 'NOT FOUND' status, which requires running pqrepair.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/persistent-queues.md#_snippet_5\n\nLANGUAGE: txt\nCODE:\n```\nbin/pqcheck /var/lib/logstash/queue/main/\nUsing bundled JDK: /usr/share/logstash/jdk\nOpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.\nChecking queue dir: /var/lib/logstash/queue/main\ncheckpoint.head, fully-acked: NO, page.2 size: NOT FOUND <1>\n  pageNum=2, firstUnackedPageNum=2, firstUnackedSeqNum=534041, minSeqNum=457916,\n  elementCount=76127, isFullyAcked=no\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Input for Logstash\nDESCRIPTION: This snippet configures the Logstash input plugin to receive connections from other Logstash instances. The `port` option allows specifying a custom port for receiving data, with 9800 being the default if none is provided. This configuration is crucial for the downstream Logstash instance.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-native.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\ninput {\n    logstash {\n        port => 9800\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Logstash Reader Role\nDESCRIPTION: This Shell code snippet shows how to create a `logstash_reader` role in Elasticsearch using the `role` API. The `logstash_reader` role grants the necessary cluster and index privileges for reading data from the Logstash indices. The 'read' and 'view_index_metadata' privileges are provided on the indices that Logstash creates, using the `logstash-*` pattern.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nPOST _security/role/logstash_reader\n{\n  \"cluster\": [\"manage_logstash_pipelines\"],\n  \"indices\": [\n    {\n      \"names\": [ \"logstash-*\" ],\n      \"privileges\": [\"read\",\"view_index_metadata\"]\n    }\n  ]\n}\n\n```\n\n----------------------------------------\n\nTITLE: Defining Log Transformation Logic in Logstash Filter\nDESCRIPTION: This filter snippet processes incoming log events to extract fields, perform parsing, and enrich data before forwarding. It may include grok filters, date parsing, and mutate filters to prepare logs for indexing. Dependencies involve the grok plugin and the mutate plugin within Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/concurrent-ruby-NOTICE.txt#_snippet_1\n\nLANGUAGE: Logstash Configuration (DSL)\nCODE:\n```\nfilter {\n  grok {\n    match => { \"message\" => \"%{COMBINEDAPACHELOG}\" }\n  }\n  date {\n    match => [\"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\"]\n  }\n  mutate {\n    add_field => { \"status_code\" => \"%{status}\" }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Logstash Monitoring in YAML\nDESCRIPTION: This YAML snippet disables the default internal monitoring of Logstash by setting the 'monitoring.enabled' property to false in 'logstash.yml'. This is a prerequisite step before configuring Metricbeat to collect monitoring data externally, ensuring no duplicate data collection. No additional dependencies are needed beyond standard Logstash configuration access. The expected input is the Logstash configuration file, with output being the change disabling default monitoring.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-with-metricbeat.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmonitoring.enabled: false\n```\n\n----------------------------------------\n\nTITLE: Monitoring Disk I/O (Linux)\nDESCRIPTION: Use Linux utilities like `iostat` or `dstat` to monitor disk input/output utilization. This helps diagnose disk saturation issues potentially caused by Logstash plugins (like file output) or excessive error logging.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/performance-troubleshooting.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\niostat\n```\n\nLANGUAGE: shell\nCODE:\n```\ndstat\n```\n\n----------------------------------------\n\nTITLE: Using an API Key in Logstash Elasticsearch Output (Ruby)\nDESCRIPTION: Configures the Logstash Elasticsearch output plugin to authenticate using an API key. Set the `api_key` parameter within the `elasticsearch` output block to the `id:api_key` string returned by the Create API key API.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_13\n\nLANGUAGE: ruby\nCODE:\n```\noutput {\n  elasticsearch {\n    api_key => \"TiNAGG4BaaMdaH1tRfuU:KnR6yE41RrSowb0kQ0HWoA\" \n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Plugin Version in Gemspec - Ruby\nDESCRIPTION: Specifies the semantic version of the Logstash plugin in the gemspec file. Required for RubyGems packaging and release processes. Ensures version consistency between local development and public distribution. Must be updated prior to publishing new releases.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_11\n\nLANGUAGE: ruby\nCODE:\n```\ns.version = '0.1.0'\n```\n\n----------------------------------------\n\nTITLE: Defining a Plugin Specification with Gemspec for Logstash Plugins - ruby\nDESCRIPTION: This Ruby code defines a gemspec for a Logstash output plugin, detailing metadata, authorship, licensing, source files, plugin classification, and gem dependencies. Key parameters include s.name (the plugin's name), s.version, s.summary, s.description, s.files for included content, and runtime/development dependencies via s.add_runtime_dependency and s.add_development_dependency. To use, edit to match your plugin and ensure all dependencies and file paths are correct before publishing your gem.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_8\n\nLANGUAGE: ruby\nCODE:\n```\nGem::Specification.new do |s|\n  s.name = 'logstash-output-example'\n  s.version = '0.1.0'\n  s.licenses = ['Apache License (2.0)']\n  s.summary = \"This output does x, y, z in Logstash\"\n  s.description = \"This gem is a logstash plugin required to be installed on top of the Logstash core pipeline using $LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\n  s.authors = [\"Elastic\"]\n  s.email = 'info@elastic.co'\n  s.homepage = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\n  s.require_paths = [\"lib\"]\n\n  # Files\n  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']\n   # Tests\n  s.test_files = s.files.grep(%r{^(test|spec|features)/})\n\n  # Special flag to let us know this is actually a logstash plugin\n  s.metadata = { \"logstash_plugin\" => \"true\", \"logstash_group\" => \"output\" }\n\n  # Gem dependencies\n  s.add_runtime_dependency \"logstash-core-plugin-api\", \">= 1.60\", \"<= 2.99\"\n  s.add_development_dependency 'logstash-devutils'\nend\n```\n\n----------------------------------------\n\nTITLE: Stopping Logstash from Console by Sending SIGTERM\nDESCRIPTION: Details how to stop Logstash running directly in the console by sending the SIGTERM signal to its process ID. This is useful for manual or development scenarios, ensuring a controlled shutdown.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/shutdown.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkill -TERM {logstash_pid}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash-xpack Module in Metricbeat YAML\nDESCRIPTION: This YAML configuration snippet details the critical settings for the 'logstash-xpack' module in Metricbeat, defining which metricsets to collect ('node' and 'node_stats'), the polling interval (10 seconds), and the hosts to connect to for monitoring data (defaulting to localhost:9600). It optionally supports username and password for authentication and enables xpack features. This configuration file directs Metricbeat to gather comprehensive Logstash monitoring metrics, which are essential to produce full visualizations in Stack Monitoring.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-with-metricbeat.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n  - module: logstash\n    metricsets:\n      - node\n      - node_stats\n    period: 10s\n    hosts: [\"localhost:9600\"]\n    #username: \"user\"\n    #password: \"secret\"\n    xpack.enabled: true\n```\n\n----------------------------------------\n\nTITLE: JVM Option Version Range Example\nDESCRIPTION: Shows how to define a JVM option that applies to a specific range of JVM versions.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/jvm-settings.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n8-9:-Xmx2g\n```\n\n----------------------------------------\n\nTITLE: Example Java Stack Trace\nDESCRIPTION: Provides a sample Java stack trace where subsequent lines start with whitespace, representing a typical multiline event scenario that needs consolidation.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/multiline.md#_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nException in thread \"main\" java.lang.NullPointerException\n        at com.example.myproject.Book.getTitle(Book.java:16)\n        at com.example.myproject.Author.getBookTitles(Author.java:25)\n        at com.example.myproject.Bootstrap.main(Bootstrap.java:14)\n```\n\n----------------------------------------\n\nTITLE: Declaring YAML v2 Dependency in Go\nDESCRIPTION: This code snippet declares a dependency on the gopkg.in/yaml.v2 library, version v2.3.0. This indicates that the project requires the YAML v2 library for parsing and serialization. It also demonstrates how to pin to a specific version.\nSOURCE: https://github.com/elastic/logstash/blob/main/docker/ironbank/go/src/env2yaml/vendor/modules.txt#_snippet_0\n\nLANGUAGE: Go\nCODE:\n```\n# gopkg.in/yaml.v2 v2.3.0\ngopkg.in/yaml.v2\n```\n\n----------------------------------------\n\nTITLE: Loading Winlogbeat Ingest Pipelines into Elasticsearch (Shell)\nDESCRIPTION: This command, executed on the Winlogbeat host, loads the ingest pipelines associated with specified Winlogbeat modules (e.g., security, sysmon) into Elasticsearch. A connection to Elasticsearch is required for this step.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/working-with-winlogbeat-modules.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwinlogbeat setup --pipelines --modules security,sysmon\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Logstash Monitoring Collection in YAML\nDESCRIPTION: This snippet shows how to disable the default collection of Logstash monitoring metrics by setting the 'monitoring.enabled' parameter to false in the logstash.yml configuration file. This is necessary to prevent conflicts when using the serverless Elastic Agent integration for metrics collection. The configuration requires modifying the YAML file on the Logstash instance.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/serverless-monitoring-with-elastic-agent.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmonitoring.enabled: false\n```\n\n----------------------------------------\n\nTITLE: Inserting Current Timestamps with Sprintf Format in Plugins (JavaScript/Logstash Configuration)\nDESCRIPTION: Illustrates adding fields with the current time in Logstash pipeline configurations. The input block uses the heartbeat plugin to add a 'heartbeat_time' field, and the mutate filter adds a 'mutate_time' field, both leveraging '%{{TIME_NOW}}' as the value. Requires heartbeat input plugin and mutate filter. Inputs are event flows in the pipeline; outputs are events with new timestamp fields reflecting the time at the plugin execution moment.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/event-dependent-configuration.md#_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\ninput {\n  heartbeat {\n    add_field => { \"heartbeat_time\" => \"%{{TIME_NOW}}\" }\n  }\n}\nfilter {\n  mutate {\n    add_field => { \"mutate_time\" => \"%{{TIME_NOW}}\" }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Logstash Plugins Including Major Versions - Shell\nDESCRIPTION: This set of commands updates installed Logstash plugins to the latest version, including those with breaking major version changes, by using the --level=major flag. The first command updates all plugins, while the second targets a specific plugin. Users should be aware that major updates can introduce breaking changes and may require manual changes to configurations.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/working-with-plugins.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin update --level=major <1>\n```\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin update --level=major logstash-input-github <2>\n```\n\n----------------------------------------\n\nTITLE: Sending Processed Data to Elasticsearch Using Logstash Output Plugin (Python)\nDESCRIPTION: This snippet shows configuring the Logstash output plugin to send processed logs to Elasticsearch. While Logstash's native configuration uses a declarative syntax, the code comment indicates a Python script interacting with Elasticsearch's REST API for advanced data handling, such as index management or custom ingestion.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/org.eclipse.core!org.eclipse.core.expressions-NOTICE.txt#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport requests\n\ndef send_to_elasticsearch(data, index):\n    url = f\"http://localhost:9200/{index}/_doc\"\n    response = requests.post(url, json=data)\n    if response.status_code == 201:\n        print(\"Document indexed successfully\")\n    else:\n        print(f\"Failed to index document: {response.text}\")\n```\n\n----------------------------------------\n\nTITLE: Asciidoc Code Block with Ruby\nDESCRIPTION: This snippet illustrates how to include Ruby code samples within AsciiDoc documentation using the `[source,ruby]` directive.  The hashmarks (#) are included to prevent the example from rendering immediately; they should be removed in the actual asciidoc file.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/plugin-doc.md#_snippet_3\n\nLANGUAGE: txt\nCODE:\n```\n# [source,ruby]\n# -----\n# match => {\n#  \"field1\" => \"value1\"\n#  \"field2\" => \"value2\"\n#  ...\n# }\n# -----\n```\n\n----------------------------------------\n\nTITLE: RSpec Test for Logstash Output Plugin\nDESCRIPTION: This RSpec code tests the behavior of a Logstash output plugin named \"riemann\". It includes tests for the `#register` and `#receive` methods, covering scenarios with different configurations, such as specifying a protocol or not. The tests verify that the methods complete without errors or raise errors as expected.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/contributing-patch-plugin.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\n 2 require \"logstash/devutils/rspec/spec_helper\"\n 3 require \"logstash/plugin\"\n 4\n 5 describe \"outputs/riemann\" do\n 6   describe \"#register\" do\n 7     let(:output) do\n 8       LogStash::Plugin.lookup(\"output\", \"riemann\").new(configuration)\n 9     end\n10\n11     context \"when no protocol is specified\" do\n12       let(:configuration) { Hash.new }\n13\n14       it \"the method completes without error\" do\n15         expect {output.register}.not_to raise_error\n16       end\n17     end\n18\n19     context \"when a bad protocol is specified\" do\n20       let(:configuration) { {\"protocol\" => \"fake\"} }\n21\n22       it \"the method fails with error\" do\n23         expect {output.register}.to raise_error\n24       end\n25     end\n26\n27     context \"when the tcp protocol is specified\" do\n28       let(:configuration) { {\"protocol\" => \"tcp\"} }\n29\n30       it \"the method completes without error\" do\n31         expect {output.register}.not_to raise_error\n32       end\n33     end\n34   end\n35\n36   describe \"#receive\" do\n37     let(:output) do\n38       LogStash::Plugin.lookup(\"output\", \"riemann\").new(configuration)\n39     end\n40\n41     context \"when operating normally\" do\n42       let(:configuration) { Hash.new }\n43       let(:event) do\n44         data = {\"message\"=>\"hello\", \"@version\"=>\"1\",\n45                 \"@timestamp\"=>\"2015-06-03T23:34:54.076Z\",\n46                 \"host\"=>\"vagrant-ubuntu-trusty-64\"}\n47         LogStash::Event.new(data)\n48       end\n49\n50       before(:example) do\n51         output.register\n52       end\n53\n54       it \"should accept the event\" do\n55         expect { output.receive event }.not_to raise_error\n56       end\n57     end\n58   end\n59 end\n```\n\n----------------------------------------\n\nTITLE: Conditional Event Cancellation Using Logstash Ruby Filter in JSON\nDESCRIPTION: Executes inline Ruby code within a Logstash filter block, cancelling 90% of events at random by evaluating 'event.cancel if rand <= 0.90'. This probabilistically reduces event volume and requires the ruby filter plugin. The code runs for each event passing through the pipeline filter stage.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/core-operations.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\nfilter {\n  ruby {\n    code => \"event.cancel if rand <= 0.90\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Logstash Plugin from RubyGems (sh)\nDESCRIPTION: Demonstrates the command used by Logstash users to install a plugin that has been published to RubyGems.org. The `bin/logstash-plugin install` command automatically downloads the gem from the repository and integrates it into the Logstash installation. The plugin name must match its listing on RubyGems.org.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin install logstash-output-mypluginname\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Parameters\nDESCRIPTION: Shows an example of how to define configuration parameters for the plugin.  It defines the parameter name, validates the data type, provides a default value, and marks it as required.  This example uses string validation.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_3\n\nLANGUAGE: ruby\nCODE:\n```\n  config :variable_name, :validate => :variable_type, :default => \"Default value\", :required => boolean, :deprecated => boolean, :obsolete => string\n```\n\n----------------------------------------\n\nTITLE: Labeling Changelog Entries for Logstash Plugins Using Markdown\nDESCRIPTION: Example snippet showing how to add labeled changelog entries using markdown bullet points with prefix tags, highlighting internal or configuration changes distinctly to improve changelog clarity. This snippet is used inside changelog files to document plugin changes and their impact.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/community-maintainer.md#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n - internal: Refactored specs for better testing\\n - config: Default timeout configuration changed from 10s to 5s\n```\n\n----------------------------------------\n\nTITLE: Creating Logstash Plugin User API Key in Elasticsearch\nDESCRIPTION: Creates an API key for a plugin user with permissions to manage index templates, monitor, and perform various operations on Logstash-related indices. The key expires after 365 days.\nSOURCE: https://github.com/elastic/logstash/blob/main/ci/serverless/README.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\nPOST /_security/api_key\n{\n  \"name\": \"plugin_user\",\n  \"expiration\": \"365d\",   \n  \"role_descriptors\": { \n    \"plugin_user_role\": {\n      \"cluster\": [\"manage_index_templates\", \"monitor\"], \n      \"indices\": [\n        {\n          \"names\": [ \"logstash\", \"logstash-*\", \"ecs-logstash\", \"ecs-logstash-*\", \"serverless*\", \"logs-*\", \"metrics-*\", \"synthetics-*\", \"traces-*\" ], \n          \"privileges\": [\"manage\", \"write\", \"create_index\", \"read\", \"view_index_metadata\"]  \n        }\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Forcing Logstash Configuration Reload with SIGUP Signal\nDESCRIPTION: Sends a SIGHUP signal to the Logstash process to force reload of the configuration when auto-reloading is not enabled. The command requires process ID, such as `kill -SIGHUP 14175`, to trigger a restart of the pipeline with the updated configuration. Note that this method is not supported on Windows systems.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/reloading-config.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkill -SIGHUP 14175\n```\n\n----------------------------------------\n\nTITLE: Defining an Example Logstash Filter Plugin Class in Ruby\nDESCRIPTION: This Ruby class demonstrates a basic Logstash filter plugin that replaces the event's message field with a configurable value. Dependencies required are 'logstash/filters/base' and 'logstash/namespace', typically installed with Logstash. Key parameters include the config_name for plugin registration, a :message option validated as a string and defaulted to 'Hello World!'. The plugin expects an event object as input and outputs the modified event. Constraints: designed purely for example purposes, not production use.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"logstash/filters/base\"\nrequire \"logstash/namespace\"\n\n# Add any asciidoc formatted documentation here\n# This example filter will replace the contents of the default\n# message field with whatever you specify in the configuration.\n#\n# It is only intended to be used as an example.\nclass LogStash::Filters::Example < LogStash::Filters::Base\n\n  # Setting the config_name here is required. This is how you\n  # configure this filter from your Logstash config.\n  #\n  # filter {\n  #   example { message => \"My message...\" }\n  # }\n  config_name \"example\"\n\n  # Replace the message with this value.\n  config :message, :validate => :string, :default => \"Hello World!\"\n\n  public\n  def register\n    # Add instance variables\n  end # def register\n\n  public\n  def filter(event)\n\n    if @message\n      # Replace the event message with our message as configured in the\n      # config file.\n      event.set(\"message\", @message)\n    end\n\n    # filter_matched should go in the last line of our successful code\n    filter_matched(event)\n  end # def filter\n\nend # class LogStash::Filters::Example\n\n```\n\n----------------------------------------\n\nTITLE: Installing Local Plugin Definition in Logstash (Shell)\nDESCRIPTION: Runs the Logstash plugin manager to install plugins defined in the Logstash Gemfile, including any local plugins added via the `:path` option. The `--no-verify` flag skips certain verification steps. This command should be executed from the Logstash installation directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/filter-plugin/README.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin install --no-verify\n```\n\n----------------------------------------\n\nTITLE: Creating Logstash Tester User API Key in Elasticsearch\nDESCRIPTION: Creates an API key for a tester user with extensive permissions to manage templates, Logstash pipelines, ingest pipelines, and perform operations on various indices. The key expires after 365 days.\nSOURCE: https://github.com/elastic/logstash/blob/main/ci/serverless/README.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\nPOST /_security/api_key\n{\n  \"name\": \"tester_user\",\n  \"expiration\": \"365d\",   \n  \"role_descriptors\": { \n    \"tester_user_role\": {\n      \"cluster\": [\"manage_index_templates\", \"manage_logstash_pipelines\",\"manage_ingest_pipelines\"], \n      \"indices\": [\n        {\n          \"names\": [ \"logstash\", \"logstash-*\", \"ecs-logstash\", \"ecs-logstash-*\", \"serverless*\", \"logs-*\", \"metrics-*\", \"synthetics-*\", \"traces-*\", \"*test*\" ], \n          \"privileges\": [\"manage\", \"write\", \"create_index\", \"read\", \"view_index_metadata\"]  \n        }\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Calling filter_matched in Logstash Filter Plugins\nDESCRIPTION: Demonstration of calling the filter_matched method, which ensures that any fields or tags added through Logstash configuration for the filter are properly handled, including add_field, remove_field, add_tag, and remove_tag actions.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_10\n\nLANGUAGE: ruby\nCODE:\n```\n  filter_matched(event)\n```\n\n----------------------------------------\n\nTITLE: Setting JVM Stack Size\nDESCRIPTION: This code snippet demonstrates how to set the JVM stack size in the `jvm.options` file. Increasing the stack size may be necessary to prevent stack overflow errors.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/jvm-settings.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n-Xss4M\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Logstash Java Filter Plugin (Java)\nDESCRIPTION: Provides a complete example of a Logstash Java filter plugin named `java_filter_example`. This plugin reverses the string value of a configurable source field (defaulting to 'message') within Logstash events. It demonstrates implementing the `co.elastic.logstash.api.Filter` interface, using the `@LogstashPlugin` annotation, defining configuration settings with `PluginConfigSpec`, handling initialization in the constructor, and implementing the core event processing logic within the `filter` method.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-filter-plugin.md#_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n@LogstashPlugin(name = \"java_filter_example\")\npublic class JavaFilterExample implements Filter {\n\n    public static final PluginConfigSpec<String> SOURCE_CONFIG =\n            PluginConfigSpec.stringSetting(\"source\", \"message\");\n\n    private String id;\n    private String sourceField;\n\n    public JavaFilterExample(String id, Configuration config, Context context) {\n        this.id = id;\n        this.sourceField = config.get(SOURCE_CONFIG);\n    }\n\n    @Override\n    public Collection<Event> filter(Collection<Event> events, FilterMatchListener matchListener) {\n        for (Event e : events) {\n            Object f = e.getField(sourceField);\n            if (f instanceof String) {\n                e.setField(sourceField, StringUtils.reverse((String)f));\n                matchListener.filterMatched(e);\n            }\n        }\n        return events;\n    }\n\n    @Override\n    public Collection<PluginConfigSpec<?>> configSchema() {\n        return Collections.singletonList(SOURCE_CONFIG);\n    }\n\n    @Override\n    public String getId() {\n        return this.id;\n    }\n\n    @Override\n    public void close() {\n        this.sourceField = null;\n        return;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with Configuration\nDESCRIPTION: This shell command executes Logstash with the provided configuration file (java_filter.conf). This starts the Logstash pipeline that includes the Java filter plugin and allows the developer to test that the plugin is correctly integrated and functioning as designed.  It requires that the Logstash configuration file is saved to a specified path.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-filter-plugin.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -f /path/to/java_filter.conf\n```\n\n----------------------------------------\n\nTITLE: Using Boolean Values in Logstash Plugin Configuration - JavaScript\nDESCRIPTION: Demonstrates the correct usage of boolean values in plugin configurations, where 'true' and 'false' are unquoted keywords. Here, the 'ssl_enable' setting is enabled using a boolean 'true' value, which controls SSL functionality within the plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nssl_enable => true\n```\n\n----------------------------------------\n\nTITLE: Specifying Keystore Location in RPM/DEB\nDESCRIPTION: This shell script shows how to create a Logstash keystore using the `logstash-keystore` tool, while specifying the settings path. This is particularly important for RPM/DEB installations. The `path.settings` must be set to ensure correct keystore location.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/keystore.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nset +o history\nexport LOGSTASH_KEYSTORE_PASS=mypassword\nset -o history\nsudo -E /usr/share/logstash/bin/logstash-keystore --path.settings /etc/logstash create\n```\n\n----------------------------------------\n\nTITLE: Cloning Logstash Repository for Plugin Development in Shell\nDESCRIPTION: Command to clone a specific branch of the Logstash repository to gain access to the Java plugin API. The branch_name should correspond to the version of Logstash containing the preferred API revision.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --branch <branch_name> --single-branch https://github.com/elastic/logstash.git <target_folder>\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Core Path in gradle.properties\nDESCRIPTION: Configuration entry for the gradle.properties file that specifies the path to the Logstash core library, enabling the plugin to find and use the required Logstash dependencies.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_1\n\nLANGUAGE: txt\nCODE:\n```\nLOGSTASH_CORE_PATH=<target_folder>/logstash-core\n```\n\n----------------------------------------\n\nTITLE: Running Plugin Tests with RSpec (Shell)\nDESCRIPTION: Executes the plugin's test suite using RSpec. The `bundle exec` command ensures the tests run within the context of the bundled gems defined in the Gemfile, isolating the test environment.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/codec-plugin/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nbundle exec rspec\n```\n\n----------------------------------------\n\nTITLE: Secure HTTP Input with SSL/TLS in Downstream Logstash\nDESCRIPTION: Configures the HTTP input plugin in the downstream Logstash instance to use SSL/TLS for secure communication.  Requires specifying the SSL key, certificate, certificate authorities, and verification mode.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-http.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\ninput {\n  http {\n    ...\n\n    ssl => true\n    ssl_key => \"server.key.pk8\"\n    ssl_certificate => \"server.crt\"\n    ssl_certificate_authorities => \"ca.crt\"\n    ssl_verify_mode => force_peer\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Removing Gatekeeper Quarantine Attribute on macOS with xattr - Shell\nDESCRIPTION: This command line snippet removes the macOS Gatekeeper quarantine attribute recursively from a specified archive or extracted directory. It is required when running Logstash artifacts on macOS Catalina or newer to bypass system security dialogs that prevent starting unsigned software. The command requires no additional dependencies, but you must specify the path to the .tar.gz archive or extracted directory as <archive-or-directory>. It does not validate the file or directory; failure to specify the correct path can result in no effect.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/working-with-plugins.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nxattr -d -r com.apple.quarantine <archive-or-directory>\n```\n\n----------------------------------------\n\nTITLE: Configuring Avro Codec for Data Deserialization in Logstash\nDESCRIPTION: This snippet configures the Avro codec to deserialize serialized Avro records from Kafka input into Logstash events, using a specified schema URI. It is useful for processing real-time Kafka streams with Avro-encoded data. Dependencies include the Avro codec plugin and a valid schema file at the specified path.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/data-deserialization.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ninput {\n  kafka {\n    codec => {\n      avro => {\n        schema_uri => \"/tmp/schema.avsc\"\n      }\n    }\n  }\n}\n...\n```\n\n----------------------------------------\n\nTITLE: Verifying Logstash Docker Image Signature (Shell)\nDESCRIPTION: Details the process of verifying the integrity and authenticity of the downloaded Logstash Docker image using Cosign. It shows commands to first download the Elastic public key with `wget` and then use it to verify the image signature. Requires Cosign and `wget` to be installed. Steps correspond to comments <1> and <2>.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/docker.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://artifacts.elastic.co/cosign.pub <1>\ncosign verify --key cosign.pub {{docker-repo}}:{{stack-version}} <2>\n```\n\n----------------------------------------\n\nTITLE: Declaring Java JAR Dependencies in `.gemspec` (Ruby)\nDESCRIPTION: Example snippet from a `.gemspec` file illustrating how to declare dependencies on Java Archive (JAR) files. It uses `s.requirements` to specify the JAR artifact coordinates (group, name, version) and adds a runtime dependency on the `jar-dependencies` gem, enabling Logstash to manage and load required JARs from repositories like Maven Central.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_10\n\nLANGUAGE: Ruby\nCODE:\n```\n  # Jar dependencies\n  s.requirements << \"jar 'org.elasticsearch:elasticsearch', '5.0.0'\"\n  s.add_runtime_dependency 'jar-dependencies'\n```\n\n----------------------------------------\n\nTITLE: Implementing the cloneCodec Method for Logstash Java Codecs\nDESCRIPTION: Shows the Java implementation of the `cloneCodec` method for a Logstash codec. This method is crucial for multi-threaded input plugins, returning a new, independent instance of the codec (sharing configuration like the delimiter) to avoid state conflicts between threads. Depends on the specific Codec implementation class (`JavaCodecExample` in this case).\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic Codec cloneCodec() {\n    return new JavaCodecExample(this.delimiter);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing a Locally Built Plugin Gem - Shell\nDESCRIPTION: This command installs a Logstash plugin from a locally built Ruby gem file, specified via the full file path. It is used for plugins not published on RubyGems.org. The path should point to a valid Logstash plugin gem file, and the operation requires access to the filesystem and may require elevated permissions.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/working-with-plugins.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin install /path/to/logstash-output-kafka-1.0.0.gem\n```\n\n----------------------------------------\n\nTITLE: Importing Elastic GPG Key for YUM Repository Management (shell)\nDESCRIPTION: This shell command uses rpm to import Elastic's GPG public signing key for use in RPM/YUM-based systems (e.g., CentOS, RHEL). The imported key is used to verify the authenticity of packages downloaded from the Elastic repository. No input parameters; output is a new GPG key entry in the RPM database.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/installing-logstash.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nsudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch\n```\n\n----------------------------------------\n\nTITLE: Setting Centralized Pipeline Management Credentials in logstash.yml (YAML)\nDESCRIPTION: Configures the username and password in `logstash.yml` that Logstash uses for Centralized Pipeline Management features. Set `xpack.management.elasticsearch.username` and `xpack.management.elasticsearch.password`. The specified user must have both `logstash_admin` and `logstash_writer` roles.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nxpack.management.elasticsearch.username: logstash_admin_user \nxpack.management.elasticsearch.password: t0p.s3cr3t\n```\n\n----------------------------------------\n\nTITLE: Using sprintf Method for Event-Dependent Configuration in Logstash\nDESCRIPTION: Example of using the sprintf method from the Event class to handle event-dependent configuration in a Logstash filter plugin. This allows dynamically evaluating configuration values against the current event.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_9\n\nLANGUAGE: ruby\nCODE:\n```\nfield_foo = event.sprintf(field)\n```\n\n----------------------------------------\n\nTITLE: Specifying an Array of Hashes in Logstash Plugin Configuration - JavaScript\nDESCRIPTION: Shows how to configure a plugin setting that takes an array of hash objects. Each hash contains key-value pairs, here representing user details such as 'id' and 'name'. This syntax supports lists of complex structured data within plugin options. Note that this array type is deprecated in favor of plugin-defined list types but remains relevant for mixed or hash list entries.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nusers => [ {id => 1, name => bob}, {id => 2, name => jane} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Hash Type Settings in Logstash Plugins - JavaScript\nDESCRIPTION: Illustrates how to define a hash data structure for plugin settings using key-value pairs enclosed in braces. Entries are separated by spaces rather than commas. The example shows both multiline and single-line syntax options used for defining match rules or other associative arrays.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\nmatch => {\n  \"field1\" => \"value1\"\n  \"field2\" => \"value2\"\n  ...\n}\n# or as a single line. No commas between entries:\nmatch => { \"field1\" => \"value1\" \"field2\" => \"value2\" }\n```\n\n----------------------------------------\n\nTITLE: Querying Elasticsearch for Documents Matching geoip.city_name using Shell\nDESCRIPTION: This shell snippet uses curl to perform an Elasticsearch search for documents indexed by the Logstash pipeline where the 'geoip.city_name' field matches a given value (e.g., 'Buffalo'). As before, replace $DATE with the correct date format corresponding to your data indices. This approach demonstrates searching on enriched fields generated by the grok and geoip plugins within Logstash. Dependencies: curl, active Elasticsearch node, valid date-based indices. Input: none directly (manual editing of $DATE and city_name needed). Output: JSON-formatted Elasticsearch search response.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\ncurl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=geoip.city_name=Buffalo'\n```\n\n----------------------------------------\n\nTITLE: Running Filebeat Setup and Modules Commands - Shell\nDESCRIPTION: These shell commands are used to set up Filebeat, enable required modules, load ingest pipelines, and start Filebeat. Dependencies include an installed Filebeat instance and connectivity to both Elasticsearch and Kibana. The commands initialize templates and dashboards (`filebeat -e setup`), enable a specific module (e.g., `system`) with `filebeat modules enable system`, load pipelines via `filebeat setup --pipelines --modules system`, and finally start Filebeat with `filebeat -e`. The required setup step ensures Filebeat is ready to send data, while module enabling and setup command ensure correct parsing and data routing. Filebeat configuration and proper permissions are prerequisites.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/use-filebeat-modules-kafka.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nfilebeat -e setup\n```\n\nLANGUAGE: shell\nCODE:\n```\nfilebeat modules enable system\n```\n\nLANGUAGE: shell\nCODE:\n```\nfilebeat setup --pipelines --modules system\n```\n\nLANGUAGE: shell\nCODE:\n```\nfilebeat -e\n```\n\n----------------------------------------\n\nTITLE: Authenticating to RubyGems.org and Securing Credentials - Shell\nDESCRIPTION: Obtains an API key from RubyGems.org and stores it securely in the ~/.gem/credentials file. Requires curl for HTTP requests and correct username and password for RubyGems.org. File permissions are set to restrict access to the credentials file for security purposes. Prerequisite for publishing gems to RubyGems.org from the command line.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ncurl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials\nchmod 0600 ~/.gem/credentials\n```\n\n----------------------------------------\n\nTITLE: Secure HTTP Output with SSL/TLS in Upstream Logstash\nDESCRIPTION: Configures the HTTP output plugin in the upstream Logstash instance to use SSL/TLS for secure communication.  Requires specifying the certificate authority certificate, client key, and client certificate.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-http.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\noutput {\n  http {\n    ...\n\n    cacert => \"ca.crt\"\n    client_key => \"client.key.pk8\"\n    client_cert => \"client.crt\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Target Elasticsearch Cluster UUID in Logstash YAML\nDESCRIPTION: This YAML snippet shows how to statically assign the target Elasticsearch cluster's UUID through the 'monitoring.cluster_uuid' setting in 'logstash.yml'. This forces Logstash monitoring data to be associated with the specified cluster in Stack Monitoring UI, especially useful when the pipeline lacks Elasticsearch output plugins that otherwise auto-calculate this value. The main parameter is the cluster UUID string; it requires obtaining the UUID from the Elasticsearch cluster stats API. Output is configuration guiding metric binding.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-with-metricbeat.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmonitoring.cluster_uuid: PRODUCTION_ES_CLUSTER_UUID\n```\n\n----------------------------------------\n\nTITLE: Verifying Java Version with Shell Command\nDESCRIPTION: This command executes `Java -version` in the command line to display the version information of the Java Runtime Environment (JRE) or Java Development Kit (JDK) installed on the system. This is crucial for verifying that a supported Java version is available for Logstash to function correctly.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash-windows.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nPS C:\\> Java -version\n```\n\n----------------------------------------\n\nTITLE: Changing the logstash_system user password in Elasticsearch\nDESCRIPTION: This API call sets a new password for the logstash_system user in Elasticsearch. This is necessary after upgrading from older versions as the built-in user is disabled for security reasons.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-troubleshooting.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nPUT _security/user/logstash_system/_password\n{\n  \"password\": \"newpassword\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example of External Link in AsciiDoc\nDESCRIPTION: This snippet shows examples of creating external links in AsciiDoc to point to content in the Logstash Reference Guide. It demonstrates how to specify the URL and the link text.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/plugin-doc.md#_snippet_2\n\nLANGUAGE: txt\nCODE:\n```\n{logstash-ref}/plugins-codecs-multiline.html[Multiline codec plugin]\n```\n\nLANGUAGE: txt\nCODE:\n```\n{logstash-ref}/getting-started-with-logstash.html\n```\n\n----------------------------------------\n\nTITLE: Implementing Core Codec Methods (decode, flush, encode) in Java\nDESCRIPTION: Provides Java implementations for the essential `decode`, `flush`, and `encode` methods of a Logstash codec. `decode` splits incoming byte data based on a specified delimiter, creating separate events. `flush` handles remaining data (simplified here to call `decode`). `encode` serializes an event to bytes, appending the delimiter. Depends on `java.nio.ByteBuffer`, `java.util.Map`, `java.util.function.Consumer`, `java.io.OutputStream`, `org.logstash.Event`, and `java.nio.charset.Charset`.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_6\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic void decode(ByteBuffer byteBuffer, Consumer<Map<String, Object>> consumer) {\n    // a not-production-grade delimiter decoder\n    byte[] byteInput = new byte[byteBuffer.remaining()];\n    byteBuffer.get(byteInput);\n    if (byteInput.length > 0) {\n        String input = new String(byteInput);\n        String[] split = input.split(delimiter);\n        for (String s : split) {\n            Map<String, Object> map = new HashMap<>();\n            map.put(\"message\", s);\n            consumer.accept(map);\n        }\n    }\n}\n\n@Override\npublic void flush(ByteBuffer byteBuffer, Consumer<Map<String, Object>> consumer) {\n    // if the codec maintains any internal state such as partially-decoded input, this\n    // method should flush that state along with any additional input supplied in\n    // the ByteBuffer\n\n    decode(byteBuffer, consumer); // this is a simplistic implementation\n}\n\n@Override\npublic void encode(Event event, OutputStream outputStream) throws IOException {\n    outputStream.write((event.toString() + delimiter).getBytes(Charset.defaultCharset()));\n}\n```\n\n----------------------------------------\n\nTITLE: Installing apt-transport-https Dependency on Debian/Ubuntu (shell)\nDESCRIPTION: This shell command installs the apt-transport-https package, which is required for APT to communicate over HTTPS with external repositories. It must be run with superuser privileges, typically via sudo, and requires network connectivity to Debian or Ubuntu package mirrors. Input is no parameters, as the command installs a fixed package. Output is an updated system with HTTPS transport support for APT sources.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/installing-logstash.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install apt-transport-https\n```\n\n----------------------------------------\n\nTITLE: Implementing the getId Method for Logstash Java Codecs\nDESCRIPTION: Demonstrates the Java implementation of the `getId` method for a Logstash codec. This method returns the unique identifier assigned to the codec instance during its creation, typically a UUID.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_8\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic String getId() {\n    return id;\n}\n```\n\n----------------------------------------\n\nTITLE: Removing CMS GC Flags in Logstash JVM Options - Shell\nDESCRIPTION: Demonstrates removal of Java garbage collector flags from the 'config/jvm.options' file to ensure compatibility with Logstash on JDK 21+. Requires access to the file system and sufficient permissions to modify configuration files. Input: 'config/jvm.options' containing CMS flags. Output: Updated file with CMS flags removed. Removing these legacy flags is essential because they are no longer supported or necessary in newer JDK versions.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/getting-started-with-logstash.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n## GC configuration\n-XX:+UseConcMarkSweepGC\n-XX:CMSInitiatingOccupancyFraction=75\n-XX:+UseCMSInitiatingOccupancyOnly\n\n```\n\n----------------------------------------\n\nTITLE: Enabling the logstash_system User via Elasticsearch API (Console)\nDESCRIPTION: Uses the Elasticsearch Security API (`PUT _security/user/logstash_system/_enable`) to enable the `logstash_system` built-in user. This user, used for Logstash monitoring, might be disabled by default in older installations or after upgrades.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\nPUT _security/user/logstash_system/_enable\n```\n\n----------------------------------------\n\nTITLE: Implementing close Method in Java\nDESCRIPTION: This Java code snippet implements the `close` method for filter plugins.  It is designed to release resources, such as database connections, that were established during the plugin's initialization phase when the pipeline is shutting down. It takes no parameters and typically involves setting resource references to null.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-filter-plugin.md#_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic void close() {\n    // shutdown a resource that was instantiated during the filter initialization phase.\n    this.sourceField = null;\n    return;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Trusted CA Certificate for Elasticsearch Monitoring in YAML\nDESCRIPTION: This YAML entry tells Logstash where to find the PEM-encoded CA certificate that will be trusted when validating the SSL identity of the Elasticsearch cluster. The path provided should point to the relevant CA on disk. Required when connecting over HTTPS if the CA is not globally trusted, and only applicable if 'xpack.monitoring.enabled' is true and SSL/TLS is enabled.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-internal-collection-legacy.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nxpack.monitoring.elasticsearch.ssl.certificate_authority: /path/to/ca.crt\n```\n\n----------------------------------------\n\nTITLE: Implementing getId Method in Java Logstash Plugin\nDESCRIPTION: This Java method overrides the getId() function to return the plugin's unique identifier, which was assigned during its instantiation. It ensures each plugin instance can be uniquely identified within Logstash. Dependencies include the main plugin class context; it's essential for plugin registration and management.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\n```java\n@Override\npublic String getId() {\n    return id;\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Elasticsearch Output to Use Ingest Pipelines (YAML)\nDESCRIPTION: Example Logstash configuration demonstrating how to read from a Beats input and send data to Elasticsearch. It conditionally uses an ingest pipeline specified in the event's metadata (`[@metadata][pipeline]`). If the metadata field exists, the `pipeline` option in the Elasticsearch output is set accordingly; otherwise, a standard output configuration is used. Key parameters like `index`, `action`, and `pipeline` are configured.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/use-ingest-pipelines.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninput {\n  beats {\n    port => 5044\n  }\n}\n\noutput {\n  if [@metadata][pipeline] {\n    elasticsearch {\n      hosts => \"https://061ab24010a2482e9d64729fdb0fd93a.us-east-1.aws.found.io:9243\"\n      manage_template => false\n      index => \"%{[\\@metadata][beat]}-%{[@metadata][version]}\" <1>\n      action => \"create\" <2>\n      pipeline => \"%{[\\@metadata][pipeline]}\" <3>\n      user => \"elastic\"\n      password => \"secret\"\n    }\n  } else {\n    elasticsearch {\n      hosts => \"https://061ab24010a2482e9d64729fdb0fd93a.us-east-1.aws.found.io:9243\"\n      manage_template => false\n      index => \"%{[\\@metadata][beat]}-%{[@metadata][version]}\" <1>\n      action => \"create\"\n      user => \"elastic\"\n      password => \"secret\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Small Persistent Queue Size in Logstash\nDESCRIPTION: Sample configuration for the Logstash settings file (`logstash.yml`) demonstrating how to enable persistent queues (`queue.type: persisted`) and set a specific, smaller maximum byte capacity (`queue.max_bytes: 10mb`) per pipeline. This configuration is useful when disk space or performance is a priority over extensive buffering.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/persistent-queues.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nqueue.type: persisted\nqueue.max_bytes: 10mb\n```\n\n----------------------------------------\n\nTITLE: Implementing Stop and AwaitStop Methods for Cooperative Shutdown in Java\nDESCRIPTION: Demonstrates the implementation of the stop method using a volatile boolean flag to indicate when event production should cease cooperatively. The awaitStop method blocks until the plugin has fully stopped by waiting on a CountDownLatch. This design supports asynchronous, cooperative plugin termination in compliance with Logstash's input lifecycle requirements.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nprivate final CountDownLatch done = new CountDownLatch(1);\nprivate volatile boolean stopped;\n\n@Override\npublic void stop() {\n    stopped = true; // set flag to request cooperative stop of input\n}\n\n@Override\npublic void awaitStop() throws InterruptedException {\n    done.await(); // blocks until input has stopped\n}\n```\n\n----------------------------------------\n\nTITLE: Running Gradle packaging task for Logstash Java plugin\nDESCRIPTION: Shell command to run the Gradle packaging task that generates Ruby source files, gemspec file, and Gemfile required to package the Java plugin as a Ruby gem.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew gem\n```\n\n----------------------------------------\n\nTITLE: Using Codec Setting in Logstash Plugins - JavaScript\nDESCRIPTION: Shows example usage of the 'codec' setting in Logstash plugins, specifying the codec as 'json'. Codecs handle data encoding and decoding transparently at the input or output level, simplifying pipeline configurations by replacing the need for separate filters to handle encoding conversions.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\ncodec => \"json\"\n```\n\n----------------------------------------\n\nTITLE: Creating Kibana User and Assigning Reader and System Roles\nDESCRIPTION: This Shell code snippet shows how to create a user in Kibana and assign them the 'logstash_reader' and 'logstash_system' roles. The `logstash_reader` role is for reading data from the logstash indices, and the `logstash_system` role provides permissions to check the availability of the supported features. This allows users to have read access to logstash data, and supports centralized pipeline management if applicable.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nPOST _security/user/logstash_user\n{\n  \"password\" : \"x-pack-test-password\",\n  \"roles\" : [ \"logstash_reader\", \"logstash_system\"], <1>\n  \"full_name\" : \"Kibana User for Logstash\"\n}\n\n```\n\n----------------------------------------\n\nTITLE: Publishing a Logstash plugin to RubyGems.org\nDESCRIPTION: Commands to prepare and publish a Logstash plugin to RubyGems.org. This process installs dependencies, runs tests, and publishes the gem if all tests pass.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_24\n\nLANGUAGE: sh\nCODE:\n```\nbundle install\nbundle exec rake vendor\nbundle exec rspec\nbundle exec rake publish_gem\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Elasticsearch Output for Winlogbeat Pipelines (YAML)\nDESCRIPTION: This Logstash configuration demonstrates how to receive data from Beats input and send it to Elasticsearch, dynamically using Winlogbeat ingest pipelines. It checks for the `[@metadata][pipeline]` field in the event and sets the `pipeline` option in the Elasticsearch output accordingly. It also shows index naming conventions suitable for data streams (default) and legacy setups.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/working-with-winlogbeat-modules.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninput {\n  beats {\n    port => 5044\n  }\n}\n\noutput {\n  if [@metadata][pipeline] {\n    elasticsearch {\n      hosts => \"https://061ab24010a2482e9d64729fdb0fd93a.us-east-1.aws.found.io:9243\"\n      manage_template => false\n      index => \"%{[\\@metadata][beat]}-%{[\\@metadata][version]}\" <1>\n      action => \"create\" <2>\n      pipeline => \"%{[\\@metadata][pipeline]}\" <3>\n      user => \"elastic\"\n      password => \"secret\"\n    }\n  } else {\n    elasticsearch {\n      hosts => \"https://061ab24010a2482e9d64729fdb0fd93a.us-east-1.aws.found.io:9243\"\n      manage_template => false\n      index => \"%{[\\@metadata][beat]}-%{[\\@metadata][version]}\" <1>\n      action => \"create\"\n      user => \"elastic\"\n      password => \"secret\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Log4j2 Log File Rollover and Strategy Configuration\nDESCRIPTION: Configures log file rotation with policies based on time and size, including max file count and file retention rules. Defines how log files are renamed, retained, or deleted during rollover events, with detailed parameters for policies, strategy, and conditions.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/logging.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nappender.rolling.type = RollingFile\nappender.rolling.name = plain_rolling\nappender.rolling.fileName = ${sys:ls.logs}/logstash-plain.log\nappender.rolling.filePattern = ${sys:ls.logs}/logstash-plain-%d{yyyy-MM-dd}-%i.log.gz\nappender.rolling.policies.type = Policies\nappender.rolling.policies.time.type = TimeBasedTriggeringPolicy\nappender.rolling.policies.time.interval = 1\nappender.rolling.policies.time.modulate = true\nappender.rolling.layout.type = PatternLayout\nappender.rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c]%notEmpty{[%X{pipeline.id}]}%notEmpty{[%X{plugin.id}]} %m%n\nappender.rolling.policies.size.type = SizeBasedTriggeringPolicy\nappender.rolling.policies.size.size = 100MB\nappender.rolling.strategy.type = DefaultRolloverStrategy\nappender.rolling.strategy.max = 30\nappender.rolling.strategy.action.type = Delete\nappender.rolling.strategy.action.basepath = ${sys:ls.logs}\nappender.rolling.strategy.action.condition.type = IfFileName\nappender.rolling.strategy.action.condition.glob = logstash-plain-*\nappender.rolling.strategy.action.condition.nested_condition.type = IfLastModified\nappender.rolling.strategy.action.condition.nested_condition.age = 7D\n```\n\n----------------------------------------\n\nTITLE: Running Gradle Packaging Task\nDESCRIPTION: This shell command executes the Gradle packaging task to create a Ruby gem from the Java plugin's source code. This gem is then used to install the plugin into Logstash. The command will generate a gem file in the root directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-filter-plugin.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew gem\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Queue Capacity in Logstash\nDESCRIPTION: YAML configuration example for setting up a persistent queue with a maximum size of 8GB, used to control back pressure and buffer events on disk.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/persistent-queues.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nqueue.type: persisted\nqueue.max_bytes: 8gb\n```\n\n----------------------------------------\n\nTITLE: Querying Logstash Monitoring API Using curl - Bash\nDESCRIPTION: This snippet demonstrates how to retrieve general health and version information about a running Logstash instance by performing an HTTP GET request with curl to the Logstash monitoring API endpoint at tcp:9600. No authentication or encryption is enabled in this example. Required: curl utility, Logstash running with HTTP API enabled (defaults to port 9600). The 'pretty' query parameter formats output for easier readability. Input: none; Output: Logstash node metadata in JSON format. Limitation: endpoint must be accessible and unsecured.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-logstash.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XGET 'localhost:9600/?pretty'\n```\n\n----------------------------------------\n\nTITLE: Example Logstash Docker Image Verification Output (Shell)\nDESCRIPTION: Shows a sample of the expected output in the console upon successful verification of the Logstash Docker image signature using the `cosign verify` command. It lists the types of checks performed during the verification process. Note: This is not code to be executed, but an example of command output.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/docker.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nVerification for {{docker-repo}}:{{stack-version}} --\nThe following checks were performed on each of these signatures:\n  - The cosign claims were validated\n  - Existence of the claims in the transparency log was verified offline\n  - The signatures were verified against the specified public key\n```\n\n----------------------------------------\n\nTITLE: Configuring Pipeline Routing in Log4j2 for Logstash - Shell\nDESCRIPTION: Provides a group of Log4j2 'log4j2.properties' configuration entries that enable detailed per-pipeline file logging in Logstash using rolling appender and pattern layout. Dependencies include the Log4j2 logging framework shipped with Logstash. Key parameters include log file location, rolling policy, and layout pattern. Input: None, but edits must be made to the 'log4j2.properties' file. Output: Logstash writes pipeline-specific logs in defined locations with rolling behavior. This is required when upgrading from older Logstash versions.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/getting-started-with-logstash.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nappender.routing.type = PipelineRouting\nappender.routing.name = pipeline_routing_appender\nappender.routing.pipeline.type = RollingFile\nappender.routing.pipeline.name = appender-${ctx:pipeline.id}\nappender.routing.pipeline.fileName = ${sys:ls.logs}/pipeline_${ctx:pipeline.id}.log\nappender.routing.pipeline.filePattern = ${sys:ls.logs}/pipeline_${ctx:pipeline.id}.%i.log.gz\nappender.routing.pipeline.layout.type = PatternLayout\nappender.routing.pipeline.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %m%n\nappender.routing.pipeline.policy.type = SizeBasedTriggeringPolicy\nappender.routing.pipeline.policy.size = 100MB\nappender.routing.pipeline.strategy.type = DefaultRolloverStrategy\nappender.routing.pipeline.strategy.max = 30\n\n```\n\n----------------------------------------\n\nTITLE: Defining the register Method for Logstash Output Plugins in Ruby\nDESCRIPTION: This Ruby snippet defines the `register` method, which serves as the initialization step for the plugin once configuration values are available. Inside this method, you can set up instance variables or initialize resources, making use of the configuration parameters. This method must be defined, and Logstash expects the plugin to set up its state or dependencies during its lifecycle initialization.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_5\n\nLANGUAGE: ruby\nCODE:\n```\n  public\n  def register\n  end # def register\n```\n\n----------------------------------------\n\nTITLE: Defining Class Structure for a Java Codec Plugin\nDESCRIPTION: Class declaration for a Java codec plugin, showing the required annotation and interface implementation. The @LogstashPlugin annotation defines the name as it will appear in pipeline configurations.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n@LogstashPlugin(name=\"java_codec_example\")\npublic class JavaCodecExample implements Codec {\n```\n\n----------------------------------------\n\nTITLE: Referencing Nested Fields in Logstash Events (Logstash Configuration)\nDESCRIPTION: This snippet shows an example event JSON with both top-level and nested fields, illustrating the data structures that field reference syntax accesses. Users can reference nested fields like '[response][status]' or '[ua][os]' directly in their pipeline configuration or plugin settings. This example requires a JSON-formatted event and demonstrates how field hierarchies are represented. Input consists of event data, and it can be accessed via the described syntax for filters, conditionals, and outputs.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/event-dependent-configuration.md#_snippet_0\n\nLANGUAGE: Logstash Configuration\nCODE:\n```\n{\n  \"agent\": \"Mozilla/5.0 (compatible; MSIE 9.0)\",\n  \"ip\": \"192.168.24.44\",\n  \"request\": \"/index.html\"\n  \"response\": {\n    \"status\": 200,\n    \"bytes\": 52353\n  },\n  \"ua\": {\n    \"os\": \"Windows 7\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running pqrepair as Logstash Service User\nDESCRIPTION: Example of running the pqrepair utility with sudo to handle folder permission issues when Logstash is run as a service.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/persistent-queues.md#_snippet_7\n\nLANGUAGE: txt\nCODE:\n```\n/usr/share/logstash$ sudo -u logstash bin/pqrepair /var/lib/logstash/queue/main/\n```\n\n----------------------------------------\n\nTITLE: Defining a Logstash Filter Plugin Class Declaration (Ruby)\nDESCRIPTION: This line demonstrates the proper way to declare the main class for a Logstash filter plugin. The class should inherit from LogStash::Filters::Base to ensure compatibility with the Logstash plugin system. The class name must match the plugin's purpose, typically reflecting the plugin filename and config_name.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\nclass LogStash::Filters::Example < LogStash::Filters::Base\n```\n\n----------------------------------------\n\nTITLE: Expected Logstash output format with Java output plugin\nDESCRIPTION: Example of the expected JSON output from Logstash when running with the Java output plugin. Shows the timestamp, message, version, host, and sequence fields.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_13\n\nLANGUAGE: txt\nCODE:\n```\n{\"@timestamp\":\"yyyy-MM-ddTHH:mm:ss.SSSZ\",\"message\":\"Hello world!\",\"@version\":\"1\",\"host\":\"<yourHostname>\",\"sequence\":0}\n```\n\n----------------------------------------\n\nTITLE: Using --pipeline.unsafe_shutdown Flag in Logstash\nDESCRIPTION: Explains that starting Logstash with the --pipeline.unsafe_shutdown flag allows forced termination in case of stalls. Warns about potential data loss if Logstash is forcefully killed or crashes without this flag, unless persistent queues are enabled.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/shutdown.md#_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Loading Filebeat Ingest Pipelines into Elasticsearch (Shell)\nDESCRIPTION: Loads ingest pipelines for specified Filebeat modules (e.g., nginx, system) into Elasticsearch using the `filebeat setup` command. This command requires a direct connection to the Elasticsearch cluster where the pipelines should be stored.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/use-ingest-pipelines.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nfilebeat setup --pipelines --modules nginx,system\n```\n\n----------------------------------------\n\nTITLE: Implementing the Decode Method\nDESCRIPTION: Illustrates the `decode` method, which transforms incoming data from an input into events. The `decode` method utilizes a `yield` statement to return decoded events to the Logstash pipeline.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_5\n\nLANGUAGE: ruby\nCODE:\n```\n  public\n  def decode(data)\n    @lines.decode(data) do |line|\n      replace = { \"message\" => line[\"message\"].to_s + @append }\n      yield LogStash::Event.new(replace)\n    end\n  end # def decode\n```\n\n----------------------------------------\n\nTITLE: Packaging Plugins with Logstash Plugin Manager - Shell\nDESCRIPTION: This shell command uses the Logstash plugin manager ('bin/logstash-plugin') to prepare an offline plugin pack for distribution. The 'prepare-offline-pack' subcommand collects specified plugins and their dependencies, compressing them into a zip archive for offline installation. Parameters include '--output' to set the output zip file location (defaulting to '/LOGSTASH_HOME/logstash-offline-plugins-9.0.0.zip'), '--overwrite' to allow overwriting existing files, and a list of plugins ('[PLUGINS]') to include; at least one plugin name is required.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/offline-plugins.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin prepare-offline-pack --output OUTPUT --overwrite [PLUGINS]\n```\n\n----------------------------------------\n\nTITLE: Custom Log File Size Limit Rollover Strategy\nDESCRIPTION: Defines a rollover action that triggers deletion of log files if their total compressed size exceeds a specified limit, helping control disk usage during log rotation.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/logging.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nappender.rolling.type = RollingFile\nappender.rolling.strategy.action.condition.glob = pipeline_${ctx:pipeline.id}.*.log.gz\nappender.rolling.strategy.action.condition.nested_condition.type = IfAccumulatedFileSize\nappender.rolling.strategy.action.condition.nested_condition.exceeds = 5MB\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Metadata in Gradle build file for Java Logstash Plugin\nDESCRIPTION: This snippet illustrates setting up plugin configuration parameters within the Gradle build script, including metadata such as group, version, description, license, author, and plugin-specific class and name. These settings automate the packaging process, linking the build configuration with plugin registration and compatibility.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\n// ===========================================================================\n// plugin info\n// ===========================================================================\ngroup                      'org.logstashplugins' // must match the package of the main plugin class\nversion                    \"${file(\"VERSION\").text.trim()}\" // read from required VERSION file\ndescription                = \"Example Java filter implementation\"\npluginInfo.licenses        = ['Apache-2.0'] // list of SPDX license IDs\npluginInfo.longDescription = \"This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using $LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\npluginInfo.authors         = ['Elasticsearch']\npluginInfo.email           = ['info@elastic.co']\npluginInfo.homepage        = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\npluginInfo.pluginType      = \"filter\"\npluginInfo.pluginClass     = \"JavaFilterExample\"\npluginInfo.pluginName      = \"java_filter_example\"\n// ===========================================================================\n```\n\n----------------------------------------\n\nTITLE: Adding Jar Dependencies in Gemspec for Logstash Plugins - ruby\nDESCRIPTION: This snippet demonstrates how to declare a jar file dependency in a plugin's gemspec. The requirements array is extended with a Maven coordinate for the required jar, and add_runtime_dependency is used to ensure the jar-dependencies gem is installed. During installation, defined jars are fetched from Maven repositories. Place this configuration within your gemspec, specifying the desired group, artifact, and version, and always include 'jar-dependencies' as a runtime dependency when using this mechanism.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_10\n\nLANGUAGE: ruby\nCODE:\n```\n  # Jar dependencies\n  s.requirements << \"jar 'org.elasticsearch:elasticsearch', '5.0.0'\"\n  s.add_runtime_dependency 'jar-dependencies'\n```\n\n----------------------------------------\n\nTITLE: Example GPLv2 Interactive Program Startup Message\nDESCRIPTION: Illustrates a short message for interactive programs to display upon startup, informing users about the GPLv2 licensing, lack of warranty, and conditions for redistribution. Suggests hypothetical commands ('show w', 'show c') for users to access detailed license information.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/jruby-NOTICE.txt#_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n    Gnomovision version 69, Copyright (C) year name of author Gnomovision comes\n    with ABSOLUTELY NO WARRANTY; for details type 'show w'.  This is free\n    software, and you are welcome to redistribute it under certain conditions;\n    type 'show c' for details.\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Composite Field References in the Event API (Logstash Configuration)\nDESCRIPTION: Demonstrates programmatic construction of composite field references within plugin or Ruby Filter contexts, where a field reference string is combined with others to access deeply nested data. Shows the distinction between the composite reference and its canonical (flattened) representation. Requires Ruby Filter plugin or Event API context. Input parameters are field names as strings; output is a valid composite field reference for Logstash's Event API.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/event-dependent-configuration.md#_snippet_2\n\nLANGUAGE: Logstash Configuration\nCODE:\n```\n    fieldReference = \"[path][to][deep nested field]\"\n    compositeFieldReference = \"[@metadata][#{fieldReference}][size]\"\n    # => \"[@metadata][[path][to][deep nested field]][size]\"\n```\n\n----------------------------------------\n\nTITLE: Running Plugin Tests with RSpec - Shell\nDESCRIPTION: Uses Bundler to execute RSpec and run the plugin's test suite. Assumes RSpec is listed as a development dependency in the Gemfile and tests are written in the spec/ directory. Outputs test results to the terminal, where failures and successes are reported. No parameters required when running all tests.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nbundle exec rspec\n```\n\n----------------------------------------\n\nTITLE: Declaring Codec Plugin Class\nDESCRIPTION: Defines a new codec plugin class that inherits from `LogStash::Codecs::Base`. This is the base class for all Logstash codecs, providing a standard interface and methods for interacting with Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nclass LogStash::Codecs::Example < LogStash::Codecs::Base\n```\n\n----------------------------------------\n\nTITLE: Setting Logstash Core Path in Gradle Properties\nDESCRIPTION: Defines the required property in a gradle.properties file informing the plugin project where to locate the compiled Logstash core jar. This configuration ensures the plugin's build process has access to the Logstash core classes necessary for compilation and runtime. The value is set to the logstash-core folder within the cloned Logstash codebase.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_2\n\nLANGUAGE: txt\nCODE:\n```\nLOGSTASH_CORE_PATH=<target_folder>/logstash-core\n```\n\n----------------------------------------\n\nTITLE: Event with GeoIP Information in JSON\nDESCRIPTION: Example of the JSON structure after applying both Grok and GeoIP filters, showing the additional geographic data derived from the client IP address, including city, country, and coordinates.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n        \"request\" => \"/presentations/logstash-monitorama-2013/images/kibana-search.png\",\n          \"agent\" => \"\\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\\\"\",\n          \"geoip\" => {\n              \"timezone\" => \"Europe/Moscow\",\n                    \"ip\" => \"83.149.9.216\",\n              \"latitude\" => 55.7485,\n        \"continent_code\" => \"EU\",\n             \"city_name\" => \"Moscow\",\n          \"country_name\" => \"Russia\",\n         \"country_code2\" => \"RU\",\n         \"country_code3\" => \"RU\",\n           \"region_name\" => \"Moscow\",\n              \"location\" => {\n            \"lon\" => 37.6184,\n            \"lat\" => 55.7485\n        },\n           \"postal_code\" => \"101194\",\n           \"region_code\" => \"MOW\",\n             \"longitude\" => 37.6184\n    },\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Settings in Java Codec\nDESCRIPTION: Definition of plugin configuration settings and the configSchema method that exposes them to Logstash. This example defines a 'delimiter' setting with a default value of a comma.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_4\n\nLANGUAGE: java\nCODE:\n```\npublic static final PluginConfigSpec<String> DELIMITER_CONFIG =\n        PluginConfigSpec.stringSetting(\"delimiter\", \",\");\n\n@Override\npublic Collection<PluginConfigSpec<?>> configSchema() {\n    return Collections.singletonList(DELIMITER_CONFIG);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring File System Paths in Logstash Plugins - JavaScript\nDESCRIPTION: Provides an example setting a file system path string for plugin options. The 'my_path' parameter is set to '/tmp/logstash', illustrating how to specify valid OS paths within configurations.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_11\n\nLANGUAGE: JavaScript\nCODE:\n```\nmy_path => \"/tmp/logstash\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Constructor and Initialization for Java Codec Plugin\nDESCRIPTION: Constructor implementation for a Java codec plugin that handles configuration retrieval and initialization. Includes both the required public constructor and a private helper constructor for internal use.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nprivate final String id;\nprivate final String delimiter;\n\npublic JavaCodecExample(final Configuration config, final Context context) {\n    this(config.get(DELIMITER_CONFIG));\n}\n\nprivate JavaCodecExample(String delimiter) {\n    this.id = UUID.randomUUID().toString();\n    this.delimiter = delimiter;\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Downstream Logstash with Beats Input - Shell/Logstash\nDESCRIPTION: Starts the downstream Logstash instance to listen for events on port 5000 using the Beats input plugin. SSL encryption is enabled with 'lumberjack.cert' as the certificate and 'lumberjack.key' as the private key. Requires Logstash and the beats input plugin. Expects events in JSON format on the specified port with valid SSL credentials.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-lumberjack.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -e 'input { beats { codec => json port => 5000 ssl_enabled => true ssl_certificate => \"lumberjack.cert\" ssl_key => \"lumberjack.key\"} }'\n```\n\n----------------------------------------\n\nTITLE: Setting Up Beats Input Plugin in Logstash Input Section (JSON)\nDESCRIPTION: This code block configures Logstash to use the Beats input plugin, specifying port 5044 for inbound connections from Filebeat or other Beats. The 'beats' input requires the presence of the plugin (installed by default) and listens for events on the defined port, matching the Filebeat 'output.logstash' configuration. No additional options are set, reflecting a basic setup for local development.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n    beats {\n        port => \"5044\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Installing Logstash as a Service with NSSM\nDESCRIPTION: These commands navigate to the Logstash directory and then run NSSM to install Logstash as a Windows service.  NSSM is used to wrap logstash.bat so it can be managed as a service. The command opens the NSSM GUI for configuring service parameters like the path to `logstash.bat`, the startup directory, and arguments for Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash-windows.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nPS C:\\Windows\\system32> cd C:\\logstash-9.0.0\\\nPS C:\\logstash-9.0.0> .\\bin\\nssm.exe install logstash\n```\n\n----------------------------------------\n\nTITLE: Starting Logstash using systemd on Linux\nDESCRIPTION: Command to start the Logstash service using systemd. This works on distributions that use systemd as their init system, including Ubuntu 15.10+, Debian Jessie, and SUSE derivatives. Logstash places its systemd unit files in /etc/systemd/system directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nsudo systemctl start logstash.service\n```\n\n----------------------------------------\n\nTITLE: Declaring a Java Input Plugin Class for Logstash in Java\nDESCRIPTION: Defines the main class of a Java input plugin annotated with @LogstashPlugin specifying the plugin name. The class implements the co.elastic.logstash.api.Input interface which establishes the contract for Logstash input plugins. This snippet serves as the foundation for the plugin, defining its identity and interface compliance for the Logstash pipeline.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n@LogstashPlugin(name=\"java_input_example\")\npublic class JavaInputExample implements Input {\n```\n\n----------------------------------------\n\nTITLE: Defining Plugin Configuration Settings Using PluginConfigSpec in Java\nDESCRIPTION: Demonstrates how to declare plugin configuration settings using PluginConfigSpec, specifying each setting's name, type, and default value. The configSchema method returns the list of those configuration specs for Logstash to recognize valid configuration parameters. This setup provides flexibility for users to customize plugin behavior via the 'count' and 'prefix' settings with defaults 3 and \"message\" respectively.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_4\n\nLANGUAGE: java\nCODE:\n```\npublic static final PluginConfigSpec<Long> EVENT_COUNT_CONFIG =\n        PluginConfigSpec.numSetting(\"count\", 3);\n\npublic static final PluginConfigSpec<String> PREFIX_CONFIG =\n        PluginConfigSpec.stringSetting(\"prefix\", \"message\");\n\n@Override\npublic Collection<PluginConfigSpec<?>> configSchema() {\n    return Arrays.asList(EVENT_COUNT_CONFIG, PREFIX_CONFIG);\n}\n```\n\n----------------------------------------\n\nTITLE: Setting HOME Environment Variable (Shell)\nDESCRIPTION: This shell command sets the `HOME` environment variable to the path `/path`.  This is used as a prerequisite for the ruby code snippet that utilizes this environment variable.  This value is then available during the configuration processing in Logstash, allowing the file path to be set dynamically.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/environment-variables.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nexport HOME=\"/path\"\n```\n\n----------------------------------------\n\nTITLE: Creating Input File - Apache Logs\nDESCRIPTION: This snippet shows sample log entries to be used as input for processing by the Logstash configuration in the previous example. These are Apache log entries in a combined format.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n71.141.244.242 - kurt [18/May/2011:01:48:10 -0700] \"GET /admin HTTP/1.1\" 301 566 \"-\" \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3\"\n134.39.72.245 - - [18/May/2011:12:40:18 -0700] \"GET /favicon.ico HTTP/1.1\" 200 1189 \"-\" \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; InfoPath.2; .NET4.0C; .NET4.0E)\"\n98.83.179.51 - - [18/May/2011:19:35:08 -0700] \"GET /css/main.css HTTP/1.1\" 200 1837 \"http://www.safesand.com/information.htm\" \"Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\"\n```\n\n----------------------------------------\n\nTITLE: Setting ECS Compatibility Defaults for Logstash Pipelines (yaml)\nDESCRIPTION: This YAML snippet configures default ECS compatibility modes for entire Logstash pipelines, specifying them in the pipelines configuration file. Each pipeline block sets the 'pipeline.ecs_compatibility' parameter, such as 'disabled' for legacy pipelines and 'v8' for ECS-compliant pipelines. This structure is placed within 'config/pipelines.yml' and applies the specified ECS mode to all plugins in a given pipeline unless overridden at the plugin level.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ecs-ls.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- pipeline.id: my-legacy-pipeline\n  path.config: \"/etc/path/to/legacy-pipeline.config\"\n  pipeline.ecs_compatibility: disabled\n- pipeline.id: my-ecs-pipeline\n  path.config: \"/etc/path/to/ecs-pipeline.config\"\n  pipeline.ecs_compatibility: v8\n```\n\n----------------------------------------\n\nTITLE: Logstash Startup Logs Example\nDESCRIPTION: This text output shows example startup logs from a Logstash Docker container. It shows the process of initializing inputs, starting the pipeline, and starting the API endpoint. The logs indicate the Beats input plugin and the 'main' pipeline being started with specified parameters.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/docker-config.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nSending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties.\n[2016-10-26T05:11:34,992][INFO ][logstash.inputs.beats    ] Beats inputs: Starting input listener {:address=>\"0.0.0.0:5044\"}\n[2016-10-26T05:11:35,068][INFO ][logstash.pipeline        ] Starting pipeline {\"id\"=>\"main\", \"pipeline.workers\"=>4, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>5, \"pipeline.max_inflight\"=>500}\n[2016-10-26T05:11:35,078][INFO ][org.logstash.beats.Server] Starting server on port: 5044\n[2016-10-26T05:11:35,078][INFO ][logstash.pipeline        ] Pipeline main started\n[2016-10-26T05:11:35,105][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}\n```\n\n----------------------------------------\n\nTITLE: Configuring Paquet Task in Rakefile (Ruby)\nDESCRIPTION: This Rakefile configuration demonstrates how to set up and use the Paquet::Task. It defines a target directory for storing bundled dependencies and uses the `pack` method to specify gems to include (e.g., 'manticore', 'logstash-output-elasticsearch') and the `ignore` method for gems that should be excluded, assuming they are provided by the target system (e.g., 'logstash-core').\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/paquet/README.md#_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"paquet\"\n\nTARGET_DIRECTORY = File.join(File.dirname(__FILE__), \"dependencies\")\n\nPaquet::Task.new(TARGET_DIRECTORY) do\n  pack \"manticore\"\n  pack \"launchy\"\n  pack \"gemoji\"\n  pack \"logstash-output-elasticsearch\"\n\n  # Everything not defined here will be assumed to be provided\n  # by the target installation\n  ignore \"logstash-core-plugin-api\"\n  ignore \"logstash-core\"\nend\n```\n\n----------------------------------------\n\nTITLE: Example Response from Creating Elasticsearch API Key - Console Output\nDESCRIPTION: This snippet shows the expected JSON response returned by Elasticsearch after successfully creating an API key. It contains a unique 'id' for the API key, the 'name' given during creation, and the generated 'api_key' string. These values are required to configure client tools like Logstash for authentication.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_20\n\nLANGUAGE: Console\nCODE:\n```\n{\n  \"id\":\"TiNAGG4BaaMdaH1tRfuU\", <1>\n  \"name\":\"logstash_host001\",\n  \"api_key\":\"KnR6yE41RrSowb0kQ0HWoA\" <2>\n}\n```\n\n----------------------------------------\n\nTITLE: JVM Option Version Range Start Example\nDESCRIPTION: Shows how to define a JVM option that applies to JVM versions greater than or equal to a specific version.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/jvm-settings.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n8-:-Xmx2g\n```\n\n----------------------------------------\n\nTITLE: Installing Java plugin in Logstash using logstash-plugin command\nDESCRIPTION: Shell command to install a packaged Java plugin (as a Ruby gem) in Logstash using the logstash-plugin utility with the --no-verify and --local flags.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin install --no-verify --local /path/to/javaPlugin.gem\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with a Custom Java Codec Plugin\nDESCRIPTION: Shell command to run Logstash, piping input data (`echo \"foo,bar\"`) to it. The `-e` flag provides an inline configuration that uses the `java_stdin` input with the custom `java_codec_example` codec to process the input.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\necho \"foo,bar\" | bin/logstash -e 'input { java_stdin { codec => java_codec_example } }'\n```\n\n----------------------------------------\n\nTITLE: Checking Package Installation Status on Debian - Ruby\nDESCRIPTION: Defines a Ruby method to determine if a package is installed on a Debian system using dpkg and sudo via a helper function. The installed? method executes a shell command to retrieve package information and checks the output for confirmation of installation status. Dependencies include the existence of a sudo_exec! helper and correct permissions. The method expects a package name and returns true if the package is present, otherwise false.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/README.md#_snippet_1\n\nLANGUAGE: Ruby\nCODE:\n```\ndef installed?(package)\n  stdout = \"\"\n  cmd = sudo_exec!(\"dpkg -s #{package}\")\n  stdout = cmd.stdout\n  stdout.match(/^Package: #{package}$/)\n  stdout.match(/^Status: install ok installed$/)\nend\n```\n\n----------------------------------------\n\nTITLE: Installing a Logstash plugin from a gem file\nDESCRIPTION: Installs a Logstash plugin directly from a built gem file into the Logstash environment, enabling the use of custom or local plugins. Specify the correct path to your plugin package to complete installation.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/input-plugin/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin install /your/local/plugin/logstash-filter-awesome.gem\n```\n\n----------------------------------------\n\nTITLE: Example Elasticsearch Create API Key Response for Monitoring (JSON)\nDESCRIPTION: Illustrates the JSON response structure received after creating an API key specifically for Logstash monitoring purposes. Contains the `id` and `api_key` needed for configuration.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\":\"TiNAGG4BaaMdaH1tRfuU\", \n  \"name\":\"logstash_host001\",\n  \"api_key\":\"KnR6yE41RrSowb0kQ0HWoA\" \n}\n```\n\n----------------------------------------\n\nTITLE: Enabling the Logstash-xpack Module in Metricbeat via Shell\nDESCRIPTION: This snippet demonstrates commands to enable the 'logstash-xpack' module in Metricbeat, which is essential for collecting Logstash monitoring metrics. Different commands are provided based on the operating system or installation method, including Debian/RPM based Linux, generic Linux/macOS, and Windows PowerShell. The main prerequisite is having Metricbeat installed on the same server as Logstash. Enabling the module activates the default monitoring configuration for Logstash metrics collection.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-with-metricbeat.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nmetricbeat modules enable logstash-xpack\n```\n\nLANGUAGE: sh\nCODE:\n```\n./metricbeat modules enable logstash-xpack\n```\n\nLANGUAGE: powershell\nCODE:\n```\nPS > .\\metricbeat.exe modules enable logstash-xpack\n```\n\n----------------------------------------\n\nTITLE: Preparing Offline Packs for Specific Plugins - Logstash - Shell\nDESCRIPTION: These sample shell commands demonstrate packaging different sets of Logstash plugins using the 'prepare-offline-pack' subcommand. They show packaging a single plugin, using wildcards to include multiple plugins by type, or packing several explicitly listed plugins, along with their dependencies. Users must provide plugin names as arguments, and a staging Logstash installation with Internet access is required for dependency resolution.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/offline-plugins.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin prepare-offline-pack logstash-input-beats\n```\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin prepare-offline-pack logstash-filter-*\n```\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin prepare-offline-pack logstash-filter-* logstash-input-beats\n```\n\n----------------------------------------\n\nTITLE: Using Ruby Filter to Set Field (Ruby)\nDESCRIPTION: This code snippet demonstrates how to use the `ruby` filter in Logstash to manipulate event data using the `event.get()` and `event.set()` methods. It retrieves the value of the `message` field, converts it to lowercase, and assigns it to a new field called `lowercase_field`.  This requires Logstash's `ruby` filter configured to execute the embedded ruby code.  The `code` parameter specifies the Ruby code to execute. No other dependencies are required.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/event-api.md#_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\nfilter {\n  ruby {\n    code => 'event.set(\"lowercase_field\", event.get(\"message\").downcase)'\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Unix Tree Output for Logstash Plugin File Structure (txt)\nDESCRIPTION: This text-based output illustrates the recommended directory structure for a custom Logstash filter plugin. It highlights core files (Gemfile, gemspec, README, etc.), code directories (lib, spec), and the expected file organization to comply with Ruby gem conventions. It is used for documentation and onboarding.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_6\n\nLANGUAGE: txt\nCODE:\n```\n$ tree logstash-filter-mypluginname\n├── Gemfile\n├── LICENSE\n├── README.md\n├── Rakefile\n├── lib\n│   └── logstash\n│       └── filters\n│           └── mypluginname.rb\n├── logstash-filter-mypluginname.gemspec\n└── spec\n    └── filters\n        └── mypluginname_spec.rb\n\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Logstash (sh)\nDESCRIPTION: Downloads the specified Logstash tarball from the official Elastic website, extracts its contents into a local directory, and then changes the current shell directory into the newly created Logstash installation folder. This is a prerequisite for testing custom plugin installations within a clean Logstash environment and requires `curl` and `tar` utilities.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\ncurl -O https://download.elastic.co/logstash/logstash/logstash-9.0.0.tar.gz\ntar xzvf logstash-9.0.0.tar.gz\ncd logstash-9.0.0\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Logstash Monitoring Collection in YAML Configuration\nDESCRIPTION: YAML configuration to disable the default collection of Logstash monitoring metrics in logstash.yml. This is required before setting up monitoring with Elastic Agent.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/dashboard-monitoring-with-elastic-agent.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmonitoring.enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Client TLS Authentication with Certificate and Key Files in YAML\nDESCRIPTION: This configuration permits Logstash to authenticate with Elasticsearch by providing separate X.509 certificate and private key files directly. The 'xpack.monitoring.elasticsearch.ssl.certificate' is the client certificate, and 'xpack.monitoring.elasticsearch.ssl.key' is its RSA private key; both files must be present and in the appropriate OpenSSL format. Needed for systems that do not use Java keystores.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-internal-collection-legacy.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nxpack.monitoring.elasticsearch.ssl.certificate: /path/to/certificate\nxpack.monitoring.elasticsearch.ssl.key: /path/to/key\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash File Output Plugin (JSON syntax)\nDESCRIPTION: Enables Logstash to write processed data to a specified file path. Dependencies include the 'file' output plugin. The 'path' parameter sets the destination file, which should be replaced with the actual target filename or directory. Useful for archiving or further offline analysis.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/multiple-input-output-plugins.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\nfile {\n    path => \"/path/to/target/file\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Logstash on Unix-like Systems\nDESCRIPTION: Basic command syntax for running Logstash from the command line on Unix-like systems such as Linux or macOS, with options placeholder for command-line flags.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash-command-line.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash [options]\n```\n\n----------------------------------------\n\nTITLE: Example Logstash Output Using the Java Codec Plugin\nDESCRIPTION: Shows the expected textual output format from Logstash when processing the input \"foo,bar\" using the example Java codec configured with a comma delimiter. Each part (\"foo\", \"bar\") becomes the 'message' field in a separate Logstash event, along with standard metadata like `@version`, `@timestamp`, and `host`.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-codec-plugin.md#_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n{\n      \"@version\" => \"1\",\n       \"message\" => \"foo\",\n    \"@timestamp\" => yyyy-MM-ddThh:mm:ss.SSSZ,\n          \"host\" => \"<yourHostName>\"\n}\n{\n      \"@version\" => \"1\",\n       \"message\" => \"bar\\n\",\n    \"@timestamp\" => yyyy-MM-ddThh:mm:ss.SSSZ,\n          \"host\" => \"<yourHostName>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Web Server Log Line in Shell\nDESCRIPTION: A representative line from a web server log showing the typical Apache combined log format, including client IP, timestamp, HTTP request details, response code, and user agent information.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-search.png\nHTTP/1.1\" 200 203023 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel\nMac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"\n```\n\n----------------------------------------\n\nTITLE: Requiring Logstash Output ZeroMQ and RSpec Helpers in Ruby\nDESCRIPTION: This snippet shows the required dependencies to include at the top of the zeromq_spec.rb test file for the Logstash ZeroMQ output plugin. 'logstash/outputs/zeromq' loads the plugin under test, while 'logstash/devutils/rspec/spec_helper' brings in RSpec utilities specific to Logstash plugin development. Both are required before defining tests. No inputs or outputs are involved; these are just Ruby require statements.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/contributing-patch-plugin.md#_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"logstash/outputs/zeromq\"\nrequire \"logstash/devutils/rspec/spec_helper\"\n```\n\n----------------------------------------\n\nTITLE: Defining External File Dependencies in `vendor.json` (JSON)\nDESCRIPTION: Example `vendor.json` file content demonstrating how to specify external file dependencies for a Logstash plugin. It includes the SHA1 hash for integrity checking, the URL to download the dependency archive, and an optional list of specific files to extract from the archive. This configuration is used by the `rake vendor` task.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_7\n\nLANGUAGE: JSON\nCODE:\n```\n[{\n        \"sha1\": \"a90fe6cc53b76b7bdd56dc57950d90787cb9c96e\",\n        \"url\": \"http://collectd.org/files/collectd-5.4.0.tar.gz\",\n        \"files\": [ \"/src/types.db\" ]\n}]\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Output in Metricbeat YAML for Monitoring Cluster\nDESCRIPTION: This YAML snippet demonstrates how to configure the Elasticsearch output in Metricbeat to direct monitoring data to one or more remote Elasticsearch nodes representing a dedicated monitoring cluster. It specifies an array of host URLs as the connection endpoints. Optional comments highlight how to enable HTTPS and authentication with username and password. This configuration ensures monitoring data is securely and reliably stored on a separate cluster to prevent impact on production clusters, requiring that the cluster supports ingest pipelines and relevant security roles for Metricbeat user credentials.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-with-metricbeat.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\noutput.elasticsearch:\n  # Array of hosts to connect to.\n  hosts: [\"http://es-mon-1:9200\", \"http://es-mon2:9200\"] <1>\n\n  # Optional protocol and basic auth credentials.\n  #protocol: \"https\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Multiline Codec for Timestamped Logs (JSON)\nDESCRIPTION: Provides a Logstash configuration for the `file` input plugin using the `multiline` codec to handle logs where events start with a timestamp. It uses `negate => true` with the `TIMESTAMP_ISO8601` grok pattern (`^%{TIMESTAMP_ISO8601} `) to identify lines *not* starting with a timestamp and merges them with the preceding line using `what => \"previous\"`.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/multiline.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\ninput {\n  file {\n    path => \"/var/log/someapp.log\"\n    codec => multiline {\n      pattern => \"^%{TIMESTAMP_ISO8601} \"\n      negate => true\n      what => previous\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing an Offline Plugin Pack with Logstash Plugin Manager - Shell\nDESCRIPTION: These shell commands enable installation of an offline plugin pack file using the Logstash plugin manager. The 'install' subcommand is invoked with a file URI ('file:///') pointing to the downloaded or transferred plugin pack (.zip). The full forward-slash path is required, and the command must be run on the system where the plugins are to be deployed. No network connectivity is needed during installation.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/offline-plugins.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin install file:///c:/path/to/logstash-offline-plugins-9.0.0.zip\n```\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin install file:///path/to/logstash-offline-plugins-9.0.0.zip\n```\n\n----------------------------------------\n\nTITLE: Using Drip as Java Command for Logstash RSpec Testing - Shell\nDESCRIPTION: This shell command sets the JAVACMD environment variable to use Drip, a JVM launcher, as the Java command when running Logstash's RSpec tests. It improves test startup speed after the first run. Dependencies: Drip must be installed and available in PATH. Limitation: Drip does not support STDIN plugins.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nJAVACMD=`which drip` bin/rspec\n```\n\n----------------------------------------\n\nTITLE: Formatting Changelog Entries Using Markdown\nDESCRIPTION: Demonstrates the recommended markdown format for Logstash plugin changelogs, including version headers, tagged change descriptions, multi-line descriptions, and proper spacing. This format ensures changelogs are easily readable and informative for users. No dependencies beyond markdown rendering are needed.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/community-maintainer.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## 1.0.x                              <1>\\n - change description                 <2>\\n - tag: change description            <3>\\n - tag1,tag2: change description      <4>\\n - tag: Multi-line description        <5>\\n   must be indented and can use\\n   additional markdown syntax\\n                                      <6>\\n## 1.0.0                              <7>\\n[...]\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Dependencies in `Gemfile` (Ruby)\nDESCRIPTION: Example `Gemfile` for a Logstash plugin. It specifies the RubyGems source, includes configurations from the `.gemspec` file, and declares a dependency on the main Logstash gem, sourced directly from the elastic/logstash GitHub repository's master branch for development and testing purposes. This file is used by Bundler to manage gem dependencies.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_8\n\nLANGUAGE: Ruby\nCODE:\n```\nsource 'https://rubygems.org'\ngemspec\ngem \"logstash\", :github => \"elastic/logstash\", :branch => \"master\"\n```\n\n----------------------------------------\n\nTITLE: Declaring Runtime and Development Dependencies in Gemspec for Logstash Plugins - ruby\nDESCRIPTION: This Ruby snippet illustrates how to declare both runtime and development gem dependencies within a gemspec using add_runtime_dependency and add_development_dependency methods. Here, the plugin mandates logstash-core-plugin-api as a runtime dependency restricted by a version range and 'logstash-devutils' for development and testing. Place this section within your plugin's gemspec and adjust dependency names and versions as required for your project.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_9\n\nLANGUAGE: ruby\nCODE:\n```\n  # Gem dependencies\n  s.add_runtime_dependency \"logstash-core-plugin-api\", \">= 1.60\", \"<= 2.99\"\n  s.add_development_dependency 'logstash-devutils'\n```\n\n----------------------------------------\n\nTITLE: Building a Logstash Plugin Gem (Shell)\nDESCRIPTION: Packages the plugin source code and metadata into a Ruby gem file using the `gem build` command and the plugin's `.gemspec` file. This creates a distributable archive of the plugin. Requires a valid gemspec file in the plugin's root directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/filter-plugin/README.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ngem build logstash-filter-awesome.gemspec\n```\n\n----------------------------------------\n\nTITLE: Managing Plugin Stop Lifecycle with Stop and awaitStop Methods in Java\nDESCRIPTION: This snippet defines lifecycle control using a volatile boolean flag and a CountDownLatch. The stop method sets the flag and counts down the latch to signal stopping. The awaitStop method blocks until the latch is released, enabling cooperative and asynchronous plugin shutdown compliant with the Logstash output plugin lifecycle API.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nprivate final CountDownLatch done = new CountDownLatch(1);\nprivate volatile boolean stopped;\n\n@Override\npublic void stop() {\n    stopped = true;\n    done.countDown();\n}\n\n@Override\npublic void awaitStop() throws InterruptedException {\n    done.await();\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Keystore Keys\nDESCRIPTION: This shell script runs the `list` command to display all keys currently stored within the Logstash keystore. It does not require any parameters.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/keystore.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-keystore list\n```\n\n----------------------------------------\n\nTITLE: Running pqrepair Utility in Logstash\nDESCRIPTION: Command for running the pqrepair utility to remove corrupted queue segments and make the queue operational again, although some data may be lost.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/persistent-queues.md#_snippet_6\n\nLANGUAGE: txt\nCODE:\n```\nbin/pqrepair <queue_directory>\n```\n\n----------------------------------------\n\nTITLE: Installing Built Plugin Gem into Logstash (Shell)\nDESCRIPTION: Installs the packaged plugin gem file into a Logstash instance using the `logstash-plugin install` command. This is an alternative method to using `:path` for testing the plugin in a production-like environment. Replace the path with the location of your built gem.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/codec-plugin/README.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin install /your/local/plugin/logstash-codec-awesome.gem\n```\n\n----------------------------------------\n\nTITLE: Testing Logstash Pipeline Configuration for Errors (Shell)\nDESCRIPTION: This Shell command runs Logstash in configuration test mode for the pipeline definition specified in 'first-pipeline.conf'. The '--config.test_and_exit' flag causes Logstash to validate the configuration syntax and report any parsing errors without running the pipeline. It requires a correctly installed Logstash binary and access to the configuration file.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -f first-pipeline.conf --config.test_and_exit\n```\n\n----------------------------------------\n\nTITLE: Implementing Output Method to Process and Print Events in Java\nDESCRIPTION: This output method iterates over a collection of Logstash events and prints each event prefixed by the configured string to the specified PrintStream. It checks a volatile stop flag to allow cooperative interruption. This method demonstrates local event output to the console and serves as the core mechanism for handling output processing in the plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic void output(final Collection<Event> events) {\n    Iterator<Event> z = events.iterator();\n    while (z.hasNext() && !stopped) {\n        String s = prefix + z.next();\n        printer.println(s);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Input and Output for Elasticsearch\nDESCRIPTION: This snippet defines a Logstash pipeline configuration that reads logs from a source and forwards them to Elasticsearch. It sets input parameters, filters for data transformation, and specifies the Elasticsearch host for output. Dependencies include Logstash's core configuration files and the Elasticsearch output plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/concurrent-ruby-NOTICE.txt#_snippet_0\n\nLANGUAGE: Logstash Configuration (DSL)\nCODE:\n```\ninput {\n  beat {\n    port => 5044\n  }\n}\n\nfilter {\n  # Add filters here\n}\n\noutput {\n  elasticsearch {\n    hosts => [\"http://localhost:9200\"]\n    index => \"logs-%{+YYYY.MM.dd}\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning Logstash Repository Using Git in Shell\nDESCRIPTION: Provides the shell command to clone a specific Logstash Git branch to a target folder, necessary for obtaining the local Logstash codebase that contains the Java plugin API. The command requires specifying the branch name corresponding to the desired Logstash version, and optionally the target folder. This enables developers to work with the correct API revision.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --branch <branch_name> --single-branch https://github.com/elastic/logstash.git <target_folder>\n```\n\n----------------------------------------\n\nTITLE: Cloning Logstash Repository using Git (Shell)\nDESCRIPTION: Clones a specific branch of the Logstash repository from GitHub into a target folder. This step is necessary to obtain the Logstash Java Filters API required for plugin development. Replace `<branch_name>` with the desired Logstash version (e.g., `7.2` or later for GA API) and `<target_folder>` with the local destination directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-filter-plugin.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --branch <branch_name> --single-branch https://github.com/elastic/logstash.git <target_folder>\n```\n\n----------------------------------------\n\nTITLE: Running All Acceptance Tests - Shell\nDESCRIPTION: Uses a shell command to execute the entire suite of acceptance tests for Logstash by invoking a Rake task. This command depends on the QA framework and previous steps of artifact building or initialization. No parameters required; outputs test results to the terminal. Should be run from the project's root or appropriate directory after dependencies have been installed.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nrake qa:acceptance:all\n```\n\n----------------------------------------\n\nTITLE: Running Filtered RSpec Acceptance Tests - Shell\nDESCRIPTION: Shows how to run a subset of acceptance tests using the RSpec command line interface filtered by an example description. This enables selective test execution by matching example strings and is run inside the acceptance/spec directory with bundle. Requires all dependencies installed via bundle; outputs detailed test results for matching examples.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nbundle exec rspec acceptance/spec -e \"is installed\"\n```\n\n----------------------------------------\n\nTITLE: Cloning Logstash Codebase using Git in Shell\nDESCRIPTION: This shell command clones the Logstash GitHub repository for a specified branch into a target folder. It is used to obtain a local copy of the Logstash codebase, which includes the Java plugin API needed for plugin development. The 'branch_name' parameter corresponds to the Logstash version, and 'target_folder' specifies where to clone the repository.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --branch <branch_name> --single-branch https://github.com/elastic/logstash.git <target_folder>\n```\n\n----------------------------------------\n\nTITLE: Running a Single Ruby Test File via Gradle - Shell\nDESCRIPTION: Runs a specific Ruby test using environment variables and the Gradle rubyTests task, targeting a file in the Logstash core. Depends on proper environment variable setup, Gradle, and RSpec test infrastructure. Input: SPEC_OPTS to specify the test file. Output: test result for the specified spec.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nSPEC_OPTS=\"-fd -P logstash-core/spec/logstash/api/commands/default_metadata_spec.rb\" ./gradlew :logstash-core:rubyTests --tests org.logstash.RSpecTests\n```\n\n----------------------------------------\n\nTITLE: Parsing Dates with Logstash Date Filter in JSON\nDESCRIPTION: Configures the Logstash date filter to parse a field named 'logdate' using the specified date pattern 'MMM dd yyyy HH:mm:ss' to set the event timestamp. This allows Logstash to interpret and convert string dates into accurate Logstash event timestamps. No additional dependencies beyond Logstash core plugins are required.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/core-operations.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\nfilter {\n  date {\n    match => [ \"logdate\", \"MMM dd yyyy HH:mm:ss\" ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting config_name for Logstash Plugin Registration (Ruby)\nDESCRIPTION: The config_name line registers the plugin's name within Logstash, associating it with its corresponding configuration section. This string should match how the filter block is invoked in a Logstash config file. It is mandatory for proper plugin recognition.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_3\n\nLANGUAGE: ruby\nCODE:\n```\n  config_name \"example\"\n```\n\n----------------------------------------\n\nTITLE: Building Plugin Gem File (Shell)\nDESCRIPTION: Command to build a distributable `.gem` file for the plugin. This file is generated from the plugin's `.gemspec` file and is used for installation in a standard Logstash environment.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/output-plugin/README.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\ngem build logstash-filter-awesome.gemspec\n```\n\n----------------------------------------\n\nTITLE: Disabling the System Module in Metricbeat via Shell\nDESCRIPTION: This shell command disables the default 'system' module in Metricbeat, which collects system-level metrics. Since system module data does not appear on the Stack Monitoring page in Kibana, disabling it avoids unnecessary metric collection and resource use unless system data is required for other purposes. The prerequisite is having Metricbeat installed and accessible. Usage is optional based on monitoring needs.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-with-metricbeat.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nmetricbeat modules disable system\n```\n\n----------------------------------------\n\nTITLE: Building a Logstash Plugin (Gem)\nDESCRIPTION: This snippet demonstrates how to build a Logstash plugin into a Ruby gem using the `gem build` command. It requires a `*.gemspec` file in the plugin's root directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\ngem build logstash-filter-example.gemspec\n```\n\n----------------------------------------\n\nTITLE: Trusting Elasticsearch CA via SHA256 Fingerprint in YAML\nDESCRIPTION: This YAML configuration allows Logstash to trust a certificate authority by specifying the SHA256 fingerprint of the CA, eliminating the need to have the CA certificate file on disk. The value must be a valid hex-encoded SHA256 fingerprint from the DER-formatted CA. Use this method when direct file access to the CA is impractical.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-internal-collection-legacy.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nxpack.monitoring.elasticsearch.ssl.ca_trusted_fingerprint: 2cfe62e474fb381cc7773c84044c28c9785ac5d1940325f942a3d736508de640\n```\n\n----------------------------------------\n\nTITLE: Debugging Logstash with a Remote JVM Debugger - Shell\nDESCRIPTION: Sets JVM startup options via the LS_JAVA_OPTS environment variable to enable remote debugging on port 5005. Useful when diagnosing test hangs by connecting an external debugger. Input: none directly. Output: JVM starts in debug mode, listening for connections.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nLS_JAVA_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\"\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with Example Plugin Configuration - Shell\nDESCRIPTION: Runs Logstash interactively with a minimal configuration provided via the -e flag, specifying the 'example' input plugin and the stdout output with rubydebug codec. Requires previously installed plugin and access to bin/logstash. Demonstrates how to test plugin functionality from the command line and modify runtime parameters such as message and interval.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -e 'input { example{} } output {stdout { codec => rubydebug }}'\n```\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -e 'input { example{ message => \"A different message\" interval => 5 } } output {stdout { codec => rubydebug }}'\n```\n\n----------------------------------------\n\nTITLE: Defining and Returning Plugin Configuration Settings in Java\nDESCRIPTION: This snippet defines a plugin configuration specification for a string parameter named 'prefix' with a default empty string value. The configSchema method returns the list of supported plugin settings to enable Logstash validation and configuration of the plugin. This pattern facilitates declarative specification of configurable parameters.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npublic static final PluginConfigSpec<String> PREFIX_CONFIG =\n        PluginConfigSpec.stringSetting(\"prefix\", \"\");\n\n@Override\npublic Collection<PluginConfigSpec<?>> configSchema() {\n    return Collections.singletonList(PREFIX_CONFIG);\n}\n```\n\n----------------------------------------\n\nTITLE: Updating APT and Installing Logstash (shell)\nDESCRIPTION: This shell command updates the APT package cache and installs the logstash package from the configured repositories. It must be run with sufficient privileges (sudo). Before running, ensure the Elastic APT repository and GPG key have been properly configured. The output is a working installation of Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/installing-logstash.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get update && sudo apt-get install logstash\n```\n\n----------------------------------------\n\nTITLE: Installing Logstash Plugin Gem Locally (sh)\nDESCRIPTION: Uses the Logstash plugin management tool found in the `bin` directory of the Logstash installation to install a plugin directly from a local `.gem` file. This command is used to test the installation of a custom-built plugin before publishing it. It requires the full path to the generated gem file, including its version.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin install /my/logstash/plugins/logstash-output-example/logstash-output-example-0.1.0.gem\n```\n\n----------------------------------------\n\nTITLE: Installing a Logstash Plugin from RubyGems (Logstash Plugin CLI)\nDESCRIPTION: This snippet installs a Logstash plugin from RubyGems.org using the `bin/logstash-plugin install` command.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_27\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin install logstash-filter-mypluginname\n```\n\n----------------------------------------\n\nTITLE: Installing a Built Plugin Gem into Logstash (Shell)\nDESCRIPTION: Installs a Logstash plugin from a locally built `.gem` file using the Logstash plugin manager. This makes the packaged plugin available to the Logstash instance. Execute from the Logstash installation directory, providing the path to the generated gem file.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/filter-plugin/README.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin install /your/local/plugin/logstash-filter-awesome.gem\n```\n\n----------------------------------------\n\nTITLE: Default Elasticsearch Output Configuration\nDESCRIPTION: This YAML snippet shows the default configuration for the Elasticsearch output used by the legacy Logstash monitoring pipeline when no explicit output settings are defined in `logstash.yml`. It specifies that monitoring data should be sent to an Elasticsearch instance running on `http://localhost:9200`. This configuration is internal to the monitoring feature and separate from any standard pipeline outputs.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-internal-collection-legacy.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nxpack.monitoring.elasticsearch.hosts: [ \"http://localhost:9200\" ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Multiline Codec for Line Continuations (JSON)\nDESCRIPTION: Shows the Logstash configuration using the `multiline` codec to combine lines ending with a backslash (`\\`) character, common in C-style line continuations. It uses the `pattern` `\\\\$` to match a literal backslash at the end of the line and `what => \"next\"` to merge it with the next line.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/multiline.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\ninput {\n  stdin {\n    codec => multiline {\n      pattern => \"\\\\$\"\n      what => \"next\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example of a Stalled Shutdown Due to Slow Filter Execution\nDESCRIPTION: Provides a shell example where Logstash, started with --pipeline.unsafe_shutdown, is interrupted during a slow Ruby filter. Demonstrates how the shutdown process stalls, logs inflight events, and results in data loss if forced to quit, highlighting the importance of safe shutdown practices.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/shutdown.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -e 'input { generator { } } filter { ruby { code => \"sleep 10000\" } } output { stdout { codec => dots } }' -w 1 --pipeline.unsafe_shutdown\nPipeline main started\n^CSIGINT received. Shutting down the agent. {:level=>:warn}\nstopping pipeline {:id=>\"main\", :level=>:warn}\nReceived shutdown signal, but pipeline is still waiting for in-flight events to be processed. Sending another ^C will force quit Logstash, but this may cause\ndata loss. {:level=>:warn}\n{\"inflight_count\"=>125, \"stalling_thread_info\"=>{\"[\"LogStash::Filters::Ruby\", {\"code\"=>\"sleep 10000\"}]\" => [{\"thread_id\"=>19, \"name\"=>\"[main]>worker0\", \"current_call\"=>\"(ruby filter code):1:in `sleep'\"}]}\"} {:level=>:warn}\n{\"inflight_count\"=>125, \"stalling_thread_info\"=>{\"[\"LogStash::Filters::Ruby\", {\"code\"=>\"sleep 10000\"}]\"=>[{\"thread_id\"=>19, \"name\"=>\"[main]>worker0\", \"current_call\"=>\"(ruby filter code):1:in `sleep'\"}]}\"} {:level=>:warn}\n{\"inflight_count\"=>125, \"stalling_thread_info\"=>{\"[\"LogStash::Filters::Ruby\", {\"code\"=>\"sleep 10000\"}]\"=>[{\"thread_id\"=>19, \"name\"=>\"[main]>worker0\", \"current_call\"=>\"(ruby filter code):1:in `sleep'\"}]}\"} {:level=>:warn}\nForcefully quitting logstash.. {:level=>:fatal}\n```\n\n----------------------------------------\n\nTITLE: Building the Logstash Plugin Gem - Shell\nDESCRIPTION: Creates a RubyGems package (.gem file) for the Logstash input plugin using the gemspec file. Requires the gem command and a properly formatted .gemspec file. Upon success, generates the .gem file in the current directory, with the version determined by s.version in the gemspec.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ngem build logstash-input-example.gemspec\n```\n\n----------------------------------------\n\nTITLE: Decoding Fluentd Msgpack Logs Using Fluent Codec in Logstash\nDESCRIPTION: This snippet configures Logstash to decode Fluentd `msgpack` schema logs received over TCP, using the fluent codec. It listens on port 4000 for incoming logs from fluent-logger-ruby, converting them into Logstash events. Dependencies include the fluent codec plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/data-deserialization.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\ninput {\n  tcp {\n    codec => fluent\n    port => 4000\n  }\n}\n...\n```\n\n----------------------------------------\n\nTITLE: Opting Out of Logstash X-Pack Using Environment Variable in Shell\nDESCRIPTION: This shell script snippet demonstrates how to disable Logstash X-Pack features by setting the 'OSS' environment variable to true, which allows running Logstash without Elastic License dependencies. It is useful for users who want OSS-only functionality by exporting 'OSS=true' before starting Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/x-pack/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport OSS=true\n```\n\n----------------------------------------\n\nTITLE: Running RSpec Test by Line Number - Shell\nDESCRIPTION: Executes a single RSpec test defined at a specific line in an acceptance test spec file. The command targets a particular Ruby spec file and line for quick, focused testing, suitable for test development or debugging. Use after dependency installation in the project; outputs result of the specified test case.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nbundle exec rspec acceptance/spec/lib/artifact_operation_spec.rb:11\n```\n\n----------------------------------------\n\nTITLE: Specifying Target Elasticsearch Cluster UUID in Logstash (YAML)\nDESCRIPTION: This optional configuration snippet sets the `monitoring.cluster_uuid` property within the `logstash.yml` file. It explicitly binds the Logstash metrics collected by the agent to the specified Elasticsearch cluster UUID (replace `PRODUCTION_ES_CLUSTER_UUID` with the actual target cluster's UUID), ensuring data is associated with the correct cluster in monitoring.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-with-elastic-agent.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmonitoring.cluster_uuid: PRODUCTION_ES_CLUSTER_UUID\n```\n\n----------------------------------------\n\nTITLE: Modifying Logstash Gemfile for Private Repository\nDESCRIPTION: Illustrates how to change the `source` line in the Logstash Gemfile to use a custom or private gem repository URL (e.g., `https://my.private.repository`) instead of the default public repository. This modification allows plugin management commands to fetch plugins from the specified private source.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/private-rubygem.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# This is a Logstash generated Gemfile.\n# If you modify this file manually all comments and formatting will be lost.\n\nsource \"https://my.private.repository\"\n```\n\n----------------------------------------\n\nTITLE: Running Logstash JMH benchmarks with Gradle\nDESCRIPTION: This command uses Gradle to build and execute the JMH benchmarks for Logstash. It runs from the project root directory and prints the benchmark results to the console.\nSOURCE: https://github.com/elastic/logstash/blob/main/logstash-core/benchmarks/Readme.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew jmh\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Output Plugin Configuration - cloud_id and cloud_auth\nDESCRIPTION: This code snippet shows how to configure the Elasticsearch output plugin in Logstash to send data to Elastic Cloud using the `cloud_id` and `cloud_auth` options. It defines the output block with `cloud_id` and `cloud_auth` parameters, replacing the standard host and authentication settings. No additional SSL configuration is needed.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/connecting-to-cloud.md#_snippet_0\n\nLANGUAGE: Logstash\nCODE:\n```\noutput {elasticsearch { cloud_id => \"<cloud id>\" cloud_auth => \"<cloud auth>\" } }\n```\n\n----------------------------------------\n\nTITLE: Configuring API credentials with curl and shell commands\nDESCRIPTION: This snippet details how to obtain and set up RubyGems API credentials on your local machine using curl, chmod, and configuration files. It highlights the commands to retrieve the API key and secure the credentials file before publishing a plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/publish-plugin.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ncurl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials\nchmod 0600 ~/.gem/credentials\n```\n\n----------------------------------------\n\nTITLE: Setting Tag Environment Variable (Shell)\nDESCRIPTION: This shell command sets the `ENV_TAG` environment variable to the string `\"tag2\"`. This demonstrates how to set up an environment variable for use by Logstash and is a prerequisite for the Ruby code that uses the variable to set a tag. The quotes are important when the value contains spaces or special characters.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/environment-variables.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexport ENV_TAG=\"tag2\"\n```\n\n----------------------------------------\n\nTITLE: Installing Logstash Default Plugins with Rake (Shell)\nDESCRIPTION: Installs the default set of Logstash plugins using the `rake plugin:install-default` command. This is necessary after bootstrapping if you want Logstash to have its standard input, filter, and output capabilities, especially when setting up the environment using Bundler.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\nrake plugin:install-default\n```\n\n----------------------------------------\n\nTITLE: Running Logstash Manually via Command Line\nDESCRIPTION: These commands navigate to the Logstash installation directory and then execute the Logstash batch file (`logstash.bat`) to start Logstash with a specified configuration file (`syslog.conf`). This allows for manual testing and validation of the Logstash configuration.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash-windows.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nPS C:\\Windows\\system32> cd C:\\logstash-9.0.0\\\nPS C:\\logstash-9.0.0> .\\bin\\logstash.bat -f .\\config\\syslog.conf\n```\n\n----------------------------------------\n\nTITLE: Running Plugin Tests with RSpec (Shell)\nDESCRIPTION: Command to execute the automated tests for the plugin using the RSpec framework. `bundle exec` ensures that the command is run within the context of the gems managed by Bundler.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/output-plugin/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nbundle exec rspec\n```\n\n----------------------------------------\n\nTITLE: Simulating Syslog Input to Logstash via Telnet - Ruby\nDESCRIPTION: Demonstrates connecting to Logstash's syslog input port using telnet, allowing for manual injection of syslog lines for testing the pipeline. Requires telnet client installed on the system and the Logstash pipeline to be running and listening on port 5000. No authentication is set up, making this suitable for local testing environments only.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_10\n\nLANGUAGE: ruby\nCODE:\n```\ntelnet localhost 5000\n```\n\n----------------------------------------\n\nTITLE: Starting Event Generation Loop in Logstash Java Input Plugin in Java\nDESCRIPTION: Implements the start method to produce events, using a cooperative loop that generates a finite number of events governed by the 'count' configuration and stops if requested. Events are created as maps with a message field constructed using the configurable prefix and a formatted count string. Events are pushed into the Logstash pipeline using Consumer.accept, demonstrating a push event mechanism within the plugin lifecycle.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-input-plugin.md#_snippet_6\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic void start(Consumer<Map<String, Object>> consumer) {\n    int eventCount = 0;\n    try {\n        while (!stopped && eventCount < count) {\n            eventCount++;\n            consumer.accept.push(Collections.singletonMap(\"message\",\n                    prefix + \" \" + StringUtils.center(eventCount + \" of \" + count, 20)));\n        }\n    } finally {\n        stopped = true;\n        done.countDown();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Obtaining and Securing RubyGems API Key (sh)\nDESCRIPTION: Uses `curl` to authenticate with RubyGems.org using a username and password, retrieves the API key, and saves it to the standard credentials file (`~/.gem/credentials`). It then uses `chmod` to set restrictive file permissions (0600) on the credentials file, ensuring only the file owner can read or write it for security.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\ncurl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials\nchmod 0600 ~/.gem/credentials\n```\n\n----------------------------------------\n\nTITLE: Run Basic Logstash Pipeline - MacOS, Linux\nDESCRIPTION: This command runs a minimal Logstash pipeline with standard input (stdin) and standard output (stdout). It allows quick testing of Logstash setup without needing a config file.  It requires Logstash to be installed and the current directory to be the Logstash installation directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/first-event.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncd logstash-9.0.0\nbin/logstash -e 'input { stdin { } } output { stdout {} }'\n```\n\n----------------------------------------\n\nTITLE: Setting the Configuration Name for a Logstash Output Plugin in Ruby\nDESCRIPTION: This code assigns a unique configuration name to the plugin instance by calling `config_name` within the plugin class. This string is required for users to specify your output plugin in their Logstash pipeline configurations. The config name should match the intended block name in user configuration files, facilitating proper plugin loading.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_3\n\nLANGUAGE: ruby\nCODE:\n```\n  config_name \"example\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Output for Logstash\nDESCRIPTION: This snippet configures the Logstash output plugin to send data to another Logstash instance. The `hosts` option specifies the receiving Logstash host and port pairs. If a port is not specified, port 9800 will be used. This configuration is intended for upstream Logstash instances.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-native.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\noutput {\n    logstash {\n        hosts => [\"10.0.0.123\", \"10.0.1.123:9800\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Establishing Connection to Elasticsearch in Ruby\nDESCRIPTION: This Ruby code snippet demonstrates how to create a client connection to an Elasticsearch server. It initializes a client object with specified host addresses, enabling subsequent operations such as indexing or searching documents. Dependencies include the 'elasticsearch' Ruby gem.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/sequel-NOTICE.txt#_snippet_0\n\nLANGUAGE: Ruby\nCODE:\n```\nrequire 'elasticsearch'\n\nclient = Elasticsearch::Client.new({\n  hosts: ['localhost:9200', 'otherhost:9200'],\n  log: true\n})\n\n# Use client to perform operations like index, search, delete, etc.\n```\n\n----------------------------------------\n\nTITLE: Building Logstash Documentation Using build_docs.pl - Shell\nDESCRIPTION: Executes a Perl script to generate Logstash documentation from AsciiDoc sources, with options for chunking output and opening the result. Requires Perl and related dependencies. Input: documentation file paths. Output: built documentation in designated format.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n./build_docs.pl --doc ../logstash/docs/index.asciidoc --chunk=1 -open\n```\n\n----------------------------------------\n\nTITLE: Executing a Program with the 'offline' Utility (Shell)\nDESCRIPTION: Demonstrates the basic command-line syntax for running a target program under the control of the 'offline' utility. The './offline' command precedes the target program and its arguments, enforcing seccomp rules to block network calls for that specific program execution.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/integration/fixtures/offline_wrapper/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n./offline <program> [args]\n```\n\n----------------------------------------\n\nTITLE: Verifying Java Version for Logstash Setup - Shell\nDESCRIPTION: Runs the command to check the currently installed Java runtime, ensuring compatibility with Logstash requirements. No dependencies beyond Java itself are needed; the 'java' binary must be present and executable in the system PATH. Input: none. Output: Java version and environment details in standard output. This command is needed to confirm Java 17 or 21 is installed prior to Logstash installation.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/getting-started-with-logstash.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\njava -version\n\n```\n\n----------------------------------------\n\nTITLE: Cloning Plugin Repository Using Git - Shell\nDESCRIPTION: Demonstrates how to clone a custom Logstash input plugin repository from GitHub using either HTTPS or SSH. Requires Git to be installed and appropriate permissions set for SSH URLs. Replace GITUSERNAME and MYPLUGINNAME with the actual GitHub username and plugin name. The command downloads the specified repository to a local directory for development.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/GITUSERNAME/logstash-input-MYPLUGINNAME.git\n```\n\nLANGUAGE: shell\nCODE:\n```\ngit clone git@github.com:GITUSERNAME/logstash-input-MYPLUGINNAME.git\n```\n\n----------------------------------------\n\nTITLE: Setting Pipeline Batch (Flat Keys) - Logstash YAML\nDESCRIPTION: Defines the Logstash pipeline's batch processing settings using flat key-value pairs in YAML. This achieves the same configuration as the hierarchical format, setting the batch size to 125 and the batch delay to 50 milliseconds. It's an alternative syntax supported by `logstash.yml`.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/logstash-settings-file.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\npipeline.batch.size: 125\npipeline.batch.delay: 50\n```\n\n----------------------------------------\n\nTITLE: Using Wildcards in Logstash Configuration\nDESCRIPTION: Demonstrates how to use wildcards to load multiple configuration files in a Logstash instance. This is useful for organizing configurations into separate files and loading them dynamically.  The command specifies the debug mode and config file paths using wildcards.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash-command-line.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash --debug -f '/tmp/{one,two,three}'\n```\n\n----------------------------------------\n\nTITLE: Logrollover with Maximum Files and Age Constraints\nDESCRIPTION: Specifies policies for log file rollover based on time intervals and file size, including maximum number of files and file age to retain, ensuring log files are rotated and purged according to specified criteria.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/logging.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nappender.rolling.strategy.max = 30\nappender.rolling.strategy.action.condition.type = IfLastModified\nappender.rolling.strategy.action.condition.age = 7D\n```\n\n----------------------------------------\n\nTITLE: Installing Ruby Dependencies with Bundler - Shell\nDESCRIPTION: Runs Bundler to install dependencies listed in the plugin's Gemfile. Assumes Bundler and Ruby are already installed. Installs all required gems necessary for building and testing the Logstash plugin locally. No parameters required; must be run inside the plugin directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Configure HTTP Input in Downstream Logstash\nDESCRIPTION: Configures the HTTP input plugin in the downstream Logstash instance to receive data. Sets a custom port and specifies the additional codecs for handling JSON data.  The `additional_codecs` setting tells Logstash to treat `application/json` content as `json_lines`.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-http.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ninput {\n    http {\n        port => 8080\n        additional_codecs => { \"application/json\" => \"json_lines\" }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Local Plugin Path to Logstash Gemfile (Ruby)\nDESCRIPTION: Adds a line to the main Logstash Gemfile to reference a locally developed plugin using its file system path. Replace 'logstash-filter-awesome' and the path accordingly. This allows Logstash to find and load the plugin from the specified development directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/filter-plugin/README.md#_snippet_3\n\nLANGUAGE: ruby\nCODE:\n```\ngem \"logstash-filter-awesome\", :path => \"/your/local/logstash-filter-awesome\"\n```\n\n----------------------------------------\n\nTITLE: Storing Credentials in Vault for CI/CD\nDESCRIPTION: Bash command to store all the necessary credentials and endpoints in HashiCorp Vault for CI/CD pipelines, including API keys, hosts, and user credentials.\nSOURCE: https://github.com/elastic/logstash/blob/main/ci/serverless/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nvault write secret/ci/elastic-logstash/serverless-test es_host=\"REDACTED\" es_superuser=\"REDACTED\" es_superuser_pw=\"REDACTED\" \" kb_host=\"REDACTED\" mb_api_key=\"REDACTED\" plugin_api_key=\"REDACTED\" integration_api_key_encoded=\"REDACTED\" tester_api_key_encoded=\"REDACTED\" cpm_api_key=\"REDACTED\"\n```\n\n----------------------------------------\n\nTITLE: Importing Elastic GPG Key for APT Repository Management (shell)\nDESCRIPTION: This shell command downloads Elastic's official GPG key and installs it as a trusted key for APT by converting it to the GPG keyring format compatible with Debian-based systems. Dependencies include wget, sudo, and gpg with the dearmor option. Input is the URL of the GPG key, output is a keyring file placed at /usr/share/keyrings/elastic-keyring.gpg, which is subsequently referenced by APT for package authentication. Make sure the required tools are installed, and run with sufficient permissions.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/installing-logstash.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elastic-keyring.gpg\n```\n\n----------------------------------------\n\nTITLE: Logging Debug Messages in Logstash Using Ruby-like Syntax\nDESCRIPTION: Example of how to emit debug level log messages inside Logstash plugins or components using Ruby-style syntax. This snippet demonstrates how to add debug messages with contextual data to help diagnose issues without harming performance. Requires an initialized logger object (e.g., @logger).\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/community-maintainer.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n@logger.debug(\"Logstash loves debug logs!\", :actions => actions)\n```\n\n----------------------------------------\n\nTITLE: Obtaining SHA256 Fingerprint of a CA Certificate Using OpenSSL (Shell)\nDESCRIPTION: This shell command outputs the SHA256 fingerprint of a PEM-encoded CA certificate. It is run on the Elasticsearch host, with the $ES_HOME environment variable pointing to the Elasticsearch home directory. The command is essential for retrieving the value needed for 'xpack.monitoring.elasticsearch.ssl.ca_trusted_fingerprint' in client configurations. Requires OpenSSL to be installed.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-internal-collection-legacy.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nopenssl x509 -fingerprint -sha256 -in $ES_HOME/config/certs/http_ca.crt\n```\n\n----------------------------------------\n\nTITLE: JVM Option Version Specific Example\nDESCRIPTION: Shows how to define a JVM option that applies to a specific JVM version.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/jvm-settings.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n8:-Xmx2g\n```\n\n----------------------------------------\n\nTITLE: Installing Default Logstash Plugins and Dependencies - Shell\nDESCRIPTION: Invoking this Gradle command installs the default set of Logstash plugins and other required dependencies. Assumes Gradle is installed. Input: none. Output: plugins installed in the development environment.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew installDefaultGems\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash with Basic Authentication\nDESCRIPTION: This snippet showcases how to configure basic user/password authentication in Logstash. It uses `username` and `password` settings to add an extra authentication layer. The configuration is added to both the input and output sections of the configuration.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-native.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n...\n  logstash {\n    ...\n\n    username => \"your-user\"\n    password => \"your-secret\"\n  }\n...\n```\n\n----------------------------------------\n\nTITLE: Setting Pipeline Batch - Logstash YAML\nDESCRIPTION: Configures the Logstash pipeline's batch processing settings using nested YAML structure. It sets the maximum batch size to 125 events and the maximum delay for collecting events before dispatching a batch to 50 milliseconds. This impacts efficiency and memory usage.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/logstash-settings-file.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\npipeline:\n  batch:\n    size: 125\n    delay: 50\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with Local Plugin Configuration (Shell)\nDESCRIPTION: Starts Logstash using a command-line configuration (`-e`) that includes the locally developed plugin (e.g., 'awesome' filter). This is used to test the plugin's functionality within a running Logstash instance. Execute from the Logstash installation directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/filter-plugin/README.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash -e 'filter {awesome {}}'\n```\n\n----------------------------------------\n\nTITLE: Adding GeoIP Filter to Logstash Pipeline in JSON\nDESCRIPTION: Logstash filter configuration that adds the GeoIP filter to lookup geographic location data based on the client IP address extracted by the Grok filter.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n    geoip {\n        source => \"clientip\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Inspecting JVM Heap with jmap\nDESCRIPTION: Utilize the `jmap` command-line utility, typically included with Java distributions, to perform accurate measurements and analysis of the Logstash JVM heap usage. This aids in tuning heap size and diagnosing garbage collection issues.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/performance-troubleshooting.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\njmap\n```\n\n----------------------------------------\n\nTITLE: Testing All Plugins with Rake - Shell\nDESCRIPTION: Runs all currently installed plugin tests using Rake, the Ruby build automation tool. Requires Rake and plugins to be installed. Input: none. Output: summarized results of all plugin tests.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nrake test:plugins\n```\n\n----------------------------------------\n\nTITLE: Setting Password Strings in Logstash Plugins - JavaScript\nDESCRIPTION: Demonstrates how to define a password string that is treated securely by Logstash — the value will not be logged or printed. This example assigns a sensitive password string to a plugin's 'my_password' setting.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_9\n\nLANGUAGE: JavaScript\nCODE:\n```\nmy_password => \"password\"\n```\n\n----------------------------------------\n\nTITLE: Shell Commands for Resetting Filebeat Registry\nDESCRIPTION: Commands for deleting the Filebeat registry file to force Filebeat to read log files from the beginning, and restarting Filebeat with debug output for the publish component.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nsudo rm data/registry\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo ./filebeat -e -c filebeat.yml -d \"publish\"\n```\n\n----------------------------------------\n\nTITLE: Returning Plugin Identifier via getId Method in Java\nDESCRIPTION: This simple method returns the unique identifier string provided during plugin instantiation. It allows Logstash to track and differentiate plugin instances, supporting consistent lifecycle management and configuration referencing.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic String getId() {\n    return id;\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Logstash Monitoring Collection (YAML)\nDESCRIPTION: This configuration snippet disables the default Logstash monitoring collection by setting `monitoring.enabled` to `false` in the `logstash.yml` file. This is a required prerequisite step when setting up monitoring collection via Elastic Agent to prevent conflicts or duplicate data.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-with-elastic-agent.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmonitoring.enabled: false\n```\n\n----------------------------------------\n\nTITLE: Running Plugin Tests with RSpec (Shell)\nDESCRIPTION: Shell command `bundle exec rspec` used to execute the RSpec test suite for the Logstash plugin. `bundle exec` ensures the tests run within the context of the bundled gems defined in the `Gemfile`.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\nbundle exec rspec\n```\n\n----------------------------------------\n\nTITLE: Using an API Key for Monitoring in logstash.yml (YAML)\nDESCRIPTION: Configures Logstash monitoring settings in `logstash.yml` to use an API key for authentication instead of username/password. Sets the `xpack.monitoring.elasticsearch.api_key` parameter to the combined `id:api_key` string.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nxpack.monitoring.elasticsearch.api_key: TiNAGG4BaaMdaH1tRfuU:KnR6yE41RrSowb0kQ0HWoA \n```\n\n----------------------------------------\n\nTITLE: Default Logstash Gemfile Source Configuration\nDESCRIPTION: Displays the standard configuration within a Logstash Gemfile, specifying the default public RubyGems repository (`https://rubygems.org`) as the source for Logstash plugins. This file is automatically generated by Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/private-rubygem.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# This is a Logstash generated Gemfile.\n# If you modify this file manually all comments and formatting will be lost.\n\nsource \"https://rubygems.org\"\n```\n\n----------------------------------------\n\nTITLE: Running Logstash for Apache Logs\nDESCRIPTION: This command runs Logstash using the specified configuration file (logstash-apache.conf). The -f flag specifies the configuration file to use.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nbin/logstash -f logstash-apache.conf\n```\n\n----------------------------------------\n\nTITLE: Running All Logstash Integration Tests with Docker (Shell)\nDESCRIPTION: Builds a Docker image tagged `logstash-integration-tests` from the current directory (Logstash root) and then runs the entire integration test suite within a new container based on that image. Requires Docker to be installed and running. The `-it --rm` flags ensure an interactive terminal and automatic container removal upon completion.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/integration/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\ndocker build  -t logstash-integration-tests .\ndocker run -it --rm logstash-integration-tests ci/integration_tests.sh\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with a Simple Configuration\nDESCRIPTION: This snippet runs Logstash with a minimal configuration specified via the `-e` flag. It uses `stdin` for input, the `example` filter, and `stdout` with `rubydebug` codec for output.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash -e 'input { stdin{} } filter { example {} } output {stdout { codec => rubydebug }}'\n```\n\n----------------------------------------\n\nTITLE: Installing Plugin Development Dependencies - Shell\nDESCRIPTION: Installs development dependencies for Logstash plugins using the logstash-plugin command. Should be run after installing a plugin to ensure all necessary libraries are available for development. Output: development dependencies installed for the specified plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin install --development\n```\n\n----------------------------------------\n\nTITLE: Adding Paquet Gem to Gemfile (Ruby)\nDESCRIPTION: This snippet shows how to add the 'paquet' gem as a dependency to a Ruby project's Gemfile. This is the first step to using the gem in your project. After adding this line, run `bundle install`.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/paquet/README.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\ngem 'paquet'\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with Inline Plugin Configuration (Shell)\nDESCRIPTION: Command to start Logstash using an inline configuration string (`-e`) that includes the developed plugin in a filter block. This is a quick way to test basic plugin functionality without a separate configuration file.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/output-plugin/README.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nbin/logstash -e 'filter {awesome {}}'\n```\n\n----------------------------------------\n\nTITLE: Installing Local Plugin in Logstash Clone (Shell)\nDESCRIPTION: Installs the plugin referenced via the `:path` in the Logstash Gemfile into the local Logstash clone. The `--no-verify` flag skips signature verification, necessary for locally developed, unsigned plugins.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/codec-plugin/README.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin install --no-verify\n```\n\n----------------------------------------\n\nTITLE: Referencing Local Plugin in Logstash Gemfile (Ruby)\nDESCRIPTION: A line to add to the Logstash Gemfile to point to a locally cloned or developed plugin. This allows a Logstash instance to load the plugin directly from your development directory using the specified local path.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/output-plugin/README.md#_snippet_3\n\nLANGUAGE: Ruby\nCODE:\n```\ngem \"logstash-filter-awesome\", :path => \"/your/local/logstash-filter-awesome\"\n```\n\n----------------------------------------\n\nTITLE: Installing Plugin from RubyGems.org in Logstash - Shell\nDESCRIPTION: Installs a released Logstash input plugin directly from RubyGems.org using the bin/logstash-plugin command. Requires the plugin to have been published to RubyGems and Logstash to be installed. Replace logstash-input-mypluginname with the actual published name.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin install logstash-input-mypluginname\n```\n\n----------------------------------------\n\nTITLE: Run All RSpec Tests\nDESCRIPTION: This command executes all RSpec test files within the `spec` directory and its subdirectories. It uses a glob pattern to match all `.rb` files in the `spec` directory. This is used to run the full test suite.\nSOURCE: https://github.com/elastic/logstash/blob/main/spec/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n`rspec spec/**/*.rb`\n```\n\n----------------------------------------\n\nTITLE: Building the Plugin Gem (Shell)\nDESCRIPTION: Builds the Logstash plugin into a distributable Ruby gem file using the `gem build` command and the plugin's `.gemspec` file. The resulting `.gem` file contains the packaged plugin code and metadata.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/codec-plugin/README.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ngem build logstash-codec-awesome.gemspec\n```\n\n----------------------------------------\n\nTITLE: Updating Plugin Dependencies for Testing (Shell)\nDESCRIPTION: Updates the plugin's dependencies as specified in the Gemfile using `bundle install`. This ensures all required gems are present and up-to-date before running the test suite.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/codec-plugin/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Creating Logstash Custom Docker Image\nDESCRIPTION: This Dockerfile demonstrates creating a custom Logstash image by copying custom pipeline and configuration files into the image. It uses the base image 'docker.elastic.co/logstash/logstash:9.0.0'. The `RUN` command removes the default logstash.conf.  The `COPY` instructions add the custom pipeline and configuration files.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/docker-config.md#_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM docker.elastic.co/logstash/logstash:9.0.0\nRUN rm -f /usr/share/logstash/pipeline/logstash.conf\nCOPY pipeline/ /usr/share/logstash/pipeline/\nCOPY config/ /usr/share/logstash/config/\n```\n\n----------------------------------------\n\nTITLE: Example JMH benchmark output\nDESCRIPTION: This is example output from running JMH benchmarks using Gradle in the Logstash project. It shows benchmark results for `QueueBenchmark.pushToPersistedQueue`, including throughput, error, and other statistical information.\nSOURCE: https://github.com/elastic/logstash/blob/main/logstash-core/benchmarks/Readme.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n➜  logstash: ./gradlew jmh\n# JMH 1.18 (released 66 days ago)\n# VM version: JDK 1.8.0_121, VM 25.121-b13\n# VM invoker: /Library/Java/JavaVirtualMachines/jdk1.8.0_121.jdk/Contents/Home/jre/bin/java\n# VM options: -Dfile.encoding=US-ASCII -Duser.country=US -Duser.language=en -Duser.variant\n# Warmup: 3 iterations, 100 ms each\n# Measurement: 10 iterations, 100 ms each\n# Timeout: 10 min per iteration\n# Threads: 1 thread, will synchronize iterations\n# Benchmark mode: Throughput, ops/time\n# Benchmark: org.logstash.benchmark.QueueBenchmark.pushToPersistedQueue\n\n# Run progress: 0.00% complete, ETA 00:00:01\n# Fork: 1 of 1\n# Warmup Iteration   1: 249.325 ops/ms\n# Warmup Iteration   2: 290.150 ops/ms\n# Warmup Iteration   3: 293.669 ops/ms\nIteration   1: 315.075 ops/ms\nIteration   2: 282.020 ops/ms\nIteration   3: 317.281 ops/ms\nIteration   4: 296.559 ops/ms\nIteration   5: 302.803 ops/ms\nIteration   6: 305.187 ops/ms\nIteration   7: 320.959 ops/ms\nIteration   8: 304.073 ops/ms\nIteration   9: 297.499 ops/ms\nIteration  10: 301.889 ops/ms\n\n\nResult \"org.logstash.benchmark.QueueBenchmark.pushToPersistedQueue\":\n  304.334 ?(99.9%) 17.264 ops/ms [Average]\n  (min, avg, max) = (282.020, 304.334, 320.959), stdev = 11.419\n  CI (99.9%): [287.070, 321.599] (assumes normal distribution)\n\n\n# Run complete. Total time: 00:00:22\n\nBenchmark                             Mode  Cnt    Score    Error   Units\nQueueBenchmark.pushToPersistedQueue  thrpt   10  304.334 ? 17.264  ops/ms\n```\n\n----------------------------------------\n\nTITLE: Finding the Default JVM Stack Size\nDESCRIPTION: This command retrieves the default JVM stack size. This is important for calculating the necessary increase in case of StackOverflowErrors. The output is filtered using `grep` to only show the `ThreadStackSize`.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/jvm-settings.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\njava -XX:+PrintFlagsFinal -version | grep ThreadStackSize\n```\n\n----------------------------------------\n\nTITLE: Building, Testing, and Publishing Plugin Gem (sh)\nDESCRIPTION: Runs a sequence of shell commands using Bundler and Rake to prepare and publish the plugin gem to RubyGems.org. This process typically includes installing project dependencies (`bundle install`), preparing vendored assets (`bundle exec rake vendor`), running unit tests (`bundle exec rspec`), and finally building the gem and pushing it to the RubyGems repository (`bundle exec rake publish_gem`).\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nbundle install\nbundle exec rake vendor\nbundle exec rspec\nbundle exec rake publish_gem\n```\n\n----------------------------------------\n\nTITLE: Downloading Logstash (Curl and Tar)\nDESCRIPTION: This snippet downloads and extracts the latest version of Logstash using `curl` and `tar`. It's used for testing the plugin in a clean Logstash environment.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_20\n\nLANGUAGE: sh\nCODE:\n```\ncurl -O https://download.elastic.co/logstash/logstash/logstash-9.0.0.tar.gz\n```\n\nLANGUAGE: sh\nCODE:\n```\ntar xzvf logstash-9.0.0.tar.gz\n```\n\nLANGUAGE: sh\nCODE:\n```\ncd logstash-9.0.0\n```\n\n----------------------------------------\n\nTITLE: Installing Plugin from Gem File (Shell)\nDESCRIPTION: Command to install a Logstash plugin using a pre-built `.gem` file. This method is used to install the plugin into an installed Logstash environment after building the gem.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/output-plugin/README.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\nbin/logstash-plugin install /your/local/plugin/logstash-filter-awesome.gem\n```\n\n----------------------------------------\n\nTITLE: Updating Dependencies Before Testing (Shell)\nDESCRIPTION: Command to ensure all required gem dependencies are installed and up-to-date before executing the plugin's test suite. It uses Bundler to manage the gems.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/output-plugin/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Verifying Logstash Installation by Running an Event - Shell\nDESCRIPTION: Runs Logstash with standard input as source and standard output as sink, allowing the user to type an event for processing. This verifies both the setup and basic event pipeline. Expects Logstash to be built and present in the bin directory. Input: typed event data, e.g., 'hello world'. Output: processed event with timestamp.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash -e 'input { stdin { } } output { stdout {} }'\n```\n\nLANGUAGE: shell\nCODE:\n```\nhello world\n2016-11-11T01:22:14.405+0000 0.0.0.0 hello world\n```\n\n----------------------------------------\n\nTITLE: Querying Elasticsearch for Successful HTTP Responses using Shell\nDESCRIPTION: This shell snippet demonstrates how to use curl to query an Elasticsearch index named 'logstash-$DATE' for documents where the 'response' field equals 200. The $DATE variable should be replaced by the current date in YYYY.MM.DD format, matching the date-based index name generated by Logstash and Filebeat. The query string utilizes Elasticsearch's query parameter to filter on the specific field, and the '--pretty' option formats the returned JSON for readability. Required dependencies: curl, working Elasticsearch cluster, existing logstash index. Input: none directly (manual editing of $DATE needed). Output: JSON search result from Elasticsearch. Ensure the index and field names match those in your pipeline.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\ncurl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=response=200'\n```\n\n----------------------------------------\n\nTITLE: Adding JAR Dependencies to Logstash Plugins in Ruby\nDESCRIPTION: Example of how to specify JAR dependencies in a Logstash plugin's gemspec file. This configuration adds the Elasticsearch JAR as a requirement and includes the jar-dependencies runtime dependency.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_14\n\nLANGUAGE: ruby\nCODE:\n```\n  # Jar dependencies\n  s.requirements << \"jar 'org.elasticsearch:elasticsearch', '5.0.0'\"\n  s.add_runtime_dependency 'jar-dependencies'\n```\n\n----------------------------------------\n\nTITLE: Filtering and Parsing Data in Logstash Using Groovy (Groovy)\nDESCRIPTION: This snippet demonstrates data filtering in Logstash using a Groovy script to transform log entries, such as parsing timestamps or manipulating fields before indexing. Dependencies include Logstash with Groovy filter plugin; it processes event data with scripting logic, enhancing data normalization.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/gene_pool-NOTICE.txt#_snippet_2\n\nLANGUAGE: Groovy\nCODE:\n```\nfilter {\n    groovy {\n        source => \"message\"\n        code => \"def timestamp = new GregorianCalendar().getTimeInMillis(); event.set('timestamp', timestamp)\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Publishing Logstash Plugin to RubyGems.org - Shell\nDESCRIPTION: Runs the Bundler and Rake commands required to prepare, test, and publish a Logstash plugin gem to RubyGems.org. Requires all build, test, and publish dependencies. Publishes the gem if tests pass and version tagging conditions are met. Must be run from the plugin directory after setting up authentication credentials.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nbundle install\nbundle exec rake vendor\nbundle exec rspec\nbundle exec rake publish_gem\n```\n\n----------------------------------------\n\nTITLE: Removing Quarantine Attribute from Default Logstash Directory - Shell\nDESCRIPTION: This shell command specifically removes the quarantine attribute from the extracted default Logstash directory, replacing <archive-or-directory> with the actual directory name (logstash-{{version}}). Useful after extracting Logstash from a .tar.gz to allow execution on macOS systems affected by Gatekeeper. It assumes the variable {{version}} is replaced with the actual Logstash release version string in your shell environment.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/working-with-plugins.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nxattr -d -r com.apple.quarantine logstash-{{version}}\n```\n\n----------------------------------------\n\nTITLE: Bootstrapping Logstash Environment with Rake (Shell)\nDESCRIPTION: Bootstraps the Logstash development environment using the `rake bootstrap` command. This step typically follows the installation of dependencies via Bundler.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\nrake bootstrap\n```\n\n----------------------------------------\n\nTITLE: Publishing a Logstash Plugin to RubyGems.org (Rake)\nDESCRIPTION: This snippet publishes a Logstash plugin to RubyGems.org using a Rake task. It first installs dependencies, runs vendor tasks and rspec, and then publishes the gem.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_26\n\nLANGUAGE: sh\nCODE:\n```\nbundle install\n```\n\nLANGUAGE: sh\nCODE:\n```\nbundle exec rake vendor\n```\n\nLANGUAGE: sh\nCODE:\n```\nbundle exec rspec\n```\n\nLANGUAGE: sh\nCODE:\n```\nbundle exec rake publish_gem\n```\n\n----------------------------------------\n\nTITLE: Installing Plugin Dependencies with Bundler (Shell)\nDESCRIPTION: Command to install the necessary gem dependencies for the Logstash plugin project using Bundler. This command reads the Gemfile and downloads/installs the specified gems.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/output-plugin/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Downloading and extracting Logstash for plugin testing\nDESCRIPTION: Commands for downloading a clean Logstash installation, extracting it, and navigating to the directory to test plugin installation.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\ncurl -O https://download.elastic.co/logstash/logstash/logstash-9.0.0.tar.gz\ntar xzvf logstash-9.0.0.tar.gz\ncd logstash-9.0.0\n```\n\n----------------------------------------\n\nTITLE: Installing a local Logstash codec plugin\nDESCRIPTION: Command for installing a locally built Logstash codec plugin using the logstash-plugin tool. Requires specifying the full path to the gem file.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin install /my/logstash/plugins/logstash-codec-example/logstash-codec-example-0.1.0.gem\n```\n\n----------------------------------------\n\nTITLE: Run a Single RSpec Test\nDESCRIPTION: This command executes a single RSpec test file. Replace `spec/the/test.rb` with the path to the desired test file. It is useful for focusing on a specific test case.\nSOURCE: https://github.com/elastic/logstash/blob/main/spec/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n`rspec spec/the/test.rb`\n```\n\n----------------------------------------\n\nTITLE: Disabling macOS Gatekeeper for Logstash with Version Placeholder\nDESCRIPTION: Example command showing how to remove the quarantine attribute from a specific Logstash version directory after extraction.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash-command-line.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nxattr -d -r com.apple.quarantine logstash-{{version}}\n```\n\n----------------------------------------\n\nTITLE: Adding Test Double and Output Instance Using RSpec let in Ruby\nDESCRIPTION: This snippet uses RSpec's 'let' construct to create instances used in testing. 'output' initializes the ZeroMQ output plugin with 'mode' set to 'server' and 'topology' to 'pushpull', matching the server scenario under test. 'tracer' initializes a test double to simulate logger behavior. Requires RSpec and the plugin; inputs are configuration hashes, and outputs are Ruby objects for later use in tests.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/contributing-patch-plugin.md#_snippet_3\n\nLANGUAGE: ruby\nCODE:\n```\nlet(:output) { described_class.new(\"mode\" => \"server\", \"topology\" => \"pushpull\") }\nlet(:tracer) { double(\"logger\") }\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License 2.0 Boilerplate Notice\nDESCRIPTION: Standard boilerplate notice provided by the Apache License 2.0 appendix. This text should be included, typically within comments, at the beginning of source files, with the bracketed fields '[yyyy]' and '[name of copyright owner]' replaced by the specific copyright year and owner information.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/aws-sdk-resourcegroups-NOTICE.txt#_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OSS-only Build - Shell\nDESCRIPTION: This shell command sets the OSS environment variable to true before building Logstash, ensuring only OSS-licensed code is used. The variable must be exported in the user's shell session or build script before proceeding with the build. Input: none. Output: sets the environment for subsequent operations.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport OSS=true\n```\n\n----------------------------------------\n\nTITLE: Initializing ServiceTester Artifact in Ruby\nDESCRIPTION: This snippet demonstrates how to create a new instance of the ServiceTester::Artifact class within a test context, preparing the object for subsequent test operations. The object represents a Logstash package artifact to be installed, upgraded, or otherwise manipulated in acceptance tests. Dependencies include the acceptance test framework and any supporting classes for ServiceTester. The variable 'logstash' is initialized and passed as a subject to shared examples in RSpec tests.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/README.md#_snippet_0\n\nLANGUAGE: Ruby\nCODE:\n```\nlogstash = ServiceTester::Artifact.new()\n\n## your test code goes here.\n\n# example:\nit_behaves_like \"installable_with_jdk\", logstash\nit_behaves_like \"updated\", logstash, from_release_branch=\"7.17\"\n```\n\n----------------------------------------\n\nTITLE: Publishing the gem with rake commands for versioning and deployment\nDESCRIPTION: This snippet describes the sequence of rake commands needed to install dependencies, vendor assets, run tests, and publish a new plugin version to RubyGems.org. It emphasizes the importance of version tags and build steps, employing the 'publish_gem' rake task.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/publish-plugin.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nbundle install\nbundle exec rake vendor\nbundle exec rspec\nbundle exec rake publish_gem\n```\n\n----------------------------------------\n\nTITLE: Running Logstash with a configuration file\nDESCRIPTION: Shell command to run Logstash with a specific configuration file. This is used to test that the Java output plugin is correctly installed and functioning.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-output-plugin.md#_snippet_12\n\nLANGUAGE: txt\nCODE:\n```\nbin/logstash -f /path/to/java_output.conf\n```\n\n----------------------------------------\n\nTITLE: Example Elasticsearch Create API Key Response (JSON)\nDESCRIPTION: Shows a sample JSON response from the Elasticsearch Create API key API. The response includes the unique `id` and the generated `api_key`, which are combined (`id:api_key`) when configuring Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\":\"TiNAGG4BaaMdaH1tRfuU\", \n  \"name\":\"logstash_host001\",\n  \"api_key\":\"KnR6yE41RrSowb0kQ0HWoA\" \n}\n```\n\n----------------------------------------\n\nTITLE: Running Logstash Filter Configuration\nDESCRIPTION: This command runs Logstash using the specified configuration file (logstash-filter.conf). It uses the -f flag to specify the configuration file.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nbin/logstash -f logstash-filter.conf\n```\n\n----------------------------------------\n\nTITLE: Listing Installed Logstash Plugins (Logstash Plugin CLI)\nDESCRIPTION: This snippet lists the currently installed Logstash plugins using the `bin/logstash-plugin list` command.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_22\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash-plugin list\n```\n\n----------------------------------------\n\nTITLE: Defining RSpec Test Structure for ZeroMQ Output Plugin in Ruby\nDESCRIPTION: This Ruby snippet defines the structure of a new RSpec test block for the Logstash Output ZeroMQ class. It specifies a describe block with the fully qualified class, a context for server mode, and an empty it block for the test logic. No dependencies beyond RSpec are required, but this serves as a template for test organization. Parameters and outputs are not applicable at this outline stage.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/contributing-patch-plugin.md#_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\ndescribe LogStash::Outputs::ZeroMQ do\n  context \"when in server mode\" do\n    it \"a 'bound' info line is logged\" do\n    end\n  end\nend\n```\n\n----------------------------------------\n\nTITLE: Importing RVM GPG Key and Installing Ruby via RVM - Shell\nDESCRIPTION: These shell commands first import the GPG key needed to verify RVM (Ruby Version Manager) installation, then download and install RVM along with the Ruby version specified in the .ruby-version file. Dependencies include curl, gpg, and a bash-compatible shell. This setup ensures the correct Ruby environment for Logstash development.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3\n\\curl -sSL https://get.rvm.io | bash -s stable --ruby=$(cat .ruby-version)\n```\n\n----------------------------------------\n\nTITLE: Run Basic Logstash Pipeline - Windows\nDESCRIPTION: This command runs a minimal Logstash pipeline on Windows with standard input (stdin) and standard output (stdout). It's useful for quickly testing Logstash setup on Windows without needing a config file. Requires Logstash to be installed and current directory to be Logstash installation directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/first-event.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncd logstash-9.0.0\n.\\bin\\logstash.bat -e \"input { stdin { } } output { stdout {} }\"\n```\n\n----------------------------------------\n\nTITLE: JVM Option Generic Example\nDESCRIPTION: Shows how to define a JVM option that applies to all JVM versions.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/jvm-settings.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n-Xmx2g\n```\n\n----------------------------------------\n\nTITLE: Entering Ctrl-C to Interrupt Logstash in Console\nDESCRIPTION: Indicates that pressing Ctrl-C in the console can be used to halt Logstash’s execution interactively, performing a graceful shutdown if properly handled.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/shutdown.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nCtrl-C\n```\n\n----------------------------------------\n\nTITLE: Generating plugin documentation with Rake\nDESCRIPTION: This Rake task generates documentation for Logstash plugins. It requires the plugins to be installed and the development gems to be present. An optional path can be provided to specify the output directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/logstash-docgen/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nrake doc:generate-plugins\nrake doc:generate-plugins[/tmp/the-new-doc] # Depending on the shell you are using you might need to quote the task name, like this rake \"doc:generate-plugins[/tmp/new-doc]\"\n```\n\n----------------------------------------\n\nTITLE: Debugging Logstash Integration Tests with Docker (Shell)\nDESCRIPTION: Outlines steps for debugging Logstash integration tests within a Docker container. It involves cleaning up previous containers, building the image, starting a persistent container named `debug`, running the test setup, attaching an interactive shell, running RSpec tests manually, and finally cleaning up the debug container. Requires Docker.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/integration/README.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\n(Mac/Linux) docker ps --all -q -f status=exited | xargs docker rm  \n(Windows) `docker ps -a` and take note of any exited containers, then `docker rm <container-id>`\ndocker build -t logstash-integration-tests . \ndocker run -d --name debug logstash-integration-tests tail -f /dev/null\ndocker exec -it debug ci/integration_tests.sh setup \ndocker exec -it debug bash\ncd qa/integration\nbundle exec rspec specs/es_output_how_spec.rb\nexit\ndocker kill debug\ndocker rm debug\n```\n\n----------------------------------------\n\nTITLE: Updating Plugin Dependencies before Testing (Shell)\nDESCRIPTION: Updates the plugin's Ruby gem dependencies based on the Gemfile using Bundler. This is typically run before executing tests to ensure the latest compatible dependencies are installed. Requires JRuby and the Bundler gem.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/filter-plugin/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Declaring Plugin License in Gemspec - Ruby\nDESCRIPTION: Sets the allowed license(s) for the plugin gem in the gemspec file. Required by RubyGems.org and open-source best practices. The example shows the Apache License 2.0 declaration complying with Logstash plugin requirements. Add or update this line as appropriate for your plugin's license.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_12\n\nLANGUAGE: ruby\nCODE:\n```\ns.licenses = ['Apache License (2.0)']\n```\n\n----------------------------------------\n\nTITLE: Executing the Test with Bundler and RSpec (Shell Command)\nDESCRIPTION: These shell commands show how to set up and execute the test suite after code changes. The first command installs dependencies via Bundler if not already present, and the second command runs RSpec tests, which validate the logging behavior in the ZeroMQ plugin. Requires Ruby, Bundler, and RSpec installed, as well as a cloned copy of the plugin repository. Inputs are shell commands, outputs are terminal test results.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/contributing-patch-plugin.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nbundle install\nbundle exec rspec\n```\n\n----------------------------------------\n\nTITLE: Retrieving Logstash Node Info Response Example - JSON\nDESCRIPTION: This snippet displays a sample JSON response returned from a Logstash monitoring API root endpoint query. It provides host identification, instance version, and HTTP address details. No transformations required. Intended for reference, not for execution. Inputs: none. Outputs: JSON metadata about Logstash instance. Limitation: values such as {logstash_version} are placeholders.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-logstash.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"host\": \"skywalker\",\n   \"version\": \"{logstash_version}\",\n   \"http_address\": \"127.0.0.1:9600\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Elasticsearch Output with Dynamic Index\nDESCRIPTION: This output plugin configuration sends processed logs to Elasticsearch, creating indices based on date formats for organized storage. It uses dynamic index naming and may include additional parameters such as user credentials or SSL settings. Dependencies include the Elasticsearch output plugin and valid Elasticsearch server connectivity.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/concurrent-ruby-NOTICE.txt#_snippet_2\n\nLANGUAGE: Logstash Configuration (DSL)\nCODE:\n```\noutput {\n  elasticsearch {\n    hosts => [\"es-server:9200\"]\n    index => \"myapp-log-%{+YYYY.MM.dd}\"\n    user => \"elastic\"\n    password => \"changeme\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Ruby Version Used by Logstash - Shell\nDESCRIPTION: This shell command prints out the installed Ruby version, which should match the version specified in the project's .ruby-version file. It has no required parameters and outputs the current Ruby version to stdout.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nruby -v\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Core Path in gradle.properties (Text)\nDESCRIPTION: Defines the `LOGSTASH_CORE_PATH` property within the `gradle.properties` file located in the root of the Java filter plugin project. This property directs the Gradle build system to the location of the compiled `logstash-core.jar`, which contains the necessary Logstash Java API classes. Ensure `<target_folder>` points to the root directory of your local Logstash codebase clone.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-filter-plugin.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nLOGSTASH_CORE_PATH=<target_folder>/logstash-core\n```\n\n----------------------------------------\n\nTITLE: Installing Plugin Dependencies using Bundler (Shell)\nDESCRIPTION: Installs the required Ruby gem dependencies defined in the plugin's Gemfile using the Bundler tool. This command needs to be run in the plugin's root directory where the Gemfile is located. Requires JRuby and the Bundler gem to be installed.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/filter-plugin/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Configuring RubyGems API Key (Curl and Chmod)\nDESCRIPTION: This snippet configures the RubyGems API key by downloading it using `curl` and setting the correct permissions using `chmod`. Replace `username` and `password` with your RubyGems.org credentials.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_25\n\nLANGUAGE: sh\nCODE:\n```\ncurl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials\n```\n\nLANGUAGE: sh\nCODE:\n```\nchmod 0600 ~/.gem/credentials\n```\n\n----------------------------------------\n\nTITLE: Example Timestamped Log Entry (Shell)\nDESCRIPTION: Displays a sample log entry, likely from Elasticsearch, that begins with an ISO8601 timestamp. This illustrates a common pattern for multiline logs where subsequent lines belonging to the same event might not have a timestamp.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/multiline.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n[2015-08-24 11:49:14,389][INFO ][env                      ] [Letha] using [1] data paths, mounts [[/\n(/dev/disk1)]], net usable_space [34.5gb], net total_space [118.9gb], types [hfs]\n```\n\n----------------------------------------\n\nTITLE: Creating Logstash Integration User API Key in Elasticsearch\nDESCRIPTION: Creates an API key for an integration user with permissions to manage index templates, read pipelines, and monitor the cluster. The key expires after 365 days.\nSOURCE: https://github.com/elastic/logstash/blob/main/ci/serverless/README.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\nPOST /_security/api_key\n{\n  \"name\": \"integration_user\",\n  \"expiration\": \"365d\",   \n  \"role_descriptors\": { \n    \"integration_user_role\": {\n      \"cluster\": [\"manage_index_templates\", \"read_pipeline\", \"monitor\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Codec Base and Namespace\nDESCRIPTION: This snippet imports the necessary classes from Logstash for creating a new codec plugin. It's a prerequisite for any codec plugin and provides the foundation for plugin functionality.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"logstash/codecs/base\"\nrequire \"logstash/namespace\"\n```\n\n----------------------------------------\n\nTITLE: Running Plugin Tests using RSpec (Shell)\nDESCRIPTION: Executes the RSpec test suite for the plugin within the context managed by Bundler. This command ensures that tests run against the specific gem versions defined in the Gemfile.lock. Requires Bundler and RSpec gems.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/filter-plugin/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nbundle exec rspec\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Plugin Path in Logstash Gemfile (Ruby)\nDESCRIPTION: Modifies the Logstash `Gemfile` to include a reference to a locally developed plugin using the `:path` option. This allows Logstash to load the plugin directly from the development directory. Replace `/your/local/logstash-codec-awesome` with the actual path.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/codec-plugin/README.md#_snippet_3\n\nLANGUAGE: ruby\nCODE:\n```\ngem \"logstash-codec-awesome\", :path => \"/your/local/logstash-codec-awesome\"\n```\n\n----------------------------------------\n\nTITLE: Example RSpec Test Success Output (Text)\nDESCRIPTION: Sample output from a successful RSpec test run for a Logstash plugin. It indicates the time taken, the number of examples (tests) run, and the number of failures (zero in this case).\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_15\n\nLANGUAGE: Text\nCODE:\n```\nFinished in 0.034 seconds\n1 example, 0 failures\n```\n\n----------------------------------------\n\nTITLE: Running Complete Logstash Test Suite (Including Integration Tests) - Shell\nDESCRIPTION: Executes all tests, including integration, using Gradle. Requires that all test dependencies are installed. Outputs: test results.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew check\n```\n\n----------------------------------------\n\nTITLE: Enabling Logstash Monitoring and Defining Elasticsearch Connection in YAML\nDESCRIPTION: This YAML configuration enables legacy monitoring in Logstash by setting 'xpack.monitoring.enabled' to true and specifying the destination Elasticsearch hosts via the 'xpack.monitoring.elasticsearch.hosts' array. It includes authentication credentials required if Elasticsearch security features are enabled ('xpack.monitoring.elasticsearch.username' and 'xpack.monitoring.elasticsearch.password'). Multiple hosts can be listed for round-robin requests, and HTTPS should be used if SSL/TLS is enabled. The destination cluster must allow legacy collection via its settings.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-internal-collection-legacy.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nxpack.monitoring.allow_legacy_collection: true\nxpack.monitoring.enabled: true\nxpack.monitoring.elasticsearch.hosts: [\"http://es-prod-node-1:9200\", \"http://es-prod-node-2:9200\"] <1>\nxpack.monitoring.elasticsearch.username: \"logstash_system\"\nxpack.monitoring.elasticsearch.password: \"changeme\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Input Plugin for File Monitoring (Groovy)\nDESCRIPTION: This snippet demonstrates setting up a Logstash input plugin to monitor log files for new entries. The code is written in Groovy, typically used in Logstash configuration files, and specifies the file path, polling interval, and start position. It is essential for ingesting log data into Logstash pipelines.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/org.eclipse.core!org.eclipse.core.expressions-NOTICE.txt#_snippet_0\n\nLANGUAGE: Groovy\nCODE:\n```\ninput {\n  file {\n    path => \"/var/log/app.log\"\n    start_position => \"beginning\"\n    sincedb_path => \"/dev/null\"\n    publish_absence => false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Logstash Benchmark CLI with Java\nDESCRIPTION: This code demonstrates how to execute the Benchmark CLI Java application by specifying the classpath (including the built jar) and invoking the main class with '--help' argument to display available options. It’s essential for accessing command-line configurations for benchmarking.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/benchmark-cli/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ java -cp 'benchmark-cli.jar:*' org.logstash.benchmark.cli.Main --help\n```\n\n----------------------------------------\n\nTITLE: Simulating Input for Logstash Filter\nDESCRIPTION: This command simulates input to the Logstash filter configuration. The input is a sample log line in Apache combined log format, used to test the filter's parsing capabilities.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\n127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \"GET /xampp/status.php HTTP/1.1\" 200 3891 \"http://cadenza/xampp/navi.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\"\n```\n\n----------------------------------------\n\nTITLE: Cloning the Plugin Repository (Shell)\nDESCRIPTION: Shell commands demonstrating how to clone a Logstash plugin's Git repository using either HTTPS or SSH protocols. Placeholders `GITUSERNAME` and `MYPLUGINNAME` should be replaced with the actual GitHub username and plugin name. It also includes the command to change into the newly cloned directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone https://github.com/GITUSERNAME/logstash-codec-MYPLUGINNAME.git\n\n# alternately, via ssh: \ngit clone git@github.com:GITUSERNAME/logstash-codec-MYPLUGINNAME.git\n\ncd logstash-codec-MYPLUGINNAME\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies for Logstash plugin development with Bundler\nDESCRIPTION: This snippet demonstrates installing project dependencies required for developing Logstash plugins using Bundler. It assumes that you have created or cloned a plugin repository and need to manage its dependencies before testing or deployment.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/input-plugin/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Boilerplate Notice Template (Plaintext)\nDESCRIPTION: This template provides the standard text required to apply the Apache License 2.0 to a software project. It includes placeholders for the copyright year ([yyyy]) and owner ([name of copyright owner]), and links to the full license text. This notice should typically be included within comments at the beginning of source files.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/elastic-enterprise-search-NOTICE.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Running a Single Integration Test Spec - Shell\nDESCRIPTION: Executes a single integration test by specifying the test file via a Gradle property. Requires Gradle and valid integration specs. Input: -PrubyIntegrationSpecs with the relative path to the spec file. Output: result of the specified integration test.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew integrationTests -PrubyIntegrationSpecs=specs/slowlog_spec.rb\n```\n\n----------------------------------------\n\nTITLE: Example Copyright Disclaimer Template\nDESCRIPTION: Offers a sample text for a copyright disclaimer, typically signed by an employer or institution, relinquishing copyright interest in a program contributed by an individual associated with them. Uses placeholders for the organization, program, and author.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/jruby-NOTICE.txt#_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n    Yoyodyne, Inc., hereby disclaims all copyright interest in the program\n    'Gnomovision' (which makes passes at compilers) written by James Hacker.\n\n    signature of Ty Coon, 1 April 1989\n\n    Ty Coon, President of Vice\n```\n\n----------------------------------------\n\nTITLE: Setting Up Offline GeoIP Database with Custom Endpoint via Docker\nDESCRIPTION: Runs an nginx server serving static MaxMind GeoIP database files to enable offline or air-gapped environments. The database files are retrieved manually, then served via Docker. Logstash is configured with the custom endpoint URL to fetch database updates.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuring-geoip-database-management.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\ndocker run -p 8080:80 -v my/database/dir:/usr/share/nginx/html:ro nginx\n```\n\nLANGUAGE: Shell\nCODE:\n```\n./bin/elasticsearch-geoip -s my/database/dir\n```\n\n----------------------------------------\n\nTITLE: Initializing Logstash Pipeline Configuration (Ruby)\nDESCRIPTION: This snippet outlines the setup of a Logstash pipeline configuration using Ruby syntax. It defines input, filter, and output sections to process data and send it to Elasticsearch. Dependencies include Logstash environment; key parameters specify data sources, filtering rules, and Elasticsearch connection details.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/gene_pool-NOTICE.txt#_snippet_0\n\nLANGUAGE: Ruby\nCODE:\n```\ninput {\n    stdin { }\n}\nfilter {\n    grok {\n        match => { \"message\" => \"%{COMMONAPACHELOG}\" }\n    }\n}\noutput {\n    elasticsearch {\n        hosts => [\"localhost:9200\"]\n        index => \"apache_logs\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating a new Logstash plugin with bin/logstash-plugin generate (shell script)\nDESCRIPTION: This snippet demonstrates how to use the command-line tool to generate the foundational structure for a new Logstash plugin. It specifies the plugin type, name, and optional directory path. The command creates necessary files, directories, and dependencies to facilitate custom plugin development. Dependencies include Logstash core, RubyGems, and related libraries, while inputs, outputs, and options depend on the plugin type.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/plugin-generator.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin generate --type input --name xkcd --path ~/ws/elastic/plugins\n```\n\n----------------------------------------\n\nTITLE: Referencing Keystore Keys in Logstash YAML\nDESCRIPTION: This code example demonstrates referencing a keystore value within the `logstash.yml` configuration file.  It uses the same `${KEY}` syntax as in config files.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/keystore.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nxpack.management.elasticsearch.password: ${ES_PWD}\n```\n\n----------------------------------------\n\nTITLE: Configuring Client TLS Authentication using Keystore in YAML\nDESCRIPTION: This snippet configures Logstash to authenticate to Elasticsearch using a client certificate stored in a Java keystore. The 'xpack.monitoring.elasticsearch.ssl.keystore.path' specifies the keystore location, while 'xpack.monitoring.elasticsearch.ssl.keystore.password' provides access. Required for mutual TLS authentication, particularly in secured clusters.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-internal-collection-legacy.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nxpack.monitoring.elasticsearch.ssl.keystore.path: /path/to/file\nxpack.monitoring.elasticsearch.ssl.keystore.password: password\n```\n\n----------------------------------------\n\nTITLE: Defining a Logstash Filter to Parse Log Lines (Ruby)\nDESCRIPTION: This snippet illustrates a Logstash filter section using Ruby code to parse and transform log entries. It leverages Ruby's scripting capabilities within Logstash to extract specific fields from unstructured logs, enabling refined data processing for analytics.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/org.eclipse.core!org.eclipse.core.expressions-NOTICE.txt#_snippet_1\n\nLANGUAGE: Ruby\nCODE:\n```\nfilter {\n  ruby {\n    code => \"\\n event.get('message').each_line { |line| \\n   if line =~ /ERROR/ \\n     event.set('error_flag', true) ) \\n   end \\n}\" \n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Logstash Elasticsearch Output Configuration (JSON)\nDESCRIPTION: This snippet provides a JSON-style configuration for Logstash output to Elasticsearch, defining host addresses and target index. It is used to specify how processed data is sent to Elasticsearch, requiring prior data filtering steps. Inputs include data streams; outputs define Elasticsearch connection details.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/gene_pool-NOTICE.txt#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"output\": {\n    \"elasticsearch\": {\n      \"hosts\": [\"localhost:9200\"],\n      \"index\": \"logs_index\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Keystore\nDESCRIPTION: The `create` command is used to create a new Logstash keystore or overwrite an existing one. It is executed using the `logstash-keystore` command-line tool. The location of the keystore is defined in the `path.settings` setting. The command will prompt to confirm overwrite if a keystore already exists.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/keystore.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-keystore create\n```\n\n----------------------------------------\n\nTITLE: Configuring PipelineRoutingFilter for Log4j2 Properties - Shell\nDESCRIPTION: Shows the replacement configuration line for 'log4j2.properties' required after upgrading Logstash, enabling the PipelineRoutingFilter for improved logging. The configuration applies to properties starting with 'appender.rolling.avoid_pipelined_filter.*' and 'appender.json_rolling.avoid_pipelined_filter.*'. Requires Logstash with Log4j2 logging backend. No input beyond the configuration file; the output is an updated logging behavior.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/getting-started-with-logstash.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nappender.rolling.avoid_pipelined_filter.type = PipelineRoutingFilter\n\n```\n\nLANGUAGE: shell\nCODE:\n```\nappender.json_rolling.avoid_pipelined_filter.type = PipelineRoutingFilter\n\n```\n\n----------------------------------------\n\nTITLE: Skeleton Logstash Pipeline Configuration (JSON)\nDESCRIPTION: This snippet demonstrates the basic structure of a Logstash configuration file in JSON-like syntax, with commented sections for input, optional filter, and output blocks. All sections are empty placeholders, intended for users to customize. Comments explain the purpose of each section, emphasizing that the configuration as shown is non-functional until further populated.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n# The # character at the beginning of a line indicates a comment. Use\n# comments to describe your configuration.\ninput {\n}\n# The filter part of this file is commented out to indicate that it is\n# optional.\n# filter {\n#\n# }\noutput {\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License 2.0 Boilerplate Notice (Plain Text)\nDESCRIPTION: This snippet provides the standard boilerplate text required for applying the Apache License 2.0 to your work. It includes copyright information and the license terms disclaimer. Users must replace the bracketed placeholders [yyyy] and [name of copyright owner] and enclose the text within the appropriate comment syntax for their specific file format.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/commons-codec!commons-codec-NOTICE.txt#_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Setting Locale via LS_JAVA_OPTS\nDESCRIPTION: This command demonstrates how to override JVM settings using the `LS_JAVA_OPTS` environment variable. In this case, it sets the locale to German (Germany).\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/jvm-settings.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nLS_JAVA_OPTS=\"-Duser.country=DE -Duser.language=de\" bin/logstash -e 'input { stdin { codec => json } }'\n```\n\n----------------------------------------\n\nTITLE: Example C-Style Line Continuation\nDESCRIPTION: Presents a C language `printf` statement using a backslash (`\\`) at the end of a line to indicate continuation onto the next line, another common multiline pattern.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/multiline.md#_snippet_2\n\nLANGUAGE: c\nCODE:\n```\nprintf (\"%10.10ld  \\t %10.10ld \\t %s\\\n  %f\", w, x, y, z );\n```\n\n----------------------------------------\n\nTITLE: Verifying LS_JAVA_HOME with PowerShell\nDESCRIPTION: This PowerShell command retrieves the value of the LS_JAVA_HOME environment variable. It's used to verify that the JVM path is correctly set for Logstash. The command outputs the path to the JVM installation directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash-windows.md#_snippet_0\n\nLANGUAGE: PowerShell\nCODE:\n```\nWrite-Host $env:LS_JAVA_HOME\n```\n\n----------------------------------------\n\nTITLE: Installing Default Plugins with Rake - Shell\nDESCRIPTION: Uses Rake to install the default set of Logstash plugins. Assumes Rake is installed and is run in the Logstash project directory. Output: installation of default plugins.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nrake test:install-default\n```\n\n----------------------------------------\n\nTITLE: Debugging Logstash Integration Tests Locally (Bash/Shell)\nDESCRIPTION: Provides a sequence of commands for debugging integration tests locally on Mac/Linux. It first runs the setup phase, changes to the integration test directory (`qa/integration`), and then uses `bundle exec rspec` to run specific or all tests directly, facilitating debugging. Requires JRuby, bundler, and RSpec.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/integration/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nci/integration_tests.sh setup \ncd qa/integration\nbundle exec rspec specs/es_output_how_spec.rb (single test)\nbundle exec rspec specs/*  (all tests)\n```\n\n----------------------------------------\n\nTITLE: Renaming Template Files Using Bash for a Logstash Plugin (bash)\nDESCRIPTION: This bash code block lists the commands for renaming template/example files to match the new plugin's name during setup. It assumes the working directory is the root of the plugin repo. No external dependencies beyond standard bash utilities are required. Inputs: the existing example filenames; Outputs: renamed files for plugin customization.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/logstash-filter-mypluginname\nmv logstash-filter-example.gemspec logstash-filter-mypluginname.gemspec\nmv lib/logstash/filters/example.rb lib/logstash/filters/mypluginname.rb\nmv spec/filters/example_spec.rb spec/filters/mypluginname_spec.rb\n\n```\n\n----------------------------------------\n\nTITLE: Minimal Logstash Configuration for Testing\nDESCRIPTION: This Java configuration demonstrates a minimal Logstash pipeline to test the installation and functionality of the Java filter plugin. It takes a simple input, applies the Java filter example, and then outputs the results to standard output using a rubydebug codec. This configuration requires a correctly packaged and installed java filter example plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/java-filter-plugin.md#_snippet_8\n\nLANGUAGE: java\nCODE:\n```\ninput {\n  generator { message => \"Hello world!\" count => 1 }\n}\nfilter {\n  java_filter_example {}\n}\noutput {\n  stdout { codec => rubydebug }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Event Output from Downstream Logstash - JSON\nDESCRIPTION: Represents a typical event structure received by the downstream Logstash instance after successful communication. Contains timestamp, sequence number, tags indicating JSON codec application, custom message, version, and host information. Input will appear like this if the configuration and SSL handshake are correct.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-lumberjack.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"@timestamp\" => 2018-02-07T12:16:39.415Z,\n  \"sequence\"   => 0\n  \"tags\"       => [\n    [0] \"beats_input_codec_json_applied\"\n  ],\n  \"message\"    => \"Hello world\",\n  \"@version\"   => \"1\",\n  \"host\"       => \"ls1.semicomplete.com\"\n}\n```\n\n----------------------------------------\n\nTITLE: Building Logstash Tarball and Zip Distributions - Shell\nDESCRIPTION: Invokes Gradle tasks to build tarball and zip distribution packages of Logstash, with output written to the LS_HOME/build directory. Gradle must be installed. Input: none. Output: distributable archives for deployment.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew assembleTarDistribution\n```\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew assembleZipDistribution\n```\n\n----------------------------------------\n\nTITLE: Building Standard and OSS Logstash Docker Images via Rake\nDESCRIPTION: Demonstrates how to build the official Logstash Docker images locally. First, check out the desired version branch (e.g., 7.0) using `git checkout`. Then, use `rake artifact:docker` to build the standard image or `rake artifact:docker_oss` to build the Open Source Software (OSS) version. This process requires Git, Rake, Docker, GNU Make, Python 3.5+ with Virtualenv, and JRuby 9.1+ to be installed.\nSOURCE: https://github.com/elastic/logstash/blob/main/docker/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout 7.0\nrake artifact:docker\n# and for the OSS package\nrake artifact:docker_oss\n```\n\n----------------------------------------\n\nTITLE: Building OSS-only RPM and Debian Packages Using Rake - Shell\nDESCRIPTION: Similar to the standard RPM/DEB build commands, these commands build RPM and Debian packages containing only OSS-licensed components. fpm is required. Output: OSS-only native packages.\nSOURCE: https://github.com/elastic/logstash/blob/main/README.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\nrake artifact:rpm_oss\n```\n\nLANGUAGE: shell\nCODE:\n```\nrake artifact:deb_oss\n```\n\n----------------------------------------\n\nTITLE: JVM Option Comment Example\nDESCRIPTION: Shows how to add comments to jvm.options file to explain configuration.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/jvm-settings.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n# this is a comment\n```\n\n----------------------------------------\n\nTITLE: Running a Single Logstash Integration Test with Docker (Shell)\nDESCRIPTION: Builds the `logstash-integration-tests` Docker image and then runs a specified RSpec test file (e.g., `specs/es_output_how_spec.rb`) inside a container. This allows for targeted testing within the Docker environment. Requires Docker.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/integration/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\ndocker build  -t logstash-integration-tests .\ndocker run -it --rm logstash-integration-tests ci/integration_tests.sh specs/es_output_how_spec.rb\n```\n\n----------------------------------------\n\nTITLE: Executing Paquet Vendor Task (Shell)\nDESCRIPTION: This shell command executes the 'paquet:vendor' Rake task defined in the Rakefile, using `bundle exec` to ensure it runs within the context of the project's bundled gems. Running this command triggers the Paquet gem to download and package the specified dependencies into the designated target directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/paquet/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbundle exec rake paquet:vendor\n```\n\n----------------------------------------\n\nTITLE: Debug a Single RSpec Test\nDESCRIPTION: This command executes a single RSpec test file in debug mode. It sets the `LOGSTASH_DEBUG` environment variable to `y`, which enables debugging output. This is useful for troubleshooting failing tests.\nSOURCE: https://github.com/elastic/logstash/blob/main/spec/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n`LOGSTASH_DEBUG=y rspec spec/the/test.rb`\n```\n\n----------------------------------------\n\nTITLE: Executing a Logstash Benchmark Test with Custom Parameters\nDESCRIPTION: This command runs the benchmark using the Java application, setting the working directory, choosing the 'baseline' test case, and specifying Logstash version 5.5.0. It initiates performance benchmarking and outputs relevant statistics such as elapsed time, event count, throughput, and CPU usage, with ASCII art banners for visual identification.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/benchmark-cli/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ java -cp 'benchmark-cli.jar:*' org.logstash.benchmark.cli.Main --workdir=/tmp/benchmark2 --testcase=baseline --distribution-version=5.5.0\n```\n\n----------------------------------------\n\nTITLE: Monitoring Network I/O (Linux)\nDESCRIPTION: Employ Linux tools such as `dstat` or `iftop` to monitor network input/output. This is useful for identifying network saturation caused by Logstash inputs or outputs performing extensive network operations.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/performance-troubleshooting.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndstat\n```\n\nLANGUAGE: shell\nCODE:\n```\niftop\n```\n\n----------------------------------------\n\nTITLE: Disabling macOS Gatekeeper for Logstash\nDESCRIPTION: Command to remove the quarantine attribute from Logstash files on macOS to prevent Gatekeeper warnings. This command needs to be run on either the downloaded archive or the extracted directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/running-logstash-command-line.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nxattr -d -r com.apple.quarantine <archive-or-directory>\n```\n\n----------------------------------------\n\nTITLE: Remove macOS Quarantine Attribute\nDESCRIPTION: This command removes the quarantine attribute from the Logstash archive or directory on macOS. This is needed to bypass macOS Gatekeeper restrictions and allow Logstash to run without interruptions.  It requires the `xattr` command-line tool and the path to either the downloaded `.tar.gz` archive or the extracted directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/first-event.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nxattr -d -r com.apple.quarantine <archive-or-directory>\n```\n\n----------------------------------------\n\nTITLE: Sample Logstash Output Event Structure - Ruby\nDESCRIPTION: Depicts a typical event object emitted by Logstash after processing a valid syslog message through the configured pipeline. The output includes normalized and extracted fields such as timestamp, host, program, PID, and message, along with calculated fields such as severity and facility codes. Field presence depends on correct Grok pattern matches; malformed input may result in missing attributes.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_12\n\nLANGUAGE: ruby\nCODE:\n```\n{\n                 \"message\" => \"Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)\",\n              \"@timestamp\" => \"2013-12-23T22:30:01.000Z\",\n                \"@version\" => \"1\",\n                    \"type\" => \"syslog\",\n                    \"host\" => \"0:0:0:0:0:0:0:1:52617\",\n        \"syslog_timestamp\" => \"Dec 23 14:30:01\",\n         \"syslog_hostname\" => \"louis\",\n          \"syslog_program\" => \"CRON\",\n              \"syslog_pid\" => \"619\",\n          \"syslog_message\" => \"(www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)\",\n             \"received_at\" => \"2013-12-23 22:49:22 UTC\",\n           \"received_from\" => \"0:0:0:0:0:0:0:1:52617\",\n    \"syslog_severity_code\" => 5,\n    \"syslog_facility_code\" => 1,\n         \"syslog_facility\" => \"user-level\",\n         \"syslog_severity\" => \"notice\"\n}\n```\n\n----------------------------------------\n\nTITLE: Obtaining RubyGems API credentials\nDESCRIPTION: Command to retrieve and securely store RubyGems API credentials for publishing gems. Requires a RubyGems.org username and password.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\ncurl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials\nchmod 0600 ~/.gem/credentials\n```\n\n----------------------------------------\n\nTITLE: Running a Single Logstash Integration Test Locally (Bash/Shell)\nDESCRIPTION: Executes the main integration test script, targeting a specific RSpec test file (e.g., `specs/es_output_how_spec.rb`). This allows for running individual tests locally from the Logstash root directory on Mac or Linux, assuming dependencies are met.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/integration/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nci/integration_tests.sh specs/es_output_how_spec.rb\n```\n\n----------------------------------------\n\nTITLE: Logstash Output Example\nDESCRIPTION: This example demonstrates the typical output from a basic Logstash pipeline when running the command `bin/logstash -e 'input { stdin { } } output { stdout {} }'` and typing `hello world` into the console. The output includes the message along with timestamp and IP address information.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/first-event.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nhello world\n2013-11-21T01:22:14.405+0000 0.0.0.0 hello world\n```\n\n----------------------------------------\n\nTITLE: Testing plugin documentation as a plugin author\nDESCRIPTION: These rake tasks allow plugin authors to test the generated documentation. They require the declaration of `logstash-docgen` as a development dependency and requiring `logstash/docgen/plugin_doc` in the `Rakefile`.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/logstash-docgen/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nbundle exec rake doc:asciidoc # return the raw asciidoc\nbundle exec rake doc:html # Give you the raw html\n```\n\n----------------------------------------\n\nTITLE: Parsing Elasticsearch Search Response - Sample Output in JSON\nDESCRIPTION: This JSON snippet is an example response returned by Elasticsearch when a search query is performed. It details various metadata about the query execution ('took', 'timed_out', '_shards') and lists the hits array, where each hit contains relevant document fields extracted and enriched by the Logstash pipeline. Key fields include IP, user agent, geoip information, request details, and parsed timestamps. No input is required; this is output only. Structure may vary based on field mappings and pipeline configuration. Useful for verifying correct ingestion, field extraction, and geo enrichment.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/advanced-pipeline.md#_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"took\": 50,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 98,\n    \"max_score\": 2.793642,\n    \"hits\": [\n      {\n        \"_index\": \"logstash-2017.11.09\",\n        \"_type\": \"doc\",\n        \"_id\": \"3IzDnl8BW52sR0fx5wdV\",\n        \"_score\": 2.793642,\n        \"_source\": {\n          \"request\": \"/presentations/logstash-monitorama-2013/images/frontend-response-codes.png\",\n          \"agent\": \"\\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\\\"\",\n          \"geoip\": {\n            \"timezone\": \"Europe/Moscow\",\n            \"ip\": \"83.149.9.216\",\n            \"latitude\": 55.7485,\n            \"continent_code\": \"EU\",\n            \"city_name\": \"Moscow\",\n            \"country_name\": \"Russia\",\n            \"country_code2\": \"RU\",\n            \"country_code3\": \"RU\",\n            \"region_name\": \"Moscow\",\n            \"location\": {\n              \"lon\": 37.6184,\n              \"lat\": 55.7485\n            },\n            \"postal_code\": \"101194\",\n            \"region_code\": \"MOW\",\n            \"longitude\": 37.6184\n          },\n          \"offset\": 2932,\n          \"auth\": \"-\",\n          \"ident\": \"-\",\n          \"verb\": \"GET\",\n          \"prospector\": {\n            \"type\": \"log\"\n          },\n          \"input\": {\n            \"type\": \"log\"\n          },\n          \"source\": \"/path/to/file/logstash-tutorial.log\",\n          \"message\": \"83.149.9.216 - - [04/Jan/2015:05:13:45 +0000] \\\"GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1\\\" 200 52878 \\\"http://semicomplete.com/presentations/logstash-monitorama-2013/\\\" \\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\\\"\",\n          \"tags\": [\n            \"beats_input_codec_plain_applied\"\n          ],\n          \"referrer\": \"\\\"http://semicomplete.com/presentations/logstash-monitorama-2013/\\\"\",\n          \"@timestamp\": \"2017-11-09T03:11:35.304Z\",\n          \"response\": \"200\",\n          \"bytes\": \"52878\",\n          \"clientip\": \"83.149.9.216\",\n          \"@version\": \"1\",\n          \"beat\": {\n            \"name\": \"My-MacBook-Pro.local\",\n            \"hostname\": \"My-MacBook-Pro.local\",\n            \"version\": \"6.0.0\"\n          },\n          \"host\": \"My-MacBook-Pro.local\",\n          \"httpversion\": \"1.1\",\n          \"timestamp\": \"04/Jan/2015:05:13:45 +0000\"\n        }\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Logstash Plugin Installation Success Output Example (sh)\nDESCRIPTION: Displays the typical informational output from the `bin/logstash-plugin install` command when a plugin gem is successfully validated and installed. It shows confirmation messages indicating the gem file was checked and the plugin was successfully integrated into the Logstash environment with its specified version.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/output-new-plugin.md#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\nvalidating /my/logstash/plugins/logstash-output-example/logstash-output-example-0.1.0.gem >= 0\nValid logstash plugin. Continuing...\nSuccessfully installed 'logstash-output-example' with version '0.1.0'\n```\n\n----------------------------------------\n\nTITLE: Define Heading IDs with Variables in AsciiDoc\nDESCRIPTION: This snippet demonstrates how to define heading anchors in AsciiDoc using variables to create unique IDs, which prevents duplication issues during Logstash Versioned Plugin Reference builds. It avoids hardcoding IDs for headings.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/plugin-doc.md#_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n[id=\"plugins-{type}s-{plugin}-config_models\"]\n==== Configuration models\n```\n\n----------------------------------------\n\nTITLE: Setting Elasticsearch Node Sniffing for Monitoring (YAML)\nDESCRIPTION: This YAML setting controls whether Logstash will discover and connect to additional nodes in the specified Elasticsearch cluster. Setting 'xpack.monitoring.elasticsearch.sniffing' to 'false' disables node discovery, while 'true' enables it. Sniffing is off by default, but may be used in advanced, dynamic clusters.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-internal-collection-legacy.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nxpack.monitoring.elasticsearch.sniffing: false\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Docker Resources for Logstash Tests (Bash/Shell)\nDESCRIPTION: Executes a script (`ci/docker_prune.sh`) intended for Mac/Linux systems to clean up Docker resources created during Logstash integration testing. This script removes most images and containers, potentially excluding the `logstash-base` image. Caution is advised as it performs extensive cleanup. Requires Docker.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/integration/README.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nci/docker_prune.sh\n```\n\n----------------------------------------\n\nTITLE: Appending a Suffix to Event Fields in Logstash with Python\nDESCRIPTION: This script adds a suffix to a specified event field, aiding in label augmentation or data tagging during processing. It modifies the event in place and is useful for trackings or identifiers.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/com.fasterxml.jackson.dataformat!jackson-dataformat-yaml-NOTICE.txt#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef append_suffix(event, field_name, suffix):\n    if field_name in event:\n        event[field_name] = str(event[field_name]) + suffix\n    return event\n```\n\n----------------------------------------\n\nTITLE: Logstash Shutdown Process Overview\nDESCRIPTION: Describes the sequential steps Logstash performs during a controlled shutdown: stopping plugins, processing in-flight events, and terminating the process. It also discusses factors that can delay shutdown, such as slow plugins or unresponsive outputs, and introduces stall detection and unsafe shutdown flags.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/shutdown.md#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Link to Content in Same AsciiDoc File\nDESCRIPTION: This snippet illustrates how to create a link to content within the same AsciiDoc file using angle brackets. The example shows a link pointing to a specific heading identified by its ID.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/plugin-doc.md#_snippet_1\n\nLANGUAGE: txt\nCODE:\n```\n<<plugins-{type}s-{plugin}-config_models>>\n```\n\n----------------------------------------\n\nTITLE: Building the Logstash Benchmark CLI with Gradle\nDESCRIPTION: This snippet shows how to compile the Benchmark CLI tool into a self-contained JAR file using Gradle. It executes the 'clean' and 'assemble' tasks to generate the output jar located at 'build/libs/benchmark-cli.jar', which is necessary for subsequent execution.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/benchmark-cli/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngradle clean assemble\n```\n\n----------------------------------------\n\nTITLE: Using Numeric Values in Logstash Plugin Configurations - JavaScript\nDESCRIPTION: Example usage of numeric (integer) values in plugin settings. The snippet assigns the 'port' parameter the integer value 33, demonstrating straightforward numeric assignments supported in plugin options for fields requiring numeric input.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/configuration-file-structure.md#_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\nport => 33\n```\n\n----------------------------------------\n\nTITLE: Building the 'offline' Utility using Make\nDESCRIPTION: Compiles the 'offline' utility using the provided Makefile. This command processes the source code (likely C, utilizing seccomp) and produces the executable file named 'offline' in the current directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/integration/fixtures/offline_wrapper/README.md#_snippet_0\n\nLANGUAGE: Makefile\nCODE:\n```\nmake offline\n```\n\n----------------------------------------\n\nTITLE: Implementing a Conditional Drop Filter in Logstash with Python\nDESCRIPTION: This filter conditionally drops events based on attribute values, useful for filtering specific data points before further processing. It evaluates event properties and discards events that meet the specified criteria.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/com.fasterxml.jackson.dataformat!jackson-dataformat-yaml-NOTICE.txt#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef drop_filter(event, attribute_name, attribute_value):\n    if event.get(attribute_name) == attribute_value:\n        return None  # Drop event\n    return event  # Keep event\n```\n\n----------------------------------------\n\nTITLE: Example GPLv2 Source File Header Notice\nDESCRIPTION: Provides a template notice to be included at the beginning of source files licensed under GPLv2. It asserts the program is free software under GPLv2 terms, disclaims warranty, and points to the full license text, using placeholders for program details and copyright information.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/jruby-NOTICE.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n    One line to give the program's name and a brief idea of what it does.\n\n    Copyright (C) <year> <name of author>\n\n    This program is free software; you can redistribute it and/or modify it\n    under the terms of the GNU General Public License as published by the Free\n    Software Foundation; either version 2 of the License, or (at your option)\n    any later version.\n\n    This program is distributed in the hope that it will be useful, but WITHOUT\n    ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n    FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n    more details.\n\n    You should have received a copy of the GNU General Public License along\n    with this program; if not, write to the Free Software Foundation, Inc., 59\n    Temple Place, Suite 330, Boston, MA 02111-1307 USA\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Boilerplate Notice Template\nDESCRIPTION: This template provides the standard boilerplate text recommended by the Apache License 2.0 appendix for inclusion in source files. Developers should replace '[yyyy]' with the copyright year(s) and '[name of copyright owner]' with the appropriate entity, enclosing the text within the relevant comment syntax for the file format.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/elastic-transport-NOTICE.txt#_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Boilerplate Notice Template\nDESCRIPTION: This is the standard boilerplate notice recommended for inclusion in source files when applying the Apache License, Version 2.0. Users should replace the bracketed fields '[yyyy]' and '[name of copyright owner]' with their specific information and enclose the notice in the appropriate comment syntax for the file format.\nSOURCE: https://github.com/elastic/logstash/blob/main/tools/dependencies-report/src/main/resources/notices/com.fasterxml.jackson.core!jackson-annotations-NOTICE.txt#_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Example Usage of 'offline' with nc and host (Shell)\nDESCRIPTION: Provides concrete examples comparing normal execution with execution via the 'offline' utility. It shows how attempts to connect using 'nc' or resolve a hostname using 'host' normally might succeed or fail due to network state, but consistently fail with permission errors when run via './offline', demonstrating the network restriction imposed by seccomp.\nSOURCE: https://github.com/elastic/logstash/blob/main/qa/integration/fixtures/offline_wrapper/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n% nc localhost 10000\nNcat: Connection refused.\n\n% ./offline nc localhost 10000\nNcat: Permission denied.\n\n% ./offline host google.com\nhost: isc_socket_bind: permission denied\n```\n\n----------------------------------------\n\nTITLE: Setting File Path with Environment Variable (Ruby)\nDESCRIPTION: This example demonstrates how to use an environment variable to dynamically set a file path. The `add_field` directive of a `mutate` filter is used to set `my_path` based on the value of the `HOME` environment variable.  This allows for configuration that adapts to the user's home directory.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/environment-variables.md#_snippet_5\n\nLANGUAGE: ruby\nCODE:\n```\nfilter {\n  mutate {\n    add_field => {\n      \"my_path\" => \"${HOME}/file.log\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Filter for Data Parsing in Logstash\nDESCRIPTION: This snippet sets up the CSV filter to parse comma-separated values into individual fields, either using default column names or specified column headers. It allows customization of the separator and field names, and is used to extract structured data from CSV inputs. Dependencies include the CSV filter plugin.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/data-deserialization.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\nfilter {\n  csv {\n    separator => \",\"\n    columns => [ \"Transaction Number\", \"Date\", \"Description\", \"Amount Debit\", \"Amount Credit\", \"Balance\" ]\n  }\n}\n...\n```\n\n----------------------------------------\n\nTITLE: Installing Plugin Dependencies using Bundler (Shell)\nDESCRIPTION: Installs the required Ruby gems specified in the plugin's Gemfile using the `bundle install` command. This command should be run within the plugin's root directory to set up the development environment.\nSOURCE: https://github.com/elastic/logstash/blob/main/lib/pluginmanager/templates/codec-plugin/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Listing Installed Logstash Plugins - Shell\nDESCRIPTION: Retrieves a list of all currently available plugins in the Logstash installation using the plugin management tool. Shows plugins grouped by type (inputs, codecs, filters, outputs). Useful for debugging or determining available functionality within a Logstash instance.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/input-new-plugin.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nbin/logstash-plugin list\n```\n\n----------------------------------------\n\nTITLE: Specifying Required Dependencies in Logstash Filter Plugins (Ruby require statements)\nDESCRIPTION: These statements import the core dependencies needed for any Logstash filter plugin. 'logstash/filters/base' provides the base filter class, and 'logstash/namespace' manages plugin namespaces. These should be placed at the start of every plugin file and require the Ruby environment provided by Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/filter-new-plugin.md#_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"logstash/filters/base\"\nrequire \"logstash/namespace\"\n\n```\n\n----------------------------------------\n\nTITLE: Defining Plugin Metadata and Dependencies in `.gemspec` (Ruby)\nDESCRIPTION: Example `.gemspec` file for a Logstash codec plugin defining metadata (name, version, license, summary, etc.), file paths, test files, and Logstash-specific flags. It demonstrates declaring runtime dependencies (like `logstash-core-plugin-api` with version constraints) needed for the plugin to function, and development dependencies (like `logstash-devutils`) required only for testing or development.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_9\n\nLANGUAGE: Ruby\nCODE:\n```\nGem::Specification.new do |s|\n  s.name = 'logstash-codec-example'\n  s.version = '0.1.0'\n  s.licenses = ['Apache License (2.0)']\n  s.summary = \"This codec does x, y, z in Logstash\"\n  s.description = \"This gem is a logstash plugin required to be installed on top of the Logstash core pipeline using $LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\n  s.authors = [\"Elastic\"]\n  s.email = 'info@elastic.co'\n  s.homepage = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\n  s.require_paths = [\"lib\"]\n\n  # Files\n  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']\n   # Tests\n  s.test_files = s.files.grep(%r{^(test|spec|features)/})\n\n  # Special flag to let us know this is actually a logstash plugin\n  s.metadata = { \"logstash_plugin\" => \"true\", \"logstash_group\" => \"codec\" }\n\n  # Gem dependencies\n  s.add_runtime_dependency \"logstash-core-plugin-api\", \">= 1.60\", \"<= 2.99\"\n  s.add_development_dependency 'logstash-devutils'\nend\n```\n\n----------------------------------------\n\nTITLE: Testing a Logstash codec plugin with custom parameters\nDESCRIPTION: Command for running Logstash with a configuration that customizes the example codec plugin by setting the 'append' parameter to a custom string.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_22\n\nLANGUAGE: sh\nCODE:\n```\nbin/logstash -e 'input { stdin{ codec => example{ append => \", I am appending this! }} } output {stdout { codec => rubydebug }}'\n```\n\n----------------------------------------\n\nTITLE: Setting Tag with Environment Variable (Ruby)\nDESCRIPTION: This snippet shows how to set a tag's value using an environment variable.  The `add_tag` directive within a `mutate` filter is used. The  `${ENV_TAG}` syntax references the environment variable. The output of the configuration is dependent on what the environment variable resolves to.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/environment-variables.md#_snippet_3\n\nLANGUAGE: ruby\nCODE:\n```\nfilter {\n  mutate {\n    add_tag => [ \"tag1\", \"${ENV_TAG}\" ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Reading from a Specific Timestamp in Dead Letter Queue in YAML\nDESCRIPTION: A Logstash pipeline configuration that starts processing events from the dead letter queue at a specific timestamp. This is useful when you don't want to process all events in the queue, especially older ones.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/dead-letter-queues.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninput {\n  dead_letter_queue {\n    path => \"/path/to/data/dead_letter_queue\"\n    start_timestamp => \"2017-06-06T23:40:37\"\n    pipeline_id => \"main\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Input with SSL/TLS\nDESCRIPTION: This snippet shows how to configure the Logstash input plugin to use SSL/TLS for secure communication. It includes settings like `ssl_enabled`, `ssl_key`, `ssl_certificate`, `ssl_certificate_authorities`, and `ssl_client_authentication`. These settings allow Logstash to encrypt data and authenticate with clients.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ls-to-ls-native.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\ninput {\n  logstash {\n    ...\n\n    ssl_enabled => true\n    ssl_key => \"server.pkcs8.key\"\n    ssl_certificate => \"server.crt\"\n    ssl_certificate_authorities => \"ca.crt\"\n    ssl_client_authentication => required\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building a Logstash codec plugin gem\nDESCRIPTION: Command for building a Logstash codec plugin gem from a gemspec file. This creates a gem file with the version specified in the gemspec.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/extend/codec-new-plugin.md#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\ngem build logstash-codec-example.gemspec\n```\n\n----------------------------------------\n\nTITLE: Configuring GeoIP Filter Plugin ECS Compatibility in Logstash Pipeline (text)\nDESCRIPTION: This snippet shows how to configure a specific instance of the GeoIP filter plugin within a Logstash pipeline to disable ECS compatibility. The configuration sets the 'ecs_compatibility' option to 'disabled', causing this plugin instance to output data in its legacy format instead of ECS-compliant fields. The snippet expects this to be placed within a Logstash pipeline configuration file and only affects the scoped plugin instance.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/ecs-ls.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nfilter {\n  geoip {\n    source => \"[host][ip]\"\n    ecs_compatibility => disabled\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Failed Event in Dead Letter Queue with Error Metadata in JSON\nDESCRIPTION: An example of a failed event stored in the dead letter queue, including metadata about the error that caused the failure. The metadata shows details about the elasticsearch plugin that generated the error and the reason for failure.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/dead-letter-queues.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"@metadata\" => {\n    \"dead_letter_queue\" => {\n       \"entry_time\" => #<Java::OrgLogstash::Timestamp:0x5b5dacd5>,\n        \"plugin_id\" => \"fb80f1925088497215b8d037e622dec5819b503e-4\",\n      \"plugin_type\" => \"elasticsearch\",\n           \"reason\" => \"Could not index event to Elasticsearch. status: 400, action: [\\\"index\\\", {:_id=>nil, :_index=>\\\"logstash-2017.06.22\\\", :_type=>\\\"doc\\\", :_routing=>nil}, 2017-06-22T01:29:29.804Z My-MacBook-Pro-2.local {\\\"geoip\\\":{\\\"location\\\":\\\"home\\\"}}], response: {\\\"index\\\"=>{\\\"_index\\\"=>\\\"logstash-2017.06.22\\\", \\\"_type\\\"=>\\\"doc\\\", \\\"_id\\\"=>\\\"AVzNayPze1iR9yDdI2MD\\\", \\\"status\\\"=>400, \\\"error\\\"=>{\\\"type\\\"=>\\\"mapper_parsing_exception\\\", \\\"reason\\\"=>\\\"failed to parse\\\", \\\"caused_by\\\"=>{\\\"type\\\"=>\\\"illegal_argument_exception\\\", \\\"reason\\\"=>\\\"illegal latitude value [266.30859375] for geoip.location\\\"}}}}\"\n    }\n  },\n  \"@timestamp\" => 2017-06-22T01:29:29.804Z,\n    \"@version\" => \"1\",\n       \"geoip\" => {\n    \"location\" => \"home\"\n  },\n        \"host\" => \"My-MacBook-Pro-2.local\",\n     \"message\" => \"{\\\"geoip\\\":{\\\"location\\\":\\\"home\\\"}}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Monitoring Credentials in logstash.yml (YAML)\nDESCRIPTION: Configures the username and password in the `logstash.yml` file for Logstash to send monitoring data to a secured Elasticsearch cluster. Uses `xpack.monitoring.elasticsearch.username` and `xpack.monitoring.elasticsearch.password`, typically with the `logstash_system` built-in user after its password has been set.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/secure-connection.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nxpack.monitoring.elasticsearch.username: logstash_system\nxpack.monitoring.elasticsearch.password: t0p.s3cr3t\n```\n\n----------------------------------------\n\nTITLE: Configuring pipeline-to-pipeline communication in Logstash - YAML\nDESCRIPTION: This YAML configuration defines two pipelines within a single Logstash instance to enable pipeline-to-pipeline communication via virtual addresses. The upstream pipeline includes a standard input plugin (`stdin`) and sends events through the `pipeline` output plugin to a virtual address `myVirtualAddress`. The downstream pipeline listens on this virtual address with the `pipeline` input plugin. This setup demonstrates the use of the `pipeline` input/output plugins for inter-pipeline event transmission using virtual addressing within the same process.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/pipeline-to-pipeline.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# config/pipelines.yml\n- pipeline.id: upstream\n  config.string: input { stdin {} } output { pipeline { send_to => [myVirtualAddress] } }\n- pipeline.id: downstream\n  config.string: input { pipeline { address => myVirtualAddress } }\n```\n\n----------------------------------------\n\nTITLE: Updating Log Levels Dynamically Using REST API\nDESCRIPTION: Demonstrates changing the logging level of a specific subsystem or plugin at runtime through a PUT request, allowing for immediate increase or decrease in logging verbosity without restarting Logstash.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/logging.md#_snippet_5\n\nLANGUAGE: js\nCODE:\n```\ncurl -XPUT 'localhost:9600/_node/logging?pretty' -H 'Content-Type: application/json' -d' { \"logger.logstash.outputs.elasticsearch\" : \"DEBUG\" } '\n```\n\n----------------------------------------\n\nTITLE: Re-enabling the logstash_system user in Elasticsearch\nDESCRIPTION: This API call re-enables the logstash_system user in Elasticsearch. After upgrading from older versions, this built-in user is disabled for security reasons and must be re-enabled for monitoring to work.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/monitoring-troubleshooting.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nPUT _security/user/logstash_system/_enable\n```\n\n----------------------------------------\n\nTITLE: Remove macOS Quarantine Attribute - Specific Directory\nDESCRIPTION: This command removes the quarantine attribute specifically from the `logstash-{{version}}` directory on macOS.  It is used to address Gatekeeper issues. This example assumes `logstash-{{version}}` is the extracted Logstash directory. It requires the `xattr` command-line tool.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/first-event.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nxattr -d -r com.apple.quarantine logstash-{{version}}\n```\n\n----------------------------------------\n\nTITLE: Creating Logstash CPM User API Key in Elasticsearch\nDESCRIPTION: Creates an API key for a Central Pipeline Management (CPM) user with permissions to manage Logstash pipelines and monitor the cluster. The key expires after 365 days.\nSOURCE: https://github.com/elastic/logstash/blob/main/ci/serverless/README.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\nPOST /_security/api_key\n{\n  \"name\": \"cpm_user\",\n  \"expiration\": \"365d\",   \n  \"role_descriptors\": { \n    \"cpm_user_role\": {\n      \"cluster\": [\"manage_logstash_pipelines\", \"monitor\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Output Conditional Example - Logstash Output\nDESCRIPTION: This snippet demonstrates the use of conditionals within the output section of a Logstash configuration. It routes events to different outputs (nagios, Elasticsearch, and statsd) based on the 'type' and 'status' fields.\nSOURCE: https://github.com/elastic/logstash/blob/main/docs/reference/config-examples.md#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\noutput {\n  if [type] == \"apache\" {\n    if [status] =~ /^5\\d\\d/ {\n      nagios { ...  }\n    } else if [status] =~ /^4\\d\\d/ {\n      elasticsearch { ... }\n    }\n    statsd { increment => \"apache.%{status}\" }\n  }\n}\n```"
  }
]
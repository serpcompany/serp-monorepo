[
  {
    "owner": "starpig1129",
    "repo": "datagen",
    "content": "TITLE: Creating Hypothesis Agent with LangChain Tools in Python\nDESCRIPTION: Imports necessary tools and creates a hypothesis agent using LangChain. The agent is equipped with various tools for data collection, web scraping, and information retrieval.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/main.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom tools.internet import google_search,scrape_webpages_with_fallback\nfrom tools.basetool import execute_code,execute_command\nfrom tools.FileEdit import create_document,read_document,edit_document,collect_data\nfrom langchain.agents import load_tools\nfrom langchain_community.tools import WikipediaQueryRun\nfrom langchain_community.utilities import WikipediaAPIWrapper\nwikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\nhypothesis_agent = create_agent(\nllm, \n[collect_data,wikipedia,google_search,scrape_webpages_with_fallback]+load_tools([\"arxiv\"],),\n'''\nAs an esteemed expert in data analysis, your task is to formulate a set of research hypotheses and outline the steps to be taken based on the information table provided. Utilize statistics, machine learning, deep learning, and artificial intelligence in developing these hypotheses. Your hypotheses should be precise, achievable, professional, and innovative. To ensure the feasibility and uniqueness of your hypotheses, thoroughly investigate relevant information. For each hypothesis, include ample references to support your claims.\n\nUpon analyzing the information table, you are required to:\n\n1. Formulate research hypotheses that leverage statistics, machine learning, deep learning, and AI techniques.\n2. Outline the steps involved in testing these hypotheses.\n3. Verify the feasibility and uniqueness of each hypothesis through a comprehensive literature review.\n\nAt the conclusion of your analysis, present the complete research hypotheses, elaborate on their uniqueness and feasibility, and provide relevant references to support your assertions. Please answer in structured way to enhance readability.\nJust answer a research hypothesis.\n''',\nmembers,WORKING_DIRECTORY)\n```\n\n----------------------------------------\n\nTITLE: Initializing Environment and Language Models for Multi-Agent System in Python\nDESCRIPTION: This snippet sets up environment variables, initializes ChatOpenAI models with different configurations, and creates a working directory. It uses a custom logger for tracking the initialization process and handles potential errors.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/main.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom logger import setup_logger\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph\nfrom load_cfg import OPENAI_API_KEY,LANGCHAIN_API_KEY,WORKING_DIRECTORY\n# Set environment variables\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\nos.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"Multi-Agent Data Analysis System\"\n\n# Set up logger\nlogger = setup_logger()\n\n# Initialize language models\ntry:\n    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=4096)\n    power_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.5, max_tokens=4096)\n    json_llm = ChatOpenAI(\n        model=\"gpt-4o\",\n        model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n        temperature=0,\n        max_tokens=4096\n    )\n    logger.info(\"Language models initialized successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error initializing language models: {str(e)}\")\n    raise\n\n# Ensure working directory exists\nif not os.path.exists(WORKING_DIRECTORY):\n    os.makedirs(WORKING_DIRECTORY)\n    logger.info(f\"Created working directory: {WORKING_DIRECTORY}\")\n\nlogger.info(\"Initialization complete.\")\n```\n\n----------------------------------------\n\nTITLE: Creating Process Supervisor Agent in Python\nDESCRIPTION: Defines a process supervisor agent responsible for overseeing the entire research project. This agent coordinates tasks between other specialized agents and ensures the completion of a comprehensive research report.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/main.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprocess_agent = create_supervisor(\n    power_llm,\n    \"\"\"\n    You are a research supervisor responsible for overseeing and coordinating a comprehensive data analysis project, resulting in a complete and cohesive research report. Your primary tasks include:\n\n    1. Validating and refining the research hypothesis to ensure it is clear, specific, and testable.\n    2. Orchestrating a thorough data analysis process, with all code well-documented and reproducible.\n    3. Compiling and refining a research report that includes:\n        - Introduction\n        - Hypothesis\n        - Methodology\n        - Results, accompanied by relevant visualizations\n        - Discussion\n        - Conclusion\n        - References\n\n    **Step-by-Step Process:**\n    1. **Planning:** Define clear objectives and expected outcomes for each phase of the project.\n    2. **Task Assignment:** Assign specific tasks to the appropriate agents (\\\"Visualization,\\\" \\\"Search,\\\" \\\"Coder,\\\" \\\"Report\\\").\n    3. **Review and Integration:** Critically review and integrate outputs from each agent, ensuring consistency, quality, and relevance.\n    4. **Feedback:** Provide feedback and further instructions as needed to refine outputs.\n    5. **Final Compilation:** Ensure all components are logically connected and meet high academic standards.\n\n    **Agent Guidelines:**\n    - **Visualization Agent:** Develop and explain data visualizations that effectively communicate key findings.\n    - **Search Agent:** Collect and summarize relevant information, and compile a comprehensive list of references.\n    - **Coder Agent:** Write and document efficient Python code for data analysis, ensuring that the code is clean and reproducible.\n    - **Report Agent:** Draft, refine, and finalize the research report, integrating inputs from all agents and ensuring the narrative is clear and cohesive.\n\n    **Workflow:**\n    1. Plan the overall analysis and reporting process.\n    2. Assign tasks to the appropriate agents and oversee their progress.\n    3. Continuously review and integrate the outputs from each agent, ensuring that each contributes effectively to the final report.\n    4. Adjust the analysis and reporting process based on emerging results and insights.\n    5. Compile the final report, ensuring all sections are complete and well-integrated.\n\n    **Completion Criteria:**\n    Respond with \\\"FINISH\\\" only when:\n    1. The hypothesis has been thoroughly tested and validated.\n    2. The data analysis is complete, with all code documented and reproducible.\n    3. All required visualizations have been created, properly labeled, and explained.\n    4. The research report is comprehensive, logically structured, and includes all necessary sections.\n    5. The reference list is complete and accurately cited.\n    6. All components are cohesively integrated into a polished final report.\n\n    Ensure that the final report delivers a clear, insightful analysis, addressing all aspects of the hypothesis and meeting the highest academic standards.\n    \"\"\",\n    [\"Visualization\", \"Search\", \"Coder\", \"Report\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Specialized Agents for Data Analysis Project in Python\nDESCRIPTION: Defines multiple specialized agents including visualization, coding, search, and report writing agents. Each agent is created with specific tools and instructions tailored to their role in the research process.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/main.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nvisualization_agent = create_agent(\n    llm, \n    [read_document, execute_code, execute_command],\n    \"\"\"\n    You are a data visualization expert tasked with creating insightful visual representations of data. Your primary responsibilities include:\n    \n    1. Designing appropriate visualizations that clearly communicate data trends and patterns.\n    2. Selecting the most suitable chart types (e.g., bar charts, scatter plots, heatmaps) for different data types and analytical purposes.\n    3. Providing executable Python code (using libraries such as matplotlib, seaborn, or plotly) that generates these visualizations.\n    4. Including well-defined titles, axis labels, legends, and saving the visualizations as files.\n    5. Offering brief but clear interpretations of the visual findings.\n\n    **File Saving Guidelines:**\n    - Save all visualizations as files with descriptive and meaningful filenames.\n    - Ensure filenames are structured to easily identify the content (e.g., 'sales_trends_2024.png' for a sales trend chart).\n    - Confirm that the saved files are organized in the working directory, making them easy for other agents to locate and use.\n\n    **Constraints:**\n    - Focus solely on visualization tasks; do not perform data analysis or preprocessing.\n    - Ensure all visual elements are suitable for the target audience, with attention to color schemes and design principles.\n    - Avoid over-complicating visualizations; aim for clarity and simplicity.\n    \"\"\",\n    members,WORKING_DIRECTORY\n    )\n\ncode_agent = create_agent(\n    power_llm,\n    [read_document,execute_code, execute_command],\n    \"\"\"\n    You are an expert Python programmer specializing in data processing and analysis. Your main responsibilities include:\n\n    1. Writing clean, efficient Python code for data manipulation, cleaning, and transformation.\n    2. Implementing statistical methods and machine learning algorithms as needed.\n    3. Debugging and optimizing existing code for performance improvements.\n    4. Adhering to PEP 8 standards and ensuring code readability with meaningful variable and function names.\n\n    Constraints:\n    - Focus solely on data processing tasks; do not generate visualizations or write non-Python code.\n    - Provide only valid, executable Python code, including necessary comments for complex logic.\n    - Avoid unnecessary complexity; prioritize readability and efficiency.\n    \"\"\",\n    members,WORKING_DIRECTORY\n)\n\nsearcher_agent= create_agent(\n    llm,\n    [create_document,read_document, collect_data,wikipedia,google_search,scrape_webpages_with_fallback]+load_tools([\"arxiv\"],),\n    \"\"\"\n    You are a skilled research assistant responsible for gathering and summarizing relevant information. Your main tasks include:\n\n    1. Conducting thorough literature reviews using academic databases and reputable online sources.\n    2. Summarizing key findings in a clear, concise manner.\n    3. Providing citations for all sources, prioritizing peer-reviewed and academically reputable materials.\n\n    Constraints:\n    - Focus exclusively on information retrieval and summarization; do not engage in data analysis or processing.\n    - Present information in an organized format, with clear attributions to sources.\n    - Evaluate the credibility of sources and prioritize high-quality, reliable information.\n    \"\"\",\n    members,WORKING_DIRECTORY\n    )\n\nreport_agent = create_agent(\n    power_llm, \n    [create_document, read_document, edit_document], \n    \"\"\"\n    You are an experienced scientific writer tasked with drafting comprehensive research reports. Your primary duties include:\n\n    1. Clearly stating the research hypothesis and objectives in the introduction.\n    2. Detailing the methodology used, including data collection and analysis techniques.\n    3. Structuring the report into coherent sections (e.g., Introduction, Methodology, Results, Discussion, Conclusion).\n    4. Synthesizing information from various sources into a unified narrative.\n    5. Integrating relevant data visualizations and ensuring they are appropriately referenced and explained.\n\n    Constraints:\n    - Focus solely on report writing; do not perform data analysis or create visualizations.\n    - Maintain an objective, academic tone throughout the report.\n    - Cite all sources using APA style and ensure that all findings are supported by evidence.\n    \"\"\",\n    members,WORKING_DIRECTORY\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Core Components for Multi-Agent System in Python\nDESCRIPTION: This snippet imports core components for the multi-agent system, including state management, node types, agent creation functions, and routing logic. These imports set up the structure for building the multi-agent data analysis system.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/main.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom core.state import State\nfrom core.node import agent_node,human_choice_node,note_agent_node,human_review_node,refiner_node\nfrom create_agent import create_agent,create_supervisor\nfrom core.router import QualityReview_router,hypothesis_router,process_router\n```\n\n----------------------------------------\n\nTITLE: Creating Quality Review Agent in Python\nDESCRIPTION: Defines a quality review agent responsible for evaluating and ensuring the high standard of all research outputs. This agent focuses on critically reviewing content, methodology, and conclusions of research reports.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/main.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nquality_review_agent=create_agent(\n    llm, \n    [create_document,read_document,edit_document], \n    '''\n    You are a meticulous quality control expert responsible for reviewing and ensuring the high standard of all research outputs. Your tasks include:\n\n    1. Critically evaluating the content, methodology, and conclusions of research reports.\n    2. Checking for consistency, accuracy, and clarity in all documents.\n    3. Identifying areas that need improvement or further elaboration.\n    ''',\n    members,WORKING_DIRECTORY\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing StateGraph Workflow in Python\nDESCRIPTION: Creates a StateGraph object for managing workflow states. Defines a list of member roles for the project.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/main.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nworkflow = StateGraph(State)\n\nmembers = [\"Hypothesis\",\"Process\",\"Visualization\", \"Search\", \"Coder\", \"Report\", \"QualityReview\",\"Refiner\"]\n```\n\n----------------------------------------\n\nTITLE: Customizing DATAGEN Input in Python\nDESCRIPTION: Example of how to modify the user input in the main Python script to analyze a different dataset.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/README.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nuser_input = '''\ndatapath:YourDataName.csv\nUse machine learning to perform data analysis and write complete graphical reports\n'''\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies in requirements.txt\nDESCRIPTION: This requirements.txt file specifies the exact versions of Python packages needed for the datagen project. It includes libraries for working with language models (langchain, openai), web scraping (beautifulsoup4, selenium), data processing (pandas), and various APIs (arxiv, wikipedia).\nSOURCE: https://github.com/starpig1129/datagen/blob/main/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\narxiv==2.1.3\nbeautifulsoup4==4.13.3\nlangchain==0.3.12\nlangchain-community==0.3.12\nlangchain-openai==0.2.13\nlanggraph==0.2.73\npandas==2.2.2\npython-dotenv==1.0.1\nselenium==4.27.1\nwikipedia==1.4.0\nfirecrawl-py==0.0.20\nopenai==1.55.3\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for DATAGEN\nDESCRIPTION: Example of environment variables configuration for DATAGEN, including paths and API keys.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/README.md#2025-04-21_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# Your data storage path(required)\nDATA_STORAGE_PATH =./data_storage/\n\n# Anaconda installation path(required)\nCONDA_PATH = /home/user/anaconda3\n\n# Conda environment name(required)\nCONDA_ENV = envname\n\n# ChromeDriver executable path(required)\nCHROMEDRIVER_PATH =./chromedriver-linux64/chromedriver\n\n# Firecrawl API key (optional)\n# Note: If this key is missing, query capabilities may be reduced\nFIRECRAWL_API_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n\n# OpenAI API key (required)\n# Warning: This key is essential; the program will not run without it\nOPENAI_API_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n\n# LangChain API key (optional)\n# Used for monitoring the processing\nLANGCHAIN_API_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n```\n\n----------------------------------------\n\nTITLE: Installing DATAGEN Dependencies\nDESCRIPTION: Command to install the required dependencies for DATAGEN using pip.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running DATAGEN Python Script\nDESCRIPTION: Command to run the main Python script for DATAGEN.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment\nDESCRIPTION: Commands to create and activate a Conda virtual environment for DATAGEN.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n data_assistant python=3.10\nconda activate data_assistant\n```\n\n----------------------------------------\n\nTITLE: Cloning DATAGEN Repository\nDESCRIPTION: Command to clone the DATAGEN repository from GitHub.\nSOURCE: https://github.com/starpig1129/datagen/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/starpig1129/DATAGEN.git\n```"
  }
]
[
  {
    "owner": "vercel",
    "repo": "ai",
    "content": "TITLE: Installing OpenAI Provider for AI SDK (Bash)\nDESCRIPTION: This command installs the necessary `@ai-sdk/openai` package using the npm package manager. This package provides the OpenAI language model support for the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/openai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/openai\n```\n\n----------------------------------------\n\nTITLE: Installing Amazon Bedrock Provider with Package Managers\nDESCRIPTION: Commands to install the Amazon Bedrock provider module using different package managers (pnpm, npm, or yarn).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/amazon-bedrock\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/amazon-bedrock\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/amazon-bedrock\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Compatible SDK with yarn\nDESCRIPTION: Installs the `@ai-sdk/openai-compatible` package using the yarn package manager. This dependency allows the Vercel AI SDK to communicate with OpenAI-compatible APIs like NVIDIA NIM.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/35-nim.mdx#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nyarn add @ai-sdk/openai-compatible\n```\n\n----------------------------------------\n\nTITLE: Installing Replicate Integration via npm (Shell)\nDESCRIPTION: Shows the installation step using npm, the Node.js package manager. Installs both ai and @ai-sdk/replicate required for the examples. This is a prerequisite for running any of the TypeScript code demonstrating Replicate provider usage.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/60-replicate.mdx#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm install ai @ai-sdk/replicate\n```\n\n----------------------------------------\n\nTITLE: Importing Default Hume Provider\nDESCRIPTION: Shows how to import the default Hume provider instance from the @ai-sdk/hume package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/150-hume.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { hume } from '@ai-sdk/hume';\n```\n\n----------------------------------------\n\nTITLE: Streaming AI-Generated Text Using pipeDataStreamToResponse in Express\nDESCRIPTION: This example demonstrates how to use the AI SDK to stream text from OpenAI's GPT-4o model to an Express server response using the pipeDataStreamToResponse method. It creates a basic POST endpoint that generates and streams AI text responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/20-express.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport express, { Request, Response } from 'express';\n\nconst app = express();\n\napp.post('/', async (req: Request, res: Response) => {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    prompt: 'Invent a new holiday and describe its traditions.',\n  });\n\n  result.pipeDataStreamToResponse(res);\n});\n\napp.listen(8080, () => {\n  console.log(`Example app listening on port ${8080}`);\n});\n```\n\n----------------------------------------\n\nTITLE: Running All AI SDK Codemods using npx\nDESCRIPTION: This shell command uses npx to execute the `@ai-sdk/codemod` package with the `upgrade` argument. It applies all available codemods to the project, intended to be run from the project's root directory. This helps automate codebase updates required by AI SDK changes. Requires Node.js and npm/npx to be installed.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/codemod/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnpx @ai-sdk/codemod upgrade\n```\n\n----------------------------------------\n\nTITLE: Generating Text with OpenAI and AI SDK (TypeScript)\nDESCRIPTION: This example demonstrates how to generate text using the Vercel AI SDK with the OpenAI provider. It imports the `openai` provider and the `generateText` function, then calls `generateText` specifying the OpenAI model ('gpt-4-turbo') and a prompt. The result contains the generated text. Dependencies include `@ai-sdk/openai` and the core `ai` package.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/openai/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: openai('gpt-4-turbo'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Zod Schema for Recipe Validation\nDESCRIPTION: Example of defining a Zod schema for validating recipe data structures. The schema specifies a recipe object with name, ingredients array with name and amount fields, and steps array of strings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/04-tools.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport z from 'zod';\n\nconst recipeSchema = z.object({\n  recipe: z.object({\n    name: z.string(),\n    ingredients: z.array(\n      z.object({\n        name: z.string(),\n        amount: z.string(),\n      }),\n    ),\n    steps: z.array(z.string()),\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Chatbot UI with useChat Hook in React\nDESCRIPTION: This snippet demonstrates the basic usage of the useChat hook to create a simple chatbot interface. It includes message rendering, input handling, and form submission.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({});\n\n  return (\n    <>\n      {messages.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input name=\"prompt\" value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Submit</button>\n      </form>\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Local Caching Middleware in TypeScript\nDESCRIPTION: A comprehensive middleware implementation for caching AI API responses locally. Includes functionality for storing and retrieving both complete responses and streaming tokens, with utilities for cache file management and request normalization.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/80-local-caching-middleware.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  type LanguageModelV1,\n  type LanguageModelV1Middleware,\n  LanguageModelV1Prompt,\n  type LanguageModelV1StreamPart,\n  simulateReadableStream,\n  wrapLanguageModel,\n} from 'ai';\nimport 'dotenv/config';\nimport fs from 'fs';\nimport path from 'path';\n\nconst CACHE_FILE = path.join(process.cwd(), '.cache/ai-cache.json');\n\nexport const cached = (model: LanguageModelV1) =>\n  wrapLanguageModel({\n    middleware: cacheMiddleware,\n    model,\n  });\n\nconst ensureCacheFile = () => {\n  const cacheDir = path.dirname(CACHE_FILE);\n  if (!fs.existsSync(cacheDir)) {\n    fs.mkdirSync(cacheDir, { recursive: true });\n  }\n  if (!fs.existsSync(CACHE_FILE)) {\n    fs.writeFileSync(CACHE_FILE, '{}');\n  }\n};\n\nconst getCachedResult = (key: string | object) => {\n  ensureCacheFile();\n  const cacheKey = typeof key === 'object' ? JSON.stringify(key) : key;\n  try {\n    const cacheContent = fs.readFileSync(CACHE_FILE, 'utf-8');\n\n    const cache = JSON.parse(cacheContent);\n\n    const result = cache[cacheKey];\n\n    return result ?? null;\n  } catch (error) {\n    console.error('Cache error:', error);\n    return null;\n  }\n};\n\nconst updateCache = (key: string, value: any) => {\n  ensureCacheFile();\n  try {\n    const cache = JSON.parse(fs.readFileSync(CACHE_FILE, 'utf-8'));\n    const updatedCache = { ...cache, [key]: value };\n    fs.writeFileSync(CACHE_FILE, JSON.stringify(updatedCache, null, 2));\n    console.log('Cache updated for key:', key);\n  } catch (error) {\n    console.error('Failed to update cache:', error);\n  }\n};\nconst cleanPrompt = (prompt: LanguageModelV1Prompt) => {\n  return prompt.map(m => {\n    if (m.role === 'assistant') {\n      return m.content.map(part =>\n        part.type === 'tool-call' ? { ...part, toolCallId: 'cached' } : part,\n      );\n    }\n    if (m.role === 'tool') {\n      return m.content.map(tc => ({\n        ...tc,\n        toolCallId: 'cached',\n        result: {},\n      }));\n    }\n\n    return m;\n  });\n};\n\nexport const cacheMiddleware: LanguageModelV1Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    const cacheKey = JSON.stringify({\n      ...cleanPrompt(params.prompt),\n      _function: 'generate',\n    });\n    console.log('Cache Key:', cacheKey);\n\n    const cached = getCachedResult(cacheKey) as Awaited<\n      ReturnType<LanguageModelV1['doGenerate']>\n    > | null;\n\n    if (cached && cached !== null) {\n      console.log('Cache Hit');\n      return {\n        ...cached,\n        response: {\n          ...cached.response,\n          timestamp: cached?.response?.timestamp\n            ? new Date(cached?.response?.timestamp)\n            : undefined,\n        },\n      };\n    }\n\n    console.log('Cache Miss');\n    const result = await doGenerate();\n\n    updateCache(cacheKey, result);\n\n    return result;\n  },\n  wrapStream: async ({ doStream, params }) => {\n    const cacheKey = JSON.stringify({\n      ...cleanPrompt(params.prompt),\n      _function: 'stream',\n    });\n    console.log('Cache Key:', cacheKey);\n\n    // Check if the result is in the cache\n    const cached = getCachedResult(cacheKey);\n\n    // If cached, return a simulated ReadableStream that yields the cached result\n    if (cached && cached !== null) {\n      console.log('Cache Hit');\n      // Format the timestamps in the cached response\n      const formattedChunks = (cached as LanguageModelV1StreamPart[]).map(p => {\n        if (p.type === 'response-metadata' && p.timestamp) {\n          return { ...p, timestamp: new Date(p.timestamp) };\n        } else return p;\n      });\n      return {\n        stream: simulateReadableStream({\n          initialDelayInMs: 0,\n          chunkDelayInMs: 10,\n          chunks: formattedChunks,\n        }),\n        rawCall: { rawPrompt: null, rawSettings: {} },\n      };\n    }\n\n    console.log('Cache Miss');\n    // If not cached, proceed with streaming\n    const { stream, ...rest } = await doStream();\n\n    const fullResponse: LanguageModelV1StreamPart[] = [];\n\n    const transformStream = new TransformStream<\n      LanguageModelV1StreamPart,\n      LanguageModelV1StreamPart\n    >({\n      transform(chunk, controller) {\n        fullResponse.push(chunk);\n        controller.enqueue(chunk);\n      },\n      flush() {\n        // Store the full response in the cache after streaming is complete\n        updateCache(cacheKey, fullResponse);\n      },\n    });\n\n    return {\n      stream: stream.pipeThrough(transformStream),\n      ...rest,\n    };\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-Side Object Generation with Zod Schema Validation\nDESCRIPTION: A server action that uses the generateObject function from the AI SDK to generate structured notification data. It defines a zod schema that specifies the exact structure of the notifications, including name, message, and timing information.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/30-generate-object.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n'use server';\n\nimport { generateObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nexport async function getNotifications(input: string) {\n  'use server';\n\n  const { object: notifications } = await generateObject({\n    model: openai('gpt-4-turbo'),\n    system: 'You generate three notifications for a messages app.',\n    prompt: input,\n    schema: z.object({\n      notifications: z.array(\n        z.object({\n          name: z.string().describe('Name of a fictional person.'),\n          message: z.string().describe('Do not use emojis or links.'),\n          minutesAgo: z.number(),\n        }),\n      ),\n    }),\n  });\n\n  return { notifications };\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Chart Configuration Schema with Zod in TypeScript\nDESCRIPTION: This snippet defines a Zod schema (`configSchema`) for validating chart configuration objects. It specifies properties like chart type, axes keys (xKey, yKeys), colors, legend visibility, and includes descriptive fields (`description`, `takeaway`) to guide the AI model. The schema uses Zod's `.describe()` method extensively to provide context for each field, aiding the AI in generating accurate configurations. An inferred TypeScript type `Config` is also exported.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_12\n\nLANGUAGE: ts\nCODE:\n```\n/* ...rest of the file... */\n\nexport const configSchema = z\n  .object({\n    description: z\n      .string()\n      .describe(\n        'Describe the chart. What is it showing? What is interesting about the way the data is displayed?',\n      ),\n    takeaway: z.string().describe('What is the main takeaway from the chart?'),\n    type: z.enum(['bar', 'line', 'area', 'pie']).describe('Type of chart'),\n    title: z.string(),\n    xKey: z.string().describe('Key for x-axis or category'),\n    yKeys: z\n      .array(z.string())\n      .describe(\n        'Key(s) for y-axis values this is typically the quantitative column',\n      ),\n    multipleLines: z\n      .boolean()\n      .describe(\n        'For line charts only: whether the chart is comparing groups of data.',\n      )\n      .optional(),\n    measurementColumn: z\n      .string()\n      .describe(\n        'For line charts only: key for quantitative y-axis column to measure against (eg. values, counts etc.)',\n      )\n      .optional(),\n    lineCategories: z\n      .array(z.string())\n      .describe(\n        'For line charts only: Categories used to compare different lines or data series. Each category represents a distinct line in the chart.',\n      )\n      .optional(),\n    colors: z\n      .record(\n        z.string().describe('Any of the yKeys'),\n        z.string().describe('Color value in CSS format (e.g., hex, rgb, hsl)'),\n      )\n      .describe('Mapping of data keys to color values for chart elements')\n      .optional(),\n    legend: z.boolean().describe('Whether to show legend'),\n  })\n  .describe('Chart configuration object');\n\nexport type Config = z.infer<typeof configSchema>;\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Embedding Model with Custom Options in TypeScript\nDESCRIPTION: This code shows how to pass optional settings such as 'dimensions' and 'user' to the OpenAI embedding model factory. By supplying an options object as the second parameter to .embedding(), users can customize the dimensionality of output embeddings and provide a unique user identifier for tracking and safety. Only models supporting custom dimensions (e.g., 'text-embedding-3') accept the 'dimensions' parameter.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_27\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = openai.embedding('text-embedding-3-large', {\n  dimensions: 512 // optional, number of dimensions for the embedding\n  user: 'test-user' // optional unique user identifier\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-Side Logic for Multi-Step Tool Calling in Next.js\nDESCRIPTION: This snippet demonstrates the server-side implementation for handling multi-step tool calls. It defines two tools (getLocation and getWeather) and uses the streamText function from the AI SDK to generate responses that can involve multiple tool invocations.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/72-call-tools-multiple-steps.mdx#2025-04-23_snippet_1\n\nLANGUAGE: ts\nCODE:\n```\nimport { ToolInvocation, streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\ninterface Message {\n  role: 'user' | 'assistant';\n  content: string;\n  toolInvocations?: ToolInvocation[];\n}\n\nfunction getLocation({ lat, lon }) {\n  return { lat: 37.7749, lon: -122.4194 };\n}\n\nfunction getWeather({ lat, lon, unit }) {\n  return { value: 25, description: 'Sunny' };\n}\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: Message[] } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'You are a helpful assistant.',\n    messages,\n    tools: {\n      getLocation: {\n        description: 'Get the location of the user',\n        parameters: z.object({}),\n        execute: async () => {\n          const { lat, lon } = getLocation();\n          return `Your location is at latitude ${lat} and longitude ${lon}`;\n        },\n      },\n      getWeather: {\n        description: 'Get the weather for a location',\n        parameters: z.object({\n          lat: z.number().describe('The latitude of the location'),\n          lon: z.number().describe('The longitude of the location'),\n          unit: z\n            .enum(['C', 'F'])\n            .describe('The unit to display the temperature in'),\n        }),\n        execute: async ({ lat, lon, unit }) => {\n          const { value, description } = getWeather({ lat, lon, unit });\n          return `It is currently ${value}Â°${unit} and ${description}!`;\n        },\n      },\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Using generateText with OpenAI Model\nDESCRIPTION: Example showing how to use the generateText function with OpenAI's GPT-4 model to generate creative text content. The function returns an object containing the generated text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: openai('gpt-4o'),\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Creating a Client Chat Interface with useAssistant Hook in Next.js\nDESCRIPTION: This code demonstrates how to build a client-side chat interface that streams responses from an OpenAI Assistant. It uses the useAssistant hook from @ai-sdk/react to manage the chat state, handle user input, and display messages in real-time with status updates.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/120-stream-assistant-response.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { Message, useAssistant } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { status, messages, input, submitMessage, handleInputChange } =\n    useAssistant({ api: '/api/assistant' });\n\n  return (\n    <div className=\"flex flex-col gap-2\">\n      <div className=\"p-2\">status: {status}</div>\n\n      <div className=\"flex flex-col p-2 gap-2\">\n        {messages.map((message: Message) => (\n          <div key={message.id} className=\"flex flex-row gap-2\">\n            <div className=\"w-24 text-zinc-500\">{`${message.role}: `}</div>\n            <div className=\"w-full\">{message.content}</div>\n          </div>\n        ))}\n      </div>\n\n      <form onSubmit={submitMessage} className=\"fixed bottom-0 p-2 w-full\">\n        <input\n          disabled={status !== 'awaiting_message'}\n          value={input}\n          onChange={handleInputChange}\n          className=\"bg-zinc-100 w-full p-2\"\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Building a Chat UI with Next.js and the useChat Hook\nDESCRIPTION: Implements a client-side chat interface using Next.js and the AI SDK's useChat hook. This component displays chat messages, including reasoning steps, and handles user input submissions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/20-sonnet-3-7.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, handleInputChange, handleSubmit, error } = useChat();\n\n  return (\n    <>\n      {messages.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, index) => {\n            // text parts:\n            if (part.type === 'text') {\n              return <div key={index}>{part.text}</div>;\n            }\n            // reasoning parts:\n            if (part.type === 'reasoning') {\n              return (\n                <pre key={index}>\n                  {part.details.map(detail =>\n                    detail.type === 'text' ? detail.text : '<redacted>',\n                  )}\n                </pre>\n              );\n            }\n          })}\n        </div>\n      ))}\n      <form onSubmit={handleSubmit}>\n        <input name=\"prompt\" value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Submit</button>\n      </form>\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Chat Interface with useChat Hook in TypeScript React\nDESCRIPTION: This code snippet demonstrates how to create a basic chat interface using the useChat hook from the AI SDK. It renders messages and provides an input form for user interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>\n          <div>{message.content}</div>\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={handleInputChange}\n          placeholder=\"Type a message...\"\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Conversation UI with React in TypeScript\nDESCRIPTION: This code snippet creates a simple conversation interface using React hooks. It allows users to input messages and displays the conversation history. The 'Send Message' button triggers the continueConversation function to generate responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/50-call-tools.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { Message, continueConversation } from './actions';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [conversation, setConversation] = useState<Message[]>([]);\n  const [input, setInput] = useState<string>('');\n\n  return (\n    <div>\n      <div>\n        {conversation.map((message, index) => (\n          <div key={index}>\n            {message.role}: {message.content}\n          </div>\n        ))}\n      </div>\n\n      <div>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button\n          onClick={async () => {\n            const { messages } = await continueConversation([\n              ...conversation,\n              { role: 'user', content: input },\n            ]);\n\n            setConversation(messages);\n          }}\n        >\n          Send Message\n        </button>\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Server Actions with Flight Search Tools in TypeScript\nDESCRIPTION: Defines server actions and tools for flight search functionality using streamUI from AI SDK. Implements searchFlights and lookupFlight tools with type-safe parameters using Zod schema validation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/04-multistep-interfaces.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { streamUI } from 'ai/rsc';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nconst searchFlights = async (\n  source: string,\n  destination: string,\n  date: string,\n) => {\n  return [\n    {\n      id: '1',\n      flightNumber: 'AA123',\n    },\n    {\n      id: '2',\n      flightNumber: 'AA456',\n    },\n  ];\n};\n\nconst lookupFlight = async (flightNumber: string) => {\n  return {\n    flightNumber: flightNumber,\n    departureTime: '10:00 AM',\n    arrivalTime: '12:00 PM',\n  };\n};\n\nexport async function submitUserMessage(input: string) {\n  'use server';\n\n  const ui = await streamUI({\n    model: openai('gpt-4o'),\n    system: 'you are a flight booking assistant',\n    prompt: input,\n    text: async ({ content }) => <div>{content}</div>,\n    tools: {\n      searchFlights: {\n        description: 'search for flights',\n        parameters: z.object({\n          source: z.string().describe('The origin of the flight'),\n          destination: z.string().describe('The destination of the flight'),\n          date: z.string().describe('The date of the flight'),\n        }),\n        generate: async function* ({ source, destination, date }) {\n          yield `Searching for flights from ${source} to ${destination} on ${date}...`;\n          const results = await searchFlights(source, destination, date);\n\n          return (\n            <div>\n              {results.map(result => (\n                <div key={result.id}>\n                  <div>{result.flightNumber}</div>\n                </div>\n              ))}\n            </div>\n          );\n        },\n      },\n      lookupFlight: {\n        description: 'lookup details for a flight',\n        parameters: z.object({\n          flightNumber: z.string().describe('The flight number'),\n        }),\n        generate: async function* ({ flightNumber }) {\n          yield `Looking up details for flight ${flightNumber}...`;\n          const details = await lookupFlight(flightNumber);\n\n          return (\n            <div>\n              <div>Flight Number: {details.flightNumber}</div>\n              <div>Departure Time: {details.departureTime}</div>\n              <div>Arrival Time: {details.arrivalTime}</div>\n            </div>\n          );\n        },\n      },\n    },\n  });\n\n  return ui.value;\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Chart Configuration with OpenAI via Server Action in TypeScript\nDESCRIPTION: This TypeScript code defines a Next.js Server Action `generateChartConfig`. It takes SQL query results (`results`) and the original user query (`userQuery`) as input. It uses the `generateObject` function from the Vercel AI SDK with the OpenAI 'gpt-4o' model to generate a chart configuration object conforming to the previously defined `configSchema`. The prompt guides the AI to visualize the data effectively based on the user's query. Finally, it overrides the generated colors with `shadcn` theme colors and returns the complete configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_13\n\nLANGUAGE: ts\nCODE:\n```\n/* ...other imports... */\nimport { Config, configSchema, explanationsSchema, Result } from '@/lib/types';\n\n/* ...rest of the file... */\n\nexport const generateChartConfig = async (\n  results: Result[],\n  userQuery: string,\n) => {\n  'use server';\n\n  try {\n    const { object: config } = await generateObject({\n      model: openai('gpt-4o'),\n      system: 'You are a data visualization expert.',\n      prompt: `Given the following data from a SQL query result, generate the chart config that best visualises the data and answers the users query.\n      For multiple groups use multi-lines.\n\n      Here is an example complete config:\n      export const chartConfig = {\n        type: \"pie\",\n        xKey: \"month\",\n        yKeys: [\"sales\", \"profit\", \"expenses\"],\n        colors: {\n          sales: \"#4CAF50\",    // Green for sales\n          profit: \"#2196F3\",   // Blue for profit\n          expenses: \"#F44336\"  // Red for expenses\n        },\n        legend: true\n      }\n\n      User Query:\n      ${userQuery}\n\n      Data:\n      ${JSON.stringify(results, null, 2)}`,\n      schema: configSchema,\n    });\n\n    // Override with shadcn theme colors\n    const colors: Record<string, string> = {};\n    config.yKeys.forEach((key, index) => {\n      colors[key] = `hsl(var(--chart-${index + 1}))`;\n    });\n\n    const updatedConfig = { ...config, colors };\n    return { config: updatedConfig };\n  } catch (e) {\n    console.error(e);\n    throw new Error('Failed to generate chart suggestion');\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Creating Streamable UI with streamUI() and AI SDK Core - Next.js TypeScript\nDESCRIPTION: This Server Action uses the new 'streamUI' function from 'ai/rsc' and the AI SDK OpenAI provider to create a provider-agnostic streamable UI. It registers a tool for city weather lookup using Zod schema validation and an async generator for incremental UI feedback. Returns result.value to the client. Dependencies: 'ai/rsc', '@ai-sdk/openai', 'zod', and relevant UI components/utilities. This approach enables streaming UI with any compatible LLM provider supported by the AI SDK Core.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/39-migration-guide-3-1.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { streamUI } from 'ai/rsc';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\nimport { Spinner, Weather } from '@/components';\nimport { getWeather } from '@/utils';\n\nasync function submitMessage(userInput = 'What is the weather in SF?') {\n  'use server';\n\n  const result = await streamUI({\n    model: openai('gpt-4-turbo'),\n    system: 'You are a helpful assistant',\n    messages: [{ role: 'user', content: userInput }],\n    text: ({ content }) => <p>{content}</p>,\n    tools: {\n      get_city_weather: {\n        description: 'Get the current weather for a city',\n        parameters: z\n          .object({\n            city: z.string().describe('Name of the city'),\n          })\n          .required(),\n        generate: async function* ({ city }) {\n          yield <Spinner />;\n          const weather = await getWeather(city);\n          return <Weather info={weather} />;\n        },\n      },\n    },\n  });\n\n  return result.value;\n}\n```\n\n----------------------------------------\n\nTITLE: Instantiating an OpenAI Chat Model Explicitly in TypeScript\nDESCRIPTION: Shows how to explicitly create an instance of an OpenAI chat model using the `.chat()` factory method on the provider instance. This ensures the OpenAI Chat Completions API is used and allows access to chat-specific features like tool calls.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = openai.chat('gpt-3.5-turbo');\n```\n\n----------------------------------------\n\nTITLE: Accessing Step Data with Vercel AI SDK in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the generateText function from the ai package to request text from an LLM (e.g., OpenAI) with a specified maxSteps. It then accesses the steps property from the response to extract all toolCalls across every step using flatMap. Dependencies include the ai library, and usage of generateText assumes proper model instantiation and authentication. The main parameters are model (set to openai('gpt-4o')) and maxSteps. The output is an array of all tool calls made across steps, providing comprehensive post-processing capability. Inputs are passed via generateText config; outputs depend on the model responses. Limitations may include LLM API step limits and tool call support.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#2025-04-23_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { generateText } from 'ai';\n\nconst { steps } = await generateText({\n  model: openai('gpt-4o'),\n  maxSteps: 10,\n  // ...\n});\n\n// extract all tool calls from the steps:\nconst allToolCalls = steps.flatMap(step => step.toolCalls);\n```\n\n----------------------------------------\n\nTITLE: Client-Side Text Generation Component in Next.js\nDESCRIPTION: A React component that uses the useCompletion hook from @ai-sdk/react to handle text generation. It creates a button that triggers text generation and displays the streamed completion results.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/20-stream-text.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useCompletion } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { completion, complete } = useCompletion({\n    api: '/api/completion',\n  });\n\n  return (\n    <div>\n      <div\n        onClick={async () => {\n          await complete('Why is the sky blue?');\n        }}\n      >\n        Generate\n      </div>\n\n      {completion}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Using an OpenAI Reasoning Model with generateText in TypeScript\nDESCRIPTION: Example demonstrating how to use an OpenAI reasoning model (e.g., 'o3-mini') with the `generateText` function. It highlights setting provider-specific options like `reasoningEffort` via the `providerOptions` parameter and accessing reasoning-specific output metadata (`reasoningTokens`) from the `providerMetadata` field in the result.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst { text, usage, providerMetadata } = await generateText({\n  model: openai('o3-mini'),\n  prompt: 'Invent a new holiday and describe its traditions.',\n  providerOptions: {\n    openai: {\n      reasoningEffort: 'low',\n    },\n  },\n});\n\nconsole.log(text);\nconsole.log('Usage:', {\n  ...usage,\n  reasoningTokens: providerMetadata?.openai?.reasoningTokens,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating React Client Component for Object Generation\nDESCRIPTION: A React client component that sends a POST request to generate notifications and displays the results. Implements loading state and result rendering.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/30-generate-object.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\n\nexport default function Page() {\n  const [generation, setGeneration] = useState();\n  const [isLoading, setIsLoading] = useState(false);\n\n  return (\n    <div>\n      <div\n        onClick={async () => {\n          setIsLoading(true);\n\n          await fetch('/api/completion', {\n            method: 'POST',\n            body: JSON.stringify({\n              prompt: 'Messages during finals week.',\n            }),\n          }).then(response => {\n            response.json().then(json => {\n              setGeneration(json.notifications);\n              setIsLoading(false);\n            });\n          });\n        }}\n      >\n        Generate\n      </div>\n\n      {isLoading ? 'Loading...' : <pre>{JSON.stringify(generation)}</pre>}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AI Chat Route Handler with Tools\nDESCRIPTION: Sets up an API route handler for the chatbot with tools for adding and retrieving information from the knowledge base. Implements streaming responses and defines tool schemas using Zod.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createResource } from '@/lib/actions/resources';\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, tool } from 'ai';\nimport { z } from 'zod';\nimport { findRelevantContent } from '@/lib/ai/embedding';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n    system: `You are a helpful assistant. Check your knowledge base before answering any questions.\n    Only respond to questions using information from tool calls.\n    if no relevant information is found in the tool calls, respond, \"Sorry, I don't know.\"`,\n    tools: {\n      addResource: tool({\n        description: `add a resource to your knowledge base.\n          If the user provides a random piece of knowledge unprompted, use this tool without asking for confirmation.`,\n        parameters: z.object({\n          content: z\n            .string()\n            .describe('the content or resource to add to the knowledge base'),\n        }),\n        execute: async ({ content }) => createResource({ content }),\n      }),\n      getInformation: tool({\n        description: `get information from your knowledge base to answer questions.`,\n        parameters: z.object({\n          question: z.string().describe('the users question'),\n        }),\n        execute: async ({ question }) => findRelevantContent(question),\n      }),\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Enum Value for Classification in TypeScript\nDESCRIPTION: This snippet shows how to use the generateObject function with the 'enum' output strategy to classify a movie genre based on a plot description.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateObject } from 'ai';\n\nconst { object } = await generateObject({\n  model: yourModel,\n  output: 'enum',\n  enum: ['action', 'comedy', 'drama', 'horror', 'sci-fi'],\n  prompt:\n    'Classify the genre of this movie plot: ' +\n    '\"A group of astronauts travel through a wormhole in search of a ' +\n    'new habitable planet for humanity.\"',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Sequential Processing Chain for Marketing Copy Generation\nDESCRIPTION: This code demonstrates a sequential workflow that generates marketing copy, evaluates its quality using predefined metrics, and automatically improves it if necessary. It uses OpenAI models through the AI SDK with a step-by-step chain pattern.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText, generateObject } from 'ai';\nimport { z } from 'zod';\n\nasync function generateMarketingCopy(input: string) {\n  const model = openai('gpt-4o');\n\n  // First step: Generate marketing copy\n  const { text: copy } = await generateText({\n    model,\n    prompt: `Write persuasive marketing copy for: ${input}. Focus on benefits and emotional appeal.`,\n  });\n\n  // Perform quality check on copy\n  const { object: qualityMetrics } = await generateObject({\n    model,\n    schema: z.object({\n      hasCallToAction: z.boolean(),\n      emotionalAppeal: z.number().min(1).max(10),\n      clarity: z.number().min(1).max(10),\n    }),\n    prompt: `Evaluate this marketing copy for:\n    1. Presence of call to action (true/false)\n    2. Emotional appeal (1-10)\n    3. Clarity (1-10)\n\n    Copy to evaluate: ${copy}`,\n  });\n\n  // If quality check fails, regenerate with more specific instructions\n  if (\n    !qualityMetrics.hasCallToAction ||\n    qualityMetrics.emotionalAppeal < 7 ||\n    qualityMetrics.clarity < 7\n  ) {\n    const { text: improvedCopy } = await generateText({\n      model,\n      prompt: `Rewrite this marketing copy with:\n      ${!qualityMetrics.hasCallToAction ? '- A clear call to action' : ''}\n      ${qualityMetrics.emotionalAppeal < 7 ? '- Stronger emotional appeal' : ''}\n      ${qualityMetrics.clarity < 7 ? '- Improved clarity and directness' : ''}\n\n      Original copy: ${copy}`,\n    });\n    return { copy: improvedCopy, qualityMetrics };\n  }\n\n  return { copy, qualityMetrics };\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Tool Calls with AI SDK and OpenAI\nDESCRIPTION: Example showing how to define and execute multiple tools in parallel using the AI SDK. The code implements two tools: one for fetching weather data and another for retrieving city attractions, both being called simultaneously when generating text responses using GPT-4.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/51-call-tools-in-parallel.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText, tool } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nconst result = await generateText({\n  model: openai('gpt-4-turbo'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }: { location: string }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n    cityAttractions: tool({\n      parameters: z.object({ city: z.string() }),\n      execute: async ({ city }: { city: string }) => {\n        if (city === 'San Francisco') {\n          return {\n            attractions: [\n              'Golden Gate Bridge',\n              'Alcatraz Island',\n              \"Fisherman's Wharf\",\n            ],\n          };\n        } else {\n          return { attractions: [] };\n        }\n      },\n    }),\n  },\n  prompt:\n    'What is the weather in San Francisco and what attractions should I visit?',\n});\n\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Streaming Structured Objects with Schema\nDESCRIPTION: Demonstrates using streamObject with Zod schema to generate and stream a structured recipe object from a language model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/04-stream-object.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { z } from 'zod';\n\nconst { partialObjectStream } = streamObject({\n  model: openai('gpt-4-turbo'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.string()),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n\nfor await (const partialObject of partialObjectStream) {\n  console.clear();\n  console.log(partialObject);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-Side Chat Route Handler in Next.js\nDESCRIPTION: Sets up a POST route handler that streams responses from the language model using the AI SDK. Configures the model to respond in Markdown format with a 60-second duration limit.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/25-markdown-chatbot-with-memoization.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport const maxDuration = 60;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    system:\n      'You are a helpful assistant. Respond to the user in Markdown format.',\n    model: openai('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Step Completion with onStepFinish in Vercel AI SDK (TSX)\nDESCRIPTION: This snippet showcases the use of the onStepFinish callback within the generateText function to react to each completed step during a multi-step text generation task. It is implemented in TSX and demonstrates custom logic inside the callback (such as storing chat history or usage metrics) after a step is finished, i.e., when all deltas, tool calls, and tool results are available. The required dependency is the ai package, and you must pass your model instance and set maxSteps. The callback receives an object containing text, toolCalls, toolResults, finishReason, and usage. The approach is suitable for real-time step monitoring or incremental UI updates.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#2025-04-23_snippet_8\n\nLANGUAGE: TSX\nCODE:\n```\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: yourModel,\n  maxSteps: 10,\n  onStepFinish({ text, toolCalls, toolResults, finishReason, usage }) {\n    // your own logic, e.g. for saving the chat history or recording usage\n  },\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing a Server API Route for OpenAI Assistant Integration in Next.js\nDESCRIPTION: This server-side API route handles communication with the OpenAI Assistant API. It creates or uses an existing thread, adds the user's message to it, and uses AssistantResponse to stream the assistant's response back to the client. The code requires an OpenAI API key and Assistant ID to be set as environment variables.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/120-stream-assistant-response.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport OpenAI from 'openai';\nimport { AssistantResponse } from 'ai';\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY || '',\n});\n\nexport async function POST(req: Request) {\n  const input: {\n    threadId: string | null;\n    message: string;\n  } = await req.json();\n\n  const threadId = input.threadId ?? (await openai.beta.threads.create({})).id;\n\n  const createdMessage = await openai.beta.threads.messages.create(threadId, {\n    role: 'user',\n    content: input.message,\n  });\n\n  return AssistantResponse(\n    { threadId, messageId: createdMessage.id },\n    async ({ forwardStream }) => {\n      const runStream = openai.beta.threads.runs.stream(threadId, {\n        assistant_id:\n          process.env.ASSISTANT_ID ??\n          (() => {\n            throw new Error('ASSISTANT_ID environment is not set');\n          })(),\n      });\n\n      await forwardStream(runStream);\n    },\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Providing Audio Input to OpenAI Models\nDESCRIPTION: Demonstrates how to send an audio file as input to the `gpt-4o-audio-preview` model using `generateText`. The audio data is read using `fs.readFileSync` and included as a `file` type object with the correct audio MIME type.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\nimport * as fs from 'fs'; // Assumed import\n\nconst result = await generateText({\n  model: openai('gpt-4o-audio-preview'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'What is the audio saying?' },\n        {\n          type: 'file',\n          mimeType: 'audio/mpeg',\n          data: fs.readFileSync('./data/galileo.mp3'),\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Handling Stream Completion in AI SDK with TypeScript\nDESCRIPTION: Demonstrates the use of the onFinish callback to process the final result of the text generation stream, including the full text, finish reason, and usage information.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\nimport { streamText } from 'ai';\n\nconst result = streamText({\n  model: yourModel,\n  prompt: 'Invent a new holiday and describe its traditions.',\n  onFinish({ text, finishReason, usage, response }) {\n    // your own logic, e.g. for saving the chat history or recording usage\n\n    const messages = response.messages; // messages that were generated\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with OpenAI in Node.js\nDESCRIPTION: Example of using AI SDK Core to generate text with OpenAI's GPT-4o model in a Node.js environment. Requires an OPENAI_API_KEY environment variable to be set.\nSOURCE: https://github.com/vercel/ai/blob/main/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai'; // Ensure OPENAI_API_KEY environment variable is set\n\nconst { text } = await generateText({\n  model: openai('gpt-4o'),\n  system: 'You are a friendly assistant!',\n  prompt: 'Why is the sky blue?',\n});\n\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Defining and Using a Weather Tool in Text Generation with AI SDK Core\nDESCRIPTION: This snippet demonstrates how to define a weather tool using the 'tool' helper function and use it in the generateText function. It includes a description, parameters schema, and an execute function that returns simulated weather data.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from 'zod';\nimport { generateText, tool } from 'ai';\n\nconst result = await generateText({\n  model: yourModel,\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Middleware\nDESCRIPTION: Example of a Retrieval Augmented Generation middleware that enhances prompts with relevant information.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { LanguageModelV1Middleware } from 'ai';\n\nexport const yourRagMiddleware: LanguageModelV1Middleware = {\n  transformParams: async ({ params }) => {\n    const lastUserMessageText = getLastUserMessageText({\n      prompt: params.prompt,\n    });\n\n    if (lastUserMessageText == null) {\n      return params; // do not use RAG (send unmodified parameters)\n    }\n\n    const instruction =\n      'Use the following information to answer the question:\\n' +\n      findSources({ text: lastUserMessageText })\n        .map(chunk => JSON.stringify(chunk))\n        .join('\\n');\n\n    return addToLastUserMessage({ params, text: instruction });\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Extracting Tool Calls from Steps in AI SDK Core\nDESCRIPTION: This snippet demonstrates how to extract all tool calls from the steps property returned by generateText. It uses the flatMap method to collect tool calls from each step into a single array.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\n\nconst { steps } = await generateText({\n  model: openai('gpt-4-turbo'),\n  maxSteps: 10,\n  // ...\n});\n\n// extract all tool calls from the steps:\nconst allToolCalls = steps.flatMap(step => step.toolCalls);\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Interface with useAssistant Hook in Next.js\nDESCRIPTION: Client-side implementation of a chat interface using the useAssistant hook from @ai-sdk/react. This component renders messages, displays the assistant's status, and provides an input form for user messages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/121-stream-assistant-response-with-tools.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { Message, useAssistant } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { status, messages, input, submitMessage, handleInputChange } =\n    useAssistant({ api: '/api/assistant' });\n\n  return (\n    <div className=\"flex flex-col gap-2\">\n      <div className=\"p-2\">status: {status}</div>\n\n      <div className=\"flex flex-col p-2 gap-2\">\n        {messages.map((message: Message) => (\n          <div key={message.id} className=\"flex flex-row gap-2\">\n            <div className=\"w-24 text-zinc-500\">{`${message.role}: `}</div>\n            <div className=\"w-full\">{message.content}</div>\n          </div>\n        ))}\n      </div>\n\n      <form onSubmit={submitMessage} className=\"fixed bottom-0 p-2 w-full\">\n        <input\n          disabled={status !== 'awaiting_message'}\n          value={input}\n          onChange={handleInputChange}\n          className=\"bg-zinc-100 w-full p-2\"\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Tool Invocation Confirmations in API Route Handler\nDESCRIPTION: This code handles the confirmation response for tool invocations. It extracts the most recent message, checks if a tool requiring confirmation was called, and processes the user's response (confirm/deny) by either executing the tool or returning an error message.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#2025-04-23_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport {\n  createDataStreamResponse,\n  formatDataStreamPart,\n  Message,\n  streamText,\n  tool,\n} from 'ai';\nimport { z } from 'zod';\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: Message[] } = await req.json();\n\n  return createDataStreamResponse({\n    execute: async dataStream => {\n      // pull out last message\n      const lastMessage = messages[messages.length - 1];\n\n      lastMessage.parts = await Promise.all(\n        // map through all message parts\n        lastMessage.parts?.map(async part => {\n          if (part.type !== 'tool-invocation') {\n            return part;\n          }\n          const toolInvocation = part.toolInvocation;\n          // return if tool isn't weather tool or in a result state\n          if (\n            toolInvocation.toolName !== 'getWeatherInformation' ||\n            toolInvocation.state !== 'result'\n          ) {\n            return part;\n          }\n\n          // switch through tool result states (set on the frontend)\n          switch (toolInvocation.result) {\n            case 'Yes, confirmed.': {\n              const result = await executeWeatherTool(toolInvocation.args);\n\n              // forward updated tool result to the client:\n              dataStream.write(\n                formatDataStreamPart('tool_result', {\n                  toolCallId: toolInvocation.toolCallId,\n                  result,\n                }),\n              );\n\n              // update the message part:\n              return { ...part, toolInvocation: { ...toolInvocation, result } };\n            }\n            case 'No, denied.': {\n              const result = 'Error: User denied access to weather information';\n\n              // forward updated tool result to the client:\n              dataStream.write(\n                formatDataStreamPart('tool_result', {\n                  toolCallId: toolInvocation.toolCallId,\n                  result,\n                }),\n              );\n\n              // update the message part:\n              return { ...part, toolInvocation: { ...toolInvocation, result } };\n            }\n            default:\n              return part;\n          }\n        }) ?? [],\n      );\n\n      const result = streamText({\n        model: openai('gpt-4o'),\n        messages,\n        tools: {\n          getWeatherInformation: tool({\n            description: 'show the weather in a given city to the user',\n            parameters: z.object({ city: z.string() }),\n          }),\n        },\n      });\n\n      result.mergeIntoDataStream(dataStream);\n    },\n  });\n}\n\nasync function executeWeatherTool({}: { city: string }) {\n  const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy'];\n  return weatherOptions[Math.floor(Math.random() * weatherOptions.length)];\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-Side Actions for AI-Powered Chat\nDESCRIPTION: This server-side code handles the conversation logic, integrates with OpenAI, and manages the rendering of UI components based on AI responses. It defines tools for showing stock information and flight status.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/90-render-visual-interface-in-chat.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { getMutableAIState, streamUI } from 'ai/rsc';\nimport { openai } from '@ai-sdk/openai';\nimport { ReactNode } from 'react';\nimport { z } from 'zod';\nimport { generateId } from 'ai';\nimport { Stock } from '@/components/stock';\nimport { Flight } from '@/components/flight';\n\nexport interface ServerMessage {\n  role: 'user' | 'assistant';\n  content: string;\n}\n\nexport interface ClientMessage {\n  id: string;\n  role: 'user' | 'assistant';\n  display: ReactNode;\n}\n\nexport async function continueConversation(\n  input: string,\n): Promise<ClientMessage> {\n  'use server';\n\n  const history = getMutableAIState();\n\n  const result = await streamUI({\n    model: openai('gpt-3.5-turbo'),\n    messages: [...history.get(), { role: 'user', content: input }],\n    text: ({ content, done }) => {\n      if (done) {\n        history.done((messages: ServerMessage[]) => [\n          ...messages,\n          { role: 'assistant', content },\n        ]);\n      }\n\n      return <div>{content}</div>;\n    },\n    tools: {\n      showStockInformation: {\n        description:\n          'Get stock information for symbol for the last numOfMonths months',\n        parameters: z.object({\n          symbol: z\n            .string()\n            .describe('The stock symbol to get information for'),\n          numOfMonths: z\n            .number()\n            .describe('The number of months to get historical information for'),\n        }),\n        generate: async ({ symbol, numOfMonths }) => {\n          history.done((messages: ServerMessage[]) => [\n            ...messages,\n            {\n              role: 'assistant',\n              content: `Showing stock information for ${symbol}`,\n            },\n          ]);\n\n          return <Stock symbol={symbol} numOfMonths={numOfMonths} />;\n        },\n      },\n      showFlightStatus: {\n        description: 'Get the status of a flight',\n        parameters: z.object({\n          flightNumber: z\n            .string()\n            .describe('The flight number to get status for'),\n        }),\n        generate: async ({ flightNumber }) => {\n          history.done((messages: ServerMessage[]) => [\n            ...messages,\n            {\n              role: 'assistant',\n              content: `Showing flight status for ${flightNumber}`,\n            },\n          ]);\n\n          return <Flight flightNumber={flightNumber} />;\n        },\n      },\n    },\n  });\n\n  return {\n    id: generateId(),\n    role: 'assistant',\n    display: result.value,\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up API Route for Chat Requests in TypeScript\nDESCRIPTION: This code sets up an API route to handle chat requests and model responses using the streamText function from the AI SDK. It processes messages and streams the model's responses back to the client.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#2025-04-23_snippet_1\n\nLANGUAGE: ts\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport async function POST(request: Request) {\n  const { messages } = await request.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'You are a friendly assistant!',\n    messages,\n    maxSteps: 5,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing API Route with Tool Definitions in TypeScript\nDESCRIPTION: Sets up an API route that defines three tools: getWeatherInformation (server-side), askForConfirmation (client interaction), and getLocation (client-side automatic). Uses OpenAI's GPT-4 model and includes parameter validation with Zod.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport { z } from 'zod';\n\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n    tools: {\n      getWeatherInformation: {\n        description: 'show the weather in a given city to the user',\n        parameters: z.object({ city: z.string() }),\n        execute: async ({}: { city: string }) => {\n          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];\n          return weatherOptions[Math.floor(Math.random() * weatherOptions.length)];\n        },\n      },\n      askForConfirmation: {\n        description: 'Ask the user for confirmation.',\n        parameters: z.object({\n          message: z.string().describe('The message to ask for confirmation.')\n        }),\n      },\n      getLocation: {\n        description: 'Get the user location. Always ask for confirmation before using this tool.',\n        parameters: z.object({}),\n      },\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Rendering Chat UI with Tool Call Interception in Next.js\nDESCRIPTION: This code snippet demonstrates how to create a chat interface using the useChat hook from the AI SDK. It includes logic to intercept tool calls and render a confirmation UI for the getWeatherInformation tool.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit, addToolResult } =\n    useChat();\n\n  return (\n    <div>\n      <div>\n        {messages?.map(m => (\n          <div key={m.id}>\n            <strong>{`${m.role}: `}</strong>\n            {m.parts?.map((part, i) => {\n              switch (part.type) {\n                case 'text':\n                  return <div key={i}>{part.text}</div>;\n                case 'tool-invocation':\n                  const toolInvocation = part.toolInvocation;\n                  const toolCallId = toolInvocation.toolCallId;\n\n                  // render confirmation tool (client-side tool with user interaction)\n                  if (\n                    toolInvocation.toolName === 'getWeatherInformation' &&\n                    toolInvocation.state === 'call'\n                  ) {\n                    return (\n                      <div key={toolCallId}>\n                        Get weather information for {toolInvocation.args.city}?\n                        <div>\n                          <button\n                            onClick={() =>\n                              addToolResult({\n                                toolCallId,\n                                result: 'Yes, confirmed.',\n                              })\n                            }\n                          >\n                            Yes\n                          </button>\n                          <button\n                            onClick={() =>\n                              addToolResult({\n                                toolCallId,\n                                result: 'No, denied.',\n                              })\n                            }\n                          >\n                            No\n                          </button>\n                        </div>\n                      </div>\n                    );\n                  }\n              }\n            })}\n            <br />\n          </div>\n        ))}\n      </div>\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing LangChain Completion API with AI SDK in Next.js\nDESCRIPTION: This server-side code snippet demonstrates how to create a completion API endpoint using LangChain and the AI SDK in a Next.js application. It uses ChatOpenAI for the language model and LangChainAdapter to stream the response to the client.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/04-adapters/01-langchain.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\nimport { LangChainAdapter } from 'ai';\n\nexport const maxDuration = 60;\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const model = new ChatOpenAI({\n    model: 'gpt-3.5-turbo-0125',\n    temperature: 0,\n  });\n\n  const stream = await model.stream(prompt);\n\n  return LangChainAdapter.toDataStreamResponse(stream);\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Sources in generateText Function\nDESCRIPTION: This code snippet shows how to access sources provided by models like Perplexity and Google Generative AI when using the generateText function. It iterates through the sources and logs their properties.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: google('gemini-2.0-flash-exp', { useSearchGrounding: true }),\n  prompt: 'List the top 5 San Francisco news from the past week.',\n});\n\nfor (const source of result.sources) {\n  if (source.sourceType === 'url') {\n    console.log('ID:', source.id);\n    console.log('Title:', source.title);\n    console.log('URL:', source.url);\n    console.log('Provider metadata:', source.providerMetadata);\n    console.log();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Structured JSON Data with OpenAI Responses API\nDESCRIPTION: Example of generating structured JSON data using AI SDK's generateObject function with a Zod schema to constrain model outputs. This creates a type-safe recipe object with defined structure.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/19-openai-responses.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: openai.responses('gpt-4o'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Handling Client-Side Loading State in Next.js with AI SDK RSC\nDESCRIPTION: This code snippet demonstrates how to handle loading state on the client side in a Next.js application using AI SDK RSC. It includes a form that submits user input, manages loading state, and displays the generated response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/06-loading-state.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { generateResponse } from './actions';\nimport { readStreamableValue } from 'ai/rsc';\n\n// Force the page to be dynamic and allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [input, setInput] = useState<string>('');\n  const [generation, setGeneration] = useState<string>('');\n  const [loading, setLoading] = useState<boolean>(false);\n\n  return (\n    <div>\n      <div>{generation}</div>\n      <form\n        onSubmit={async e => {\n          e.preventDefault();\n          setLoading(true);\n          const response = await generateResponse(input);\n\n          let textContent = '';\n\n          for await (const delta of readStreamableValue(response)) {\n            textContent = `${textContent}${delta}`;\n            setGeneration(textContent);\n          }\n          setInput('');\n          setLoading(false);\n        }}\n      >\n        <input\n          type=\"text\"\n          value={input}\n          disabled={loading}\n          className=\"disabled:opacity-50\"\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button>Send Message</button>\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Azure OpenAI Chat Model with Specific Options\nDESCRIPTION: Shows how to initialize an Azure OpenAI chat model instance by passing an optional second argument to the provider function. This allows setting model-specific options like 'logitBias' or 'user' directly during model creation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = azure('your-deployment-name', {\n  logitBias: { // optional likelihood for specific tokens\n    '50256': -100, \n  },\n  user: 'test-user', // optional unique user identifier\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Server Action for Text Generation with OpenAI\nDESCRIPTION: A server action that uses the AI SDK's generateText function with OpenAI model to process text generation requests. It takes a question as input and returns the generated text along with metadata such as finish reason and usage statistics.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/10-generate-text.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n'use server';\n\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nexport async function getAnswer(question: string) {\n  const { text, finishReason, usage } = await generateText({\n    model: openai('gpt-3.5-turbo'),\n    prompt: question,\n  });\n\n  return { text, finishReason, usage };\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Default Azure OpenAI Provider Instance\nDESCRIPTION: Shows how to import the default pre-configured Azure OpenAI provider instance from the '@ai-sdk/azure' package. This instance typically relies on environment variables for configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { azure } from '@ai-sdk/azure';\n```\n\n----------------------------------------\n\nTITLE: Configuring Response Generation with Provider Options\nDESCRIPTION: This example shows how to generate text using the generateText function, specifying a response model and configuring provider options such as parallel tool calls, storage, user identity, and reasoning efforts. It highlights customizing generation parameters for tailored response behavior.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_18\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst result = await generateText({\n  model: openai.responses('gpt-4o-mini'),\n  providerOptions: {\n    openai: {\n      parallelToolCalls: false,\n      store: false,\n      user: 'user_123',\n      // ...\n    }\n  },\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Tool Calling with AI SDK in TypeScript\nDESCRIPTION: This server-side code implements the continueConversation function using the AI SDK's generateText. It defines a celsiusToFahrenheit tool that converts temperatures from Celsius to Fahrenheit. The function uses Zod for parameter validation and OpenAI's GPT-3.5-turbo model for text generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/50-call-tools.mdx#2025-04-23_snippet_1\n\nLANGUAGE: ts\nCODE:\n```\n'use server';\n\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nexport interface Message {\n  role: 'user' | 'assistant';\n  content: string;\n}\n\nexport async function continueConversation(history: Message[]) {\n  'use server';\n\n  const { text, toolResults } = await generateText({\n    model: openai('gpt-3.5-turbo'),\n    system: 'You are a friendly assistant!',\n    messages: history,\n    tools: {\n      celsiusToFahrenheit: {\n        description: 'Converts celsius to fahrenheit',\n        parameters: z.object({\n          value: z.string().describe('The value in celsius'),\n        }),\n        execute: async ({ value }) => {\n          const celsius = parseFloat(value);\n          const fahrenheit = celsius * (9 / 5) + 32;\n          return `${celsius}Â°C is ${fahrenheit.toFixed(2)}Â°F`;\n        },\n      },\n    },\n  });\n\n  return {\n    messages: [\n      ...history,\n      {\n        role: 'assistant' as const,\n        content:\n          text || toolResults.map(toolResult => toolResult.result).join('\\n'),\n      },\n    ],\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Using Web Search Tool with OpenAI Responses API\nDESCRIPTION: Shows how to use the built-in webSearch tool with the OpenAI Responses API to ground model responses in current information from the internet.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/19-openai-responses.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: openai.responses('gpt-4o-mini'),\n  prompt: 'What happened in San Francisco last week?',\n  tools: {\n    web_search_preview: openai.tools.webSearchPreview(),\n  },\n});\n\nconsole.log(result.text);\nconsole.log(result.sources);\n```\n\n----------------------------------------\n\nTITLE: Implementing Structured Answers with AI SDK in TypeScript\nDESCRIPTION: This snippet shows how to use an answer tool and the toolChoice setting to force the LLM to provide structured outputs. It demonstrates how to define tools, including an answer tool without an execute function, and use maxSteps for multi-step problem-solving.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText, tool } from 'ai';\nimport 'dotenv/config';\nimport { z } from 'zod';\n\nconst { toolCalls } = await generateText({\n  model: openai('gpt-4o-2024-08-06', { structuredOutputs: true }),\n  tools: {\n    calculate: tool({\n      description:\n        'A tool for evaluating mathematical expressions. Example expressions: ' +\n        \"'1.2 * (2 + 4.5)', '12.7 cm to inch', 'sin(45 deg) ^ 2'.\",\n      parameters: z.object({ expression: z.string() }),\n      execute: async ({ expression }) => mathjs.evaluate(expression),\n    }),\n    // answer tool: the LLM will provide a structured answer\n    answer: tool({\n      description: 'A tool for providing the final answer.',\n      parameters: z.object({\n        steps: z.array(\n          z.object({\n            calculation: z.string(),\n            reasoning: z.string(),\n          }),\n        ),\n        answer: z.string(),\n      }),\n      // no execute function - invoking it will terminate the agent\n    }),\n  },\n  toolChoice: 'required',\n  maxSteps: 10,\n  system:\n    'You are solving math problems. ' +\n    'Reason step by step. ' +\n    'Use the calculator when necessary. ' +\n    'The calculator can only do simple additions, subtractions, multiplications, and divisions. ' +\n    'When you give the final answer, provide an explanation for how you got it.',\n  prompt:\n    'A taxi driver earns $9461 per 1-hour work. ' +\n    'If he works 12 hours a day and in 1 hour he uses 14-liters petrol with price $134 for 1-liter. ' +\n    'How much money does he earn in one day?',\n});\n\nconsole.log(`FINAL TOOL CALLS: ${JSON.stringify(toolCalls, null, 2)}`);\n```\n\n----------------------------------------\n\nTITLE: Explicitly Defining Tool Schemas for the MCP Client in AI SDK using TypeScript\nDESCRIPTION: This snippet shows how to provide explicit zod-based schemas corresponding to tool definitions when calling the MCP client's 'tools' method. It enables tight TypeScript integration, autocompletion, and early error detection at development time, but requires manual upkeep of schema definitions. The example includes a tool with parameters and a zero-argument tool using an empty schema. Inputs are tool names and their parameter schemas; the outputs are a strongly typed toolset.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_23\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from 'zod';\n\nconst tools = await mcpClient.tools({\n  schemas: {\n    'get-data': {\n      parameters: z.object({\n        query: z.string().describe('The data query'),\n        format: z.enum(['json', 'text']).optional(),\n      }),\n    },\n    // For tools with zero arguments, you should use an empty object:\n    'tool-with-no-args': {\n      parameters: z.object({}),\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Handling OpenAI Assistant API Requests in Next.js\nDESCRIPTION: This server-side code snippet shows how to handle OpenAI assistant API requests in a Next.js route. It creates or uses an existing thread, adds user messages, and streams the assistant's response, including handling tool calls.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/10-openai-assistants.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AssistantResponse } from 'ai';\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY || '',\n});\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  // Parse the request body\n  const input: {\n    threadId: string | null;\n    message: string;\n  } = await req.json();\n\n  // Create a thread if needed\n  const threadId = input.threadId ?? (await openai.beta.threads.create({})).id;\n\n  // Add a message to the thread\n  const createdMessage = await openai.beta.threads.messages.create(threadId, {\n    role: 'user',\n    content: input.message,\n  });\n\n  return AssistantResponse(\n    { threadId, messageId: createdMessage.id },\n    async ({ forwardStream, sendDataMessage }) => {\n      // Run the assistant on the thread\n      const runStream = openai.beta.threads.runs.stream(threadId, {\n        assistant_id:\n          process.env.ASSISTANT_ID ??\n          (() => {\n            throw new Error('ASSISTANT_ID is not set');\n          })(),\n      });\n\n      // forward run status would stream message deltas\n      let runResult = await forwardStream(runStream);\n\n      // status can be: queued, in_progress, requires_action, cancelling, cancelled, failed, completed, or expired\n      while (\n        runResult?.status === 'requires_action' &&\n        runResult.required_action?.type === 'submit_tool_outputs'\n      ) {\n        const tool_outputs =\n          runResult.required_action.submit_tool_outputs.tool_calls.map(\n            (toolCall: any) => {\n              const parameters = JSON.parse(toolCall.function.arguments);\n\n              switch (toolCall.function.name) {\n                // configure your tool calls here\n\n                default:\n                  throw new Error(\n                    `Unknown tool call function: ${toolCall.function.name}`,\n                  );\n              }\n            },\n          );\n\n        runResult = await forwardStream(\n          openai.beta.threads.runs.submitToolOutputsStream(\n            threadId,\n            runResult.id,\n            { tool_outputs },\n          ),\n        );\n      }\n    },\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Web Search with Azure OpenAI\nDESCRIPTION: Demonstrates how to use Azure OpenAI's web search tool with custom configuration options including search context size and user location settings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: azure.responses('your-deployment-name'),\n  prompt: 'What happened in San Francisco last week?',\n  tools: {\n    web_search_preview: azure.tools.webSearchPreview({\n      // optional configuration:\n      searchContextSize: 'high',\n      userLocation: {\n        type: 'approximate',\n        city: 'San Francisco',\n        region: 'California',\n      },\n    }),\n  },\n  // Force web search tool:\n  toolChoice: { type: 'tool', toolName: 'web_search_preview' },\n});\n\n// URL sources\nconst sources = result.sources;\n```\n\n----------------------------------------\n\nTITLE: Server Action for Resource Creation with Embeddings\nDESCRIPTION: Implements a Next.js server action that creates resources and their corresponding embeddings in the database using Zod for validation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_9\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport {\n  NewResourceParams,\n  insertResourceSchema,\n  resources,\n} from '@/lib/db/schema/resources';\nimport { db } from '../db';\nimport { generateEmbeddings } from '../ai/embedding';\nimport { embeddings as embeddingsTable } from '../db/schema/embeddings';\n\nexport const createResource = async (input: NewResourceParams) => {\n  try {\n    const { content } = insertResourceSchema.parse(input);\n\n    const [resource] = await db\n      .insert(resources)\n      .values({ content })\n      .returning();\n\n    const embeddings = await generateEmbeddings(content);\n    await db.insert(embeddingsTable).values(\n      embeddings.map(embedding => ({\n        resourceId: resource.id,\n        ...embedding,\n      })),\n    );\n\n    return 'Resource successfully created and embedded.';\n  } catch (error) {\n    return error instanceof Error && error.message.length > 0\n      ? error.message\n      : 'Error, please try again.';\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Sending Provider-Specific Options in API Requests using TypeScript\nDESCRIPTION: Demonstrates how to include provider-specific options in the request body when calling functions like `generateText`. This is achieved by passing an object to the `providerOptions` parameter, keyed by the provider name specified during instance creation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/index.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst provider = createOpenAICompatible({\n  name: 'provider-name',\n  apiKey: process.env.PROVIDER_API_KEY,\n  baseURL: 'https://api.provider.com/v1',\n});\n\nconst { text } = await generateText({\n  model: provider('model-id'),\n  prompt: 'Hello',\n  providerOptions: {\n    'provider-name': { customOption: 'magic-value' },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Azure OpenAI Provider Package\nDESCRIPTION: Demonstrates how to install the '@ai-sdk/azure' package using different package managers (pnpm, npm, yarn). This package provides the necessary modules to interact with Azure OpenAI services via the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/azure\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/azure\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/azure\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Step Text Streaming on Server-Side with AI SDK\nDESCRIPTION: This code snippet demonstrates how to create a multi-step text streaming process using the AI SDK on the server-side. It uses OpenAI models, tool calls, and data stream manipulation to create a workflow with different steps and settings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/24-stream-text-multistep.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { createDataStreamResponse, streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  return createDataStreamResponse({\n    execute: async dataStream => {\n      // step 1 example: forced tool call\n      const result1 = streamText({\n        model: openai('gpt-4o-mini', { structuredOutputs: true }),\n        system: 'Extract the user goal from the conversation.',\n        messages,\n        toolChoice: 'required', // force the model to call a tool\n        tools: {\n          extractGoal: tool({\n            parameters: z.object({ goal: z.string() }),\n            execute: async ({ goal }) => goal, // no-op extract tool\n          }),\n        },\n      });\n\n      // forward the initial result to the client without the finish event:\n      result1.mergeIntoDataStream(dataStream, {\n        experimental_sendFinish: false, // omit the finish event\n      });\n\n      // note: you can use any programming construct here, e.g. if-else, loops, etc.\n      // workflow programming is normal programming with this approach.\n\n      // example: continue stream with forced tool call from previous step\n      const result2 = streamText({\n        // different system prompt, different model, no tools:\n        model: openai('gpt-4o'),\n        system:\n          'You are a helpful assistant with a different system prompt. Repeat the extract user goal in your answer.',\n        // continue the workflow stream with the messages from the previous step:\n        messages: [\n          ...convertToCoreMessages(messages),\n          ...(await result1.response).messages,\n        ],\n      });\n\n      // forward the 2nd result to the client (incl. the finish event):\n      result2.mergeIntoDataStream(dataStream, {\n        experimental_sendStart: false, // omit the start event\n      });\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Calling Server Actions from Client with useActions\nDESCRIPTION: This example demonstrates how to call server actions from a client component using the useActions hook, and how to update the UI state based on the responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useActions, useUIState } from 'ai/rsc';\nimport { AI } from './ai';\n\nexport default function Page() {\n  const { sendMessage } = useActions<typeof AI>();\n  const [messages, setMessages] = useUIState();\n\n  const handleSubmit = async event => {\n    event.preventDefault();\n\n    setMessages([\n      ...messages,\n      { id: Date.now(), role: 'user', display: event.target.message.value },\n    ]);\n\n    const response = await sendMessage(event.target.message.value);\n\n    setMessages([\n      ...messages,\n      { id: Date.now(), role: 'assistant', display: response },\n    ]);\n  };\n\n  return (\n    <>\n      <ul>\n        {messages.map(message => (\n          <li key={message.id}>{message.display}</li>\n        ))}\n      </ul>\n      <form onSubmit={handleSubmit}>\n        <input type=\"text\" name=\"message\" />\n        <button type=\"submit\">Send</button>\n      </form>\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Interface with Visual Components in Next.js\nDESCRIPTION: This code snippet shows how to create a chat interface that renders visual components for weather information and handles various tool calls. It uses the useChat hook from the AI SDK and implements custom rendering for different tool invocations.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/90-render-visual-interface-in-chat.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { ToolInvocation } from 'ai';\nimport { Message, useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit, addToolResult } =\n    useChat({\n      api: '/api/use-chat',\n      maxSteps: 5,\n\n      // run client-side tools that are automatically executed:\n      async onToolCall({ toolCall }) {\n        if (toolCall.toolName === 'getLocation') {\n          const cities = [\n            'New York',\n            'Los Angeles',\n            'Chicago',\n            'San Francisco',\n          ];\n          return cities[Math.floor(Math.random() * cities.length)];\n        }\n      },\n    });\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch gap-4\">\n      {messages?.map((m: Message) => (\n        <div key={m.id} className=\"whitespace-pre-wrap flex flex-col gap-1\">\n          <strong>{`${m.role}: `}</strong>\n          {m.content}\n          {m.toolInvocations?.map((toolInvocation: ToolInvocation) => {\n            const toolCallId = toolInvocation.toolCallId;\n\n            // render confirmation tool (client-side tool with user interaction)\n            if (toolInvocation.toolName === 'askForConfirmation') {\n              return (\n                <div\n                  key={toolCallId}\n                  className=\"text-gray-500 flex flex-col gap-2\"\n                >\n                  {toolInvocation.args.message}\n                  <div className=\"flex gap-2\">\n                    {'result' in toolInvocation ? (\n                      <b>{toolInvocation.result}</b>\n                    ) : (\n                      <>\n                        <button\n                          className=\"px-4 py-2 font-bold text-white bg-blue-500 rounded hover:bg-blue-700\"\n                          onClick={() =>\n                            addToolResult({\n                              toolCallId,\n                              result: 'Yes, confirmed.',\n                            })\n                          }\n                        >\n                          Yes\n                        </button>\n                        <button\n                          className=\"px-4 py-2 font-bold text-white bg-red-500 rounded hover:bg-red-700\"\n                          onClick={() =>\n                            addToolResult({\n                              toolCallId,\n                              result: 'No, denied',\n                            })\n                          }\n                        >\n                          No\n                        </button>\n                      </>\n                    )}\n                  </div>\n                </div>\n              );\n            }\n\n            // other tools:\n            return 'result' in toolInvocation ? (\n              toolInvocation.toolName === 'getWeatherInformation' ? (\n                <div\n                  key={toolCallId}\n                  className=\"flex flex-col gap-2 p-4 bg-blue-400 rounded-lg\"\n                >\n                  <div className=\"flex flex-row justify-between items-center\">\n                    <div className=\"text-4xl text-blue-50 font-medium\">\n                      {toolInvocation.result.value}Â°\n                      {toolInvocation.result.unit === 'celsius' ? 'C' : 'F'}\n                    </div>\n\n                    <div className=\"h-9 w-9 bg-amber-400 rounded-full flex-shrink-0\" />\n                  </div>\n                  <div className=\"flex flex-row gap-2 text-blue-50 justify-between\">\n                    {toolInvocation.result.weeklyForecast.map(\n                      (forecast: any) => (\n                        <div\n                          key={forecast.day}\n                          className=\"flex flex-col items-center\"\n                        >\n                          <div className=\"text-xs\">{forecast.day}</div>\n                          <div>{forecast.value}Â°</div>\n                        </div>\n                      ),\n                    )}\n                  </div>\n                </div>\n              ) : toolInvocation.toolName === 'getLocation' ? (\n                <div\n                  key={toolCallId}\n                  className=\"text-gray-500 bg-gray-100 rounded-lg p-4\"\n                >\n                  User is in {toolInvocation.result}.\n                </div>\n              ) : (\n                <div key={toolCallId} className=\"text-gray-500\">\n                  Tool call {`${toolInvocation.toolName}: `}\n                  {toolInvocation.result}\n                </div>\n              )\n            ) : (\n              <div key={toolCallId} className=\"text-gray-500\">\n                Calling {toolInvocation.toolName}...\n              </div>\n            );\n          })}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          className=\"fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Structured Output using JSON Schema with generateObject and generateText\nDESCRIPTION: This snippet shows how to generate structured outputs using 'generateObject' with a Zod schema and 'generateText' with an 'experimental_output' schema. It ensures the generated data conforms to the specified schema, useful for predictable data extraction.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_24\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Using generateObject\nconst result = await generateObject({\n  model: openai.responses('gpt-4.1'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(\n        z.object({\n          name: z.string(),\n          amount: z.string(),\n        }),\n      ),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n\n// Using generateText\nconst result = await generateText({\n  model: openai.responses('gpt-4.1'),\n  prompt: 'How do I make a pizza?',\n  experimental_output: Output.object({\n    schema: z.object({\n      ingredients: z.array(z.string()),\n      steps: z.array(z.string()),\n    }),\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Specifying Experimental Structured Output Settings (TypeScript)\nDESCRIPTION: This snippet describes the experimental_output structure in TypeScript, designed to configure output generation modes like text forwarding and structured JSON creation based on a schema. It uses an Output interface with methods such as Output.text() and Output.object(), where the latter accepts schema configuration. It depends on appropriate schema definitions (e.g., zod) and allows for type-safe structured output, making it ideal for integrations needing structured or validated AI responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  name: 'experimental_output',\n  type: 'Output',\n  isOptional: true,\n  description: 'Experimental setting for generating structured outputs.',\n  properties: [\n    {\n      type: 'Output',\n      parameters: [\n        {\n          name: 'Output.text()',\n          type: 'Output',\n          description: 'Forward text output.',\n        },\n        {\n          name: 'Output.object()',\n          type: 'Output',\n          description: 'Generate a JSON object of type OBJECT.',\n          properties: [\n            {\n              type: 'Options',\n              parameters: [\n                {\n                  name: 'schema',\n                  type: 'Schema<OBJECT>',\n                  description: 'The schema of the JSON object to generate.',\n                },\n              ],\n            },\n          ],\n        },\n      ],\n    },\n  ],\n},\n```\n\n----------------------------------------\n\nTITLE: Accessing Response Headers and Body in TypeScript\nDESCRIPTION: This snippet shows how to access the raw response headers and body from the model provider after generating text using the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  // ...\n});\n\nconsole.log(JSON.stringify(result.response.headers, null, 2));\nconsole.log(JSON.stringify(result.response.body, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Sending Custom Data with createDataStream in Hono\nDESCRIPTION: This example shows how to use createDataStream to send custom data to the client, including streaming text generation with enhanced error handling. It demonstrates merging custom data with AI-generated content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/30-hono.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { serve } from '@hono/node-server';\nimport { createDataStream, streamText } from 'ai';\nimport { Hono } from 'hono';\nimport { stream } from 'hono/streaming';\n\nconst app = new Hono();\n\napp.post('/stream-data', async c => {\n  // immediately start streaming the response\n  const dataStream = createDataStream({\n    execute: async dataStreamWriter => {\n      dataStreamWriter.writeData('initialized call');\n\n      const result = streamText({\n        model: openai('gpt-4o'),\n        prompt: 'Invent a new holiday and describe its traditions.',\n      });\n\n      result.mergeIntoDataStream(dataStreamWriter);\n    },\n    onError: error => {\n      // Error messages are masked by default for security reasons.\n      // If you want to expose the error message to the client, you can do so here:\n      return error instanceof Error ? error.message : String(error);\n    },\n  });\n\n  // Mark the response as a v1 data stream:\n  c.header('X-Vercel-AI-Data-Stream', 'v1');\n  c.header('Content-Type', 'text/plain; charset=utf-8');\n\n  return stream(c, stream =>\n    stream.pipe(dataStream.pipeThrough(new TextEncoderStream())),\n  );\n});\n\nserve({ fetch: app.fetch, port: 8080 });\n```\n\n----------------------------------------\n\nTITLE: Implementing streamToResponse with Node.js HTTP Server\nDESCRIPTION: Complete example showing how to use streamToResponse to pipe a data stream to a Node.js HTTP server response. Demonstrates integration with OpenAI, stream handling, and data management using StreamData.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/05-stream-to-response.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { StreamData, streamText, streamToResponse } from 'ai';\nimport { createServer } from 'http';\n\ncreateServer(async (req, res) => {\n  const result = streamText({\n    model: openai('gpt-4-turbo'),\n    prompt: 'What is the weather in San Francisco?',\n  });\n\n  // use stream data\n  const data = new StreamData();\n\n  data.append('initialized call');\n\n  streamToResponse(\n    result.toAIStream({\n      onFinal() {\n        data.append('call completed');\n        data.close();\n      },\n    }),\n    res,\n    {},\n    data,\n  );\n}).listen(8080);\n```\n\n----------------------------------------\n\nTITLE: Implementing Function Calling with Language Models in TypeScript\nDESCRIPTION: This code snippet demonstrates how to set up a language model to execute specific functions based on user queries. It defines a sendMessage function that uses a GPT-3.5-turbo model to process weather-related queries and potentially call a getWeather function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/08-model-as-router.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst sendMessage = (prompt: string) =>\n  generateText({\n    model: 'gpt-3.5-turbo',\n    system: 'you are a friendly weather assistant!',\n    prompt,\n    tools: {\n      getWeather: {\n        description: 'Get the weather in a location',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }: { location: string }) => ({\n          location,\n          temperature: 72 + Math.floor(Math.random() * 21) - 10,\n        }),\n      },\n    },\n  });\n\nsendMessage('What is the weather in San Francisco?'); // getWeather is called\nsendMessage('What is the weather in New York?'); // getWeather is called\nsendMessage('What events are happening in London?'); // No function is called\n```\n\n----------------------------------------\n\nTITLE: DeepInfra Text Generation Example\nDESCRIPTION: Example of text generation using Meta's Llama 3.1 model on DeepInfra with the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#2025-04-23_snippet_11\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText } from 'ai';\nimport { deepinfra } from '@ai-sdk/deepinfra';\n\nconst { text } = await generateText({\n  model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),\n  prompt: 'What is love?',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing API Route for Chat with Tool Calling in Next.js\nDESCRIPTION: This code snippet demonstrates how to create an API route in Next.js that handles chat requests with tool calling. It uses the streamText function from the AI SDK to generate responses and defines a tool for getting weather information.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/70-call-tools.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ToolInvocation, streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\ninterface Message {\n  role: 'user' | 'assistant';\n  content: string;\n  toolInvocations?: ToolInvocation[];\n}\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: Message[] } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'You are a helpful assistant.',\n    messages,\n    tools: {\n      getWeather: {\n        description: 'Get the weather for a location',\n        parameters: z.object({\n          city: z.string().describe('The city to get the weather for'),\n          unit: z\n            .enum(['C', 'F'])\n            .describe('The unit to display the temperature in'),\n        }),\n        execute: async ({ city, unit }) => {\n          const weather = {\n            value: 24,\n            description: 'Sunny',\n          };\n\n          return `It is currently ${weather.value}Â°${unit} and ${weather.description} in ${city}!`;\n        },\n      },\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding a Single Value with OpenAI Model in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the 'embed' function from the AI SDK to embed a single text value using an OpenAI embedding model. It returns an embedding object containing a vector representation of the input text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/30-embeddings.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { embed } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n// 'embedding' is a single embedding object (number[])\nconst { embedding } = await embed({\n  model: openai.embedding('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Google Generative AI\nDESCRIPTION: Uses the generateText function with a Google Generative AI model to generate a vegetarian lasagna recipe with a simple text prompt.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: google('gemini-1.5-pro-latest'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Stream with Hono and AI SDK\nDESCRIPTION: This snippet demonstrates how to use the toDataStream method to stream AI-generated text from OpenAI to the client through a Hono server. It sets up necessary headers for Vercel AI data streaming.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/30-hono.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { serve } from '@hono/node-server';\nimport { streamText } from 'ai';\nimport { Hono } from 'hono';\nimport { stream } from 'hono/streaming';\n\nconst app = new Hono();\n\napp.post('/', async c => {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    prompt: 'Invent a new holiday and describe its traditions.',\n  });\n\n  // Mark the response as a v1 data stream:\n  c.header('X-Vercel-AI-Data-Stream', 'v1');\n  c.header('Content-Type', 'text/plain; charset=utf-8');\n\n  return stream(c, stream => stream.pipe(result.toDataStream()));\n});\n\nserve({ fetch: app.fetch, port: 8080 });\n```\n\n----------------------------------------\n\nTITLE: Custom Error Handler with Message Replacement in React\nDESCRIPTION: Shows how to implement a custom submit handler that removes the last message when an error occurs and resubmits the form.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/21-error-handling.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const {\n    handleInputChange,\n    handleSubmit,\n    error,\n    input,\n    messages,\n    setMessages,\n  } = useChat({});\n\n  function customSubmit(event: React.FormEvent<HTMLFormElement>) {\n    if (error != null) {\n      setMessages(messages.slice(0, -1)); // remove last message\n    }\n\n    handleSubmit(event);\n  }\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      {error && <div>An error occurred.</div>}\n\n      <form onSubmit={customSubmit}>\n        <input value={input} onChange={handleInputChange} />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Server-side Multi-Step Tool Calls\nDESCRIPTION: Demonstrates setting up server-side multi-step tool calls with execute functions using the streamText configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport { z } from 'zod';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n    tools: {\n      getWeatherInformation: {\n        description: 'show the weather in a given city to the user',\n        parameters: z.object({ city: z.string() }),\n        // tool has execute function:\n        execute: async ({}: { city: string }) => {\n          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];\n          return weatherOptions[\n            Math.floor(Math.random() * weatherOptions.length)\n          ];\n        },\n      },\n    },\n    maxSteps: 5,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Route for Chatbot Backend\nDESCRIPTION: This code sets up an API route to handle chat requests using OpenAI's GPT-4 model. It streams text responses and limits the duration to 30 seconds.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_1\n\nLANGUAGE: ts\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4-turbo'),\n    system: 'You are a helpful assistant.',\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Server Actions for OpenAI Assistant API Integration\nDESCRIPTION: This server-side component defines actions for interacting with the OpenAI Assistant API. It handles message submission, thread creation, run management, and tool calls for email searching. The function streams responses and GUI updates to the client.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/121-stream-assistant-response-with-tools.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { generateId } from 'ai';\nimport { createStreamableUI, createStreamableValue } from 'ai/rsc';\nimport { OpenAI } from 'openai';\nimport { ReactNode } from 'react';\nimport { searchEmails } from './function';\nimport { Message } from './message';\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\nexport interface ClientMessage {\n  id: string;\n  status: ReactNode;\n  text: ReactNode;\n  gui: ReactNode;\n}\n\nconst ASSISTANT_ID = 'asst_xxxx';\nlet THREAD_ID = '';\nlet RUN_ID = '';\n\nexport async function submitMessage(question: string): Promise<ClientMessage> {\n  const status = createStreamableUI('thread.init');\n  const textStream = createStreamableValue('');\n  const textUIStream = createStreamableUI(\n    <Message textStream={textStream.value} />,\n  );\n  const gui = createStreamableUI();\n\n  const runQueue = [];\n\n  (async () => {\n    if (THREAD_ID) {\n      await openai.beta.threads.messages.create(THREAD_ID, {\n        role: 'user',\n        content: question,\n      });\n\n      const run = await openai.beta.threads.runs.create(THREAD_ID, {\n        assistant_id: ASSISTANT_ID,\n        stream: true,\n      });\n\n      runQueue.push({ id: generateId(), run });\n    } else {\n      const run = await openai.beta.threads.createAndRun({\n        assistant_id: ASSISTANT_ID,\n        stream: true,\n        thread: {\n          messages: [{ role: 'user', content: question }],\n        },\n      });\n\n      runQueue.push({ id: generateId(), run });\n    }\n\n    while (runQueue.length > 0) {\n      const latestRun = runQueue.shift();\n\n      if (latestRun) {\n        for await (const delta of latestRun.run) {\n          const { data, event } = delta;\n\n          status.update(event);\n\n          if (event === 'thread.created') {\n            THREAD_ID = data.id;\n          } else if (event === 'thread.run.created') {\n            RUN_ID = data.id;\n          } else if (event === 'thread.message.delta') {\n            data.delta.content?.map((part: any) => {\n              if (part.type === 'text') {\n                if (part.text) {\n                  textStream.append(part.text.value);\n                }\n              }\n            });\n          } else if (event === 'thread.run.requires_action') {\n            if (data.required_action) {\n              if (data.required_action.type === 'submit_tool_outputs') {\n                const { tool_calls } = data.required_action.submit_tool_outputs;\n                const tool_outputs = [];\n\n                for (const tool_call of tool_calls) {\n                  const { id: toolCallId, function: fn } = tool_call;\n                  const { name, arguments: args } = fn;\n\n                  if (name === 'search_emails') {\n                    const { query, has_attachments } = JSON.parse(args);\n\n                    gui.append(\n                      <div className=\"flex flex-row gap-2 items-center\">\n                        <div>\n                          Searching for emails: {query}, has_attachments:\n                          {has_attachments ? 'true' : 'false'}\n                        </div>\n                      </div>,\n                    );\n\n                    await new Promise(resolve => setTimeout(resolve, 2000));\n\n                    const fakeEmails = searchEmails({ query, has_attachments });\n\n                    gui.append(\n                      <div className=\"flex flex-col gap-2\">\n                        {fakeEmails.map(email => (\n                          <div\n                            key={email.id}\n                            className=\"p-2 bg-zinc-100 rounded-md flex flex-row gap-2 items-center justify-between\"\n                          >\n                            <div className=\"flex flex-row gap-2 items-center\">\n                              <div>{email.subject}</div>\n                            </div>\n                            <div className=\"text-zinc-500\">{email.date}</div>\n                          </div>\n                        ))}\n                      </div>,\n                    );\n\n                    tool_outputs.push({\n                      tool_call_id: toolCallId,\n                      output: JSON.stringify(fakeEmails),\n                    });\n                  }\n                }\n\n                const nextRun: any =\n                  await openai.beta.threads.runs.submitToolOutputs(\n                    THREAD_ID,\n                    RUN_ID,\n                    {\n                      tool_outputs,\n                      stream: true,\n                    },\n                  );\n\n                runQueue.push({ id: generateId(), run: nextRun });\n              }\n            }\n          } else if (event === 'thread.run.failed') {\n            console.log(data);\n          }\n        }\n      }\n    }\n\n    status.done();\n    textUIStream.done();\n    gui.done();\n  })();\n\n  return {\n    id: generateId(),\n    status: status.value,\n    text: textUIStream.value,\n    gui: gui.value,\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Lazy Stream Generation in JavaScript\nDESCRIPTION: This snippet demonstrates a lazy approach to stream generation using a pull handler in ReadableStream. It addresses back-pressure issues by producing values only when requested by the consumer, ensuring efficient memory usage and proper cancellation handling.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/03-backpressure.mdx#2025-04-23_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\nfunction createStream(iterator) {\n  return new ReadableStream({\n    async pull(controller) {\n      const { value, done } = await iterator.next();\n\n      if (done) {\n        controller.close();\n      } else {\n        controller.enqueue(value);\n      }\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Structured JSON Data with AI SDK and OpenAI in TypeScript\nDESCRIPTION: This code demonstrates how to use the AI SDK's generateObject function with OpenAI's GPT-4 model to create structured JSON data. It defines a zod schema for a recipe object with name, ingredients, and steps, then generates a lasagna recipe matching this structure and logs the result.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/30-generate-object.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nconst result = await generateObject({\n  model: openai('gpt-4-turbo'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(\n        z.object({\n          name: z.string(),\n          amount: z.string(),\n        }),\n      ),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n\nconsole.log(JSON.stringify(result.object.recipe, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Implementing PDF Chat Server Route Handler in Next.js\nDESCRIPTION: Creates a server-side route handler that processes chat messages and PDFs using Anthropic's Claude model. The handler streams text responses back to the client.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/23-chat-with-pdf.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { streamText } from 'ai';\n\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: anthropic('claude-3-5-sonnet-latest'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Object with Schema in TypeScript\nDESCRIPTION: Example showing how to generate a structured object (recipe) using a Zod schema with the generateObject function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/03-generate-object.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: openai('gpt-4-turbo'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.string()),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n\nconsole.log(JSON.stringify(object, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Implementing AI Chatbot with Streaming Responses\nDESCRIPTION: Main application code implementing an interactive chatbot using the AI SDK, with real-time streaming responses from OpenAI's GPT model. Includes terminal interface setup and message history management.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { CoreMessage, streamText } from 'ai';\nimport dotenv from 'dotenv';\nimport * as readline from 'node:readline/promises';\n\ndotenv.config();\n\nconst terminal = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n});\n\nconst messages: CoreMessage[] = [];\n\nasync function main() {\n  while (true) {\n    const userInput = await terminal.question('You: ');\n\n    messages.push({ role: 'user', content: userInput });\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages,\n    });\n\n    let fullResponse = '';\n    process.stdout.write('\\nAssistant: ');\n    for await (const delta of result.textStream) {\n      fullResponse += delta;\n      process.stdout.write(delta);\n    }\n    process.stdout.write('\\n\\n');\n\n    messages.push({ role: 'assistant', content: fullResponse });\n  }\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Implementing AI Chat Frontend with User Confirmation in React\nDESCRIPTION: This React component implements the frontend for an AI chat interface. It uses the useChat hook, handles tool calls that require user confirmation, and renders messages with different types of content including text and tool invocations.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { Message, useChat } from '@ai-sdk/react';\nimport {\n  APPROVAL,\n  getToolsRequiringConfirmation,\n} from '../api/use-chat-human-in-the-loop/utils';\nimport { tools } from '../api/use-chat-human-in-the-loop/tools';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit, addToolResult } =\n    useChat({\n      maxSteps: 5,\n    });\n\n  const toolsRequiringConfirmation = getToolsRequiringConfirmation(tools);\n\n  // used to disable input while confirmation is pending\n  const pendingToolCallConfirmation = messages.some((m: Message) =>\n    m.parts?.some(\n      part =>\n        part.type === 'tool-invocation' &&\n        part.toolInvocation.state === 'call' &&\n        toolsRequiringConfirmation.includes(part.toolInvocation.toolName),\n    ),\n  );\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages?.map((m: Message) => (\n        <div key={m.id} className=\"whitespace-pre-wrap\">\n          <strong>{`${m.role}: `}</strong>\n          {m.parts?.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={i}>{part.text}</div>;\n              case 'tool-invocation':\n                const toolInvocation = part.toolInvocation;\n                const toolCallId = toolInvocation.toolCallId;\n                const dynamicInfoStyles = 'font-mono bg-gray-100 p-1 text-sm';\n\n                // render confirmation tool (client-side tool with user interaction)\n                if (\n                  toolsRequiringConfirmation.includes(\n                    toolInvocation.toolName,\n                  ) &&\n                  toolInvocation.state === 'call'\n                ) {\n                  return (\n                    <div key={toolCallId} className=\"text-gray-500\">\n                      Run{' '}\n                      <span className={dynamicInfoStyles}>\n                        {toolInvocation.toolName}\n                      </span>{' '}\n                      with args:{' '}\n                      <span className={dynamicInfoStyles}>\n                        {JSON.stringify(toolInvocation.args)}\n                      </span>\n                      <div className=\"flex gap-2 pt-2\">\n                        <button\n                          className=\"px-4 py-2 font-bold text-white bg-blue-500 rounded hover:bg-blue-700\"\n                          onClick={() =>\n                            addToolResult({\n                              toolCallId,\n                              result: APPROVAL.YES,\n                            })\n                          }\n                        >\n                          Yes\n                        </button>\n                        <button\n                          className=\"px-4 py-2 font-bold text-white bg-red-500 rounded hover:bg-red-700\"\n                          onClick={() =>\n                            addToolResult({\n                              toolCallId,\n                              result: APPROVAL.NO,\n                            })\n                          }\n                        >\n                          No\n                        </button>\n                      </div>\n                    </div>\n                  );\n                }\n            }\n          })}\n          <br />\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          disabled={pendingToolCallConfirmation}\n          className=\"fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Building Client-side Chat Interface with Tool Integration in React\nDESCRIPTION: Implements a client-side chat interface using the useChat hook, handling tool invocations, and managing user interactions. Features include automatic tool execution, confirmation dialogs, and real-time message streaming with tool results display.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { ToolInvocation } from 'ai';\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit, addToolResult } =\n    useChat({\n      maxSteps: 5,\n\n      async onToolCall({ toolCall }) {\n        if (toolCall.toolName === 'getLocation') {\n          const cities = ['New York', 'Los Angeles', 'Chicago', 'San Francisco'];\n          return cities[Math.floor(Math.random() * cities.length)];\n        }\n      },\n    });\n\n  return (\n    <>\n      {messages?.map(message => (\n        <div key={message.id}>\n          <strong>{`${message.role}: `}</strong>\n          {message.parts.map(part => {\n            switch (part.type) {\n              case 'text':\n                return part.text;\n\n              case 'tool-invocation': {\n                const callId = part.toolInvocation.toolCallId;\n\n                switch (part.toolInvocation.toolName) {\n                  case 'askForConfirmation': {\n                    switch (part.toolInvocation.state) {\n                      case 'call':\n                        return (\n                          <div key={callId}>\n                            {part.toolInvocation.args.message}\n                            <div>\n                              <button onClick={() => addToolResult({\n                                toolCallId: callId,\n                                result: 'Yes, confirmed.'\n                              })}>\n                                Yes\n                              </button>\n                              <button onClick={() => addToolResult({\n                                toolCallId: callId,\n                                result: 'No, denied'\n                              })}>\n                                No\n                              </button>\n                            </div>\n                          </div>\n                        );\n                      case 'result':\n                        return (\n                          <div key={callId}>\n                            Location access allowed: {part.toolInvocation.result}\n                          </div>\n                        );\n                    }\n                    break;\n                  }\n\n                  case 'getLocation': {\n                    switch (part.toolInvocation.state) {\n                      case 'call':\n                        return <div key={callId}>Getting location...</div>;\n                      case 'result':\n                        return (\n                          <div key={callId}>\n                            Location: {part.toolInvocation.result}\n                          </div>\n                        );\n                    }\n                    break;\n                  }\n\n                  case 'getWeatherInformation': {\n                    switch (part.toolInvocation.state) {\n                      case 'partial-call':\n                        return (\n                          <pre key={callId}>\n                            {JSON.stringify(part.toolInvocation, null, 2)}\n                          </pre>\n                        );\n                      case 'call':\n                        return (\n                          <div key={callId}>\n                            Getting weather information for {part.toolInvocation.args.city}...\n                          </div>\n                        );\n                      case 'result':\n                        return (\n                          <div key={callId}>\n                            Weather in {part.toolInvocation.args.city}: {part.toolInvocation.result}\n                          </div>\n                        );\n                    }\n                    break;\n                  }\n                }\n              }\n            }\n          })}\n          <br />\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n      </form>\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Object with Schema Name and Description in TypeScript\nDESCRIPTION: This snippet shows how to use the generateObject function with a schema name and description for additional LLM guidance when generating a recipe object.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: yourModel,\n  schemaName: 'Recipe',\n  schemaDescription: 'A recipe for a dish.',\n  schema: z.object({\n    name: z.string(),\n    ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n    steps: z.array(z.string()),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Anthropic Provider: Facade Replacement - TypeScript\nDESCRIPTION: Demonstrates substituting the removed Anthropic facade constructor with the createAnthropic factory function for provider initialization. Requires ai-sdk 4.0+ and proper imports. The input options object configures authentication and endpoints; output is a provider instance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_2\n\nLANGUAGE: ts\nCODE:\n```\nconst anthropic = new Anthropic({\n  // ...\n});\n```\n\nLANGUAGE: ts\nCODE:\n```\nconst anthropic = createAnthropic({\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Extracting and Defining a Reusable Weather Tool with AI SDK in TypeScript\nDESCRIPTION: This code illustrates the extraction of a tool definition into a dedicated module using the AI SDK 'tool' helper for strong type inference. The 'weatherTool' exposes a weather-fetching operation, describing its schema via the 'zod' schema builder, and supports asynchronous execution returning mock temperature data. Dependencies include the AI SDK and 'zod'. The defined parameter is the 'location' string, with the output including randomized temperature and the requested location.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nimport { tool } from 'ai';\nimport { z } from 'zod';\n\n// the `tool` helper function ensures correct type inference:\nexport const weatherTool = tool({\n  description: 'Get the weather in a location',\n  parameters: z.object({\n    location: z.string().describe('The location to get the weather for'),\n  }),\n  execute: async ({ location }) => ({\n    location,\n    temperature: 72 + Math.floor(Math.random() * 21) - 10,\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Creating an Ollama Chat Model Client (TypeScript)\nDESCRIPTION: Creates a language model client configured to use a specific Ollama chat model (e.g., 'phi3') via the Ollama Chat Completion API. The client is created by calling the provider instance (default or custom) as a function with the model ID as the argument.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/03-ollama.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = ollama('phi3');\n```\n\n----------------------------------------\n\nTITLE: Implementing Protected Server Action with Authentication in Next.js\nDESCRIPTION: Demonstrates how to create a protected server action that validates authentication tokens before returning streamed UI components. The example shows a weather data endpoint that checks for a valid token in cookies before proceeding with the response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/09-authentication.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { cookies } from 'next/headers';\nimport { createStremableUI } from 'ai/rsc';\nimport { validateToken } from '../utils/auth';\n\nexport const getWeather = async () => {\n  const token = cookies().get('token');\n\n  if (!token || !validateToken(token)) {\n    return {\n      error: 'This action requires authentication',\n    };\n  }\n  const streamableDisplay = createStreamableUI(null);\n\n  streamableDisplay.update(<Skeleton />);\n  streamableDisplay.done(<Weather />);\n\n  return {\n    display: streamableDisplay.value,\n  };\n};\n```\n\n----------------------------------------\n\nTITLE: Repairing Invalid JSON in Object Generation with TypeScript\nDESCRIPTION: This snippet shows how to use the experimental repairText function to attempt to repair invalid or malformed JSON generated by the model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateObject } from 'ai';\n\nconst { object } = await generateObject({\n  model,\n  schema,\n  prompt,\n  experimental_repairText: async ({ text, error }) => {\n    // example: add a closing brace to the text\n    return text + '}';\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: StreamUI with Weather Tool Implementation\nDESCRIPTION: Extended example showing how to implement a weather tool with streamUI, including loading states and async weather data fetching.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/02-streaming-react-components.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nconst result = await streamUI({\n  model: openai('gpt-4o'),\n  prompt: 'Get the weather for San Francisco',\n  text: ({ content }) => <div>{content}</div>,\n  tools: {\n    getWeather: {\n      description: 'Get the weather for a location',\n      parameters: z.object({ location: z.string() }),\n      generate: async function* ({ location }) {\n        yield <LoadingComponent />;\n        const weather = await getWeather(location);\n        return <WeatherComponent weather={weather} location={location} />;\n      },\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Server-Side Chat Generation with OpenAI\nDESCRIPTION: A server action that handles the chat generation logic using OpenAI's GPT-3.5-turbo model. Implements conversation continuation by generating responses based on chat history and a system prompt.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/11-generate-text-with-chat-prompt.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n'use server';\n\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nexport interface Message {\n  role: 'user' | 'assistant';\n  content: string;\n}\n\nexport async function continueConversation(history: Message[]) {\n  'use server';\n\n  const { text } = await generateText({\n    model: openai('gpt-3.5-turbo'),\n    system: 'You are a friendly assistant!',\n    messages: history,\n  });\n\n  return {\n    messages: [\n      ...history,\n      {\n        role: 'assistant' as const,\n        content: text,\n      },\n    ],\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Closing an MCP Client After Streaming Responses in AI SDK using TypeScript\nDESCRIPTION: This snippet demonstrates proper resource cleanup by closing the MCP client after concluding a streaming interaction with a language model using 'streamText'. The callback 'onFinish' ensures closure occurs when the LLM response ends. Dependencies include the AI SDK supporting both MCP and text streaming models. Inputs include model, prompt, and tools; output is streamed text with controlled resource management.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_20\n\nLANGUAGE: typescript\nCODE:\n```\nconst mcpClient = await experimental_createMCPClient({\n  // ...\n});\n\nconst result = await streamText({\n  model: openai('gpt-4o'),\n  tools: mcpClient.tools(),\n  prompt: 'What is the weather in Brooklyn, New York?',\n  onFinish: async () => {\n    await mcpClient.close();\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using MCP Clients with AI SDK in TypeScript\nDESCRIPTION: This code snippet demonstrates how to initialize MCP clients using different transport methods (stdio, SSE, and custom), retrieve tools from these clients, and use them with the generateText function. It also shows proper resource management by closing the clients after use.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/54-mcp-tools.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_createMCPClient, generateText } from 'ai';\nimport { Experimental_StdioMCPTransport } from 'ai/mcp-stdio';\nimport { openai } from '@ai-sdk/openai';\n\nlet clientOne;\nlet clientTwo;\nlet clientThree;\n\ntry {\n  // Initialize an MCP client to connect to a `stdio` MCP server:\n  const transport = new Experimental_StdioMCPTransport({\n    command: 'node',\n    args: ['src/stdio/dist/server.js'],\n  });\n  clientOne = await experimental_createMCPClient({\n    transport,\n  });\n\n  // Alternatively, you can connect to a Server-Sent Events (SSE) MCP server:\n  clientTwo = await experimental_createMCPClient({\n    transport: {\n      type: 'sse',\n      url: 'http://localhost:3000/sse',\n    },\n  });\n\n  // Similarly to the stdio example, you can pass in your own custom transport as long as it implements the `MCPTransport` interface:\n  const transport = new MyCustomTransport({\n    // ...\n  });\n  clientThree = await experimental_createMCPClient({\n    transport,\n  });\n\n  const toolSetOne = await clientOne.tools();\n  const toolSetTwo = await clientTwo.tools();\n  const toolSetThree = await clientThree.tools();\n  const tools = {\n    ...toolSetOne,\n    ...toolSetTwo,\n    ...toolSetThree, // note: this approach causes subsequent tool sets to override tools with the same name\n  };\n\n  const response = await generateText({\n    model: openai('gpt-4o'),\n    tools,\n    messages: [\n      {\n        role: 'user',\n        content: 'Find products under $100',\n      },\n    ],\n  });\n\n  console.log(response.text);\n} catch (error) {\n  console.error(error);\n} finally {\n  await Promise.all([clientOne.close(), clientTwo.close()]);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAI Assistant Chat UI in React\nDESCRIPTION: This snippet demonstrates how to create a chat interface using the useAssistant hook in a React component. It handles message rendering, input management, and submission of user messages to the assistant API.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/10-openai-assistants.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { Message, useAssistant } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { status, messages, input, submitMessage, handleInputChange } =\n    useAssistant({ api: '/api/assistant' });\n\n  return (\n    <div>\n      {messages.map((m: Message) => (\n        <div key={m.id}>\n          <strong>{`${m.role}: `}</strong>\n          {m.role !== 'data' && m.content}\n          {m.role === 'data' && (\n            <>\n              {(m.data as any).description}\n              <br />\n              <pre className={'bg-gray-200'}>\n                {JSON.stringify(m.data, null, 2)}\n              </pre>\n            </>\n          )}\n        </div>\n      ))}\n\n      {status === 'in_progress' && <div />}\n\n      <form onSubmit={submitMessage}>\n        <input\n          disabled={status !== 'awaiting_message'}\n          value={input}\n          placeholder=\"What is the temperature in the living room?\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Query Generation Prompt for GPT-4o\nDESCRIPTION: Comprehensive prompt engineering for OpenAI's models to generate SQL queries from natural language. This prompt includes schema information, query rules, and specific guidance for handling the unicorn dataset.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_6\n\nLANGUAGE: txt\nCODE:\n```\nYou are a SQL (postgres) and data visualization expert. Your job is to help the user write a SQL query to retrieve the data they need. The table schema is as follows:\n\nunicorns (\n  id SERIAL PRIMARY KEY,\n  company VARCHAR(255) NOT NULL UNIQUE,\n  valuation DECIMAL(10, 2) NOT NULL,\n  date_joined DATE,\n  country VARCHAR(255) NOT NULL,\n  city VARCHAR(255) NOT NULL,\n  industry VARCHAR(255) NOT NULL,\n  select_investors TEXT NOT NULL\n);\n\nOnly retrieval queries are allowed.\n\nFor things like industry, company names and other string fields, use the ILIKE operator and convert both the search term and the field to lowercase using LOWER() function. For example: LOWER(industry) ILIKE LOWER('%search_term%').\n\nNote: select_investors is a comma-separated list of investors. Trim whitespace to ensure you're grouping properly. Note, some fields may be null or have only one value.\nWhen answering questions about a specific field, ensure you are selecting the identifying column (ie. what is Vercel's valuation would select company and valuation').\n\nThe industries available are:\n- healthcare & life sciences\n- consumer & retail\n- financial services\n- enterprise tech\n- insurance\n- media & entertainment\n- industrials\n- health\n\nIf the user asks for a category that is not in the list, infer based on the list above.\n\nNote: valuation is in billions of dollars so 10b would be 10.0.\nNote: if the user asks for a rate, return it as a decimal. For example, 0.1 would be 10%.\n\nIf the user asks for 'over time' data, return by year.\n\nWhen searching for UK or USA, write out United Kingdom or United States respectively.\n\nEVERY QUERY SHOULD RETURN QUANTITATIVE DATA THAT CAN BE PLOTTED ON A CHART! There should always be at least two columns. If the user asks for a single column, return the column and the count of the column. If the user asks for a rate, return the rate as a decimal. For example, 0.1 would be 10%.\n```\n\n----------------------------------------\n\nTITLE: Example Google Vertex Grounding Metadata Response (JSON)\nDESCRIPTION: Provides a representative JSON object structure for groundingMetadata, showing fields for search queries, entry points, support segment data, grounding chunk references, and confidence scores. This structure is included in provider metadata when search grounding is enabled.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"groundingMetadata\": {\n    \"retrievalQueries\": [\"What's the weather in Chicago this weekend?\"],\n    \"searchEntryPoint\": {\n      \"renderedContent\": \"...\"\n    },\n    \"groundingSupports\": [\n      {\n        \"segment\": {\n          \"startIndex\": 0,\n          \"endIndex\": 65,\n          \"text\": \"Chicago weather changes rapidly, so layers let you adjust easily.\"\n        },\n        \"groundingChunkIndices\": [0],\n        \"confidenceScores\": [0.99]\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat UI with React Client Component\nDESCRIPTION: A React client component that manages chat conversation state and handles user input. Uses useState for state management and includes a form for message input with a send button that triggers the conversation continuation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/11-generate-text-with-chat-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { Message, continueConversation } from './actions';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [conversation, setConversation] = useState<Message[]>([]);\n  const [input, setInput] = useState<string>('');\n\n  return (\n    <div>\n      <div>\n        {conversation.map((message, index) => (\n          <div key={index}>\n            {message.role}: {message.content}\n          </div>\n        ))}\n      </div>\n\n      <div>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button\n          onClick={async () => {\n            const { messages } = await continueConversation([\n              ...conversation,\n              { role: 'user', content: input },\n            ]);\n\n            setConversation(messages);\n          }}\n        >\n          Send Message\n        </button>\n      </div>\n    </div>\n  );\n```\n\n----------------------------------------\n\nTITLE: Sending Custom Data in Stream with Node.js HTTP Server\nDESCRIPTION: Creates a Node.js HTTP server that immediately starts streaming a response with custom data before generating text with OpenAI's GPT-4o model. It also implements error handling to optionally expose error messages to clients.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/10-node-http-server.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { pipeDataStreamToResponse, streamText } from 'ai';\nimport { createServer } from 'http';\n\ncreateServer(async (req, res) => {\n  // immediately start streaming the response\n  pipeDataStreamToResponse(res, {\n    execute: async dataStreamWriter => {\n      dataStreamWriter.writeData('initialized call');\n\n      const result = streamText({\n        model: openai('gpt-4o'),\n        prompt: 'Invent a new holiday and describe its traditions.',\n      });\n\n      result.mergeIntoDataStream(dataStreamWriter);\n    },\n    onError: error => {\n      // Error messages are masked by default for security reasons.\n      // If you want to expose the error message to the client, you can do so here:\n      return error instanceof Error ? error.message : String(error);\n    },\n  });\n}).listen(8080);\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-side API Route for Parallel Tool Calling\nDESCRIPTION: This snippet demonstrates the implementation of a server-side API route that handles parallel tool calling. It uses the streamText function from the AI SDK to generate responses and includes a custom getWeather tool with Zod schema validation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/71-call-tools-in-parallel.mdx#2025-04-23_snippet_1\n\nLANGUAGE: ts\nCODE:\n```\nimport { ToolInvocation, streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\ninterface Message {\n  role: 'user' | 'assistant';\n  content: string;\n  toolInvocations?: ToolInvocation[];\n}\n\nfunction getWeather({ city, unit }) {\n  return { value: 25, description: 'Sunny' };\n}\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: Message[] } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'You are a helpful assistant.',\n    messages,\n    tools: {\n      getWeather: {\n        description: 'Get the weather for a location',\n        parameters: z.object({\n          city: z.string().describe('The city to get the weather for'),\n          unit: z\n            .enum(['C', 'F'])\n            .describe('The unit to display the temperature in'),\n        }),\n        execute: async ({ city, unit }) => {\n          const { value, description } = getWeather({ city, unit });\n          return `It is currently ${value}Â°${unit} and ${description} in ${city}!`;\n        },\n      },\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Chat Route Handler with Next.js and AI SDK\nDESCRIPTION: This code snippet shows how to create a route handler for a chat endpoint in a Next.js application using the AI SDK and OpenAI's GPT-4.5 model. It sets up streaming text generation with a 30-second response time limit.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/22-gpt-4-5.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\n// Allow responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4.5-preview'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Interface with Streaming in Next.js\nDESCRIPTION: This code snippet shows how to create a React component using the useChat hook from the AI SDK. It sets up a chat interface that streams responses in real-time when the user sends a message.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/21-stream-text-with-chat-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, setInput, append } = useChat();\n\n  return (\n    <div>\n      <input\n        value={input}\n        onChange={event => {\n          setInput(event.target.value);\n        }}\n        onKeyDown={async event => {\n          if (event.key === 'Enter') {\n            append({ content: input, role: 'user' });\n          }\n        }}\n      />\n\n      {messages.map((message, index) => (\n        <div key={index}>{message.content}</div>\n      ))}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Computer Tool with Vertex Anthropic in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to register and implement a Computer Tool using vertexAnthropic.tools.computer_20241022 for remote control of keyboard and mouse actions. The 'execute' async handler selects behavior based on the action parameter ('screenshot', mouse and key actions, etc.), reading and encoding PNG images for screenshots and logging or executing other actions as required. The 'experimental_toToolResultContent' function changes the return format for LLM consumption, distinguishing between string and image results. Dependencies include the vertexAnthropic library, the Node.js 'fs' module for file operations, and TypeScript for type-checked development. Key parameters 'action', 'coordinate', and 'text' customize functionality. Input actions determine the flow, while outputs are either tool-readable strings or image data; some actions require auxiliary data such as coordinates or text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_31\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst computerTool = vertexAnthropic.tools.computer_20241022({\n  displayWidthPx: 1920,\n  displayHeightPx: 1080,\n  displayNumber: 0, // Optional, for X11 environments\n\n  execute: async ({ action, coordinate, text }) => {\n    // Implement your computer control logic here\n    // Return the result of the action\n\n    // Example code:\n    switch (action) {\n      case 'screenshot': {\n        // multipart result:\n        return {\n          type: 'image',\n          data: fs\n            .readFileSync('./data/screenshot-editor.png')\n            .toString('base64'),\n        };\n      }\n      default: {\n        console.log('Action:', action);\n        console.log('Coordinate:', coordinate);\n        console.log('Text:', text);\n        return `executed ${action}`;\n      }\n    }\n  },\n\n  // map to tool result content for LLM consumption:\n  experimental_toToolResultContent(result) {\n    return typeof result === 'string'\n      ? [{ type: 'text', text: result }]\n      : [{ type: 'image', data: result.data, mimeType: 'image/png' }];\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Structured Object Generation in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the streamObject function from the AI SDK to stream a structured object as it's being generated.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamObject } from 'ai';\n\nconst { partialObjectStream } = streamObject({\n  // ...\n});\n\n// use partialObjectStream as an async iterable\nfor await (const partialObject of partialObjectStream) {\n  console.log(partialObject);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a React Component for Completion Using AI SDK\nDESCRIPTION: This code snippet shows how to create a React component that uses the AI SDK's useCompletion hook to handle completion requests. It renders the completion result and provides an input form for user prompts.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/04-adapters/02-llamaindex.mdx#2025-04-23_snippet_1\n\nLANGUAGE: TSX\nCODE:\n```\n'use client';\n\nimport { useCompletion } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { completion, input, handleInputChange, handleSubmit } =\n    useCompletion();\n\n  return (\n    <div>\n      {completion}\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a File Upload Form with React in Next.js\nDESCRIPTION: This code snippet shows how to create a client-side form for uploading PDF files in a Next.js application. It handles form submission, sends the file to an API endpoint, and displays the analysis result.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/31-generate-object-with-file-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\n\nexport default function Page() {\n  const [description, setDescription] = useState<string>();\n  const [loading, setLoading] = useState(false);\n\n  return (\n    <div>\n      <form\n        action={async formData => {\n          try {\n            setLoading(true);\n            const response = await fetch('/api/analyze', {\n              method: 'POST',\n              body: formData,\n            });\n            setLoading(false);\n\n            if (response.ok) {\n              setDescription(await response.text());\n            }\n          } catch (error) {\n            console.error('Analysis failed:', error);\n          }\n        }}\n      >\n        <div>\n          <label>Upload Image</label>\n          <input name=\"pdf\" type=\"file\" accept=\"application/pdf\" />\n        </div>\n        <button type=\"submit\" disabled={loading}>\n          Submit{loading && 'ing...'}\n        </button>\n      </form>\n      {description && <pre>{description}</pre>}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating Weather Component in Chat Interface with TypeScript React\nDESCRIPTION: This code updates the chat interface to render the Weather component when the model calls the weather tool. It checks for tool invocations in messages and dynamically renders the appropriate UI based on the tool state and name.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\nimport { Weather } from '@/components/weather';\n\nexport default function Page() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>\n          <div>{message.content}</div>\n\n          <div>\n            {message.toolInvocations?.map(toolInvocation => {\n              const { toolName, toolCallId, state } = toolInvocation;\n\n              if (state === 'result') {\n                if (toolName === 'displayWeather') {\n                  const { result } = toolInvocation;\n                  return (\n                    <div key={toolCallId}>\n                      <Weather {...result} />\n                    </div>\n                  );\n                }\n              } else {\n                return (\n                  <div key={toolCallId}>\n                    {toolName === 'displayWeather' ? (\n                      <div>Loading weather...</div>\n                    ) : null}\n                  </div>\n                );\n              }\n            })}\n          </div>\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={handleInputChange}\n          placeholder=\"Type a message...\"\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Route Handler for AI Chat with Tool Processing in TypeScript\nDESCRIPTION: This code snippet shows how to update a route handler to use the processToolCalls utility function. It handles incoming chat messages, processes tool calls, and streams responses using OpenAI's GPT-4 model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { createDataStreamResponse, Message, streamText } from 'ai';\nimport { processToolCalls } from './utils';\nimport { tools } from './tools';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: Message[] } = await req.json();\n\n  return createDataStreamResponse({\n    execute: async dataStream => {\n      // Utility function to handle tools that require human confirmation\n      // Checks for confirmation in last message and then runs associated tool\n      const processedMessages = await processToolCalls(\n        {\n          messages,\n          dataStream,\n          tools,\n        },\n        {\n          // type-safe object for tools without an execute function\n          getWeatherInformation: async ({ city }) => {\n            const conditions = ['sunny', 'cloudy', 'rainy', 'snowy'];\n            return `The weather in ${city} is ${\n              conditions[Math.floor(Math.random() * conditions.length)]\n            }.`;\n          },\n        },\n      );\n\n      const result = streamText({\n        model: openai('gpt-4o'),\n        messages: processedMessages,\n        tools,\n      });\n\n      result.mergeIntoDataStream(dataStream);\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Custom Data with useChat Hook in React\nDESCRIPTION: This snippet demonstrates how to access custom data and message annotations on the client side using the useChat hook from the AI SDK. It shows how to destructure data and messages, and display annotations.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/20-streaming-data.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { Message, useChat } from '@ai-sdk/react';\n\nconst { messages } = useChat();\n\nconst result = (\n  <>\n    {messages?.map((m: Message) => (\n      <div key={m.id}>\n        {m.annotations && <>{JSON.stringify(m.annotations)}</>}\n      </div>\n    ))}\n  </>\n);\n```\n\n----------------------------------------\n\nTITLE: Implementing Stock Price Tool in TypeScript\nDESCRIPTION: Defines a new stock price tool using createTool helper and updates the tools object to include both weather and stock tools. Includes a simulated API call with timeout.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// Add a new stock tool\nexport const stockTool = createTool({\n  description: 'Get price for a stock',\n  parameters: z.object({\n    symbol: z.string().describe('The stock symbol to get the price for'),\n  }),\n  execute: async function ({ symbol }) {\n    // Simulated API call\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    return { symbol, price: 100 };\n  },\n});\n\n// Update the tools object\nexport const tools = {\n  displayWeather: weatherTool,\n  getStockPrice: stockTool,\n};\n```\n\n----------------------------------------\n\nTITLE: Streaming Structured Objects with OpenAI and Zod Schema Validation in TypeScript\nDESCRIPTION: This code demonstrates how to use the `streamObject` function to generate and stream a structured object (a recipe) from OpenAI's GPT-4 Turbo model. It defines a schema using Zod for validation and streams the partial object updates as they are generated, allowing for real-time UI updates.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/40-stream-object.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { z } from 'zod';\n\nconst { partialObjectStream } = streamObject({\n  model: openai('gpt-4-turbo'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.string()),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n\nfor await (const partialObject of partialObjectStream) {\n  console.clear();\n  console.log(partialObject);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing API Route for OpenAI Assistant with Tool Functionality\nDESCRIPTION: Server-side API route implementation that handles assistant messages, creates threads, and processes tool calls. It includes specific logic for the celsiusToFahrenheit tool and uses AssistantResponse to stream results back to the client.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/121-stream-assistant-response-with-tools.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { AssistantResponse } from 'ai';\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY || '',\n});\n\nexport async function POST(req: Request) {\n  const input: {\n    threadId: string | null;\n    message: string;\n  } = await req.json();\n\n  const threadId = input.threadId ?? (await openai.beta.threads.create({})).id;\n\n  const createdMessage = await openai.beta.threads.messages.create(threadId, {\n    role: 'user',\n    content: input.message,\n  });\n\n  return AssistantResponse(\n    { threadId, messageId: createdMessage.id },\n    async ({ forwardStream }) => {\n      const runStream = openai.beta.threads.runs.stream(threadId, {\n        assistant_id:\n          process.env.ASSISTANT_ID ??\n          (() => {\n            throw new Error('ASSISTANT_ID is not set');\n          })(),\n      });\n\n      let runResult = await forwardStream(runStream);\n\n      while (\n        runResult?.status === 'requires_action' &&\n        runResult.required_action?.type === 'submit_tool_outputs'\n      ) {\n        const tool_outputs =\n          runResult.required_action.submit_tool_outputs.tool_calls.map(\n            (toolCall: any) => {\n              const parameters = JSON.parse(toolCall.function.arguments);\n\n              switch (toolCall.function.name) {\n                case 'celsiusToFahrenheit':\n                  const celsius = parseFloat(parameters.value);\n                  const fahrenheit = celsius * (9 / 5) + 32;\n\n                  return {\n                    tool_call_id: toolCall.id,\n                    output: `${celsius}Â°C is ${fahrenheit.toFixed(2)}Â°F`,\n                  };\n\n                default:\n                  throw new Error(\n                    `Unknown tool call function: ${toolCall.function.name}`,\n                  );\n              }\n            },\n          );\n\n        runResult = await forwardStream(\n          openai.beta.threads.runs.submitToolOutputsStream(\n            threadId,\n            runResult.id,\n            { tool_outputs },\n          ),\n        );\n      }\n    },\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Array Elements with Schema\nDESCRIPTION: Shows how to stream an array of structured objects using streamObject with element-wise streaming for RPG hero descriptions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/04-stream-object.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { z } from 'zod';\n\nconst { elementStream } = streamObject({\n  model: openai('gpt-4-turbo'),\n  output: 'array',\n  schema: z.object({\n    name: z.string(),\n    class: z\n      .string()\n      .describe('Character class, e.g. warrior, mage, or thief.'),\n    description: z.string(),\n  }),\n  prompt: 'Generate 3 hero descriptions for a fantasy role playing game.',\n});\n\nfor await (const hero of elementStream) {\n  console.log(hero);\n}\n```\n\n----------------------------------------\n\nTITLE: Using Image URLs in User Messages in TypeScript\nDESCRIPTION: This example demonstrates how to include an image via URL in a user message for multimodal AI processing.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'Describe the image in detail.' },\n        {\n          type: 'image',\n          image:\n            'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Speech Audio with OpenAI in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the generateSpeech function from the AI SDK to generate speech audio using OpenAI's TTS model. It imports necessary modules, calls the function with specified parameters, and logs the resulting audio.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/12-generate-speech.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { readFile } from 'fs/promises';\n\nconst { audio } = await generateSpeech({\n  model: openai.speech('tts-1'),\n  text: 'Hello from the AI SDK!',\n});\n\nconsole.log(audio);\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Chat API Route\nDESCRIPTION: Initial implementation of the chat API route handler with OpenAI integration and streaming response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_12\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Chat API Endpoint for Streaming Text in Next.js\nDESCRIPTION: This server-side code snippet demonstrates how to create an API endpoint for chat completion. It uses the streamText function from the AI SDK to generate and stream the assistant's response based on the conversation history.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/21-stream-text-with-chat-prompt.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText, UIMessage } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'You are a helpful assistant.',\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Image URL Input - TypeScript\nDESCRIPTION: This example shows how to generate text using GPT-4 Turbo model with an image provided via URL. The code uses the AI SDK to analyze an image of a solar eclipse and answer questions about it.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/12-generate-text-with-image-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst result = await generateText({\n  model: openai('gpt-4-turbo'),\n  maxTokens: 512,\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'what are the red things in this image?',\n        },\n        {\n          type: 'image',\n          image: new URL(\n            'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/2024_Solar_Eclipse_Prominences.jpg/720px-2024_Solar_Eclipse_Prominences.jpg',\n          ),\n        },\n      ],\n    },\n  ],\n});\n\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Permissioned Context using Crosshatch - TypeScript\nDESCRIPTION: Illustrates how to generate personalized text responses using the Crosshatch provider and the AI SDK's 'generateText' function. The model is instantiated with context-aware parameters, including a user authentication token and a custom data query that filters restaurants from the user's timeline. The system prompt injects recent user activity, and the user input is passed as a message. Inputs include the model and user/system messages, and the output is the generated text. '@crosshatch/ai-provider' and 'ai' modules are required, and a valid user auth token is necessary. Constraints: correct Crosshatch API access and valid token.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/21-crosshatch.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport createCrosshatch from '@crosshatch/ai-provider':\nconst crosshatch = createCrosshatch();\n\nconst { text } = await generateText({\n  model: crosshatch.languageModel(\"gpt-4o-mini\", {\n    token: 'YOUR_ACCESS_TOKEN',\n    replace: {\n      restaurants: {\n        select: [\"entity_name\", \"entity_city\", \"entity_region\"],\n        from: \"personalTimeline\",\n        where: [\n          { field: \"event\", op: \"=\", value: \"confirmed\" },\n          { field: \"entity_subtype2\", op: \"=\", value: \"RESTAURANTS\" }\n        ],\n        groupby: [\"entity_name\", \"entity_city\", \"entity_region\"],\n        orderby: \"count DESC\",\n        limit: 5\n      }\n    }\n  }),\n  system: `The user recently ate at these restaurants: {restaurants}`,\n  messages: [{role: \"user\", content: \"Where should I stay in Paris?\"}]\n});\n```\n\n----------------------------------------\n\nTITLE: Installing AWS Credential Providers Package\nDESCRIPTION: Commands to install the AWS SDK credential providers package for credential chain authentication.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @aws-sdk/credential-providers\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @aws-sdk/credential-providers\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @aws-sdk/credential-providers\n```\n\n----------------------------------------\n\nTITLE: Reading AI State in Client Component with useAIState\nDESCRIPTION: This example shows how to access the AI state in a client-side component using the useAIState hook from ai/rsc.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useAIState } from 'ai/rsc';\n\nexport default function Page() {\n  const [messages, setMessages] = useAIState();\n\n  return (\n    <ul>\n      {messages.map(message => (\n        <li key={message.id}>{message.content}</li>\n      ))}\n    </ul>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Tool Integration with OpenAI o1\nDESCRIPTION: Demonstrates how to integrate external tools with the AI SDK and OpenAI o1 model using custom tool definitions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/23-o1.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText, tool } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { text } = await generateText({\n  model: openai('o1'),\n  prompt: 'What is the weather like today?',\n  tools: {\n    getWeather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with AI SDK in TypeScript\nDESCRIPTION: Demonstrates how to use the generateText function to generate text for a given prompt and model. It shows a basic example of creating a vegetarian lasagna recipe.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: yourModel,\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Stream API Route in Next.js\nDESCRIPTION: Server-side API route handler that processes completion requests and returns streamed responses. Uses OpenAI's GPT-4 model and implements a 30-second timeout for streaming responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { prompt }: { prompt: string } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    prompt,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: OpenAI Text Generation Example\nDESCRIPTION: Example of text generation using OpenAI's GPT-4 model with the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#2025-04-23_snippet_10\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { text } = await generateText({\n  model: openai('gpt-4-turbo'),\n  prompt: 'What is love?',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Logging Middleware\nDESCRIPTION: Example of a logging middleware that tracks parameters and generated text for both standard and streaming calls.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { LanguageModelV1Middleware, LanguageModelV1StreamPart } from 'ai';\n\nexport const yourLogMiddleware: LanguageModelV1Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    console.log('doGenerate called');\n    console.log(`params: ${JSON.stringify(params, null, 2)}`);\n\n    const result = await doGenerate();\n\n    console.log('doGenerate finished');\n    console.log(`generated text: ${result.text}`);\n\n    return result;\n  },\n\n  wrapStream: async ({ doStream, params }) => {\n    console.log('doStream called');\n    console.log(`params: ${JSON.stringify(params, null, 2)}`);\n\n    const { stream, ...rest } = await doStream();\n\n    let generatedText = '';\n\n    const transformStream = new TransformStream<\n      LanguageModelV1StreamPart,\n      LanguageModelV1StreamPart\n    >({\n      transform(chunk, controller) {\n        if (chunk.type === 'text-delta') {\n          generatedText += chunk.textDelta;\n        }\n\n        controller.enqueue(chunk);\n      },\n\n      flush() {\n        console.log('doStream finished');\n        console.log(`generated text: ${generatedText}`);\n      },\n    });\n\n    return {\n      stream: stream.pipeThrough(transformStream),\n      ...rest,\n    };\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Generation with Baseten Model using streamText - TypeScript\nDESCRIPTION: Illustrates how to stream text responses from a Baseten model with the streamText function from the AI SDK in TypeScript. The snippet initiates a streaming text generation request and asynchronously iterates through the resulting textStream, logging output messages in real time. Requires prior setup of Baseten provider and model, and async/await for handling the stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/40-baseten.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { streamText } from 'ai';\n\nconst BASETEN_MODEL_ID = '<model-id>'; // e.g. 5q3z8xcw\nconst BASETEN_MODEL_URL = `https://model-${BASETEN_MODEL_ID}.api.baseten.co/environments/production/sync/v1`;\n\nconst baseten = createOpenAICompatible({\n  name: 'baseten',\n  baseURL: BASETEN_MODEL_URL,\n  headers: {\n    Authorization: `Bearer ${process.env.BASETEN_API_KEY ?? ''}`,\n  },\n});\n\nconst result = streamText({\n  model: baseten('llama'),\n  prompt: 'Tell me about yourself in one sentence',\n});\n\nfor await (const message of result.textStream) {\n  console.log(message);\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Responses with Tools Call using Mem0 - TypeScript\nDESCRIPTION: Illustrates integrating tool-based calling in AI inference using Mem0, supporting tool definitions via 'zod'. Dependencies: '@mem0/vercel-ai-provider', 'ai', and 'zod'. A Mem0 client is created for 'anthropic', a prompt is prepared, and 'generateText' is invoked with a tool (weather info) using structured parameters. Required input: model, tools, structured prompt. Output: response containing AI completion and tool call result. Best for extending LLMs with custom tool APIs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/70-mem0.mdx#2025-04-23_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { createMem0 } from '@mem0/vercel-ai-provider';\nimport { z } from 'zod';\n\nconst mem0 = createMem0({\n  provider: 'anthropic',\n  apiKey: 'anthropic-api-key',\n  mem0Config: {\n    // Global User ID\n    user_id: 'borat',\n  },\n});\n\nconst prompt = 'What the temperature in the city that I live in?';\n\nconst result = await generateText({\n  model: mem0('claude-3-5-sonnet-20240620'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n  prompt: prompt,\n});\n\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Tool Messages and Results in TypeScript\nDESCRIPTION: This snippet shows how to generate text using an AI model with tool messages and tool results, including image input and nutrition data output.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'How many calories are in this block of cheese?',\n        },\n        { type: 'image', image: fs.readFileSync('./data/roquefort.jpg') },\n      ],\n    },\n    {\n      role: 'assistant',\n      content: [\n        {\n          type: 'tool-call',\n          toolCallId: '12345',\n          toolName: 'get-nutrition-data',\n          args: { cheese: 'Roquefort' },\n        },\n        // there could be more tool calls here (parallel calling)\n      ],\n    },\n    {\n      role: 'tool',\n      content: [\n        {\n          type: 'tool-result',\n          toolCallId: '12345', // needs to match the tool call id\n          toolName: 'get-nutrition-data',\n          result: {\n            name: 'Cheese, roquefort',\n            calories: 369,\n            fat: 31,\n            protein: 22,\n          },\n        },\n        // there could be more tool results here (parallel calling)\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Streaming with OpenAI Chat in TypeScript\nDESCRIPTION: Demonstrates how to set up and use text streaming with the AI SDK for chat completions. The code initializes a chat context with system and user messages, configures the GPT-3.5-turbo model, and streams the response using async iteration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/21-stream-text-with-chat-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst result = streamText({\n  model: openai('gpt-3.5-turbo'),\n  maxTokens: 1024,\n  system: 'You are a helpful chatbot.',\n  messages: [\n    {\n      role: 'user',\n      content: 'Hello!',\n    },\n    {\n      role: 'assistant',\n      content: 'Hello! How can I help you today?',\n    },\n    {\n      role: 'user',\n      content: 'I need help with my computer.',\n    },\n  ],\n});\n\nfor await (const textPart of result.textStream) {\n  console.log(textPart);\n}\n```\n\n----------------------------------------\n\nTITLE: Importing generateText Function\nDESCRIPTION: Simple import statement showing how to import the generateText function from the AI package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Defining AI API Parameters in TypeScript\nDESCRIPTION: This code snippet defines the structure and types for various parameters used in the AI API, including model selection, system prompts, messages, and tools. It covers complex nested structures for different types of messages and tool configurations.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\n{\n  name: 'model',\n  type: 'LanguageModel',\n  description: \"The language model to use. Example: openai('gpt-4o')\",\n},\n{\n  name: 'system',\n  type: 'string',\n  description:\n    'The system prompt to use that specifies the behavior of the model.',\n},\n{\n  name: 'prompt',\n  type: 'string',\n  description: 'The input prompt to generate the text from.',\n},\n{\n  name: 'messages',\n  type: 'Array<CoreSystemMessage | CoreUserMessage | CoreAssistantMessage | CoreToolMessage> | Array<UIMessage>',\n  description:\n    'A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.',\n  properties: [\n    {\n      type: 'CoreSystemMessage',\n      parameters: [\n        {\n          name: 'role',\n          type: \"'system'\",\n          description: 'The role for the system message.',\n        },\n        {\n          name: 'content',\n          type: 'string',\n          description: 'The content of the message.',\n        },\n      ],\n    },\n    {\n      type: 'CoreUserMessage',\n      parameters: [\n        {\n          name: 'role',\n          type: \"'user'\",\n          description: 'The role for the user message.',\n        },\n        {\n          name: 'content',\n          type: 'string | Array<TextPart | ImagePart | FilePart>',\n          description: 'The content of the message.',\n          properties: [\n            {\n              type: 'TextPart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'text'\",\n                  description: 'The type of the message part.',\n                },\n                {\n                  name: 'text',\n                  type: 'string',\n                  description: 'The text content of the message part.',\n                },\n              ],\n            },\n            {\n              type: 'ImagePart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'image'\",\n                  description: 'The type of the message part.',\n                },\n                {\n                  name: 'image',\n                  type: 'string | Uint8Array | Buffer | ArrayBuffer | URL',\n                  description:\n                    'The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.',\n                },\n                {\n                  name: 'mimeType',\n                  type: 'string',\n                  isOptional: true,\n                  description: 'The mime type of the image. Optional.',\n                },\n              ],\n            },\n            {\n              type: 'FilePart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'file'\",\n                  description: 'The type of the message part.',\n                },\n                {\n                  name: 'data',\n                  type: 'string | Uint8Array | Buffer | ArrayBuffer | URL',\n                  description:\n                    'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.',\n                },\n                {\n                  name: 'mimeType',\n                  type: 'string',\n                  description: 'The mime type of the file.',\n                },\n              ],\n            },\n          ],\n        },\n      ],\n    },\n    {\n      type: 'CoreAssistantMessage',\n      parameters: [\n        {\n          name: 'role',\n          type: \"'assistant'\",\n          description: 'The role for the assistant message.',\n        },\n        {\n          name: 'content',\n          type: 'string | Array<TextPart | ReasoningPart | RedactedReasoningPart | ToolCallPart>',\n          description: 'The content of the message.',\n          properties: [\n            {\n              type: 'TextPart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'text'\",\n                  description: 'The type of the message part.',\n                },\n                {\n                  name: 'text',\n                  type: 'string',\n                  description: 'The text content of the message part.',\n                },\n              ],\n            },\n            {\n              type: 'ReasoningPart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'reasoning'\",\n                },\n                {\n                  name: 'text',\n                  type: 'string',\n                  description: 'The reasoning text.',\n                },\n                {\n                  name: 'signature',\n                  type: 'string',\n                  isOptional: true,\n                  description: 'The signature for the reasoning.',\n                },\n              ],\n            },\n            {\n              type: 'RedactedReasoningPart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'redacted-reasoning'\",\n                },\n                {\n                  name: 'data',\n                  type: 'string',\n                  description: 'The redacted data.',\n                },\n              ],\n            },\n            {\n              type: 'ToolCallPart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'tool-call'\",\n                  description: 'The type of the message part.',\n                },\n                {\n                  name: 'toolCallId',\n                  type: 'string',\n                  description: 'The id of the tool call.',\n                },\n                {\n                  name: 'toolName',\n                  type: 'string',\n                  description:\n                    'The name of the tool, which typically would be the name of the function.',\n                },\n                {\n                  name: 'args',\n                  type: 'object based on zod schema',\n                  description:\n                    'Parameters generated by the model to be used by the tool.',\n                },\n              ],\n            },\n          ],\n        },\n      ],\n    },\n    {\n      type: 'CoreToolMessage',\n      parameters: [\n        {\n          name: 'role',\n          type: \"'tool'\",\n          description: 'The role for the assistant message.',\n        },\n        {\n          name: 'content',\n          type: 'Array<ToolResultPart>',\n          description: 'The content of the message.',\n          properties: [\n            {\n              type: 'ToolResultPart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'tool-result'\",\n                  description: 'The type of the message part.',\n                },\n                {\n                  name: 'toolCallId',\n                  type: 'string',\n                  description:\n                    'The id of the tool call the result corresponds to.',\n                },\n                {\n                  name: 'toolName',\n                  type: 'string',\n                  description:\n                    'The name of the tool the result corresponds to.',\n                },\n                {\n                  name: 'result',\n                  type: 'unknown',\n                  description:\n                    'The result returned by the tool after execution.',\n                },\n                {\n                  name: 'isError',\n                  type: 'boolean',\n                  isOptional: true,\n                  description:\n                    'Whether the result is an error or an error message.',\n                },\n              ],\n            },\n          ],\n        },\n      ],\n    },\n  ],\n},\n{\n  name: 'tools',\n  type: 'ToolSet',\n  description:\n    'Tools that are accessible to and can be called by the model. The model needs to support calling tools.',\n  properties: [\n    {\n      type: 'Tool',\n      parameters: [\n        {\n          name: 'description',\n          isOptional: true,\n          type: 'string',\n          description:\n            'Information about the purpose of the tool including details on how and when it can be used by the model.',\n        },\n        {\n          name: 'parameters',\n          type: 'Zod Schema | JSON Schema',\n          description:\n            'The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function).',\n        },\n        {\n          name: 'execute',\n          isOptional: true,\n          type: 'function',\n          description: 'The function to execute when the tool is called.',\n        },\n      ],\n    },\n  ],\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Images with OpenAI's DALL-E 3 Model in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the generateImage function to create multiple images based on a prompt using OpenAI's DALL-E 3 model. It specifies the number of images to generate and their size.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/10-generate-image.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { images } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'A futuristic cityscape at sunset',\n  n: 3,\n  size: '1024x1024',\n});\n\nconsole.log(images);\n```\n\n----------------------------------------\n\nTITLE: Generating Weather JSON with OpenAI Model in TypeScript\nDESCRIPTION: This snippet modifies the previous example to return a JSON object instead of text. This allows for more flexible rendering of the weather information, such as using a React component.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/07-rendering-ui-with-language-models.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nconst text = generateText({\n  model: openai('gpt-3.5-turbo'),\n  system: 'You are a friendly assistant',\n  prompt: 'What is the weather in SF?',\n  tools: {\n    getWeather: {\n      description: 'Get the weather for a location',\n      parameters: z.object({\n        city: z.string().describe('The city to get the weather for'),\n        unit: z\n          .enum(['C', 'F'])\n          .describe('The unit to display the temperature in'),\n      }),\n      execute: async ({ city, unit }) => {\n        const weather = getWeather({ city, unit });\n        const { temperature, unit, description, forecast } = weather;\n\n        return {\n          temperature,\n          unit,\n          description,\n          forecast,\n        };\n      },\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Reading AI State in Server Action with getAIState\nDESCRIPTION: This snippet demonstrates how to access the AI state within a server action using the getAIState function from ai/rsc.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { getAIState } from 'ai/rsc';\n\nexport async function sendMessage(message: string) {\n  'use server';\n\n  const history = getAIState();\n\n  const response = await generateText({\n    model: openai('gpt-3.5-turbo'),\n    messages: [...history, { role: 'user', content: message }],\n  });\n\n  return response;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Customized Voyage AI Provider Instance in TypeScript\nDESCRIPTION: Imports the `createVoyage` factory function and demonstrates how to create a custom Voyage AI provider instance. This allows specifying configuration options like a custom `baseURL` for the API and providing the API key via environment variables (`process.env.VOYAGE_API_KEY`).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/61-voyage-ai.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createVoyage } from 'voyage-ai-provider';\n\nconst voyage = createVoyage({\n  baseURL: 'https://api.voyageai.com/v1',\n  apiKey: process.env.VOYAGE_API_KEY,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Voyage Text Embedding Model Instance in TypeScript\nDESCRIPTION: Demonstrates how to create an instance of a specific Voyage text embedding model (e.g., 'voyage-3') using the `.textEmbeddingModel()` factory method on a Voyage provider instance. This model object can then be used to generate embeddings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/61-voyage-ai.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { voyage } from 'voyage-ai-provider';\n\nconst embeddingModel = voyage.textEmbeddingModel('voyage-3');\n```\n\n----------------------------------------\n\nTITLE: Generating Text using Anthropic Vertex and AI SDK in TypeScript\nDESCRIPTION: Provides a complete example of generating text using the Anthropic Vertex provider. It imports the provider, imports the `generateText` function from the AI SDK core, creates a model instance for a specific Anthropic model via Vertex, and calls `generateText` with the model and a prompt. It depends on the `anthropic-vertex-ai` and `ai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/91-anthropic-vertex-ai.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropicVertex } from 'anthropic-vertex-ai';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: anthropicVertex('claude-3-sonnet@20240229'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Using Gladia Transcription with Provider-Specific Options\nDESCRIPTION: Demonstrates how to use the Gladia transcription API with additional provider-specific options like summarization.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/120-gladia.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { gladia } from '@ai-sdk/gladia';\nimport { readFile } from 'fs/promises';\n\nconst result = await transcribe({\n  model: gladia.transcription(),\n  audio: await readFile('audio.mp3'),\n  providerOptions: { gladia: { summarize: true } },\n});\n```\n\n----------------------------------------\n\nTITLE: Defining HTTP Headers Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `headers` parameter as a record of string key-value pairs (`Record<string, string>`). These additional headers are sent with the request, applicable only for HTTP-based providers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nRecord<string, string>\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Outputs with OpenAI and Zod\nDESCRIPTION: Demonstrates using `generateObject` from the Vercel AI SDK to generate structured JSON output conforming to a Zod schema by enabling OpenAI's structured outputs feature. This ensures the AI's response strictly follows the defined schema for a recipe.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst result = await generateObject({\n  model: openai('gpt-4o-2024-08-06', {\n    structuredOutputs: true,\n  }),\n  schemaName: 'recipe',\n  schemaDescription: 'A recipe for lasagna.',\n  schema: z.object({\n    name: z.string(),\n    ingredients: z.array(\n      z.object({\n        name: z.string(),\n        amount: z.string(),\n      }),\n    ),\n    steps: z.array(z.string()),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n\nconsole.log(JSON.stringify(result.object, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Implementing an API Route with Multiple MCP Tool Connections in Next.js\nDESCRIPTION: This code creates a Next.js API route handler that connects to multiple MCP servers (stdio, SSE, and custom), retrieves tools from each, and uses them with OpenAI's GPT-4o model to generate streaming text responses based on the input prompt.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/73-mcp-tools.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_createMCPClient, streamText } from 'ai';\nimport { Experimental_StdioMCPTransport } from 'ai/mcp-stdio';\nimport { openai } from '@ai-sdk/openai';\n\nexport async function POST(req: Request) {\n  const { prompt }: { prompt: string } = await req.json();\n\n  try {\n    // Initialize an MCP client to connect to a `stdio` MCP server:\n    const transport = new Experimental_StdioMCPTransport({\n      command: 'node',\n      args: ['src/stdio/dist/server.js'],\n    });\n    const stdioClient = await experimental_createMCPClient({\n      transport,\n    });\n\n    // Alternatively, you can connect to a Server-Sent Events (SSE) MCP server:\n    const sseClient = await experimental_createMCPClient({\n      transport: {\n        type: 'sse',\n        url: 'https://actions.zapier.com/mcp/[YOUR_KEY]/sse',\n      },\n    });\n\n    // Similarly to the stdio example, you can pass in your own custom transport as long as it implements the `MCPTransport` interface:\n    const transport = new MyCustomTransport({\n      // ...\n    });\n    const customTransportClient = await experimental_createMCPClient({\n      transport,\n    });\n\n    const toolSetOne = await stdioClient.tools();\n    const toolSetTwo = await sseClient.tools();\n    const toolSetThree = await customTransportClient.tools();\n    const tools = {\n      ...toolSetOne,\n      ...toolSetTwo,\n      ...toolSetThree, // note: this approach causes subsequent tool sets to override tools with the same name\n    };\n\n    const response = await streamText({\n      model: openai('gpt-4o'),\n      tools,\n      prompt,\n      // When streaming, the client should be closed after the response is finished:\n      onFinish: async () => {\n        await stdioClient.close();\n        await sseClient.close();\n        await customTransportClient.close();\n      },\n    });\n\n    return response.toDataStreamResponse();\n  } catch (error) {\n    return new Response('Internal Server Error', { status: 500 });\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using AbortSignal - AI SDK\nDESCRIPTION: This code snippet demonstrates the use of `AbortSignal` to control the image generation process. The `abortSignal` parameter is used to specify a timeout, allowing you to abort the request after a certain duration. This enables control over the image generation process for efficiency or preventing excessive wait times. It requires the `ai` and a model provider like `@ai-sdk/openai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_8\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing Image Data - Base64/Uint8Array\nDESCRIPTION: This snippet shows how to retrieve the image data from the generated image using the properties `base64` or `uint8Array`.  It directly accesses the `base64` property to get the image in base64 format and `uint8Array` to get the image data in Uint8Array format.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst base64 = image.base64; // base64 image data\nconst uint8Array = image.uint8Array; // Uint8Array image data\n```\n\n----------------------------------------\n\nTITLE: Initializing Cloudflare AI Gateway Provider with API Key - TypeScript\nDESCRIPTION: Illustrates how to create an aigateway provider instance using the createAiGateway function and authenticating via an API key. This setup is required when the Cloudflare AI Gateway requires authenticated requests. The options parameter supports customizing request-level behaviors, such as disabling caching. Input parameters include accountId, gateway, apiKey, and optional options; the output is a configured provider instance for subsequent model usage.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/12-cloudflare-ai-gateway.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAiGateway } from 'ai-gateway-provider';\n\nconst aigateway = createAiGateway({\n  accountId: 'your-cloudflare-account-id',\n  gateway: 'your-gateway-name',\n  apiKey: 'your-cloudflare-api-key', // Only required if your gateway has authentication enabled\n  options: {\n    skipCache: true, // Optional request-level settings\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Weather Tool in Route Handler\nDESCRIPTION: Implementation of a route handler that adds a weather tool to fetch temperature data for a given location using OpenAI's GPT-4 model and Zod for parameter validation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Chat and Completion Models with Portkey - TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to create chat and completion model instances with the Portkey provider by calling chatModel('') and completionModel(''). Model name can be an empty string if specified in the portkeyConfig. Models returned can be passed to generateText or streamText for text generation tasks. Dependencies include a configured Portkey provider instance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/10-portkey.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst chatModel = portkey.chatModel('');\nconst completionModel = portkey.completionModel('');\n```\n\n----------------------------------------\n\nTITLE: Implementing Error Handling in Chatbot UI\nDESCRIPTION: This snippet shows how to handle and display errors in the chatbot UI, including a retry button and disabling the input when an error occurs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit, error, reload } =\n    useChat({});\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      {error && (\n        <>\n          <div>An error occurred.</div>\n          <button type=\"button\" onClick={() => reload()}>\n            Retry\n          </button>\n        </>\n      )}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={handleInputChange}\n          disabled={error != null}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Chat UI Component\nDESCRIPTION: React component implementation for displaying chat messages and tool invocations, including handling of different message part types and user input.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n              case 'tool-invocation':\n                return (\n                  <pre key={`${message.id}-${i}`}>\n                    {JSON.stringify(part.toolInvocation, null, 2)}\n                  </pre>\n                );\n            }\n          })}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Text with AI SDK using For-Await Loop\nDESCRIPTION: This snippet demonstrates how to stream text from an OpenAI model using the AI SDK with a for-await loop. It configures parameters including the model, maximum tokens, temperature, and retry settings, then iterates through the text stream to display content as it's generated.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/20-stream-text.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst result = streamText({\n  model: openai('gpt-3.5-turbo'),\n  maxTokens: 512,\n  temperature: 0.3,\n  maxRetries: 5,\n  prompt: 'Invent a new holiday and describe its traditions.'\n});\n\nfor await (const textPart of result.textStream) {\n  console.log(textPart);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Error Handling with useChat Hook in React\nDESCRIPTION: Demonstrates basic error handling implementation using the useChat hook, including error display, retry functionality, and input field disabling when an error occurs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/21-error-handling.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit, error, reload } =\n    useChat({});\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      {error && (\n        <>\n          <div>An error occurred.</div>\n          <button type=\"button\" onClick={() => reload()}>\n            Retry\n          </button>\n        </>\n      )}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={handleInputChange}\n          disabled={error != null}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Text Inputs with OpenAI and AI SDK in TypeScript\nDESCRIPTION: This code demonstrates how to batch process text embeddings using the AI SDK and OpenAI. It imports the necessary dependencies, creates embeddings for multiple text inputs in a single API call, and logs both the resulting embeddings and usage information. This approach is more efficient than embedding texts individually when working with large datasets.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/61-embed-text-batch.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { embedMany } from 'ai';\nimport 'dotenv/config';\n\nasync function main() {\n  const { embeddings, usage } = await embedMany({\n    model: openai.embedding('text-embedding-3-small'),\n    values: [\n      'sunny day at the beach',\n      'rainy afternoon in the city',\n      'snowy night in the mountains',\n    ],\n  });\n\n  console.log(embeddings);\n  console.log(usage);\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Implementing Abort Signals and Timeouts\nDESCRIPTION: Demonstrates how to use AbortSignal to set timeouts or manually abort transcription processes.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/36-transcription.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { readFile } from 'fs/promises';\n\nconst transcript = await transcribe({\n  model: openai.transcription('whisper-1'),\n  audio: await readFile('audio.mp3'),\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\n});\n```\n\n----------------------------------------\n\nTITLE: Rendering Streamable Message Component in React\nDESCRIPTION: This client-side component renders a message using a streamable value. It utilizes the useStreamableValue hook from ai/rsc to handle the streaming of text content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/121-stream-assistant-response-with-tools.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { StreamableValue, useStreamableValue } from 'ai/rsc';\n\nexport function Message({ textStream }: { textStream: StreamableValue }) {\n  const [text] = useStreamableValue(textStream);\n\n  return <div>{text}</div>;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Audio Transcription with ElevenLabs\nDESCRIPTION: Example showing how to use the ElevenLabs provider for audio transcription, including model specification and audio file processing.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/elevenlabs/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { elevenlabs } from '@ai-sdk/elevenlabs';\nimport { experimental_transcribe as transcribe } from 'ai';\n\nconst { text } = await transcribe({\n  model: elevenlabs.transcription('scribe_v1'),\n  audio: new URL(\n    'https://github.com/vercel/ai/raw/refs/heads/main/examples/ai-core/data/galileo.mp3',\n  ),\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Provider with OpenAI Models\nDESCRIPTION: Example showing how to create a custom provider with specialized model settings, including structured outputs configuration and model aliasing. Demonstrates integration with OpenAI provider and fallback configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/42-custom-provider.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { customProvider } from 'ai';\n\n// custom provider with different model settings:\nexport const myOpenAI = customProvider({\n  languageModels: {\n    // replacement model with custom settings:\n    'gpt-4': openai('gpt-4', { structuredOutputs: true }),\n    // alias model with custom settings:\n    'gpt-4o-structured': openai('gpt-4o', { structuredOutputs: true }),\n  },\n  fallbackProvider: openai,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI Image Generation Model in TypeScript\nDESCRIPTION: This snippet initializes an OpenAI image model for generative tasks using the .image() factory method. The example specifies the model id 'dall-e-3', which is required for image synthesis via the OpenAI API. No further configuration is provided, but the model instance can accept provider-specific options depending on the selected model's capabilities.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_28\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = openai.image('dall-e-3');\n```\n\n----------------------------------------\n\nTITLE: Implementing Route Handler for Chat API with OpenAI\nDESCRIPTION: Implementation of a Next.js route handler for the chat API endpoint that uses the OpenAI provider from AI SDK. It streams text responses from the GPT-4o model with a 30-second timeout.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/02-multi-modal-chatbot.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, Message } from 'ai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: Message[] } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Route Handler with DeepInfra and Next.js\nDESCRIPTION: Server-side implementation of a chat endpoint using DeepInfra's Llama 3.1 model. Handles streaming text responses with a 30-second duration limit.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\nimport { streamText } from 'ai';\nimport { deepinfra } from '@ai-sdk/deepinfra';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),\n    system: 'You are a helpful assistant.',\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Azure OpenAI Responses Model\nDESCRIPTION: Demonstrates how to create a model instance specifically for the Azure OpenAI responses API using the `azure.responses()` factory method, passing the deployment name.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = azure.responses('your-deployment-name');\n```\n\n----------------------------------------\n\nTITLE: Implementing Server Actions with Token Usage Recording\nDESCRIPTION: This server action file defines the core functionality for processing AI requests. It highlights the onFinish callback in streamUI which captures token usage statistics after streaming completes. The file also includes a tool for simulating repository deployment with step-by-step feedback.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/92-stream-ui-record-token-usage.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { createAI, getMutableAIState, streamUI } from 'ai/rsc';\nimport { openai } from '@ai-sdk/openai';\nimport { ReactNode } from 'react';\nimport { z } from 'zod';\nimport { generateId } from 'ai';\n\nexport interface ServerMessage {\n  role: 'user' | 'assistant';\n  content: string;\n}\n\nexport interface ClientMessage {\n  id: string;\n  role: 'user' | 'assistant';\n  display: ReactNode;\n}\n\nexport async function continueConversation(\n  input: string,\n): Promise<ClientMessage> {\n  'use server';\n\n  const history = getMutableAIState();\n\n  const result = await streamUI({\n    model: openai('gpt-3.5-turbo'),\n    messages: [...history.get(), { role: 'user', content: input }],\n    text: ({ content, done }) => {\n      if (done) {\n        history.done((messages: ServerMessage[]) => [\n          ...messages,\n          { role: 'assistant', content },\n        ]);\n      }\n\n      return <div>{content}</div>;\n    },\n    tools: {\n      deploy: {\n        description: 'Deploy repository to vercel',\n        parameters: z.object({\n          repositoryName: z\n            .string()\n            .describe('The name of the repository, example: vercel/ai-chatbot'),\n        }),\n        generate: async function* ({ repositoryName }) {\n          yield <div>Cloning repository {repositoryName}...</div>; // [!code highlight:5]\n          await new Promise(resolve => setTimeout(resolve, 3000));\n          yield <div>Building repository {repositoryName}...</div>;\n          await new Promise(resolve => setTimeout(resolve, 2000));\n          return <div>{repositoryName} deployed!</div>;\n        },\n      },\n    },\n    onFinish: ({ usage }) => {\n      const { promptTokens, completionTokens, totalTokens } = usage;\n      // your own logic, e.g. for saving the chat history or recording usage\n      console.log('Prompt tokens:', promptTokens);\n      console.log('Completion tokens:', completionTokens);\n      console.log('Total tokens:', totalTokens);\n    },\n  });\n\n  return {\n    id: generateId(),\n    role: 'assistant',\n    display: result.value,\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: React Chat UI Implementation with AI SDK\nDESCRIPTION: Creates a React-based chat interface using the AI SDK's useChat hook for handling chat interactions and message streaming.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_10\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      <div className=\"space-y-4\">\n        {messages.map(m => (\n          <div key={m.id} className=\"whitespace-pre-wrap\">\n            <div>\n              <div className=\"font-bold\">{m.role}</div>\n              <p>{m.content}</p>\n            </div>\n          </div>\n        ))}\n      </div>\n\n      <form onSubmit={handleSubmit}>\n        <input\n          className=\"fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Using createDataStreamResponse Example\nDESCRIPTION: Demonstrates creating a streaming response with custom headers, data writing, annotations, and stream merging. Shows error handling and various stream writing methods.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/41-create-data-stream-response.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nconst response = createDataStreamResponse({\n  status: 200,\n  statusText: 'OK',\n  headers: {\n    'Custom-Header': 'value',\n  },\n  async execute(dataStream) {\n    // Write data\n    dataStream.writeData({ value: 'Hello' });\n\n    // Write annotation\n    dataStream.writeMessageAnnotation({ type: 'status', value: 'processing' });\n\n    // Merge another stream\n    const otherStream = getAnotherStream();\n    dataStream.merge(otherStream);\n  },\n  onError: error => `Custom error: ${error.message}`,\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Step (Agentic) Generations\nDESCRIPTION: Example showing how to set maxSteps to enable the model to perform multiple actions automatically without user intervention.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/05-computer-use.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst stream = streamText({\n  model: anthropic('claude-3-5-sonnet-20241022'),\n  prompt: 'Open the browser and navigate to vercel.com',\n  tools: { computer: computerTool },\n  maxSteps: 10, // experiment with this value based on your use case\n});\n```\n\n----------------------------------------\n\nTITLE: Using Tools with OpenAI GPT-4.5 and AI SDK\nDESCRIPTION: This example demonstrates how to use tool calling with the AI SDK and OpenAI's GPT-4.5 model. It defines a getWeather tool that simulates fetching weather data, allowing the model to provide more accurate and context-aware responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/22-gpt-4-5.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText, tool } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { text } = await generateText({\n  model: openai('gpt-4.5-preview'),\n  prompt: 'What is the weather like today in San Francisco?',\n  tools: {\n    getWeather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Chat Interface with Streaming in React\nDESCRIPTION: A React client component that manages a conversation state, handles user input, and streams model responses in real-time. It uses readStreamableValue to process the streamed response and updates the UI as new content arrives.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/21-stream-text-with-chat-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { Message, continueConversation } from './actions';\nimport { readStreamableValue } from 'ai/rsc';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [conversation, setConversation] = useState<Message[]>([]);\n  const [input, setInput] = useState<string>('');\n\n  return (\n    <div>\n      <div>\n        {conversation.map((message, index) => (\n          <div key={index}>\n            {message.role}: {message.content}\n          </div>\n        ))}\n      </div>\n\n      <div>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button\n          onClick={async () => {\n            const { messages, newMessage } = await continueConversation([\n              ...conversation,\n              { role: 'user', content: input },\n            ]);\n\n            let textContent = '';\n\n            for await (const delta of readStreamableValue(newMessage)) {\n              textContent = `${textContent}${delta}`;\n\n              setConversation([\n                ...messages,\n                { role: 'assistant', content: textContent },\n              ]);\n            }\n          }}\n        >\n          Send Message\n        </button>\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Chat UI Component with React\nDESCRIPTION: A React component that implements the chat interface using the useChat hook from the AI SDK. It displays messages and provides an input form for user interactions, handling state management and form submission.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/02-multi-modal-chatbot.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(m => (\n        <div key={m.id} className=\"whitespace-pre-wrap\">\n          {m.role === 'user' ? 'User: ' : 'AI: '}\n          {m.content}\n        </div>\n      ))}\n\n      <form\n        onSubmit={handleSubmit}\n        className=\"fixed bottom-0 w-full max-w-md mb-8 border border-gray-300 rounded shadow-xl\"\n      >\n        <input\n          className=\"w-full p-2\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-side Streaming with streamObject in TypeScript\nDESCRIPTION: This server-side route handler uses the streamObject function to generate and stream notifications based on a given context. It uses OpenAI's GPT-4 model to generate the content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/08-object-generation.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { notificationSchema } from './schema';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const context = await req.json();\n\n  const result = streamObject({\n    model: openai('gpt-4-turbo'),\n    schema: notificationSchema,\n    prompt:\n      `Generate 3 notifications for a messages app in this context:` + context,\n  });\n\n  return result.toTextStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Detailed Error Logging for Anthropic API Call in TypeScript\nDESCRIPTION: This code snippet demonstrates comprehensive error logging for a failed Anthropic API call. It includes the error stack trace, request details, response headers, and other relevant information for debugging purposes.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/ai-core/data/error-message.txt#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nAPICallError [AI_APICallError]: Failed to process error response\n    at postToApi (/Users/larsgrammel/repositories/ai/packages/provider-utils/dist/index.js:382:15)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    ... 4 lines matching cause stack trace ...\n    at async fn (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:2723:36)\n    at async /Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:339:22\n    at async main (/Users/larsgrammel/repositories/ai/examples/ai-core/src/generate-text/anthropic-cache-control.ts:54:361) {\n  cause: TypeError: Body is unusable\n      at consumeBody (node:internal/deps/undici/undici:4281:15)\n      at _Response.text (node:internal/deps/undici/undici:4236:18)\n      at /Users/larsgrammel/repositories/ai/packages/provider-utils/dist/index.js:443:39\n      at postToApi (/Users/larsgrammel/repositories/ai/packages/provider-utils/dist/index.js:373:34)\n      at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n      at async AnthropicMessagesLanguageModel.doGenerate (/Users/larsgrammel/repositories/ai/packages/anthropic/dist/index.js:316:50)\n      at async fn (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:2748:34)\n      at async /Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:339:22\n      at async _retryWithExponentialBackoff (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:170:12)\n      at async fn (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:2723:36),\n  url: 'https://api.anthropic.com/v1/messages',\n  requestBodyValues: {\n    model: 'claude-3-5-sonnet-20240620',\n    top_k: undefined,\n    max_tokens: 4096,\n    temperature: 0,\n    top_p: undefined,\n    stop_sequences: undefined,\n    system: undefined,\n    messages: [ [Object] ],\n    tools: undefined,\n    tool_choice: undefined\n  },\n  statusCode: 400,\n  responseHeaders: {\n    'cf-cache-status': 'DYNAMIC',\n    'cf-ray': '8b39b87a8f684541-TXL',\n    connection: 'keep-alive',\n    'content-length': '173',\n    'content-type': 'application/json',\n    date: 'Thu, 15 Aug 2024 14:02:08 GMT',\n    'request-id': 'req_01YZqjpifTdvLZqfwBieLs44',\n    server: 'cloudflare',\n    via: '1.1 google',\n    'x-cloud-trace-context': '00f2b1629d0dc8c6a4714db1dbdb4c2c',\n    'x-robots-tag': 'none',\n    'x-should-retry': 'false'\n  },\n  responseBody: undefined,\n  isRetryable: false,\n  data: undefined,\n  [Symbol(vercel.ai.error)]: true,\n  [Symbol(vercel.ai.error.AI_APICallError)]: true\n}\n```\n\n----------------------------------------\n\nTITLE: Client-side Chat Component with Image URL Submission in Next.js\nDESCRIPTION: This client-side component renders a chat interface and handles submitting user messages along with an image URL. It uses the useChat hook from the AI SDK to manage chat state and interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/22-stream-text-with-image-prompt.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role === 'user' ? 'User: ' : 'AI: '}\n          {m.content}\n        </div>\n      ))}\n\n      <form\n        onSubmit={e => {\n          handleSubmit(e, {\n            data: { imageUrl: 'https://somewhere.com/image.png' },\n          });\n        }}\n      >\n        <input\n          value={input}\n          placeholder=\"What does the image show...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Interface with Streaming Responses in React\nDESCRIPTION: This client-side component creates a chat interface that allows users to send messages and receive streamed responses from an AI assistant. It manages message state and handles user input submission.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/121-stream-assistant-response-with-tools.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { ClientMessage, submitMessage } from './actions';\nimport { useActions } from 'ai/rsc';\n\nexport default function Home() {\n  const [input, setInput] = useState('');\n  const [messages, setMessages] = useState<ClientMessage[]>([]);\n  const { submitMessage } = useActions();\n\n  const handleSubmission = async () => {\n    setMessages(currentMessages => [\n      ...currentMessages,\n      {\n        id: '123',\n        status: 'user.message.created',\n        text: input,\n        gui: null,\n      },\n    ]);\n\n    const response = await submitMessage(input);\n    setMessages(currentMessages => [...currentMessages, response]);\n    setInput('');\n  };\n\n  return (\n    <div className=\"flex flex-col-reverse\">\n      <div className=\"flex flex-row gap-2 p-2 bg-zinc-100 w-full\">\n        <input\n          className=\"bg-zinc-100 w-full p-2 outline-none\"\n          value={input}\n          onChange={event => setInput(event.target.value)}\n          placeholder=\"Ask a question\"\n          onKeyDown={event => {\n            if (event.key === 'Enter') {\n              handleSubmission();\n            }\n          }}\n        />\n        <button\n          className=\"p-2 bg-zinc-900 text-zinc-100 rounded-md\"\n          onClick={handleSubmission}\n        >\n          Send\n        </button>\n      </div>\n\n      <div className=\"flex flex-col h-[calc(100dvh-56px)] overflow-y-scroll\">\n        <div>\n          {messages.map(message => (\n            <div key={message.id} className=\"flex flex-col gap-1 border-b p-2\">\n              <div className=\"flex flex-row justify-between\">\n                <div className=\"text-sm text-zinc-500\">{message.status}</div>\n              </div>\n              <div className=\"flex flex-col gap-2\">{message.gui}</div>\n              <div>{message.text}</div>\n            </div>\n          ))}\n        </div>\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Next.js Server Action Implementation\nDESCRIPTION: Complete server action implementation for streaming React components with weather functionality in Next.js.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/02-streaming-react-components.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { streamUI } from 'ai/rsc';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nconst LoadingComponent = () => (\n  <div className=\"animate-pulse p-4\">getting weather...</div>\n);\n\nconst getWeather = async (location: string) => {\n  await new Promise(resolve => setTimeout(resolve, 2000));\n  return '82Â°Fï¸ âï¸';\n};\n\ninterface WeatherProps {\n  location: string;\n  weather: string;\n}\n\nconst WeatherComponent = (props: WeatherProps) => (\n  <div className=\"border border-neutral-200 p-4 rounded-lg max-w-fit\">\n    The weather in {props.location} is {props.weather}\n  </div>\n);\n\nexport async function streamComponent() {\n  const result = await streamUI({\n    model: openai('gpt-4o'),\n    prompt: 'Get the weather for San Francisco',\n    text: ({ content }) => <div>{content}</div>,\n    tools: {\n      getWeather: {\n        description: 'Get the weather for a location',\n        parameters: z.object({\n          location: z.string(),\n        }),\n        generate: async function* ({ location }) {\n          yield <LoadingComponent />;\n          const weather = await getWeather(location);\n          return <WeatherComponent weather={weather} location={location} />;\n        },\n      },\n    },\n  });\n\n  return result.value;\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Text with OpenAI's GPT-3.5 using AI SDK in TypeScript\nDESCRIPTION: This code demonstrates how to generate text responses using the AI SDK with OpenAI's GPT-3.5 Turbo model. It imports the necessary functions, initializes the OpenAI model, sends a simple prompt asking why the sky is blue, and logs the result.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/10-generate-text.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst result = await generateText({\n  model: openai('gpt-3.5-turbo'),\n  prompt: 'Why is the sky blue?',\n});\n\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Creating Custom DeepSeek Provider in TypeScript\nDESCRIPTION: Creating a customized DeepSeek provider instance with specific configuration options, including API key setup.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/30-deepseek.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createDeepSeek } from '@ai-sdk/deepseek';\n\nconst deepseek = createDeepSeek({\n  apiKey: process.env.DEEPSEEK_API_KEY ?? '',\n});\n```\n\n----------------------------------------\n\nTITLE: Using pipeDataStreamToResponse Function - TypeScript/TSX\nDESCRIPTION: Demonstrates how to use pipeDataStreamToResponse to handle streaming data with custom headers, data writing, annotations, and stream merging. Includes error handling and status code configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/42-pipe-data-stream-to-response.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\npipeDataStreamToResponse(serverResponse, {\n  status: 200,\n  statusText: 'OK',\n  headers: {\n    'Custom-Header': 'value',\n  },\n  async execute(dataStream) {\n    // Write data\n    dataStream.writeData({ value: 'Hello' });\n\n    // Write annotation\n    dataStream.writeMessageAnnotation({ type: 'status', value: 'processing' });\n\n    // Merge another stream\n    const otherStream = getAnotherStream();\n    dataStream.merge(otherStream);\n  },\n  onError: error => `Custom error: ${error.message}`,\n});\n```\n\n----------------------------------------\n\nTITLE: Sending Custom Data with AI SDK in Nest.js\nDESCRIPTION: This controller shows how to use pipeDataStreamToResponse to send custom data to the client in addition to AI-generated content. It writes initial data before streaming the AI response and includes error handling that can optionally expose error messages to the client.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/50-nest.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Controller, Post, Res } from '@nestjs/common';\nimport { openai } from '@ai-sdk/openai';\nimport { pipeDataStreamToResponse, streamText } from 'ai';\nimport { Response } from 'express';\n\n@Controller()\nexport class AppController {\n  @Post('/stream-data')\n  async streamData(@Res() res: Response) {\n    pipeDataStreamToResponse(res, {\n      execute: async dataStreamWriter => {\n        dataStreamWriter.writeData('initialized call');\n\n        const result = streamText({\n          model: openai('gpt-4o'),\n          prompt: 'Invent a new holiday and describe its traditions.',\n        });\n\n        result.mergeIntoDataStream(dataStreamWriter);\n      },\n      onError: error => {\n        // Error messages are masked by default for security reasons.\n        // If you want to expose the error message to the client, you can do so here:\n        return error instanceof Error ? error.message : String(error);\n      },\n    });\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Telemetry with AI SDK for Traceloop Integration\nDESCRIPTION: Demonstrates how to use the experimental_telemetry option in the AI SDK to enable telemetry for Traceloop. It includes setting custom metadata for the telemetry data.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/traceloop.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'What is 2 + 2?',\n  experimental_telemetry: {\n    isEnabled: true,\n    metadata: {\n      query: 'weather',\n      location: 'San Francisco',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Sending Custom Sources with AI SDK in TypeScript\nDESCRIPTION: This snippet shows how to send custom sources to the client using the writeSource method on the DataStreamWriter. It demonstrates sending a URL source as an example.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/20-streaming-data.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { createDataStreamResponse, streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  return createDataStreamResponse({\n    execute: dataStream => {\n      // write a custom url source to the stream:\n      dataStream.writeSource({\n        sourceType: 'url',\n        id: 'source-1',\n        url: 'https://example.com',\n        title: 'Example Source',\n      });\n\n      const result = streamText({\n        model: openai('gpt-4o'),\n        messages,\n      });\n\n      result.mergeIntoDataStream(dataStream);\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LangWatch Exporter for Node.js\nDESCRIPTION: TypeScript code snippet for setting up LangWatch exporter in a Node.js OpenTelemetry SDK configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langwatch.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LangWatchExporter } from 'langwatch';\n\nconst sdk = new NodeSDK({\n  traceExporter: new LangWatchExporter({\n    apiKey: process.env.LANGWATCH_API_KEY,\n  }),\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Experimental Tool Call Repair Function Type in TypeScript\nDESCRIPTION: Specifies the type for the optional experimental function `experimental_repairToolCall`. This async function (`(options: ToolCallRepairOptions) => Promise<LanguageModelV1FunctionToolCall | null>`) attempts to repair a tool call that failed parsing, receiving details in `ToolCallRepairOptions`. It returns the repaired tool call or null.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_25\n\nLANGUAGE: typescript\nCODE:\n```\n(options: ToolCallRepairOptions) => Promise<LanguageModelV1FunctionToolCall | null>\n```\n\n----------------------------------------\n\nTITLE: Model Configuration Interface Definition\nDESCRIPTION: Defines the configuration interface for the language model including settings for tool choice, token limits, temperature, penalties, and various experimental features.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  name: 'toolChoice',\n  isOptional: true,\n  type: '\"auto\" | \"none\" | \"required\" | { \"type\": \"tool\", \"toolName\": string }',\n  description: 'The tool choice setting. It specifies how tools are selected for execution.'\n},\n{\n  name: 'maxTokens',\n  type: 'number',\n  isOptional: true,\n  description: 'Maximum number of tokens to generate.'\n}\n```\n\n----------------------------------------\n\nTITLE: Structured Data Generation with OpenAI o1\nDESCRIPTION: Shows how to generate structured JSON data using zod schema validation with the AI SDK and OpenAI o1 model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/23-o1.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: openai('o1'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Server Action for Streaming AI Responses with OpenAI\nDESCRIPTION: A server-side function that processes the conversation history, creates a streamable value, and uses the AI SDK to stream text responses from OpenAI. It returns both the conversation history and the new streamed message to the client.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/21-stream-text-with-chat-prompt.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n'use server';\n\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { createStreamableValue } from 'ai/rsc';\n\nexport interface Message {\n  role: 'user' | 'assistant';\n  content: string;\n}\n\nexport async function continueConversation(history: Message[]) {\n  'use server';\n\n  const stream = createStreamableValue();\n\n  (async () => {\n    const { textStream } = streamText({\n      model: openai('gpt-3.5-turbo'),\n      system:\n        \"You are a dude that doesn't drop character until the DVD commentary.\",\n      messages: history,\n    });\n\n    for await (const text of textStream) {\n      stream.update(text);\n    }\n\n    stream.done();\n  })();\n\n  return {\n    messages: history,\n    newMessage: stream.value,\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Using experimental_createMCPClient with OpenAI in TypeScript\nDESCRIPTION: Demonstrates how to create an MCP client, get tools, and use them with the generateText function from the AI SDK and OpenAI. It also shows proper error handling and resource cleanup.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/21-create-mcp-client.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_createMCPClient, generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\ntry {\n  const client = await experimental_createMCPClient({\n    transport: {\n      type: 'stdio',\n      command: 'node server.js',\n    },\n  });\n\n  const tools = await client.tools();\n\n  const response = await generateText({\n    model: openai('gpt-4o-mini'),\n    tools,\n    messages: [{ role: 'user', content: 'Query the data' }],\n  });\n\n  console.log(response);\n} finally {\n  await client.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Response with Llama 3.1\nDESCRIPTION: This code demonstrates how to stream the model's response as it's being generated using the streamText function from the AI SDK with DeepInfra's Llama 3.1 405B model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { streamText } from 'ai';\nimport { deepinfra } from '@ai-sdk/deepinfra';\n\nconst { textStream } = streamText({\n  model: deepinfra('meta-llama/Meta-Llama-3.1-405B-Instruct'),\n  prompt: 'What is love?',\n});\n```\n\n----------------------------------------\n\nTITLE: Safely Closing an MCP Client Using try/finally in AI SDK with TypeScript\nDESCRIPTION: This code provides a pattern for safely closing (disposing of) an MCP client regardless of whether client creation or communication completes successfully, using a try/finally block. This approach prevents resource leaks in sync or async usage. Dependencies are the AI SDK and appropriate error handling context. Inputs and outputs relate to MCP client lifecycle control.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_21\n\nLANGUAGE: typescript\nCODE:\n```\nlet mcpClient: MCPClient | undefined;\n\ntry {\n  mcpClient = await experimental_createMCPClient({\n    // ...\n  });\n} finally {\n  await mcpClient?.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming AI Responses with Memory Context - TypeScript\nDESCRIPTION: Demonstrates how to use Mem0 with the 'streamText' function to stream AI-generated responses. Requires the 'ai' and '@mem0/vercel-ai-provider' packages. Input parameters: model (from createMem0), user context, and a prompt. Output: a text stream that is iterated and written directly, enabling real-time output. Suitable for applications where prompt responses are required.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/70-mem0.mdx#2025-04-23_snippet_6\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { createMem0 } from '@mem0/vercel-ai-provider';\n\nconst mem0 = createMem0();\n\nconst { textStream } = await streamText({\n  model: mem0('gpt-4-turbo', {\n    user_id: 'borat',\n  }),\n  prompt:\n    'Suggest me a good car to buy! Why is it better than the other cars for me? Give options for every price range.',\n});\n\nfor await (const textPart of textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Rate Limiting with Helicone in JavaScript\nDESCRIPTION: This code demonstrates how to implement rate limiting in your AI SDK requests using Helicone. It sets a rate limit policy and optionally limits by user, allowing control over usage.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/helicone.mdx#2025-04-23_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst response = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'Generate creative content',\n  headers: {\n    // Allow 10,000 requests per hour\n    'Helicone-RateLimit-Policy': '10000;w=3600',\n\n    // Optional: limit by user\n    'Helicone-User-Id': 'user@example.com',\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-Side Logic for Parallel Tool Calls with AI SDK\nDESCRIPTION: This server-side code snippet demonstrates how to set up parallel tool calls using the AI SDK. It defines a 'getWeather' function and integrates it into the text generation process, allowing the AI to fetch weather information for multiple cities concurrently.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/51-call-tools-in-parallel.mdx#2025-04-23_snippet_1\n\nLANGUAGE: ts\nCODE:\n```\n'use server';\n\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nexport interface Message {\n  role: 'user' | 'assistant';\n  content: string;\n}\n\nfunction getWeather({ city, unit }) {\n  // This function would normally make an\n  // API request to get the weather.\n\n  return { value: 25, description: 'Sunny' };\n}\n\nexport async function continueConversation(history: Message[]) {\n  'use server';\n\n  const { text, toolResults } = await generateText({\n    model: openai('gpt-3.5-turbo'),\n    system: 'You are a friendly weather assistant!',\n    messages: history,\n    tools: {\n      getWeather: {\n        description: 'Get the weather for a location',\n        parameters: z.object({\n          city: z.string().describe('The city to get the weather for'),\n          unit: z\n            .enum(['C', 'F'])\n            .describe('The unit to display the temperature in'),\n        }),\n        execute: async ({ city, unit }) => {\n          const weather = getWeather({ city, unit });\n          return `It is currently ${weather.value}Â°${unit} and ${weather.description} in ${city}!`;\n        },\n      },\n    },\n  });\n\n  return {\n    messages: [\n      ...history,\n      {\n        role: 'assistant' as const,\n        content:\n          text || toolResults.map(toolResult => toolResult.result).join('\\n'),\n      },\n    ],\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a customized Rev.ai provider instance\nDESCRIPTION: Demonstrates how to create a customized Rev.ai provider instance with specific settings such as a custom fetch implementation. This is useful when you need more control over the provider's behavior.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/160-revai.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createRevai } from '@ai-sdk/revai';\n\nconst revai = createRevai({\n  // custom settings, e.g.\n  fetch: customFetch,\n});\n```\n\n----------------------------------------\n\nTITLE: Registering LangfuseExporter in Next.js Instrumentation (TypeScript)\nDESCRIPTION: This snippet shows how to register the LangfuseExporter in a Next.js application using the OpenTelemetry instrumentation. It sets up the exporter for tracing AI SDK operations.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langfuse.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { registerOTel } from '@vercel/otel';\nimport { LangfuseExporter } from 'langfuse-vercel';\n\nexport function register() {\n  registerOTel({\n    serviceName: 'langfuse-vercel-ai-nextjs-example',\n    traceExporter: new LangfuseExporter(),\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a React Client Component for Text Generation\nDESCRIPTION: A React client component that allows users to trigger text generation with a button click. When clicked, it calls the server action getAnswer and displays the generated text. The component includes a maxDuration setting to allow streaming responses up to 30 seconds.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/10-generate-text.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { getAnswer } from './actions';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [generation, setGeneration] = useState<string>('');\n\n  return (\n    <div>\n      <button\n        onClick={async () => {\n          const { text } = await getAnswer('Why is the sky blue?');\n          setGeneration(text);\n        }}\n      >\n        Answer\n      </button>\n      <div>{generation}</div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Stream Client Component in Next.js\nDESCRIPTION: Client-side React component that implements the data stream protocol using the useCompletion hook from the AI SDK. Sets up a form with input handling and completion display.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useCompletion } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { completion, input, handleInputChange, handleSubmit } = useCompletion({\n    streamProtocol: 'data', // optional, this is the default\n  });\n\n  return (\n    <form onSubmit={handleSubmit}>\n      <input name=\"prompt\" value={input} onChange={handleInputChange} />\n      <button type=\"submit\">Submit</button>\n      <div>{completion}</div>\n    </form>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Defining the ToolExecutionOptions Type in TypeScript\nDESCRIPTION: Defines the structure for options passed to a tool's execution function. It includes `toolCallId` (string for tracking), `messages` (CoreMessage[] leading to the tool call), and an optional `abortSignal` (AbortSignal for cancellation).\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nToolExecutionOptions\n```\n\n----------------------------------------\n\nTITLE: Initializing an MCP Client with Stdio Transport in AI SDK using TypeScript\nDESCRIPTION: This snippet shows how to set up an MCP client utilizing the standard input/output transport, intended for running local tool servers such as CLI tools. It requires both the AI SDK and the 'ai/mcp-stdio' package. The sample configuration provides a Node.js command with script arguments. Input parameters include the transport instance and process arguments; output is an initialized 'mcpClient' for local server communication.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_createMCPClient as createMCPClient } from 'ai';\nimport { Experimental_StdioMCPTransport as StdioMCPTransport } from 'ai/mcp-stdio';\n\nconst mcpClient = await createMCPClient({\n  transport: new StdioMCPTransport({\n    command: 'node',\n    args: ['src/stdio/dist/server.js'],\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Fireworks Models\nDESCRIPTION: How to use Fireworks image models to generate images with specific prompts and aspect ratios.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/26-fireworks.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fireworks } from '@ai-sdk/fireworks';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: fireworks.image('accounts/fireworks/models/flux-1-dev-fp8'),\n  prompt: 'A futuristic cityscape at sunset',\n  aspectRatio: '16:9',\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Random Seed Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `seed` parameter as an integer (`number`). If set and supported by the model, this ensures deterministic results for random sampling.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nnumber\n```\n\n----------------------------------------\n\nTITLE: Setting Cache Control for Anthropic Prompt Caching (TypeScript)\nDESCRIPTION: This snippet illustrates the use of the providerOptions.anthropic.cacheControl property within message parts in a generateText request to enable ephemeral cache breakpoints for Anthropic models. The example demonstrates customizing provider metadata and retrieving cache creation input token counts from the result. Dependencies include '@ai-sdk/google-vertex/anthropic' and 'ai'. Key parameters are model, messages array, and error message variable. Returns generated text and provider metadata.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_27\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic';\nimport { generateText } from 'ai';\n\nconst errorMessage = '... long error message ...';\n\nconst result = await generateText({\n  model: vertexAnthropic('claude-3-5-sonnet-20240620'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'You are a JavaScript expert.' },\n        {\n          type: 'text',\n          text: `Error message: ${errorMessage}`,\n          providerOptions: {\n            anthropic: { cacheControl: { type: 'ephemeral' } },\n          },\n        },\n        { type: 'text', text: 'Explain the error message.' },\n      ],\n    },\n  ],\n});\n\nconsole.log(result.text);\nconsole.log(result.providerMetadata?.anthropic);\n// e.g. { cacheCreationInputTokens: 2118, cacheReadInputTokens: 0 }\n```\n\n----------------------------------------\n\nTITLE: Error Handling - AI SDK\nDESCRIPTION: This code snippet shows how to handle errors when the `generateImage` function fails to generate a valid image. It catches the `AI_NoImageGeneratedError` and logs the cause of the error and related metadata. This allows developers to handle potential image generation failures. Requires the `ai` and a model provider like `@ai-sdk/openai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_11\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { generateImage, NoImageGeneratedError } from 'ai';\n\ntry {\n  await generateImage({ model, prompt });\n} catch (error) {\n  if (NoImageGeneratedError.isInstance(error)) {\n    console.log('NoImageGeneratedError');\n    console.log('Cause:', error.cause);\n    console.log('Responses:', error.responses);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Navigating to Project Directory in Terminal\nDESCRIPTION: Command to change directory to the newly created SvelteKit project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd my-ai-app\n```\n\n----------------------------------------\n\nTITLE: Defining AI Function Parameters and Return Types in TypeScript\nDESCRIPTION: Type definitions for an AI function's configuration parameters and return values. Includes settings for model control (temperature, tokens, etc.), telemetry options, and detailed response metadata structures.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/03-generate-object.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  name: 'maxTokens',\n  type: 'number',\n  isOptional: true,\n  description: 'Maximum number of tokens to generate.',\n},\n{\n  name: 'temperature',\n  type: 'number',\n  isOptional: true,\n  description: 'Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.',\n},\n{\n  name: 'topP',\n  type: 'number',\n  isOptional: true,\n  description: 'Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.',\n}\n```\n\n----------------------------------------\n\nTITLE: Init Tool Call Streaming Start Event - TypeScript\nDESCRIPTION: This snippet details the parameter schema for signaling the initiation of a streaming tool call event, including type, tool call ID, and the tool's name. It is used to demarcate the start of streaming operations for tool integrations within live AI sessions. Inputs are metadata strings; output is propagated event definitions for further processing.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_19\n\nLANGUAGE: TypeScript\nCODE:\n```\nparameters: [\n  {\n    name: 'type',\n    type: '\\'tool-call-streaming-start\\'',\n    description: 'Indicates the start of a tool call streaming. Only available when streaming tool calls.',\n  },\n  {\n    name: 'toolCallId',\n    type: 'string',\n    description: 'The id of the tool call.',\n  },\n  {\n    name: 'toolName',\n    type: 'string',\n    description: 'The name of the tool, which typically would be the name of the function.',\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Enabling Dynamic Retrieval for Grounding in Gemini (TypeScript)\nDESCRIPTION: Illustrates how to configure 'generateText' with dynamic retrieval, allowing the model to decide when to use Google Search based on need. Requires the Gemini 1.5 Flash model. Parameters include 'dynamicRetrievalConfig' with mode and a threshold. Returns a grounded response to queries, such as recent event results, and is recommended for scenarios demanding dynamic, context-dependent search activation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst { text, providerMetadata } = await generateText({\n  model: google('gemini-1.5-flash', {\n    useSearchGrounding: true,\n    dynamicRetrievalConfig: {\n      mode: 'MODE_DYNAMIC',\n      dynamicThreshold: 0.8,\n    },\n  }),\n  prompt: 'Who won the latest F1 grand prix?',\n});\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Tool Calls in AI SDK\nDESCRIPTION: This snippet shows how to access and handle tool calls generated by the language model. It demonstrates type-safe access to the arguments passed to different tools using a switch statement to handle each tool type.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/50-call-tools.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText, tool } from 'ai';\nimport dotenv from 'dotenv';\nimport { z } from 'zod';\n\ndotenv.config();\n\nasync function main() {\n  const result = await generateText({\n    model: openai('gpt-3.5-turbo'),\n    maxTokens: 512,\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => ({\n          location,\n          temperature: 72 + Math.floor(Math.random() * 21) - 10,\n        }),\n      }),\n      cityAttractions: tool({\n        parameters: z.object({ city: z.string() }),\n      }),\n    },\n    prompt:\n      'What is the weather in San Francisco and what attractions should I visit?',\n  });\n\n  // typed tool calls:\n  for (const toolCall of result.toolCalls) {\n    switch (toolCall.toolName) {\n      case 'cityAttractions': {\n        toolCall.args.city; // string\n        break;\n      }\n\n      case 'weather': {\n        toolCall.args.location; // string\n        break;\n      }\n    }\n  }\n\n  console.log(JSON.stringify(result, null, 2));\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Implementing Retrieval Augmented Generation with AI SDK and OpenAI in TypeScript\nDESCRIPTION: This code snippet demonstrates the implementation of Retrieval Augmented Generation using the AI SDK and OpenAI. It processes an essay, creates embeddings, performs similarity searches, and generates text based on retrieved context. The script uses in-memory vector storage for simplicity.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/100-retrieval-augmented-generation.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport fs from 'fs';\nimport path from 'path';\nimport dotenv from 'dotenv';\nimport { openai } from '@ai-sdk/openai';\nimport { cosineSimilarity, embed, embedMany, generateText } from 'ai';\n\ndotenv.config();\n\nasync function main() {\n  const db: { embedding: number[]; value: string }[] = [];\n\n  const essay = fs.readFileSync(path.join(__dirname, 'essay.txt'), 'utf8');\n  const chunks = essay\n    .split('.')\n    .map(chunk => chunk.trim())\n    .filter(chunk => chunk.length > 0 && chunk !== '\\n');\n\n  const { embeddings } = await embedMany({\n    model: openai.embedding('text-embedding-3-small'),\n    values: chunks,\n  });\n  embeddings.forEach((e, i) => {\n    db.push({\n      embedding: e,\n      value: chunks[i],\n    });\n  });\n\n  const input =\n    'What were the two main things the author worked on before college?';\n\n  const { embedding } = await embed({\n    model: openai.embedding('text-embedding-3-small'),\n    value: input,\n  });\n  const context = db\n    .map(item => ({\n      document: item,\n      similarity: cosineSimilarity(embedding, item.embedding),\n    }))\n    .sort((a, b) => b.similarity - a.similarity)\n    .slice(0, 3)\n    .map(r => r.document.value)\n    .join('\\n');\n\n  const { text } = await generateText({\n    model: openai('gpt-4o'),\n    prompt: `Answer the following question based only on the provided context:\n             ${context}\n\n             Question: ${input}`,\n  });\n  console.log(text);\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Initializing Together.ai Language Model\nDESCRIPTION: Shows how to initialize a language model from Together.ai using the provider instance with a specific model ID.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/24-togetherai.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = togetherai('google/gemma-2-9b-it');\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for AI Providers\nDESCRIPTION: This snippet shows how to set up environment variables for both OpenAI and Anthropic API keys, allowing the application to switch between different AI providers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/02-multi-modal-chatbot.mdx#2025-04-23_snippet_6\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=xxxxxxxxx\nANTHROPIC_API_KEY=xxxxxxxxx\n```\n\n----------------------------------------\n\nTITLE: Passing Custom Attributes in AI SDK Telemetry (TypeScript)\nDESCRIPTION: This snippet shows how to pass custom attributes and metadata in the telemetry configuration of an AI SDK text generation request. It includes examples of setting trace name, tags, user ID, and session ID.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langfuse.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  prompt: 'Write a short story about a cat.',\n  experimental_telemetry: {\n    isEnabled: true,\n    functionId: 'my-awesome-function', // Trace name\n    metadata: {\n      langfuseTraceId: 'trace-123', // Langfuse trace\n      tags: ['story', 'cat'], // Custom tags\n      userId: 'user-123', // Langfuse user\n      sessionId: 'session-456', // Langfuse session\n      foo: 'bar', // Any custom attribute recorded in metadata\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Stream Protocol in React Frontend\nDESCRIPTION: Example of using useCompletion hook with text stream protocol in a Next.js client component. Shows how to set up a basic form with streaming completion responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useCompletion } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { completion, input, handleInputChange, handleSubmit } = useCompletion({\n    streamProtocol: 'text',\n  });\n\n  return (\n    <form onSubmit={handleSubmit}>\n      <input name=\"prompt\" value={input} onChange={handleInputChange} />\n      <button type=\"submit\">Submit</button>\n      <div>{completion}</div>\n    </form>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-Side Text Stream Generator\nDESCRIPTION: A server action that creates and manages a streamable text value. It uses OpenAI's GPT-3.5-turbo model to generate text and streams the response in real-time using createStreamableValue.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/20-stream-text.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n'use server';\n\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { createStreamableValue } from 'ai/rsc';\n\nexport async function generate(input: string) {\n  const stream = createStreamableValue('');\n\n  (async () => {\n    const { textStream } = streamText({\n      model: openai('gpt-3.5-turbo'),\n      prompt: input,\n    });\n\n    for await (const delta of textStream) {\n      stream.update(delta);\n    }\n\n    stream.done();\n  })();\n\n  return { output: stream.value };\n}\n```\n\n----------------------------------------\n\nTITLE: Defining AI Model Event and Streaming Types - TypeScript\nDESCRIPTION: This snippet provides TypeScript type/interface definitions for AI model API responses. It includes property typings for generated files, tool calls/results, finish reason codes, token usage tracking, request/response metadata, and provider-specific key/value metadata. This code is necessary for strong type-checking and IntelliSense when consuming these APIs in a TypeScript project. Inputs are AI provider response payloads, while outputs are strongly-typed objects describing content, streams, files, and related meta-information.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\n            {\n              name: 'files',\n              type: 'Array<GeneratedFile>',\n              description: 'Files that were generated in this step.',\n              properties: [\n                {\n                  type: 'GeneratedFile',\n                  parameters: [\n                    {\n                      name: 'base64',\n                      type: 'string',\n                      description: 'File as a base64 encoded string.',\n                    },\n                    {\n                      name: 'uint8Array',\n                      type: 'Uint8Array',\n                      description: 'File as a Uint8Array.',\n                    },\n                    {\n                      name: 'mimeType',\n                      type: 'string',\n                      description: 'MIME type of the file.',\n                    },\n                  ],\n                },\n              ],\n            },\n            {\n              name: 'toolCalls',\n              type: 'array',\n              description: 'A list of tool calls made by the model.',\n            },\n            {\n              name: 'toolResults',\n              type: 'array',\n              description:\n                'A list of tool results returned as responses to earlier tool calls.',\n            },\n            {\n              name: 'finishReason',\n              type: \"'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'\",\n              description: 'The reason the model finished generating the text.',\n            },\n            {\n              name: 'usage',\n              type: 'CompletionTokenUsage',\n              description: 'The token usage of the generated text.',\n              properties: [\n                {\n                  type: 'CompletionTokenUsage',\n                  parameters: [\n                    {\n                      name: 'promptTokens',\n                      type: 'number',\n                      description: 'The total number of tokens in the prompt.',\n                    },\n                    {\n                      name: 'completionTokens',\n                      type: 'number',\n                      description:\n                        'The total number of tokens in the completion.',\n                    },\n                    {\n                      name: 'totalTokens',\n                      type: 'number',\n                      description: 'The total number of tokens generated.',\n                    },\n                  ],\n                },\n              ],\n            },\n            {\n              name: 'request',\n              type: 'RequestMetadata',\n              isOptional: true,\n              description: 'Request metadata.',\n              properties: [\n                {\n                  type: 'RequestMetadata',\n                  parameters: [\n                    {\n                      name: 'body',\n                      type: 'string',\n                      description:\n                        'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).',\n                    },\n                  ],\n                },\n              ],\n            },\n            {\n              name: 'response',\n              type: 'ResponseMetadata',\n              isOptional: true,\n              description: 'Response metadata.',\n              properties: [\n                {\n                  type: 'ResponseMetadata',\n                  parameters: [\n                    {\n                      name: 'id',\n                      type: 'string',\n                      description:\n                        'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.',\n                    },\n                    {\n                      name: 'model',\n                      type: 'string',\n                      description:\n                        'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.',\n                    },\n                    {\n                      name: 'timestamp',\n                      type: 'Date',\n                      description:\n                        'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.',\n                    },\n                    {\n                      name: 'headers',\n                      isOptional: true,\n                      type: 'Record<string, string>',\n                      description: 'Optional response headers.',\n                    },\n                    {\n                      name: 'messages',\n                      type: 'Array<ResponseMessage>',\n                      description:\n                        'The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls.  When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.',\n                    },\n                  ],\n                },\n              ],\n            },\n            {\n              name: 'warnings',\n              type: 'Warning[] | undefined',\n              description:\n                'Warnings from the model provider (e.g. unsupported settings).',\n            },\n            {\n              name: 'isContinued',\n              type: 'boolean',\n              description:\n                'True when there will be a continuation step with a continuation text.',\n            },\n            {\n              name: 'providerMetadata',\n              type: 'Record<string,Record<string,JSONValue>> | undefined',\n              isOptional: true,\n              description:\n                'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.',\n            },\n```\n\n----------------------------------------\n\nTITLE: Generating Structured JSON Objects with FriendliAI and Zod\nDESCRIPTION: Demonstrates generating a structured JSON object that conforms to a Zod schema using the `generateObject` function from the AI SDK and a FriendliAI model. The `schema` and `schemaName` are provided to guide the model's output. Dependencies: `@friendliai/ai-provider`, `ai`, `zod`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { friendli } from '@friendliai/ai-provider';\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: friendli('meta-llama-3.3-70b-instruct'),\n  schemaName: 'CalendarEvent',\n  schema: z.object({\n    name: z.string(),\n    date: z.string(),\n    participants: z.array(z.string()),\n  }),\n  system: 'Extract the event information.',\n  prompt: 'Alice and Bob are going to a science fair on Friday.',\n});\n\nconsole.log(object);\n```\n\n----------------------------------------\n\nTITLE: Instantiating an OpenAI Language Model with Options in TypeScript\nDESCRIPTION: Demonstrates passing additional settings as a second argument when creating a language model instance. The available options depend on whether the underlying API chosen is chat or completion.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = openai('gpt-4-turbo', {\n  // additional settings\n});\n```\n\n----------------------------------------\n\nTITLE: Rendering Multi-Step Text Streaming on Client-Side with AI SDK React Hook\nDESCRIPTION: This code snippet shows how to implement the client-side rendering of multi-step text streaming using the AI SDK's React hook. It handles displaying messages, tool invocations, and user input for the chat interface.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/24-stream-text-multistep.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n\n  return (\n    <div>\n      {messages?.map(message => (\n        <div key={message.id}>\n          <strong>{`${message.role}: `}</strong>\n          {message.parts.map((part, index) => {\n            switch (part.type) {\n              case 'text':\n                return <span key={index}>{part.text}</span>;\n              case 'tool-invocation': {\n                return (\n                  <pre key={index}>\n                    {JSON.stringify(part.toolInvocation, null, 2)}\n                  </pre>\n                );\n              }\n            }\n          })}\n        </div>\n      ))}\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Streamable UI with Server Action in TypeScript\nDESCRIPTION: Illustrates how to create a streamable UI using createStreamableUI in a server action. The function updates the UI stream with a loading state and then the final weather information after a delay.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/05-streaming-values.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { createStreamableUI } from 'ai/rsc';\n\nexport async function getWeather() {\n  const weatherUI = createStreamableUI();\n\n  weatherUI.update(<div style={{ color: 'gray' }}>Loading...</div>);\n\n  setTimeout(() => {\n    weatherUI.done(<div>It&apos;s a sunny day!</div>);\n  }, 1000);\n\n  return weatherUI.value;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Firecrawl Web Scraping Tool with AI SDK in TypeScript\nDESCRIPTION: This example implements a web scraping tool using the Firecrawl API. It creates a tool that can crawl a specified URL and return its content, then demonstrates how to use this tool with OpenAI's model to retrieve information from a specific blog post. The code includes error handling and supports multiple content formats.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/56-web-search-agent.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText, tool } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\nimport FirecrawlApp from '@mendable/firecrawl-js';\nimport 'dotenv/config';\n\nconst app = new FirecrawlApp({ apiKey: process.env.FIRECRAWL_API_KEY });\n\nexport const webSearch = tool({\n  description: 'Search the web for up-to-date information',\n  parameters: z.object({\n    urlToCrawl: z\n      .string()\n      .url()\n      .min(1)\n      .max(100)\n      .describe('The URL to crawl (including http:// or https://)'),\n  }),\n  execute: async ({ urlToCrawl }) => {\n    const crawlResponse = await app.crawlUrl(urlToCrawl, {\n      limit: 1,\n      scrapeOptions: {\n        formats: ['markdown', 'html'],\n      },\n    });\n    if (!crawlResponse.success) {\n      throw new Error(`Failed to crawl: ${crawlResponse.error}`);\n    }\n    return crawlResponse.data;\n  },\n});\n\nconst main = async () => {\n  const { text } = await generateText({\n    model: openai('gpt-4o-mini'), // can be any model that supports tools\n    prompt: 'Get the latest blog post from vercel.com/blog',\n    tools: {\n      webSearch,\n    },\n    maxSteps: 2,\n  });\n  console.log(text);\n};\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Text Stream Consumer in React\nDESCRIPTION: A React component that handles text generation streaming using the readStreamableValue function. It maintains the generated text in state and updates it as new chunks arrive from the stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/20-stream-text.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { generate } from './actions';\nimport { readStreamableValue } from 'ai/rsc';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [generation, setGeneration] = useState<string>('');\n\n  return (\n    <div>\n      <button\n        onClick={async () => {\n          const { output } = await generate('Why is the sky blue?');\n\n          for await (const delta of readStreamableValue(output)) {\n            setGeneration(currentGeneration => `${currentGeneration}${delta}`);\n          }\n        }}\n      >\n        Ask\n      </button>\n\n      <div>{generation}</div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Experimental Features Interface\nDESCRIPTION: Defines interfaces for experimental features including telemetry settings, stream transformations, and tool call repair functionality.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  name: 'experimental_telemetry',\n  type: 'TelemetrySettings',\n  isOptional: true,\n  description: 'Telemetry configuration. Experimental feature.',\n  properties: [\n    {\n      type: 'TelemetrySettings',\n      parameters: [\n        {\n          name: 'isEnabled',\n          type: 'boolean',\n          isOptional: true,\n          description: 'Enable or disable telemetry. Disabled by default while experimental.'\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Converting StringOutputParser Stream with LangChain\nDESCRIPTION: Shows how to combine ChatOpenAI with StringOutputParser and convert the resulting stream using LangChainAdapter. Includes piping the model output through a string parser.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/16-langchain-adapter.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\nimport { LangChainAdapter } from 'ai';\nimport { StringOutputParser } from '@langchain/core/output_parsers';\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const model = new ChatOpenAI({\n    model: 'gpt-3.5-turbo-0125',\n    temperature: 0,\n  });\n\n  const parser = new StringOutputParser();\n  const stream = await model.pipe(parser).stream(prompt);\n\n  return LangChainAdapter.toDataStreamResponse(stream);\n}\n```\n\n----------------------------------------\n\nTITLE: Using cache control with system messages\nDESCRIPTION: Shows how to implement cache control for specific system messages to optimize performance in complex applications. This allows selective caching of system instructions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: anthropic('claude-3-5-sonnet-20240620'),\n  messages: [\n    {\n      role: 'system',\n      content: 'Cached system message part',\n      providerOptions: {\n        anthropic: { cacheControl: { type: 'ephemeral' } },\n      },\n    },\n    {\n      role: 'system',\n      content: 'Uncached system message part',\n    },\n    {\n      role: 'user',\n      content: 'User prompt',\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Conditional Rendering of Multiple UI Components in React TypeScript\nDESCRIPTION: This snippet demonstrates how to conditionally render multiple UI components based on the tool call response. It shows the complexity that can arise when dealing with multiple tools and interfaces.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/07-rendering-ui-with-language-models.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n{\n  message.role === 'tool' ? (\n    message.name === 'api-search-course' ? (\n      <Courses courses={message.content} />\n    ) : message.name === 'api-search-profile' ? (\n      <People people={message.content} />\n    ) : message.name === 'api-meetings' ? (\n      <Meetings meetings={message.content} />\n    ) : message.name === 'api-search-building' ? (\n      <Buildings buildings={message.content} />\n    ) : message.name === 'api-events' ? (\n      <Events events={message.content} />\n    ) : message.name === 'api-meals' ? (\n      <Meals meals={message.content} />\n    ) : null\n  ) : (\n    <div>{message.content}</div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Replacing Model Registry Function with Provider Registry Function in TypeScript\nDESCRIPTION: Illustrates the replacement of the removed `experimental_createModelRegistry` function with the `experimental_createProviderRegistry` function in AI SDK 4.0, reflecting the removal of the model registry concept.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_23\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_createModelRegistry } from 'ai';\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_createProviderRegistry } from 'ai';\n```\n\n----------------------------------------\n\nTITLE: Streaming Data with pipeDataStreamToResponse in Node.js HTTP Server\nDESCRIPTION: Creates a Node.js HTTP server that uses the AI SDK to generate text with OpenAI's GPT-4o model and streams the data response to the client using pipeDataStreamToResponse method.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/10-node-http-server.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport { createServer } from 'http';\n\ncreateServer(async (req, res) => {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    prompt: 'Invent a new holiday and describe its traditions.',\n  });\n\n  result.pipeDataStreamToResponse(res);\n}).listen(8080);\n```\n\n----------------------------------------\n\nTITLE: TypeScript Interface Definition for Core Message Types\nDESCRIPTION: Type definitions for system, user, assistant, and tool messages used in the Vercel AI API. Includes detailed parameter specifications for different message parts like text, image, file, reasoning, and tool calls.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/03-generate-object.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ninterface CoreSystemMessage {\n  role: 'system';\n  content: string;\n}\n\ninterface CoreUserMessage {\n  role: 'user';\n  content: string | Array<TextPart | ImagePart | FilePart>;\n}\n\ninterface CoreAssistantMessage {\n  role: 'assistant';\n  content: string | Array<TextPart | ReasoningPart | RedactedReasoningPart | ToolCallPart>;\n}\n\ninterface CoreToolMessage {\n  role: 'tool';\n  content: Array<ToolResultPart>;\n}\n```\n\n----------------------------------------\n\nTITLE: Checking for InvalidDataContentError Instance in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to check if a caught error object is an instance of `InvalidDataContentError` imported from the 'ai' (Vercel AI SDK) package. It uses the static `isInstance` method for type guarding, allowing specific handling for this error type, which occurs when invalid data content is provided in a multi-modal message part. Requires the 'ai' package to be installed.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-invalid-data-content-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nimport { InvalidDataContentError } from 'ai';\n\nif (InvalidDataContentError.isInstance(error)) {\n  // Handle the error\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Updating useObject Hook Methods\nDESCRIPTION: Shows the change from setInput to submit helper in the experimental_useObject hook.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_44\n\nLANGUAGE: typescript\nCODE:\n```\nconst { object, setInput } = useObject({\n  // ...\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst { object, submit } = useObject({\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring AI Instance with Database Persistence (TypeScript)\nDESCRIPTION: Creates an AI instance with database persistence using the onSetAIState callback. When conversation state changes and is marked as done, it automatically saves the state to a database via the saveChat function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/60-save-messages-to-database.mdx#2025-04-23_snippet_3\n\nLANGUAGE: ts\nCODE:\n```\nimport { createAI } from 'ai/rsc';\nimport { ServerMessage, ClientMessage, continueConversation } from './actions';\n\nexport const AI = createAI<ServerMessage[], ClientMessage[]>({\n  actions: {\n    continueConversation,\n  },\n  onSetAIState: async ({ state, done }) => {\n    'use server';\n\n    if (done) {\n      saveChat(state);\n    }\n  },\n  onGetUIState: async () => {\n    'use server';\n\n    const history: ServerMessage[] = getAIState();\n\n    return history.map(({ role, content }) => ({\n      id: generateId(),\n      role,\n      display:\n        role === 'function' ? <Stock {...JSON.parse(content)} /> : content,\n    }));\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Interface with React in Next.js\nDESCRIPTION: This code snippet creates a client-side chat interface using React in Next.js. It manages the chat state, handles user input, and sends requests to the server for generating responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/11-generate-text-with-chat-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { CoreMessage } from 'ai';\nimport { useState } from 'react';\n\nexport default function Page() {\n  const [input, setInput] = useState('');\n  const [messages, setMessages] = useState<CoreMessage[]>([]);\n\n  return (\n    <div>\n      <input\n        value={input}\n        onChange={event => {\n          setInput(event.target.value);\n        }}\n        onKeyDown={async event => {\n          if (event.key === 'Enter') {\n            setMessages(currentMessages => [\n              ...currentMessages,\n              { role: 'user', content: input },\n            ]);\n\n            const response = await fetch('/api/chat', {\n              method: 'POST',\n              body: JSON.stringify({\n                messages: [...messages, { role: 'user', content: input }],\n              }),\n            });\n\n            const { messages: newMessages } = await response.json();\n\n            setMessages(currentMessages => [\n              ...currentMessages,\n              ...newMessages,\n            ]);\n          }\n        }}\n      />\n\n      {messages.map((message, index) => (\n        <div key={`${message.role}-${index}`}>\n          {typeof message.content === 'string'\n            ? message.content\n            : message.content\n                .filter(part => part.type === 'text')\n                .map((part, partIndex) => (\n                  <div key={partIndex}>{part.text}</div>\n                ))}\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Tool Schema Discovery Using the MCP Client in AI SDK with TypeScript\nDESCRIPTION: This snippet invokes the 'tools' method on an MCP client without specifying any schema, causing the client to fetch and infer all available remote tools based on the server-provided schemas. It's best for quick prototyping or cases where all tools are needed, but provides no TypeScript safety or editor autocompletion. Dependencies include an initialized MCP client and a running MCP tool server. Outputs are a toolset mapped to the server's schema.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_22\n\nLANGUAGE: typescript\nCODE:\n```\nconst tools = await mcpClient.tools();\n```\n\n----------------------------------------\n\nTITLE: Customizing AI Gateway Request Options - TypeScript\nDESCRIPTION: Outlines how to set advanced options such as caching, metadata attachment, and retry policy when creating the AI Gateway provider instance. These settings control cache TTL, request metadata, retry attempts, strategies, and timeouts to manage request performance and error resilience. Key input parameters include accountId, gateway, apiKey, options with sub-parameters cacheTtl, metadata, retries, etc.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/12-cloudflare-ai-gateway.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst aigateway = createAiGateway({\n  accountId: 'your-cloudflare-account-id',\n  gateway: 'your-gateway-name',\n  apiKey: 'your-cloudflare-api-key',\n  options: {\n    cacheTtl: 3600, // Cache for 1 hour\n    metadata: { userId: 'user123' },\n    retries: {\n      maxAttempts: 3,\n      retryDelayMs: 1000,\n      backoff: 'exponential',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Exporting a Server Action Using OpenAI With Next.js (TypeScript)\nDESCRIPTION: This TypeScript code defines an exported asynchronous server action, 'getAnswer', which utilizes the 'ai' SDK and the '@ai-sdk/openai' client to query the gpt-3.5-turbo model with a given prompt. The function is annotated with 'use server' both at the file and function scope to ensure it is executed on the server, not in the client component. It imports the 'generateText' utility and the OpenAI integration; when called with a 'question' string, it returns an object with the model's generated answer. This pattern allows client components to interact with server-side generative models by exporting actions from a dedicated server module. Dependencies include the 'ai' and '@ai-sdk/openai' packages, and callers must provide a string input. Output is an object containing the generated 'answer'. File must be used server-side and cannot be declared inline within client components.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/03-server-actions-in-client-components.mdx#2025-04-23_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\n&#x60;'use server';\n\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nexport async function getAnswer(question: string) {\n  'use server';\n\n  const { text } = await generateText({\n    model: openai.chat('gpt-3.5-turbo'),\n    prompt: question,\n  });\n\n  return { answer: text };\n}\n&#x60;\n```\n\n----------------------------------------\n\nTITLE: Accessing Tool Results in AI SDK\nDESCRIPTION: This snippet illustrates how to access the results returned from executed tools. It demonstrates type-safe access to both the arguments passed to the tool and the results returned by the execute function of the tool.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/50-call-tools.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText, tool } from 'ai';\nimport dotenv from 'dotenv';\nimport { z } from 'zod';\n\ndotenv.config();\n\nasync function main() {\n  const result = await generateText({\n    model: openai('gpt-3.5-turbo'),\n    maxTokens: 512,\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => ({\n          location,\n          temperature: 72 + Math.floor(Math.random() * 21) - 10,\n        }),\n      }),\n      cityAttractions: tool({\n        parameters: z.object({ city: z.string() }),\n      }),\n    },\n    prompt:\n      'What is the weather in San Francisco and what attractions should I visit?',\n  });\n\n  // typed tool results for tools with execute method:\n  for (const toolResult of result.toolResults) {\n    switch (toolResult.toolName) {\n      case 'weather': {\n        toolResult.args.location; // string\n        toolResult.result.location; // string\n        toolResult.result.temperature; // number\n        break;\n      }\n    }\n  }\n\n  console.log(JSON.stringify(result, null, 2));\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Generating Text with File Input in Google Vertex Model (TypeScript)\nDESCRIPTION: Demonstrates prompting the Vertex model with both text and file (PDF) content, combining multiple types of input using an array of message content objects. File is read synchronously using Node's fs API. The generateText function processes the file and text, returning a text result. Dependencies: '@ai-sdk/google-vertex', 'ai', Node.js 'fs'.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertex } from '@ai-sdk/google-vertex';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: vertex('gemini-1.5-pro'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'What is an embedding model according to this document?',\n        },\n        {\n          type: 'file',\n          data: fs.readFileSync('./data/ai.pdf'),\n          mimeType: 'application/pdf',\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Creating OpenRouter Provider Instance - TypeScript\nDESCRIPTION: Shows how to import and initialize the OpenRouter provider using the createOpenRouter function from the @openrouter/ai-sdk-provider module in TypeScript. Requires an OpenRouter API key, which should be obtained from the OpenRouter Dashboard. The resulting instance (openrouter) is used for subsequent model and text generation operations. Input: configuration object containing apiKey. Output: provider instance ready for use in AI SDK workflows.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/13-openrouter.mdx#2025-04-23_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { createOpenRouter } from '@openrouter/ai-sdk-provider';\n\nconst openrouter = createOpenRouter({\n  apiKey: 'YOUR_OPENROUTER_API_KEY',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Session Tracking with Helicone in JavaScript\nDESCRIPTION: This code demonstrates how to implement session tracking in your AI SDK requests using Helicone. It groups related requests into coherent conversations by including session ID, name, and path in the headers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/helicone.mdx#2025-04-23_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst response = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'Tell me more about that',\n  headers: {\n    'Helicone-Session-Id': 'convo-123',\n    'Helicone-Session-Name': 'Travel Planning',\n    'Helicone-Session-Path': '/chats/travel',\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Tool Call ID in Tool Execution with AI SDK Core\nDESCRIPTION: This snippet demonstrates how to access and use the tool call ID in the execute function of a tool. It shows how to send tool-call related information with stream data using the StreamData class.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { StreamData, streamText, tool } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const data = new StreamData();\n\n  const result = streamText({\n    // ...\n    messages,\n    tools: {\n      myTool: tool({\n        // ...\n        execute: async (args, { toolCallId }) => {\n          // return e.g. custom status for tool call\n          data.appendMessageAnnotation({\n            type: 'tool-status',\n            toolCallId,\n            status: 'in-progress',\n          });\n          // ...\n        },\n      }),\n    },\n    onFinish() {\n      data.close();\n    },\n  });\n\n  return result.toDataStreamResponse({ data });\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat UI with React Hooks in Next.js\nDESCRIPTION: This code snippet creates the chat interface using the useChat hook from the AI SDK. It renders chat messages and provides an input field for user interaction, demonstrating how to handle user input and display AI responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#2025-04-23_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n            }\n          })}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Reasoning Tokens\nDESCRIPTION: Demonstrates how to enable and handle reasoning tokens in the response stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_16\n\nLANGUAGE: ts\nCODE:\n```\nimport { deepseek } from '@ai-sdk/deepseek';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: deepseek('deepseek-reasoner'),\n    messages,\n  });\n\n  return result.toDataStreamResponse({\n    sendReasoning: true,\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Error Handling with generateText\nDESCRIPTION: Example of handling various tool-related errors using try/catch blocks with generateText.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\ntry {\n  const result = await generateText({\n    //...\n  });\n} catch (error) {\n  if (NoSuchToolError.isInstance(error)) {\n    // handle the no such tool error\n  } else if (InvalidToolArgumentsError.isInstance(error)) {\n    // handle the invalid tool arguments error\n  } else if (ToolExecutionError.isInstance(error)) {\n    // handle the tool execution error\n  } else {\n    // handle other errors\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Object Streaming with React\nDESCRIPTION: A React client component that handles streaming object generation. Uses useState for state management and implements a button click handler to trigger object generation and stream updates.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/40-stream-object.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { generate } from './actions';\nimport { readStreamableValue } from 'ai/rsc';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [generation, setGeneration] = useState<string>('');\n\n  return (\n    <div>\n      <button\n        onClick={async () => {\n          const { object } = await generate('Messages during finals week.');\n\n          for await (const partialObject of readStreamableValue(object)) {\n            if (partialObject) {\n              setGeneration(\n                JSON.stringify(partialObject.notifications, null, 2),\n              );\n            }\n          }\n        }}\n      >\n        Ask\n      </button>\n\n      <pre>{generation}</pre>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing System Prompts with Text Prompts in TypeScript\nDESCRIPTION: This example shows how to combine system prompts with text prompts to guide the model's behavior for more controlled responses in travel planning.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  system:\n    `You help planning travel itineraries. ` +\n    `Respond to the users' request with a list ` +\n    `of the best stops to make in their destination.`,\n  prompt:\n    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +\n    `Please suggest the best tourist activities for me to do.`,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Chat API Route with Next.js and Claude 3.7 Sonnet\nDESCRIPTION: Implements a Next.js API route handler for a chat endpoint using Claude 3.7 Sonnet. This server-side code streams text responses and enables the model's extended thinking capability.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/20-sonnet-3-7.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: anthropic('claude-3-7-sonnet-20250219'),\n    messages,\n    providerOptions: {\n      anthropic: {\n        thinking: { type: 'enabled', budgetTokens: 12000 },\n      } satisfies AnthropicProviderOptions,\n    },\n  });\n\n  return result.toDataStreamResponse({\n    sendReasoning: true,\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Parameter Type Definitions - Message Types and Content Structures\nDESCRIPTION: Type definitions for various message formats and content structures used in the AI SDK, including system, user, assistant, and tool messages with their respective content types and parameters.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/04-stream-object.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ntype CoreSystemMessage = {\n  role: 'system',\n  content: string\n}\n\ntype CoreUserMessage = {\n  role: 'user',\n  content: string | Array<TextPart | ImagePart | FilePart>\n}\n\ntype CoreAssistantMessage = {\n  role: 'assistant',\n  content: string | Array<TextPart | ReasoningPart | RedactedReasoningPart | ToolCallPart>\n}\n\ntype CoreToolMessage = {\n  role: 'tool',\n  content: Array<ToolResultPart>\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with OpenAI Model in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the embed() function with an OpenAI embedding model to generate an embedding for a single text value. It imports necessary dependencies and calls the embed() function with a specified model and input value.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/05-embed.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { embed } from 'ai';\n\nconst { embedding } = await embed({\n  model: openai.embedding('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n});\n```\n\n----------------------------------------\n\nTITLE: Applying Cache Control to Anthropic System Messages (TypeScript)\nDESCRIPTION: This example shows how to apply cache control provider options to system messages in a generateText call. The messages array includes multiple system and user messages with and without cache control settings, illustrating granular cache configuration at the message part level. No external dependencies are shown, but this requires a properly configured provider and model instance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_28\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst result = await generateText({\n  model: vertexAnthropic('claude-3-5-sonnet-20240620'),\n  messages: [\n    {\n      role: 'system',\n      content: 'Cached system message part',\n      providerOptions: {\n        anthropic: { cacheControl: { type: 'ephemeral' } },\n      },\n    },\n    {\n      role: 'system',\n      content: 'Uncached system message part',\n    },\n    {\n      role: 'user',\n      content: 'User prompt',\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic OpenAI Compatible Provider Instance in TypeScript\nDESCRIPTION: Illustrates how to create an instance of an OpenAI compatible provider using the `createOpenAICompatible` function. It requires specifying a provider name, API key (typically from environment variables), and the base URL of the provider's API endpoint.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/index.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\n\nconst provider = createOpenAICompatible({\n  name: 'provider-name',\n  apiKey: process.env.PROVIDER_API_KEY,\n  baseURL: 'https://api.provider.com/v1',\n});\n```\n\n----------------------------------------\n\nTITLE: Abort Signal and Timeout Implementation\nDESCRIPTION: Demonstrates how to use AbortSignal to set timeouts or cancel speech generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/37-speech.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\nimport { readFile } from 'fs/promises';\n\nconst audio = await generateSpeech({\n  model: openai.speech('tts-1'),\n  text: 'Hello, world!',\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Headers to Embedding Request in TypeScript\nDESCRIPTION: This code demonstrates how to add custom headers to the embedding request using the 'headers' parameter. It allows you to include additional information or authentication in the request to the embedding API.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/30-embeddings.mdx#2025-04-23_snippet_6\n\nLANGUAGE: ts\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { embed } from 'ai';\n\nconst { embedding } = await embed({\n  model: openai.embedding('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n  headers: { 'X-Custom-Header': 'custom-value' },\n});\n```\n\n----------------------------------------\n\nTITLE: Chat Component Implementation with useChat Hook\nDESCRIPTION: Implements a React client component that handles chat interactions using the useChat hook with message persistence support.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-message-persistence.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { Message, useChat } from '@ai-sdk/react';\n\nexport default function Chat({\n  id,\n  initialMessages,\n}: { id?: string | undefined; initialMessages?: Message[] } = {}) {\n  const { input, handleInputChange, handleSubmit, messages } = useChat({\n    id, // use the provided chat ID\n    initialMessages, // initial messages if provided\n    sendExtraMessageFields: true, // send id and createdAt for each message\n  });\n\n  // simplified rendering code, extend as needed:\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role === 'user' ? 'User: ' : 'AI: '}\n          {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Defining CoreUserMessage and UserContent Types in TypeScript\nDESCRIPTION: Defines the structure for a user message that can contain text or a combination of text, images, and files. It includes a role of 'user' and a UserContent type.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/30-core-message.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ntype CoreUserMessage = {\n  role: 'user';\n  content: UserContent;\n};\n\ntype UserContent = string | Array<TextPart | ImagePart | FilePart>;\n```\n\n----------------------------------------\n\nTITLE: Processing Full Stream Events in AI SDK with TypeScript\nDESCRIPTION: Shows how to use the fullStream property to handle all types of events in the stream, including text deltas, reasoning, sources, tool calls, and errors.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\nimport { streamText } from 'ai';\nimport { z } from 'zod';\n\nconst result = streamText({\n  model: yourModel,\n  tools: {\n    cityAttractions: {\n      parameters: z.object({ city: z.string() }),\n      execute: async ({ city }) => ({\n        attractions: ['attraction1', 'attraction2', 'attraction3'],\n      }),\n    },\n  },\n  prompt: 'What are some San Francisco tourist attractions?',\n});\n\nfor await (const part of result.fullStream) {\n  switch (part.type) {\n    case 'text-delta': {\n      // handle text delta here\n      break;\n    }\n    case 'reasoning': {\n      // handle reasoning here\n      break;\n    }\n    case 'source': {\n      // handle source here\n      break;\n    }\n    case 'tool-call': {\n      switch (part.toolName) {\n        case 'cityAttractions': {\n          // handle tool call here\n          break;\n        }\n      }\n      break;\n    }\n    case 'tool-result': {\n      switch (part.toolName) {\n        case 'cityAttractions': {\n          // handle tool result here\n          break;\n        }\n      }\n      break;\n    }\n    case 'finish': {\n      // handle finish here\n      break;\n    }\n    case 'error': {\n      // handle error here\n      break;\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating text with Anthropic model\nDESCRIPTION: Demonstrates how to use the Anthropic model with the generateText function to create text based on a prompt. This example generates a vegetarian lasagna recipe.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: anthropic('claude-3-haiku-20240307'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Default Qwen Provider Instance - TypeScript\nDESCRIPTION: This TypeScript snippet imports the default 'qwen' provider instance from the 'qwen-ai-provider' package. It is necessary to import this default export before constructing or using Qwen-based models. Requires the 'qwen-ai-provider' package to be installed.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/02-qwen.mdx#2025-04-23_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { qwen } from 'qwen-ai-provider';\n```\n\n----------------------------------------\n\nTITLE: Implementing Resource Tool Integration\nDESCRIPTION: Advanced implementation adding a tool for managing knowledge base resources with parameter validation using Zod.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_14\n\nLANGUAGE: tsx\nCODE:\n```\nimport { createResource } from '@/lib/actions/resources';\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, tool } from 'ai';\nimport { z } from 'zod';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: `You are a helpful assistant. Check your knowledge base before answering any questions.\n    Only respond to questions using information from tool calls.\n    if no relevant information is found in the tool calls, respond, \"Sorry, I don't know.\"`,\n    messages,\n    tools: {\n      addResource: tool({\n        description: `add a resource to your knowledge base.\n          If the user provides a random piece of knowledge unprompted, use this tool without asking for confirmation.`,\n        parameters: z.object({\n          content: z\n            .string()\n            .describe('the content or resource to add to the knowledge base'),\n        }),\n        execute: async ({ content }) => createResource({ content }),\n      }),\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Intercepting OpenAI API Requests with Custom Fetch in TypeScript\nDESCRIPTION: This code snippet shows how to create a custom fetch function to intercept OpenAI API requests. It logs the URL, headers, and request body before forwarding the request. The example also demonstrates how to use this custom fetch with the createOpenAI function and generate text using the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/70-intercept-fetch-requests.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst openai = createOpenAI({\n  // example fetch wrapper that logs the input to the API call:\n  fetch: async (url, options) => {\n    console.log('URL', url);\n    console.log('Headers', JSON.stringify(options!.headers, null, 2));\n    console.log(\n      `Body ${JSON.stringify(JSON.parse(options!.body! as string), null, 2)}`,\n    );\n    return await fetch(url, options);\n  },\n});\n\nconst { text } = await generateText({\n  model: openai('gpt-3.5-turbo'),\n  prompt: 'Why is the sky blue?',\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Structured JSON Data with Llama 3.1\nDESCRIPTION: This snippet shows how to generate structured JSON data using the generateObject function from the AI SDK with the Llama 3.1 70B model. It uses Zod to define a schema for a recipe structure, ensuring type-safe output.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateObject } from 'ai';\nimport { deepinfra } from '@ai-sdk/deepinfra';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Max Duration in Next.js App Router (tsx)\nDESCRIPTION: This TypeScript export sets the `maxDuration` configuration option for a Vercel Function within a Next.js App Router application. Setting it to 30 increases the maximum allowed execution time to 30 seconds, helping to prevent timeouts for longer-running server-side operations, such as AI streaming responses. This should be placed in the relevant route file (e.g., `route.ts`) or the page/component invoking a Server Action.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/06-timeout-on-vercel.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nexport const maxDuration = 30;\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom OpenAI Provider with Model Settings\nDESCRIPTION: Creates a custom OpenAI provider with overridden model settings and aliases for structured outputs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/45-provider-management.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai as originalOpenAI } from '@ai-sdk/openai';\nimport { customProvider } from 'ai';\n\n// custom provider with different model settings:\nexport const openai = customProvider({\n  languageModels: {\n    // replacement model with custom settings:\n    'gpt-4o': originalOpenAI('gpt-4o', { structuredOutputs: true }),\n    // alias model with custom settings:\n    'gpt-4o-mini-structured': originalOpenAI('gpt-4o-mini', {\n      structuredOutputs: true,\n    }),\n  },\n  fallbackProvider: originalOpenAI,\n});\n```\n\n----------------------------------------\n\nTITLE: Structured Data Generation with o3-mini\nDESCRIPTION: Demonstrates generating structured JSON data using o3-mini with schema validation through Zod. This example shows how to generate a recipe with typed output.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/24-o3.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: openai('o3-mini'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Using PDF Files in Response Messages\nDESCRIPTION: This code demonstrates passing a PDF file within the message content to the OpenAI responses API. The PDF content is included as a file object with data, mimeType, and optional filename, allowing the model to process and respond based on the PDF content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_23\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst result = await generateText({\n  model: openai.responses('gpt-4o'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'What is an embedding model?',\n        },\n        {\n          type: 'file',\n          data: fs.readFileSync('./data/ai.pdf'),\n          mimeType: 'application/pdf',\n          filename: 'ai.pdf', // optional\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Import and Initialize ChromeAI Provider - TypeScript\nDESCRIPTION: Demonstrates how to import the 'chromeai' provider from the 'chrome-ai' module and instantiate the default language model. No dependencies are required beyond the installed 'chrome-ai' package. The resulting 'model' instance is ready for use with supported SDK functions, with no parameters required for initialization.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/04-chrome-ai.mdx#2025-04-23_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { chromeai } from 'chrome-ai';\\n\\nconst model = chromeai();\n```\n\n----------------------------------------\n\nTITLE: Streaming Object Response with URL Image Input\nDESCRIPTION: Demonstrates how to stream a structured object response using an image provided via URL. Uses OpenAI's GPT-4 Turbo model with Zod schema validation to process passport stamp information from an image.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/41-stream-object-with-image-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport dotenv from 'dotenv';\nimport { z } from 'zod';\n\ndotenv.config();\n\nasync function main() {\n  const { partialObjectStream } = streamObject({\n    model: openai('gpt-4-turbo'),\n    maxTokens: 512,\n    schema: z.object({\n      stamps: z.array(\n        z.object({\n          country: z.string(),\n          date: z.string(),\n        }),\n      ),\n    }),\n    messages: [\n      {\n        role: 'user',\n        content: [\n          {\n            type: 'text',\n            text: 'list all the stamps in these passport pages?',\n          },\n          {\n            type: 'image',\n            image: new URL(\n              'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/WW2_Spanish_official_passport.jpg/1498px-WW2_Spanish_official_passport.jpg',\n            ),\n          },\n        ],\n      },\n    ],\n  });\n\n  for await (const partialObject of partialObjectStream) {\n    console.clear();\n    console.log(partialObject);\n  }\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Testing Express AI Endpoint with curl\nDESCRIPTION: A simple curl command to test the Express server endpoint by making a POST request to localhost port 8080.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/20-express.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8080\n```\n\n----------------------------------------\n\nTITLE: Updating API Route with Weather Tool in TypeScript\nDESCRIPTION: This snippet shows how to modify the server-side API route to include a weather tool. It demonstrates importing necessary functions, defining the tool with parameters and execution logic, and integrating it into the streamText function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText, tool } from 'ai';\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nexport default defineLazyEventHandler(async () => {\n  const apiKey = useRuntimeConfig().openaiApiKey;\n  if (!apiKey) throw new Error('Missing OpenAI API key');\n  const openai = createOpenAI({\n    apiKey: apiKey,\n  });\n\n  return defineEventHandler(async (event: any) => {\n    const { messages } = await readBody(event);\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages,\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (fahrenheit)',\n          parameters: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => {\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n            return {\n              location,\n              temperature,\n            };\n          },\n        }),\n      },\n    });\n\n    return result.toDataStreamResponse();\n  });\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Chat Message Data Structure with Parts (TypeScript)\nDESCRIPTION: This snippet outlines the 'UIMessage' data structure, which represents a chat message and its properties including id, role, creation time, content, optional annotations, various message parts, and optional attachments. It specifies each type and nested object, enforcing strong typing for message roles, associating an array of disparate part types (text, reasoning, tools, sources, steps), and documenting possible attachment metadata. Dependencies include awareness of the various UI part and attachment types, and some parameters are explicitly optional. Inputs and outputs are JavaScript/TypeScript objects conforming to these type definitions; this design is used to ensure consistent message serialization, parsing, and rendering by frontend chat UIs. Limitations: does not validate actual content, just types.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#2025-04-23_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\ninterface UIMessage {\n  id: string; // The unique identifier of the message.\n  role: 'system' | 'user' | 'assistant' | 'data'; // The role of the message.\n  createdAt?: Date; // The creation date of the message (optional).\n  content: string; // The content of the message.\n  annotations?: Array<JSONValue>; // Additional annotations sent along with the message (optional).\n  parts: Array<\n    TextUIPart |\n    ReasoningUIPart |\n    ToolInvocationUIPart |\n    SourceUIPart |\n    StepStartUIPart\n  >; // Message parts associated with the message.\n  experimental_attachments?: Array<Attachment>; // Additional attachments sent along with the message (optional).\n}\n\n```\n\n----------------------------------------\n\nTITLE: Loading Existing Chat Messages\nDESCRIPTION: Implements a dynamic route that loads existing chat messages for a given chat ID and renders the chat interface.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-message-persistence.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { loadChat } from '@tools/chat-store';\nimport Chat from '@ui/chat';\n\nexport default async function Page(props: { params: Promise<{ id: string }> }) {\n  const { id } = await props.params; // get the chat ID from the URL\n  const messages = await loadChat(id); // load the chat messages\n  return <Chat id={id} initialMessages={messages} />; // display the chat\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing the onChunk Streaming Callback (TypeScript)\nDESCRIPTION: This snippet defines the onChunk callback signature in TypeScript, used to handle each chunk of an AI-generated stream. It includes TypeScript interface definitions for OnChunkResult and nested TextStreamPart variants, detailing fields like type, textDelta, toolCallId, toolName, args, and source. The callback is pausable depending on Promise resolution, and is highly extensible for different streaming part types. The main input is an 'event' containing the detailed chunk, and no return value is expected unless the callback is asynchronous.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  name: 'onChunk',\n  type: '(event: OnChunkResult) => Promise<void> |void',\n  isOptional: true,\n  description:\n    'Callback that is called for each chunk of the stream. The stream processing will pause until the callback promise is resolved.',\n  properties: [\n    {\n      type: 'OnChunkResult',\n      parameters: [\n        {\n          name: 'chunk',\n          type: 'TextStreamPart',\n          description: 'The chunk of the stream.',\n          properties: [\n            {\n              type: 'TextStreamPart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'text-delta'\",\n                  description:\n                    'The type to identify the object as text delta.',\n                },\n                {\n                  name: 'textDelta',\n                  type: 'string',\n                  description: 'The text delta.',\n                },\n              ],\n            },\n            {\n              type: 'TextStreamPart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'reasoning'\",\n                  description:\n                    'The type to identify the object as reasoning.',\n                },\n                {\n                  name: 'textDelta',\n                  type: 'string',\n                  description: 'The reasoning text delta.',\n                },\n              ],\n            },\n            {\n              type: 'TextStreamPart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'source'\",\n                  description: 'The type to identify the object as source.',\n                },\n                {\n                  name: 'source',\n                  type: 'Source',\n                  description: 'The source.',\n                },\n              ],\n            },\n            {\n              type: 'TextStreamPart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'tool-call'\",\n                  description:\n                    'The type to identify the object as tool call.',\n                },\n                {\n                  name: 'toolCallId',\n                  type: 'string',\n                  description: 'The id of the tool call.',\n                },\n                {\n                  name: 'toolName',\n                  type: 'string',\n                  description:\n                    'The name of the tool, which typically would be the name of the function.',\n                },\n                {\n                  name: 'args',\n                  type: 'object based on zod schema',\n                  description:\n                    'Parameters generated by the model to be used by the tool.',\n                },\n              ],\n            },\n            {\n              type: 'TextStreamPart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'tool-call-streaming-start'\",\n                  description:\n                    'Indicates the start of a tool call streaming. Only available when streaming tool calls.',\n                },\n                {\n                  name: 'toolCallId',\n                  type: 'string',\n                  description: 'The id of the tool call.',\n                },\n                {\n                  name: 'toolName',\n                  type: 'string',\n                  description:\n                    'The name of the tool, which typically would be the name of the function.',\n                },\n              ],\n            },\n            {\n              type: 'TextStreamPart',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'tool-call-delta'\",\n                  description:\n                    'The type to identify the object as tool call delta. Only available when streaming tool calls.',\n                },\n                {\n                  name: 'toolCallId',\n                  type: 'string',\n                  description: 'The id of the tool call.',\n                },\n                {\n                  name: 'toolName',\n                  type: 'string',\n                  description:\n                    'The name of the tool, which typically would be the name of the function.',\n                },\n                {\n                  name: 'argsTextDelta',\n                  type: 'string',\n                  description: 'The text delta of the tool call arguments.',\n                },\n              ],\n            },\n            {\n              type: 'TextStreamPart',\n              description: 'The result of a tool call execution.',\n              parameters: [\n                {\n                  name: 'type',\n                  type: \"'tool-result'\",\n                  description:\n                    'The type to identify the object as tool result.',\n                },\n                {\n                  name: 'toolCallId',\n                  type: 'string',\n                  description: 'The id of the tool call.',\n                },\n                {\n                  name: 'toolName',\n                  type: 'string',\n                  description:\n                    'The name of the tool, which typically would be the name of the function.',\n                },\n                {\n                  name: 'args',\n                  type: 'object based on zod schema',\n                  description:\n                    'Parameters generated by the model to be used by the tool.',\n                },\n                {\n                  name: 'result',\n                  type: 'any',\n                  description:\n                    'The result returned by the tool after execution has completed.',\n                },\n              ],\n            },\n          ],\n        },\n      ],\n    },\n  ],\n},\n```\n\n----------------------------------------\n\nTITLE: Checking Flight Status Using Multiple Composed Tools\nDESCRIPTION: This snippet demonstrates how multiple tools (lookupContacts, lookupBooking, and lookupFlight) can be composed to provide a comprehensive response to a user query about flight status. It showcases how tool composition can create more powerful and context-aware interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/09-multistep-interfaces.mdx#2025-04-23_snippet_3\n\nLANGUAGE: txt\nCODE:\n```\nUser: What's the status of my wife's upcoming flight?\nTool: lookupContacts() -> [\"John Doe\", \"Jane Doe\"]\nTool: lookupBooking(\"Jane Doe\") -> \"BA123 confirmed\"\nTool: lookupFlight(\"BA123\") -> \"Flight BA123 is scheduled to depart on 12th December.\"\nModel: Your wife's flight BA123 is confirmed and scheduled to depart on 12th December.\n```\n\n----------------------------------------\n\nTITLE: Adjusting Temperature in Slogan Generator Prompt in Markdown\nDESCRIPTION: This snippet demonstrates how to adjust the temperature setting in the prompt to control the variability of generated slogans.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/01-prompt-engineering.mdx#2025-04-23_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n<InlinePrompt\n  initialInput={`Create three slogans for a business with unique features.\n\nBusiness: Bookstore with cats\nSlogans: \"Purr-fect Pages\", \"Books and Whiskers\", \"Novels and Nuzzles\"\nBusiness: Gym with rock climbing\nSlogans: \"Peak Performance\", \"Reach New Heights\", \"Climb Your Way Fit\"\nBusiness: Coffee shop with live music\nSlogans:`}\nshowTemp={true}\ninitialTemperature={1}\n/>\n```\n\n----------------------------------------\n\nTITLE: Handling General Stream Errors with StreamableValue in TypeScript\nDESCRIPTION: Demonstrates error handling for non-UI streaming data using createStreamableValue. Includes error object return pattern and sequential data updates.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/08-error-handling.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { createStreamableValue } from 'ai/rsc';\nimport { fetchData, emptyData } from '../utils/data';\n\nexport const getStreamedData = async () => {\n  const streamableData = createStreamableValue<string>(emptyData);\n\n  try {\n    (() => {\n      const data1 = await fetchData();\n      streamableData.update(data1);\n\n      const data2 = await fetchData();\n      streamableData.update(data2);\n\n      const data3 = await fetchData();\n      streamableData.done(data3);\n    })();\n\n    return { data: streamableData.value };\n  } catch (e) {\n    return { error: e.message };\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Streaming Loading Components with streamUI in AI SDK RSC\nDESCRIPTION: This server-side code demonstrates how to use the streamUI function to stream loading components to the client. It uses a generator function to yield a loading component while the main content is being generated.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/06-loading-state.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n'use server';\n\nimport { openai } from '@ai-sdk/openai';\nimport { streamUI } from 'ai/rsc';\n\nexport async function generateResponse(prompt: string) {\n  const result = await streamUI({\n    model: openai('gpt-4o'),\n    prompt,\n    text: async function* ({ content }) {\n      yield <div>loading...</div>;\n      return <div>{content}</div>;\n    },\n  });\n\n  return result.value;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Chat Interface\nDESCRIPTION: Creates the client-side chat interface using the useChat hook and MemoizedMarkdown component. Implements message display and input handling with performance optimizations through throttling and component separation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/25-markdown-chatbot-with-memoization.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\nimport { MemoizedMarkdown } from '@/components/memoized-markdown';\n\nexport default function Page() {\n  const { messages } = useChat({\n    id: 'chat',\n    // Throttle the messages and data updates to 50ms:\n    experimental_throttle: 50,\n  });\n\n  return (\n    <div className=\"flex flex-col w-full max-w-xl py-24 mx-auto stretch\">\n      <div className=\"space-y-8 mb-4\">\n        {messages.map(message => (\n          <div key={message.id}>\n            <div className=\"font-bold mb-2\">\n              {message.role === 'user' ? 'You' : 'Assistant'}\n            </div>\n            <div className=\"prose space-y-2\">\n              <MemoizedMarkdown id={message.id} content={message.content} />\n            </div>\n          </div>\n        ))}\n      </div>\n      <MessageInput />\n    </div>\n  );\n}\n\nconst MessageInput = () => {\n  const { input, handleSubmit, handleInputChange } = useChat({ id: 'chat' });\n  return (\n    <form onSubmit={handleSubmit}>\n      <input\n        className=\"fixed bottom-0 w-full max-w-xl p-2 mb-8 dark:bg-zinc-900 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n        placeholder=\"Say something...\"\n        value={input}\n        onChange={handleInputChange}\n      />\n    </form>\n  );\n};\n```\n\n----------------------------------------\n\nTITLE: Updating API Route to Include Custom Tool in TypeScript\nDESCRIPTION: This code updates the API route to include the custom weather tool in the streamText function call, allowing the model to use the tool for weather-related queries.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#2025-04-23_snippet_3\n\nLANGUAGE: ts\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport { tools } from '@/ai/tools';\n\nexport async function POST(request: Request) {\n  const { messages } = await request.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'You are a friendly assistant!',\n    messages,\n    maxSteps: 5,\n    tools,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Weather and Temperature Conversion Tools\nDESCRIPTION: Enhanced route handler implementation that includes both weather information retrieval and Fahrenheit to Celsius conversion tools.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n      convertFahrenheitToCelsius: tool({\n        description: 'Convert a temperature in fahrenheit to celsius',\n        parameters: z.object({\n          temperature: z\n            .number()\n            .describe('The temperature in fahrenheit to convert'),\n        }),\n        execute: async ({ temperature }) => {\n          const celsius = Math.round((temperature - 32) * (5 / 9));\n          return {\n            celsius,\n          };\n        },\n      }),\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Chat Component with File Upload in React\nDESCRIPTION: This code snippet shows how to modify the main Chat component to include file upload functionality. It adds state management for files, updates the form submission process, and renders uploaded images.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/02-multi-modal-chatbot.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\nimport { useRef, useState } from 'react';\nimport Image from 'next/image';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n\n  const [files, setFiles] = useState<FileList | undefined>(undefined);\n  const fileInputRef = useRef<HTMLInputElement>(null);\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(m => (\n        <div key={m.id} className=\"whitespace-pre-wrap\">\n          {m.role === 'user' ? 'User: ' : 'AI: '}\n          {m.content}\n          <div>\n            {m?.experimental_attachments\n              ?.filter(attachment =>\n                attachment?.contentType?.startsWith('image/'),\n              )\n              .map((attachment, index) => (\n                <Image\n                  key={`${m.id}-${index}`}\n                  src={attachment.url}\n                  width={500}\n                  height={500}\n                  alt={attachment.name ?? `attachment-${index}`}\n                />\n              ))}\n          </div>\n        </div>\n      ))}\n\n      <form\n        className=\"fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl space-y-2\"\n        onSubmit={event => {\n          handleSubmit(event, {\n            experimental_attachments: files,\n          });\n\n          setFiles(undefined);\n\n          if (fileInputRef.current) {\n            fileInputRef.current.value = '';\n          }\n        }}\n      >\n        <input\n          type=\"file\"\n          className=\"\"\n          onChange={event => {\n            if (event.target.files) {\n              setFiles(event.target.files);\n            }\n          }}\n          multiple\n          ref={fileInputRef}\n        />\n        <input\n          className=\"w-full p-2\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Updating API Route with Weather Tool in TypeScript\nDESCRIPTION: This snippet shows how to modify the API route to include a weather tool. It uses the 'ai' package and Zod for schema validation, defining a tool that simulates getting weather data for a given location.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n    },\n  });\n\n  return result.toDataStreamResponse({\n    headers: {\n      'Content-Type': 'application/octet-stream',\n      'Content-Encoding': 'none',\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Evaluator-Optimizer Pattern with AI SDK in TypeScript\nDESCRIPTION: This snippet showcases the evaluator-optimizer pattern for quality control in AI workflows. It uses the AI SDK to translate text, evaluate the translation quality, and iteratively improve the result based on feedback.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText, generateObject } from 'ai';\nimport { z } from 'zod';\n\nasync function translateWithFeedback(text: string, targetLanguage: string) {\n  let currentTranslation = '';\n  let iterations = 0;\n  const MAX_ITERATIONS = 3;\n\n  // Initial translation\n  const { text: translation } = await generateText({\n    model: openai('gpt-4o-mini'), // use small model for first attempt\n    system: 'You are an expert literary translator.',\n    prompt: `Translate this text to ${targetLanguage}, preserving tone and cultural nuances:\n    ${text}`,\n  });\n\n  currentTranslation = translation;\n\n  // Evaluation-optimization loop\n  while (iterations < MAX_ITERATIONS) {\n    // Evaluate current translation\n    const { object: evaluation } = await generateObject({\n      model: openai('gpt-4o'), // use a larger model to evaluate\n      schema: z.object({\n        qualityScore: z.number().min(1).max(10),\n        preservesTone: z.boolean(),\n        preservesNuance: z.boolean(),\n        culturallyAccurate: z.boolean(),\n        specificIssues: z.array(z.string()),\n        improvementSuggestions: z.array(z.string()),\n      }),\n      system: 'You are an expert in evaluating literary translations.',\n      prompt: `Evaluate this translation:\n\n      Original: ${text}\n      Translation: ${currentTranslation}\n\n      Consider:\n      1. Overall quality\n      2. Preservation of tone\n      3. Preservation of nuance\n      4. Cultural accuracy`,\n    });\n\n    // Check if quality meets threshold\n    if (\n      evaluation.qualityScore >= 8 &&\n      evaluation.preservesTone &&\n      evaluation.preservesNuance &&\n      evaluation.culturallyAccurate\n    ) {\n      break;\n    }\n\n    // Generate improved translation based on feedback\n    const { text: improvedTranslation } = await generateText({\n      model: openai('gpt-4o'), // use a larger model\n      system: 'You are an expert literary translator.',\n      prompt: `Improve this translation based on the following feedback:\n      ${evaluation.specificIssues.join('\\n')}\n      ${evaluation.improvementSuggestions.join('\\n')}\n\n      Original: ${text}\n      Current Translation: ${currentTranslation}`,\n    });\n\n    currentTranslation = improvedTranslation;\n    iterations++;\n  }\n\n  return {\n    finalTranslation: currentTranslation,\n    iterationsRequired: iterations,\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Chat UI in Expo with React Native\nDESCRIPTION: This code implements a chat interface using React Native components and the useChat hook from the AI SDK. It handles user input, displays messages, and manages the chat state while supporting streaming responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateAPIUrl } from '@/utils';\nimport { useChat } from '@ai-sdk/react';\nimport { fetch as expoFetch } from 'expo/fetch';\nimport { View, TextInput, ScrollView, Text, SafeAreaView } from 'react-native';\n\nexport default function App() {\n  const { messages, error, handleInputChange, input, handleSubmit } = useChat({\n    fetch: expoFetch as unknown as typeof globalThis.fetch,\n    api: generateAPIUrl('/api/chat'),\n    onError: error => console.error(error, 'ERROR'),\n  });\n\n  if (error) return <Text>{error.message}</Text>;\n\n  return (\n    <SafeAreaView style={{ height: '100%' }}>\n      <View\n        style={{\n          height: '95%',\n          display: 'flex',\n          flexDirection: 'column',\n          paddingHorizontal: 8,\n        }}\n      >\n        <ScrollView style={{ flex: 1 }}>\n          {messages.map(m => (\n            <View key={m.id} style={{ marginVertical: 8 }}>\n              <View>\n                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\n                <Text>{m.content}</Text>\n              </View>\n            </View>\n          ))}\n        </ScrollView>\n\n        <View style={{ marginTop: 8 }}>\n          <TextInput\n            style={{ backgroundColor: 'white', padding: 8 }}\n            placeholder=\"Say something...\"\n            value={input}\n            onChange={e =>\n              handleInputChange({\n                ...e,\n                target: {\n                  ...e.target,\n                  value: e.nativeEvent.text,\n                },\n              } as unknown as React.ChangeEvent<HTMLInputElement>)\n            }\n            onSubmitEditing={e => {\n              handleSubmit(e);\n              e.preventDefault();\n            }}\n            autoFocus={true}\n          />\n        </View>\n      </View>\n    </SafeAreaView>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Text with FriendliAI and AI SDK\nDESCRIPTION: Demonstrates generating text using a FriendliAI model within the AI SDK. It imports the provider and the `generateText` function, creates a model instance, and calls `generateText` with the model and a prompt. Dependencies include `@friendliai/ai-provider` and `ai`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { friendli } from '@friendliai/ai-provider';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: friendli('meta-llama-3.1-8b-instruct'),\n  prompt: 'What is the meaning of life?',\n});\n\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Using Perplexity Provider with AI SDK generateText (TypeScript)\nDESCRIPTION: Illustrates generating text using a specific Perplexity model ('sonar-pro') via the AI SDK's generateText function. Requires both @ai-sdk/perplexity and ai modules. The prompt parameter provides the input for completion, and the output's text property contains the model's response. Ensures Perplexity provider is properly configured and accessible.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/70-perplexity.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { perplexity } from '@ai-sdk/perplexity';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: perplexity('sonar-pro'),\n  prompt: 'What are the latest developments in quantum computing?',\n});\n```\n\n----------------------------------------\n\nTITLE: Building a Client-Side Chat Interface with Image Display in Next.js\nDESCRIPTION: This client-side React component creates a chat interface using the useChat hook from the AI SDK. It renders chat messages and displays generated images when the assistant invokes the generateImage tool, using Next.js Image component to show the base64-encoded images.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/12-generate-image-with-chat-prompt.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\nimport Image from 'next/image';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      <div className=\"space-y-4\">\n        {messages.map(m => (\n          <div key={m.id} className=\"whitespace-pre-wrap\">\n            <div key={m.id}>\n              <div className=\"font-bold\">{m.role}</div>\n              {m.toolInvocations ? (\n                m.toolInvocations.map(ti =>\n                  ti.toolName === 'generateImage' ? (\n                    ti.state === 'result' ? (\n                      <Image\n                        key={ti.toolCallId}\n                        src={`data:image/png;base64,${ti.result.image}`}\n                        alt={ti.result.prompt}\n                        height={400}\n                        width={400}\n                      />\n                    ) : (\n                      <div key={ti.toolCallId} className=\"animate-pulse\">\n                        Generating image...\n                      </div>\n                    )\n                  ) : null,\n                )\n              ) : (\n                <p>{m.content}</p>\n              )}\n            </div>\n          </div>\n        ))}\n      </div>\n\n      <form onSubmit={handleSubmit}>\n        <input\n          className=\"fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Updating API Route Handler for PDF Support\nDESCRIPTION: This code modifies the API route handler to support PDF processing using Anthropic's Claude model. It checks for PDF attachments and routes requests to the appropriate AI model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/02-multi-modal-chatbot.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { streamText, type Message } from 'ai';\n\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: Message[] } = await req.json();\n\n  // check if user has sent a PDF\n  const messagesHavePDF = messages.some(message =>\n    message.experimental_attachments?.some(\n      a => a.contentType === 'application/pdf',\n    ),\n  );\n\n  const result = streamText({\n    model: messagesHavePDF\n      ? anthropic('claude-3-5-sonnet-latest')\n      : openai('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Migrating from Completions API to Responses API\nDESCRIPTION: Demonstrates how to migrate from the OpenAI Completions API to the new Responses API by changing the model provider instance format while maintaining the same functionality.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/19-openai-responses.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n// Completions API\nconst { text } = await generateText({\n  model: openai('gpt-4o'),\n  prompt: 'Explain the concept of quantum entanglement.',\n});\n\n// Responses API\nconst { text } = await generateText({\n  model: openai.responses('gpt-4o'),\n  prompt: 'Explain the concept of quantum entanglement.',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Customized Mixedbread Provider Instance - TypeScript\nDESCRIPTION: This TypeScript code shows how to import and use the createMixedbread function to initialize a customized Mixedbread provider with a specified base URL and API key. The configuration object allows specifying the API URL and custom API keys, usually loaded from environment variables. This enables advanced scenarios such as using self-hosted endpoints or passing extra headers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/60-mixedbread.mdx#2025-04-23_snippet_2\n\nLANGUAGE: ts\nCODE:\n```\nimport { createMixedbread } from 'mixedbread-ai-provider';\n\nconst mixedbread = createMixedbread({\n  baseURL: 'https://api.mixedbread.ai/v1',\n  apiKey: process.env.MIXEDBREAD_API_KEY,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Transcription Model in TypeScript\nDESCRIPTION: Shows the simplest way to create a Gladia transcription model instance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/120-gladia.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = gladia.transcription();\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Callbacks\nDESCRIPTION: Example of implementing lifecycle event callbacks for completion requests, including response handling and error management.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/05-completion.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\nconst { ... } = useCompletion({\n  onResponse: (response: Response) => {\n    console.log('Received response from server:', response)\n  },\n  onFinish: (message: Message) => {\n    console.log('Finished streaming message:', message)\n  },\n  onError: (error: Error) => {\n    console.error('An error occurred:', error)\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Replacing experimental_StreamData with StreamData Import in TypeScript\nDESCRIPTION: Illustrates the removal of the `experimental_StreamData` export in AI SDK 4.0. The stable `StreamData` export should be used instead.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_34\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_StreamData } from 'ai';\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { StreamData } from 'ai';\n```\n\n----------------------------------------\n\nTITLE: Server-Side UI Rendering with AI SDK RSC in TypeScript\nDESCRIPTION: This snippet illustrates how to use the AI SDK RSC to render React components on the server and stream them to the client. It simplifies the process of managing multiple UI components by handling the rendering on the server side.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/07-rendering-ui-with-language-models.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { createStreamableUI } from 'ai/rsc'\n\nconst uiStream = createStreamableUI();\n\nconst text = generateText({\n  model: openai('gpt-3.5-turbo'),\n  system: 'you are a friendly assistant'\n  prompt: 'what is the weather in SF?'\n  tools: {\n    getWeather: {\n      description: 'Get the weather for a location',\n      parameters: z.object({\n        city: z.string().describe('The city to get the weather for'),\n        unit: z\n          .enum(['C', 'F'])\n          .describe('The unit to display the temperature in')\n      }),\n      execute: async ({ city, unit }) => {\n        const weather = getWeather({ city, unit })\n        const { temperature, unit, description, forecast } = weather\n\n        uiStream.done(\n          <WeatherCard\n            weather={{\n              temperature: 47,\n              unit: 'F',\n              description: 'sunny'\n              forecast,\n            }}\n          />\n        )\n      }\n    }\n  }\n})\n\nreturn {\n  display: uiStream.value\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring a Custom Google Vertex Provider in Edge Runtime using AI SDK\nDESCRIPTION: Demonstrates creating a custom Google Vertex provider for Edge runtimes using `createVertex` from `@ai-sdk/google-vertex/edge`. It allows setting `project`, `location`, and explicit `googleCredentials` (containing `clientEmail` and `privateKey`). These credentials take precedence over environment variables. The resulting custom provider is used with `generateText`.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google-vertex/README.md#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createVertex } from '@ai-sdk/google-vertex/edge';\nimport { generateText } from 'ai';\n\nconst customProvider = createVertex({\n  project: 'your-project-id',\n  location: 'us-central1',\n  googleCredentials: {\n    clientEmail: 'your-client-email',\n    privateKey: 'your-private-key',\n  },\n});\n\nconst { text } = await generateText({\n  model: customProvider('gemini-1.5-flash'),\n  prompt: 'Write a vegetarian lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Images with a DeepInfra Image Model - TypeScript\nDESCRIPTION: Provides an example of image generation using DeepInfra's image model interface within the AI SDK. The experimental_generateImage function is used with a selected model, prompt text, and aspectRatio parameter. The result contains the generated image. Some models support additional image generation options (see further model documentation).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/11-deepinfra.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepinfra } from '@ai-sdk/deepinfra';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: deepinfra.image('stabilityai/sd3.5'),\n  prompt: 'A futuristic cityscape at sunset',\n  aspectRatio: '16:9',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-Side Response Generation with AI SDK RSC\nDESCRIPTION: This server-side code snippet shows how to generate a response using the streamText function from AI SDK RSC. It creates a streamable value to handle the response and uses the OpenAI model for text generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/06-loading-state.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n'use server';\n\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { createStreamableValue } from 'ai/rsc';\n\nexport async function generateResponse(prompt: string) {\n  const stream = createStreamableValue();\n\n  (async () => {\n    const { textStream } = streamText({\n      model: openai('gpt-4o'),\n      prompt,\n    });\n\n    for await (const text of textStream) {\n      stream.update(text);\n    }\n\n    stream.done();\n  })();\n\n  return stream.value;\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Reasoning for Bedrock generateText in TypeScript\nDESCRIPTION: Demonstrates how to enable reasoning for compatible Amazon Bedrock models (like Claude 3.7 Sonnet) using the Vercel AI SDK. It utilizes the `providerOptions.bedrock.reasoningConfig` setting within the `generateText` function, specifying a token budget (min 1024, max 64000), and shows how to access the reasoning output alongside the main text response. Requires `@ai-sdk/amazon-bedrock` and `ai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_14\n\nLANGUAGE: ts\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\nimport { generateText } from 'ai';\n\nconst { text, reasoning, reasoningDetails } = await generateText({\n  model: bedrock('us.anthropic.claude-3-7-sonnet-20250219-v1:0'),\n  prompt: 'How many people will live in the world in 2040?',\n  providerOptions: {\n    bedrock: {\n      reasoningConfig: { type: 'enabled', budgetTokens: 1024 },\n    },\n  },\n});\n\nconsole.log(reasoning); // reasoning text\nconsole.log(reasoningDetails); // reasoning details including redacted reasoning\nconsole.log(text); // text response\n```\n\n----------------------------------------\n\nTITLE: Implementing Route Handler with streamObject (After)\nDESCRIPTION: This refactored code uses a route handler instead of a server action. It creates a POST endpoint that accepts a context, generates notifications using streamObject with openai model, and returns a text stream response that can be consumed by the AI SDK UI hooks.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { notificationSchema } from '@/utils/schemas';\n\nexport async function POST(req: Request) {\n  const context = await req.json();\n\n  const result = streamObject({\n    model: openai('gpt-4-turbo'),\n    schema: notificationSchema,\n    prompt:\n      `Generate 3 notifications for a messages app in this context:` + context,\n  });\n\n  return result.toTextStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Changelog Entry for Version 4.3.9\nDESCRIPTION: Shows the patch change for version 4.3.9, which adds support for detecting id3 tags in the utils/detect-mimetype module.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/CHANGELOG.md#2025-04-23_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n## 4.3.9\n\n### Patch Changes\n\n- b69a253: fix(utils/detect-mimetype): add support for detecting id3 tags\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Object with Zod Schema in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the generateObject function from the AI SDK to generate a structured object (a lasagna recipe) based on a Zod schema.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: yourModel,\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Defining CoreAssistantMessage and AssistantContent Types in TypeScript\nDESCRIPTION: Defines the structure for an assistant message that can contain text, tool calls, or a combination of both. It includes a role of 'assistant' and an AssistantContent type.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/30-core-message.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ntype CoreAssistantMessage = {\n  role: 'assistant';\n  content: AssistantContent;\n};\n\ntype AssistantContent = string | Array<TextPart | ToolCallPart>;\n```\n\n----------------------------------------\n\nTITLE: Returning Data Stream Response Using AI SDK with Uncompressed Header in TypeScript\nDESCRIPTION: This snippet demonstrates how to invoke the AI SDK's response streaming functionality by using the toDataStreamResponse method and setting the 'Content-Encoding' header to 'none'. This header modification disables response compression imposed by intermediaries, allowing streaming to function correctly through proxy middleware. To use, ensure your result object provides the toDataStreamResponse method, and apply the custom headers as shown; the returned response will stream data incrementally to the client instead of buffering the entire payload.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/06-streaming-not-working-when-proxied.mdx#2025-04-23_snippet_0\n\nLANGUAGE: TSX\nCODE:\n```\nreturn result.toDataStreamResponse({\n  headers: {\n    'Content-Encoding': 'none',\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Defining a Weather Tool with TypeScript Inference\nDESCRIPTION: This snippet demonstrates how to use the 'tool' helper function to define a weather tool with proper TypeScript type inference for the execute method parameters.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/20-tool.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { tool } from 'ai';\nimport { z } from 'zod';\n\nexport const weatherTool = tool({\n  description: 'Get the weather in a location',\n  parameters: z.object({\n    location: z.string().describe('The location to get the weather for'),\n  }),\n  // location below is inferred to be a string:\n  execute: async ({ location }) => ({\n    location,\n    temperature: 72 + Math.floor(Math.random() * 21) - 10,\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Fireworks Completion Model\nDESCRIPTION: How to create a model that calls the Fireworks completions API using the .completion() factory method.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/26-fireworks.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = fireworks.completion('accounts/fireworks/models/firefunction-v1');\n```\n\n----------------------------------------\n\nTITLE: Next.js Client Page Implementation\nDESCRIPTION: Client-side page implementation for rendering streamed components from the server action.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/02-streaming-react-components.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { Button } from '@/components/ui/button';\nimport { streamComponent } from './actions';\n\nexport default function Page() {\n  const [component, setComponent] = useState<React.ReactNode>();\n\n  return (\n    <div>\n      <form\n        onSubmit={async e => {\n          e.preventDefault();\n          setComponent(await streamComponent());\n        }}\n      >\n        <Button>Stream Component</Button>\n      </form>\n      <div>{component}</div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Provider Factory with Utility Functions in TypeScript\nDESCRIPTION: This snippet demonstrates how to initialize a custom provider factory using the Vercel AI SDK pattern in TypeScript. Dependencies include '@ai-sdk/provider-utils' for utility functions and local modules for model and settings types. It covers creation of well-typed factory functions, custom settings, header composition, and safe API key handling. The entry point exposes both the generic factory and a 'chat' method, plus a default singleton provider instance. Required parameters include modelId, optional chat settings, and provider-wide settings like baseURL and headers. The provider imposes a constraint that it must not be instantiated with the 'new' keyword.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/01-custom-providers.mdx#2025-04-23_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport {\n  generateId,\n  loadApiKey,\n  withoutTrailingSlash,\n} from '@ai-sdk/provider-utils';\nimport { CustomChatLanguageModel } from './custom-chat-language-model';\nimport { CustomChatModelId, CustomChatSettings } from './custom-chat-settings';\n\n// model factory function with additional methods and properties\nexport interface CustomProvider {\n  (\n    modelId: CustomChatModelId,\n    settings?: CustomChatSettings,\n  ): CustomChatLanguageModel;\n\n  // explicit method for targeting a specific API in case there are several\n  chat(\n    modelId: CustomChatModelId,\n    settings?: CustomChatSettings,\n  ): CustomChatLanguageModel;\n}\n\n// optional settings for the provider\nexport interface CustomProviderSettings {\n  /**\nUse a different URL prefix for API calls, e.g. to use proxy servers.\n   */\n  baseURL?: string;\n\n  /**\nAPI key.\n   */\n  apiKey?: string;\n\n  /**\nCustom headers to include in the requests.\n     */\n  headers?: Record<string, string>;\n}\n\n// provider factory function\nexport function createCustomProvider(\n  options: CustomProviderSettings = {},\n): CustomProvider {\n  const createModel = (\n    modelId: CustomChatModelId,\n    settings: CustomChatSettings = {},\n  ) =>\n    new CustomChatLanguageModel(modelId, settings, {\n      provider: 'custom.chat',\n      baseURL:\n        withoutTrailingSlash(options.baseURL) ?? 'https://custom.ai/api/v1',\n      headers: () => ({\n        Authorization: `Bearer ${loadApiKey({\n          apiKey: options.apiKey,\n          environmentVariableName: 'CUSTOM_API_KEY',\n          description: 'Custom Provider',\n        })}`,\n        ...options.headers,\n      }),\n      generateId: options.generateId ?? generateId,\n    });\n\n  const provider = function (\n    modelId: CustomChatModelId,\n    settings?: CustomChatSettings,\n  ) {\n    if (new.target) {\n      throw new Error(\n        'The model factory function cannot be called with the new keyword.',\n      );\n    }\n\n    return createModel(modelId, settings);\n  };\n\n  provider.chat = createModel;\n\n  return provider;\n}\n\n/**\n * Default custom provider instance.\n */\nexport const customProvider = createCustomProvider();\n```\n\n----------------------------------------\n\nTITLE: Implementing API Route Handler for Object Generation\nDESCRIPTION: Server-side API route handler that uses the generateObject function with OpenAI's GPT-4 model to generate structured notification data based on a zod schema.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/30-generate-object.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nexport async function POST(req: Request) {\n  const { prompt }: { prompt: string } = await req.json();\n\n  const result = await generateObject({\n    model: openai('gpt-4'),\n    system: 'You generate three notifications for a messages app.',\n    prompt,\n    schema: z.object({\n      notifications: z.array(\n        z.object({\n          name: z.string().describe('Name of a fictional person.'),\n          message: z.string().describe('Do not use emojis or links.'),\n          minutesAgo: z.number(),\n        }),\n      ),\n    }),\n  });\n\n  return result.toJsonResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Type Helpers for AI SDK Tools\nDESCRIPTION: Shows how to use type helpers for tool calls and results, demonstrating type inference from ToolSet definitions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { ToolCallUnion, ToolResultUnion, generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst myToolSet = {\n  firstTool: tool({\n    description: 'Greets the user',\n    parameters: z.object({ name: z.string() }),\n    execute: async ({ name }) => `Hello, ${name}!`,\n  }),\n  secondTool: tool({\n    description: 'Tells the user their age',\n    parameters: z.object({ age: z.number() }),\n    execute: async ({ age }) => `You are ${age} years old!`,\n  }),\n};\n\ntype MyToolCall = ToolCallUnion<typeof myToolSet>;\ntype MyToolResult = ToolResultUnion<typeof myToolSet>;\n\nasync function generateSomething(prompt: string): Promise<{\n  text: string;\n  toolCalls: Array<MyToolCall>; // typed tool calls\n  toolResults: Array<MyToolResult>; // typed tool results\n}> {\n  return generateText({\n    model: openai('gpt-4o'),\n    tools: myToolSet,\n    prompt,\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Text with Image Prompt using AI SDK and Anthropic's Claude Model in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the AI SDK to stream text responses based on an image input. It utilizes Anthropic's Claude model for multimodal processing, reading an image file and sending it along with a text prompt to generate a detailed description.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/22-stream-text-with-image-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { streamText } from 'ai';\nimport 'dotenv/config';\nimport fs from 'node:fs';\n\nasync function main() {\n  const result = streamText({\n    model: anthropic('claude-3-5-sonnet-20240620'),\n    messages: [\n      {\n        role: 'user',\n        content: [\n          { type: 'text', text: 'Describe the image in detail.' },\n          { type: 'image', image: fs.readFileSync('./data/comic-cat.png') },\n        ],\n      },\n    ],\n  });\n\n  for await (const textPart of result.textStream) {\n    process.stdout.write(textPart);\n  }\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Importing the Google Vertex Anthropic Provider for Edge Runtimes in TypeScript\nDESCRIPTION: Shows how to import the specific version of the Google Vertex Anthropic provider designed for Edge runtimes (like Vercel Edge Functions or Cloudflare Workers). The import path is `@ai-sdk/google-vertex/anthropic/edge`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_22\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic/edge';\n```\n\n----------------------------------------\n\nTITLE: Creating Customized LMNT Provider Instance in TypeScript\nDESCRIPTION: How to create a custom LMNT provider instance with specific settings using the createLMNT factory function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/140-lmnt.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createLMNT } from '@ai-sdk/lmnt';\n\nconst lmnt = createLMNT({\n  // custom settings, e.g.\n  fetch: customFetch,\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Crosshatch Provider via npm - Shell\nDESCRIPTION: Installs the '@crosshatch/ai-provider' NPM package using npm, the default Node.js package manager. This enables use of Crosshatch as a provider in your AI SDK workflow. Requires npm to be installed and run from your project directory.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/21-crosshatch.mdx#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @crosshatch/ai-provider\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-Side Object Streaming with OpenAI\nDESCRIPTION: This server-side route uses the streamObject function to generate and stream objects. It uses OpenAI's GPT-4 model to generate notifications based on a given context.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/40-stream-object.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { notificationSchema } from './schema';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const context = await req.json();\n\n  const result = streamObject({\n    model: openai('gpt-4-turbo'),\n    schema: notificationSchema,\n    prompt:\n      `Generate 3 notifications for a messages app in this context:` + context,\n  });\n\n  return result.toTextStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Rendering Streamed UI Components in React TypeScript\nDESCRIPTION: This snippet shows how to render the UI components streamed from the server using the AI SDK RSC. It simplifies the client-side code by directly rendering the streamed UI.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/07-rendering-ui-with-language-models.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\nreturn (\n  <div>\n    {messages.map(message => (\n      <div>{message.display}</div>\n    ))}\n  </div>\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Weather Component in TypeScript React\nDESCRIPTION: This code defines a React component to display weather information. It takes temperature, weather condition, and location as props and renders them in a structured format.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\ntype WeatherProps = {\n  temperature: number;\n  weather: string;\n  location: string;\n};\n\nexport const Weather = ({ temperature, weather, location }: WeatherProps) => {\n  return (\n    <div>\n      <h2>Current Weather for {location}</h2>\n      <p>Condition: {weather}</p>\n      <p>Temperature: {temperature}Â°C</p>\n    </div>\n  );\n};\n```\n\n----------------------------------------\n\nTITLE: Using Dynamic Text Prompts with Template Literals in TypeScript\nDESCRIPTION: This snippet demonstrates how to incorporate dynamic variables into a text prompt using template literals for customized content generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  prompt:\n    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +\n    `Please suggest the best tourist activities for me to do.`,\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Interface with Tool Calling in React\nDESCRIPTION: This code snippet shows how to create a React component that uses the useChat hook from the AI SDK to implement a chat interface with tool calling capabilities. It sets up the chat UI and handles user input.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/70-call-tools.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, setInput, append } = useChat({\n    api: '/api/chat',\n    maxSteps: 2,\n  });\n\n  return (\n    <div>\n      <input\n        value={input}\n        onChange={event => {\n          setInput(event.target.value);\n        }}\n        onKeyDown={async event => {\n          if (event.key === 'Enter') {\n            append({ content: input, role: 'user' });\n          }\n        }}\n      />\n\n      {messages.map((message, index) => (\n        <div key={index}>{message.content}</div>\n      ))}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring a Custom Google Vertex Anthropic Provider in Edge Runtime using AI SDK\nDESCRIPTION: Shows how to create a custom Google Vertex Anthropic provider for Edge runtimes using `createVertexAnthropic` from `@ai-sdk/google-vertex/anthropic/edge`. This allows specifying configuration options such as `project` and `location`. The custom provider is then used with `generateText` to select the Anthropic model.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google-vertex/README.md#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic/edge';\nimport { generateText } from 'ai';\n\nconst customProvider = createVertexAnthropic({\n  project: 'your-project-id',\n  location: 'us-east5',\n});\n\nconst { text } = await generateText({\n  model: customProvider('claude-3-5-sonnet@20240620'),\n  prompt: 'Write a vegetarian lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Customized OpenAI Provider Instance in TypeScript\nDESCRIPTION: Illustrates how to import the `createOpenAI` factory function and use it to create a customized OpenAI provider instance. This allows setting specific configurations like `baseURL`, `apiKey`, `compatibility` mode ('strict' recommended for OpenAI API), or custom headers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst openai = createOpenAI({\n  // custom settings, e.g.\n  compatibility: 'strict', // strict mode, enable when using the OpenAI API\n});\n```\n\n----------------------------------------\n\nTITLE: Using Image Generation Models from Provider Registry\nDESCRIPTION: Shows how to generate images using an image model accessed through the registry using the provider:model ID format.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/40-provider-registry.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateImage } from 'ai';\nimport { registry } from './registry';\n\nconst { image } = await generateImage({\n  model: registry.imageModel('openai:dall-e-3'),\n  prompt: 'A beautiful sunset over a calm ocean',\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Memories to Enhance Context - TypeScript\nDESCRIPTION: Demonstrates how to add user-specific memories to the AI context, improving conversational continuity. Requires '@mem0/vercel-ai-provider' and 'ai' installed, as well as a valid messages array. The 'addMemories' function takes messages formatted as 'LanguageModelV1Prompt' and user identification; it stores new memory for the user. Inputs: formatted message array and options (e.g., user_id). Output: the function returns a promise indicating completion.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/70-mem0.mdx#2025-04-23_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { LanguageModelV1Prompt } from 'ai';\nimport { addMemories } from '@mem0/vercel-ai-provider';\n\nconst messages: LanguageModelV1Prompt = [\n  { role: 'user', content: [{ type: 'text', text: 'I love red cars.' }] },\n];\n\nawait addMemories(messages, { user_id: 'borat' });\n```\n\n----------------------------------------\n\nTITLE: Reading Streamable Value on Client-Side in React\nDESCRIPTION: Shows how to read a streamable value on the client-side using readStreamableValue. The code demonstrates using an async iterator to log the streamed values as they are updated.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/05-streaming-values.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { readStreamableValue } from 'ai/rsc';\nimport { runThread } from '@/actions';\n\nexport default function Page() {\n  return (\n    <button\n      onClick={async () => {\n        const { status } = await runThread();\n\n        for await (const value of readStreamableValue(status)) {\n          console.log(value);\n        }\n      }}\n    >\n      Ask\n    </button>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Streamable Value with Server Action in TypeScript\nDESCRIPTION: Demonstrates how to create a streamable value using createStreamableValue in a server action. The function updates the streamable value over time to represent different stages of a thread's execution.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/05-streaming-values.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { createStreamableValue } from 'ai/rsc';\n\nexport const runThread = async () => {\n  const streamableStatus = createStreamableValue('thread.init');\n\n  setTimeout(() => {\n    streamableStatus.update('thread.run.create');\n    streamableStatus.update('thread.run.update');\n    streamableStatus.update('thread.run.end');\n    streamableStatus.done('thread.end');\n  }, 1000);\n\n  return {\n    status: streamableStatus.value,\n  };\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Headers and Body in useChat Hook\nDESCRIPTION: Demonstrates how to customize API requests with headers, body fields, and credentials using the useChat hook.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_9\n\nLANGUAGE: tsx\nCODE:\n```\nconst { messages, input, handleInputChange, handleSubmit } = useChat({\n  api: '/api/custom-chat',\n  headers: {\n    Authorization: 'your_token',\n  },\n  body: {\n    user_id: '123',\n  },\n  credentials: 'same-origin',\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Array of Objects with OpenAI in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the streamObject function with the 'array' output strategy to generate and stream an array of hero descriptions for a fantasy game.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { z } from 'zod';\n\nconst { elementStream } = streamObject({\n  model: openai('gpt-4-turbo'),\n  output: 'array',\n  schema: z.object({\n    name: z.string(),\n    class: z\n      .string()\n      .describe('Character class, e.g. warrior, mage, or thief.'),\n    description: z.string(),\n  }),\n  prompt: 'Generate 3 hero descriptions for a fantasy role playing game.',\n});\n\nfor await (const hero of elementStream) {\n  console.log(hero);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AI Binding in wrangler.toml - Bash\nDESCRIPTION: This bash snippet configures an AI binding in the wrangler.toml file for Cloudflare Workers. It establishes a binding named 'AI' for use in project code. No dependencies beyond Cloudflare Wrangler are necessary. Ensure the file is placed in your worker project root.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/11-cloudflare-workers-ai.mdx#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n[ai]\nbinding = \"AI\"\n```\n\n----------------------------------------\n\nTITLE: Streaming Loading State from Server with AI SDK RSC\nDESCRIPTION: This server-side code demonstrates how to stream loading state from the server using AI SDK RSC. It creates separate streamable values for the response and loading state, allowing for more granular tracking of the generation process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/06-loading-state.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n'use server';\n\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { createStreamableValue } from 'ai/rsc';\n\nexport async function generateResponse(prompt: string) {\n  const stream = createStreamableValue();\n  const loadingState = createStreamableValue({ loading: true });\n\n  (async () => {\n    const { textStream } = streamText({\n      model: openai('gpt-4o'),\n      prompt,\n    });\n\n    for await (const text of textStream) {\n      stream.update(text);\n    }\n\n    stream.done();\n    loadingState.done({ loading: false });\n  })();\n\n  return { response: stream.value, loadingState: loadingState.value };\n}\n```\n\n----------------------------------------\n\nTITLE: Replacing ExperimentalTool Type with CoreTool - TypeScript\nDESCRIPTION: Shows the change from the removed ExperimentalTool type to CoreTool in the ai package. Update all import statements and usages and ensure downstream type annotations are updated. Input: none. Output: import of CoreTool type.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_16\n\nLANGUAGE: ts\nCODE:\n```\nimport { ExperimentalTool } from 'ai';\n```\n\nLANGUAGE: ts\nCODE:\n```\nimport { CoreTool } from 'ai';\n```\n\n----------------------------------------\n\nTITLE: Basic StreamUI Implementation in TypeScript\nDESCRIPTION: Simple example of using streamUI function with OpenAI's GPT-4 model to stream basic text responses as React components.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/02-streaming-react-components.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nconst result = await streamUI({\n  model: openai('gpt-4o'),\n  prompt: 'Get the weather for San Francisco',\n  text: ({ content }) => <div>{content}</div>,\n  tools: {},\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Routing Pattern for Customer Query Classification and Response\nDESCRIPTION: This code demonstrates a routing pattern that first classifies a customer query by type and complexity, then routes it to the appropriate model with specialized system prompts. It adaptively selects different OpenAI models based on the query's complexity.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject, generateText } from 'ai';\nimport { z } from 'zod';\n\nasync function handleCustomerQuery(query: string) {\n  const model = openai('gpt-4o');\n\n  // First step: Classify the query type\n  const { object: classification } = await generateObject({\n    model,\n    schema: z.object({\n      reasoning: z.string(),\n      type: z.enum(['general', 'refund', 'technical']),\n      complexity: z.enum(['simple', 'complex']),\n    }),\n    prompt: `Classify this customer query:\n    ${query}\n\n    Determine:\n    1. Query type (general, refund, or technical)\n    2. Complexity (simple or complex)\n    3. Brief reasoning for classification`,\n  });\n\n  // Route based on classification\n  // Set model and system prompt based on query type and complexity\n  const { text: response } = await generateText({\n    model:\n      classification.complexity === 'simple'\n        ? openai('gpt-4o-mini')\n        : openai('o3-mini'),\n    system: {\n      general:\n        'You are an expert customer service agent handling general inquiries.',\n      refund:\n        'You are a customer service agent specializing in refund requests. Follow company policy and collect necessary information.',\n      technical:\n        'You are a technical support specialist with deep product knowledge. Focus on clear step-by-step troubleshooting.',\n    }[classification.type],\n    prompt: query,\n  });\n\n  return { response, classification };\n}\n```\n\n----------------------------------------\n\nTITLE: Using File Inputs with Google Generative AI\nDESCRIPTION: Demonstrates how to use file inputs (such as PDF files) with Google Generative AI models for document analysis and question answering.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: google('gemini-1.5-flash'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'What is an embedding model according to this document?',\n        },\n        {\n          type: 'file',\n          data: fs.readFileSync('./data/ai.pdf'),\n          mimeType: 'application/pdf',\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Version Entry 1.2.10\nDESCRIPTION: Changelog entry documenting dependency updates to provider packages\nSOURCE: https://github.com/vercel/ai/blob/main/packages/anthropic/CHANGELOG.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## 1.2.10\n\n### Patch Changes\n\n- Updated dependencies [beef951]\n  - @ai-sdk/provider@1.1.3\n  - @ai-sdk/provider-utils@2.2.7\n```\n\n----------------------------------------\n\nTITLE: Migrating ExperimentalMessage Type Imports to CoreMessage - TypeScript\nDESCRIPTION: Updates import statements to replace deprecated experimental types (ExperimentalMessage, ExperimentalUserMessage, etc.) with their new Core-prefixed counterparts. All code using these types should replace references accordingly. Input: none; output: code using Core* types.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_15\n\nLANGUAGE: ts\nCODE:\n```\nimport {\n  ExperimentalMessage,\n  ExperimentalUserMessage,\n  ExperimentalAssistantMessage,\n  ExperimentalToolMessage,\n} from 'ai';\n```\n\nLANGUAGE: ts\nCODE:\n```\nimport {\n  CoreMessage,\n  CoreUserMessage,\n  CoreAssistantMessage,\n  CoreToolMessage,\n} from 'ai';\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Interface with AI SDK in React\nDESCRIPTION: This code snippet shows the client-side implementation of a chat interface using React and AI SDK. It manages the conversation state, handles user input, and renders messages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/90-render-visual-interface-in-chat.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { ClientMessage } from './actions';\nimport { useActions, useUIState } from 'ai/rsc';\nimport { generateId } from 'ai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [input, setInput] = useState<string>('');\n  const [conversation, setConversation] = useUIState();\n  const { continueConversation } = useActions();\n\n  return (\n    <div>\n      <div>\n        {conversation.map((message: ClientMessage) => (\n          <div key={message.id}>\n            {message.role}: {message.display}\n          </div>\n        ))}\n      </div>\n\n      <div>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button\n          onClick={async () => {\n            setConversation((currentConversation: ClientMessage[]) => [\n              ...currentConversation,\n              { id: generateId(), role: 'user', display: input },\n            ]);\n\n            const message = await continueConversation(input);\n\n            setConversation((currentConversation: ClientMessage[]) => [\n              ...currentConversation,\n              message,\n            ]);\n          }}\n        >\n          Send Message\n        </button>\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Chat Route Handler with DeepSeek R1\nDESCRIPTION: This code creates a route handler for a chat endpoint using Next.js and AI SDK. It imports the DeepSeek provider and uses the streamText function to generate responses. The handler also forwards reasoning tokens to the client.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/25-r1.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepseek } from '@ai-sdk/deepseek';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: deepseek('deepseek-reasoner'),\n    messages,\n  });\n\n  return result.toDataStreamResponse({\n    sendReasoning: true,\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Cancellation Issues with Eager Streams in JavaScript\nDESCRIPTION: This snippet illustrates the cancellation problem with eager stream generation. It shows how the stream continues to produce values even after the consumer has stopped reading, potentially leading to memory issues.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/03-backpressure.mdx#2025-04-23_snippet_2\n\nLANGUAGE: jsx\nCODE:\n```\n// A generator that will yield positive integers\nasync function* integers() {\n  let i = 1;\n  while (true) {\n    console.log(`yielding ${i}`);\n    yield i++;\n\n    await sleep(100);\n  }\n}\nfunction sleep(ms) {\n  return new Promise(resolve => setTimeout(resolve, ms));\n}\n\n// Wraps a generator into a ReadableStream\nfunction createStream(iterator) {\n  return new ReadableStream({\n    async start(controller) {\n      for await (const v of iterator) {\n        controller.enqueue(v);\n      }\n      controller.close();\n    },\n  });\n}\n// Collect data from stream\nasync function run() {\n  // Set up a stream that of integers\n  const stream = createStream(integers());\n\n  // Read values from our stream\n  const reader = stream.getReader();\n  // We're only reading 3 items this time:\n  for (let i = 0; i < 3; i++) {\n    // we know our stream is infinite, so there's no need to check `done`.\n    const { value } = await reader.read();\n    console.log(`read ${value}`);\n\n    await sleep(1000);\n  }\n}\nrun();\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side No-Schema Streaming in React\nDESCRIPTION: This React component shows how to use the useObject hook without a predefined schema. It uses z.unknown() to handle any type of data structure returned by the API.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/40-stream-object.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\nimport { z } from 'zod';\n\nexport default function Page() {\n  const { object, submit, isLoading, stop } = useObject({\n    api: '/api/use-object',\n    schema: z.unknown(),\n  });\n\n  return (\n    <div>\n      <button\n        onClick={() => submit('Messages during finals week.')}\n        disabled={isLoading}\n      >\n        Generate notifications\n      </button>\n\n      {isLoading && (\n        <div>\n          <div>Loading...</div>\n          <button type=\"button\" onClick={() => stop()}>\n            Stop\n          </button>\n        </div>\n      )}\n\n      {object?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification.name}</p>\n          <p>{notification.message}</p>\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Image Detail Level for Vision Models\nDESCRIPTION: Demonstrates how to specify the desired detail level (`low`, `high`, or `auto`) for image analysis when using OpenAI vision models like 'gpt-4o'. This is configured within the `providerOptions.openai` object associated with the image content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai'; // Assumed import\nimport { generateText } from 'ai'; // Assumed import\n\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'Describe the image in detail.' },\n        {\n          type: 'image',\n          image:\n            'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',\n\n          // OpenAI specific options - image detail:\n          providerOptions: {\n            openai: { imageDetail: 'low' },\n          },\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Using Character Reference for Consistent Generations (TypeScript)\nDESCRIPTION: Presents the use of 'character_ref' to enforce subject consistency in generated images. Up to four images of the same character can be included via 'providerOptions.luma.character_ref', enhancing personalized character continuity. Each identity entry specifies a unique identity ID and a list of image URLs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/80-luma.mdx#2025-04-23_snippet_9\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Example: Generate character-based image\nawait generateImage({\n  model: luma.image('photon-1'),\n  prompt: 'A woman with a cat riding a broomstick in a forest',\n  providerOptions: {\n    luma: {\n      character_ref: {\n        identity0: {\n          images: ['https://example.com/character.jpg'],\n        },\n      },\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Defining OnStepFinishResult Interface in TypeScript\nDESCRIPTION: This code snippet defines the structure of the OnStepFinishResult interface used in the AI SDK. It includes properties such as stepType, finishReason, usage, text, toolCalls, toolResults, warnings, response, isContinued, and providerMetadata. Each property is documented with its type and description.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_31\n\nLANGUAGE: TypeScript\nCODE:\n```\ninterface OnStepFinishResult {\n  stepType: \"initial\" | \"continue\" | \"tool-result\";\n  finishReason: \"stop\" | \"length\" | \"content-filter\" | \"tool-calls\" | \"error\" | \"other\" | \"unknown\";\n  usage: TokenUsage;\n  text: string;\n  toolCalls: ToolCall[];\n  toolResults: ToolResult[];\n  warnings?: Warning[];\n  response?: Response;\n  isContinued: boolean;\n  providerMetadata?: Record<string, Record<string, JSONValue>>;\n}\n\ninterface TokenUsage {\n  promptTokens: number;\n  completionTokens: number;\n  totalTokens: number;\n}\n\ninterface Response {\n  id: string;\n  model: string;\n  timestamp: Date;\n  headers?: Record<string, string>;\n  body?: unknown;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining the AI Generation Result Object Structure in TypeScript\nDESCRIPTION: This snippet outlines the structure of an object returned by an AI generation process. It details properties like `reasoningSteps` (including type and redacted data), `sources` used (URLs, IDs, metadata), `files` generated (base64, Uint8Array, MIME type), `toolCalls` and `toolResults`, `finishReason`, token `usage`, `request` and `response` metadata, `warnings`, `isContinued` flag, and `providerMetadata`. This structure provides comprehensive information about the AI operation's outcome.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_33\n\nLANGUAGE: typescript\nCODE:\n```\n                      type: \"'redacted'\",\n                      description: 'The type of the reasoning detail.',\n                    },\n                    {\n                      name: 'data',\n                      type: 'string',\n                      description:\n                        'The redacted data content (only for type \"redacted\").',\n                    },\n                  ],\n                },\n              ],\n            },\n            {\n              name: 'sources',\n              type: 'Array<Source>',\n              description:\n                'Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.',\n              properties: [\n                {\n                  type: 'Source',\n                  parameters: [\n                    {\n                      name: 'sourceType',\n                      type: \"'url'\",\n                      description:\n                        'A URL source. This is return by web search RAG models.',\n                    },\n                    {\n                      name: 'id',\n                      type: 'string',\n                      description: 'The ID of the source.',\n                    },\n                    {\n                      name: 'url',\n                      type: 'string',\n                      description: 'The URL of the source.',\n                    },\n                    {\n                      name: 'title',\n                      type: 'string',\n                      isOptional: true,\n                      description: 'The title of the source.',\n                    },\n                    {\n                      name: 'providerMetadata',\n                      type: 'LanguageModelV1ProviderMetadata',\n                      isOptional: true,\n                      description:\n                        'Additional provider metadata for the source.',\n                    },\n                  ],\n                },\n              ],\n            },\n            {\n              name: 'files',\n              type: 'Array<GeneratedFile>',\n              description: 'Files that were generated in this step.',\n              properties: [\n                {\n                  type: 'GeneratedFile',\n                  parameters: [\n                    {\n                      name: 'base64',\n                      type: 'string',\n                      description: 'File as a base64 encoded string.',\n                    },\n                    {\n                      name: 'uint8Array',\n                      type: 'Uint8Array',\n                      description: 'File as a Uint8Array.',\n                    },\n                    {\n                      name: 'mimeType',\n                      type: 'string',\n                      description: 'MIME type of the file.',\n                    },\n                  ],\n                },\n              ],\n            },\n            {\n              name: 'toolCalls',\n              type: 'array',\n              description: 'A list of tool calls made by the model.',\n            },\n            {\n              name: 'toolResults',\n              type: 'array',\n              description:\n                'A list of tool results returned as responses to earlier tool calls.',\n            },\n            {\n              name: 'finishReason',\n              type: \"'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'\",\n              description: 'The reason the model finished generating the text.',\n            },\n            {\n              name: 'usage',\n              type: 'CompletionTokenUsage',\n              description: 'The token usage of the generated text.',\n              properties: [\n                {\n                  type: 'CompletionTokenUsage',\n                  parameters: [\n                    {\n                      name: 'promptTokens',\n                      type: 'number',\n                      description: 'The total number of tokens in the prompt.',\n                    },\n                    {\n                      name: 'completionTokens',\n                      type: 'number',\n                      description:\n                        'The total number of tokens in the completion.',\n                    },\n                    {\n                      name: 'totalTokens',\n                      type: 'number',\n                      description: 'The total number of tokens generated.',\n                    },\n                  ],\n                },\n              ],\n            },\n            {\n              name: 'request',\n              type: 'RequestMetadata',\n              isOptional: true,\n              description: 'Request metadata.',\n              properties: [\n                {\n                  type: 'RequestMetadata',\n                  parameters: [\n                    {\n                      name: 'body',\n                      type: 'string',\n                      description:\n                        'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).',\n                    },\n                  ],\n                },\n              ],\n            },\n            {\n              name: 'response',\n              type: 'ResponseMetadata',\n              isOptional: true,\n              description: 'Response metadata.',\n              properties: [\n                {\n                  type: 'ResponseMetadata',\n                  parameters: [\n                    {\n                      name: 'id',\n                      type: 'string',\n                      description:\n                        'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.',\n                    },\n                    {\n                      name: 'model',\n                      type: 'string',\n                      description:\n                        'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.',\n                    },\n                    {\n                      name: 'timestamp',\n                      type: 'Date',\n                      description:\n                        'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.',\n                    },\n                    {\n                      name: 'headers',\n                      isOptional: true,\n                      type: 'Record<string, string>',\n                      description: 'Optional response headers.',\n                    },\n                    {\n                      name: 'body',\n                      isOptional: true,\n                      type: 'unknown',\n                      description: 'Optional response body.',\n                    },\n                    {\n                      name: 'messages',\n                      type: 'Array<ResponseMessage>',\n                      description:\n                        'The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls.  When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.',\n                    },\n                  ],\n                },\n              ],\n            },\n            {\n              name: 'warnings',\n              type: 'Warning[] | undefined',\n              description:\n                'Warnings from the model provider (e.g. unsupported settings).',\n            },\n            {\n              name: 'isContinued',\n              type: 'boolean',\n              description:\n                'True when there will be a continuation step with a continuation text.',\n            },\n            {\n              name: 'providerMetadata',\n              type: 'Record<string,Record<string,JSONValue>> | undefined',\n              isOptional: true,\n              description:\n                'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.',\n            },\n          ],\n        },\n      ],\n    },\n```\n\n----------------------------------------\n\nTITLE: Using Language Models with Provider Registry\nDESCRIPTION: Demonstrates how to use language models through the provider registry with model ID prefixes.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/45-provider-management.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { registry } from './registry';\n\nconst { text } = await generateText({\n  model: registry.languageModel('openai:gpt-4-turbo'), // default separator\n  // or with custom separator:\n  // model: customSeparatorRegistry.languageModel('openai > gpt-4-turbo'),\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing the Anthropic Text Editor Tool (TypeScript)\nDESCRIPTION: This code snippet demonstrates creating a Text Editor Tool using the vertexAnthropic provider's tools API, passing an asynchronous execute function which handles multiple text editor commands (such as 'view', 'create', 'str_replace', 'insert', 'undo_edit'). The function receives parameters relevant to file manipulation and should return the operation result. Custom implementation is required for the execution logic, allowing use cases such as file viewing, editing, and replacement.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_30\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst textEditorTool = vertexAnthropic.tools.textEditor_20241022({\n  execute: async ({\n    command,\n    path,\n    file_text,\n    insert_line,\n    new_str,\n    old_str,\n    view_range,\n  }) => {\n    // Implement your text editing logic here\n    // Return the result of the text editing operation\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with xAI Model\nDESCRIPTION: This example demonstrates generating text using an xAI language model and the `generateText` function from the 'ai' module. It requires importing `xai` and `generateText`.  The `prompt` parameter provides the input text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_6\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { xai } from '@ai-sdk/xai';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: xai('grok-3'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing Run Name in LangSmith Tracing\nDESCRIPTION: Example of customizing the run name when using AISDKExporter for tracing.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AISDKExporter } from 'langsmith/vercel';\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nawait generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  experimental_telemetry: AISDKExporter.getSettings({\n    runName: 'my-custom-run-name',\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Inspecting AI SDK Warnings in TypeScript\nDESCRIPTION: Shows how to check for warnings when using AI SDK features that might not be supported by all providers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/20-prompt-engineering.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  prompt: 'Hello, world!',\n});\n\nconsole.log(result.warnings);\n```\n\n----------------------------------------\n\nTITLE: Migrating Tool Result Method in useChat Hook\nDESCRIPTION: Shows the change from experimental_addToolResult to addToolResult method in the useChat hook.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_38\n\nLANGUAGE: typescript\nCODE:\n```\nconst { messages, experimental_addToolResult } = useChat({\n  // ...\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst { messages, addToolResult } = useChat({\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Server Action Creating Streamable Value\nDESCRIPTION: Demonstrates creating and updating a streamable value on the server using server actions. Shows how to update the value multiple times and complete it with a final value.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/05-read-streamable-value.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync function generate() {\n  'use server';\n  const streamable = createStreamableValue();\n\n  streamable.update(1);\n  streamable.update(2);\n  streamable.done(3);\n\n  return streamable.value;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Streaming with OpenAI GPT-4 using AI SDK\nDESCRIPTION: This snippet demonstrates how to stream text generation from OpenAI's gpt-4-turbo model using the AI SDK's streamText function. It initializes the stream with a model and prompt, then iterates through the text stream parts as they become available, logging each part to the console.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/05-streaming.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst { textStream } = streamText({\n  model: openai('gpt-4-turbo'),\n  prompt: 'Write a poem about embedding models.',\n});\n\nfor await (const textPart of textStream) {\n  console.log(textPart);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AI Actions for React Server Components\nDESCRIPTION: This file sets up the AI configuration for React Server Components. It imports the submitMessage action and creates an AI instance with initial states for AI and UI.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/121-stream-assistant-response-with-tools.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAI } from 'ai/rsc';\nimport { submitMessage } from './actions';\n\nexport const AI = createAI({\n  actions: {\n    submitMessage,\n  },\n  initialAIState: [],\n  initialUIState: [],\n});\n```\n\n----------------------------------------\n\nTITLE: Reading UI State in Client Component with useUIState\nDESCRIPTION: This code demonstrates how to access and render the UI state in a client-side component using the useUIState hook provided by ai/rsc.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useUIState } from 'ai/rsc';\n\nexport default function Page() {\n  const [messages, setMessages] = useUIState();\n\n  return (\n    <ul>\n      {messages.map(message => (\n        <li key={message.id}>{message.display}</li>\n      ))}\n    </ul>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Renaming formatStreamPart to formatDataStreamPart in TypeScript\nDESCRIPTION: Shows the renaming of the `formatStreamPart` function to `formatDataStreamPart` in AI SDK 4.0. The function's purpose, formatting a part of the data stream, remains the same.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\nformatStreamPart('text', 'Hello, world!');\n```\n\nLANGUAGE: typescript\nCODE:\n```\nformatDataStreamPart('text', 'Hello, world!');\n```\n\n----------------------------------------\n\nTITLE: Implementing Smooth Streaming of Japanese Text with AI SDK in React\nDESCRIPTION: This code snippet shows how to use the 'smoothStream' function from the 'ai' package along with the 'useChat' hook from '@ai-sdk/react' to achieve smooth streaming of Japanese text. It utilizes a regex pattern that splits on either words or Japanese characters for chunking.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/22-smooth-stream-japanese.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { smoothStream } from 'ai';\nimport { useChat } from '@ai-sdk/react';\n\nconst { data } = useChat({\n  experimental_transform: smoothStream({\n    chunking: /[\\u3040-\\u309F\\u30A0-\\u30FF]|\\S+\\s+/,\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Creating React Component for Multi-Step Tool Calling with useChat Hook\nDESCRIPTION: This snippet shows how to create a React component using the useChat hook from the AI SDK. It sets up a chat interface that can handle multiple tool calls in sequence, with a maximum of 5 steps.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/72-call-tools-multiple-steps.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, setInput, append } = useChat({\n    api: '/api/chat',\n    maxSteps: 5,\n  });\n\n  return (\n    <div>\n      <input\n        value={input}\n        onChange={event => {\n          setInput(event.target.value);\n        }}\n        onKeyDown={async event => {\n          if (event.key === 'Enter') {\n            append({ content: input, role: 'user' });\n          }\n        }}\n      />\n\n      {messages.map((message, index) => (\n        <div key={index}>{message.content}</div>\n      ))}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side UI with React for Streaming AI Responses\nDESCRIPTION: This code creates a client-side React component that manages a conversation interface with state management via useUIState and useActions hooks from the AI SDK. It allows users to send messages and displays both user inputs and AI responses as they stream in.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/91-stream-updates-to-visual-interfaces.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { ClientMessage } from './actions';\nimport { useActions, useUIState } from 'ai/rsc';\nimport { generateId } from 'ai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [input, setInput] = useState<string>('');\n  const [conversation, setConversation] = useUIState();\n  const { continueConversation } = useActions();\n\n  return (\n    <div>\n      <div>\n        {conversation.map((message: ClientMessage) => (\n          <div key={message.id}>\n            {message.role}: {message.display}\n          </div>\n        ))}\n      </div>\n\n      <div>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button\n          onClick={async () => {\n            setConversation((currentConversation: ClientMessage[]) => [\n              ...currentConversation,\n              { id: generateId(), role: 'user', display: input },\n            ]);\n\n            const message = await continueConversation(input);\n\n            setConversation((currentConversation: ClientMessage[]) => [\n              ...currentConversation,\n              message,\n            ]);\n          }}\n        >\n          Send Message\n        </button>\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Wrapping Language Model with Middleware in TypeScript\nDESCRIPTION: Example showing how to use wrapLanguageModel to enhance a language model with middleware. The function takes a model and middleware parameters to create a wrapped model instance with additional functionality.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/60-wrap-language-model.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { wrapLanguageModel } from 'ai';\n\nconst wrappedLanguageModel = wrapLanguageModel({\n  model: yourModel,\n  middleware: yourLanguageModelMiddleware,\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with a DeepInfra Language Model - TypeScript\nDESCRIPTION: Shows how to use a DeepInfra provider instance to specify a model and perform text generation via the generateText function from the AI SDK. Requires prior setup of the provider and installation of the ai package. Parameters include the model name (as DeepInfra model ID) and a custom prompt. The output is destructured to access the generated text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/11-deepinfra.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepinfra } from '@ai-sdk/deepinfra';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Using FriendliAI Built-in Tools with AI SDK\nDESCRIPTION: Shows how to use FriendliAI's built-in tools (e.g., 'web:search', 'math:calculator') with a compatible model by passing them in the `tools` array during model instantiation. The example uses `streamText` to process the potentially tool-enhanced output. Dependencies: `@friendliai/ai-provider`, `ai`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { friendli } from '@friendliai/ai-provider';\nimport { streamText } from 'ai';\n\nconst result = streamText({\n  model: friendli('meta-llama-3.3-70b-instruct', {\n    tools: [{ type: 'web:search' }, { type: 'math:calculator' }],\n  }),\n  prompt:\n    'Find the current USD to CAD exchange rate and calculate how much $5,000 USD would be in Canadian dollars.',\n});\n\nfor await (const textPart of result.textStream) {\n  console.log(textPart);\n}\n```\n\n----------------------------------------\n\nTITLE: Importing createStreamableUI for AI SDK RSC (TypeScript/JavaScript)\nDESCRIPTION: Imports the `createStreamableUI` function from the `ai/rsc` module. This function is the entry point for creating a streamable UI instance, which allows React components (UI nodes) generated on the server to be progressively streamed and rendered on the client.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/03-create-streamable-ui.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createStreamableUI } from \"ai/rsc\"\n```\n\n----------------------------------------\n\nTITLE: Implementing toDataStream with Fastify and OpenAI\nDESCRIPTION: This example shows how to use the toDataStream method to stream AI-generated text from OpenAI's gpt-4o model through a Fastify server. It sets proper headers for the Vercel AI data stream protocol.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/40-fastify.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport Fastify from 'fastify';\n\nconst fastify = Fastify({ logger: true });\n\nfastify.post('/', async function (request, reply) {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    prompt: 'Invent a new holiday and describe its traditions.',\n  });\n\n  // Mark the response as a v1 data stream:\n  reply.header('X-Vercel-AI-Data-Stream', 'v1');\n  reply.header('Content-Type', 'text/plain; charset=utf-8');\n\n  return reply.send(result.toDataStream({ data }));\n});\n\nfastify.listen({ port: 8080 });\n```\n\n----------------------------------------\n\nTITLE: Using Tools with Llama 3.1 for Weather Information\nDESCRIPTION: This example demonstrates how to extend Llama 3.1's capabilities by integrating tools. It shows how to implement a weather tool that allows the model to query weather data for a specified location, enhancing its ability to provide contextual responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText, tool } from 'ai';\nimport { deepinfra } from '@ai-sdk/deepinfra';\nimport { getWeather } from './weatherTool';\n\nconst { text } = await generateText({\n  model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),\n  prompt: 'What is the weather like today?',\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Providing a Seed - AI SDK\nDESCRIPTION: This snippet shows how to control image generation output by providing a seed value. If supported by the model, providing the same seed value will result in the same generated image. The `seed` parameter is passed to the `generateImage` function. It requires the `ai` and a model provider like `@ai-sdk/openai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_6\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { image } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n  seed: 1234567890,\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Amazon Bedrock Provider Package\nDESCRIPTION: Command to install the Amazon Bedrock provider package via npm\nSOURCE: https://github.com/vercel/ai/blob/main/packages/amazon-bedrock/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/amazon-bedrock\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Text with Regex using FriendliAI\nDESCRIPTION: Illustrates how to constrain the output format of a FriendliAI model using a regular expression. The `regex` option is passed during model creation to ensure the generated text matches the specified pattern (e.g., only Korean characters, numbers, and specific punctuation). Dependencies: `@friendliai/ai-provider`, `ai`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { friendli } from '@friendliai/ai-provider';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: friendli('meta-llama-3.1-8b-instruct', {\n    regex: new RegExp('[\n ,.?!0-9\\uac00-\\ud7af]*'),\n  }),\n  prompt: 'Who is the first king of the Joseon Dynasty?',\n});\n\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Implementing Server Actions for OpenAI Assistant Integration\nDESCRIPTION: A server-side action that handles communication with the OpenAI Assistant API. It creates threads, adds messages, initiates runs, and streams back both status updates and message content to the client using streamable UI and values.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/120-stream-assistant-response.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { generateId } from 'ai';\nimport { createStreamableUI, createStreamableValue } from 'ai/rsc';\nimport { OpenAI } from 'openai';\nimport { ReactNode } from 'react';\nimport { Message } from './message';\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\nexport interface ClientMessage {\n  id: string;\n  status: ReactNode;\n  text: ReactNode;\n}\n\nconst ASSISTANT_ID = 'asst_xxxx';\nlet THREAD_ID = '';\nlet RUN_ID = '';\n\nexport async function submitMessage(question: string): Promise<ClientMessage> {\n  const statusUIStream = createStreamableUI('thread.init');\n\n  const textStream = createStreamableValue('');\n  const textUIStream = createStreamableUI(\n    <Message textStream={textStream.value} />,\n  );\n\n  const runQueue = [];\n\n  (async () => {\n    if (THREAD_ID) {\n      await openai.beta.threads.messages.create(THREAD_ID, {\n        role: 'user',\n        content: question,\n      });\n\n      const run = await openai.beta.threads.runs.create(THREAD_ID, {\n        assistant_id: ASSISTANT_ID,\n        stream: true,\n      });\n\n      runQueue.push({ id: generateId(), run });\n    } else {\n      const run = await openai.beta.threads.createAndRun({\n        assistant_id: ASSISTANT_ID,\n        stream: true,\n        thread: {\n          messages: [{ role: 'user', content: question }],\n        },\n      });\n\n      runQueue.push({ id: generateId(), run });\n    }\n\n    while (runQueue.length > 0) {\n      const latestRun = runQueue.shift();\n\n      if (latestRun) {\n        for await (const delta of latestRun.run) {\n          const { data, event } = delta;\n\n          statusUIStream.update(event);\n\n          if (event === 'thread.created') {\n            THREAD_ID = data.id;\n          } else if (event === 'thread.run.created') {\n            RUN_ID = data.id;\n          } else if (event === 'thread.message.delta') {\n            data.delta.content?.map(part => {\n              if (part.type === 'text') {\n                if (part.text) {\n                  textStream.append(part.text.value as string);\n                }\n              }\n            });\n          } else if (event === 'thread.run.failed') {\n            console.error(data);\n          }\n        }\n      }\n    }\n\n    statusUIStream.done();\n    textStream.done();\n  })();\n\n  return {\n    id: generateId(),\n    status: statusUIStream.value,\n    text: textUIStream.value,\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Output Configuration Type in TypeScript\nDESCRIPTION: Defines the structure for the `Output` type used in `experimental_output`. It includes static methods like `Output.text()` to forward text output and `Output.object()` to generate a JSON object based on a provided schema.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_29\n\nLANGUAGE: typescript\nCODE:\n```\nOutput\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 Client with Configuration - TypeScript\nDESCRIPTION: This snippet shows how to initialize the Mem0 Client with custom configuration for AI SDK integration. It requires installing '@mem0/vercel-ai-provider' and fetching a Mem0 API key. Key parameters are 'provider', 'mem0ApiKey', 'apiKey', 'config', and the optional 'mem0Config' for user/org/project context. Inputs include provider/API keys and configuration, output is a Mem0 client instance. Security best practices suggest usage of environment variables for credentials.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/70-mem0.mdx#2025-04-23_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { createMem0 } from '@mem0/vercel-ai-provider';\n\nconst mem0 = createMem0({\n  provider: 'openai',\n  mem0ApiKey: 'm0-xxx',\n  apiKey: 'provider-api-key',\n  config: {\n    compatibility: 'strict',\n  },\n  // Optional Mem0 Global Config\n  mem0Config: {\n    user_id: 'mem0-user-id',\n    org_id: 'mem0-org-id',\n    project_id: 'mem0-project-id',\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Maximum Token Limit Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `maxTokens` parameter as a number. This setting limits the maximum number of tokens the language model will generate in its response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nnumber\n```\n\n----------------------------------------\n\nTITLE: Displaying Changelog Entry for Version 4.3.8\nDESCRIPTION: Details the patch change for version 4.3.8, which adds a transcribe feature to the fal provider.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/CHANGELOG.md#2025-04-23_snippet_1\n\nLANGUAGE: Markdown\nCODE:\n```\n## 4.3.8\n\n### Patch Changes\n\n- 6e8a73b: feat(providers/fal): add transcribe\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Text Completion UI in React\nDESCRIPTION: Basic example showing how to implement a text completion interface using the useCompletion hook with a form input and submission handler.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/05-completion.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useCompletion } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { completion, input, handleInputChange, handleSubmit } = useCompletion({\n    api: '/api/completion',\n  });\n\n  return (\n    <form onSubmit={handleSubmit}>\n      <input\n        name=\"prompt\"\n        value={input}\n        onChange={handleInputChange}\n        id=\"input\"\n      />\n      <button type=\"submit\">Submit</button>\n      <div>{completion}</div>\n    </form>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Replacing rawResponse with response Property in TypeScript\nDESCRIPTION: Shows the removal of the `rawResponse` property from results of functions like `generateText`. In AI SDK 4.0, the `response` property should be used instead to access the underlying response details.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_24\n\nLANGUAGE: typescript\nCODE:\n```\nconst { text, rawResponse } = await generateText({\n  // ...\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst { text, response } = await generateText({\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Search Grounded Text Using Google Gemini (TypeScript)\nDESCRIPTION: Shows how to call the 'generateText' function with a Gemini provider to generate text responses grounded in recent web search results. Dependencies include '@ai-sdk/google' and 'ai'. The snippet configures 'useSearchGrounding', receives generation output, and safely accesses provider metadata, including grounding details and safety ratings. Required parameters are the model selection and prompt; the result includes the answer plus detailed metadata. Intended for applications needing up-to-date, well-sourced content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { GoogleGenerativeAIProviderMetadata } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst { text, providerMetadata } = await generateText({\n  model: google('gemini-1.5-pro', {\n    useSearchGrounding: true,\n  }),\n  prompt:\n    'List the top 5 San Francisco news from the past week.' +\n    'You must include the date of each article.',\n});\n\n// access the grounding metadata. Casting to the provider metadata type\n// is optional but provides autocomplete and type safety.\nconst metadata = providerMetadata?.google as\n  | GoogleGenerativeAIProviderMetadata\n  | undefined;\nconst groundingMetadata = metadata?.groundingMetadata;\nconst safetyRatings = metadata?.safetyRatings;\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Stop and Regenerate Functionality in Chatbot UI\nDESCRIPTION: This example demonstrates how to implement stop and regenerate buttons in the chatbot UI, allowing users to abort streaming responses or request message regeneration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\nconst { stop, status, ... } = useChat()\n\nreturn <>\n  <button onClick={stop} disabled={!(status === 'streaming' || status === 'submitted')}>Stop</button>\n  ...\n\nconst { reload, status, ... } = useChat()\n\nreturn <>\n  <button onClick={reload} disabled={!(status === 'ready' || status === 'error')}>Regenerate</button>\n  ...\n</>\n```\n\n----------------------------------------\n\nTITLE: Customizing Google Authentication Options for Vertex AI\nDESCRIPTION: Code demonstrating how to customize Google authentication options when creating a Vertex provider instance. This example shows direct credential specification with client_email and private_key.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createVertex } from '@ai-sdk/google-vertex';\n\nconst vertex = createVertex({\n  googleAuthOptions: {\n    credentials: {\n      client_email: 'my-email',\n      private_key: 'my-private-key',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Text Generation with OpenAI o1\nDESCRIPTION: Shows how to switch between different OpenAI models by changing the model parameter while keeping the same interface.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/23-o1.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { text } = await generateText({\n  model: openai('o1'),\n  prompt: 'Explain the concept of quantum entanglement.',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Groq Provider Instance\nDESCRIPTION: Create a customized Groq provider instance with specific settings using the createGroq function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/09-groq.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createGroq } from '@ai-sdk/groq';\n\nconst groq = createGroq({\n  // custom settings\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Zhipu API Key Environment Variable\nDESCRIPTION: Sets the Zhipu API key as an environment variable named `ZHIPU_API_KEY`. This variable is automatically read by the default provider instance. Replace `<your-api-key>` with your actual key obtained from the Zhipu BigModel platform.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/95-zhipu.mdx#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nZHIPU_API_KEY=<your-api-key>\n```\n\n----------------------------------------\n\nTITLE: Generating Text with SambaNova Model in TypeScript\nDESCRIPTION: Complete example of text generation using the SambaNova provider, including provider creation, model initialization, and text generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/96-sambanova.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createSambaNova } from 'sambanova-ai-provider';\nimport { generateText } from 'ai';\n\nconst sambanova = createSambaNova({\n  apiKey: 'YOUR_API_KEY',\n});\n\nconst model = sambanova('Meta-Llama-3.1-70B-Instruct');\n\nconst { text } = await generateText({\n  model,\n  prompt: 'Hello, nice to meet you.',\n});\n\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Creating a React Client Component for Text Generation in Next.js\nDESCRIPTION: A client-side Next.js component that sends a POST request to the /api/completion endpoint when a button is clicked. The component displays a loading state while waiting for the response and then shows the generated text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/10-generate-text.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\n\nexport default function Page() {\n  const [generation, setGeneration] = useState('');\n  const [isLoading, setIsLoading] = useState(false);\n\n  return (\n    <div>\n      <div\n        onClick={async () => {\n          setIsLoading(true);\n\n          await fetch('/api/completion', {\n            method: 'POST',\n            body: JSON.stringify({\n              prompt: 'Why is the sky blue?',\n            }),\n          }).then(response => {\n            response.json().then(json => {\n              setGeneration(json.text);\n              setIsLoading(false);\n            });\n          });\n        }}\n      >\n        Generate\n      </div>\n\n      {isLoading ? 'Loading...' : generation}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing an Agent with Llama 3.1 for Math Problem Solving\nDESCRIPTION: This snippet demonstrates how to create an AI agent using Llama 3.1 that can solve math problems. It utilizes the maxSteps parameter to allow the model to make multiple decisions and tool calls within a single interaction, showcasing the AI SDK's agent capabilities.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText, tool } from 'ai';\nimport { deepinfra } from '@ai-sdk/deepinfra';\nimport * as mathjs from 'mathjs';\nimport { z } from 'zod';\n\nconst problem =\n  'Calculate the profit for a day if revenue is $5000 and expenses are $3500.';\n\nconst { text: answer } = await generateText({\n  model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),\n  system:\n    'You are solving math problems. Reason step by step. Use the calculator when necessary.',\n  prompt: problem,\n  tools: {\n    calculate: tool({\n      description: 'A tool for evaluating mathematical expressions.',\n      parameters: z.object({ expression: z.string() }),\n      execute: async ({ expression }) => mathjs.evaluate(expression),\n    }),\n  },\n  maxSteps: 5,\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Notification Schema in TypeScript\nDESCRIPTION: This snippet defines a Zod schema for notifications, which is used to structure the generated data. It includes fields for name and message.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/40-stream-object.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from 'zod';\n\n// define a schema for the notifications\nexport const notificationSchema = z.object({\n  notifications: z.array(\n    z.object({\n      name: z.string().describe('Name of a fictional person.'),\n      message: z.string().describe('Message. Do not use emojis or links.'),\n    }),\n  ),\n});\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Values with OpenAI Model in TypeScript\nDESCRIPTION: This code shows how to use the 'embedMany' function from the AI SDK to embed multiple text values simultaneously using an OpenAI embedding model. It returns an array of embedding objects, maintaining the order of the input values.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/30-embeddings.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { embedMany } from 'ai';\n\n// 'embeddings' is an array of embedding objects (number[][]).\n// It is sorted in the same order as the input values.\nconst { embeddings } = await embedMany({\n  model: openai.embedding('text-embedding-3-small'),\n  values: [\n    'sunny day at the beach',\n    'rainy afternoon in the city',\n    'snowy night in the mountains',\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Importing FriendliAI Provider Instance in TypeScript\nDESCRIPTION: Imports the default `friendli` provider instance from the `@friendliai/ai-provider` package. This instance is used to create language model objects.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { friendli } from '@friendliai/ai-provider';\n```\n\n----------------------------------------\n\nTITLE: Fixing Failed to Parse Stream Error with streamProtocol Parameter in useChat\nDESCRIPTION: Sets the streamProtocol parameter to 'text' when using the useChat hook to resolve stream parsing errors. This solution is applicable when working with older versions of the AI SDK in the backend, custom text stream providers, or raw LangChain stream results.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/08-use-chat-failed-to-parse-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nconst { messages, append } = useChat({ streamProtocol: 'text' });\n```\n\n----------------------------------------\n\nTITLE: Adding Temperature Conversion Tool\nDESCRIPTION: Enhanced route handler that includes both weather and temperature conversion tools with Zod schema validation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n      convertFahrenheitToCelsius: tool({\n        description: 'Convert a temperature in fahrenheit to celsius',\n        parameters: z.object({\n          temperature: z\n            .number()\n            .describe('The temperature in fahrenheit to convert'),\n        }),\n        execute: async ({ temperature }) => {\n          const celsius = Math.round((temperature - 32) * (5 / 9));\n          return {\n            celsius,\n          };\n        },\n      }),\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 Provider - Shell\nDESCRIPTION: These installation commands add the '@mem0/vercel-ai-provider' package to your project using different package managers. No dependencies other than a working Node.js and package manager environment are required. The installed package enables Mem0 integration within your TypeScript or JavaScript project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/70-mem0.mdx#2025-04-23_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npnpm add @mem0/vercel-ai-provider\n```\n\nLANGUAGE: Shell\nCODE:\n```\nnpm install @mem0/vercel-ai-provider\n```\n\nLANGUAGE: Shell\nCODE:\n```\nyarn add @mem0/vercel-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Using valibotSchema to Create a Recipe Schema in TypeScript\nDESCRIPTION: This example demonstrates how to use the valibotSchema function to create a structured schema for a recipe, including name, ingredients, and steps. It imports necessary functions from the AI SDK and Valibot library.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/27-valibot-schema.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { valibotSchema } from '@ai-sdk/valibot';\nimport { object, string, array } from 'valibot';\n\nconst recipeSchema = valibotSchema(\n  object({\n    name: string(),\n    ingredients: array(\n      object({\n        name: string(),\n        amount: string(),\n      }),\n    ),\n    steps: array(string()),\n  }),\n);\n```\n\n----------------------------------------\n\nTITLE: Rendering Tool Call States in React Component\nDESCRIPTION: Implements UI rendering for different tool call states (partial-call, call, result) in a React component using the useChat hook.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nexport default function Chat() {\n  // ...\n  return (\n    <>\n      {messages?.map(message => (\n        <div key={message.id}>\n          {message.parts.map(part => {\n            if (part.type === 'tool-invocation') {\n              switch (part.toolInvocation.state) {\n                case 'partial-call':\n                  return <>render partial tool call</>;\n                case 'call':\n                  return <>render full tool call</>;\n                case 'result':\n                  return <>render tool result</>;\n              }\n            }\n          })}\n        </div>\n      ))}\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Server Actions with AI SDK RSC\nDESCRIPTION: Server-side implementation using streamUI for dynamic component generation with weather tool integration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#2025-04-23_snippet_8\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { streamUI } from 'ai/rsc';\nimport { deepinfra } from '@ai-sdk/deepinfra';\nimport { z } from 'zod';\n\nexport async function streamComponent() {\n  const result = await streamUI({\n    model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),\n    prompt: 'Get the weather for San Francisco',\n    text: ({ content }) => <div>{content}</div>,\n    tools: {\n      getWeather: {\n        description: 'Get the weather for a location',\n        parameters: z.object({ location: z.string() }),\n        generate: async function* ({ location }) {\n          yield <div>loading...</div>;\n          const weather = '25c'; // await getWeather(location);\n          return (\n            <div>\n              the weather in {location} is {weather}.\n            </div>\n          );\n        },\n      },\n    },\n  });\n  return result.value;\n}\n```\n\n----------------------------------------\n\nTITLE: Rendering Multi-Modal Message Parts with useChat Hook in React (JavaScript)\nDESCRIPTION: This React component demonstrates how to render complex AI model responses using the useChat hook from the AI SDK 4.2. It iterates through messages and their parts array, conditionally rendering UI elements for each part type (text, source, reasoning, tool-invocation, file). Key parameters include the part object and its type, and some renderings use base64-encoded data. Required dependencies are React and the useChat hook from the AI SDK. The component expects message.parts to follow the new array format and may need additional handling for unsupported types. Returns JSX elements in a div.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/27-migration-guide-4-2.mdx#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nfunction Chat() {\n  const { messages } = useChat();\n  return (\n    <div>\n      {messages.map(message =>\n        message.parts.map((part, i) => {\n          switch (part.type) {\n            case 'text':\n              return <p key={i}>{part.text}</p>;\n            case 'source':\n              return <p key={i}>{part.source.url}</p>;\n            case 'reasoning':\n              return <div key={i}>{part.reasoning}</div>;\n            case 'tool-invocation':\n              return <div key={i}>{part.toolInvocation.toolName}</div>;\n            case 'file':\n              return (\n                <img\n                  key={i}\n                  src={`data:${part.mimeType};base64,${part.data}`}\n                />\n              );\n          }\n        }),\n      )}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Chat Component to Display PDFs in React\nDESCRIPTION: This snippet extends the Chat component to render PDF attachments using an iframe, in addition to displaying images. It filters attachments based on their content type and renders them accordingly.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/02-multi-modal-chatbot.mdx#2025-04-23_snippet_8\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\nimport { useRef, useState } from 'react';\nimport Image from 'next/image';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n\n  const [files, setFiles] = useState<FileList | undefined>(undefined);\n  const fileInputRef = useRef<HTMLInputElement>(null);\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(m => (\n        <div key={m.id} className=\"whitespace-pre-wrap\">\n          {m.role === 'user' ? 'User: ' : 'AI: '}\n          {m.content}\n          <div>\n            {m?.experimental_attachments\n              ?.filter(\n                attachment =>\n                  attachment?.contentType?.startsWith('image/') ||\n                  attachment?.contentType?.startsWith('application/pdf'),\n              )\n              .map((attachment, index) =>\n                attachment.contentType?.startsWith('image/') ? (\n                  <Image\n                    key={`${m.id}-${index}`}\n                    src={attachment.url}\n                    width={500}\n                    height={500}\n                    alt={attachment.name ?? `attachment-${index}`}\n                  />\n                ) : attachment.contentType?.startsWith('application/pdf') ? (\n                  <iframe\n                    key={`${m.id}-${index}`}\n                    src={attachment.url}\n                    width={500}\n                    height={600}\n                    title={attachment.name ?? `attachment-${index}`}\n                  />\n                ) : null,\n              )}\n          </div>\n        </div>\n      ))}\n\n      <form\n        className=\"fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl space-y-2\"\n        onSubmit={event => {\n          handleSubmit(event, {\n            experimental_attachments: files,\n          });\n\n          setFiles(undefined);\n\n          if (fileInputRef.current) {\n            fileInputRef.current.value = '';\n          }\n        }}\n      >\n        <input\n          type=\"file\"\n          className=\"\"\n          onChange={event => {\n            if (event.target.files) {\n              setFiles(event.target.files);\n            }\n          }}\n          multiple\n          ref={fileInputRef}\n        />\n        <input\n          className=\"w-full p-2\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Error Handling for Object Generation in TypeScript\nDESCRIPTION: This snippet demonstrates how to handle errors when generating objects, specifically for the AI_NoObjectGeneratedError, including logging of error details.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateObject, NoObjectGeneratedError } from 'ai';\n\ntry {\n  await generateObject({ model, schema, prompt });\n} catch (error) {\n  if (NoObjectGeneratedError.isInstance(error)) {\n    console.log('NoObjectGeneratedError');\n    console.log('Cause:', error.cause);\n    console.log('Text:', error.text);\n    console.log('Response:', error.response);\n    console.log('Usage:', error.usage);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Array with Schema in TypeScript\nDESCRIPTION: Example demonstrating how to generate an array of objects using a schema for RPG hero descriptions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/03-generate-object.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: openai('gpt-4-turbo'),\n  output: 'array',\n  schema: z.object({\n    name: z.string(),\n    class: z\n      .string()\n      .describe('Character class, e.g. warrior, mage, or thief.'),\n    description: z.string(),\n  }),\n  prompt: 'Generate 3 hero descriptions for a fantasy role playing game.',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Gladia Provider in TypeScript\nDESCRIPTION: Demonstrates how to create a customized Gladia provider instance with specific settings like a custom fetch implementation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/120-gladia.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createGladia } from '@ai-sdk/gladia';\n\nconst gladia = createGladia({\n  // custom settings, e.g.\n  fetch: customFetch,\n});\n```\n\n----------------------------------------\n\nTITLE: Handling Warnings in Speech Generation\nDESCRIPTION: Example of accessing warnings from the speech generation process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/37-speech.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\nimport { readFile } from 'fs/promises';\n\nconst audio = await generateSpeech({\n  model: openai.speech('tts-1'),\n  text: 'Hello, world!',\n});\n\nconst warnings = audio.warnings;\n```\n\n----------------------------------------\n\nTITLE: Server-Side Text Generation API Route in Next.js\nDESCRIPTION: A Next.js API route handler that processes text generation requests using OpenAI's GPT-4 model. It uses the streamText function to generate and stream text responses back to the client.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/20-stream-text.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nexport async function POST(req: Request) {\n  const { prompt }: { prompt: string } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4'),\n    system: 'You are a helpful assistant.',\n    prompt,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Response Interface Structure - TypeScript\nDESCRIPTION: Detailed TypeScript interface definition for AI response objects, including nested properties for sources, files, tool calls, and step results. Contains comprehensive type information and descriptions for each field.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  name: 'sources',\n  type: 'Array<Source>',\n  description: 'Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.',\n  properties: [\n    {\n      type: 'Source',\n      parameters: [\n        {\n          name: 'sourceType',\n          type: \"'url'\",\n          description: 'A URL source. This is return by web search RAG models.'\n        },\n        {\n          name: 'id',\n          type: 'string',\n          description: 'The ID of the source.'\n        },\n        {\n          name: 'url',\n          type: 'string',\n          description: 'The URL of the source.'\n        },\n        {\n          name: 'title',\n          type: 'string',\n          isOptional: true,\n          description: 'The title of the source.'\n        },\n        {\n          name: 'providerMetadata',\n          type: 'LanguageModelV1ProviderMetadata',\n          isOptional: true,\n          description: 'Additional provider metadata for the source.'\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Customized xAI Provider Instance\nDESCRIPTION: This snippet shows how to create a custom xAI provider instance using `createXai`. It allows setting the API key, base URL, custom headers, and a custom fetch implementation to configure the provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { createXai } from '@ai-sdk/xai';\n\nconst xai = createXai({\n  apiKey: 'your-api-key',\n});\n```\n\n----------------------------------------\n\nTITLE: Processing PDF Inputs with OpenAI Chat API\nDESCRIPTION: Shows how to send a PDF file as part of the user message content to an OpenAI model using `generateText`. The PDF data is read using `fs.readFileSync` and included as a `file` type object with the appropriate MIME type.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai'; // Assumed import\nimport { generateText } from 'ai'; // Assumed import\nimport * as fs from 'fs'; // Assumed import\n\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'What is an embedding model?',\n        },\n        {\n          type: 'file',\n          data: fs.readFileSync('./data/ai.pdf'),\n          mimeType: 'application/pdf',\n          filename: 'ai.pdf', // optional\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with OpenAI Model using AI SDK in TypeScript\nDESCRIPTION: Provides a basic example of using the `generateText` function from the `ai` core package with an OpenAI model instance created via the provider. It sends a prompt and retrieves the generated text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: openai('gpt-4-turbo'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Handling Custom Body Fields on Server Side\nDESCRIPTION: Example of extracting custom fields from the request body on the server side.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_11\n\nLANGUAGE: ts\nCODE:\n```\nexport async function POST(req: Request) {\n  // Extract addition information (\"customKey\") from the body of the request:\n  const { messages, customKey } = await req.json();\n  //...\n}\n```\n\n----------------------------------------\n\nTITLE: Replacing with Route Handler and Streaming Props Data (UI)\nDESCRIPTION: This snippet demonstrates how to replace the server action with a route handler that streams props data to the client using streamText in AI SDK UI.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from 'zod';\nimport { openai } from '@ai-sdk/openai';\nimport { getWeather } from '@/utils/queries';\nimport { streamText } from 'ai';\n\nexport async function POST(request) {\n  const { messages } = await request.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'you are a friendly assistant!',\n    messages,\n    tools: {\n      displayWeather: {\n        description: 'Display the weather for a location',\n        parameters: z.object({\n          latitude: z.number(),\n          longitude: z.number(),\n        }),\n        execute: async function ({ latitude, longitude }) {\n          const props = await getWeather({ latitude, longitude });\n          return props;\n        },\n      },\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiple Parallel Streamable UIs in TypeScript\nDESCRIPTION: Demonstrates how to create and manage multiple independent streamable UI components that can be updated separately. Shows implementation of weather and forecast data streaming with loading states.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/05-multiple-streamables.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { createStreamableUI } from 'ai/rsc';\n\nexport async function getWeather() {\n  const weatherUI = createStreamableUI();\n  const forecastUI = createStreamableUI();\n\n  weatherUI.update(<div>Loading weather...</div>);\n  forecastUI.update(<div>Loading forecast...</div>);\n\n  getWeatherData().then(weatherData => {\n    weatherUI.done(<div>{weatherData}</div>);\n  });\n\n  getForecastData().then(forecastData => {\n    forecastUI.done(<div>{forecastData}</div>);\n  });\n\n  // Return both streamable UIs and other data fields.\n  return {\n    requestedAt: Date.now(),\n    weather: weatherUI.value,\n    forecast: forecastUI.value,\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Error Handling with useCompletion\nDESCRIPTION: Implementation of error handling using the error state from useCompletion hook with toast notifications and UI feedback.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/05-completion.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nconst { error, ... } = useCompletion()\n\nuseEffect(() => {\n  if (error) {\n    toast.error(error.message)\n  }\n}, [error])\n\n// Or display the error message in the UI:\nreturn (\n  <>\n    {error ? <div>{error.message}</div> : null}\n  </>\n)\n```\n\n----------------------------------------\n\nTITLE: Streaming Data in Nest.js with AI SDK\nDESCRIPTION: This Nest.js controller demonstrates how to use the AI SDK's pipeDataStreamToResponse method to stream data from OpenAI's GPT-4o model to the client. It creates a POST endpoint that streams text generated from a prompt about inventing a new holiday.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/50-nest.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Controller, Post, Res } from '@nestjs/common';\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport { Response } from 'express';\n\n@Controller()\nexport class AppController {\n  @Post()\n  async example(@Res() res: Response) {\n    const result = streamText({\n      model: openai('gpt-4o'),\n      prompt: 'Invent a new holiday and describe its traditions.',\n    });\n\n    result.pipeDataStreamToResponse(res);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating Tools in Page Component\nDESCRIPTION: Main page component that implements useChat hook and handles both weather and stock tool invocations. Includes loading states and form handling for user input.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#2025-04-23_snippet_8\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\nimport { Weather } from '@/components/weather';\nimport { Stock } from '@/components/stock';\n\nexport default function Page() {\n  const { messages, input, setInput, handleSubmit } = useChat();\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>{message.role}</div>\n          <div>{message.content}</div>\n\n          <div>\n            {message.toolInvocations?.map(toolInvocation => {\n              const { toolName, toolCallId, state } = toolInvocation;\n\n              if (state === 'result') {\n                if (toolName === 'displayWeather') {\n                  const { result } = toolInvocation;\n                  return (\n                    <div key={toolCallId}>\n                      <Weather {...result} />\n                    </div>\n                  );\n                } else if (toolName === 'getStockPrice') {\n                  const { result } = toolInvocation;\n                  return <Stock key={toolCallId} {...result} />;\n                }\n              } else {\n                return (\n                  <div key={toolCallId}>\n                    {toolName === 'displayWeather' ? (\n                      <div>Loading weather...</div>\n                    ) : toolName === 'getStockPrice' ? (\n                      <div>Loading stock price...</div>\n                    ) : (\n                      <div>Loading...</div>\n                    )}\n                  </div>\n                );\n              }\n            })}\n          </div>\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Streaming Middleware in TypeScript\nDESCRIPTION: Basic import statement for the simulateStreamingMiddleware function from the ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/67-simulate-streaming-middleware.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { simulateStreamingMiddleware } from 'ai';\n\nconst middleware = simulateStreamingMiddleware();\n```\n\n----------------------------------------\n\nTITLE: Customizing Luma Image Model Generation Settings (TypeScript)\nDESCRIPTION: Shows how to configure advanced generation parameters for a Luma image model. You can control the maximum number of images per call, polling intervals to check image job completion, and timeout settings for polling attempts. Tune these parameters as needed for efficiency and performance, especially when processing high-latency or batch workloads.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/80-luma.mdx#2025-04-23_snippet_6\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = luma.image('photon-1', {\n  maxImagesPerCall: 1, // Maximum number of images to generate per API call\n  pollIntervalMillis: 5000, // How often to check for completed images (in ms)\n  maxPollAttempts: 10, // Maximum number of polling attempts before timeout\n});\n```\n\n----------------------------------------\n\nTITLE: Provider Options at Message Level in TypeScript\nDESCRIPTION: This example shows how to configure provider-specific options at the message level, setting a cache control breakpoint for Anthropic.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst messages = [\n  {\n    role: 'system',\n    content: 'Cached system message',\n    providerOptions: {\n      // Sets a cache control breakpoint on the system message\n      anthropic: { cacheControl: { type: 'ephemeral' } },\n    },\n  },\n];\n```\n\n----------------------------------------\n\nTITLE: Error Handling for Speech Generation\nDESCRIPTION: Demonstrates comprehensive error handling with AI_NoAudioGeneratedError, including accessing error details and responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/37-speech.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  experimental_generateSpeech as generateSpeech,\n  AI_NoAudioGeneratedError,\n} from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { readFile } from 'fs/promises';\n\ntry {\n  await generateSpeech({\n    model: openai.speech('tts-1'),\n    text: 'Hello, world!',\n  });\n} catch (error) {\n  if (AI_NoAudioGeneratedError.isInstance(error)) {\n    console.log('AI_NoAudioGeneratedError');\n    console.log('Cause:', error.cause);\n    console.log('Responses:', error.responses);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating custom data streams with Fastify and AI SDK\nDESCRIPTION: This example demonstrates how to use createDataStream to send custom data to the client while streaming AI-generated content. It includes error handling and allows merging AI-generated content into an existing data stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/40-fastify.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { createDataStream, streamText } from 'ai';\nimport Fastify from 'fastify';\n\nconst fastify = Fastify({ logger: true });\n\nfastify.post('/stream-data', async function (request, reply) {\n  // immediately start streaming the response\n  const dataStream = createDataStream({\n    execute: async dataStreamWriter => {\n      dataStreamWriter.writeData('initialized call');\n\n      const result = streamText({\n        model: openai('gpt-4o'),\n        prompt: 'Invent a new holiday and describe its traditions.',\n      });\n\n      result.mergeIntoDataStream(dataStreamWriter);\n    },\n    onError: error => {\n      // Error messages are masked by default for security reasons.\n      // If you want to expose the error message to the client, you can do so here:\n      return error instanceof Error ? error.message : String(error);\n    },\n  });\n\n  // Mark the response as a v1 data stream:\n  reply.header('X-Vercel-AI-Data-Stream', 'v1');\n  reply.header('Content-Type', 'text/plain; charset=utf-8');\n\n  return reply.send(dataStream);\n});\n\nfastify.listen({ port: 8080 });\n```\n\n----------------------------------------\n\nTITLE: Generating Speech with LMNT Provider Options in TypeScript\nDESCRIPTION: Complete example of generating speech with the LMNT provider, including model selection and provider-specific options like language setting.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/140-lmnt.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\nimport { lmnt } from '@ai-sdk/lmnt';\n\nconst result = await generateSpeech({\n  model: lmnt.speech('aurora'),\n  text: 'Hello, world!',\n  providerOptions: { lmnt: { language: 'en' } },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing FriendliAI Provider with yarn\nDESCRIPTION: Installs the necessary FriendliAI provider package using the yarn package manager. This package facilitates the use of FriendliAI models in conjunction with the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nyarn add @friendliai/ai-provider\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Google Vertex Gemini in Edge Runtime using AI SDK\nDESCRIPTION: Shows how to generate text using the `gemini-1.5-flash` model via the Google Vertex provider specifically designed for Edge runtimes (`@ai-sdk/google-vertex/edge`). The code imports `vertex` from the `/edge` submodule and uses `generateText`. Authentication relies on environment variables like `GOOGLE_CLIENT_EMAIL` and `GOOGLE_PRIVATE_KEY` following Application Default Credentials.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google-vertex/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertex } from '@ai-sdk/google-vertex/edge';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: vertex('gemini-1.5-flash'),\n  prompt: 'Write a vegetarian lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Abort Signals in AI SDK Tools\nDESCRIPTION: Demonstrates how to use abort signals with generateText and tool execution, including forwarding signals to fetch calls inside tools.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from 'zod';\nimport { generateText, tool } from 'ai';\n\nconst result = await generateText({\n  model: yourModel,\n  abortSignal: myAbortSignal, // signal that will be forwarded to tools\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({ location: z.string() }),\n      execute: async ({ location }, { abortSignal }) => {\n        return fetch(\n          `https://api.weatherapi.com/v1/current.json?q=${location}`,\n          { signal: abortSignal }, // forward the abort signal to fetch\n        );\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Client-Side Conversation Interface with React\nDESCRIPTION: This code defines a React client component that manages a conversational UI. It uses the AI SDK's useUIState and useActions hooks to maintain conversation state and trigger server actions. The component renders messages and provides an input form for user interaction.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/92-stream-ui-record-token-usage.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { ClientMessage } from './actions';\nimport { useActions, useUIState } from 'ai/rsc';\nimport { generateId } from 'ai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [input, setInput] = useState<string>('');\n  const [conversation, setConversation] = useUIState();\n  const { continueConversation } = useActions();\n\n  return (\n    <div>\n      <div>\n        {conversation.map((message: ClientMessage) => (\n          <div key={message.id}>\n            {message.role}: {message.display}\n          </div>\n        ))}\n      </div>\n\n      <div>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button\n          onClick={async () => {\n            setConversation((currentConversation: ClientMessage[]) => [\n              ...currentConversation,\n              { id: generateId(), role: 'user', display: input },\n            ]);\n\n            const message = await continueConversation(input);\n\n            setConversation((currentConversation: ClientMessage[]) => [\n              ...currentConversation,\n              message,\n            ]);\n          }}\n        >\n          Send Message\n        </button>\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Installing the OpenAI Provider Package (pnpm/npm/yarn)\nDESCRIPTION: Shows how to install the `@ai-sdk/openai` package using different package managers (pnpm, npm, yarn). This package is required to use the OpenAI provider with the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add @ai-sdk/openai\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @ai-sdk/openai\n```\n\nLANGUAGE: shell\nCODE:\n```\nyarn add @ai-sdk/openai\n```\n\n----------------------------------------\n\nTITLE: Implementing Orchestrator-Worker Pattern with AI SDK in TypeScript\nDESCRIPTION: This snippet demonstrates the orchestrator-worker pattern where a primary model coordinates specialized workers for complex tasks. It uses the AI SDK to generate implementation plans and execute changes across multiple files.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nasync function implementFeature(featureRequest: string) {\n  // Orchestrator: Plan the implementation\n  const { object: implementationPlan } = await generateObject({\n    model: openai('o3-mini'),\n    schema: z.object({\n      files: z.array(\n        z.object({\n          purpose: z.string(),\n          filePath: z.string(),\n          changeType: z.enum(['create', 'modify', 'delete']),\n        }),\n      ),\n      estimatedComplexity: z.enum(['low', 'medium', 'high']),\n    }),\n    system:\n      'You are a senior software architect planning feature implementations.',\n    prompt: `Analyze this feature request and create an implementation plan:\n    ${featureRequest}`,\n  });\n\n  // Workers: Execute the planned changes\n  const fileChanges = await Promise.all(\n    implementationPlan.files.map(async file => {\n      // Each worker is specialized for the type of change\n      const workerSystemPrompt = {\n        create:\n          'You are an expert at implementing new files following best practices and project patterns.',\n        modify:\n          'You are an expert at modifying existing code while maintaining consistency and avoiding regressions.',\n        delete:\n          'You are an expert at safely removing code while ensuring no breaking changes.',\n      }[file.changeType];\n\n      const { object: change } = await generateObject({\n        model: openai('gpt-4o'),\n        schema: z.object({\n          explanation: z.string(),\n          code: z.string(),\n        }),\n        system: workerSystemPrompt,\n        prompt: `Implement the changes for ${file.filePath} to support:\n        ${file.purpose}\n\n        Consider the overall feature context:\n        ${featureRequest}`,\n      });\n\n      return {\n        file,\n        implementation: change,\n      };\n    }),\n  );\n\n  return {\n    plan: implementationPlan,\n    changes: fileChanges,\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Describing Individual Message Part Types for Chat UI (TypeScript)\nDESCRIPTION: This snippet specifies the different types of message parts that can compose a UIMessage: textual parts, reasoning content, tool invocation sequences (including their states), reference sources, and logical step starts. Each part type is implemented as a separate interface, sometimes with additional nested structures (for tools or sources). Required dependencies are all sub-type interfaces and, where needed, auxiliary types such as ToolInvocation or Source. Inputs and outputs are strongly typed objects keyed by discriminator fields to permit type-safe processing in UIs or flows. Limitations: actual type-checking and JSON parsing must ensure these discriminators are correctly set.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#2025-04-23_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\ninterface TextUIPart {\n  type: 'text';\n  text: string; // The text content of the part.\n}\n\ninterface ReasoningUIPart {\n  type: 'reasoning';\n  reasoning: string; // The reasoning content of the part.\n}\n\ninterface ToolInvocationUIPart {\n  type: 'tool-invocation';\n  toolInvocation: ToolInvocation; // Partially or fully formed tool invocation structure.\n}\n\ninterface SourceUIPart {\n  type: 'source';\n  source: Source; // The associated source reference.\n}\n\ninterface StepStartUIPart {\n  type: 'step-start';\n}\n\n```\n\n----------------------------------------\n\nTITLE: Using Google Gemini with Search Grounding in TypeScript\nDESCRIPTION: This example demonstrates how to enable search grounding with Google's Gemini model to access the latest information using Google search. It configures the model to use search grounding and includes code to access the grounding metadata and safety ratings from the response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/56-web-search-agent.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst { text, sources, providerMetadata } = await generateText({\n  model: google('gemini-1.5-pro', {\n    useSearchGrounding: true,\n  }),\n  prompt:\n    'List the top 5 San Francisco news from the past week.' +\n    'You must include the date of each article.',\n});\n\nconsole.log(text);\nconsole.log(sources);\n\n// access the grounding metadata.\nconst metadata = providerMetadata?.google;\nconst groundingMetadata = metadata?.groundingMetadata;\nconst safetyRatings = metadata?.safetyRatings;\n```\n\n----------------------------------------\n\nTITLE: Server-Side Object Stream Generation with OpenAI\nDESCRIPTION: Server action that implements object streaming using OpenAI's GPT-4 model. Uses Zod for schema validation and handles stream generation and updates through the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/40-stream-object.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n'use server';\n\nimport { streamObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { createStreamableValue } from 'ai/rsc';\nimport { z } from 'zod';\n\nexport async function generate(input: string) {\n  'use server';\n\n  const stream = createStreamableValue();\n\n  (async () => {\n    const { partialObjectStream } = streamObject({\n      model: openai('gpt-4-turbo'),\n      system: 'You generate three notifications for a messages app.',\n      prompt: input,\n      schema: z.object({\n        notifications: z.array(\n          z.object({\n            name: z.string().describe('Name of a fictional person.'),\n            message: z.string().describe('Do not use emojis or links.'),\n            minutesAgo: z.number(),\n          }),\n        ),\n      }),\n    });\n\n    for await (const partialObject of partialObjectStream) {\n      stream.update(partialObject);\n    }\n\n    stream.done();\n  })();\n\n  return { object: stream.value };\n}\n```\n\n----------------------------------------\n\nTITLE: Using Language Models from Provider Registry\nDESCRIPTION: Shows how to generate text using a language model accessed through the registry using the provider:model ID format.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/40-provider-registry.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { registry } from './registry';\n\nconst { text } = await generateText({\n  model: registry.languageModel('openai:gpt-4-turbo'),\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Assistant Message Array Content in TypeScript\nDESCRIPTION: This example shows how to generate text using an AI model with an assistant message containing text content in an array format.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  messages: [\n    { role: 'user', content: 'Hi!' },\n    {\n      role: 'assistant',\n      content: [{ type: 'text', text: 'Hello, how can I help?' }],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Client-Side Stream Cancellation with useCompletion Hook\nDESCRIPTION: Shows how to implement client-side stream cancellation using the useCompletion hook from the AI SDK UI. The example includes a stop button that appears during submission or streaming states.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/02-stopping-streams.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useCompletion } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { input, completion, stop, status, handleSubmit, handleInputChange } =\n    useCompletion();\n\n  return (\n    <div>\n      {(status === 'submitted' || status === 'streaming') && (\n        <button type=\"button\" onClick={() => stop()}>\n          Stop\n        </button>\n      )}\n      {completion}\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Using Base64-Encoded Images in User Messages in TypeScript\nDESCRIPTION: This snippet demonstrates how to include a base64-encoded image in a user message for multimodal AI processing.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'Describe the image in detail.' },\n        {\n          type: 'image',\n          image: fs.readFileSync('./data/comic-cat.png').toString('base64'),\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Next.js App with AI SDK and OpenAI Example using Yarn\nDESCRIPTION: This command uses Yarn to create a new Next.js application with the AI SDK and OpenAI example. It bootstraps the project by cloning the example from the Vercel AI repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Google Vertex Model (TypeScript)\nDESCRIPTION: Illustrates generating text from a Vertex model using the generateText helper function. The snippet imports necessary providers, sets up the model, and passes an English prompt for text creation. Requires '@ai-sdk/google-vertex' and 'ai' as dependencies. The returned object contains a text field with the generated response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertex } from '@ai-sdk/google-vertex';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: vertex('gemini-1.5-pro'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Defining AI Text Stream Interface - TypeScript\nDESCRIPTION: This snippet defines a property type for a text stream, represented as an object implementing both AsyncIterable<string> and ReadableStream<string>. The stream emits generated text deltas incrementally, with suppressed errors to avoid server-side exceptions. Consumers should supply an onError callback to handle diagnostics. This is primarily for real-time streaming of textual outputs from an AI model, and inputs/outputs are plain text fragments.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\n    {\n      name: 'textStream',\n      type: 'AsyncIterable<string> & ReadableStream<string>',\n      description:\n        'A text stream that returns only the generated text deltas. You can use it as either an AsyncIterable or a ReadableStream. Errors are suppressed to prevent server crashes. Use the onError callback to log errors.',\n    },\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Memory Context - TypeScript\nDESCRIPTION: Shows how to use language models combined with Mem0-enhanced memory for contextual text generation. Assumes installation of 'ai' and '@mem0/vercel-ai-provider'. The 'generateText' function is called using a model initialized via 'createMem0', passing a prompt and user context. Required input: prompt string, optional user_id, output: generated text as part of the response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/70-mem0.mdx#2025-04-23_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { createMem0 } from '@mem0/vercel-ai-provider';\n\nconst mem0 = createMem0();\n\nconst { text } = await generateText({\n  model: mem0('gpt-4-turbo', { user_id: 'borat' }),\n  prompt: 'Suggest me a good car to buy!',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing the Default Anthropic Vertex Provider Instance in TypeScript\nDESCRIPTION: Shows how to import the default, pre-configured `anthropicVertex` provider instance from the `anthropic-vertex-ai` package. This is the simplest way to start using the provider if default settings (relying on environment variables) are sufficient.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/91-anthropic-vertex-ai.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropicVertex } from 'anthropic-vertex-ai';\n```\n\n----------------------------------------\n\nTITLE: Importing the Azure Provider Instance (TypeScript)\nDESCRIPTION: This TypeScript snippet demonstrates how to import the default `azure` provider instance from the installed `@ai-sdk/azure` package. This instance is a factory function used to configure and access specific Azure OpenAI model deployments.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/azure/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { azure } from '@ai-sdk/azure';\n```\n\n----------------------------------------\n\nTITLE: Importing the Default Cohere Provider in TypeScript\nDESCRIPTION: Demonstrates how to import the default Cohere provider instance for use with the AI SDK. This is the simplest way to use Cohere models without custom configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/25-cohere.mdx#2025-04-23_snippet_0\n\nLANGUAGE: ts\nCODE:\n```\nimport { cohere } from '@ai-sdk/cohere';\n```\n\n----------------------------------------\n\nTITLE: Importing Default Groq Provider\nDESCRIPTION: Import the default Groq provider instance from the @ai-sdk/groq package for standard usage with predefined settings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/09-groq.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { groq } from '@ai-sdk/groq';\n```\n\n----------------------------------------\n\nTITLE: Importing Default Fireworks Provider Instance\nDESCRIPTION: How to import the default Fireworks provider instance from the package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/26-fireworks.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fireworks } from '@ai-sdk/fireworks';\n```\n\n----------------------------------------\n\nTITLE: Restoring AI State in Root Layout Component\nDESCRIPTION: Implementation of AI state restoration in the root layout component. Loads chat history from database and passes it as initialAIState to the AI context provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-saving-and-restoring-states.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { ReactNode } from 'react';\nimport { AI } from './ai';\n\nexport default async function RootLayout({\n  children,\n}: Readonly<{ children: ReactNode }>) {\n  const chat = await loadChatFromDB();\n\n  return (\n    <html lang=\"en\">\n      <body>\n        <AI initialAIState={chat}>{children}</AI>\n      </body>\n    </html>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Passing Provider Options for OpenAI Image Model in TypeScript\nDESCRIPTION: This code shows how to use the second argument of the .image() factory to supply providerOptions, such as specifying 'quality' for the OpenAI image generation API. The options are passed within the providerOptions.openai object and may change depending on the selected model. It is important to verify available options for each API version and model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_29\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = openai.image('gpt-image-1', {\n  providerOptions: {\n    openai: { quality: 'high' },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Step Start Parts Rendering\nDESCRIPTION: Shows how to render boundaries between tool invocations using step-start parts in a React component.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nmessage.parts.map((part, index) => {\n  switch (part.type) {\n    case 'step-start':\n      // show step boundaries as horizontal lines:\n      return index > 0 ? (\n        <div key={index} className=\"text-gray-500\">\n          <hr className=\"my-2 border-gray-300\" />\n        </div>\n      ) : null;\n    case 'text':\n    // ...\n    case 'tool-invocation':\n    // ...\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Provider Registry Module\nDESCRIPTION: Simple import statement showing how to import the createProviderRegistry function from the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/40-provider-registry.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createProviderRegistry } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Installing Polyfill Dependencies\nDESCRIPTION: Package installation commands for required polyfill dependencies using different package managers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ungap/structured-clone @stardazed/streams-text-encoding\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ungap/structured-clone @stardazed/streams-text-encoding\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ungap/structured-clone @stardazed/streams-text-encoding\n```\n\n----------------------------------------\n\nTITLE: Streaming Text with Image Prompt API Route in Next.js\nDESCRIPTION: This server-side code handles the API request for streaming text with an image prompt. It processes the user's message and image URL, sends them to the GPT-4 model, and returns a streamed response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/22-stream-text-with-image-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport const maxDuration = 60;\n\nexport async function POST(req: Request) {\n  // 'data' contains the additional data that you have sent:\n  const { messages, data } = await req.json();\n\n  const initialMessages = messages.slice(0, -1);\n  const currentMessage = messages[messages.length - 1];\n\n  // Call the language model\n  const result = streamText({\n    model: openai('gpt-4-turbo'),\n    messages: [\n      ...initialMessages,\n      {\n        role: 'user',\n        content: [\n          { type: 'text', text: currentMessage.content },\n          { type: 'image', image: new URL(data.imageUrl) },\n        ],\n      },\n    ],\n  });\n\n  // Respond with the stream\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key in Environment Variables\nDESCRIPTION: Environment variable configuration for the OpenAI API key, which is required for authenticating requests to the OpenAI service.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/02-multi-modal-chatbot.mdx#2025-04-23_snippet_2\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=xxxxxxxxx\n```\n\n----------------------------------------\n\nTITLE: Importing StreamUI from AI SDK RSC\nDESCRIPTION: Shows how to import the streamUI function from the ai/rsc package for use in React Server Components.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/01-stream-ui.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { streamUI } from \"ai/rsc\"\n```\n\n----------------------------------------\n\nTITLE: Updating UI to Display Tool Invocations in React Native TSX\nDESCRIPTION: This snippet updates the UI component to display tool invocations in the chat interface. It checks each message for tool calls and displays them as stringified JSON, otherwise showing the message content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateAPIUrl } from '@/utils';\nimport { useChat } from '@ai-sdk/react';\nimport { fetch as expoFetch } from 'expo/fetch';\nimport { View, TextInput, ScrollView, Text, SafeAreaView } from 'react-native';\n\nexport default function App() {\n  const { messages, error, handleInputChange, input, handleSubmit } = useChat({\n    fetch: expoFetch as unknown as typeof globalThis.fetch,\n    api: generateAPIUrl('/api/chat'),\n    onError: error => console.error(error, 'ERROR'),\n  });\n\n  if (error) return <Text>{error.message}</Text>;\n\n  return (\n    <SafeAreaView style={{ height: '100vh' }}>\n      <View\n        style={{\n          height: '95%',\n          display: 'flex',\n          flexDirection: 'column',\n          paddingHorizontal: 8,\n        }}\n      >\n        <ScrollView style={{ flex: 1 }}>\n          {messages.map(m => (\n            <View key={m.id} style={{ marginVertical: 8 }}>\n              <View>\n                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\n                {m.toolInvocations ? (\n                  <Text>{JSON.stringify(m.toolInvocations, null, 2)}</Text>\n                ) : (\n                  <Text>{m.content}</Text>\n                )}\n              </View>\n            </View>\n          ))}\n        </ScrollView>\n\n        <View style={{ marginTop: 8 }}>\n          <TextInput\n            style={{ backgroundColor: 'white', padding: 8 }}\n            placeholder=\"Say something...\"\n            value={input}\n            onChange={e =>\n              handleInputChange({\n                ...e,\n                target: {\n                  ...e.target,\n                  value: e.nativeEvent.text,\n                },\n              } as unknown as React.ChangeEvent<HTMLInputElement>)\n            }\n            onSubmitEditing={e => {\n              handleSubmit(e);\n              e.preventDefault();\n            }}\n            autoFocus={true}\n          />\n        </View>\n      </View>\n    </SafeAreaView>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Using Custom AI SDK Provider for Text Generation in TypeScript\nDESCRIPTION: Demonstrates how a user would consume the custom provider package (`@company-name/example`). It imports the default provider instance (`example`) and uses it with the `generateText` function from the core `ai` package to interact with a specific chat model (`example/chat-model-1`) defined within the custom provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { example } from '@company-name/example';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: example('example/chat-model-1'),\n  prompt: 'Hello, how are you?',\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with an OpenAI Compatible Provider in TypeScript\nDESCRIPTION: Provides a basic example of using the `generateText` function from the 'ai' package with a model created using `createOpenAICompatible`. It configures the provider with a base URL, name, and API key (from environment variables) to connect to a specific OpenAI-compatible API endpoint (e.g., Llama 3 via a custom host) and generate text based on a prompt.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/openai-compatible/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: createOpenAICompatible({\n    baseURL: 'https://api.example.com/v1',\n    name: 'example',\n    apiKey: process.env.MY_API_KEY,\n  }).chatModel('meta-llama/Llama-3-70b-chat-hf'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Azure OpenAI Provider Instance\nDESCRIPTION: Illustrates how to create a customized Azure OpenAI provider instance using the 'createAzure' factory function. This allows specifying configuration options like resource name and API key directly in the code, overriding default environment variable lookups.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAzure } from '@ai-sdk/azure';\n\nconst azure = createAzure({\n  resourceName: 'your-resource-name', // Azure resource name\n  apiKey: 'your-api-key',\n});\n```\n\n----------------------------------------\n\nTITLE: Processing URL-based PDF with Claude 3.5 Sonnet Model in TypeScript\nDESCRIPTION: This code snippet shows how to use the Claude 3.5 Sonnet model to process a PDF file from a URL. It demonstrates passing the PDF file as part of the message content and asking a question about its contents.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: anthropic('claude-3-5-sonnet-20241022'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'What is an embedding model according to this document?',\n        },\n        {\n          type: 'file',\n          data: new URL(\n            'https://github.com/vercel/ai/blob/main/examples/ai-core/data/ai.pdf?raw=true',\n          ),\n          mimeType: 'application/pdf',\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Updating Query Viewer to Display SQL Explanations in TypeScript\nDESCRIPTION: Modifies the handleExplainQuery function in the query viewer component to use the explainQuery server action and display the results.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst handleExplainQuery = async () => {\n  setQueryExpanded(true);\n  setLoadingExplanation(true);\n\n  const explanations = await explainQuery(inputValue, activeQuery);\n  setQueryExplanations(explanations);\n\n  setLoadingExplanation(false);\n};\n```\n\n----------------------------------------\n\nTITLE: Defining Schema for Notifications using Zod in TypeScript\nDESCRIPTION: This snippet demonstrates how to define a schema for notifications using the Zod library. It creates a structure for an array of notifications, each containing a name and a message.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/08-object-generation.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from 'zod';\n\n// define a schema for the notifications\nexport const notificationSchema = z.object({\n  notifications: z.array(\n    z.object({\n      name: z.string().describe('Name of a fictional person.'),\n      message: z.string().describe('Message. Do not use emojis or links.'),\n    }),\n  ),\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Fireworks Image Model\nDESCRIPTION: Example showing how to generate and save images using the Fireworks image model with the Flux-1 model.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/fireworks/README.md#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fireworks } from '@ai-sdk/fireworks';\nimport { experimental_generateImage as generateImage } from 'ai';\nimport fs from 'fs';\n\nconst { image } = await generateImage({\n  model: fireworks.image('accounts/fireworks/models/flux-1-dev-fp8'),\n  prompt: 'A serene mountain landscape at sunset',\n});\nconst filename = `image-${Date.now()}.png`;\nfs.writeFileSync(filename, image.uint8Array);\nconsole.log(`Image saved to ${filename}`);\n```\n\n----------------------------------------\n\nTITLE: Handling Custom Request Body in Server-Side API Route (TypeScript)\nDESCRIPTION: This snippet demonstrates how to adjust the server-side API route to handle the custom request body sent by the useChat hook. It shows how to receive only the text of the last message and combine it with the message history loaded from storage.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/80-send-custom-body-from-use-chat.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai'\nimport { streamText } from 'ai'\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30\n\nexport async function POST(req: Request) {\n  // we receive only the text from the last message\n  const text = await req.json()\n\n  // e.g. load message history from storage\n  const history = await loadHistory()\n\n  // Call the language model\n  const result = streamText({\n    model: openai('gpt-4-turbo'),\n    messages: [...history, { role: 'user', content: text }]\n    onFinish({ text }) {\n      // e.g. save the message and the response to storage\n    }\n  })\n\n  // Respond with the stream\n  return result.toDataStreamResponse()\n}\n```\n\n----------------------------------------\n\nTITLE: Error Callback Implementation in React\nDESCRIPTION: Example of implementing an error callback function with the useChat hook to handle errors programmatically.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/21-error-handling.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const {\n    /* ... */\n  } = useChat({\n    // handle error:\n    onError: error => {\n      console.error(error);\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Error Handler Function in TypeScript\nDESCRIPTION: A utility function that formats different types of errors into readable string messages. This function handles null values, string errors, Error objects, and other error types by converting them to appropriate string representations.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/12-use-chat-an-error-occurred.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nexport function errorHandler(error: unknown) {\n  if (error == null) {\n    return 'unknown error';\n  }\n\n  if (typeof error === 'string') {\n    return error;\n  }\n\n  if (error instanceof Error) {\n    return error.message;\n  }\n\n  return JSON.stringify(error);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Bedrock Embedding Model in TypeScript\nDESCRIPTION: Shows the basic usage of the `bedrock.embedding()` factory method from the `@ai-sdk/amazon-bedrock` package to create an instance for a specific Amazon Bedrock embedding model (`amazon.titan-embed-text-v1`). This model instance can then be used for generating embeddings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_15\n\nLANGUAGE: ts\nCODE:\n```\nconst model = bedrock.embedding('amazon.titan-embed-text-v1');\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-Side Array Streaming with OpenAI\nDESCRIPTION: This server-side route demonstrates how to use the streamObject function in array output mode. It generates an array of notifications using OpenAI's GPT-4 model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/40-stream-object.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { notificationSchema } from './schema';\n\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const context = await req.json();\n\n  const result = streamObject({\n    model: openai('gpt-4-turbo'),\n    output: 'array',\n    schema: notificationSchema,\n    prompt:\n      `Generate 3 notifications for a messages app in this context:` + context,\n  });\n\n  return result.toTextStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Basic LLM Query Example\nDESCRIPTION: Demonstrates a basic query to an LLM without context, showing its limitation in knowing personal information.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n**input**\nWhat is my favorite food?\n\n**generation**\nI don't have access to personal information about individuals, including their\nfavorite foods.\n```\n\n----------------------------------------\n\nTITLE: Adding Loading State to useObject Hook Implementation in React\nDESCRIPTION: This React component enhances the previous example by adding a loading state. It disables the submit button and shows a spinner while the notifications are being generated.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/08-object-generation.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useObject } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { isLoading, object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n  });\n\n  return (\n    <>\n      {isLoading && <Spinner />}\n\n      <button\n        onClick={() => submit('Messages during finals week.')}\n        disabled={isLoading}\n      >\n        Generate notifications\n      </button>\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Defining TextPart Interface in TypeScript\nDESCRIPTION: Defines the structure for a text content part of a prompt. It includes a type of 'text' and a text string.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/30-core-message.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nexport interface TextPart {\n  type: 'text';\n  /**\n   * The text content.\n   */\n  text: string;\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Azure OpenAI Language Model\nDESCRIPTION: Demonstrates how to create an Azure OpenAI language model instance by invoking the provider instance function with the Azure deployment name. This model object can then be used with AI SDK Core functions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = azure('your-deployment-name');\n```\n\n----------------------------------------\n\nTITLE: Installing Cerebras Provider Package\nDESCRIPTION: Command line instructions for installing the Cerebras provider package using different package managers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/40-cerebras.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/cerebras\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/cerebras\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/cerebras\n```\n\n----------------------------------------\n\nTITLE: Capturing Step Finish Event and Metadata - TypeScript\nDESCRIPTION: Defines the structure for marking the completion of a processing step, including log probabilities, request/response metadata, provider warnings, token usage, and finish reasons. It covers optional and required fields, supporting detailed tracking of language model interactions and output post-processing. Dependencies span types for logs, tokens, and provider details; consumers must handle both required and optional object fields.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_23\n\nLANGUAGE: TypeScript\nCODE:\n```\nparameters: [\n  {\n    name: 'type',\n    type: '\\'step-finish\\'',\n    description: 'Indicates the end of the current step in the stream.',\n  },\n  {\n    name: 'messageId',\n    type: 'string',\n    description: 'The ID of the assistant message that ended the step.',\n  },\n  {\n    name: 'logprobs',\n    type: 'LogProbs',\n    isOptional: true,\n    description: 'Optional log probabilities for tokens returned by some providers.',\n  },\n  {\n    name: 'request',\n    type: 'RequestMetadata',\n    description: 'Information about the request that was sent to the language model provider.',\n    properties: [\n      {\n        type: 'RequestMetadata',\n        parameters: [\n          {\n            name: 'body',\n            type: 'string',\n            description: 'Raw request HTTP body that was sent to the provider API as a string.',\n          }\n        ]\n      }\n    ]\n  },\n  {\n    name: 'warnings',\n    type: 'Warning[]',\n    isOptional: true,\n    description: 'Warnings from the model provider (e.g. unsupported settings).',\n  },\n  {\n    name: 'response',\n    type: 'ResponseMetadata',\n    description: 'Response metadata from the language model provider.',\n    properties: [\n      {\n        type: 'ResponseMetadata',\n        parameters: [\n          {\n            name: 'id',\n            type: 'string',\n            description: 'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.',\n          },\n          {\n            name: 'model',\n            type: 'string',\n            description: 'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.',\n          },\n          {\n            name: 'timestamp',\n            type: 'Date',\n            description: 'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.',\n          },\n          {\n            name: 'headers',\n            type: 'Record<string, string>',\n            description: 'The response headers.',\n          }\n        ]\n      }\n    ]\n  },\n  {\n    name: 'usage',\n    type: 'TokenUsage',\n    description: 'The token usage of the generated text.',\n    properties: [\n      {\n        type: 'TokenUsage',\n        parameters: [\n          {\n            name: 'promptTokens',\n            type: 'number',\n            description: 'The total number of tokens in the prompt.',\n          },\n          {\n            name: 'completionTokens',\n            type: 'number',\n            description: 'The total number of tokens in the completion.',\n          },\n          {\n            name: 'totalTokens',\n            type: 'number',\n            description: 'The total number of tokens generated.',\n          }\n        ]\n      }\n    ]\n  },\n  {\n    name: 'finishReason',\n    type: '\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'',\n    description: 'The reason the model finished generating the text.',\n  },\n  {\n    name: 'providerMetadata',\n    type: 'Record<string,Record<string,JSONValue>> | undefined',\n    isOptional: true,\n    description: 'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.',\n  },\n  {\n    name: 'isContinued',\n    type: 'boolean',\n    description: 'True when there will be a continuation step with a continuation text.',\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Logic for Parallel Tool Calls in React\nDESCRIPTION: This code snippet shows the client-side implementation of a React component that handles user input and displays conversation history. It uses React hooks and server actions to manage state and trigger text generation with parallel tool calls.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/51-call-tools-in-parallel.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { Message, continueConversation } from './actions';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [conversation, setConversation] = useState<Message[]>([]);\n  const [input, setInput] = useState<string>('');\n\n  return (\n    <div>\n      <div>\n        {conversation.map((message, index) => (\n          <div key={index}>\n            {message.role}: {message.content}\n          </div>\n        ))}\n      </div>\n\n      <div>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button\n          onClick={async () => {\n            const { messages } = await continueConversation([\n              ...conversation,\n              { role: 'user', content: input },\n            ]);\n\n            setConversation(messages);\n          }}\n        >\n          Send Message\n        </button>\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Tools with and without Confirmation Requirements\nDESCRIPTION: This code defines AI tools with different confirmation requirements. The getWeatherInformation tool requires human confirmation (no execute function), while getLocalTime includes an execute function that runs automatically without confirmation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#2025-04-23_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { tool } from 'ai';\nimport { z } from 'zod';\n\nconst getWeatherInformation = tool({\n  description: 'show the weather in a given city to the user',\n  parameters: z.object({ city: z.string() }),\n  // no execute function, we want human in the loop\n});\n\nconst getLocalTime = tool({\n  description: 'get the local time for a specified location',\n  parameters: z.object({ location: z.string() }),\n  // including execute function -> no confirmation required\n  execute: async ({ location }) => {\n    console.log(`Getting local time for ${location}`);\n    return '10am';\n  },\n});\n\nexport const tools = {\n  getWeatherInformation,\n  getLocalTime,\n};\n```\n\n----------------------------------------\n\nTITLE: Generating Multiple Images - AI SDK\nDESCRIPTION: This snippet demonstrates how to generate multiple images at once using the `n` parameter.  The `n` parameter specifies the number of images to generate. The SDK automatically calls the model as needed to fulfill the request, managing batching if necessary. This requires the `ai` and a model provider like `@ai-sdk/openai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { images } = await generateImage({\n  model: openai.image('dall-e-2'),\n  prompt: 'Santa Claus driving a Cadillac',\n  n: 4, // number of images to generate\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for AI SDK\nDESCRIPTION: Sets up the necessary environment variables for using the AI SDK, particularly the OpenAI API key. This step is crucial for authenticating with the OpenAI service.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Creating New Chat with Next.js Route Handler\nDESCRIPTION: Implements a page route that creates a new chat and redirects to the chat page with the generated ID.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-message-persistence.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { redirect } from 'next/navigation';\nimport { createChat } from '@tools/chat-store';\n\nexport default async function Page() {\n  const id = await createChat(); // create a new chat\n  redirect(`/chat/${id}`); // redirect to chat page, see below\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring a Google Vertex AI Text Embedding Model in TypeScript\nDESCRIPTION: Shows how to create a Google Vertex AI text embedding model ('text-embedding-004') with additional settings. An options object is passed to `.textEmbeddingModel()` to specify parameters like `outputDimensionality`, which truncates the embedding to the desired number of dimensions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = vertex.textEmbeddingModel('text-embedding-004', {\n  outputDimensionality: 512, // optional, number of dimensions for the embedding\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Google Vertex Anthropic Provider for Edge Runtimes in TypeScript\nDESCRIPTION: Illustrates creating a customized Google Vertex Anthropic provider instance specifically for Edge runtimes using the `createVertexAnthropic` factory function from the `@ai-sdk/google-vertex/anthropic/edge` module. Optional settings like `project` and `location` can be provided.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_23\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createVertexAnthropic } from '@ai-sdk/google-vertex/anthropic/edge';\n\nconst vertexAnthropic = createVertexAnthropic({\n  project: 'my-project', // optional\n  location: 'us-central1', // optional\n});\n```\n\n----------------------------------------\n\nTITLE: Importing createAI Function from AI SDK RSC\nDESCRIPTION: Shows how to import the createAI function from the ai/rsc package for use in applications.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/02-create-ai.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createAI } from \"ai/rsc\"\n```\n\n----------------------------------------\n\nTITLE: Creating Stock Information Component in React\nDESCRIPTION: This component fetches and displays stock information for a given symbol and number of months. It's used as a visual element in the chat interface when stock information is requested.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/90-render-visual-interface-in-chat.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nexport async function Stock({ symbol, numOfMonths }) {\n  const data = await fetch(\n    `https://api.example.com/stock/${symbol}/${numOfMonths}`,\n  );\n\n  return (\n    <div>\n      <div>{symbol}</div>\n\n      <div>\n        {data.timeline.map(data => (\n          <div>\n            <div>{data.date}</div>\n            <div>{data.value}</div>\n          </div>\n        ))}\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating LangfuseExporter in Node.js Application (TypeScript)\nDESCRIPTION: This code demonstrates how to set up LangfuseExporter in a Node.js application using the OpenTelemetry SDK. It includes configuration for the exporter and an example of generating text with telemetry enabled.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langfuse.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\nimport { NodeSDK } from '@opentelemetry/sdk-node';\nimport { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';\nimport { LangfuseExporter } from 'langfuse-vercel';\n\nconst sdk = new NodeSDK({\n  traceExporter: new LangfuseExporter(),\n  instrumentations: [getNodeAutoInstrumentations()],\n});\n\nsdk.start();\n\nasync function main() {\n  const result = await generateText({\n    model: openai('gpt-4o'),\n    maxTokens: 50,\n    prompt: 'Invent a new holiday and describe its traditions.',\n    experimental_telemetry: {\n      isEnabled: true,\n      functionId: 'my-awesome-function',\n      metadata: {\n        something: 'custom',\n        someOtherThing: 'other-value',\n      },\n    },\n  });\n\n  console.log(result.text);\n\n  await sdk.shutdown(); // Flushes the trace to Langfuse\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Importing createDataStream from AI SDK\nDESCRIPTION: Shows how to import the createDataStream function from the AI package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/40-create-data-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createDataStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Using createDataStream with Multiple Stream Operations\nDESCRIPTION: Demonstrates creating a data stream with writing data, annotations, and merging streams. Shows the complete usage pattern including error handling and stream execution.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/40-create-data-stream.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nconst stream = createDataStream({\n  async execute(dataStream) {\n    // Write data\n    dataStream.writeData({ value: 'Hello' });\n\n    // Write annotation\n    dataStream.writeMessageAnnotation({ type: 'status', value: 'processing' });\n\n    // Merge another stream\n    const otherStream = getAnotherStream();\n    dataStream.merge(otherStream);\n  },\n  onError: error => `Custom error: ${error.message}`,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI-powered Chat API Route in TypeScript\nDESCRIPTION: This code creates an API endpoint that handles chat requests by streaming responses from OpenAI's GPT-4o model. It uses the streamText function from the AI SDK to manage the streaming response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse({\n    headers: {\n      'Content-Type': 'application/octet-stream',\n      'Content-Encoding': 'none',\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Speech from Text using OpenAI Speech Model in TypeScript\nDESCRIPTION: This code sample demonstrates end-to-end text-to-speech generation by importing experimental_generateSpeech from 'ai' and creating a speech model instance with 'tts-1'. The example submits the text prompt 'Hello, world!' and passes an (empty) providerOptions object. Proper setup requires installation of '@ai-sdk/openai' and conforming input types for speech synthesis.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_33\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst result = await generateSpeech({\n  model: openai.speech('tts-1'),\n  text: 'Hello, world!',\n  providerOptions: { openai: {} },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Tool Call Streaming in API Route\nDESCRIPTION: Sets up tool call streaming in a Next.js API route by enabling the toolCallStreaming option in streamText configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nexport async function POST(req: Request) {\n  // ...\n\n  const result = streamText({\n    toolCallStreaming: true,\n    // ...\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Multiple Transformations in streamText Function\nDESCRIPTION: This snippet demonstrates how to apply multiple transformations to the streamText function. The transformations are applied in the order they are provided in the experimental_transform array.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_11\n\nLANGUAGE: tsx\nCODE:\n```\nconst result = streamText({\n  model,\n  prompt,\n  experimental_transform: [firstTransform, secondTransform],\n});\n```\n\n----------------------------------------\n\nTITLE: Updating Frontend to Use SQL Query Generation in TypeScript\nDESCRIPTION: Updates the handleSubmit function in the frontend to use the generateQuery server action and display the results.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst handleSubmit = async (suggestion?: string) => {\n  clearExistingData();\n\n  const question = suggestion ?? inputValue;\n  if (inputValue.length === 0 && !suggestion) return;\n\n  if (question.trim()) {\n    setSubmitted(true);\n  }\n\n  setLoading(true);\n  setLoadingStep(1);\n  setActiveQuery('');\n\n  try {\n    const query = await generateQuery(question);\n\n    if (query === undefined) {\n      toast.error('An error occurred. Please try again.');\n      setLoading(false);\n      return;\n    }\n\n    setActiveQuery(query);\n    setLoadingStep(2);\n\n    const companies = await runGeneratedSQLQuery(query);\n    const columns = companies.length > 0 ? Object.keys(companies[0]) : [];\n    setResults(companies);\n    setColumns(columns);\n\n    setLoading(false);\n  } catch (e) {\n    toast.error('An error occurred. Please try again.');\n    setLoading(false);\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Tool Call Repair Using Structured Outputs\nDESCRIPTION: Demonstrates tool call repair using a model with structured outputs to fix invalid tool arguments.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject, generateText, NoSuchToolError, tool } from 'ai';\n\nconst result = await generateText({\n  model,\n  tools,\n  prompt,\n\n  experimental_repairToolCall: async ({\n    toolCall,\n    tools,\n    parameterSchema,\n    error,\n  }) => {\n    if (NoSuchToolError.isInstance(error)) {\n      return null; // do not attempt to fix invalid tool names\n    }\n\n    const tool = tools[toolCall.toolName as keyof typeof tools];\n\n    const { object: repairedArgs } = await generateObject({\n      model: openai('gpt-4o', { structuredOutputs: true }),\n      schema: tool.parameters,\n      prompt: [\n        `The model tried to call the tool \"${toolCall.toolName}\"` +\n          ` with the following arguments:`,\n        JSON.stringify(toolCall.args),\n        `The tool accepts the following schema:`,\n        JSON.stringify(parameterSchema(toolCall)),\n        'Please fix the arguments.',\n      ].join('\\n'),\n    });\n\n    return { ...toolCall, args: JSON.stringify(repairedArgs) };\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text Using Portkey and Vercel AI SDK - JavaScript\nDESCRIPTION: This JavaScript code demonstrates how to use Portkey with the Vercel AI SDK's generateText function. After importing createPortkey and generateText, the snippet creates a Portkey provider with the user's API key and configuration. The generateText function is called with a chatModel from Portkey and a user prompt, returning generated text. Requires both @portkey-ai/vercel-provider and ai packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/10-portkey.mdx#2025-04-23_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPortkey } from '@portkey-ai/vercel-provider';\nimport { generateText } from 'ai';\n\nconst portkey = createPortkey({\n  apiKey: 'YOUR_PORTKEY_API_KEY',\n  config: portkeyConfig,\n});\n\nconst { text } = await generateText({\n  model: portkey.chatModel(''),\n  prompt: 'What is Portkey?',\n});\n\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Flight List Component with useActions Hook\nDESCRIPTION: Legacy implementation of a flight list component using useActions and useUIState hooks from AI SDK RSC.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useActions, useUIState } from 'ai/rsc';\n\nexport function ListFlights({ flights }) {\n  const { sendMessage } = useActions();\n  const [_, setMessages] = useUIState();\n\n  return (\n    <div>\n      {flights.map(flight => (\n        <div\n          key={flight.id}\n          onClick={async () => {\n            const response = await sendMessage(\n              `I would like to choose flight ${flight.id}!`,\n            );\n\n            setMessages(msgs => [...msgs, response]);\n          }}\n        >\n          {flight.name}\n        </div>\n      ))}\n    </div>\n  );\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangSmith and OpenAI\nDESCRIPTION: Bash commands to set required environment variables for LangSmith tracing and OpenAI API key.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport LANGCHAIN_TRACING_V2=true\nexport LANGCHAIN_API_KEY=<your-api-key>\n\nexport OPENAI_API_KEY=<your-openai-api-key> # The examples use OpenAI (replace with your selected provider)\n```\n\n----------------------------------------\n\nTITLE: Streaming Text and Accessing Response Headers with Bedrock in TypeScript\nDESCRIPTION: Demonstrates using the `streamText` function from the 'ai' package with a Bedrock model to stream text responses. It also shows how to access the response headers after the stream has completed by awaiting `result.response`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_23\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\nimport { streamText } from 'ai';\n\nconst result = streamText({\n  model: bedrock('meta.llama3-70b-instruct-v1:0'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\nconsole.log('Response headers:', (await result.response).headers);\n```\n\n----------------------------------------\n\nTITLE: Implementing Error Callback for Stream Handling (TypeScript)\nDESCRIPTION: This snippet documents the onError callback for AI streaming, designed in TypeScript to handle errors raised during a streaming session. It defines an event carrying an unknown error field, allowing implementers to log or respond to stream errors as needed. The callback is optional, expects an 'event' with an 'error' property, and returns nothing unless used asynchronously.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  name: 'onError',\n  type: '(event: OnErrorResult) => Promise<void> |void',\n  isOptional: true,\n  description:\n    'Callback that is called when an error occurs during streaming. You can use it to log errors.',\n  properties: [\n    {\n      type: 'OnErrorResult',\n      parameters: [\n        {\n          name: 'error',\n          type: 'unknown',\n          description: 'The error that occurred.',\n        },\n      ],\n    },\n  ],\n},\n```\n\n----------------------------------------\n\nTITLE: Importing jsonSchema Function\nDESCRIPTION: Simple import statement showing how to import the jsonSchema function from the ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/25-json-schema.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { jsonSchema } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating a Google Vertex AI Text Embedding Model in TypeScript\nDESCRIPTION: Demonstrates how to create a Google Vertex AI text embedding model instance using the `.textEmbeddingModel()` factory method from the `vertex` object. This specifies the 'text-embedding-004' model ID.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = vertex.textEmbeddingModel('text-embedding-004');\n```\n\n----------------------------------------\n\nTITLE: Grouping Multiple Executions in One Langfuse Trace (TypeScript)\nDESCRIPTION: This snippet illustrates how to group multiple AI SDK executions under a single Langfuse trace. It creates a parent trace and links multiple text generation calls to it, demonstrating hierarchical tracing.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langfuse.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { randomUUID } from 'crypto';\nimport { Langfuse } from 'langfuse';\n\nconst langfuse = new Langfuse();\nconst parentTraceId = randomUUID();\n\nlangfuse.trace({\n  id: parentTraceId,\n  name: 'holiday-traditions',\n});\n\nfor (let i = 0; i < 3; i++) {\n  const result = await generateText({\n    model: openai('gpt-3.5-turbo'),\n    maxTokens: 50,\n    prompt: 'Invent a new holiday and describe its traditions.',\n    experimental_telemetry: {\n      isEnabled: true,\n      functionId: `holiday-tradition-${i}`,\n      metadata: {\n        langfuseTraceId: parentTraceId,\n        langfuseUpdateParent: false, // Do not update the parent trace with execution results\n      },\n    },\n  });\n\n  console.log(result.text);\n}\n\nawait langfuse.flushAsync();\nawait sdk.shutdown();\n```\n\n----------------------------------------\n\nTITLE: Defining AI Full Stream Event Interface - TypeScript\nDESCRIPTION: This snippet defines an event stream type combining AsyncIterable and ReadableStream for objects representing detailed streaming events (TextStreamPart), including text deltas, reasoning, file events, redactions, and more. The structure establishes the API contract for consuming full lifecycle AI events in real-time. Inputs are event objects conforming to the TextStreamPart structure; outputs are emitted event objects or errors if a network interruption occurs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\n    {\n      name: 'fullStream',\n      type: 'AsyncIterable<TextStreamPart> & ReadableStream<TextStreamPart>',\n      description:\n        'A stream with all events, including text deltas, tool calls, tool results, and errors. You can use it as either an AsyncIterable or a ReadableStream. Only errors that stop the stream, such as network errors, are thrown.',\n      properties: [\n        {\n          type: 'TextStreamPart',\n          parameters: [\n            {\n              name: 'type',\n              type: \"'text-delta'\",\n              description: 'The type to identify the object as text delta.',\n            },\n            {\n              name: 'textDelta',\n              type: 'string',\n              description: 'The text delta.',\n            },\n          ],\n        },\n        {\n          type: 'TextStreamPart',\n          parameters: [\n            {\n              name: 'type',\n              type: \"'reasoning'\",\n              description: 'The type to identify the object as reasoning.',\n            },\n            {\n              name: 'textDelta',\n              type: 'string',\n              description: 'The reasoning text delta.',\n            },\n          ],\n        },\n        {\n          type: 'TextStreamPart',\n          parameters: [\n            {\n              name: 'type',\n              type: \"'reasoning-signature'\",\n              description:\n                'The signature for the previous reasoning chunks. Send by some providers.',\n            },\n            {\n              name: 'signature',\n              type: 'string',\n              description: 'The signature.',\n            },\n          ],\n        },\n        {\n          type: 'TextStreamPart',\n          parameters: [\n            {\n              name: 'type',\n              type: \"'redacted-reasoning'\",\n              description:\n                'The type to identify the object as redacted reasoning.',\n            },\n            {\n              name: 'data',\n              type: 'string',\n              description: 'The redacted data.',\n            },\n          ],\n        },\n        {\n          type: 'TextStreamPart',\n          parameters: [\n            {\n              name: 'type',\n              type: \"'source'\",\n              description: 'The type to identify the object as source.',\n            },\n            {\n              name: 'source',\n              type: 'Source',\n              description: 'The source.',\n            },\n          ],\n        },\n        {\n          type: 'TextStreamPart',\n          parameters: [\n            {\n              name: 'type',\n              type: \"'file'\",\n              description: 'The type to identify the object as file.',\n            },\n            {\n              name: 'base64',\n              type: 'string',\n              description: 'File as a base64 encoded string.',\n            },\n            {\n              name: 'uint8Array',\n              type: 'Uint8Array',\n```\n\n----------------------------------------\n\nTITLE: Using Streaming Middleware with Language Model\nDESCRIPTION: Complete example showing how to integrate the streaming middleware with a non-streaming language model to enable streaming functionality. Demonstrates wrapping the model and processing the stream chunks.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/67-simulate-streaming-middleware.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { wrapLanguageModel } from 'ai';\nimport { simulateStreamingMiddleware } from 'ai';\n\n// Example with a non-streaming model\nconst result = streamText({\n  model: wrapLanguageModel({\n    model: nonStreamingModel,\n    middleware: simulateStreamingMiddleware(),\n  }),\n  prompt: 'Your prompt here',\n});\n\n// Now you can use the streaming interface\nfor await (const chunk of result.fullStream) {\n  // Process streaming chunks\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Google topK Setting Migration - TypeScript\nDESCRIPTION: Shows the migration of the topK parameter from model-specific settings inside the google model factory to a top-level option in generateText. Inputs: google model identifier; outputs: generated text object. Applies to Google Generative AI provider in AI SDK 4.0+.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_5\n\nLANGUAGE: ts\nCODE:\n```\nconst result = await generateText({\n  model: google('gemini-1.5-flash', {\n    topK: 0.5,\n  }),\n});\n```\n\nLANGUAGE: ts\nCODE:\n```\nconst result = await generateText({\n  model: google('gemini-1.5-flash'),\n  topK: 0.5,\n});\n```\n\n----------------------------------------\n\nTITLE: Forcing Web Search via Response with Web Search Tool\nDESCRIPTION: This snippet demonstrates how to invoke web search within response generation by setting the 'toolChoice' parameter to 'web_search_preview', including optional configurations such as search context and user location. Useful for real-time information retrieval.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_20\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst result = await generateText({\n  model: openai.responses('gpt-4o-mini'),\n  prompt: 'What happened in San Francisco last week?',\n  tools: {\n    web_search_preview: openai.tools.webSearchPreview({\n      // optional configuration:\n      searchContextSize: 'high',\n      userLocation: {\n        type: 'approximate',\n        city: 'San Francisco',\n        region: 'California',\n      },\n    }),\n  },\n  // Force web search tool:\n  toolChoice: { type: 'tool', toolName: 'web_search_preview' },\n});\n\n// URL sources\nconst sources = result.sources;\n```\n\n----------------------------------------\n\nTITLE: Generating Text with DeepInfra LLM\nDESCRIPTION: Complete example showing how to generate text using the DeepInfra provider with the Llama-3 model\nSOURCE: https://github.com/vercel/ai/blob/main/packages/deepinfra/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepinfra } from '@ai-sdk/deepinfra';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: deepinfra('meta-llama/Llama-3.3-70B-Instruct'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Source Reference Structure for Chat Messages (TypeScript)\nDESCRIPTION: This snippet provides the 'Source' interface representing a source reference in a chat message, including sourceType (currently only 'url'), a unique ID, the source URL, and an optional title. Essential for tracking message citations, inputs/outputs are basic objects that must conform to this structure. Dependency: none except consistency of ID and URL. Limitation: Only 'url' sources are supported by default.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#2025-04-23_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\ninterface Source {\n  sourceType: 'url'; // Type of the source.\n  id: string; // ID of the source.\n  url: string; // URL reference.\n  title?: string; // Optional title.\n}\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up AI Context for React Server Components\nDESCRIPTION: Creates an AI context using the createAI function from ai/rsc. This context provides the submitMessage action to all child components and initializes the AI and UI state.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/120-stream-assistant-response.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nimport { createAI } from 'ai/rsc';\nimport { submitMessage } from './actions';\n\nexport const AI = createAI({\n  actions: {\n    submitMessage,\n  },\n  initialAIState: [],\n  initialUIState: [],\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Metadata to LangSmith Traces with Vercel AI SDK\nDESCRIPTION: This code snippet demonstrates how to add custom metadata to traces when using the Vercel AI SDK with LangSmith. The example shows generating text with OpenAI's GPT-4o-mini model while attaching user ID and language preference metadata to the trace record.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AISDKExporter } from 'langsmith/vercel';\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nawait generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  experimental_telemetry: AISDKExporter.getSettings({\n    metadata: { userId: '123', language: 'english' },\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Spark Provider - TypeScript\nDESCRIPTION: Shows how to instantiate the Spark provider, select a model, and generate text asynchronously using 'generateText'. Replace the empty apiKey string with your credentials. The function takes a provider instance for a specific model and a prompt, returning generated text. Requires both 'spark-ai-provider' and 'ai' (for generateText). Input parameters include API key, model identifier, and prompt string; output includes the generated text. Ensure the necessary packages are installed and required credentials are provided.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/92-spark.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createSparkProvider } from './index.mjs';\nimport { generateText } from 'ai';\nconst spark = createSparkProvider({\n  apiKey: '',\n});\nconst { text } = await generateText({\n  model: spark('lite'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Single Notification Schema for Array Mode\nDESCRIPTION: This snippet defines a Zod schema for a single notification, used in the array output mode. It includes fields for name and message.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/40-stream-object.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from 'zod';\n\n// define a schema for a single notification\nexport const notificationSchema = z.object({\n  name: z.string().describe('Name of a fictional person.'),\n  message: z.string().describe('Message. Do not use emojis or links.'),\n});\n```\n\n----------------------------------------\n\nTITLE: Sending Custom Data with pipeDataStreamToResponse in Express\nDESCRIPTION: This example shows how to send custom data to the client using pipeDataStreamToResponse in an Express server. It demonstrates writing initial data before streaming AI-generated text and handling errors during the streaming process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/20-express.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { pipeDataStreamToResponse, streamText } from 'ai';\nimport express, { Request, Response } from 'express';\n\nconst app = express();\n\napp.post('/stream-data', async (req: Request, res: Response) => {\n  // immediately start streaming the response\n  pipeDataStreamToResponse(res, {\n    execute: async dataStreamWriter => {\n      dataStreamWriter.writeData('initialized call');\n\n      const result = streamText({\n        model: openai('gpt-4o'),\n        prompt: 'Invent a new holiday and describe its traditions.',\n      });\n\n      result.mergeIntoDataStream(dataStreamWriter);\n    },\n    onError: error => {\n      // Error messages are masked by default for security reasons.\n      // If you want to expose the error message to the client, you can do so here:\n      return error instanceof Error ? error.message : String(error);\n    },\n  });\n});\n\napp.listen(8080, () => {\n  console.log(`Example app listening on port ${8080}`);\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Azure OpenAI via Vercel AI SDK (TypeScript)\nDESCRIPTION: This example shows how to use the imported `azure` provider instance with the `generateText` function from the `ai` package. It configures the request to use a specific Azure OpenAI deployment (identified by 'gpt-4o' in this case, which should be replaced with your actual deployment name) and provides a text prompt. The function asynchronously returns the generated text.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/azure/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { azure } from '@ai-sdk/azure';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: azure('gpt-4o'), // your deployment name\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Retrieving Source Citations from Perplexity Responses (TypeScript)\nDESCRIPTION: Shows how to access the sources property from the generateText result using the Perplexity model, allowing users to inspect web sources cited in the response. This aids research and fact-checking by revealing model-grounded references. Requires both @ai-sdk/perplexity and ai modules; model input and standard provider configuration apply.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/70-perplexity.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { perplexity } from '@ai-sdk/perplexity';\nimport { generateText } from 'ai';\n\nconst { text, sources } = await generateText({\n  model: perplexity('sonar-pro'),\n  prompt: 'What are the latest developments in quantum computing?',\n});\n\nconsole.log(sources);\n```\n\n----------------------------------------\n\nTITLE: Handling App Mentions in Slack with TypeScript\nDESCRIPTION: This function processes app mentions in Slack. It checks if the message is from a bot, creates a status updater, retrieves thread history if applicable, generates an AI response, and updates the initial message with the response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/03-slackbot.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AppMentionEvent } from '@slack/web-api';\nimport { client, getThread } from './slack-utils';\nimport { generateResponse } from './ai';\n\nconst updateStatusUtil = async (\n  initialStatus: string,\n  event: AppMentionEvent,\n) => {\n  const initialMessage = await client.chat.postMessage({\n    channel: event.channel,\n    thread_ts: event.thread_ts ?? event.ts,\n    text: initialStatus,\n  });\n\n  if (!initialMessage || !initialMessage.ts)\n    throw new Error('Failed to post initial message');\n\n  const updateMessage = async (status: string) => {\n    await client.chat.update({\n      channel: event.channel,\n      ts: initialMessage.ts as string,\n      text: status,\n    });\n  };\n  return updateMessage;\n};\n\nexport async function handleNewAppMention(\n  event: AppMentionEvent,\n  botUserId: string,\n) {\n  console.log('Handling app mention');\n  if (event.bot_id || event.bot_id === botUserId || event.bot_profile) {\n    console.log('Skipping app mention');\n    return;\n  }\n\n  const { thread_ts, channel } = event;\n  const updateMessage = await updateStatusUtil('is thinking...', event);\n\n  if (thread_ts) {\n    const messages = await getThread(channel, thread_ts, botUserId);\n    const result = await generateResponse(messages, updateMessage);\n    updateMessage(result);\n  } else {\n    const result = await generateResponse(\n      [{ role: 'user', content: event.text }],\n      updateMessage,\n    );\n    updateMessage(result);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Usage Example with OpenAI Model\nDESCRIPTION: Complete example showing how to use defaultSettingsMiddleware with a wrapped language model, including default settings configuration and stream text generation with parameter overrides.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/68-default-settings-middleware.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { wrapLanguageModel } from 'ai';\nimport { defaultSettingsMiddleware } from 'ai';\nimport { openai } from 'ai';\n\n// Create a model with default settings\nconst modelWithDefaults = wrapLanguageModel({\n  model: openai.ChatTextGenerator({ model: 'gpt-4' }),\n  middleware: defaultSettingsMiddleware({\n    settings: {\n      temperature: 0.5,\n      maxTokens: 800,\n      providerMetadata: {\n        openai: {\n          tags: ['production'],\n        },\n      },\n    },\n  }),\n});\n\n// Use the model - default settings will be applied\nconst result = await streamText({\n  model: modelWithDefaults,\n  prompt: 'Your prompt here',\n  // These parameters will override the defaults\n  temperature: 0.8,\n});\n```\n\n----------------------------------------\n\nTITLE: Importing createDataStreamResponse from Vercel AI SDK\nDESCRIPTION: Shows how to import the createDataStreamResponse function from the ai package\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/41-create-data-stream-response.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createDataStreamResponse } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Object Generation Using ChromeAI and Zod Schema - JavaScript\nDESCRIPTION: Shows how to use ChromeAI's language model with the SDK's 'generateObject' method and validates outputs using a Zod schema. Dependencies are the 'ai' SDK, 'chrome-ai', and 'zod' for schema validation. Inputs include both a model kind and a Zod schema definition; outputs are strongly typed JavaScript objects generated based on user prompts and validated against the schema.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/04-chrome-ai.mdx#2025-04-23_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { generateObject } from 'ai';\\nimport { chromeai } from 'chrome-ai';\\nimport { z } from 'zod';\\n\\nconst { object } = await generateObject({\\n  model: chromeai('text'),\\n  schema: z.object({\\n    recipe: z.object({\\n      name: z.string(),\\n      ingredients: z.array(\\n        z.object({\\n          name: z.string(),\\n          amount: z.string(),\\n        }),\\n      ),\\n      steps: z.array(z.string()),\\n    }),\\n  }),\\n  prompt: 'Generate a lasagna recipe.',\\n});\\n\\nconsole.log(object);\\n// { recipe: {...} }\n```\n\n----------------------------------------\n\nTITLE: Instantiating an OpenAI Chat Model with Specific Options in TypeScript\nDESCRIPTION: Illustrates passing OpenAI-specific chat model settings, such as `logitBias` or `user`, as an options object when creating a chat model instance using `openai.chat()`. These options are in addition to standard call settings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = openai.chat('gpt-3.5-turbo', {\n  logitBias: { // optional likelihood for specific tokens\n    '50256': -100,\n  },\n  user: 'test-user', // optional unique user identifier\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Extract Reasoning Middleware\nDESCRIPTION: Shows how to use extractReasoningMiddleware to capture reasoning information from model outputs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { wrapLanguageModel, extractReasoningMiddleware } from 'ai';\n\nconst model = wrapLanguageModel({\n  model: yourModel,\n  middleware: extractReasoningMiddleware({ tagName: 'think' }),\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing an MCP Client with a Custom Transport in AI SDK using TypeScript\nDESCRIPTION: This example illustrates how to provide a custom transport implementation by subclassing or creating an MCPTransport that fulfills the required interface. It's suited for projects with unique networking or communication needs unsupported by the built-in transports. Dependencies are the AI SDK and a user-defined 'MyCustomTransport'. The input is a custom transport instance and the output is an MCP client capable of using that transport.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MCPTransport, createMCPClient } from 'ai';\n\nconst mcpClient = await createMCPClient({\n  transport: new MyCustomTransport({\n    // ...\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Enabling Token Counts in OpenAI Streaming with Strict Compatibility Mode\nDESCRIPTION: This code snippet shows how to fix NaN token counts when using streamText with OpenAI models by enabling the strict compatibility mode in the createOpenAI function. This setting allows token counts to be properly streamed.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/20-nan-token-counts-openai-streaming.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst openai = createOpenAI({\n  compatibility: 'strict',\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Together.ai Model via Vercel AI SDK (TypeScript)\nDESCRIPTION: Provides a basic example of generating text using the Vercel AI SDK with a Together.ai model. It imports the `togetherai` provider and the `generateText` function, configures a specific Together.ai model ('meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'), and sends a prompt to generate text. Requires the `@ai-sdk/togetherai` and `ai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/togetherai/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { togetherai } from '@ai-sdk/togetherai';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: togetherai('meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'),\n  prompt: 'Write a Python function that sorts a list:',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Callbacks in Chatbot\nDESCRIPTION: This example demonstrates how to use event callbacks like onFinish, onError, and onResponse to handle different stages of the chatbot lifecycle, enabling additional actions such as logging or analytics.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_8\n\nLANGUAGE: tsx\nCODE:\n```\nimport { Message } from '@ai-sdk/react';\n\nconst {\n  /* ... */\n} = useChat({\n  onFinish: (message, { usage, finishReason }) => {\n    console.log('Finished streaming message:', message);\n    console.log('Token usage:', usage);\n    console.log('Finish reason:', finishReason);\n  },\n  onError: error => {\n    console.error('An error occurred:', error);\n  },\n  onResponse: response => {\n    console.log('Received HTTP response from server:', response);\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_LoadAPIKeyError Instance in TypeScript\nDESCRIPTION: Demonstrates how to use the static `isInstance` method of `LoadAPIKeyError` (imported from the 'ai' package) to check if a given error object is specifically an instance of `AI_LoadAPIKeyError`. This pattern allows for type-safe error handling specific to API key loading failures.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-load-api-key-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nimport { LoadAPIKeyError } from 'ai';\n\nif (LoadAPIKeyError.isInstance(error)) {\n  // Handle the error\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Error Handler\nDESCRIPTION: Creates a custom error handler function to process different types of errors in the application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\nexport function errorHandler(error: unknown) {\n  if (error == null) {\n    return 'unknown error';\n  }\n\n  if (typeof error === 'string') {\n    return error;\n  }\n\n  if (error instanceof Error) {\n    return error.message;\n  }\n\n  return JSON.stringify(error);\n}\n```\n\n----------------------------------------\n\nTITLE: Basic simulateReadableStream Implementation\nDESCRIPTION: Shows the basic implementation without any delays specified.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/75-simulate-readable-stream.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst stream = simulateReadableStream({\n  chunks: ['Hello', ' ', 'World'],\n});\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_InvalidMessageRoleError in TypeScript AI Library\nDESCRIPTION: This TypeScript snippet demonstrates how to import the InvalidMessageRoleError from the 'ai' library and use its static isInstance method to check if a caught error is an instance of AI_InvalidMessageRoleError. Developers must have the 'ai' package installed in their project. The snippet uses a conditional to ensure that only errors of this specific type are handled, helping mitigate issues caused by invalid message roles. Inputs are caught errors (typically exceptions), and outputs are Boolean checksâactions inside the block can be modified as needed.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-invalid-message-role-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { InvalidMessageRoleError } from 'ai';\\n\\nif (InvalidMessageRoleError.isInstance(error)) {\\n  // Handle the error\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Migrating streamObject API to Synchronous Return - TypeScript\nDESCRIPTION: Transforms usage of streamObject to direct immediate return rather than a Promise in AI SDK 4.0. Removes the need for 'await'. Input: options object. Output: streaming object result.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_11\n\nLANGUAGE: ts\nCODE:\n```\nconst result = await streamObject({\n  // ...\n});\n```\n\nLANGUAGE: ts\nCODE:\n```\nconst result = streamObject({\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Aspect Ratio - AI SDK\nDESCRIPTION: This code snippet shows how to configure the aspect ratio of the generated image.  It uses the `aspectRatio` parameter in the `generateImage` function.  The aspect ratio is specified as a string in the format `{width}:{height}`. Requires appropriate model support.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { vertex } from '@ai-sdk/google-vertex';\n\nconst { image } = await generateImage({\n  model: vertex.image('imagen-3.0-generate-001'),\n  prompt: 'Santa Claus driving a Cadillac',\n  aspectRatio: '16:9',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-side Rendering with useObject Hook in React\nDESCRIPTION: This React component uses the useObject hook to stream and display notifications. It includes a button to trigger notification generation and renders the notifications as they are received.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/08-object-generation.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\nimport { notificationSchema } from './api/notifications/schema';\n\nexport default function Page() {\n  const { object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n  });\n\n  return (\n    <>\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Laminar in JavaScript\nDESCRIPTION: Initialize Laminar tracing in a JavaScript application using the project API key.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/laminar.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Laminar } from '@lmnr-ai/lmnr';\n\nLaminar.initialize({\n  projectApiKey: '...',\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Request Options\nDESCRIPTION: Example demonstrating how to configure custom request options including headers, body parameters, and credentials for the completion API.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/05-completion.mdx#2025-04-23_snippet_8\n\nLANGUAGE: tsx\nCODE:\n```\nconst { messages, input, handleInputChange, handleSubmit } = useCompletion({\n  api: '/api/custom-completion',\n  headers: {\n    Authorization: 'your_token',\n  },\n  body: {\n    user_id: '123',\n  },\n  credentials: 'same-origin',\n});\n```\n\n----------------------------------------\n\nTITLE: Specifying Tool Call Stream Part Structure - TypeScript\nDESCRIPTION: This code defines a TextStreamPart variant for tool invocation events with parameters for type identification, tool call ID, tool name, and generated arguments. It depends on a zod schema for argument validation. Inputs are model-generated tool parameters; output is a strongly-typed structure used to trigger specific tool functionality within the AI workflow.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_18\n\nLANGUAGE: TypeScript\nCODE:\n```\nparameters: [\n  {\n    name: 'type',\n    type: '\\'tool-call\\'',\n    description: 'The type to identify the object as tool call.',\n  },\n  {\n    name: 'toolCallId',\n    type: 'string',\n    description: 'The id of the tool call.',\n  },\n  {\n    name: 'toolName',\n    type: 'string',\n    description: 'The name of the tool, which typically would be the name of the function.',\n  },\n  {\n    name: 'args',\n    type: 'object based on zod schema',\n    description: 'Parameters generated by the model to be used by the tool.',\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Streaming Structured Output with streamText in TypeScript\nDESCRIPTION: This snippet shows how to use the streamText function with the experimental_output setting to stream structured data. It uses the same schema as the generateText example, defining a person object with properties like name, age, contact, and occupation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#2025-04-23_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\n// experimental_partialOutputStream contains generated partial objects:\nconst { experimental_partialOutputStream } = await streamText({\n  // ...\n  experimental_output: Output.object({\n    schema: z.object({\n      name: z.string(),\n      age: z.number().nullable().describe('Age of the person.'),\n      contact: z.object({\n        type: z.literal('email'),\n        value: z.string(),\n      }),\n      occupation: z.object({\n        type: z.literal('employed'),\n        company: z.string(),\n        position: z.string(),\n      }),\n    }),\n  }),\n  prompt: 'Generate an example person for testing.',\n});\n```\n\n----------------------------------------\n\nTITLE: Using Text Embedding Models with Provider Registry\nDESCRIPTION: Shows how to access text embedding models through the provider registry interface.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/45-provider-management.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { embed } from 'ai';\nimport { registry } from './registry';\n\nconst { embedding } = await embed({\n  model: registry.textEmbeddingModel('openai:text-embedding-3-small'),\n  value: 'sunny day at the beach',\n});\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Responses API for Web Search in TypeScript\nDESCRIPTION: This snippet demonstrates how to use OpenAI's Responses API with the built-in web search tool. It utilizes the AI SDK to generate text with the gpt-4o-mini model and the web_search_preview tool to search the web for information about recent events in San Francisco.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/56-web-search-agent.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst { text, sources } = await generateText({\n  model: openai.responses('gpt-4o-mini'),\n  prompt: 'What happened in San Francisco last week?',\n  tools: {\n    web_search_preview: openai.tools.webSearchPreview(),\n  },\n});\n\nconsole.log(text);\nconsole.log(sources);\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Anthropic Model using Vercel AI SDK in TypeScript\nDESCRIPTION: This TypeScript example shows how to generate text using the Vercel AI SDK's `generateText` function configured with the Anthropic provider. It imports necessary functions, specifies an Anthropic model (e.g., 'claude-3-haiku-20240307') via the imported `anthropic` instance, provides a text prompt, and asynchronously awaits the generated text response. Dependencies include `@ai-sdk/anthropic` and the core `ai` package.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/anthropic/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: anthropic('claude-3-haiku-20240307'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Environment File for API Keys\nDESCRIPTION: Command to create a .env.local file for storing environment variables like the OpenAI API key.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntouch .env.local\n```\n\n----------------------------------------\n\nTITLE: Accessing DeepSeek's Cache Token Usage Metrics\nDESCRIPTION: Example showing how to access and use DeepSeek's cache token usage metrics from the providerMetadata property in the response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/30-deepseek.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepseek } from '@ai-sdk/deepseek';\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: deepseek('deepseek-chat'),\n  prompt: 'Your prompt here',\n});\n\nconsole.log(result.providerMetadata);\n// Example output: { deepseek: { promptCacheHitTokens: 1856, promptCacheMissTokens: 5 } }\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Properties to Requests with Helicone in JavaScript\nDESCRIPTION: This snippet shows how to add custom properties to your AI SDK requests for filtering and analysis in Helicone. It includes properties for feature, source, and language in the request headers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/helicone.mdx#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst response = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'Translate this text to French',\n  headers: {\n    'Helicone-Property-Feature': 'translation',\n    'Helicone-Property-Source': 'mobile-app',\n    'Helicone-Property-Language': 'French',\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Importing useObject Hook\nDESCRIPTION: Code snippet showing how to import the experimental useObject hook from the AI SDK React package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/03-use-object.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { experimental_useObject as useObject } from '@ai-sdk/react'\n```\n\n----------------------------------------\n\nTITLE: Defining Provider-Specific Options Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `providerOptions` parameter. It's a nested record (`Record<string, Record<string, JSONValue>>`) where the outer key is the provider name and the inner record contains provider-specific metadata. Can be `undefined`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_23\n\nLANGUAGE: typescript\nCODE:\n```\nRecord<string,Record<string,JSONValue>> | undefined\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Object Streaming in React\nDESCRIPTION: This React component uses the useObject hook to stream object generation. It renders a button to trigger generation and displays the notifications as they are received.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/40-stream-object.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\nimport { notificationSchema } from './api/use-object/schema';\n\nexport default function Page() {\n  const { object, submit } = useObject({\n    api: '/api/use-object',\n    schema: notificationSchema,\n  });\n\n  return (\n    <div>\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Content Part Type Definitions - Message Components\nDESCRIPTION: Detailed type definitions for various content parts that can be included in messages, including text, image, file, reasoning, and tool call parts with their respective properties.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/04-stream-object.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ntype TextPart = {\n  type: 'text',\n  text: string\n}\n\ntype ImagePart = {\n  type: 'image',\n  image: string | Uint8Array | Buffer | ArrayBuffer | URL,\n  mimeType?: string\n}\n\ntype FilePart = {\n  type: 'file',\n  data: string | Uint8Array | Buffer | ArrayBuffer | URL,\n  mimeType: string\n}\n\ntype ReasoningPart = {\n  type: 'reasoning',\n  text: string,\n  signature?: string\n}\n\ntype RedactedReasoningPart = {\n  type: 'redacted-reasoning',\n  data: string\n}\n\ntype ToolCallPart = {\n  type: 'tool-call',\n  toolCallId: string,\n  toolName: string,\n  args: object\n}\n\ntype ToolResultPart = {\n  type: 'tool-result',\n  toolCallId: string,\n  toolName: string,\n  result: unknown\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing throttling with useCompletion hook in React\nDESCRIPTION: This code snippet shows how to use the experimental_throttle option with the useCompletion hook to limit UI updates to every 50ms. This prevents the \"Maximum update depth exceeded\" error by reducing the frequency of re-renders during streaming AI responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/50-react-maximum-update-depth-exceeded.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nconst { completion, ... } = useCompletion({\n  // Throttle the completion and data updates to 50ms:\n  experimental_throttle: 50\n})\n```\n\n----------------------------------------\n\nTITLE: Using Custom Tools with OpenAI Responses API\nDESCRIPTION: Demonstrates how to use custom tools with the OpenAI Responses API. This example creates a weather tool that the model can call to get weather information for a specified location.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/19-openai-responses.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText, tool } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { text } = await generateText({\n  model: openai.responses('gpt-4o'),\n  prompt: 'What is the weather like today in San Francisco?',\n  tools: {\n    getWeather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Text with pipeTextStreamToResponse in Node.js HTTP Server\nDESCRIPTION: Creates a Node.js HTTP server that uses the AI SDK to generate text with OpenAI's GPT-4o model and streams plain text to the client using pipeTextStreamToResponse method.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/10-node-http-server.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport { createServer } from 'http';\n\ncreateServer(async (req, res) => {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    prompt: 'Invent a new holiday and describe its traditions.',\n  });\n\n  result.pipeTextStreamToResponse(res);\n}).listen(8080);\n```\n\n----------------------------------------\n\nTITLE: Setting Thinking Budget for Google Generative AI Models\nDESCRIPTION: Configures a thinking budget for Google Generative AI thinking models, controlling how many tokens the model can use for its thinking process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { GoogleGenerativeAIProviderOptions } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: google('gemini-2.5-flash-preview-04-17'),\n  providerOptions: {\n    google: {\n      thinkingConfig: {\n        thinkingBudget: 2048,\n      },\n    } satisfies GoogleGenerativeAIProviderOptions,\n  },\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Output with generateText in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the generateText function with the experimental_output setting to generate structured data. It defines a schema for a person object with properties like name, age, contact, and occupation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\n// experimental_output is a structured object that matches the schema:\nconst { experimental_output } = await generateText({\n  // ...\n  experimental_output: Output.object({\n    schema: z.object({\n      name: z.string(),\n      age: z.number().nullable().describe('Age of the person.'),\n      contact: z.object({\n        type: z.literal('email'),\n        value: z.string(),\n      }),\n      occupation: z.object({\n        type: z.literal('employed'),\n        company: z.string(),\n        position: z.string(),\n      }),\n    }),\n  }),\n  prompt: 'Generate an example person for testing.',\n});\n```\n\n----------------------------------------\n\nTITLE: Applying Style Reference to Luma Model Generation (TypeScript)\nDESCRIPTION: Illustrates how to influence the visual style of generated images by passing style reference images using 'providerOptions.luma.style_ref'. The URL specifies the style image, and the weight determines its influence (0-1). Suitable for enforcing a consistent visual style across outputs. Input: valid image URL and weight.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/80-luma.mdx#2025-04-23_snippet_8\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Example: Generate with style reference\nawait generateImage({\n  model: luma.image('photon-1'),\n  prompt: 'A blue cream Persian cat launching its website on Vercel',\n  providerOptions: {\n    luma: {\n      style_ref: [\n        {\n          url: 'https://example.com/style.jpg',\n          weight: 0.8,\n        },\n      ],\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Empty Submissions\nDESCRIPTION: Shows how to allow empty submissions in the chat interface.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_15\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form\n        onSubmit={event => {\n          handleSubmit(event, {\n            allowEmptySubmit: true,\n          });\n        }}\n      >\n        <input value={input} onChange={handleInputChange} />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Next.js App with AI SDK and OpenAI Example using npx\nDESCRIPTION: This command uses create-next-app to bootstrap a new Next.js application with the AI SDK and OpenAI example. It clones the example from the Vercel AI repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app\n```\n\n----------------------------------------\n\nTITLE: Initializing Bedrock Image Model in TypeScript\nDESCRIPTION: Demonstrates how to create an instance of an Amazon Bedrock image generation model using the `.image()` factory method from the `@ai-sdk/amazon-bedrock` provider. This specific example initializes the 'amazon.nova-canvas-v1:0' model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = bedrock.image('amazon.nova-canvas-v1:0');\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Generation with OpenAI GPT-4 in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the streamText function to generate text from OpenAI's GPT-4 model. It imports necessary modules, sets up the stream, and iterates over the text parts, writing them to stdout.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst { textStream } = streamText({\n  model: openai('gpt-4o'),\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n\nfor await (const textPart of textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Text Editor Tool for file operations\nDESCRIPTION: Shows how to create and use the Text Editor Tool, which provides functionality for viewing and editing text files through the Anthropic API. This allows models to interact with files.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst textEditorTool = anthropic.tools.textEditor_20241022({\n  execute: async ({\n    command,\n    path,\n    file_text,\n    insert_line,\n    new_str,\n    old_str,\n    view_range,\n  }) => {\n    // Implement your text editing logic here\n    // Return the result of the text editing operation\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Cohere Model via AI SDK - TypeScript\nDESCRIPTION: Shows how to use the generateText function alongside the imported Cohere provider instance to request text generation from Cohere's 'command-r-plus' model. The code composes a prompt and receives generated text asynchronously. Requires installation of both @ai-sdk/cohere and the 'ai' package; an async context is required for 'await'. Inputs: model selection via cohere(), and a string prompt. Output: generated text within the returned object.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/cohere/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { cohere } from '@ai-sdk/cohere';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: cohere('command-r-plus'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Google Gemini via Vercel AI SDK in TypeScript\nDESCRIPTION: This example demonstrates using the imported `google` provider and the `generateText` function from the `ai` package to generate text with a Google model. It initializes the provider with the 'gemini-1.5-pro-latest' model and sends a prompt, awaiting the generated text response. Dependencies include `@ai-sdk/google` and `ai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: google('gemini-1.5-pro-latest'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Test Errors in API Route Handler\nDESCRIPTION: Shows how to inject errors for testing purposes by throwing an error in the API route handler.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/21-error-handling.mdx#2025-04-23_snippet_3\n\nLANGUAGE: ts\nCODE:\n```\nexport async function POST(req: Request) {\n  throw new Error('This is a test error');\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Step Tool Usage with AI SDK in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the AI SDK's maxSteps parameter to create an agent that solves math problems using a calculator tool. It showcases how to define tools, use structured outputs, and implement multi-step problem-solving.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText, tool } from 'ai';\nimport * as mathjs from 'mathjs';\nimport { z } from 'zod';\n\nconst { text: answer } = await generateText({\n  model: openai('gpt-4o-2024-08-06', { structuredOutputs: true }),\n  tools: {\n    calculate: tool({\n      description:\n        'A tool for evaluating mathematical expressions. ' +\n        'Example expressions: ' +\n        \"'1.2 * (2 + 4.5)', '12.7 cm to inch', 'sin(45 deg) ^ 2'.\",\n      parameters: z.object({ expression: z.string() }),\n      execute: async ({ expression }) => mathjs.evaluate(expression),\n    }),\n  },\n  maxSteps: 10,\n  system:\n    'You are solving math problems. ' +\n    'Reason step by step. ' +\n    'Use the calculator when necessary. ' +\n    'When you give the final answer, ' +\n    'provide an explanation for how you arrived at it.',\n  prompt:\n    'A taxi driver earns $9461 per 1-hour of work. ' +\n    'If he works 12 hours a day and in 1 hour ' +\n    'he uses 12 liters of petrol with a price  of $134 for 1 liter. ' +\n    'How much money does he earn in one day?',\n});\n\nconsole.log(`ANSWER: ${answer}`);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Computer Tool with AI SDK\nDESCRIPTION: Example of how to configure the Computer Tool in the AI SDK. This includes implementing screenshot functionality and action execution with proper result formatting for the model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/05-computer-use.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { getScreenshot, executeComputerAction } from '@/utils/computer-use';\n\nconst computerTool = anthropic.tools.computer_20241022({\n  displayWidthPx: 1920,\n  displayHeightPx: 1080,\n  execute: async ({ action, coordinate, text }) => {\n    switch (action) {\n      case 'screenshot': {\n        return {\n          type: 'image',\n          data: getScreenshot(),\n        };\n      }\n      default: {\n        return executeComputerAction(action, coordinate, text);\n      }\n    }\n  },\n  experimental_toToolResultContent(result) {\n    return typeof result === 'string'\n      ? [{ type: 'text', text: result }]\n      : [{ type: 'image', data: result.data, mimeType: 'image/png' }];\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Azure OpenAI Model\nDESCRIPTION: Provides a basic example of using the 'generateText' function from the 'ai' package with an Azure OpenAI language model. It sends a prompt and retrieves the generated text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { azure } from '@ai-sdk/azure';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: azure('your-deployment-name'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Getting AI Memory Sources from Mem0 - TypeScript\nDESCRIPTION: This code demonstrates extracting source documents or references from memories when generating AI output with Mem0. Requires '@mem0/vercel-ai-provider' and 'ai'. The 'generateText' call returns both generated text and associated sources for further inspection or traceability. Input: prompt string, Mem0-enhanced model. Output: generated text and a sources array. Also compatible with streamed responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/70-mem0.mdx#2025-04-23_snippet_8\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst { text, sources } = await generateText({\n  model: mem0('gpt-4-turbo'),\n  prompt: 'Suggest me a good car to buy!',\n});\n\nconsole.log(sources);\n```\n\n----------------------------------------\n\nTITLE: Streaming Object with onFinish Callback in TypeScript\nDESCRIPTION: Demonstrates how to use the onFinish callback to handle the final streamed object. The callback receives the final object and potential error, with type validation using Zod schema.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/46-stream-object-record-final-object.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { z } from 'zod';\n\nconst result = streamObject({\n  model: openai('gpt-4-turbo'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.string()),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n  onFinish({ object, error }) {\n    // handle type validation failure (when the object does not match the schema):\n    if (object === undefined) {\n      console.error('Error:', error);\n      return;\n    }\n\n    console.log('Final object:', JSON.stringify(object, null, 2));\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Active Tools Feature\nDESCRIPTION: Demonstrates how to limit available tools using the experimental_activeTools property.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: openai('gpt-4o'),\n  tools: myToolSet,\n  experimental_activeTools: ['firstTool'],\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Next.js App with AI SDK and OpenAI Example using pnpm\nDESCRIPTION: This command uses pnpm to initialize a new Next.js application with the AI SDK and OpenAI example. It sets up the project by cloning the example from the Vercel AI repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app\n```\n\n----------------------------------------\n\nTITLE: Implementing Error Boundary for UI Stream Components in React\nDESCRIPTION: Shows how to implement client-side error handling using React Error Boundary component to catch rendering errors in streamed UI components.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/08-error-handling.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { getStreamedUI } from '@/actions';\nimport { useState } from 'react';\nimport { ErrorBoundary } from './ErrorBoundary';\n\nexport default function Page() {\n  const [streamedUI, setStreamedUI] = useState(null);\n\n  return (\n    <div>\n      <button\n        onClick={async () => {\n          const newUI = await getStreamedUI();\n          setStreamedUI(newUI);\n        }}\n      >\n        What does the new UI look like?\n      </button>\n      <ErrorBoundary>{streamedUI}</ErrorBoundary>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Callback Chunking Implementation\nDESCRIPTION: Demonstrates how to implement custom callback-based chunking with smoothStream, including string search and text slicing.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/80-smooth-stream.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nsmoothStream({\n  chunking: text => {\n    const findString = 'some string';\n    const index = text.indexOf(findString);\n\n    if (index === -1) {\n      return null;\n    }\n\n    return text.slice(0, index) + findString;\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Appending Values to StreamData\nDESCRIPTION: Shows the method signature for appending values to the stream data. Accepts any JSON-serializable value as input.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/45-stream-data.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ndata.append(value: JSONValue)\n```\n\n----------------------------------------\n\nTITLE: Defining Presence Penalty Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `presencePenalty` parameter as a number. This influences the model's likelihood to repeat information already present in the prompt. The valid range depends on the provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nnumber\n```\n\n----------------------------------------\n\nTITLE: Creating an NVIDIA NIM Provider Instance in TypeScript\nDESCRIPTION: Demonstrates how to create a custom provider instance for NVIDIA NIM using the `createOpenAICompatible` function from `@ai-sdk/openai-compatible`. It configures the provider name, base URL for the NVIDIA API ('https://integrate.api.nvidia.com/v1'), and sets the Authorization header using an environment variable `NIM_API_KEY`. This instance (`nim`) is then used for subsequent model interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/35-nim.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\n\nconst nim = createOpenAICompatible({\n  name: 'nim',\n  baseURL: 'https://integrate.api.nvidia.com/v1',\n  headers: {\n    Authorization: `Bearer ${process.env.NIM_API_KEY}`,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: TypeScript Interface Definition for Message Parts\nDESCRIPTION: Type definitions for different message part types including text, image, file, reasoning, and tool-related parts used in message content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/03-generate-object.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\ninterface TextPart {\n  type: 'text';\n  text: string;\n}\n\ninterface ImagePart {\n  type: 'image';\n  image: string | Uint8Array | Buffer | ArrayBuffer | URL;\n  mimeType?: string;\n}\n\ninterface FilePart {\n  type: 'file';\n  data: string | Uint8Array | Buffer | ArrayBuffer | URL;\n  mimeType: string;\n}\n\ninterface ReasoningPart {\n  type: 'reasoning';\n  text: string;\n  signature?: string;\n}\n\ninterface RedactedReasoningPart {\n  type: 'redacted-reasoning';\n  data: string;\n}\n\ninterface ToolCallPart {\n  type: 'tool-call';\n  toolCallId: string;\n  toolName: string;\n  args: object;\n}\n\ninterface ToolResultPart {\n  type: 'tool-result';\n  toolCallId: string;\n  toolName: string;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing @ai-sdk/openai-compatible using pnpm - Shell\nDESCRIPTION: Installs the @ai-sdk/openai-compatible package using pnpm. This package is necessary for interacting with OpenAI-compatible providers, such as Baseten, within the AI SDK. Ensure pnpm is installed on your system before running this command.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/40-baseten.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add @ai-sdk/openai-compatible\n```\n\n----------------------------------------\n\nTITLE: Setting Up API Route for Text Completion\nDESCRIPTION: Server-side implementation of the completion API endpoint using OpenAI's GPT-3.5-turbo model with streaming response handling.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/05-completion.mdx#2025-04-23_snippet_1\n\nLANGUAGE: ts\nCODE:\n```\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { prompt }: { prompt: string } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-3.5-turbo'),\n    prompt,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Image with AI SDK - Core\nDESCRIPTION: This code snippet demonstrates the basic usage of the `generateImage` function to generate an image from a prompt using a specified image model. It requires the `ai` and a model provider like `@ai-sdk/openai` packages.  The output is an image object containing the generated image data.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { image } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n});\n```\n\n----------------------------------------\n\nTITLE: Extracting Reasoning with FriendliAI Middleware\nDESCRIPTION: Shows how to wrap a FriendliAI model (like 'deepseek-r1') with `extractReasoningMiddleware` from the AI SDK. This allows extracting intermediate reasoning steps tagged with `<think>` into a separate `reasoning` property in the result. Dependencies: `@friendliai/ai-provider`, `ai`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { friendli } from '@friendliai/ai-provider';\nimport { wrapLanguageModel, extractReasoningMiddleware } from 'ai';\n\nconst enhancedModel = wrapLanguageModel({\n  model: friendli('deepseek-r1'),\n  middleware: extractReasoningMiddleware({ tagName: 'think' }),\n});\n\nconst { text, reasoning } = await generateText({\n  model: enhancedModel,\n  prompt: 'Explain quantum entanglement.',\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing Request Body in useChat Hook (TypeScript/React)\nDESCRIPTION: This snippet shows how to use the experimental_prepareRequestBody option in the useChat hook to only send the text of the last message to the server. It demonstrates the client-side implementation in a Next.js application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/80-send-custom-body-from-use-chat.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    experimental_prepareRequestBody: ({ messages }) => {\n      // e.g. only the text of the last message:\n      return messages[messages.length - 1].content;\n    },\n  });\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role === 'user' ? 'User: ' : 'AI: '}\n          {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating API Route for LLM Chat in Nuxt\nDESCRIPTION: Implements a server API route that processes chat messages, streams text responses from OpenAI's GPT-4o model, and returns the result as a data stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { createOpenAI } from '@ai-sdk/openai';\n\nexport default defineLazyEventHandler(async () => {\n  const apiKey = useRuntimeConfig().openaiApiKey;\n  if (!apiKey) throw new Error('Missing OpenAI API key');\n  const openai = createOpenAI({\n    apiKey: apiKey,\n  });\n\n  return defineEventHandler(async (event: any) => {\n    const { messages } = await readBody(event);\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages,\n    });\n\n    return result.toDataStreamResponse();\n  });\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Together.ai Provider from AI SDK\nDESCRIPTION: Shows how to import the default Together.ai provider instance from the AI SDK package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/24-togetherai.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { togetherai } from '@ai-sdk/togetherai';\n```\n\n----------------------------------------\n\nTITLE: Adding Error Handling to useObject Hook Implementation in React\nDESCRIPTION: This React component demonstrates how to handle and display errors that may occur during the object generation process using the useObject hook.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/08-object-generation.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useObject } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { error, object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n  });\n\n  return (\n    <>\n      {error && <div>An error occurred.</div>}\n\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing LM Studio Language Model in TypeScript\nDESCRIPTION: Shows how to access a specific language model (e.g., 'llama-3.2-1b') served by LM Studio using the previously created provider instance. The model ID is passed as an argument to the provider function. Requires the specified model to be downloaded and loaded in LM Studio first.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/30-lmstudio.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = lmstudio('llama-3.2-1b');\n```\n\n----------------------------------------\n\nTITLE: Initializing a Language Model with Safe Prompt Option - TypeScript\nDESCRIPTION: This snippet demonstrates initializing a Cloudflare Workers AI language model with the option to enable a safe prompt. Utilizes createWorkersAI and specifies the model along with configuration. Inputs are the AI binding and settings such as safePrompt. Output is a model instance for use with text generation functions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/11-cloudflare-workers-ai.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createWorkersAI } from 'workers-ai-provider';\n\nconst workersai = createWorkersAI({ binding: env.AI });\nconst model = workersai('@cf/meta/llama-3.1-8b-instruct', {\n  // additional settings\n  safePrompt: true,\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Experimental Telemetry Settings Type in TypeScript\nDESCRIPTION: Specifies the type for the optional experimental feature `experimental_telemetry` as `TelemetrySettings`. This configures telemetry collection. `TelemetrySettings` include optional booleans `isEnabled`, `recordInputs`, `recordOutputs`, an optional `functionId` (string), and optional `metadata` (Record).\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_20\n\nLANGUAGE: typescript\nCODE:\n```\nTelemetrySettings\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings with OpenAI and AI SDK in TypeScript\nDESCRIPTION: This code snippet shows how to use the AI SDK and OpenAI's embedding model to convert text into a numerical representation. It imports necessary modules, defines an async function that calls the embed function with a specified model and input text, and logs the resulting embedding and usage information.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/60-embed-text.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { embed } from 'ai';\nimport 'dotenv/config';\n\nasync function main() {\n  const { embedding, usage } = await embed({\n    model: openai.embedding('text-embedding-3-small'),\n    value: 'sunny day at the beach',\n  });\n\n  console.log(embedding);\n  console.log(usage);\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Embedding Generation with OpenAI\nDESCRIPTION: Implements embedding generation using OpenAI's text-embedding-ada-002 model via the AI SDK. Takes text chunks and returns array of embeddings with corresponding content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_8\n\nLANGUAGE: tsx\nCODE:\n```\nimport { embedMany } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst embeddingModel = openai.embedding('text-embedding-ada-002');\n\nconst generateChunks = (input: string): string[] => {\n  return input\n    .trim()\n    .split('.')\n    .filter(i => i !== '');\n};\n\nexport const generateEmbeddings = async (\n  value: string,\n): Promise<Array<{ embedding: number[]; content: string }>> => {\n  const chunks = generateChunks(value);\n  const { embeddings } = await embedMany({\n    model: embeddingModel,\n    values: chunks,\n  });\n  return embeddings.map((e, i) => ({ content: chunks[i], embedding: e }));\n};\n```\n\n----------------------------------------\n\nTITLE: Initializing LangfuseExporter in TypeScript\nDESCRIPTION: This code demonstrates how to initialize the LangfuseExporter with configuration options. It sets up the exporter with the necessary API keys and base URL for the Langfuse service.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langfuse.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LangfuseExporter } from 'langfuse-vercel';\n\nnew LangfuseExporter({\n  secretKey: 'sk-lf-...',\n  publicKey: 'pk-lf-...',\n  baseUrl: 'https://cloud.langfuse.com', // ðªðº EU region\n  // baseUrl: \"https://us.cloud.langfuse.com\", // ðºð¸ US region\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Default Amazon Bedrock Provider\nDESCRIPTION: Simple import of the default Bedrock provider instance from the Amazon Bedrock package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\n```\n\n----------------------------------------\n\nTITLE: Catching and Inspecting AI_NoObjectGeneratedError in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to handle errors during AI object generation using the `generateObject` function from the 'ai' library. It uses a try...catch block to capture potential errors and specifically checks if the caught error is an instance of `NoObjectGeneratedError` using `NoObjectGeneratedError.isInstance()`. If it is, the code logs various properties of the error object, such as the underlying cause, the generated text, response metadata, token usage, and the finish reason, for debugging and analysis. This requires importing `generateObject` and `NoObjectGeneratedError` from the 'ai' package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-no-object-generated-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nimport { generateObject, NoObjectGeneratedError } from 'ai';\n\ntry {\n  await generateObject({ model, schema, prompt });\n} catch (error) {\n  if (NoObjectGeneratedError.isInstance(error)) {\n    console.log('NoObjectGeneratedError');\n    console.log('Cause:', error.cause);\n    console.log('Text:', error.text);\n    console.log('Response:', error.response);\n    console.log('Usage:', error.usage);\n    console.log('Finish Reason:', error.finishReason);\n  }\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Environment Variables\nDESCRIPTION: Environment variable configuration for OpenAI API authentication.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#2025-04-23_snippet_2\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=xxxxxxxxx\n```\n\n----------------------------------------\n\nTITLE: Defining Maximum Steps Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `maxSteps` parameter as a number. It limits the maximum number of sequential LLM calls (steps), primarily to prevent infinite loops with tool calls. Defaults to 1.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\nnumber\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Interface with React Client Component\nDESCRIPTION: This client-side component manages the chat interface, displaying the conversation history and handling user input. It uses hooks from the AI SDK to manage UI state and perform actions like continuing the conversation with AI responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/61-restore-messages-from-database.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState, useEffect } from 'react';\nimport { ClientMessage } from './actions';\nimport { useActions, useUIState } from 'ai/rsc';\nimport { generateId } from 'ai';\n\nexport default function Home() {\n  const [conversation, setConversation] = useUIState();\n  const [input, setInput] = useState<string>('');\n  const { continueConversation } = useActions();\n\n  return (\n    <div>\n      <div className=\"conversation-history\">\n        {conversation.map((message: ClientMessage) => (\n          <div key={message.id} className={`message ${message.role}`}>\n            {message.role}: {message.display}\n          </div>\n        ))}\n      </div>\n\n      <div className=\"input-area\">\n        <input\n          type=\"text\"\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          placeholder=\"Type your message...\"\n        />\n        <button\n          onClick={async () => {\n            // Add user message to UI\n            setConversation((currentConversation: ClientMessage[]) => [\n              ...currentConversation,\n              { id: generateId(), role: 'user', display: input },\n            ]);\n\n            // Get AI response\n            const message = await continueConversation(input);\n\n            // Add AI response to UI\n            setConversation((currentConversation: ClientMessage[]) => [\n              ...currentConversation,\n              message,\n            ]);\n\n            setInput('');\n          }}\n        >\n          Send\n        </button>\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a customized Anthropic provider instance\nDESCRIPTION: Demonstrates how to create a customized Anthropic provider instance with specific settings using the createAnthropic function. This allows for configuration of base URL, API keys, headers, and fetch implementation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAnthropic } from '@ai-sdk/anthropic';\n\nconst anthropic = createAnthropic({\n  // custom settings\n});\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Computer Use Tools\nDESCRIPTION: Example demonstrating how to combine computer, bash, and text editor tools in a single request to enable complex workflows.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/05-computer-use.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst computerTool = anthropic.tools.computer_20241022({\n  ...\n});\n\nconst bashTool = anthropic.tools.bash_20241022({\n  execute: async ({ command, restart }) => execSync(command).toString()\n});\n\nconst textEditorTool = anthropic.tools.textEditor_20241022({\n  execute: async ({\n    command,\n    path,\n    file_text,\n    insert_line,\n    new_str,\n    old_str,\n    view_range\n  }) => {\n    // Handle file operations based on command\n    switch(command) {\n      return executeTextEditorFunction({\n        command,\n        path,\n        fileText: file_text,\n        insertLine: insert_line,\n        newStr: new_str,\n        oldStr: old_str,\n        viewRange: view_range\n      });\n    }\n  }\n});\n\n\nconst response = await generateText({\n  model: anthropic(\"claude-3-5-sonnet-20241022\"),\n  prompt: \"Create a new file called example.txt, write 'Hello World' to it, and run 'cat example.txt' in the terminal\",\n  tools: {\n    computer: computerTool,\n    bash: bashTool\n    str_replace_editor: textEditorTool,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with DeepSeek Model\nDESCRIPTION: Complete example showing how to use the DeepSeek provider with the AI SDK's generateText function to create text content using the deepseek-chat model.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/deepseek/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepseek } from '@ai-sdk/deepseek';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: deepseek('deepseek-chat'),\n  prompt: 'Write a JavaScript function that sorts a list:',\n});\n```\n\n----------------------------------------\n\nTITLE: Using Provider Options with Response Modalities\nDESCRIPTION: Demonstrates how to use Google Generative AI provider options to specify response modalities when generating text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { GoogleGenerativeAIProviderOptions } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: google('gemini-1.5-pro-latest'),\n  providerOptions: {\n    google: {\n      responseModalities: ['TEXT', 'IMAGE'],\n    } satisfies GoogleGenerativeAIProviderOptions,\n  },\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing UI Update Throttling in Chatbot\nDESCRIPTION: This snippet shows how to throttle UI updates in the chatbot to improve performance, especially useful for high-frequency updates during streaming.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\nconst { messages, ... } = useChat({\n  // Throttle the messages and data updates to 50ms:\n  experimental_throttle: 50\n})\n```\n\n----------------------------------------\n\nTITLE: Using Examples in Slogan Generator Prompt in Markdown\nDESCRIPTION: This snippet shows how to incorporate examples in the prompt to guide the model in generating more appropriate slogans.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/01-prompt-engineering.mdx#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n<InlinePrompt\n  initialInput={`Create three slogans for a business with unique features.\n\nBusiness: Bookstore with cats\nSlogans: \"Purr-fect Pages\", \"Books and Whiskers\", \"Novels and Nuzzles\"\nBusiness: Gym with rock climbing\nSlogans: \"Peak Performance\", \"Reach New Heights\", \"Climb Your Way Fit\"\nBusiness: Coffee shop with live music\nSlogans:`}\n/>\n```\n\n----------------------------------------\n\nTITLE: Importing the Default Replicate Provider Instance (TypeScript)\nDESCRIPTION: Shows how to import the pre-configured Replicate provider instance from the @ai-sdk/replicate package. This gives quick access to standard Replicate functionality for use with the AI SDK without having to manually set options or environment variables.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/60-replicate.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { replicate } from '@ai-sdk/replicate';\n```\n\n----------------------------------------\n\nTITLE: Implementing Controlled Input in Chatbot UI\nDESCRIPTION: This snippet shows how to use more granular APIs like setInput and append with custom input and submit button components for advanced scenarios.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\nconst { input, setInput, append } = useChat()\n\nreturn <>\n  <MyCustomInput value={input} onChange={value => setInput(value)} />\n  <MySubmitButton onClick={() => {\n    // Send a new message to the AI provider\n    append({\n      role: 'user',\n      content: input,\n    })\n  }}/>\n  ...\n```\n\n----------------------------------------\n\nTITLE: Properly Closing Streamable UI in React Server Components\nDESCRIPTION: This example demonstrates how to fix slow UI updates by ensuring a stream is properly closed. The solution involves calling the .done() method after updating the stream, which ensures the stream is properly terminated.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/07-unclosed-streams.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { createStreamableUI } from 'ai/rsc';\n\nconst submitMessage = async () => {\n  'use server';\n\n  const stream = createStreamableUI('1');\n\n  stream.update('2');\n  stream.append('3');\n  stream.done('4'); // [!code ++]\n\n  return stream.value;\n};\n```\n\n----------------------------------------\n\nTITLE: Defining the Async Tool Execution Function Type in TypeScript\nDESCRIPTION: Specifies the type for an optional asynchronous function that executes a tool. It receives tool parameters (`T`) and execution options (`ToolExecutionOptions`), returning a result (`RESULT`). `ToolExecutionOptions` include `toolCallId` (string), `messages` (CoreMessage[] excluding system prompt and assistant response), and an optional `abortSignal` (AbortSignal). If this function isn't provided, the tool won't be automatically executed.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nasync (parameters: T, options: ToolExecutionOptions) => RESULT\n```\n\n----------------------------------------\n\nTITLE: Creating a Weather Tool with Zod Schema in TypeScript\nDESCRIPTION: This code defines a weather tool using the AI SDK's tool function and Zod for parameter validation. It simulates fetching weather information for a given location with a delay.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#2025-04-23_snippet_2\n\nLANGUAGE: ts\nCODE:\n```\nimport { tool as createTool } from 'ai';\nimport { z } from 'zod';\n\nexport const weatherTool = createTool({\n  description: 'Display the weather for a location',\n  parameters: z.object({\n    location: z.string().describe('The location to get the weather for'),\n  }),\n  execute: async function ({ location }) {\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    return { weather: 'Sunny', temperature: 75, location };\n  },\n});\n\nexport const tools = {\n  displayWeather: weatherTool,\n};\n```\n\n----------------------------------------\n\nTITLE: React IndexCards Component Usage\nDESCRIPTION: Component implementation showing the documentation structure for AI SDK RSC features, including streamUI, createAI, and various hooks and utilities\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/index.mdx#2025-04-23_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<IndexCards\n  cards={[\n    {\n      title: 'streamUI',\n      description:\n        'Use a helper function that streams React Server Components on tool execution.',\n      href: '/docs/reference/ai-sdk-rsc/stream-ui',\n    },\n    {\n      title: 'createAI',\n      description:\n        'Create a context provider that wraps your application and shares state between the client and language model on the server.',\n      href: '/docs/reference/ai-sdk-rsc/create-ai',\n    },\n    {\n      title: 'createStreamableUI',\n      description:\n        'Create a streamable UI component that can be rendered on the server and streamed to the client.',\n      href: '/docs/reference/ai-sdk-rsc/create-streamable-ui',\n    },\n    {\n      title: 'createStreamableValue',\n      description:\n        'Create a streamable value that can be rendered on the server and streamed to the client.',\n      href: '/docs/reference/ai-sdk-rsc/create-streamable-value',\n    },\n    {\n      title: 'getAIState',\n      description: 'Read the AI state on the server.',\n      href: '/docs/reference/ai-sdk-rsc/get-ai-state',\n    },\n    {\n      title: 'getMutableAIState',\n      description: 'Read and update the AI state on the server.',\n      href: '/docs/reference/ai-sdk-rsc/get-mutable-ai-state',\n    },\n    {\n      title: 'useAIState',\n      description: 'Get the AI state on the client from the context provider.',\n      href: '/docs/reference/ai-sdk-rsc/use-ai-state',\n    },\n    {\n      title: 'useUIState',\n      description: 'Get the UI state on the client from the context provider.',\n      href: '/docs/reference/ai-sdk-rsc/use-ui-state',\n    },\n    {\n      title: 'useActions',\n      description: 'Call server actions from the client.',\n      href: '/docs/reference/ai-sdk-rsc/use-actions',\n    },\n  ]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Defining Experimental Continue Steps Flag Type in TypeScript\nDESCRIPTION: Specifies the type for the optional experimental feature `experimental_continueSteps` as a boolean. It enables or disables the continuation of steps. Disabled by default.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\nboolean\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Zhipu Provider Instance (TypeScript)\nDESCRIPTION: Creates a customized Zhipu provider instance using the `createZhipu` function. This allows overriding default settings like the `baseURL` for API calls and explicitly providing the `apiKey`, useful for environments without environment variable support or when using proxies. Optional `headers` can also be provided.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/95-zhipu.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createZhipu } from 'zhipu-ai-provider';\n\nconst zhipu = createZhipu({\n  baseURL: 'https://open.bigmodel.cn/api/paas/v4',\n  apiKey: 'your-api-key',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat UI with AI SDK in Vue Component\nDESCRIPTION: Creates a Vue.js component that uses the AI SDK's useChat hook to manage conversation state, handle user input, and display streaming AI responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n<script setup lang=\"ts\">\nimport { useChat } from '@ai-sdk/vue';\n\nconst { messages, input, handleSubmit } = useChat();\n</script>\n\n<template>\n  <div>\n    <div\n      v-for=\"m in messages\"\n      :key=\"m.id ? m.id : index\"\n    >\n      {{ m.role === 'user' ? 'User: ' : 'AI: ' }}\n      <div v-for=\"part in m.parts\" :key=\"part.id\">\n        <div v-if=\"part.type === 'text'\">{{ part.text }}</div>\n      </div>\n    </div>\n\n    <form @submit=\"handleSubmit\">\n      <input\n        v-model=\"input\"\n        placeholder=\"Say something...\"\n      />\n    </form>\n  </div>\n</template>\n```\n\n----------------------------------------\n\nTITLE: Creating PDF-enabled Chat Interface in Next.js\nDESCRIPTION: Implements a client-side chat interface with PDF upload functionality. Uses the useChat hook to handle file uploads and message streaming, displaying both text responses and uploaded PDFs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/23-chat-with-pdf.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\nimport { useRef, useState } from 'react';\nimport Image from 'next/image';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n\n  const [files, setFiles] = useState<FileList | undefined>(undefined);\n  const fileInputRef = useRef<HTMLInputElement>(null);\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(m => (\n        <div key={m.id} className=\"whitespace-pre-wrap\">\n          {m.role === 'user' ? 'User: ' : 'AI: '}\n          {m.content}\n          <div>\n            {m?.experimental_attachments\n              ?.filter(\n                attachment =>\n                  attachment?.contentType?.startsWith('image/') ||\n                  attachment?.contentType?.startsWith('application/pdf'),\n              )\n              .map((attachment, index) =>\n                attachment.contentType?.startsWith('image/') ? (\n                  <Image\n                    key={`${m.id}-${index}`}\n                    src={attachment.url}\n                    width={500}\n                    height={500}\n                    alt={attachment.name ?? `attachment-${index}`}\n                  />\n                ) : attachment.contentType?.startsWith('application/pdf') ? (\n                  <iframe\n                    key={`${m.id}-${index}`}\n                    src={attachment.url}\n                    width=\"500\"\n                    height=\"600\"\n                    title={attachment.name ?? `attachment-${index}`}\n                  />\n                ) : null,\n              )}\n          </div>\n        </div>\n      ))}\n\n      <form\n        className=\"fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl space-y-2\"\n        onSubmit={event => {\n          handleSubmit(event, {\n            experimental_attachments: files,\n          });\n\n          setFiles(undefined);\n\n          if (fileInputRef.current) {\n            fileInputRef.current.value = '';\n          }\n        }}\n      >\n        <input\n          type=\"file\"\n          className=\"\"\n          onChange={event => {\n            if (event.target.files) {\n              setFiles(event.target.files);\n            }\n          }}\n          multiple\n          ref={fileInputRef}\n        />\n        <input\n          className=\"w-full p-2\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n```\n\n----------------------------------------\n\nTITLE: Setting Up Azure OpenAI Image Generation\nDESCRIPTION: Details the setup and usage of DALL-E image generation models with configuration options for response format and user identification.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = azure.imageModel('your-dalle-deployment-name');\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = azure.imageModel('your-dalle-deployment-name', {\n  user: 'test-user', // optional unique user identifier\n  responseFormat: 'url', // 'url' or 'b64_json', defaults to 'url'\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { azure } from '@ai-sdk/azure';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: azure.imageModel('your-dalle-deployment-name'),\n  prompt: 'A photorealistic image of a cat astronaut floating in space',\n  size: '1024x1024', // '1024x1024', '1792x1024', or '1024x1792' for DALL-E 3\n});\n\n// image contains the URL or base64 data of the generated image\nconsole.log(image);\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Step Tool Calls\nDESCRIPTION: Configuration update to enable multi-step tool calls for improved user experience.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_16\n\nLANGUAGE: tsx\nCODE:\n```\nconst { messages, input, handleInputChange, handleSubmit } = useChat({\n  maxSteps: 3,\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Next.js Project with AI SDK and FastAPI Example using Yarn\nDESCRIPTION: This command uses Yarn to create a new Next.js project with the AI SDK and FastAPI example from the Vercel AI repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-fastapi/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-fastapi next-fastapi-app\n```\n\n----------------------------------------\n\nTITLE: Implementing Guardrails Middleware\nDESCRIPTION: Example of a guardrails middleware that filters and modifies generated text for safety and appropriateness.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { LanguageModelV1Middleware } from 'ai';\n\nexport const yourGuardrailMiddleware: LanguageModelV1Middleware = {\n  wrapGenerate: async ({ doGenerate }) => {\n    const { text, ...rest } = await doGenerate();\n\n    // filtering approach, e.g. for PII or other sensitive information:\n    const cleanedText = text?.replace(/badword/g, '<REDACTED>');\n\n    return { text: cleanedText, ...rest };\n  },\n\n  // here you would implement the guardrail logic for streaming\n  // Note: streaming guardrails are difficult to implement, because\n  // you do not know the full content of the stream until it's finished.\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Vertex Anthropic Provider Authentication for Node.js in TypeScript\nDESCRIPTION: Demonstrates customizing authentication for the Google Vertex Anthropic provider in a Node.js environment. The `createVertexAnthropic` function accepts `googleAuthOptions`, allowing configuration via the `google-auth-library`, such as providing service account credentials directly.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_21\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createVertexAnthropic } from '@ai-sdk/google-vertex/anthropic';\n\nconst vertexAnthropic = createVertexAnthropic({\n  googleAuthOptions: {\n    credentials: {\n      client_email: 'my-email',\n      private_key: 'my-private-key',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Language Models with Automatic Fallback - TypeScript\nDESCRIPTION: Demonstrates creating a composite model instance that leverages multiple providers (Anthropic and OpenAI) via the AI Gateway Provider. The provider uses the specified models in order, automatically falling back on subsequent models in case of failure. This approach increases reliability and enables seamless chaining of different vendor models using consistent configuration inputs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/12-cloudflare-ai-gateway.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAiGateway } from 'ai-gateway-provider';\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { createAnthropic } from '@ai-sdk/anthropic';\n\nconst aigateway = createAiGateway({\n  accountId: 'your-cloudflare-account-id',\n  gateway: 'your-gateway-name',\n  apiKey: 'your-cloudflare-api-key',\n});\n\nconst openai = createOpenAI({ apiKey: 'openai-api-key' });\nconst anthropic = createAnthropic({ apiKey: 'anthropic-api-key' });\n\nconst model = aigateway([\n  anthropic('claude-3-5-haiku-20241022'), // Primary model\n  openai('gpt-4o-mini'), // Fallback model\n]);\n```\n\n----------------------------------------\n\nTITLE: Defining Message Properties for Assistant Output in JavaScript\nDESCRIPTION: This snippet demonstrates the previous format for representing assistant message outputs, where different output types (content, reasoning, tool invocations) are stored in separate properties of a message object. The example shows how content, reasoning, and tool calls were individually assigned. This structure is now considered limiting and is being replaced by a parts array. Requires a JavaScript environment and context for what message refers to.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/27-migration-guide-4-2.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nmessage.content = \"Final answer: 42\";\nmessage.reasoning = \"First I'll calculate X, then Y...\";\nmessage.toolInvocations = [{toolName: \"calculator\", args: {...}}];\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Bedrock and AI SDK in TypeScript\nDESCRIPTION: Illustrates how to generate an image using the `experimental_generateImage` function from the 'ai' package with a Bedrock image model. It requires importing necessary modules, specifying the model, prompt, desired size, and an optional seed for reproducibility. The result contains the generated image.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: bedrock.imageModel('amazon.nova-canvas-v1:0'),\n  prompt: 'A beautiful sunset over a calm ocean',\n  size: '512x512',\n  seed: 42,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Bedrock Embedding Model with Options in TypeScript\nDESCRIPTION: Demonstrates creating an Amazon Bedrock embedding model instance (`amazon.titan-embed-text-v2:0`) with specific configuration options using the `bedrock.embedding()` method from `@ai-sdk/amazon-bedrock`. It shows how to set the desired output `dimensions` (e.g., 512, default 1024) and whether to `normalize` the embeddings (default true).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_16\n\nLANGUAGE: ts\nCODE:\n```\nconst model = bedrock.embedding('amazon.titan-embed-text-v2:0', {\n  dimensions: 512 // optional, number of dimensions for the embedding\n  normalize: true // optional  normalize the output embeddings\n})\n```\n\n----------------------------------------\n\nTITLE: Using embedMany with OpenAI Embedding Model in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the embedMany function with an OpenAI embedding model to generate embeddings for multiple text strings. It imports necessary dependencies, specifies the model, and provides an array of values to embed.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/06-embed-many.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { embedMany } from 'ai';\n\nconst { embeddings } = await embedMany({\n  model: openai.embedding('text-embedding-3-small'),\n  values: [\n    'sunny day at the beach',\n    'rainy afternoon in the city',\n    'snowy night in the mountains',\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Using Custom Tracer in AI SDK Telemetry (TypeScript)\nDESCRIPTION: This snippet illustrates how to use a custom tracer with AI SDK telemetry. It creates a NodeTracerProvider and uses it to get a custom tracer for the AI SDK, allowing for more control over the tracing process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/60-telemetry.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst tracerProvider = new NodeTracerProvider();\nconst result = await generateText({\n  model: openai('gpt-4-turbo'),\n  prompt: 'Write a short story about a cat.',\n  experimental_telemetry: {\n    isEnabled: true,\n    tracer: tracerProvider.getTracer('ai'),\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Google Vertex Model in TypeScript\nDESCRIPTION: Demonstrates how to instantiate a Google Vertex language model instance using its model ID string (e.g., 'gemini-1.5-pro'). The vertex function is called from a provider and returns a model config object to be used in text or object generation requests. No external context required beyond the AI SDK's provider setup.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = vertex('gemini-1.5-pro');\n```\n\n----------------------------------------\n\nTITLE: Creating a Crosshatch Provider Instance - TypeScript\nDESCRIPTION: Demonstrates how to import the 'createCrosshatch' function and create an instance of the Crosshatch provider. The provider instance enables creation of customized models by specifying supported model names and options, including authentication tokens. This snippet requires that '@crosshatch/ai-provider' is installed and available.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/21-crosshatch.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createCrosshatch } from '@crosshatch/ai-provider';\nconst crosshatch = createCrosshatch();\n```\n\n----------------------------------------\n\nTITLE: Configuring Source Information\nDESCRIPTION: Shows how to enable and configure source information in the response stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_18\n\nLANGUAGE: ts\nCODE:\n```\nimport { perplexity } from '@ai-sdk/perplexity';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: perplexity('sonar-pro'),\n    messages,\n  });\n\n  return result.toDataStreamResponse({\n    sendSources: true,\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Text with AI Gateway and Vercel AI SDK - TypeScript\nDESCRIPTION: Exemplifies streaming multi-part AI-generated text via the AI Gateway using streamText from the Vercel AI SDK. The snippet sets up the provider, model, and prompt, then streams the result by iterating asynchronously through textStream. Inputs include the model and prompt; output is provided in pieces via a stream, which is accumulated in a string variable for final consumption.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/12-cloudflare-ai-gateway.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAiGateway } from 'ai-gateway-provider';\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst aigateway = createAiGateway({\n  accountId: 'your-cloudflare-account-id',\n  gateway: 'your-gateway-name',\n  apiKey: 'your-cloudflare-api-key',\n});\n\nconst openai = createOpenAI({ apiKey: 'openai-api-key' });\n\nconst result = await streamText({\n  model: aigateway([openai('gpt-4o-mini')]),\n  prompt: 'Write a multi-part greeting.',\n});\n\nlet accumulatedText = '';\nfor await (const chunk of result.textStream) {\n  accumulatedText += chunk;\n}\n\nconsole.log(accumulatedText); // Output: \"Hello world!\"\n```\n\n----------------------------------------\n\nTITLE: Importing the Mistral Provider Instance in TypeScript\nDESCRIPTION: This TypeScript code snippet demonstrates how to import the default Mistral provider instance from the `@ai-sdk/mistral` package. This instance is required to configure and use Mistral models with the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/mistral/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { mistral } from '@ai-sdk/mistral';\n```\n\n----------------------------------------\n\nTITLE: Generating Text with OpenAI Responses API using AI SDK\nDESCRIPTION: Basic example of using the AI SDK to generate text with OpenAI's Responses API using the gpt-4o model. This demonstrates the simplest way to get started with text generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/19-openai-responses.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { text } = await generateText({\n  model: openai.responses('gpt-4o'),\n  prompt: 'Explain the concept of quantum entanglement.',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Processing for Multi-Faceted Code Review\nDESCRIPTION: This code demonstrates parallel processing by running multiple specialized code reviews simultaneously (security, performance, and maintainability), then aggregating the results. The implementation uses Promise.all with the AI SDK to optimize review time while maintaining specialization.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText, generateObject } from 'ai';\nimport { z } from 'zod';\n\n// Example: Parallel code review with multiple specialized reviewers\nasync function parallelCodeReview(code: string) {\n  const model = openai('gpt-4o');\n\n  // Run parallel reviews\n  const [securityReview, performanceReview, maintainabilityReview] =\n    await Promise.all([\n      generateObject({\n        model,\n        system:\n          'You are an expert in code security. Focus on identifying security vulnerabilities, injection risks, and authentication issues.',\n        schema: z.object({\n          vulnerabilities: z.array(z.string()),\n          riskLevel: z.enum(['low', 'medium', 'high']),\n          suggestions: z.array(z.string()),\n        }),\n        prompt: `Review this code:\n      ${code}`,\n      }),\n\n      generateObject({\n        model,\n        system:\n          'You are an expert in code performance. Focus on identifying performance bottlenecks, memory leaks, and optimization opportunities.',\n        schema: z.object({\n          issues: z.array(z.string()),\n          impact: z.enum(['low', 'medium', 'high']),\n          optimizations: z.array(z.string()),\n        }),\n        prompt: `Review this code:\n      ${code}`,\n      }),\n\n      generateObject({\n        model,\n        system:\n          'You are an expert in code quality. Focus on code structure, readability, and adherence to best practices.',\n        schema: z.object({\n          concerns: z.array(z.string()),\n          qualityScore: z.number().min(1).max(10),\n          recommendations: z.array(z.string()),\n        }),\n        prompt: `Review this code:\n      ${code}`,\n      }),\n    ]);\n\n  const reviews = [\n    { ...securityReview.object, type: 'security' },\n    { ...performanceReview.object, type: 'performance' },\n    { ...maintainabilityReview.object, type: 'maintainability' },\n  ];\n\n  // Aggregate results using another model instance\n  const { text: summary } = await generateText({\n    model,\n    system: 'You are a technical lead summarizing multiple code reviews.',\n    prompt: `Synthesize these code review results into a concise summary with key actions:\n    ${JSON.stringify(reviews, null, 2)}`,\n  });\n\n  return { reviews, summary };\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Timeout for AI SDK Text Generation in TypeScript\nDESCRIPTION: This example shows how to set a timeout for text generation using the abortSignal option. It demonstrates setting a 5-second timeout for the API call.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/25-settings.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  prompt: 'Invent a new holiday and describe its traditions.',\n  abortSignal: AbortSignal.timeout(5000), // 5 seconds\n});\n```\n\n----------------------------------------\n\nTITLE: Importing useCompletion Hook - Solid\nDESCRIPTION: Shows how to import the useCompletion hook in a Solid application\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/02-use-completion.mdx#2025-04-23_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { useCompletion } from '@ai-sdk/solid'\n```\n\n----------------------------------------\n\nTITLE: Streaming Object with MockLanguageModelV1 in TypeScript\nDESCRIPTION: This example shows how to use MockLanguageModelV1 and simulateReadableStream to test the streamObject function. It simulates a stream of JSON chunks that are parsed according to a Zod schema.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/55-testing.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamObject, simulateReadableStream } from 'ai';\nimport { MockLanguageModelV1 } from 'ai/test';\nimport { z } from 'zod';\n\nconst result = streamObject({\n  model: new MockLanguageModelV1({\n    defaultObjectGenerationMode: 'json',\n    doStream: async () => ({\n      stream: simulateReadableStream({\n        chunks: [\n          { type: 'text-delta', textDelta: '{ ' },\n          { type: 'text-delta', textDelta: '\"content\": ' },\n          { type: 'text-delta', textDelta: `\"Hello, ` },\n          { type: 'text-delta', textDelta: `world` },\n          { type: 'text-delta', textDelta: `!\"` },\n          { type: 'text-delta', textDelta: ' }' },\n          {\n            type: 'finish',\n            finishReason: 'stop',\n            logprobs: undefined,\n            usage: { completionTokens: 10, promptTokens: 3 },\n          },\n        ],\n      }),\n      rawCall: { rawPrompt: null, rawSettings: {} },\n    }),\n  }),\n  schema: z.object({ content: z.string() }),\n  prompt: 'Hello, test!',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing ReplicateStream in React\nDESCRIPTION: Shows how to import the ReplicateStream utility from the AI package in a React application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/18-replicate-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ReplicateStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Llama 3.1 using Amazon Bedrock Provider\nDESCRIPTION: This snippet shows how to use the same AI SDK interface but with Amazon Bedrock as the provider for Llama 3.1 405B model. It demonstrates the flexibility of the AI SDK to easily switch between different model providers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText } from 'ai';\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\n\nconst { text } = await generateText({\n  model: bedrock('meta.llama3-1-405b-instruct-v1'),\n  prompt: 'What is love?',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-Side Actions for AI Response Processing\nDESCRIPTION: This server-side component defines the actions for processing AI interactions using the AI SDK. It includes a continueConversation function that streams UI updates from the AI model, maintains conversation history, and integrates custom tools like a simulated repository deployment function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/91-stream-updates-to-visual-interfaces.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { getMutableAIState, streamUI } from 'ai/rsc';\nimport { openai } from '@ai-sdk/openai';\nimport { ReactNode } from 'react';\nimport { z } from 'zod';\nimport { generateId } from 'ai';\n\nexport interface ServerMessage {\n  role: 'user' | 'assistant';\n  content: string;\n}\n\nexport interface ClientMessage {\n  id: string;\n  role: 'user' | 'assistant';\n  display: ReactNode;\n}\n\nexport async function continueConversation(\n  input: string,\n): Promise<ClientMessage> {\n  'use server';\n\n  const history = getMutableAIState();\n\n  const result = await streamUI({\n    model: openai('gpt-3.5-turbo'),\n    messages: [...history.get(), { role: 'user', content: input }],\n    text: ({ content, done }) => {\n      if (done) {\n        history.done((messages: ServerMessage[]) => [\n          ...messages,\n          { role: 'assistant', content },\n        ]);\n      }\n\n      return <div>{content}</div>;\n    },\n    tools: {\n      deploy: {\n        description: 'Deploy repository to vercel',\n        parameters: z.object({\n          repositoryName: z\n            .string()\n            .describe('The name of the repository, example: vercel/ai-chatbot'),\n        }),\n        generate: async function* ({ repositoryName }) {\n          yield <div>Cloning repository {repositoryName}...</div>;\n          await new Promise(resolve => setTimeout(resolve, 3000));\n          yield <div>Building repository {repositoryName}...</div>;\n          await new Promise(resolve => setTimeout(resolve, 2000));\n          return <div>{repositoryName} deployed!</div>;\n        },\n      },\n    },\n  });\n\n  return {\n    id: generateId(),\n    role: 'assistant',\n    display: result.value,\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Controlled Input\nDESCRIPTION: Example of using controlled input with the useCompletion hook for custom input components and advanced form handling.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/05-completion.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nconst { input, setInput } = useCompletion();\n\nreturn (\n  <>\n    <MyCustomInput value={input} onChange={value => setInput(value)} />\n  </>\n);\n```\n\n----------------------------------------\n\nTITLE: Language Model Response Type Definitions - TypeScript\nDESCRIPTION: Comprehensive type definitions for language model response handling, including token usage metrics, generated content, reasoning metadata, source tracking, file generation, and tool execution results.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  name: 'finishReason',\n  type: '\"stop\" | \"length\" | \"content-filter\" | \"tool-calls\" | \"error\" | \"other\" | \"unknown\"',\n  description: 'The reason the model finished generating the text.',\n},\n{\n  name: 'usage',\n  type: 'TokenUsage',\n  description: 'The token usage of the generated text.',\n  properties: [\n    {\n      type: 'TokenUsage',\n      parameters: [\n        {\n          name: 'promptTokens',\n          type: 'number',\n          description: 'The total number of tokens in the prompt.',\n        },\n        {\n          name: 'completionTokens',\n          type: 'number',\n          description: 'The total number of tokens in the completion.',\n        },\n        {\n          name: 'totalTokens',\n          type: 'number',\n          description: 'The total number of tokens generated.',\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Message Component for Displaying Streamed Text\nDESCRIPTION: A client component that utilizes the useStreamableValue hook to display text streamed from the server. It's responsible for rendering the message content as it arrives.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/120-stream-assistant-response.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { StreamableValue, useStreamableValue } from 'ai/rsc';\n\nexport function Message({ textStream }: { textStream: StreamableValue }) {\n  const [text] = useStreamableValue(textStream);\n\n  return <div>{text}</div>;\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Weather Text with OpenAI Model in TypeScript\nDESCRIPTION: This snippet demonstrates how to use an OpenAI model to generate weather information as text using a custom tool. It shows the basic structure of using a language model with a tool to produce text output.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/07-rendering-ui-with-language-models.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nconst text = generateText({\n  model: openai('gpt-3.5-turbo'),\n  system: 'You are a friendly assistant',\n  prompt: 'What is the weather in SF?',\n  tools: {\n    getWeather: {\n      description: 'Get the weather for a location',\n      parameters: z.object({\n        city: z.string().describe('The city to get the weather for'),\n        unit: z\n          .enum(['C', 'F'])\n          .describe('The unit to display the temperature in'),\n      }),\n      execute: async ({ city, unit }) => {\n        const weather = getWeather({ city, unit });\n        return `It is currently ${weather.value}Â°${unit} and ${weather.description} in ${city}!`;\n      },\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI Embedding Model in TypeScript\nDESCRIPTION: This snippet initializes an OpenAI embedding model using the .embedding() factory method from the AI SDK. It demonstrates how to specify the model id 'text-embedding-3-large' required for accessing embedding functionality. No additional dependencies beyond the SDK are necessary. The resulting model instance is ready for generating embeddings; further configuration can be performed as needed.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_26\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = openai.embedding('text-embedding-3-large');\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Environment Variables for Braintrust in Next.js\nDESCRIPTION: Sets up environment variables in a Next.js app's .env file to enable sending telemetry data to Braintrust using OpenTelemetry.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/braintrust.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOTEL_EXPORTER_OTLP_ENDPOINT=https://api.braintrust.dev/otel\nOTEL_EXPORTER_OTLP_HEADERS=\"Authorization=Bearer <Your API Key>, x-bt-parent=project_id:<Your Project ID>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure AI Provider Environment Variables (Bash)\nDESCRIPTION: Demonstrates the setup of required environment variables in a .env file to connect to the Azure AI resource. AZURE_API_ENDPOINT should be set to your Azure resource endpoint URL and AZURE_API_KEY set to the corresponding API key. These must be present in your environment for authentication, and are accessed by the provider instance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/14-azure-ai.mdx#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nAZURE_API_ENDPOINT=https://<your-resource>.services.ai.azure.com/models\nAZURE_API_KEY=<your-api-key>\n```\n\n----------------------------------------\n\nTITLE: Checking for ToolCallRepairError Instance - TypeScript\nDESCRIPTION: Demonstrates how to check if a given error object is an instance of ToolCallRepairError using the isInstance static method. Requires the 'ai' SDK module and depends on the ToolCallRepairError class being properly exported from 'ai'. Takes an error object and, if it matches ToolCallRepairError, allows for specialized error handling. Input is a generic error object; output is a boolean indicating whether the error matches. Requires TypeScript support and proper import handling.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-tool-call-repair-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ToolCallRepairError } from 'ai';\n\nif (ToolCallRepairError.isInstance(error)) {\n  // Handle the error\n}\n```\n\n----------------------------------------\n\nTITLE: Instantiating an OpenAI Language Model in TypeScript\nDESCRIPTION: Shows the basic way to create an OpenAI language model instance by calling the provider instance function (default or custom) with the desired model ID (e.g., 'gpt-4-turbo'). The AI SDK automatically selects the appropriate OpenAI API (chat or completion) based on the model ID.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = openai('gpt-4-turbo');\n```\n\n----------------------------------------\n\nTITLE: Generating Text with MockLanguageModelV1 in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the MockLanguageModelV1 to test the generateText function from the AI SDK. It sets up a mock language model that returns a predefined response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/55-testing.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { MockLanguageModelV1 } from 'ai/test';\n\nconst result = await generateText({\n  model: new MockLanguageModelV1({\n    doGenerate: async () => ({\n      rawCall: { rawPrompt: null, rawSettings: {} },\n      finishReason: 'stop',\n      usage: { promptTokens: 10, completionTokens: 20 },\n      text: `Hello, world!`,\n    }),\n  }),\n  prompt: 'Hello, test!',\n});\n```\n\n----------------------------------------\n\nTITLE: Using Text Editor Tool with generateText\nDESCRIPTION: Demonstrates how to use the Text Editor Tool with the generateText function to create and manipulate files. This example creates a file and reads its contents.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await generateText({\n  model: anthropic('claude-3-5-sonnet-20241022'),\n  prompt:\n    \"Create a new file called example.txt, write 'Hello World' to it, and run 'cat example.txt' in the terminal\",\n  tools: {\n    str_replace_editor: textEditorTool,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Weather Tool in Route Handler\nDESCRIPTION: Route handler implementation that adds a weather tool to fetch temperature data for a given location using OpenAI's GPT-4 model and Zod schema validation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom DeepInfra Provider Instance - TypeScript\nDESCRIPTION: Demonstrates how to instantiate a customized DeepInfra provider using the createDeepInfra factory method. Includes custom API key configuration (recommended to be sourced from environment variables), allowing further provider customization such as base URLs, headers, or a customer fetch implementation as needed for advanced setups.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/11-deepinfra.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createDeepInfra } from '@ai-sdk/deepinfra';\n\nconst deepinfra = createDeepInfra({\n  apiKey: process.env.DEEPINFRA_API_KEY ?? '',\n});\n```\n\n----------------------------------------\n\nTITLE: Advanced Text Generation with System Prompt in TypeScript\nDESCRIPTION: Shows how to use generateText with a more complex prompt, including a system message to set the context for the AI's response. This example demonstrates summarizing an article.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: yourModel,\n  system:\n    'You are a professional writer. ' +\n    'You write simple, clear, and concise content.',\n  prompt: `Summarize the following article in 3-5 sentences: ${article}`,\n});\n```\n\n----------------------------------------\n\nTITLE: Handling Tool Results Client-Side with onToolCall and useChat (TypeScript/React)\nDESCRIPTION: Illustrates client-side processing for tool results using the 'onToolCall' callback within the 'useChat' React hook from the Vercel AI toolkit. No 'execute' function is needed; instead, when a tool is called (e.g., 'getLocation'), the handler fetches data via a custom async function (like 'getLocationData') and returns it, satisfying the model's requirement for a result. Dependencies: 'useChat' React hook and user-implemented async data-fetching functions. Key input: object with 'toolCall' details; output: async return value as the tool result. Note: Every tool invocation must produce a result before further messages are processed.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/05-tool-invocation-missing-result.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst { messages } = useChat({\n  // Option 1: Handle using onToolCall\n  onToolCall: async ({ toolCall }) => {\n    if (toolCall.toolName === 'getLocation') {\n      const result = await getLocationData();\n      return result; // This becomes the tool result\n    }\n  },\n});\n\n}\n```\n\n----------------------------------------\n\nTITLE: Basic ID Generator Creation and Usage in TypeScript\nDESCRIPTION: Demonstrates how to import and create a basic ID generator with custom prefix and separator options, then use it to generate a unique identifier.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/91-create-id-generator.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createIdGenerator } from 'ai';\n\nconst generateCustomId = createIdGenerator({\n  prefix: 'user',\n  separator: '_',\n});\n\nconst id = generateCustomId(); // Example: \"user_1a2b3c4d5e6f7g8h\"\n```\n\n----------------------------------------\n\nTITLE: Checking for ToolExecutionError in Vercel AI SDK (TypeScript)\nDESCRIPTION: This TypeScript code snippet demonstrates how to check if a caught error is an instance of `ToolExecutionError` from the Vercel AI SDK. It imports `ToolExecutionError` from the 'ai' package and uses the static `isInstance` method for type checking, allowing for specific handling of tool execution failures.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-tool-execution-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nimport { ToolExecutionError } from 'ai';\n\nif (ToolExecutionError.isInstance(error)) {\n  // Handle the error\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_InvalidDataContent Error Instance in TypeScript\nDESCRIPTION: This TypeScript code demonstrates how to identify if a caught error is specifically an `AI_InvalidDataContent` error. It imports the `InvalidDataContent` class from the 'ai' package and uses the static `isInstance` method to perform the type check within a conditional block. This pattern allows for specific error handling logic tailored to invalid data content issues.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-invalid-data-content.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nimport { InvalidDataContent } from 'ai';\n\nif (InvalidDataContent.isInstance(error)) {\n  // Handle the error\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Enhanced AI Response Generator with Tools\nDESCRIPTION: Advanced implementation adding weather and web search tools to the generateResponse function, enabling multi-step conversations and tool-based interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/03-slackbot.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { CoreMessage, generateText, tool } from 'ai';\nimport { z } from 'zod';\nimport { exa } from './utils';\n\nexport const generateResponse = async (\n  messages: CoreMessage[],\n  updateStatus?: (status: string) => void,\n) => {\n  const { text } = await generateText({\n    model: openai('gpt-4o'),\n    system: `You are a Slack bot assistant. Keep your responses concise and to the point.\n    - Do not tag users.\n    - Current date is: ${new Date().toISOString().split('T')[0]}\n    - Always include sources in your final response if you use web search.`,\n    messages,\n    maxSteps: 10,\n    tools: {\n      getWeather: tool({\n        description: 'Get the current weather at a location',\n        parameters: z.object({\n          latitude: z.number(),\n          longitude: z.number(),\n          city: z.string(),\n        }),\n        execute: async ({ latitude, longitude, city }) => {\n          updateStatus?.(`is getting weather for ${city}...`);\n\n          const response = await fetch(\n            `https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature_2m,weathercode,relativehumidity_2m&timezone=auto`,\n          );\n\n          const weatherData = await response.json();\n          return {\n            temperature: weatherData.current.temperature_2m,\n            weatherCode: weatherData.current.weathercode,\n            humidity: weatherData.current.relativehumidity_2m,\n            city,\n          };\n        },\n      }),\n      searchWeb: tool({\n        description: 'Use this to search the web for information',\n        parameters: z.object({\n          query: z.string(),\n          specificDomain: z\n            .string()\n            .nullable()\n            .describe(\n              'a domain to search if the user specifies e.g. bbc.com. Should be only the domain name without the protocol',\n            ),\n        }),\n        execute: async ({ query, specificDomain }) => {\n          updateStatus?.(`is searching the web for ${query}...`);\n          const { results } = await exa.searchAndContents(query, {\n            livecrawl: 'always',\n            numResults: 3,\n            includeDomains: specificDomain ? [specificDomain] : undefined,\n          });\n\n          return {\n            results: results.map(result => ({\n              title: result.title,\n              url: result.url,\n              snippet: result.text.slice(0, 1000),\n            })),\n          };\n        },\n      }),\n    },\n  });\n\n  // Convert markdown to Slack mrkdwn format\n  return text.replace(/\\[(.*?)\\]\\((.*?)\\)/g, '<$2|$1>').replace(/\\*\\*/g, '*');\n};\n```\n\n----------------------------------------\n\nTITLE: Importing streamText Function in TypeScript\nDESCRIPTION: This code snippet shows how to import the streamText function from the 'ai' module. It's a simple import statement that allows you to use the streamText function in your TypeScript code.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Defining Chat Model Settings for Custom AI SDK Provider in TypeScript\nDESCRIPTION: Defines TypeScript types for custom chat model IDs (`ExampleChatModelId`) and extends `OpenAICompatibleChatSettings` to create specific settings (`ExampleChatSettings`) for the chat model within the provider. This allows for defining unique model identifiers and potentially adding custom configuration options beyond the standard OpenAI compatible ones. It depends on `@ai-sdk/openai-compatible`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAICompatibleChatSettings } from '@ai-sdk/openai-compatible';\n\nexport type ExampleChatModelId =\n  | 'example/chat-model-1'\n  | 'example/chat-model-2'\n  | (string & {});\n\nexport interface ExampleChatSettings extends OpenAICompatibleChatSettings {\n  // Add any custom settings here\n}\n```\n\n----------------------------------------\n\nTITLE: Framework Support Comparison Table\nDESCRIPTION: A markdown table comparing the support of various AI SDK UI functions across React, Svelte, Vue.js, and SolidJS frameworks. Shows feature availability with checkmarks and crosses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/index.mdx#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Function                                                  | React               | Svelte                               | Vue.js              | SolidJS (deprecated) |\n| --------------------------------------------------------- | ------------------- | ------------------------------------ | ------------------- | -------------------- |\n| [useChat](/docs/reference/ai-sdk-ui/use-chat)             | <Check size={18} /> | <Check size={18} /> Chat             | <Check size={18} /> | <Check size={18} />  |\n| [useCompletion](/docs/reference/ai-sdk-ui/use-completion) | <Check size={18} /> | <Check size={18} /> Completion       | <Check size={18} /> | <Check size={18} />  |\n| [useObject](/docs/reference/ai-sdk-ui/use-object)         | <Check size={18} /> | <Check size={18} /> StructuredObject | <Cross size={18} /> | <Check size={18} />  |\n| [useAssistant](/docs/reference/ai-sdk-ui/use-assistant)   | <Check size={18} /> | <Cross size={18} />                  | <Check size={18} /> | <Check size={18} />  |\n```\n\n----------------------------------------\n\nTITLE: Displaying AI Model Compatibility in Markdown Table\nDESCRIPTION: This snippet shows a row from a markdown table that displays compatibility information for the 'gemma2-9b-it' model with the Groq provider. It uses icons to indicate feature support.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/02-providers-and-models.mdx#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| [Groq](/providers/ai-sdk-providers/groq)                                 | `gemma2-9b-it`                              | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\n```\n\n----------------------------------------\n\nTITLE: Generating Text with LM Studio and AI SDK in TypeScript\nDESCRIPTION: Provides a complete example of generating text using a local model via LM Studio. It imports necessary functions (`createOpenAICompatible`, `generateText`), creates the LM Studio provider instance pointing to the local server, specifies the model ID ('llama-3.2-1b'), provides a prompt, and uses the `generateText` function from the 'ai' package. It sets `maxRetries` to 1 to immediately surface errors if the LM Studio server isn't running.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/30-lmstudio.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { generateText } from 'ai';\n\nconst lmstudio = createOpenAICompatible({\n  name: 'lmstudio',\n  baseURL: 'https://localhost:1234/v1',\n});\n\nconst { text } = await generateText({\n  model: lmstudio('llama-3.2-1b'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  maxRetries: 1, // immediately error if the server is not running\n});\n```\n\n----------------------------------------\n\nTITLE: Using the Caching Middleware with OpenAI\nDESCRIPTION: Example implementation showing how to integrate the caching middleware with the OpenAI API client. Demonstrates streaming text generation with cache integration and response handling.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/80-local-caching-middleware.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport 'dotenv/config';\nimport { cached } from '../middleware/your-cache-middleware';\n\nasync function main() {\n  const result = streamText({\n    model: cached(openai('gpt-4o')),\n    maxTokens: 512,\n    temperature: 0.3,\n    maxRetries: 5,\n    prompt: 'Invent a new holiday and describe its traditions.',\n  });\n\n  for await (const textPart of result.textStream) {\n    process.stdout.write(textPart);\n  }\n\n  console.log();\n  console.log('Token usage:', await result.usage);\n  console.log('Finish reason:', await result.finishReason);\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Together.ai\nDESCRIPTION: Demonstrates how to use Together.ai image models to generate images using the experimental_generateImage function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/24-togetherai.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { togetherai } from '@ai-sdk/togetherai';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { images } = await generateImage({\n  model: togetherai.image('black-forest-labs/FLUX.1-dev'),\n  prompt: 'A delighted resplendent quetzal mid flight amidst raindrops',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Cohere Embedding Model in TypeScript\nDESCRIPTION: Shows how to create an embedding model using Cohere's embedding API. This example uses the 'embed-english-v3.0' model which provides 1024-dimensional embeddings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/25-cohere.mdx#2025-04-23_snippet_4\n\nLANGUAGE: ts\nCODE:\n```\nconst model = cohere.embedding('embed-english-v3.0');\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Streaming Object Generation with TypeScript\nDESCRIPTION: This snippet shows how to add an onError callback to the streamObject function for error logging during streaming object generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamObject } from 'ai';\n\nconst result = streamObject({\n  // ...\n  onError({ error }) {\n    console.error(error); // your error logging logic here\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key in Environment File\nDESCRIPTION: Environment variable setup for storing the OpenAI API key in the .env.local file.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_4\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=xxxxxxxxx\n```\n\n----------------------------------------\n\nTITLE: Re-ranking Items by Recent Purchases using streamObject & Crosshatch - TypeScript\nDESCRIPTION: Demonstrates using Crosshatch and the 'streamObject' function from 'ai' to re-rank a list of items based on the user's recent purchases. The model is configured to pull recent purchase events from the user's timeline via a structured 'replace' query. Asynchronously streams JSON results validating response items against a schema with 'id' and 'reason'. Prerequisites: '@crosshatch/ai-provider', 'ai', and a valid authentication token. Inputs are a list of item summaries and user context; outputs are result streams conforming to a specified schema. Limitations include the requirement for valid data schemas and tokens.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/21-crosshatch.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamObject } from 'ai';\nimport createCrosshatch from `@crosshatch/ai-provider`\nconst crosshatch = createCrosshatch();\n\nconst itemSummaries = [...]; // list of items\nconst ids = (itemSummaries?.map(({ itemId }) => itemId) ?? []) as string[];\n\nconst { elementStream } = streamObject({\n  output: \"array\",\n  mode: \"json\",\n  model: crosshatch.languageModel(\"gpt-4o-mini\", {\n    token,\n    replace: {\n      \"orders\": {\n        select: [\"originalTimestamp\", \"entity_name\", \"order_total\", \"order_summary\"],\n        from: \"personalTimeline\",\n        where: [{ field: \"event\", op: \"=\", value: \"purchased\" }],\n        orderBy: [{ field: \"originalTimestamp\", dir: \"desc\" }],\n        limit: 5,\n      },\n    },\n  }),\n  system: `Rerank the following items based on alignment with users recent purchases {orders}`,\n  messages: [{role: \"user\", content: \"Heres a list of item: ${JSON.stringify(itemSummaries)\"},],\n  schema: jsonSchema<{ id: string; reason: string }>({\n    type: \"object\",\n    properties: {\n      id: { type: \"string\", enum: ids },\n      reason: { type: \"string\", description: \"Explain your ranking.\" },\n    },\n  }),\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a Cohere Language Model in TypeScript\nDESCRIPTION: Demonstrates how to create a language model instance using the Cohere provider. This example specifies the model ID 'command-r-plus', which is one of Cohere's models that supports tool calls.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/25-cohere.mdx#2025-04-23_snippet_2\n\nLANGUAGE: ts\nCODE:\n```\nconst model = cohere('command-r-plus');\n```\n\n----------------------------------------\n\nTITLE: Checking for NoSuchProviderError Instance in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to identify if a caught error object is an instance of `NoSuchProviderError` from the 'ai' library. It uses the static `isInstance` method for type checking, allowing developers to implement specific error handling logic when an AI provider ID is not found. Requires the 'ai' package dependency.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-no-such-provider-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nimport { NoSuchProviderError } from 'ai';\n\nif (NoSuchProviderError.isInstance(error)) {\n  // Handle the error\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Utilizing DeepSeek's Reasoning Capabilities\nDESCRIPTION: Example demonstrating how to leverage DeepSeek's reasoning capabilities with the deepseek-reasoner model to get both the final text and reasoning steps.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/30-deepseek.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepseek } from '@ai-sdk/deepseek';\nimport { generateText } from 'ai';\n\nconst { text, reasoning } = await generateText({\n  model: deepseek('deepseek-reasoner'),\n  prompt: 'How many people will live in the world in 2040?',\n});\n\nconsole.log(reasoning);\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Enabling Multi-Step Tool Processing\nDESCRIPTION: Simple update to the Chat component to enable multi-step tool processing with maxSteps configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    maxSteps: 5,\n  });\n\n  // ... rest of your component code\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Error Handler to Stream Response\nDESCRIPTION: Shows how to apply the custom error handler to streamText response using toDataStreamResponse.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\nconst result = streamText({\n  // ...\n});\n\nreturn result.toDataStreamResponse({\n  getErrorMessage: errorHandler,\n});\n```\n\n----------------------------------------\n\nTITLE: Upgrading AI SDK Dependency with pnpm - Shell\nDESCRIPTION: This snippet shows how to update the Vercel AI SDK to version 3.1 using the pnpm package manager. It is intended to be run in your project's root directory. No prior code context is required besides a Node.js project using pnpm and the AI SDK package. The command fetches and installs the latest SDK version as specified, allowing you to access new features and APIs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/39-migration-guide-3-1.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add ai@3.1\n```\n\n----------------------------------------\n\nTITLE: Creating LM Studio Provider Instance in TypeScript\nDESCRIPTION: Demonstrates how to create a custom provider instance for LM Studio using the `createOpenAICompatible` function from `@ai-sdk/openai-compatible`. It configures the provider with a name ('lmstudio') and the default base URL (`http://localhost:1234/v1`) for the LM Studio local server API. This instance is then used to interact with models served by LM Studio.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/30-lmstudio.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\n\nconst lmstudio = createOpenAICompatible({\n  name: 'lmstudio',\n  baseURL: 'http://localhost:1234/v1',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating AI Context with createAI in TypeScript\nDESCRIPTION: This code creates an AI context using the createAI function from the ai/rsc package. It defines the AI and UI state types and initializes the context with empty states and a sendMessage action.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAI } from 'ai/rsc';\nimport { ClientMessage, ServerMessage, sendMessage } from './actions';\n\nexport type AIState = ServerMessage[];\nexport type UIState = ClientMessage[];\n\nexport const AI = createAI<AIState, UIState>({\n  initialAIState: [],\n  initialUIState: [],\n  actions: {\n    sendMessage,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Clearing Data on Form Submit with useChat Hook in React\nDESCRIPTION: This snippet demonstrates a complete example of using the useChat hook in a React component. It shows how to display messages and data, handle form submission, and clear stream data when submitting a new message.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/20-streaming-data.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { Message, useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit, data, setData } =\n    useChat();\n\n  return (\n    <>\n      {data && <pre>{JSON.stringify(data, null, 2)}</pre>}\n\n      {messages?.map((m: Message) => (\n        <div key={m.id}>{`${m.role}: ${m.content}`}</div>\n      ))}\n\n      <form\n        onSubmit={e => {\n          setData(undefined); // clear stream data\n          handleSubmit(e);\n        }}\n      >\n        <input value={input} onChange={handleInputChange} />\n      </form>\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Describing LLM Stream and Return Types - Vercel AI (TypeScript)\nDESCRIPTION: This snippet details the structure of objects returned from an LLM API call, including the shape of streamed events and final output values. It uses TypeScript notation for properties such as the ReactNode UI result, provider warnings, optional raw HTTP response data, and the rich structure of the streaming API (AsyncIterable & ReadableStream of StreamPart variants). Each stream event (text delta, tool call, error, finish) is explicitly broken down, ensuring type safety and exhaustiveness. Consumption of the stream and values depends on compatible consumer code and Vercel AI conventions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/01-stream-ui.mdx#2025-04-23_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\n// LLM Streaming and Return Types for Vercel AI\n[\n  {\n    name: 'value',\n    type: 'ReactNode',\n    description: 'The user interface based on the stream output.',\n  },\n  {\n    name: 'rawResponse',\n    type: 'RawResponse',\n    isOptional: true,\n    description: 'Optional raw response data.',\n    properties: [\n      {\n        type: 'RawResponse',\n        parameters: [\n          {\n            name: 'headers',\n            isOptional: true,\n            type: 'Record<string, string>',\n            description: 'Response headers.',\n          },\n        ],\n      },\n    ],\n  },\n  {\n    name: 'warnings',\n    type: 'Warning[] | undefined',\n    description:\n      'Warnings from the model provider (e.g. unsupported settings).',\n  },\n  {\n    name: 'stream',\n    type: 'AsyncIterable<StreamPart> & ReadableStream<StreamPart>',\n    description:\n      'A stream with all events, including text deltas, tool calls, tool results, and errors. You can use it as either an AsyncIterable or a ReadableStream. When an error occurs, the stream will throw the error.',\n    properties: [\n      {\n        type: 'StreamPart',\n        parameters: [\n          {\n            name: 'type',\n            type: \"'text-delta'\",\n            description: 'The type to identify the object as text delta.',\n          },\n          {\n            name: 'textDelta',\n            type: 'string',\n            description: 'The text delta.',\n          },\n        ],\n      },\n      {\n        type: 'StreamPart',\n        parameters: [\n          {\n            name: 'type',\n            type: \"'tool-call'\",\n            description: 'The type to identify the object as tool call.',\n          },\n          {\n            name: 'toolCallId',\n            type: 'string',\n            description: 'The id of the tool call.',\n          },\n          {\n            name: 'toolName',\n            type: 'string',\n            description:\n              'The name of the tool, which typically would be the name of the function.',\n          },\n          {\n            name: 'args',\n            type: 'object based on zod schema',\n            description:\n              'Parameters generated by the model to be used by the tool.',\n          },\n        ],\n      },\n      {\n        type: 'StreamPart',\n        parameters: [\n          {\n            name: 'type',\n            type: \"'error'\",\n            description: 'The type to identify the object as error.',\n          },\n          {\n            name: 'error',\n            type: 'Error',\n            description:\n              'Describes the error that may have occurred during execution.',\n          },\n        ],\n      },\n      {\n        type: 'StreamPart',\n        parameters: [\n          {\n            name: 'type',\n            type: \"'finish'\",\n            description: 'The type to identify the object as finish.',\n          },\n          {\n            name: 'finishReason',\n            type: \"'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'\",\n            description: 'The reason the model finished generating the text.',\n          },\n          {\n            name: 'usage',\n            type: 'TokenUsage',\n            description: 'The token usage of the generated text.',\n            properties: [\n              {\n                type: 'TokenUsage',\n                parameters: [\n                  {\n                    name: 'promptTokens',\n                    type: 'number',\n                    description: 'The total number of tokens in the prompt.',\n                  },\n                  {\n                    name: 'completionTokens',\n                    type: 'number',\n                    description:\n                      'The total number of tokens in the completion.',\n                  },\n                  {\n                    name: 'totalTokens',\n                    type: 'number',\n                    description: 'The total number of tokens generated.',\n                  },\n                ],\n              },\n            ],\n          },\n        ],\n      },\n    ],\n  },\n]\n\n```\n\n----------------------------------------\n\nTITLE: Creating Deepgram Transcription Model in TypeScript\nDESCRIPTION: Initialize a Deepgram transcription model by specifying the model ID. This example uses the 'nova-3' model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/110-deepgram.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = deepgram.transcription('nova-3');\n```\n\n----------------------------------------\n\nTITLE: Next.js Chat Interface Implementation\nDESCRIPTION: Creates a client-side chat interface using the useChat hook from AI SDK UI. Implements message display and input handling functionality.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/24-o3.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, handleInputChange, handleSubmit, error } = useChat();\n\n  return (\n    <>\n      {messages.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.content}\n        </div>\n      ))}\n      <form onSubmit={handleSubmit}>\n        <input name=\"prompt\" value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Submit</button>\n      </form>\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Next.js Chat Route Handler Setup\nDESCRIPTION: Implements a Next.js route handler for chat functionality using o3-mini. Sets up streaming text responses with a 5-minute duration limit.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/24-o3.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\n// Allow responses up to 5 minutes\nexport const maxDuration = 300;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('o3-mini'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Search Grounding Enabled (TypeScript)\nDESCRIPTION: Shows how to enable Google Search-based grounding via the 'useSearchGrounding' option on the model, and access detailed provider metadata (groundingMetadata and safetyRatings) from the generateText response. Illustrates typing with GoogleGenerativeAIProviderMetadata for type safety. Requires '@ai-sdk/google-vertex', '@ai-sdk/google', and 'ai'.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertex } from '@ai-sdk/google-vertex';\nimport { GoogleGenerativeAIProviderMetadata } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst { text, providerMetadata } = await generateText({\n  model: vertex('gemini-1.5-pro', {\n    useSearchGrounding: true,\n  }),\n  prompt:\n    'List the top 5 San Francisco news from the past week.' +\n    'You must include the date of each article.',\n});\n\n// access the grounding metadata. Casting to the provider metadata type\n// is optional but provides autocomplete and type safety.\nconst metadata = providerMetadata?.google as\n  | GoogleGenerativeAIProviderMetadata\n  | undefined;\nconst groundingMetadata = metadata?.groundingMetadata;\nconst safetyRatings = metadata?.safetyRatings;\n```\n\n----------------------------------------\n\nTITLE: Using pipeTextStreamToResponse for Text Streaming in Express\nDESCRIPTION: This example demonstrates how to use the pipeTextStreamToResponse method to send a text stream to the client in an Express server. It creates a simple POST endpoint that generates and streams plain text from the AI model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/20-express.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport express, { Request, Response } from 'express';\n\nconst app = express();\n\napp.post('/', async (req: Request, res: Response) => {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    prompt: 'Invent a new holiday and describe its traditions.',\n  });\n\n  result.pipeTextStreamToResponse(res);\n});\n\napp.listen(8080, () => {\n  console.log(`Example app listening on port ${8080}`);\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with System Message in TypeScript\nDESCRIPTION: This snippet illustrates how to generate text using an AI model with a system message to guide the assistant's behavior for travel itinerary planning.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  messages: [\n    { role: 'system', content: 'You help planning travel itineraries.' },\n    {\n      role: 'user',\n      content:\n        'I am planning a trip to Berlin for 3 days. Please suggest the best tourist activities for me to do.',\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_APICallError Type in TypeScript\nDESCRIPTION: Demonstrates how to verify if an error is an instance of AI_APICallError using the isInstance static method from the AI library.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-api-call-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { APICallError } from 'ai';\n\nif (APICallError.isInstance(error)) {\n  // Handle the error\n}\n```\n\n----------------------------------------\n\nTITLE: Generating an Image with a Replicate Model (TypeScript)\nDESCRIPTION: Shows an end-to-end example of generating an image using the Replicate provider and saving it to disk. The code uses replicate.image to specify the model, generates an image with the provided prompt and aspect ratio, and saves the result in webp format. Required dependencies: ai, @ai-sdk/replicate, and Node.js fs/promises module. Inputs include the model name, prompt, and aspect ratio; output is a saved image file.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/60-replicate.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { replicate } from '@ai-sdk/replicate';\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { writeFile } from 'node:fs/promises';\n\nconst { image } = await generateImage({\n  model: replicate.image('black-forest-labs/flux-schnell'),\n  prompt: 'The Loch Ness Monster getting a manicure',\n  aspectRatio: '16:9',\n});\n\nawait writeFile('image.webp', image.uint8Array);\n\nconsole.log('Image saved as image.webp');\n```\n\n----------------------------------------\n\nTITLE: Adding Response Messages to Conversation History in AI SDK Core\nDESCRIPTION: This snippet demonstrates how to add the generated assistant and tool messages to a conversation history using the response.messages property from generateText. It appends the new messages to an existing array of CoreMessage objects.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\n\nconst messages: CoreMessage[] = [\n  // ...\n];\n\nconst { response } = await generateText({\n  // ...\n  messages,\n});\n\n// add the response messages to your conversation history:\nmessages.push(...response.messages); // streamText: ...((await response).messages)\n```\n\n----------------------------------------\n\nTITLE: AI SDK Configuration Parameters TypeScript Interface\nDESCRIPTION: TypeScript interface defining configuration parameters for the Vercel AI SDK. Includes settings for model behavior, token limits, randomization controls, retry logic, telemetry, and callback functions for error handling and completion events.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/04-stream-object.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  maxTokens?: number;\n  temperature?: number;\n  topP?: number;\n  topK?: number;\n  presencePenalty?: number;\n  frequencyPenalty?: number;\n  seed?: number;\n  maxRetries?: number;\n  abortSignal?: AbortSignal;\n  headers?: Record<string, string>;\n  experimental_telemetry?: TelemetrySettings;\n  providerOptions?: Record<string,Record<string,JSONValue>>;\n  onError?: (event: OnErrorResult) => Promise<void> | void;\n  onFinish?: (result: OnFinishResult) => void;\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Streamed Loading State on Client with AI SDK RSC\nDESCRIPTION: This client-side code snippet shows how to handle streamed loading state from the server using AI SDK RSC. It updates the UI based on both the response stream and the loading state stream, providing more detailed feedback about the generation process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/06-loading-state.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { generateResponse } from './actions';\nimport { readStreamableValue } from 'ai/rsc';\n\n// Force the page to be dynamic and allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [input, setInput] = useState<string>('');\n  const [generation, setGeneration] = useState<string>('');\n  const [loading, setLoading] = useState<boolean>(false);\n\n  return (\n    <div>\n      <div>{generation}</div>\n      <form\n        onSubmit={async e => {\n          e.preventDefault();\n          setLoading(true);\n          const { response, loadingState } = await generateResponse(input);\n\n          let textContent = '';\n\n          for await (const responseDelta of readStreamableValue(response)) {\n            textContent = `${textContent}${responseDelta}`;\n            setGeneration(textContent);\n          }\n          for await (const loadingDelta of readStreamableValue(loadingState)) {\n            if (loadingDelta) {\n              setLoading(loadingDelta.loading);\n            }\n          }\n          setInput('');\n          setLoading(false);\n        }}\n      >\n        <input\n          type=\"text\"\n          value={input}\n          disabled={loading}\n          className=\"disabled:opacity-50\"\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button>Send Message</button>\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Reasoning from Deepseek-R1 Model\nDESCRIPTION: How to enhance a model with middleware to extract reasoning from the deepseek-r1 model that exposes thinking in <think> tags.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/26-fireworks.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fireworks } from '@ai-sdk/fireworks';\nimport { wrapLanguageModel, extractReasoningMiddleware } from 'ai';\n\nconst enhancedModel = wrapLanguageModel({\n  model: fireworks('accounts/fireworks/models/deepseek-r1'),\n  middleware: extractReasoningMiddleware({ tagName: 'think' }),\n});\n```\n\n----------------------------------------\n\nTITLE: Context-Enhanced LLM Query Example\nDESCRIPTION: Shows how providing context to an LLM can improve its response accuracy by including relevant information in the prompt.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_1\n\nLANGUAGE: txt\nCODE:\n```\n**input**\nRespond to the user's prompt using only the provided context.\nuser prompt: 'What is my favorite food?'\ncontext: user loves chicken nuggets\n\n**generation**\nYour favorite food is chicken nuggets!\n```\n\n----------------------------------------\n\nTITLE: Modeling Tool Invocation States for Chat (TypeScript)\nDESCRIPTION: This snippet documents the 'ToolInvocation' type, which encodes the process of calling a tool, tracking the lifecycle state: partial-call, call, or result. The relevant fields include a state discriminator, toolCallId, toolName, arguments (JSON-serializable), and for the 'result' state, an additional result field. Prerequisites are type access to any tool argument/result schema, and the IDs should match between invocation steps. Inputs must set state appropriately; outputs represent tool call progress or completion. Limitation: Business logic for matching toolCallId or schema validation is not provided.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#2025-04-23_snippet_6\n\nLANGUAGE: TypeScript\nCODE:\n```\ntype ToolInvocation =\n  | {\n      state: 'partial-call';\n      toolCallId: string;\n      toolName: string;\n      args: any; // Partial arguments, JSON-serializable.\n    }\n  | {\n      state: 'call';\n      toolCallId: string;\n      toolName: string;\n      args: any; // Arguments matching the tool's input schema.\n    }\n  | {\n      state: 'result';\n      toolCallId: string;\n      toolName: string;\n      args: any;\n      result: any; // Tool call result.\n    };\n\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Amazon Bedrock Model\nDESCRIPTION: Example of using Amazon Bedrock with the generateText function to create a vegetarian lasagna recipe.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: bedrock('meta.llama3-70b-instruct-v1:0'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text and Accessing Response Headers with Bedrock in TypeScript\nDESCRIPTION: Shows how to generate text using a Bedrock model (`meta.llama3-70b-instruct-v1:0`) via the `generateText` function from the 'ai' package and access the underlying HTTP response headers returned by the Bedrock API. The headers are available on the `response.headers` property of the result object.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_21\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: bedrock('meta.llama3-70b-instruct-v1:0'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n\nconsole.log(result.response.headers);\n```\n\n----------------------------------------\n\nTITLE: Migrating Roundtrip Settings to maxSteps and Updated Return Structure - TypeScript\nDESCRIPTION: Shows how to remove the maxToolRoundtrips and maxAutomaticRoundtrips parameters from generateText and use maxSteps instead. Also updates usage from the roundtrips property to the steps property in the result. Input: generateText configuration. Output: object with text and steps properties.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_12\n\nLANGUAGE: ts\nCODE:\n```\nconst { text, roundtrips } = await generateText({\n  maxToolRoundtrips: 1, // or maxAutomaticRoundtrips\n  // ...\n});\n```\n\nLANGUAGE: ts\nCODE:\n```\nconst { text, steps } = await generateText({\n  maxSteps: 2,\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Generative AI Embedding Model with Options\nDESCRIPTION: Creating a Google Generative AI text embedding model with additional configuration options including outputDimensionality and taskType settings for specialized embedding generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = google.textEmbeddingModel('text-embedding-004', {\n  outputDimensionality: 512, // optional, number of dimensions for the embedding\n  taskType: 'SEMANTIC_SIMILARITY', // optional, specifies the task type for generating embeddings\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Cohere Embedding Model with Custom Settings in TypeScript\nDESCRIPTION: Demonstrates creating an embedding model with additional configuration options. This example specifies the inputType as 'search_document', which is optimized for embeddings stored in a vector database.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/25-cohere.mdx#2025-04-23_snippet_5\n\nLANGUAGE: ts\nCODE:\n```\nconst model = cohere.embedding('embed-english-v3.0', {\n  inputType: 'search_document',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Default LMNT Provider Instance in TypeScript\nDESCRIPTION: How to import the pre-configured default LMNT provider instance for immediate use in applications.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/140-lmnt.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { lmnt } from '@ai-sdk/lmnt';\n```\n\n----------------------------------------\n\nTITLE: Using reasoning capabilities with Anthropic models\nDESCRIPTION: Shows how to enable and use the reasoning capabilities of compatible Anthropic models, specifically with claude-3-7-sonnet. This feature allows the model to show its thinking process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';\nimport { generateText } from 'ai';\n\nconst { text, reasoning, reasoningDetails } = await generateText({\n  model: anthropic('claude-3-7-sonnet-20250219'),\n  prompt: 'How many people will live in the world in 2040?',\n  providerOptions: {\n    anthropic: {\n      thinking: { type: 'enabled', budgetTokens: 12000 },\n    } satisfies AnthropicProviderOptions,\n  },\n});\n\nconsole.log(reasoning); // reasoning text\nconsole.log(reasoningDetails); // reasoning details including redacted reasoning\nconsole.log(text); // text response\n```\n\n----------------------------------------\n\nTITLE: Checking for NoContentGeneratedError in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to check if a caught error is an instance of `NoContentGeneratedError` from the 'ai' package. It imports the specific error class and uses its static `isInstance` method for type checking, allowing for targeted error handling when the AI fails to produce content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-no-content-generated-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nimport { NoContentGeneratedError } from 'ai';\n\nif (NoContentGeneratedError.isInstance(error)) {\n  // Handle the error\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Importing the embed Function in TypeScript\nDESCRIPTION: This snippet shows the import statement for the embed function from the 'ai' package. It's a simple one-line import that allows developers to use the embed function in their code.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/05-embed.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { embed } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Generating AI Prompt for LLM Usage\nDESCRIPTION: Demonstrates how to format a prompt for asking questions about the AI SDK using an LLM, incorporating the SDK's documentation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/01-introduction/index.mdx#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```prompt\nDocumentation:\n{paste documentation here}\n---\nBased on the above documentation, answer the following:\n{your question}\n```\n```\n\n----------------------------------------\n\nTITLE: Wrapping Application with AI Context in TSX\nDESCRIPTION: This snippet shows how to wrap the root layout of a Next.js application with the AI context provider, enabling state management across the entire app.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { type ReactNode } from 'react';\nimport { AI } from './ai';\n\nexport default function RootLayout({\n  children,\n}: Readonly<{ children: ReactNode }>) {\n  return (\n    <AI>\n      <html lang=\"en\">\n        <body>{children}</body>\n      </html>\n    </AI>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Lifecycle Callbacks for AI SDK Caching\nDESCRIPTION: Example of using lifecycle callbacks with Next.js API route to implement caching for OpenAI responses using Redis. Includes response streaming and cache checking with a 1-hour expiration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/04-caching.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { formatDataStreamPart, streamText } from 'ai';\nimport { Redis } from '@upstash/redis';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nconst redis = new Redis({\n  url: process.env.KV_URL,\n  token: process.env.KV_TOKEN,\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  // come up with a key based on the request:\n  const key = JSON.stringify(messages);\n\n  // Check if we have a cached response\n  const cached = await redis.get(key);\n  if (cached != null) {\n    return new Response(formatDataStreamPart('text', cached), {\n      status: 200,\n      headers: { 'Content-Type': 'text/plain' },\n    });\n  }\n\n  // Call the language model:\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n    async onFinish({ text }) {\n      // Cache the response text:\n      await redis.set(key, text);\n      await redis.expire(key, 60 * 60);\n    },\n  });\n\n  // Respond with the stream\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Generating an Image with Luma Photon Model (TypeScript)\nDESCRIPTION: Demonstrates how to generate an image using the Luma provider and the `experimental_generateImage` function from the 'ai' package. It specifies the 'photon-1' model, provides a prompt, generates the image, and saves it as a PNG file using the Node.js `fs` module. Dependencies include `@ai-sdk/luma`, `ai`, and the Node.js `fs` module.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/luma/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { luma } from '@ai-sdk/luma';\nimport { experimental_generateImage as generateImage } from 'ai';\nimport fs from 'fs';\n\nconst { image } = await generateImage({\n  model: luma.image('photon-1'),\n  prompt: 'A serene mountain landscape at sunset',\n});\n\nconst filename = `image-${Date.now()}.png`;\nfs.writeFileSync(filename, image.uint8Array);\nconsole.log(`Image saved to ${filename}`);\n```\n\n----------------------------------------\n\nTITLE: Creating Memoized Markdown Component\nDESCRIPTION: Implements a memoized Markdown component that optimizes rendering performance by splitting content into blocks and only re-rendering changed blocks. Uses marked for parsing and React's memo for optimization.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/25-markdown-chatbot-with-memoization.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { marked } from 'marked';\nimport { memo, useMemo } from 'react';\nimport ReactMarkdown from 'react-markdown';\n\nfunction parseMarkdownIntoBlocks(markdown: string): string[] {\n  const tokens = marked.lexer(markdown);\n  return tokens.map(token => token.raw);\n}\n\nconst MemoizedMarkdownBlock = memo(\n  ({ content }: { content: string }) => {\n    return <ReactMarkdown>{content}</ReactMarkdown>;\n  },\n  (prevProps, nextProps) => {\n    if (prevProps.content !== nextProps.content) return false;\n    return true;\n  },\n);\n\nMemoizedMarkdownBlock.displayName = 'MemoizedMarkdownBlock';\n\nexport const MemoizedMarkdown = memo(\n  ({ content, id }: { content: string; id: string }) => {\n    const blocks = useMemo(() => parseMarkdownIntoBlocks(content), [content]);\n\n    return blocks.map((block, index) => (\n      <MemoizedMarkdownBlock content={block} key={`${id}-block_${index}`} />\n    ));\n  },\n);\n\nMemoizedMarkdown.displayName = 'MemoizedMarkdown';\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Environment Variables\nDESCRIPTION: Setup of environment variables for OpenAI API authentication in a .env.local file.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#2025-04-23_snippet_0\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=xxxxxxxxx\n```\n\n----------------------------------------\n\nTITLE: Updating Chat UI for Tool Invocations\nDESCRIPTION: UI component implementation that handles the display of both text messages and tool invocations in the chat interface.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n              case 'tool-invocation':\n                return (\n                  <pre key={`${message.id}-${i}`}>\n                    {JSON.stringify(part.toolInvocation, null, 2)}\n                  </pre>\n                );\n            }\n          })}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Defining ToolResultPart Interface in TypeScript\nDESCRIPTION: Defines the structure for the result of a tool call in a tool message. It includes a type of 'tool-result', a tool call ID, tool name, result, optional experimental content, and an optional error flag.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/30-core-message.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nexport interface ToolResultPart {\n  type: 'tool-result';\n\n  /**\n   * ID of the tool call that this result is associated with.\n   */\n  toolCallId: string;\n\n  /**\n   * Name of the tool that generated this result.\n   */\n  toolName: string;\n\n  /**\n   * Result of the tool call. This is a JSON-serializable object.\n   */\n  result: unknown;\n\n  /**\n   * Multi-part content of the tool result. Only for tools that support multipart results.\n   */\n  experimental_content?: ToolResultContent;\n\n  /**\n   * Optional flag if the result is an error or an error message.\n   */\n  isError?: boolean;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Tools with AI SDK in TypeScript\nDESCRIPTION: This snippet demonstrates how to define tools with parameters and execute functions using Zod for schema validation. It shows creating weather and city attractions tools and using them with a language model to process a prompt.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/50-call-tools.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText, tool } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nconst result = await generateText({\n  model: openai('gpt-4-turbo'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n    cityAttractions: tool({\n      parameters: z.object({ city: z.string() }),\n    }),\n  },\n  prompt:\n    'What is the weather in San Francisco and what attractions should I visit?',\n});\n```\n\n----------------------------------------\n\nTITLE: Converting Screenshot Results to LLM Multi-modal Content using AI SDK in TypeScript\nDESCRIPTION: This snippet demonstrates how to convert a screenshot file into a content part suitable for Large Language Model (LLM) consumption by using an optional experimental function in the AI SDK. The 'experimental_toToolResultContent' maps output either as text or as a base64-encoded image with an associated MIME type. Dependencies include Node.js file system ('fs'), the AI SDK, and proper image file access. Key parameters are tool 'action', result types, and model specification. Input is a command triggering a 'screenshot' action, and output is a formatted result expected by the consuming LLM.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: anthropic('claude-3-5-sonnet-20241022'),\n  tools: {\n    computer: anthropic.tools.computer_20241022({\n      // ...\n      async execute({ action, coordinate, text }) {\n        switch (action) {\n          case 'screenshot': {\n            return {\n              type: 'image',\n              data: fs\n                .readFileSync('./data/screenshot-editor.png')\n                .toString('base64'),\n            };\n          }\n          default: {\n            return `executed ${action}`;\n          }\n        }\n      },\n\n      // map to tool result content for LLM consumption:\n      experimental_toToolResultContent(result) {\n        return typeof result === 'string'\n          ? [{ type: 'text', text: result }]\n          : [{ type: 'image', data: result.data, mimeType: 'image/png' }];\n      },\n    }),\n  },\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Enabling Telemetry in AI SDK Function Calls for Braintrust\nDESCRIPTION: Demonstrates how to use the experimental_telemetry option to enable telemetry on supported AI SDK function calls, sending data to Braintrust.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/braintrust.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst openai = createOpenAI();\n\nasync function main() {\n  const result = await generateText({\n    model: openai('gpt-4o-mini'),\n    prompt: 'What is 2 + 2?',\n    experimental_telemetry: {\n      isEnabled: true,\n      metadata: {\n        query: 'weather',\n        location: 'San Francisco',\n      },\n    },\n  });\n  console.log(result);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Cloning the Natural Language Postgres Starter Repository\nDESCRIPTION: Commands for cloning the starter repository for the natural language Postgres project and checking out the starter branch. This provides the foundation for building the AI-powered database application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/vercel-labs/natural-language-postgres\ncd natural-language-postgres\ngit checkout starter\n```\n\n----------------------------------------\n\nTITLE: Installing FriendliAI Provider with pnpm\nDESCRIPTION: Installs the necessary FriendliAI provider package using the pnpm package manager. This package integrates FriendliAI models with the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add @friendliai/ai-provider\n```\n\n----------------------------------------\n\nTITLE: Adding Temperature Conversion Tool to API Route in TypeScript\nDESCRIPTION: This snippet updates the API route to add a new tool for converting temperature from Fahrenheit to Celsius. It demonstrates how to create multiple tools for more complex interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n      convertFahrenheitToCelsius: tool({\n        description: 'Convert a temperature in fahrenheit to celsius',\n        parameters: z.object({\n          temperature: z\n            .number()\n            .describe('The temperature in fahrenheit to convert'),\n        }),\n        execute: async ({ temperature }) => {\n          const celsius = Math.round((temperature - 32) * (5 / 9));\n          return {\n            celsius,\n          };\n        },\n      }),\n    },\n  });\n\n  return result.toDataStreamResponse({\n    headers: {\n      'Content-Type': 'application/octet-stream',\n      'Content-Encoding': 'none',\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Query Parameters for Provider Requests in TypeScript\nDESCRIPTION: Illustrates adding custom query parameters to all API requests made by the provider instance using the `queryParams` option during `createOpenAICompatible` configuration. This is useful for providers like Azure AI that require specific parameters like `api-version`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/index.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\n\nconst provider = createOpenAICompatible({\n  name: 'provider-name',\n  apiKey: process.env.PROVIDER_API_KEY,\n  baseURL: 'https://api.provider.com/v1',\n  queryParams: {\n    'api-version': '1.0.0',\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Data with OpenAI GPT-4.5 and AI SDK\nDESCRIPTION: This code snippet shows how to generate structured JSON data using the AI SDK with OpenAI's GPT-4.5 model. It uses the generateObject function and a Zod schema to ensure the generated data conforms to a specific structure.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/22-gpt-4-5.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: openai('gpt-4.5-preview'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Hume Provider Instance\nDESCRIPTION: Demonstrates how to create a customized Hume provider instance with specific settings like a custom fetch implementation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/150-hume.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createHume } from '@ai-sdk/hume';\n\nconst hume = createHume({\n  // custom settings, e.g.\n  fetch: customFetch,\n});\n```\n\n----------------------------------------\n\nTITLE: Getting Reasoning Summary in Non-Streaming Response\nDESCRIPTION: This example shows how to request a reasoning summary with the generateText function, which provides the model's reasoning in the 'reasoning' field of the response object for non-streaming responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_22\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: openai.responses('o3-mini'),\n  prompt: 'Tell me about the Mission burrito debate in San Francisco.',\n  providerOptions: {\n    openai: {\n      reasoningSummary: 'detailed',\n    },\n  },\n});\nconsole.log('Reasoning:', result.reasoning);\n```\n\n----------------------------------------\n\nTITLE: Rendering Weather Component in React TypeScript\nDESCRIPTION: This snippet shows how to use the JSON object returned by the getWeather function to render a React component. It demonstrates conditional rendering based on the message type and content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/07-rendering-ui-with-language-models.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nreturn (\n  <div>\n    {messages.map(message => {\n      if (message.role === 'function') {\n        const { name, content } = message\n        const { temperature, unit, description, forecast } = content;\n\n        return (\n          <WeatherCard\n            weather={{\n              temperature: 47,\n              unit: 'F',\n              description: 'sunny'\n              forecast,\n            }}\n          />\n        )\n      }\n    })}\n  </div>\n)\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_UnsupportedFunctionalityError in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to check if a given error object is an instance of `AI_UnsupportedFunctionalityError`. It imports `UnsupportedFunctionalityError` from the 'ai' package and uses the static `isInstance` method for type checking, allowing for specific error handling logic.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-unsupported-functionality-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { UnsupportedFunctionalityError } from 'ai';\n\nif (UnsupportedFunctionalityError.isInstance(error)) {\n  // Handle the error\n}\n```\n\n----------------------------------------\n\nTITLE: Using Error Handler with createDataStreamResponse\nDESCRIPTION: Example showing how to implement error handling with the createDataStreamResponse method by using the onError parameter. This approach is used as an alternative to the streamText method when creating custom data streams.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/12-use-chat-an-error-occurred.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nconst response = createDataStreamResponse({\n  // ...\n  async execute(dataStream) {\n    // ...\n  },\n  onError: errorHandler,\n});\n```\n\n----------------------------------------\n\nTITLE: Instantiating LangDB Provider (TypeScript)\nDESCRIPTION: Creates an instance of the LangDB provider using the `createLangDB` function from `@langdb/vercel-provider`. Requires a LangDB API key (`apiKey`) and project ID (`projectId`). Optional parameters like `threadId`, `runId`, `label`, and custom `headers` can also be provided for enhanced tracking and routing.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/94-langdb.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { createLangDB } from '@langdb/vercel-provider';\n\nconst langdb = createLangDB({\n  apiKey: process.env.LANGDB_API_KEY, // Required\n  projectId: 'your-project-id', // Required\n  threadId: uuidv4(), // Optional\n  runId: uuidv4(), // Optional\n  label: 'code-agent', // Optional\n  headers: { 'Custom-Header': 'value' }, // Optional\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Assistant Tool Call in TypeScript\nDESCRIPTION: This snippet illustrates how to generate text using an AI model with an assistant message containing a tool call for retrieving nutrition data.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  messages: [\n    { role: 'user', content: 'How many calories are in this block of cheese?' },\n    {\n      role: 'assistant',\n      content: [\n        {\n          type: 'tool-call',\n          toolCallId: '12345',\n          toolName: 'get-nutrition-data',\n          args: { cheese: 'Roquefort' },\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Maximum Retries Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `maxRetries` parameter as a number. It determines the maximum number of retry attempts for a call. Set to 0 to disable retries. Defaults to 2.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nnumber\n```\n\n----------------------------------------\n\nTITLE: Enabling Model ID Auto-Completion with TypeScript Generics\nDESCRIPTION: Shows how to use TypeScript generics with `createOpenAICompatible` to define expected chat, completion, and embedding model IDs. This enhances developer experience by providing auto-completion for model IDs when calling methods like `model.chatModel`, while still allowing arbitrary string IDs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/index.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { generateText } from 'ai';\n\ntype ExampleChatModelIds =\n  | 'meta-llama/Llama-3-70b-chat-hf'\n  | 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'\n  | (string & {});\n\ntype ExampleCompletionModelIds =\n  | 'codellama/CodeLlama-34b-Instruct-hf'\n  | 'Qwen/Qwen2.5-Coder-32B-Instruct'\n  | (string & {});\n\ntype ExampleEmbeddingModelIds =\n  | 'BAAI/bge-large-en-v1.5'\n  | 'bert-base-uncased'\n  | (string & {});\n\nconst model = createOpenAICompatible<\n  ExampleChatModelIds,\n  ExampleCompletionModelIds,\n  ExampleEmbeddingModelIds\n>({\n  name: 'example',\n  apiKey: process.env.PROVIDER_API_KEY,\n  baseURL: 'https://api.example.com/v1',\n});\n\n// Subsequent calls to e.g. `model.chatModel` will auto-complete the model id\n// from the list of `ExampleChatModelIds` while still allowing free-form\n// strings as well.\n\nconst { text } = await generateText({\n  model: model.chatModel('meta-llama/Llama-3-70b-chat-hf'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Server Action for SQL Query Generation in TypeScript\nDESCRIPTION: Creates an asynchronous server action that generates a SQL query based on a natural language input using the Vercel AI SDK and OpenAI model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nexport const generateQuery = async (input: string) => {\n  'use server';\n  try {\n    const result = await generateObject({\n      model: openai('gpt-4o'),\n      system: `You are a SQL (postgres) ...`, // SYSTEM PROMPT AS ABOVE - OMITTED FOR BREVITY\n      prompt: `Generate the query necessary to retrieve the data the user wants: ${input}`,\n      schema: z.object({\n        query: z.string(),\n      }),\n    });\n    return result.object.query;\n  } catch (e) {\n    console.error(e);\n    throw new Error('Failed to generate query');\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Chat Store Implementation with File System\nDESCRIPTION: Implements chat storage functionality using the file system, including methods for creating and retrieving chat files.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-message-persistence.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateId } from 'ai';\nimport { existsSync, mkdirSync } from 'fs';\nimport { writeFile } from 'fs/promises';\nimport path from 'path';\n\nexport async function createChat(): Promise<string> {\n  const id = generateId(); // generate a unique chat ID\n  await writeFile(getChatFile(id), '[]'); // create an empty chat file\n  return id;\n}\n\nfunction getChatFile(id: string): string {\n  const chatDir = path.join(process.cwd(), '.chats');\n  if (!existsSync(chatDir)) mkdirSync(chatDir, { recursive: true });\n  return path.join(chatDir, `${id}.json`);\n}\n```\n\n----------------------------------------\n\nTITLE: Generating an Image with Luma AI and AI SDK (TypeScript)\nDESCRIPTION: Illustrates basic image creation using Luma's image model within the AI SDK. The snippet imports both the Luma provider and 'generateImage' from 'ai', then writes the resulting image buffer to disk. Key parameters include the image model identifier (e.g., 'photon-1'), a natural language prompt, and the desired aspect ratio. Input: prompt and model. Output: uint8 image array saved as PNG. Requires Node.js built-in 'fs' module.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/80-luma.mdx#2025-04-23_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { luma } from '@ai-sdk/luma';\nimport { experimental_generateImage as generateImage } from 'ai';\nimport fs from 'fs';\n\nconst { image } = await generateImage({\n  model: luma.image('photon-1'),\n  prompt: 'A serene mountain landscape at sunset',\n  aspectRatio: '16:9',\n});\n\nconst filename = `image-${Date.now()}.png`;\nfs.writeFileSync(filename, image.uint8Array);\nconsole.log(`Image saved to ${filename}`);\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Generation with streamText in a Cloudflare Worker - TypeScript\nDESCRIPTION: This TypeScript code provides an endpoint for chunked, streaming text generation using streamText with a Cloudflare Workers AI model. It sets specific headers to ensure proper streaming behavior. Requires 'workers-ai-provider' and 'ai' packages and a suitable Env type. Inputs are HTTP requests and the Cloudflare environment; output is a streamed HTTP response with chunked/streamed text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/11-cloudflare-workers-ai.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createWorkersAI } from 'workers-ai-provider';\nimport { streamText } from 'ai';\n\ntype Env = {\n  AI: Ai;\n};\n\nexport default {\n  async fetch(_: Request, env: Env) {\n    const workersai = createWorkersAI({ binding: env.AI });\n    const result = streamText({\n      model: workersai('@cf/meta/llama-2-7b-chat-int8'),\n      prompt: 'Write a 50-word essay about hello world.',\n    });\n\n    return result.toTextStreamResponse({\n      headers: {\n        // add these headers to ensure that the\n        // response is chunked and streamed\n        'Content-Type': 'text/x-unknown',\n        'content-encoding': 'identity',\n        'transfer-encoding': 'chunked',\n      },\n    });\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Enabling Multi-Step Tool Calls in React Native TSX\nDESCRIPTION: This snippet updates the client-side code to enable multi-step tool calls using the 'maxSteps' option in the useChat hook. This allows the model to use tool results for additional generations.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\nimport { useChat } from '@ai-sdk/react';\n// ... rest of your imports\n\nexport default function App() {\n  const { messages, error, handleInputChange, input, handleSubmit } = useChat({\n    fetch: expoFetch as unknown as typeof globalThis.fetch,\n    api: generateAPIUrl('/api/chat'),\n    onError: error => console.error(error, 'ERROR'),\n    maxSteps: 5,\n  });\n\n  // ... rest of your component code\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI Transcription Model in TypeScript\nDESCRIPTION: This snippet shows initialization of an OpenAI transcription model using the .transcription() factory method with the model id 'whisper-1'. The resulting model instance enables audio transcription with further provider-specific options for enhanced accuracy and feature control. Requires the AI SDK and relevant OpenAI credentials.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_30\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = openai.transcription('whisper-1');\n```\n\n----------------------------------------\n\nTITLE: Updating UI to Display Tool Invocations in SvelteKit\nDESCRIPTION: This snippet demonstrates how to update the Svelte component to display tool invocations in the UI. It iterates through message parts and renders text or tool invocation results accordingly.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_9\n\nLANGUAGE: svelte\nCODE:\n```\n<script>\n  import { Chat } from '@ai-sdk/svelte';\n\n  const chat = new Chat();\n</script>\n\n<main>\n  <ul>\n    {#each chat.messages as message, messageIndex (messageIndex)}\n      <li>\n        <div>{message.role}</div>\n        <div>\n          {#each message.parts as part, partIndex (partIndex)}\n            {#if part.type === 'text'}\n              <div>{part.text}</div>\n            {:else if part.type === 'tool-invocation'}\n              <pre>{JSON.stringify(part.toolInvocation, null, 2)}</pre>\n            {/if}\n          {/each}\n        </div>\n      </li>\n    {/each}\n  </ul>\n  <form onsubmit={chat.handleSubmit}>\n    <input bind:value={chat.input} />\n    <button type=\"submit\">Send</button>\n  </form>\n</main>\n```\n\n----------------------------------------\n\nTITLE: Disabling Structured Outputs to Work Around Zod Schema Limitations (TypeScript)\nDESCRIPTION: Demonstrates disabling structured outputs for Gemini models to avoid schema-related errors when using unsupported Zod features such as unions. Leverages 'generateObject' with 'structuredOutputs: false' and a Zod schema. Requires appropriate TypeScript setup and dependencies. The schema includes nested union types, with the prompt instructing generation of a test person. Essential for developers working around the restricted OpenAPI 3.0 subset supported by Gemini.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateObject({\n  model: google('gemini-1.5-pro-latest', {\n    structuredOutputs: false,\n  }),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    contact: z.union([\n      z.object({\n        type: z.literal('email'),\n        value: z.string(),\n      }),\n      z.object({\n        type: z.literal('phone'),\n        value: z.string(),\n      }),\n    ]),\n  }),\n  prompt: 'Generate an example person for testing.',\n});\n\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Cohere Model in TypeScript\nDESCRIPTION: Shows a complete example of using a Cohere language model with the generateText function from the AI SDK. This code generates a vegetarian lasagna recipe using the command-r-plus model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/25-cohere.mdx#2025-04-23_snippet_3\n\nLANGUAGE: ts\nCODE:\n```\nimport { cohere } from '@ai-sdk/cohere';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: cohere('command-r-plus'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with DeepSeek R1 using AI SDK Core\nDESCRIPTION: This snippet demonstrates how to use the AI SDK Core to generate text using the DeepSeek R1 model. It imports the necessary functions and uses the generateText method to create output based on a given prompt.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/25-r1.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepseek } from '@ai-sdk/deepseek';\nimport { generateText } from 'ai';\n\nconst { reasoning, text } = await generateText({\n  model: deepseek('deepseek-reasoner'),\n  prompt: 'Explain quantum entanglement.',\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Object without Schema in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the generateObject function without a schema, using the 'no-schema' output strategy for dynamic user requests.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject } from 'ai';\n\nconst { object } = await generateObject({\n  model: openai('gpt-4-turbo'),\n  output: 'no-schema',\n  prompt: 'Generate a lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Running AI SDK Codemods with npx - Shell\nDESCRIPTION: Demonstrates how to programmatically apply all available codemods for upgrading to AI SDK 4.0 using npx. No coding in the project is required to run this command, but the package @ai-sdk/codemod must be accessible via npm. Accepts additional codemod names and paths as arguments for targeted upgrades.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnpx @ai-sdk/codemod upgrade\n```\n\nLANGUAGE: sh\nCODE:\n```\nnpx @ai-sdk/codemod <codemod-name> <path>\n```\n\n----------------------------------------\n\nTITLE: Creating AI Instance with State Management\nDESCRIPTION: This code creates the AI instance with createAI from the SDK, defining actions and implementing onGetUIState to transform server messages into client-friendly format. It handles the conversion of function responses into appropriate React components.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/61-restore-messages-from-database.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { createAI } from 'ai/rsc';\nimport { ServerMessage, ClientMessage, continueConversation } from './actions';\nimport { Stock } from '@ai-studio/components/stock';\nimport { generateId } from 'ai';\n\nexport const AI = createAI<ServerMessage[], ClientMessage[]>({\n  actions: {\n    continueConversation,\n  },\n  onGetUIState: async () => {\n    'use server';\n\n    // Get the current AI state (stored messages)\n    const history: ServerMessage[] = getAIState();\n\n    // Transform server messages into client messages\n    return history.map(({ role, content }) => ({\n      id: generateId(),\n      role,\n      display:\n        role === 'function' ? <Stock {...JSON.parse(content)} /> : content,\n    }));\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating and Handling Image Outputs from Gemini Models (TypeScript)\nDESCRIPTION: Shows how to enable and process image responses alongside text using Gemini's generative AI. Requires use of a model supporting image output and setting 'responseModalities' to include 'IMAGE'. The code executes an image request via 'generateText' and iterates over files in the result, allowing image handling in downstream code. Useful for multimodal applications that consume both text and image responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: google('gemini-2.0-flash-exp'),\n  providerOptions: {\n    google: { responseModalities: ['TEXT', 'IMAGE'] },\n  },\n  prompt: 'Generate an image of a comic cat',\n});\n\nfor (const file of result.files) {\n  if (file.mimeType.startsWith('image/')) {\n    // show the image\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Stream Protocol\nDESCRIPTION: Shows how to configure useChat to handle plain text streams.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_14\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages } = useChat({\n    streamProtocol: 'text',\n  });\n\n  return <>...</>;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Exa Web Search Tool with AI SDK in TypeScript\nDESCRIPTION: This snippet shows how to create a custom web search tool using the Exa API. It defines a tool that searches the web and returns relevant information, then demonstrates how to use this tool with OpenAI's model to answer questions about current events in San Francisco using a multi-step execution approach.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/56-web-search-agent.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText, tool } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\nimport Exa from 'exa-js';\n\nexport const exa = new Exa(process.env.EXA_API_KEY);\n\nexport const webSearch = tool({\n  description: 'Search the web for up-to-date information',\n  parameters: z.object({\n    query: z.string().min(1).max(100).describe('The search query'),\n  }),\n  execute: async ({ query }) => {\n    const { results } = await exa.searchAndContents(query, {\n      livecrawl: 'always',\n      numResults: 3,\n    });\n    return results.map(result => ({\n      title: result.title,\n      url: result.url,\n      content: result.text.slice(0, 1000), // take just the first 1000 characters\n      publishedDate: result.publishedDate,\n    }));\n  },\n});\n\nconst { text } = await generateText({\n  model: openai('gpt-4o-mini'), // can be any model that supports tools\n  prompt: 'What happened in San Francisco last week?',\n  tools: {\n    webSearch,\n  },\n  maxSteps: 2,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Workers AI Provider Instance - TypeScript\nDESCRIPTION: This TypeScript snippet shows how to create a provider instance using createWorkersAI, passing in the AI binding (typically from env.AI) as configuration. Requires 'workers-ai-provider' installed and an active AI binding. Output is a workersai instance used to access models; input is an options object containing the binding.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/11-cloudflare-workers-ai.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createWorkersAI } from 'workers-ai-provider';\n\nconst workersai = createWorkersAI({ binding: env.AI });\n```\n\n----------------------------------------\n\nTITLE: Creating Next.js App with Yarn for AI Chat Example\nDESCRIPTION: This command uses create-next-app to bootstrap a new Next.js application with the AI chat example using Yarn.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-langchain/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-langchain next-langchain-app\n```\n\n----------------------------------------\n\nTITLE: Implementing the Anthropic Bash Tool (TypeScript)\nDESCRIPTION: This snippet constructs the Bash Tool using the tools API from the vertexAnthropic provider, taking an asynchronous execute callback for bash command execution. The function receives an object with 'command' and optional 'restart' boolean, and must return the output of the executed command. No default execution logic is provided; the implementation depends on integration context. No explicit dependencies, but requires a properly initialized Anthropic provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_29\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst bashTool = vertexAnthropic.tools.bash_20241022({\n  execute: async ({ command, restart }) => {\n    // Implement your bash command execution logic here\n    // Return the result of the command execution\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Replacing Experimental AI Function Exports with Stable APIs - TypeScript\nDESCRIPTION: Migrates import of experimental exports (experimental_generateText, experimental_streamText, etc.) to their stable function equivalents in AI SDK 4.0. Update code throughout the codebase to reference the new function names. Input: none. Output: updated function imports.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_17\n\nLANGUAGE: ts\nCODE:\n```\nimport {\n  experimental_generateText,\n  experimental_streamText,\n  experimental_generateObject,\n  experimental_streamObject,\n} from 'ai';\n```\n\nLANGUAGE: ts\nCODE:\n```\nimport { generateText, streamText, generateObject, streamObject } from 'ai';\n```\n\n----------------------------------------\n\nTITLE: Initializing Bedrock Image Model with Custom Settings in TypeScript\nDESCRIPTION: Demonstrates initializing a Bedrock image model (`amazon.nova-canvas-v1:0`) with additional configuration settings. This example sets the `maxImagesPerCall` option, which limits the number of images generated in a single API request.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_20\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = bedrock.imageModel('amazon.nova-canvas-v1:0', {\n  maxImagesPerCall: 1, // Maximum number of images to generate per API call\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Unstructured JSON in TypeScript\nDESCRIPTION: Example demonstrating how to generate unstructured JSON without a schema definition.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/03-generate-object.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject } from 'ai';\n\nconst { object } = await generateObject({\n  model: openai('gpt-4-turbo'),\n  output: 'no-schema',\n  prompt: 'Generate a lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Saving AI State with onSetAIState in TypeScript\nDESCRIPTION: Example showing how to save AI state to a database when generation is complete using the onSetAIState callback. The callback is triggered whenever AI state updates and saves chat history when done is true.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-saving-and-restoring-states.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nexport const AI = createAI<ServerMessage[], ClientMessage[]>({\n  actions: {\n    continueConversation,\n  },\n  onSetAIState: async ({ state, done }) => {\n    'use server';\n\n    if (done) {\n      saveChatToDB(state);\n    }\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Anthropic Vertex Provider Instance in TypeScript\nDESCRIPTION: Illustrates how to import the `createAnthropicVertex` factory function and use it to create a customized provider instance. This allows specifying configurations like `region`, `projectId`, `googleAuth`, `baseURL`, `headers`, or a custom `fetch` implementation directly in the code instead of relying solely on environment variables.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/91-anthropic-vertex-ai.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAnthropicVertex } from 'anthropic-vertex-ai';\n\nconst anthropicVertex = createAnthropicVertex({\n  region: 'us-central1',\n  projectId: 'your-project-id',\n  // other options\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-Side No-Schema Streaming with OpenAI\nDESCRIPTION: This server-side route demonstrates how to use the streamObject function in no-schema output mode. It generates JSON data based on the prompt without a predefined schema.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/40-stream-object.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { notificationSchema } from './schema';\n\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const context = await req.json();\n\n  const result = streamObject({\n    model: openai('gpt-4-turbo'),\n    output: 'no-schema',\n    prompt:\n      `Generate 3 notifications for a messages app in this context:` + context,\n  });\n\n  return result.toTextStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with OpenAI Transcription Model in TypeScript\nDESCRIPTION: This code demonstrates complete transcription of an audio stream using the experimental_transcribe function from 'ai'. It imports the required transcribe utility and initializes a transcription model for 'whisper-1', then provides a Uint8Array audio input and specifies language via providerOptions. It is necessary to have '@ai-sdk/openai' installed and proper audio input formatting.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_31\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst result = await transcribe({\n  model: openai.transcription('whisper-1'),\n  audio: new Uint8Array([1, 2, 3, 4]),\n  providerOptions: { openai: { language: 'en' } },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Custom Bedrock Provider Options in TypeScript\nDESCRIPTION: Shows how to pass Bedrock-specific options during image generation using the `providerOptions` parameter within the `experimental_generateImage` function. This example sets the image quality to 'premium'.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: bedrock.imageModel('amazon.nova-canvas-v1:0'),\n  prompt: 'A beautiful sunset over a calm ocean',\n  size: '512x512',\n  seed: 42,\n  providerOptions: { bedrock: { quality: 'premium' } },\n});\n```\n\n----------------------------------------\n\nTITLE: Smoothing Text Streams with AI SDK in TypeScript\nDESCRIPTION: Demonstrates how to use the smoothStream function to smooth out text streaming, which can be applied as a transformation to the streamText function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_8\n\nLANGUAGE: tsx\nCODE:\n```\nimport { smoothStream, streamText } from 'ai';\n\nconst result = streamText({\n  model,\n  prompt,\n  experimental_transform: smoothStream(),\n});\n```\n\n----------------------------------------\n\nTITLE: One-Shot Text Generation with Computer Tool\nDESCRIPTION: Example of using the generateText function with the Computer Tool to perform a one-time task like taking a screenshot.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/05-computer-use.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: anthropic('claude-3-5-sonnet-20241022'),\n  prompt: 'Move the cursor to the center of the screen and take a screenshot',\n  tools: { computer: computerTool },\n});\n\nconsole.log(response.text);\n```\n\n----------------------------------------\n\nTITLE: Selecting an NVIDIA NIM Chat Model in TypeScript\nDESCRIPTION: Shows how to select a specific NVIDIA NIM chat model, in this case 'deepseek-ai/deepseek-r1', using the `chatModel` method on the previously created provider instance (`nim`). This model object can then be used for text generation or streaming operations with the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/35-nim.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = nim.chatModel('deepseek-ai/deepseek-r1');\n```\n\n----------------------------------------\n\nTITLE: Exporting Public API for Custom AI SDK Provider in TypeScript\nDESCRIPTION: Defines the public interface of the custom provider package (`index.ts`). It exports the `createExample` factory function, the default pre-configured `example` instance, and the `ExampleProvider` and `ExampleProviderSettings` types from the main implementation file (`./example-provider`). This makes the core functionalities and types available for users importing the package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nexport { createExample, example } from './example-provider';\nexport type {\n  ExampleProvider,\n  ExampleProviderSettings,\n} from './example-provider';\n```\n\n----------------------------------------\n\nTITLE: Configuring AI with createAI Function\nDESCRIPTION: This TypeScript file creates an AI instance using the createAI function from the AI SDK. It imports the server actions and message types, then initializes the AI with empty arrays for both server and client state.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/92-stream-ui-record-token-usage.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAI } from 'ai/rsc';\nimport { ServerMessage, ClientMessage, continueConversation } from './actions';\n\nexport const AI = createAI<ServerMessage[], ClientMessage[]>({\n  actions: {\n    continueConversation,\n  },\n  initialAIState: [],\n  initialUIState: [],\n});\n```\n\n----------------------------------------\n\nTITLE: Converting Recursive Zod Schema to JSON Schema in TypeScript\nDESCRIPTION: This example demonstrates how to use the zodSchema() function to convert a recursive Zod schema into a JSON schema. It defines a Category type with subcategories, creates a recursive schema using z.lazy(), and uses zodSchema() with the useReferences option enabled for recursive support.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/26-zod-schema.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { zodSchema } from 'ai';\nimport { z } from 'zod';\n\n// Define a base category schema\nconst baseCategorySchema = z.object({\n  name: z.string(),\n});\n\n// Define the recursive Category type\ntype Category = z.infer<typeof baseCategorySchema> & {\n  subcategories: Category[];\n};\n\n// Create the recursive schema using z.lazy\nconst categorySchema: z.ZodType<Category> = baseCategorySchema.extend({\n  subcategories: z.lazy(() => categorySchema.array()),\n});\n\n// Create the final schema with useReferences enabled for recursive support\nconst mySchema = zodSchema(\n  z.object({\n    category: categorySchema,\n  }),\n  { useReferences: true },\n);\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_InvalidResponseDataError Instance (TypeScript)\nDESCRIPTION: This TypeScript snippet demonstrates how to check if a caught error variable (`error`) is an instance of `AI_InvalidResponseDataError`. It imports `InvalidResponseDataError` from the 'ai' package (Vercel AI SDK) and uses the static method `isInstance(error)` for type checking, allowing for specific handling of this error type within an error handling block.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-invalid-response-data-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nimport { InvalidResponseDataError } from 'ai';\n\nif (InvalidResponseDataError.isInstance(error)) {\n  // Handle the error\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Accessing Raw Response Headers and Body in TypeScript\nDESCRIPTION: Demonstrates how to access the raw response headers and body from the model provider using the response property of the generateText result.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_2\n\nLANGUAGE: ts\nCODE:\n```\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  // ...\n});\n\nconsole.log(JSON.stringify(result.response.headers, null, 2));\nconsole.log(JSON.stringify(result.response.body, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Enabling Detailed Reasoning Summaries in Response Stream\nDESCRIPTION: This code streams text responses using the openai.responses() model, enabling detailed reasoning summaries. It processes parts of the stream, differentiating between reasoning and text delta parts for real-time insight into the model's thought process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_21\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst result = streamText({\n  model: openai.responses('o3-mini'),\n  prompt: 'Tell me about the Mission burrito debate in San Francisco.',\n  providerOptions: {\n    openai: {\n      reasoningSummary: 'detailed', // 'auto' for condensed or 'detailed' for comprehensive\n    },\n  },\n});\n\nfor await (const part of result.fullStream) {\n  if (part.type === 'reasoning') {\n    console.log(`Reasoning: ${part.textDelta}`);\n  } else if (part.type === 'text-delta') {\n    process.stdout.write(part.textDelta);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Interface Component with OpenAI Assistant API in React\nDESCRIPTION: A client-side React component that creates a chat interface for interacting with the OpenAI Assistant API. It manages message state, handles user input, and submits messages to the server action.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/120-stream-assistant-response.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { ClientMessage } from './actions';\nimport { useActions } from 'ai/rsc';\n\nexport default function Home() {\n  const [input, setInput] = useState('');\n  const [messages, setMessages] = useState<ClientMessage[]>([]);\n  const { submitMessage } = useActions();\n\n  const handleSubmission = async () => {\n    setMessages(currentMessages => [\n      ...currentMessages,\n      {\n        id: '123',\n        status: 'user.message.created',\n        text: input,\n        gui: null,\n      },\n    ]);\n\n    const response = await submitMessage(input);\n    setMessages(currentMessages => [...currentMessages, response]);\n    setInput('');\n  };\n\n  return (\n    <div className=\"flex flex-col-reverse\">\n      <div className=\"flex flex-row gap-2 p-2 bg-zinc-100 w-full\">\n        <input\n          className=\"bg-zinc-100 w-full p-2 outline-none\"\n          value={input}\n          onChange={event => setInput(event.target.value)}\n          placeholder=\"Ask a question\"\n          onKeyDown={event => {\n            if (event.key === 'Enter') {\n              handleSubmission();\n            }\n          }}\n        />\n        <button\n          className=\"p-2 bg-zinc-900 text-zinc-100 rounded-md\"\n          onClick={handleSubmission}\n        >\n          Send\n        </button>\n      </div>\n\n      <div className=\"flex flex-col h-[calc(100dvh-56px)] overflow-y-scroll\">\n        <div>\n          {messages.map(message => (\n            <div key={message.id} className=\"flex flex-col gap-1 border-b p-2\">\n              <div className=\"flex flex-row justify-between\">\n                <div className=\"text-sm text-zinc-500\">{message.status}</div>\n              </div>\n              <div>{message.text}</div>\n            </div>\n          ))}\n        </div>\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AI Binding in wrangler.toml - bash\nDESCRIPTION: Provides the TOML configuration lines for establishing an AI binding in wrangler.toml, which is mandatory for leveraging Cloudflare AI bindings in Worker environments. The AI binding associates a named AI gateway with your Worker, enabling secure, runtime-level integration without exposing API keys in application code.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/12-cloudflare-ai-gateway.mdx#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n[AI]\nbinding = \"AI\"\n```\n\n----------------------------------------\n\nTITLE: Adding and Retrieving User Memories - TypeScript\nDESCRIPTION: Illustrates usage of 'addMemories', 'retrieveMemories', and 'getMemories' to manage persistent user-specific memory for AI interactions. Dependencies include '@mem0/vercel-ai-provider'. These functions require options such as user_id, mem0ApiKey, org_id, and project_id. Input parameters define the context for the memory actions. 'getMemories' returns a raw array of memory objects, while 'retrieveMemories' returns a system prompt string, and 'addMemories' acknowledges successful addition. Ensure API keys are provided securely.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/70-mem0.mdx#2025-04-23_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nawait addMemories(messages, {\n  user_id: 'borat',\n  mem0ApiKey: 'm0-xxx',\n  org_id: 'org_xx',\n  project_id: 'proj_xx',\n});\nawait retrieveMemories(prompt, {\n  user_id: 'borat',\n  mem0ApiKey: 'm0-xxx',\n  org_id: 'org_xx',\n  project_id: 'proj_xx',\n});\nawait getMemories(prompt, {\n  user_id: 'borat',\n  mem0ApiKey: 'm0-xxx',\n  org_id: 'org_xx',\n  project_id: 'proj_xx',\n});\n```\n\n----------------------------------------\n\nTITLE: Checking for InvalidArgumentError using isInstance in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to check if a caught error is specifically an AI_InvalidArgumentError by using the static `isInstance` method provided by the `InvalidArgumentError` class from the 'ai' library. It involves importing the necessary class and using the method within a conditional statement for type-safe error handling.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-invalid-argument-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nimport { InvalidArgumentError } from 'ai';\n\nif (InvalidArgumentError.isInstance(error)) {\n  // Handle the error\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI Speech Synthesis Model in TypeScript\nDESCRIPTION: This snippet shows how to initialize an OpenAI speech synthesis model using the .speech() factory method with the 'tts-1' model id. The created model instance supports conversion of text to speech, with additional fine-tuning possible through provider options. Usage requires OpenAI API access and the necessary SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_32\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = openai.speech('tts-1');\n```\n\n----------------------------------------\n\nTITLE: Creating Amazon Bedrock Provider with Credential Chain\nDESCRIPTION: Example of creating a custom Amazon Bedrock provider instance using the AWS credential provider chain for authentication.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAmazonBedrock } from '@ai-sdk/amazon-bedrock';\nimport { fromNodeProviderChain } from '@aws-sdk/credential-providers';\n\nconst bedrock = createAmazonBedrock({\n  region: 'us-east-1',\n  credentialProvider: fromNodeProviderChain(),\n});\n```\n\n----------------------------------------\n\nTITLE: Creating SvelteKit API Endpoint for Chat Functionality\nDESCRIPTION: TypeScript code for a SvelteKit API route that handles chat requests by streaming responses from OpenAI's GPT model using the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nimport { OPENAI_API_KEY } from '$env/static/private';\n\nconst openai = createOpenAI({\n  apiKey: OPENAI_API_KEY,\n});\n\nexport async function POST({ request }) {\n  const { messages } = await request.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Dependency Installation Command\nDESCRIPTION: Command to install project dependencies using pnpm package manager.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_4\n\nLANGUAGE: txt\nCODE:\n```\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Updating AI State in Server Action with getMutableAIState\nDESCRIPTION: This code shows how to update the AI state within a server action using the getMutableAIState function from ai/rsc, including updating the state with new messages and responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { getMutableAIState } from 'ai/rsc';\n\nexport async function sendMessage(message: string) {\n  'use server';\n\n  const history = getMutableAIState();\n\n  history.update([...history.get(), { role: 'user', content: message }]);\n\n  const response = await generateText({\n    model: openai('gpt-3.5-turbo'),\n    messages: history.get(),\n  });\n\n  history.done([...history.get(), { role: 'assistant', content: response }]);\n\n  return response;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom AssemblyAI Provider Instance in TypeScript\nDESCRIPTION: Demonstrates how to create a customized AssemblyAI provider instance with specific settings like a custom fetch implementation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/100-assemblyai.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAssemblyAI } from '@ai-sdk/assemblyai';\n\nconst assemblyai = createAssemblyAI({\n  // custom settings, e.g.\n  fetch: customFetch,\n});\n```\n\n----------------------------------------\n\nTITLE: Error Handling with streamText\nDESCRIPTION: Shows how to handle errors in streaming responses using toDataStreamResponse and custom error messages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = streamText({\n  // ...\n});\n\nreturn result.toDataStreamResponse({\n  getErrorMessage: error => {\n    if (NoSuchToolError.isInstance(error)) {\n      return 'The model tried to call a unknown tool.';\n    } else if (InvalidToolArgumentsError.isInstance(error)) {\n      return 'The model called a tool with invalid arguments.';\n    } else if (ToolExecutionError.isInstance(error)) {\n      return 'An error occurred during tool execution.';\n    } else {\n      return 'An unknown error occurred.';\n    }\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with NVIDIA NIM and AI SDK in TypeScript\nDESCRIPTION: Provides a full example of generating text using an NVIDIA NIM model via the AI SDK. It initializes the NIM provider, selects the 'deepseek-ai/deepseek-r1' model, calls the `generateText` function with a specific prompt, and logs the generated text, token usage, and finish reason. Requires `@ai-sdk/openai-compatible` and `ai` packages, and the `NIM_API_KEY` environment variable must be set.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/35-nim.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { generateText } from 'ai';\n\nconst nim = createOpenAICompatible({\n  name: 'nim',\n  baseURL: 'https://integrate.api.nvidia.com/v1',\n  headers: {\n    Authorization: `Bearer ${process.env.NIM_API_KEY}`,\n  },\n});\n\nconst { text, usage, finishReason } = await generateText({\n  model: nim.chatModel('deepseek-ai/deepseek-r1'),\n  prompt: 'Tell me the history of the San Francisco Mission-style burrito.',\n});\n\nconsole.log(text);\nconsole.log('Token usage:', usage);\nconsole.log('Finish reason:', finishReason);\n```\n\n----------------------------------------\n\nTITLE: Installing AI SDK and Anthropic Provider\nDESCRIPTION: Command to install the AI SDK and Anthropic provider packages using pnpm.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/05-computer-use.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add ai @ai-sdk/anthropic\n```\n\n----------------------------------------\n\nTITLE: Checking AI Download Error Instance\nDESCRIPTION: Shows how to check if an error is an instance of AI_DownloadError using the isInstance static method.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-download-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DownloadError } from 'ai';\n\nif (DownloadError.isInstance(error)) {\n  // Handle the error\n}\n```\n\n----------------------------------------\n\nTITLE: Calling Server Action and Updating UI State (RSC)\nDESCRIPTION: This snippet demonstrates how to call a server action and update the UI state using the useUIState hook in AI SDK RSC.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState, ReactNode } from 'react';\nimport { useActions, useUIState } from 'ai/rsc';\n\nexport default function Page() {\n  const { sendMessage } = useActions();\n  const [input, setInput] = useState('');\n  const [messages, setMessages] = useUIState();\n\n  return (\n    <div>\n      {messages.map(message => message)}\n\n      <form\n        onSubmit={async () => {\n          const response: ReactNode = await sendMessage(input);\n          setMessages(msgs => [...msgs, response]);\n        }}\n      >\n        <input type=\"text\" />\n        <button type=\"submit\">Submit</button>\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Spark Provider Constructor - TypeScript\nDESCRIPTION: Demonstrates how to import the 'createSparkProvider' function from the 'spark-ai-provider' package. This import is required before constructing a provider instance which enables interaction with the Spark AI models. Ensure the package is installed for this import to resolve successfully.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/92-spark.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createSparkProvider } from 'spark-ai-provider';\n```\n\n----------------------------------------\n\nTITLE: Enhancing Chatbot UI with Status Indicators and Stop Button\nDESCRIPTION: This example extends the basic chatbot UI to include status indicators, a stop button for aborting responses, and disabling the input when not ready.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, handleInputChange, handleSubmit, status, stop } =\n    useChat({});\n\n  return (\n    <>\n      {messages.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.content}\n        </div>\n      ))}\n\n      {(status === 'submitted' || status === 'streaming') && (\n        <div>\n          {status === 'submitted' && <Spinner />}\n          <button type=\"button\" onClick={() => stop()}>\n            Stop\n          </button>\n        </div>\n      )}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          name=\"prompt\"\n          value={input}\n          onChange={handleInputChange}\n          disabled={status !== 'ready'}\n        />\n        <button type=\"submit\">Submit</button>\n      </form>\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Mistral AI Model\nDESCRIPTION: Complete example showing how to use a Mistral AI model with the generateText function to create a vegetarian lasagna recipe.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/20-mistral.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { mistral } from '@ai-sdk/mistral';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: mistral('mistral-large-latest'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with OpenRouter Chat Model - JavaScript\nDESCRIPTION: Illustrates synchronous text generation using the generateText function alongside an OpenRouter chat model. Requires proper imports, a configured provider instance, and a prompt string. Returns a text output in response to the given prompt. Dependencies: '@openrouter/ai-sdk-provider', 'ai' package. Inputs: model handler (openrouter.chat), prompt string. Output: generated text string (extracted from the returned object).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/13-openrouter.mdx#2025-04-23_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { createOpenRouter } from '@openrouter/ai-sdk-provider';\nimport { generateText } from 'ai';\n\nconst openrouter = createOpenRouter({\n  apiKey: 'YOUR_OPENROUTER_API_KEY',\n});\n\nconst { text } = await generateText({\n  model: openrouter.chat('anthropic/claude-3.5-sonnet'),\n  prompt: 'What is OpenRouter?',\n});\n\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Generating Long Text with Continuations in TypeScript\nDESCRIPTION: This code snippet shows how to generate long text beyond the model's output limit using the experimental_continueSteps setting. It uses the OpenAI GPT-4 model and sets a maximum of 5 steps for continuation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_14\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst {\n  text, // combined text\n  usage, // combined usage of all steps\n} = await generateText({\n  model: openai('gpt-4o'), // 4096 output tokens\n  maxSteps: 5, // enable multi-step calls\n  experimental_continueSteps: true,\n  prompt:\n    'Write a book about Roman history, ' +\n    'from the founding of the city of Rome ' +\n    'to the fall of the Western Roman Empire. ' +\n    'Each chapter MUST HAVE at least 1000 words.',\n});\n```\n\n----------------------------------------\n\nTITLE: Running a Specific AI SDK Codemod using npx\nDESCRIPTION: This shell command uses npx to execute a specific codemod from the `@ai-sdk/codemod` package. The user must replace `<codemod-name>` with the actual name of the codemod and `<path>` with the target directory or file path for the transformation. This allows for applying targeted updates. Requires Node.js and npm/npx.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/codemod/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnpx @ai-sdk/codemod <codemod-name> <path>\n```\n\n----------------------------------------\n\nTITLE: Creating Server Action for SQL Query Explanation in TypeScript\nDESCRIPTION: Implements a server action that generates explanations for SQL queries using the Vercel AI SDK and OpenAI model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nexport const explainQuery = async (input: string, sqlQuery: string) => {\n  'use server';\n  try {\n    const result = await generateObject({\n      model: openai('gpt-4o'),\n      system: `You are a SQL (postgres) expert. ...`, // SYSTEM PROMPT AS ABOVE - OMITTED FOR BREVITY\n      prompt: `Explain the SQL query you generated to retrieve the data the user wanted. Assume the user is not an expert in SQL. Break down the query into steps. Be concise.\n\n      User Query:\n      ${input}\n\n      Generated SQL Query:\n      ${sqlQuery}`,\n      schema: explanationSchema,\n      output: 'array',\n    });\n    return result.object;\n  } catch (e) {\n    console.error(e);\n    throw new Error('Failed to generate query');\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Installing AI SDK Chat Example with Next.js and OpenAI using pnpm\nDESCRIPTION: This command uses pnpm to create a new Next.js application with the AI SDK, OpenAI, and Sentry integration example.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry-sentry/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry-sentry next-openai-telemetry-sentry-app\n```\n\n----------------------------------------\n\nTITLE: Linking Langfuse Prompts to AI SDK Traces (TypeScript)\nDESCRIPTION: This code demonstrates how to link Langfuse prompts to AI SDK generations. It fetches a prompt from Langfuse and includes it in the metadata of the text generation request, allowing for prompt tracking in traces.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langfuse.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { Langfuse } from 'langfuse';\n\nconst langfuse = new Langfuse();\n\nconst fetchedPrompt = await langfuse.getPrompt('my-prompt');\n\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  prompt: fetchedPrompt.prompt,\n  experimental_telemetry: {\n    isEnabled: true,\n    metadata: {\n      langfusePrompt: fetchedPrompt.toJSON(),\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Importing AWSBedrockStream in React\nDESCRIPTION: Shows how to import the AWSBedrockStream function from the 'ai' package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/09-aws-bedrock-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { AWSBedrockStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Defining File Structure for Custom AI SDK Provider in Bash\nDESCRIPTION: Illustrates the recommended directory and file organization for creating a custom AI SDK provider package. This structure separates concerns like model settings, provider implementation, tests, and public exports.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npackages/example/\nâââ src/\nâ   âââ example-chat-settings.ts       # Chat model types and settings\nâ   âââ example-completion-settings.ts # Completion model types and settings\nâ   âââ example-embedding-settings.ts  # Embedding model types and settings\nâ   âââ example-image-settings.ts      # Image model types and settings\nâ   âââ example-provider.ts            # Main provider implementation\nâ   âââ example-provider.test.ts       # Provider tests\nâ   âââ index.ts                       # Public exports\nâââ package.json\nâââ tsconfig.json\nâââ tsup.config.ts                     # Build configuration\nâââ README.md\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install all the required dependencies for the natural language Postgres project using pnpm package manager.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Client-Side Handling of Streamed UI Components with AI SDK RSC\nDESCRIPTION: This client-side code snippet shows how to handle streamed UI components from the server using AI SDK RSC. It updates the UI with the streamed components, providing a seamless loading experience for the user.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/06-loading-state.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { generateResponse } from './actions';\nimport { readStreamableValue } from 'ai/rsc';\n\n// Force the page to be dynamic and allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [input, setInput] = useState<string>('');\n  const [generation, setGeneration] = useState<React.ReactNode>();\n\n  return (\n    <div>\n      <div>{generation}</div>\n      <form\n        onSubmit={async e => {\n          e.preventDefault();\n          const result = await generateResponse(input);\n          setGeneration(result);\n          setInput('');\n        }}\n      >\n        <input\n          type=\"text\"\n          value={input}\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button>Send Message</button>\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Transcription with ElevenLabs and Custom Options in TypeScript\nDESCRIPTION: Shows how to use the `experimental_transcribe` function from the `ai` package with an ElevenLabs transcription model. It demonstrates passing audio data (as a Uint8Array) and including provider-specific options, such as setting the `languageCode` to 'en' for potentially improved performance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/90-elevenlabs.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { elevenlabs } from '@ai-sdk/elevenlabs';\n\nconst result = await transcribe({\n  model: elevenlabs.transcription('scribe_v1'),\n  audio: new Uint8Array([1, 2, 3, 4]),\n  providerOptions: { elevenlabs: { languageCode: 'en' } },\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing Messages in Tool Execution with AI SDK Core\nDESCRIPTION: This example shows how to access the messages that were sent to the language model in the execute function of a tool. It demonstrates that these messages can be used in calls to other language models or for additional context.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText, tool } from 'ai';\n\nconst result = await generateText({\n  // ...\n  tools: {\n    myTool: tool({\n      // ...\n      execute: async (args, { messages }) => {\n        // use the message history in e.g. calls to other language models\n        return something;\n      },\n    }),\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Metadata to AI SDK Telemetry (TypeScript)\nDESCRIPTION: This code snippet shows how to add custom metadata and a function ID to the telemetry data for an AI SDK function call. It enables telemetry and includes additional information for better tracing and identification.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/60-telemetry.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: openai('gpt-4-turbo'),\n  prompt: 'Write a short story about a cat.',\n  experimental_telemetry: {\n    isEnabled: true,\n    functionId: 'my-awesome-function',\n    metadata: {\n      something: 'custom',\n      someOtherThing: 'other-value',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for AI SDK in Nest.js\nDESCRIPTION: This snippet shows the content of the .env file required for setting up the AI SDK with Nest.js. It includes the OpenAI API key and mentions that additional settings may be needed depending on the providers used.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/nest/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Google Vertex Anthropic in Node.js using AI SDK\nDESCRIPTION: Example of using the `vertexAnthropic` provider (`@ai-sdk/google-vertex/anthropic`) to generate text with an Anthropic Claude model (e.g., `claude-3-5-sonnet@20240620`) in a Node.js environment. It imports `vertexAnthropic` from the `/anthropic` submodule and uses `generateText`. Relies on standard Google Cloud authentication methods.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google-vertex/README.md#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: vertexAnthropic('claude-3-5-sonnet@20240620'),\n  prompt: 'Write a vegetarian lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Using Error Handler with streamText and toDataStreamResponse\nDESCRIPTION: Implementation showing how to attach a custom error handler to the toDataStreamResponse method. This allows detailed error messages to be forwarded to the client instead of the generic \"An error occurred\" message.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/12-use-chat-an-error-occurred.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nconst result = streamText({\n  // ...\n});\n\nreturn result.toDataStreamResponse({\n  getErrorMessage: errorHandler,\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Default Zhipu Provider Instance (TypeScript)\nDESCRIPTION: Imports the default `zhipu` provider instance from the `zhipu-ai-provider` package. This instance automatically configures itself using the `ZHIPU_API_KEY` environment variable.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/95-zhipu.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { zhipu } from 'zhipu-ai-provider';\n```\n\n----------------------------------------\n\nTITLE: Setting Up Next.js App with pnpm for AI SDK and OpenAI Integration\nDESCRIPTION: This command uses pnpm to create a new Next.js application with the AI SDK and OpenAI integration example. It sets up the project environment and installs necessary dependencies.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry/app/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app\n```\n\n----------------------------------------\n\nTITLE: Implementing a Next.js API Route with Cached Language Model\nDESCRIPTION: A server-side API route in Next.js that uses the caching middleware to wrap an OpenAI language model. It provides a chat endpoint that handles messages and supports tool invocation for weather information.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/122-caching-middleware.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { cacheMiddleware } from '@/ai/middleware';\nimport { openai } from '@ai-sdk/openai';\nimport { wrapLanguageModel, streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst wrappedModel = wrapLanguageModel({\n  model: openai('gpt-4o-mini'),\n  middleware: cacheMiddleware,\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: wrappedModel,\n    messages,\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => ({\n          location,\n          temperature: 72 + Math.floor(Math.random() * 21) - 10,\n        }),\n      }),\n    },\n  });\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Using File Inputs with Amazon Bedrock\nDESCRIPTION: Example of using file inputs (PDF) with Amazon Bedrock Claude 3 Haiku model for analysis.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: bedrock('anthropic.claude-3-haiku-20240307-v1:0'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'Describe the pdf in detail.' },\n        {\n          type: 'file',\n          data: fs.readFileSync('./data/ai.pdf'),\n          mimeType: 'application/pdf',\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Perplexity Provider Instance with Configuration (TypeScript)\nDESCRIPTION: Demonstrates how to import and use the createPerplexity factory to build a custom provider instance with specific API keys or other options. Useful for scenarios requiring custom API endpoints, headers, or environment-based configuration. Expects @ai-sdk/perplexity to be installed; apiKey (e.g., from process.env) is needed for authentication.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/70-perplexity.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createPerplexity } from '@ai-sdk/perplexity';\n\nconst perplexity = createPerplexity({\n  apiKey: process.env.PERPLEXITY_API_KEY ?? '',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Step Calls with maxSteps in AI SDK Core\nDESCRIPTION: This example shows how to enable multi-step calls in generateText by setting the maxSteps parameter. It allows the model to make multiple tool calls and generate responses based on tool results, up to a maximum of 5 steps.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from 'zod';\nimport { generateText, tool } from 'ai';\n\nconst { text, steps } = await generateText({\n  model: yourModel,\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n  maxSteps: 5, // allow up to 5 steps\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Generation with OpenRouter Chat Model - JavaScript\nDESCRIPTION: Demonstrates how to use the streamText function with an OpenRouter chat model for asynchronous, chunked output. Suitable for scenarios where generated text should be processed or displayed incrementally. Input dependencies include model handler, prompt string, and for-await-of iteration for streaming results. Outputs generated text chunks as they are received. Dependencies: '@openrouter/ai-sdk-provider', 'ai'.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/13-openrouter.mdx#2025-04-23_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { createOpenRouter } from '@openrouter/ai-sdk-provider';\nimport { streamText } from 'ai';\n\nconst openrouter = createOpenRouter({\n  apiKey: 'YOUR_OPENROUTER_API_KEY',\n});\n\nconst result = streamText({\n  model: openrouter.chat('meta-llama/llama-3.1-405b-instruct'),\n  prompt: 'Write a short story about AI.',\n});\n\nfor await (const chunk of result) {\n  console.log(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Selecting Chat and Completion Models with OpenRouter - TypeScript\nDESCRIPTION: Demonstrates obtaining chat and completion model handlers from an OpenRouter provider instance. The .chat() method configures for conversational (chat) models, while .completion() configures for traditional completion models. Requires model identifiers matching OpenRouter's supported models. Inputs: provider instance, model string. Outputs: configured model handler objects for downstream text generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/13-openrouter.mdx#2025-04-23_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Chat models (recommended)\nconst chatModel = openrouter.chat('anthropic/claude-3.5-sonnet');\n\n// Completion models\nconst completionModel = openrouter.completion(\n  'meta-llama/llama-3.1-405b-instruct',\n);\n```\n\n----------------------------------------\n\nTITLE: Testing Nest.js AI SDK Endpoint with Curl\nDESCRIPTION: This curl command tests the Nest.js server endpoint that utilizes the AI SDK. It sends a POST request to the local server running on port 8080, which can be used to verify the functionality of the AI SDK integration.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/nest/README.md#2025-04-23_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncurl -X POST http://localhost:8080\n```\n\n----------------------------------------\n\nTITLE: Defining setRoomTemperature Function in JSON for OpenAI Assistant\nDESCRIPTION: This JSON structure defines the 'setRoomTemperature' function for the OpenAI assistant. It specifies the function name, description, and required parameters including the room name as an enumerated string and the temperature as a number.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai/app/api/assistant/assistant-setup.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \"name\": \"setRoomTemperature\",\\n  \"description\": \"Set the temperature in a room\",\\n  \"parameters\": {\\n    \"type\": \"object\",\\n    \"properties\": {\\n      \"room\": {\\n        \"type\": \"string\",\\n        \"enum\": [\"bedroom\", \"home office\", \"living room\", \"kitchen\", \"bathroom\"]\\n      },\\n      \"temperature\": { \"type\": \"number\" }\\n    },\\n    \"required\": [\"room\", \"temperature\"]\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Multiple Middleware Layers\nDESCRIPTION: Demonstrates how to apply multiple middleware layers to a language model in sequence.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst wrappedLanguageModel = wrapLanguageModel({\n  model: yourModel,\n  middleware: [firstMiddleware, secondMiddleware],\n});\n\n// applied as: firstMiddleware(secondMiddleware(yourModel))\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAI Chat API Route in Next.js\nDESCRIPTION: Creates an API route handler for chat functionality using OpenAI and the AI SDK. Sets up streaming text responses with a 5-minute timeout limit.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/23-o1.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\n// Allow responses up to 5 minutes\nexport const maxDuration = 300;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('o1-mini'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Utility Functions for Tool Confirmations\nDESCRIPTION: These utility functions provide a higher-level abstraction for handling tool confirmations. The code includes functions to process tool calls requiring human input, determine which tools need confirmation, and manage approval states consistently.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#2025-04-23_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { formatDataStreamPart, Message } from '@ai-sdk/ui-utils';\nimport {\n  convertToCoreMessages,\n  DataStreamWriter,\n  ToolExecutionOptions,\n  ToolSet,\n} from 'ai';\nimport { z } from 'zod';\n\n// Approval string to be shared across frontend and backend\nexport const APPROVAL = {\n  YES: 'Yes, confirmed.',\n  NO: 'No, denied.',\n} as const;\n\nfunction isValidToolName<K extends PropertyKey, T extends object>(\n  key: K,\n  obj: T,\n): key is K & keyof T {\n  return key in obj;\n}\n\n/**\n * Processes tool invocations where human input is required, executing tools when authorized.\n *\n * @param options - The function options\n * @param options.tools - Map of tool names to Tool instances that may expose execute functions\n * @param options.dataStream - Data stream for sending results back to the client\n * @param options.messages - Array of messages to process\n * @param executionFunctions - Map of tool names to execute functions\n * @returns Promise resolving to the processed messages\n */\nexport async function processToolCalls<\n  Tools extends ToolSet,\n  ExecutableTools extends {\n    [Tool in keyof Tools as Tools[Tool] extends { execute: Function }\n      ? never\n      : Tool]: Tools[Tool];\n  },\n>(\n  {\n    dataStream,\n    messages,\n  }: {\n    tools: Tools; // used for type inference\n    dataStream: DataStreamWriter;\n    messages: Message[];\n  },\n  executeFunctions: {\n    [K in keyof Tools & keyof ExecutableTools]?: (\n      args: z.infer<ExecutableTools[K]['parameters']>,\n      context: ToolExecutionOptions,\n    ) => Promise<any>;\n  },\n): Promise<Message[]> {\n  const lastMessage = messages[messages.length - 1];\n  const parts = lastMessage.parts;\n  if (!parts) return messages;\n\n  const processedParts = await Promise.all(\n    parts.map(async part => {\n      // Only process tool invocations parts\n      if (part.type !== 'tool-invocation') return part;\n\n      const { toolInvocation } = part;\n      const toolName = toolInvocation.toolName;\n\n      // Only continue if we have an execute function for the tool (meaning it requires confirmation) and it's in a 'result' state\n      if (!(toolName in executeFunctions) || toolInvocation.state !== 'result')\n        return part;\n\n      let result;\n\n      if (toolInvocation.result === APPROVAL.YES) {\n        // Get the tool and check if the tool has an execute function.\n        if (\n          !isValidToolName(toolName, executeFunctions) ||\n          toolInvocation.state !== 'result'\n        ) {\n          return part;\n        }\n\n        const toolInstance = executeFunctions[toolName];\n        if (toolInstance) {\n          result = await toolInstance(toolInvocation.args, {\n            messages: convertToCoreMessages(messages),\n            toolCallId: toolInvocation.toolCallId,\n          });\n        } else {\n          result = 'Error: No execute function found on tool';\n        }\n      } else if (toolInvocation.result === APPROVAL.NO) {\n        result = 'Error: User denied access to tool execution';\n      } else {\n        // For any unhandled responses, return the original part.\n        return part;\n      }\n\n      // Forward updated tool result to the client.\n      dataStream.write(\n        formatDataStreamPart('tool_result', {\n          toolCallId: toolInvocation.toolCallId,\n          result,\n        }),\n      );\n\n      // Return updated toolInvocation with the actual result.\n      return {\n        ...part,\n        toolInvocation: {\n          ...toolInvocation,\n          result,\n        },\n      };\n    }),\n  );\n\n  // Finally return the processed messages\n  return [...messages.slice(0, -1), { ...lastMessage, parts: processedParts }];\n}\n\nexport function getToolsRequiringConfirmation<T extends ToolSet>(\n  tools: T,\n): string[] {\n  return (Object.keys(tools) as (keyof T)[]).filter(key => {\n    const maybeTool = tools[key];\n    return typeof maybeTool.execute !== 'function';\n  }) as string[];\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing a Baseten Model Instance - TypeScript\nDESCRIPTION: Shows how to obtain a model instance (e.g. 'llama') from a previously created Baseten provider in TypeScript. The returned model instance is required for subsequent API calls to perform text generation or streaming operations using AI SDK functions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/40-baseten.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = baseten('llama');\n```\n\n----------------------------------------\n\nTITLE: Implementing Stop Functionality for useObject Hook in React\nDESCRIPTION: This React component adds a stop button that allows users to cancel the object generation process. The stop function is provided by the useObject hook.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/08-object-generation.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useObject } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { isLoading, stop, object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n  });\n\n  return (\n    <>\n      {isLoading && (\n        <button type=\"button\" onClick={() => stop()}>\n          Stop\n        </button>\n      )}\n\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Error Checking Methods\nDESCRIPTION: Shows the migration from isXXXError static methods to isInstance method for error checking.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_45\n\nLANGUAGE: typescript\nCODE:\n```\nimport { APICallError } from 'ai';\n\nAPICallError.isAPICallError(error);\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { APICallError } from 'ai';\n\nAPICallError.isInstance(error);\n```\n\n----------------------------------------\n\nTITLE: Importing AnthropicStream in React\nDESCRIPTION: Shows how to import the AnthropicStream utility from the 'ai' package in a React application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/08-anthropic-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { AnthropicStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Defining AI and UI State Types in TypeScript\nDESCRIPTION: This snippet defines the types for server and client messages, which represent the AI and UI states respectively. It also includes a server action for sending messages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nexport type ServerMessage = {\n  role: 'user' | 'assistant';\n  content: string;\n};\n\nexport type ClientMessage = {\n  id: string;\n  role: 'user' | 'assistant';\n  display: ReactNode;\n};\n\nexport const sendMessage = async (input: string): Promise<ClientMessage> => {\n  \"use server\"\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Importing extractReasoningMiddleware Module\nDESCRIPTION: Simple import statement for the extractReasoningMiddleware function from the ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/66-extract-reasoning-middleware.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { extractReasoningMiddleware } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Generating IDs with Updated generateId Default Length - TypeScript\nDESCRIPTION: Illustrates using the generateId function from ai-sdk-core, which now produces 16-character IDs by default. Update all usages and downstream systems such as database field lengths as needed. Input: none or options for length/entropy; output: string ID.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_14\n\nLANGUAGE: ts\nCODE:\n```\nimport { generateId } from 'ai';\n\nconst id = generateId(); // now 16 characters\n```\n\n----------------------------------------\n\nTITLE: Creating Directory Structure for Chat API\nDESCRIPTION: Shell command to create the necessary directory structure and route file for the chat API endpoint.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p app/api/chat && touch app/api/chat/route.ts\n```\n\n----------------------------------------\n\nTITLE: Providing Model IDs for Autocompletion with `createOpenAICompatible` in TypeScript\nDESCRIPTION: Demonstrates how to enhance developer experience by providing specific model ID types (for chat, completion, and embedding models) as generic arguments to `createOpenAICompatible`. This enables TypeScript autocompletion for known model IDs when calling methods like `chatModel`, while still allowing arbitrary string inputs for flexibility. It requires defining custom types for chat, completion, and embedding model IDs.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/openai-compatible/README.md#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { generateText } from 'ai';\n\ntype ExampleChatModelIds =\n  | 'meta-llama/Llama-3-70b-chat-hf'\n  | 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'\n  | (string & {});\n\ntype ExampleCompletionModelIds =\n  | 'codellama/CodeLlama-34b-Instruct-hf'\n  | 'Qwen/Qwen2.5-Coder-32B-Instruct'\n  | (string & {});\n\ntype ExampleEmbeddingModelIds =\n  | 'BAAI/bge-large-en-v1.5'\n  | 'bert-base-uncased'\n  | (string & {});\n\nconst model = createOpenAICompatible<\n  ExampleChatModelIds,\n  ExampleCompletionModelIds,\n  ExampleEmbeddingModelIds\n>({\n  baseURL: 'https://api.example.com/v1',\n  name: 'example',\n  apiKey: process.env.MY_API_KEY,\n});\n\n// Subsequent calls to e.g. `model.chatModel` will auto-complete the model id\n// from the list of `ExampleChatModelIds` while still allowing free-form\n// strings as well.\n\nconst { text } = await generateText({\n  model: model.chatModel('meta-llama/Llama-3-70b-chat-hf'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing the Inflection AI Provider Instance in TypeScript\nDESCRIPTION: Imports the default provider instance named `inflection` from the `inflection-ai-sdk-provider` package. This instance is used to configure the Vercel AI SDK to interact with the Inflection AI API.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/93-inflection-ai.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { inflection } from 'inflection-ai-sdk-provider';\n```\n\n----------------------------------------\n\nTITLE: Importing Default Cerebras Provider in TypeScript\nDESCRIPTION: Shows how to import the default pre-configured Cerebras provider instance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/40-cerebras.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { cerebras } from '@ai-sdk/cerebras';\n```\n\n----------------------------------------\n\nTITLE: Configuring a Voyage Text Embedding Model with Settings in TypeScript\nDESCRIPTION: Shows how to instantiate a Voyage text embedding model ('voyage-3') with specific settings. The example configures the `inputType` (document or query), disables `truncation`, sets the `outputDimension` (e.g., 1024), and specifies the `outputDtype` (e.g., 'float'). These settings customize the embedding generation process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/61-voyage-ai.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { voyage } from 'voyage-ai-provider';\n\nconst embeddingModel = voyage.textEmbeddingModel('voyage-3', {\n  inputType: 'document', // 'document' or 'query'\n  truncation: false,\n  outputDimension: 1024, // the new model voyage-code-3, voyage-3-large has 4 different output dimensions: 256, 512, 1024 (default), 2048\n  outputDtype: 'float', // output data types - int8, uint8, binary, ubinary are supported by the new model voyage-code-3, voyage-3-large\n});\n```\n\n----------------------------------------\n\nTITLE: Starting a development server for a Solid.js project\nDESCRIPTION: This code snippet shows how to start a development server for a Solid.js project after installing dependencies, with an option to open the app in a new browser tab.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/solidstart-openai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm run dev\n\n# or start the server and open the app in a new browser tab\nnpm run dev -- --open\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Azure AI Provider (TypeScript)\nDESCRIPTION: Shows how to use the Azure provider instance in conjunction with the generateText function from the ai package to perform a text generation operation. The model function accepts a deployment name and a prompt. Requires prior initialization of the azure instance and the ai package. Returns a text object containing the generated string.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/14-azure-ai.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: azure('your-deployment-name'),\n  prompt: 'Write a story about a robot.',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing onError Callback with streamText in Vercel AI SDK\nDESCRIPTION: This code snippet demonstrates how to implement an onError callback with the streamText function to properly handle and log errors that would otherwise be silently included in the stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/15-stream-text-not-working.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { streamText } from 'ai';\n\nconst result = streamText({\n  model: yourModel,\n  prompt: 'Invent a new holiday and describe its traditions.',\n  onError({ error }) {\n    console.error(error); // your error logging logic here\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Azure OpenAI Transcription\nDESCRIPTION: Shows how to set up and use Azure OpenAI's transcription services with configuration options for language, timestamps, and other parameters.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = azure.transcription('whisper-1');\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { azure } from '@ai-sdk/azure';\nimport { readFile } from 'fs/promises';\n\nconst result = await transcribe({\n  model: azure.transcription('whisper-1'),\n  audio: await readFile('audio.mp3'),\n  providerOptions: { azure: { language: 'en' } },\n});\n```\n\n----------------------------------------\n\nTITLE: Rendering Generated Images\nDESCRIPTION: Shows how to handle and render generated images in the chat interface.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_20\n\nLANGUAGE: tsx\nCODE:\n```\nmessages.map(message => (\n  <div key={message.id}>\n    {message.role === 'user' ? 'User: ' : 'AI: '}\n    {message.parts.map((part, index) => {\n      if (part.type === 'text') {\n        return <div key={index}>{part.text}</div>;\n      } else if (part.type === 'file' && part.mimeType.startsWith('image/')) {\n        return (\n          <img key={index} src={`data:${part.mimeType};base64,${part.data}`} />\n        );\n      }\n    })}\n  </div>\n));\n```\n\n----------------------------------------\n\nTITLE: Generating Text with OpenAI GPT-4.5 using AI SDK\nDESCRIPTION: This snippet demonstrates how to use the AI SDK to generate text using OpenAI's GPT-4.5 model. It imports necessary functions and uses the generateText function to create a response based on a given prompt.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/22-gpt-4-5.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { text } = await generateText({\n  model: openai('gpt-4.5-preview'),\n  prompt: 'Explain the concept of quantum entanglement.',\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Custom Data with AI SDK in TypeScript\nDESCRIPTION: This snippet demonstrates how to use createDataStreamResponse and streamText to send custom data and annotations alongside the model's response. It includes writing data, message annotations, and handling errors.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/20-streaming-data.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateId, createDataStreamResponse, streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  // immediately start streaming (solves RAG issues with status, etc.)\n  return createDataStreamResponse({\n    execute: dataStream => {\n      dataStream.writeData('initialized call');\n\n      const result = streamText({\n        model: openai('gpt-4o'),\n        messages,\n        onChunk() {\n          dataStream.writeMessageAnnotation({ chunk: '123' });\n        },\n        onFinish() {\n          // message annotation:\n          dataStream.writeMessageAnnotation({\n            id: generateId(), // e.g. id from saved DB record\n            other: 'information',\n          });\n\n          // call annotation:\n          dataStream.writeData('call completed');\n        },\n      });\n\n      result.mergeIntoDataStream(dataStream);\n    },\n    onError: error => {\n      // Error messages are masked by default for security reasons.\n      // If you want to expose the error message to the client, you can do so here:\n      return error instanceof Error ? error.message : String(error);\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Next.js App with AI SDK and OpenAI Telemetry Example (pnpm)\nDESCRIPTION: This command uses pnpm to bootstrap a new Next.js application with the AI SDK and OpenAI telemetry example. It clones the example from the Vercel AI repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry next-openai-telemetry-app\n```\n\n----------------------------------------\n\nTITLE: Importing useUIState Hook from AI SDK RSC\nDESCRIPTION: Demonstrates how to import the useUIState hook from the AI SDK RSC package for managing UI state in AI components.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/10-use-ui-state.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { useUIState } from \"ai/rsc\"\n```\n\n----------------------------------------\n\nTITLE: Defining Telemetry Settings Structure Type in TypeScript\nDESCRIPTION: Defines the structure for telemetry configuration (`TelemetrySettings`). Parameters include `isEnabled` (boolean, optional, default: false), `recordInputs` (boolean, optional, default: true), `recordOutputs` (boolean, optional, default: true), `functionId` (string, optional), and `metadata` (Record of primitive types or arrays, optional).\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_21\n\nLANGUAGE: typescript\nCODE:\n```\nTelemetrySettings\n```\n\n----------------------------------------\n\nTITLE: Tracing Function Arguments and Return Values\nDESCRIPTION: Use Laminar's observe wrapper to trace function input arguments and return values.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/laminar.mdx#2025-04-23_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst result = await observe(\n  { name: 'poem writer' },\n  async (topic: string, mood: string) => {\n    const { text } = await generateText({\n      model: openai('gpt-4o-mini'),\n      prompt: `Write a poem about ${topic} in ${mood} mood.`,\n    });\n    return text;\n  },\n  'Laminar flow',\n  'happy',\n);\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Fireworks Model\nDESCRIPTION: Example of using a Fireworks model with the generateText function to create a recipe.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/26-fireworks.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fireworks } from '@ai-sdk/fireworks';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: fireworks('accounts/fireworks/models/firefunction-v1'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Cerebras Provider via npm (bash)\nDESCRIPTION: This code snippet demonstrates how to install the Cerebras provider for the AI SDK using npm from the command line. The package '@ai-sdk/cerebras' must be installed before importing or using any related API functionality in a TypeScript or JavaScript project. Installing this dependency ensures that all Cerebras-related classes and functions are available for use in your application.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/cerebras/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/cerebras\n```\n\n----------------------------------------\n\nTITLE: Importing StreamingTextResponse from AI SDK\nDESCRIPTION: Shows how to import the StreamingTextResponse utility from the AI package. Note that this has been deprecated in AI SDK 4.0 in favor of streamText.toDataStreamResponse().\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/02-streaming-text-response.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { StreamingTextResponse } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Generation with AI SDK in TypeScript\nDESCRIPTION: Shows how to use the streamText function to stream text from a language model. This example demonstrates creating a new holiday and its traditions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_3\n\nLANGUAGE: ts\nCODE:\n```\nimport { streamText } from 'ai';\n\nconst result = streamText({\n  model: yourModel,\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n\n// example: use textStream as an async iterable\nfor await (const textPart of result.textStream) {\n  console.log(textPart);\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Git Repository Commands\nDESCRIPTION: Basic Git commands for committing local changes and preparing for repository setup.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/10-vercel-deployment-guide.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit add .\ngit commit -m \"init\"\n```\n\n----------------------------------------\n\nTITLE: Using Claude 3.7 Sonnet via Amazon Bedrock\nDESCRIPTION: Shows how to use Claude 3.7 Sonnet through Amazon Bedrock instead of Anthropic's direct API. Demonstrates the SDK's provider flexibility with minimal code changes.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/20-sonnet-3-7.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\nimport { generateText } from 'ai';\n\nconst { reasoning, text } = await generateText({\n  model: bedrock('anthropic.claude-3-7-sonnet-20250219-v1:0'),\n  prompt: 'How many people will live in the world in 2040?',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Cohere Provider in TypeScript\nDESCRIPTION: Shows how to create a customized Cohere provider instance with specific settings. This approach allows for configuration options like custom base URLs, API keys, headers, or fetch implementations.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/25-cohere.mdx#2025-04-23_snippet_1\n\nLANGUAGE: ts\nCODE:\n```\nimport { createCohere } from '@ai-sdk/cohere';\n\nconst cohere = createCohere({\n  // custom settings\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Tool Call Logging\nDESCRIPTION: Enhanced version that adds logging of tool calls and results to help debug and understand the AI's interaction with the weather tool.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { CoreMessage, streamText, tool } from 'ai';\nimport dotenv from 'dotenv';\nimport { z } from 'zod';\nimport * as readline from 'node:readline/promises';\n\ndotenv.config();\n\nconst terminal = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n});\n\nconst messages: CoreMessage[] = [];\n\nasync function main() {\n  while (true) {\n    const userInput = await terminal.question('You: ');\n\n    messages.push({ role: 'user', content: userInput });\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages,\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (in Celsius)',\n          parameters: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => ({\n            location,\n            temperature: Math.round((Math.random() * 30 + 5) * 10) / 10,\n          }),\n        }),\n      },\n    });\n\n    let fullResponse = '';\n    process.stdout.write('\\nAssistant: ');\n    for await (const delta of result.textStream) {\n      fullResponse += delta;\n      process.stdout.write(delta);\n    }\n    process.stdout.write('\\n\\n');\n\n    console.log(await result.toolCalls);\n    console.log(await result.toolResults);\n    messages.push({ role: 'assistant', content: fullResponse });\n  }\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Updating Provider Registry Type Imports in TypeScript\nDESCRIPTION: Shows the migration from experimental provider registry types in AI SDK 3.4 to the stable `Provider` type in AI SDK 4.0. The `experimental_Provider` and `experimental_ProviderRegistry` interfaces have been removed.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_22\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_Provider, experimental_ProviderRegistry } from 'ai';\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Provider } from 'ai';\n```\n\n----------------------------------------\n\nTITLE: Configuring a Provider Instance with a Custom Metadata Extractor in TypeScript\nDESCRIPTION: Shows how to pass a custom `MetadataExtractor` object (e.g., `myMetadataExtractor`) to the `createOpenAICompatible` function via the `metadataExtractor` option. This enables the provider instance to use the custom logic for extracting metadata from API responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/index.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst provider = createOpenAICompatible({\n  name: 'my-provider',\n  apiKey: process.env.PROVIDER_API_KEY,\n  baseURL: 'https://api.provider.com/v1',\n  metadataExtractor: myMetadataExtractor,\n});\n```\n\n----------------------------------------\n\nTITLE: Message Component with Loading States\nDESCRIPTION: React component for rendering messages with tool invocations and loading states, supporting weather data display.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_9\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nexport function Message({ role, content, toolInvocations }) {\n  return (\n    <div>\n      <div>{role}</div>\n      <div>{content}</div>\n\n      {toolInvocations && (\n        <div>\n          {toolInvocations.map(toolInvocation => {\n            const { toolName, toolCallId, state } = toolInvocation;\n\n            if (state === 'result') {\n              const { result } = toolInvocation;\n\n              return (\n                <div key={toolCallId}>\n                  {toolName === 'getWeather' ? (\n                    <Weather weatherAtLocation={result} />\n                  ) : null}\n                </div>\n              );\n            } else {\n              return (\n                <div key={toolCallId}>\n                  {toolName === 'getWeather' ? (\n                    <Weather isLoading={true} />\n                  ) : (\n                    <div>Loading...</div>\n                  )}\n                </div>\n              );\n            }\n          })}\n        </div>\n      )}\n    </div>\n  );\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Mistral using Vercel AI SDK in TypeScript\nDESCRIPTION: This TypeScript example shows how to generate text using the Mistral model via the Vercel AI SDK. It imports the `mistral` provider and the `generateText` function, then calls `generateText` specifying the 'mistral-large-latest' model and a prompt. The generated text is extracted from the result.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/mistral/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { mistral } from '@ai-sdk/mistral';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: mistral('mistral-large-latest'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiple Tools in Vercel AI Chatbot - TypeScript\nDESCRIPTION: Implementation of a chatbot with two tools: one for getting weather data and another for converting temperatures from Celsius to Fahrenheit. Uses the Vercel AI SDK with OpenAI's GPT-4 model and includes terminal-based interface for user interaction.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { CoreMessage, streamText, tool } from 'ai';\nimport dotenv from 'dotenv';\nimport { z } from 'zod';\nimport * as readline from 'node:readline/promises';\n\ndotenv.config();\n\nconst terminal = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n});\n\nconst messages: CoreMessage[] = [];\n\nasync function main() {\n  while (true) {\n    const userInput = await terminal.question('You: ');\n\n    messages.push({ role: 'user', content: userInput });\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages,\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (in Celsius)',\n          parameters: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => ({\n            location,\n            temperature: Math.round((Math.random() * 30 + 5) * 10) / 10, // Random temp between 5Â°C and 35Â°C\n          }),\n        }),\n        convertCelsiusToFahrenheit: tool({\n          description: 'Convert a temperature from Celsius to Fahrenheit',\n          parameters: z.object({\n            celsius: z\n              .number()\n              .describe('The temperature in Celsius to convert'),\n          }),\n          execute: async ({ celsius }) => {\n            const fahrenheit = (celsius * 9) / 5 + 32;\n            return { fahrenheit: Math.round(fahrenheit * 100) / 100 };\n          },\n        }),\n      },\n      maxSteps: 5,\n      onStepFinish: step => {\n        console.log(JSON.stringify(step, null, 2));\n      },\n    });\n\n    let fullResponse = '';\n    process.stdout.write('\\nAssistant: ');\n    for await (const delta of result.textStream) {\n      fullResponse += delta;\n      process.stdout.write(delta);\n    }\n    process.stdout.write('\\n\\n');\n\n    messages.push({ role: 'assistant', content: fullResponse });\n  }\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_RetryError Instance in TypeScript\nDESCRIPTION: Demonstrates how to import the RetryError class from the 'ai' package and check if a caught error is an instance of AI_RetryError. Requires the 'ai' module as a dependency and expects an error object, verifying it using RetryError.isInstance. The key output is a conditional branch for handling retry errors.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-retry-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RetryError } from 'ai';\n\nif (RetryError.isInstance(error)) {\n  // Handle the error\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Anthropic language model instance\nDESCRIPTION: Shows how to create an Anthropic language model by specifying the model ID. This instance can be used with the AI SDK's generation functions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = anthropic('claude-3-haiku-20240307');\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Chat Prompt using OpenAI GPT-3.5-turbo in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the AI SDK to generate text based on a chat prompt. It imports necessary functions, sets up the OpenAI model, defines system and user messages, and generates text with a maximum token limit.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/11-generate-text-with-chat-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst result = await generateText({\n  model: openai('gpt-3.5-turbo'),\n  maxTokens: 1024,\n  system: 'You are a helpful chatbot.',\n  messages: [\n    {\n      role: 'user',\n      content: 'Hello!',\n    },\n    {\n      role: 'assistant',\n      content: 'Hello! How can I help you today?',\n    },\n    {\n      role: 'user',\n      content: 'I need help with my computer.',\n    },\n  ],\n});\n\nconsole.log(result.text);\n```\n\n----------------------------------------\n\nTITLE: Configuring AI SDK Settings for Text Generation in TypeScript\nDESCRIPTION: This snippet demonstrates how to use common settings when generating text with the AI SDK. It shows the usage of maxTokens, temperature, and maxRetries along with the model and prompt.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/25-settings.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  maxTokens: 512,\n  temperature: 0.3,\n  maxRetries: 5,\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing AIStream in React\nDESCRIPTION: Shows how to import the AIStream helper function from the ai package in a React application. Note that this is deprecated in AI SDK 4.0 in favor of streamText.toDataStreamResponse().\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/01-ai-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { AIStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with AssemblyAI and AI SDK (TypeScript)\nDESCRIPTION: Demonstrates transcribing an audio file using the Vercel AI SDK and the AssemblyAI provider. It imports the `assemblyai` provider and the `experimental_transcribe` function, then calls `transcribe` with the AssemblyAI transcription model ('best') and the URL of the audio file. The transcribed text is extracted from the result.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/assemblyai/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { assemblyai } from '@ai-sdk/assemblyai';\nimport { experimental_transcribe as transcribe } from 'ai';\n\nconst { text } = await transcribe({\n  model: assemblyai.transcription('best'),\n  audio: new URL(\n    'https://github.com/vercel/ai/raw/refs/heads/main/examples/ai-core/data/galileo.mp3',\n  ),\n});\n```\n\n----------------------------------------\n\nTITLE: Importing HuggingFaceStream in React\nDESCRIPTION: Shows how to import the HuggingFaceStream function from the AI SDK package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/15-hugging-face-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HuggingFaceStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Embedding Generation and Similarity Search in TypeScript\nDESCRIPTION: Defines functions for generating embeddings and finding relevant content using OpenAI's embedding model. Includes methods for chunking text, generating single/multiple embeddings, and performing similarity searches using cosine distance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_17\n\nLANGUAGE: tsx\nCODE:\n```\nimport { embed, embedMany } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { db } from '../db';\nimport { cosineDistance, desc, gt, sql } from 'drizzle-orm';\nimport { embeddings } from '../db/schema/embeddings';\n\nconst embeddingModel = openai.embedding('text-embedding-ada-002');\n\nconst generateChunks = (input: string): string[] => {\n  return input\n    .trim()\n    .split('.')\n    .filter(i => i !== '');\n};\n\nexport const generateEmbeddings = async (\n  value: string,\n): Promise<Array<{ embedding: number[]; content: string }>> => {\n  const chunks = generateChunks(value);\n  const { embeddings } = await embedMany({\n    model: embeddingModel,\n    values: chunks,\n  });\n  return embeddings.map((e, i) => ({ content: chunks[i], embedding: e }));\n};\n\nexport const generateEmbedding = async (value: string): Promise<number[]> => {\n  const input = value.replaceAll('\\\\n', ' ');\n  const { embedding } = await embed({\n    model: embeddingModel,\n    value: input,\n  });\n  return embedding;\n};\n\nexport const findRelevantContent = async (userQuery: string) => {\n  const userQueryEmbedded = await generateEmbedding(userQuery);\n  const similarity = sql<number>`1 - (${cosineDistance(\n    embeddings.embedding,\n    userQueryEmbedded,\n  )})`;\n  const similarGuides = await db\n    .select({ name: embeddings.content, similarity })\n    .from(embeddings)\n    .where(gt(similarity, 0.5))\n    .orderBy(t => desc(t.similarity))\n    .limit(4);\n  return similarGuides;\n};\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Data with DeepSeek Reasoner and OpenAI Models in TypeScript\nDESCRIPTION: This code demonstrates a two-step process for generating structured data. First, it uses the DeepSeek reasoner model to generate detailed text predictions about future city populations. Then, it passes that output to OpenAI's gpt-4o-mini model to extract structured data according to a Zod schema, resulting in an array of city objects with specified properties.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/30-generate-object-reasoning.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepseek } from '@ai-sdk/deepseek';\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject, generateText } from 'ai';\nimport 'dotenv/config';\nimport { z } from 'zod';\n\nasync function main() {\n  const { text: rawOutput } = await generateText({\n    model: deepseek('deepseek-reasoner'),\n    prompt:\n      'Predict the top 3 largest city by 2050. For each, return the name, the country, the reason why it will on the list, and the estimated population in millions.',\n  });\n\n  const { object } = await generateObject({\n    model: openai('gpt-4o-mini'),\n    prompt: 'Extract the desired information from this text: \\n' + rawOutput,\n    schema: z.object({\n      name: z.string().describe('the name of the city'),\n      country: z.string().describe('the name of the country'),\n      reason: z\n        .string()\n        .describe(\n          'the reason why the city will be one of the largest cities by 2050',\n        ),\n      estimatedPopulation: z.number(),\n    }),\n    output: 'array',\n  });\n\n  console.log(object);\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry with AISDKExporter in Next.js\nDESCRIPTION: TypeScript code for setting up OpenTelemetry instrumentation with AISDKExporter in a Next.js application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { registerOTel } from '@vercel/otel';\nimport { AISDKExporter } from 'langsmith/vercel';\n\nexport function register() {\n  registerOTel({\n    serviceName: 'langsmith-vercel-ai-sdk-example',\n    traceExporter: new AISDKExporter(),\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Rendering Messages with Reasoning\nDESCRIPTION: Shows how to render messages that include reasoning parts on the client side.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_17\n\nLANGUAGE: tsx\nCODE:\n```\nmessages.map(message => (\n  <div key={message.id}>\n    {message.role === 'user' ? 'User: ' : 'AI: '}\n    {message.parts.map((part, index) => {\n      // text parts:\n      if (part.type === 'text') {\n        return <div key={index}>{part.text}</div>;\n      }\n\n      // reasoning parts:\n      if (part.type === 'reasoning') {\n        return (\n          <pre key={index}>\n            {part.details.map(detail =>\n              detail.type === 'text' ? detail.text : '<redacted>',\n            )}\n          </pre>\n        );\n      }\n    })}\n  </div>\n));\n```\n\n----------------------------------------\n\nTITLE: Making API Call to Anthropic Claude Model in TypeScript\nDESCRIPTION: This code snippet shows the structure of an API request to Anthropic's Claude 3.5 Sonnet model. It includes the model parameters, message content, and cache control settings. The request fails due to insufficient token count in the initial message.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/ai-core/data/error-message.txt#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nBody {\n  \"model\": \"claude-3-5-sonnet-20240620\",\n  \"max_tokens\": 4096,\n  \"temperature\": 0,\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"You are a JavaScript expert.\"\n        },\n        {\n          \"type\": \"text\",\n          \"text\": \"Error messages: \\nAPICallError [AI_APICallError]: Failed to process error response\\n    at postToApi (/Users/larsgrammel/repositories/ai/packages/provider-utils/dist/index.js:382:15)\\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\\n    ... 4 lines matching cause stack trace ...\\n    at async fn (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:2723:36)\\n    at async /Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:339:22\\n    at async main (/Users/larsgrammel/repositories/ai/examples/ai-core/src/generate-text/anthropic-cache-control.ts:2:1351) {\\n  cause: TypeError: Body is unusable\\n      at consumeBody (node:internal/deps/undici/undici:4281:15)\\n      at _Response.text (node:internal/deps/undici/undici:4236:18)\\n      at /Users/larsgrammel/repositories/ai/packages/provider-utils/dist/index.js:443:39\\n      at postToApi (/Users/larsgrammel/repositories/ai/packages/provider-utils/dist/index.js:373:34)\\n      at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\\n      at async AnthropicMessagesLanguageModel.doGenerate (/Users/larsgrammel/repositories/ai/packages/anthropic/dist/index.js:316:50)\\n      at async fn (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:2748:34)\\n      at async /Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:339:22\\n      at async _retryWithExponentialBackoff (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:170:12)\\n      at async fn (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:2723:36),\\n  url: 'https://api.anthropic.com/v1/messages',\\n  requestBodyValues: {\\n    model: 'claude-3-5-sonnet-20240620',\\n    top_k: undefined,\\n    max_tokens: 4096,\\n    temperature: 0,\\n    top_p: undefined,\\n    stop_sequences: undefined,\\n    system: undefined,\\n    messages: [ [Object] ],\\n    tools: undefined,\\n    tool_choice: undefined\\n  },\\n  statusCode: 400,\\n  responseHeaders: {\\n    'cf-cache-status': 'DYNAMIC',\\n    'cf-ray': '8b39b60ab8734516-TXL',\\n    connection: 'keep-alive',\\n    'content-length': '171',\\n    'content-type': 'application/json',\\n    date: 'Thu, 15 Aug 2024 14:00:28 GMT',\\n    'request-id': 'req_01PLrS159iiihG7kS9PFQiqx',\\n    server: 'cloudflare',\\n    via: '1.1 google',\\n    'x-cloud-trace-context': '1371f8e6d358102b79d109db3829d62e',\\n    'x-robots-tag': 'none',\\n    'x-should-retry': 'false'\\n  },\\n  responseBody: undefined,\\n  isRetryable: false,\\n  data: undefined,\\n  [Symbol(vercel.ai.error)]: true,\\n  [Symbol(vercel.ai.error.AI_APICallError)]: true\\n}\",\n          \"cache_control\": {\n            \"type\": \"ephemeral\"\n          }\n        },\n        {\n          \"type\": \"text\",\n          \"text\": \"Explain the error message.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Client to Use Chat Hook (UI)\nDESCRIPTION: This snippet demonstrates how to update the client-side code to use the useChat hook from AI SDK UI for handling chat interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, setInput, handleSubmit } = useChat();\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>{message.role}</div>\n          <div>{message.content}</div>\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Using Rev.ai for audio transcription with additional options\nDESCRIPTION: Demonstrates a complete example of using the Rev.ai provider for audio transcription, including loading an audio file and specifying provider-specific options like the input language. Shows how to use the experimental_transcribe function from the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/160-revai.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { revai } from '@ai-sdk/revai';\nimport { readFile } from 'fs/promises';\n\nconst result = await transcribe({\n  model: revai.transcription('machine'),\n  audio: await readFile('audio.mp3'),\n  providerOptions: { revai: { language: 'en' } },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for AI SDK\nDESCRIPTION: Sets up the required OpenAI API key in an environment file for use with the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/node-http-server/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: JSON Schema for Celsius to Fahrenheit Tool\nDESCRIPTION: JSON schema that defines a tool for converting Celsius to Fahrenheit temperature values. This schema needs to be provided to the OpenAI Assistant Dashboard when setting up the assistant.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/121-stream-assistant-response-with-tools.mdx#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"celsiusToFahrenheit\",\n  \"description\": \"convert celsius to fahrenheit.\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"value\": {\n        \"type\": \"number\",\n        \"description\": \"the value in celsius.\"\n      }\n    },\n    \"required\": [\"value\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing AI SDK Endpoint with Curl\nDESCRIPTION: A curl command to test the AI SDK endpoint by sending a POST request to the local server running on port 8080.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/fastify/README.md#2025-04-23_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncurl -X POST http://localhost:8080\n```\n\n----------------------------------------\n\nTITLE: Client Component Reading Streamable Value\nDESCRIPTION: Shows how to consume a streamable value in a React client component using readStreamableValue. The example demonstrates reading stream updates and updating component state with each new value.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/05-read-streamable-value.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { readStreamableValue } from 'ai/rsc';\n\nexport default function Page() {\n  const [generation, setGeneration] = useState('');\n\n  return (\n    <div>\n      <button\n        onClick={async () => {\n          const stream = await generate();\n\n          for await (const delta of readStreamableValue(stream)) {\n            setGeneration(generation => generation + delta);\n          }\n        }}\n      >\n        Generate\n      </button>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Default Settings Middleware\nDESCRIPTION: Shows how to use defaultSettingsMiddleware to apply default configuration to a language model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { wrapLanguageModel, defaultSettingsMiddleware } from 'ai';\n\nconst model = wrapLanguageModel({\n  model: yourModel,\n  middleware: defaultSettingsMiddleware({\n    settings: {\n      temperature: 0.5,\n      maxTokens: 800,\n      // note: use providerMetadata instead of providerOptions here:\n      providerMetadata: { openai: { store: false } },\n    },\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Interface with React and AI SDK\nDESCRIPTION: Frontend implementation of chat interface using AI SDK's useChat hook, including message display and input handling.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n            }\n          })}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Transcription Model with AssemblyAI in TypeScript\nDESCRIPTION: Shows how to create a transcription model using the AssemblyAI provider with the 'best' model option.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/100-assemblyai.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = assemblyai.transcription('best');\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Types and Error Handling Interfaces (TypeScript)\nDESCRIPTION: This snippet defines the foundational shape for registering tools and error handling in the streaming system using TypeScript objects. It specifies the name, type, and description of a set of expected fields, including how to retrieve a parameter schema and capture error types related to tool parsing. No external dependencies are required besides TypeScript. It expects objects to conform to specific property names and types for tools integration and error propagation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  name: 'tools',\n  type: 'TOOLS',\n  description: 'The tools that are available.',\n},\n{\n  name: 'parameterSchema',\n  type: '(options: { toolName: string }) => JSONSchema7',\n  description:\n    'A function that returns the JSON Schema for a tool.',\n},\n{\n  name: 'error',\n  type: 'NoSuchToolError | InvalidToolArgumentsError',\n  description:\n    'The error that occurred while parsing the tool call.',\n},\n```\n\n----------------------------------------\n\nTITLE: Generating Text with generateText in a Cloudflare Worker - TypeScript\nDESCRIPTION: This TypeScript snippet is a complete example of a Cloudflare Worker endpoint that generates text using a Cloudflare Workers AI language model. It utilizes generateText from the ai package alongside a model from workersai, accepting a prompt and returning generated text as an HTTP response. Depends on 'workers-ai-provider' and 'ai' packages; requires type Env with an AI property. Input is an HTTP request and the Cloudflare environment, output is a Response with the generated text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/11-cloudflare-workers-ai.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createWorkersAI } from 'workers-ai-provider';\nimport { generateText } from 'ai';\n\ntype Env = {\n  AI: Ai;\n};\n\nexport default {\n  async fetch(_: Request, env: Env) {\n    const workersai = createWorkersAI({ binding: env.AI });\n    const result = await generateText({\n      model: workersai('@cf/meta/llama-2-7b-chat-int8'),\n      prompt: 'Write a 50-word essay about hello world.',\n    });\n\n    return new Response(result.text);\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Installing xAI Grok Package (npm)\nDESCRIPTION: This snippet demonstrates installing the xAI Grok provider using npm. It's a prerequisite for using the xAI Grok models.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nnpm install @ai-sdk/xai\n```\n\n----------------------------------------\n\nTITLE: Importing LangChainStream from AI SDK\nDESCRIPTION: Shows how to import the LangChainStream utility from the AI SDK package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/16-langchain-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LangChainStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Mistral Provider: Facade Replacement - TypeScript\nDESCRIPTION: Details the migration from the deprecated Mistral facade to the createMistral factory in AI SDK 4.0. The provider is instantiated via a function rather than a class. Requires correct import and typical provider configuration object as input.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_7\n\nLANGUAGE: ts\nCODE:\n```\nconst mistral = new Mistral({\n  // ...\n});\n```\n\nLANGUAGE: ts\nCODE:\n```\nconst mistral = createMistral({\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Anthropic Provider with Model Aliases\nDESCRIPTION: Sets up a custom Anthropic provider with model name aliases for different Claude versions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/45-provider-management.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic as originalAnthropic } from '@ai-sdk/anthropic';\nimport { customProvider } from 'ai';\n\n// custom provider with alias names:\nexport const anthropic = customProvider({\n  languageModels: {\n    opus: originalAnthropic('claude-3-opus-20240229'),\n    sonnet: originalAnthropic('claude-3-5-sonnet-20240620'),\n    haiku: originalAnthropic('claude-3-haiku-20240307'),\n  },\n  fallbackProvider: originalAnthropic,\n});\n```\n\n----------------------------------------\n\nTITLE: Rendering Example Links Component in JSX\nDESCRIPTION: This code snippet defines an ExampleLinks component that takes an array of example objects, each with a title and link property, and renders them as a list of links for navigation purposes within the Vercel AI project documentation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/04-stream-object.mdx#2025-04-23_snippet_7\n\nLANGUAGE: jsx\nCODE:\n```\n<ExampleLinks\n  examples={[\n    {\n      title: 'Streaming Object Generation with RSC',\n      link: '/examples/next-app/basics/streaming-object-generation',\n    },\n    {\n      title: 'Streaming Object Generation with useObject',\n      link: '/examples/next-pages/basics/streaming-object-generation',\n    },\n    {\n      title: 'Streaming Partial Objects',\n      link: '/examples/node/streaming-structured-data/stream-object',\n    },\n    {\n      title: 'Recording Token Usage',\n      link: '/examples/node/streaming-structured-data/token-usage',\n    },\n    {\n      title: 'Recording Final Object',\n      link: '/examples/node/streaming-structured-data/object',\n    },\n  ]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Fixing Prettier Formatting Issues in AI SDK\nDESCRIPTION: Command for automatically fixing formatting issues in the AI SDK codebase using Prettier, which should be run before submitting a pull request.\nSOURCE: https://github.com/vercel/ai/blob/main/CONTRIBUTING.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npnpm prettier-fix\n```\n\n----------------------------------------\n\nTITLE: Slack Events API Endpoint Configuration\nDESCRIPTION: URL configuration for Slack Events API integration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/03-slackbot.mdx#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhttps://your-vercel-url.vercel.app/api/events\n```\n\n----------------------------------------\n\nTITLE: Example Structure of Gemini Safety Ratings Response (JSON)\nDESCRIPTION: Provides an example JSON structure for safety ratings that accompany Gemini model outputs, detailing categories, probability, severity, and blocking information. Appropriate for those implementing safety-aware pipelines or needing to validate/parse safety info when assessing model responses for sensitive content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"safetyRatings\": [\n    {\n      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probabilityScore\": 0.11027937,\n      \"severity\": \"HARM_SEVERITY_LOW\",\n      \"severityScore\": 0.28487435\n    },\n    {\n      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n      \"probability\": \"HIGH\",\n      \"blocked\": true,\n      \"probabilityScore\": 0.95422274,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severityScore\": 0.43398145\n    },\n    {\n      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probabilityScore\": 0.11085559,\n      \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n      \"severityScore\": 0.19027223\n    },\n    {\n      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probabilityScore\": 0.22901751,\n      \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n      \"severityScore\": 0.09089675\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Recipe Schema with TypeScript JSON Schema\nDESCRIPTION: Example showing how to create a typed JSON schema for a recipe object using the jsonSchema helper function. The schema defines a structure for recipes including name, ingredients array, and steps array.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/25-json-schema.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { jsonSchema } from 'ai';\n\nconst mySchema = jsonSchema<{\n  recipe: {\n    name: string;\n    ingredients: { name: string; amount: string }[];\n    steps: string[];\n  };\n}>({\n  type: 'object',\n  properties: {\n    recipe: {\n      type: 'object',\n      properties: {\n        name: { type: 'string' },\n        ingredients: {\n          type: 'array',\n          items: {\n            type: 'object',\n            properties: {\n              name: { type: 'string' },\n              amount: { type: 'string' },\n            },\n            required: ['name', 'amount'],\n          },\n        },\n        steps: {\n          type: 'array',\n          items: { type: 'string' },\n        },\n      },\n      required: ['name', 'ingredients', 'steps'],\n    },\n  },\n  required: ['recipe'],\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text Using Cerebras Model with AI SDK (TypeScript)\nDESCRIPTION: This TypeScript example demonstrates how to use the imported Cerebras provider with the AI SDK's 'generateText' function to invoke a specific Cerebras model ('llama3.1-8b'). The code constructs a prompt and requests text generation, returning the generated text via destructuring. Dependencies include both '@ai-sdk/cerebras' and the 'ai' package. Users must ensure async context, and limits such as model context window (8192 tokens) apply. Input is a completion prompt string; output is an object containing the model's generated text.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/cerebras/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { cerebras } from '@ai-sdk/cerebras';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: cerebras('llama3.1-8b'),\n  prompt: 'Write a JavaScript function that sorts a list:',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Bedrock Language Model Instance\nDESCRIPTION: Example of creating an Amazon Bedrock language model instance by specifying the model ID.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = bedrock('meta.llama3-70b-instruct-v1:0');\n```\n\n----------------------------------------\n\nTITLE: Importing Default Perplexity Provider (TypeScript)\nDESCRIPTION: Imports the default Perplexity provider instance from the @ai-sdk/perplexity package. This is the recommended entry point for most users who want to quickly integrate Perplexity functionality into their applications. No additional setup required beyond package installation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/70-perplexity.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { perplexity } from '@ai-sdk/perplexity';\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Uppercase Transformation in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a custom transformation function that converts all text to uppercase. It uses a TransformStream to modify the text-delta chunks in the stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst upperCaseTransform =\n  <TOOLS extends ToolSet>() =>\n  (options: { tools: TOOLS; stopStream: () => void }) =>\n    new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({\n      transform(chunk, controller) {\n        controller.enqueue(\n          // for text-delta chunks, convert the text to uppercase:\n          chunk.type === 'text-delta'\n            ? { ...chunk, textDelta: chunk.textDelta.toUpperCase() }\n            : chunk,\n        );\n      },\n    });\n```\n\n----------------------------------------\n\nTITLE: Calculating Cosine Similarity Between Text Embeddings in TypeScript\nDESCRIPTION: Example demonstrating how to calculate cosine similarity between embeddings of two text strings using OpenAI embeddings. The code shows importing required functions, generating embeddings, and computing their similarity.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/50-cosine-similarity.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { cosineSimilarity, embedMany } from 'ai';\n\nconst { embeddings } = await embedMany({\n  model: openai.embedding('text-embedding-3-small'),\n  values: ['sunny day at the beach', 'rainy afternoon in the city'],\n});\n\nconsole.log(\n  `cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,\n);\n```\n\n----------------------------------------\n\nTITLE: Removing init Option in pipeDataStreamToResponse/toDataStreamResponse (TypeScript)\nDESCRIPTION: Demonstrates the removal of the nested `init` option in `pipeDataStreamToResponse` and `toDataStreamResponse` functions in AI SDK 4.0. Configuration properties like `headers` should now be passed directly within the main options object.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_25\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await streamText({\n  // ...\n});\n\nresult.toDataStreamResponse(response, {\n  init: {\n    headers: {\n      'X-Custom-Header': 'value',\n    },\n  },\n  // ...\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = streamText({\n  // ...\n});\n\nresult.toDataStreamResponse(response, {\n  headers: {\n    'X-Custom-Header': 'value',\n  },\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Luma Provider Instance (TypeScript)\nDESCRIPTION: Demonstrates how to import 'createLuma' and configure a custom provider instance. You can specify an API key, a base URL endpoint, and custom headers for requests. This approach is suitable for users needing authentication control, endpoint overrides, or middleware (e.g., custom fetch), and all settings are optional with sensible defaults (environment variables and standard host URLs).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/80-luma.mdx#2025-04-23_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { createLuma } from '@ai-sdk/luma';\n\nconst luma = createLuma({\n  apiKey: 'your-api-key', // optional, defaults to LUMA_API_KEY environment variable\n  baseURL: 'custom-url', // optional\n  headers: {\n    /* custom headers */\n  }, // optional\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Image-Based Tool Calls with OpenAI GPT-4 Turbo\nDESCRIPTION: Example showing how to integrate image processing with tool calls using the AI SDK. The code demonstrates logging food items by analyzing an image using GPT-4 Turbo and executing a custom tool to store food information in a database.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/52-call-tools-with-image-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText, tool } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nconst result = await generateText({\n  model: openai('gpt-4-turbo'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'can you log this meal for me?' },\n        {\n          type: 'image',\n          image: new URL(\n            'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Cheeseburger_%2817237580619%29.jpg/640px-Cheeseburger_%2817237580619%29.jpg',\n          ),\n        },\n      ],\n    },\n  ],\n  tools: {\n    logFood: tool({\n      description: 'Log a food item',\n      parameters: z.object({\n        name: z.string(),\n        calories: z.number(),\n      }),\n      execute({ name, calories }) {\n        storeInDatabase({ name, calories });\n      },\n    }),\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Example Google Vertex Safety Ratings Response (JSON)\nDESCRIPTION: Provides a sample JSON response for safetyRatings returned from the Vertex model, showing multiple harm categories, probability and severity scores, and block flags. Used for auditing and filtering outputs after generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"safetyRatings\": [\n    {\n      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probabilityScore\": 0.11027937,\n      \"severity\": \"HARM_SEVERITY_LOW\",\n      \"severityScore\": 0.28487435\n    },\n    {\n      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n      \"probability\": \"HIGH\",\n      \"blocked\": true,\n      \"probabilityScore\": 0.95422274,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severityScore\": 0.43398145\n    },\n    {\n      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probabilityScore\": 0.11085559,\n      \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n      \"severityScore\": 0.19027223\n    },\n    {\n      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probabilityScore\": 0.22901751,\n      \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n      \"severityScore\": 0.09089675\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Sources in streamText Function\nDESCRIPTION: This snippet demonstrates how to access sources when using the streamText function. It uses a for-await loop to iterate through the fullStream and log source information as it becomes available.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_13\n\nLANGUAGE: tsx\nCODE:\n```\nconst result = streamText({\n  model: google('gemini-2.0-flash-exp', { useSearchGrounding: true }),\n  prompt: 'List the top 5 San Francisco news from the past week.',\n});\n\nfor await (const part of result.fullStream) {\n  if (part.type === 'source' && part.source.sourceType === 'url') {\n    console.log('ID:', part.source.id);\n    console.log('Title:', part.source.title);\n    console.log('URL:', part.source.url);\n    console.log('Provider metadata:', part.source.providerMetadata);\n    console.log();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Text Using vertexAnthropic and generateText (TypeScript)\nDESCRIPTION: This snippet imports the vertexAnthropic provider and generateText function to interact with an Anthropic language model hosted on Google Vertex. The generateText function is called with a model instance (claude-3-haiku-20240307) and a prompt string, and the resulting object provides the generated text. Key parameters include the model identifier and the text prompt. Dependencies are '@ai-sdk/google-vertex/anthropic' and 'ai'.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_25\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: vertexAnthropic('claude-3-haiku-20240307'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Body Fields Per Request\nDESCRIPTION: Shows how to set custom body fields for individual requests using the handleSubmit function in a React component.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_10\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form\n        onSubmit={event => {\n          handleSubmit(event, {\n            body: {\n              customKey: 'customValue',\n            },\n          });\n        }}\n      >\n        <input value={input} onChange={handleInputChange} />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Google Vertex Provider Instance\nDESCRIPTION: Code to create a customized Google Vertex provider instance with specific project and location settings. This allows for configuring the provider with custom project ID and location for API calls.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createVertex } from '@ai-sdk/google-vertex';\n\nconst vertex = createVertex({\n  project: 'my-project', // optional\n  location: 'us-central1', // optional\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Schema for SQL Query Explanations in TypeScript\nDESCRIPTION: Creates a Zod schema for SQL query explanations, defining the structure of each explanation section.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from 'zod';\n\n/* ...rest of the file... */\n\nexport const explanationSchema = z.object({\n  section: z.string(),\n  explanation: z.string(),\n});\n\nexport type QueryExplanation = z.infer<typeof explanationSchema>;\n```\n\n----------------------------------------\n\nTITLE: Recording Token Usage with Usage Promise in TypeScript\nDESCRIPTION: Demonstrates how to access and handle token usage information using the usage promise returned by streamObject. Includes examples of both promise-based and async/await approaches, along with proper stream consumption.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/45-stream-object-record-token-usage.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject, TokenUsage } from 'ai';\nimport { z } from 'zod';\n\nconst result = streamObject({\n  model: openai('gpt-4-turbo'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.string()),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n\n// your custom function to record token usage:\nfunction recordTokenUsage({\n  promptTokens,\n  completionTokens,\n  totalTokens,\n}: TokenUsage) {\n  console.log('Prompt tokens:', promptTokens);\n  console.log('Completion tokens:', completionTokens);\n  console.log('Total tokens:', totalTokens);\n}\n\n// use as promise:\nresult.usage.then(recordTokenUsage);\n\n// use with async/await:\nrecordTokenUsage(await result.usage);\n\n// note: the stream needs to be consumed because of backpressure\nfor await (const partialObject of result.partialObjectStream) {\n}\n```\n\n----------------------------------------\n\nTITLE: Installing and Building AI SDK Project\nDESCRIPTION: Commands for installing dependencies and building the AI SDK project. These steps should be executed from the root directory of the AI SDK repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/fastify/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npnpm install\npnpm build\n```\n\n----------------------------------------\n\nTITLE: Creating a Qwen Embedding Model Instance - TypeScript\nDESCRIPTION: This snippet demonstrates how to instantiate a Qwen embedding model using the '.textEmbeddingModel' factory method. The method takes a model identifier and returns a model object suited for generating embeddings. Requires the 'qwen' provider instance to be available.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/02-qwen.mdx#2025-04-23_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = qwen.textEmbeddingModel('text-embedding-v3');\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Assistant Message in TypeScript\nDESCRIPTION: This snippet demonstrates how to generate text using an AI model with a simple assistant message containing text content.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  messages: [\n    { role: 'user', content: 'Hi!' },\n    { role: 'assistant', content: 'Hello, how can I help?' },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_LoadSettingError Instance in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to check if a caught error variable (`error`) is an instance of the `LoadSettingError` class imported from the 'ai' library. It uses the static `isInstance` method provided by the `LoadSettingError` class for type guarding, allowing specific handling for this error type within the `if` block.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-load-setting-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LoadSettingError } from 'ai';\n\nif (LoadSettingError.isInstance(error)) {\n  // Handle the error\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Cosine Similarity Between Embeddings in TypeScript\nDESCRIPTION: This snippet demonstrates how to calculate the cosine similarity between two embeddings using the 'cosineSimilarity' function from the AI SDK. It first embeds multiple values and then compares the similarity between the first two embeddings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/30-embeddings.mdx#2025-04-23_snippet_2\n\nLANGUAGE: ts\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { cosineSimilarity, embedMany } from 'ai';\n\nconst { embeddings } = await embedMany({\n  model: openai.embedding('text-embedding-3-small'),\n  values: ['sunny day at the beach', 'rainy afternoon in the city'],\n});\n\nconsole.log(\n  `cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Web Search with Query-Specific Metadata\nDESCRIPTION: Demonstrates how to enhance web search results by providing query-specific metadata like search context size and user location to improve relevance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/19-openai-responses.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: openai.responses('gpt-4o-mini'),\n  prompt: 'What happened in San Francisco last week?',\n  tools: {\n    web_search_preview: openai.tools.webSearchPreview({\n      searchContextSize: 'high',\n      userLocation: {\n        type: 'approximate',\n        city: 'San Francisco',\n        region: 'California',\n      },\n    }),\n  },\n});\n\nconsole.log(result.text);\nconsole.log(result.sources);\n```\n\n----------------------------------------\n\nTITLE: Handling Transcription Warnings\nDESCRIPTION: Demonstrates how to access warnings from the transcription process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/36-transcription.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { readFile } from 'fs/promises';\n\nconst transcript = await transcribe({\n  model: openai.transcription('whisper-1'),\n  audio: await readFile('audio.mp3'),\n});\n\nconst warnings = transcript.warnings;\n```\n\n----------------------------------------\n\nTITLE: Installing Replicate Integration via yarn (Shell)\nDESCRIPTION: Provides the installation command using yarn to add ai and @ai-sdk/replicate dependencies. Before proceeding with image generation code, ensure these two packages are present in your project using this command.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/60-replicate.mdx#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nyarn add ai @ai-sdk/replicate\n```\n\n----------------------------------------\n\nTITLE: Structured Message Format with Memory - TypeScript\nDESCRIPTION: Shows how to send structured messages (multi-part, with types) when generating text using Mem0-enhanced models. Needs 'ai' and '@mem0/vercel-ai-provider'. Inputs are an array of message objects, which can have multiple text parts; these provide richer context for the AI. Output is the generated text from the AI model. Useful for advanced conversational formats.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/70-mem0.mdx#2025-04-23_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { createMem0 } from '@mem0/vercel-ai-provider';\n\nconst mem0 = createMem0();\n\nconst { text } = await generateText({\n  model: mem0('gpt-4-turbo', { user_id: 'borat' }),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'Suggest me a good car to buy.' },\n        { type: 'text', text: 'Why is it better than the other cars for me?' },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Importing the Together.ai Provider Instance (TypeScript)\nDESCRIPTION: Demonstrates how to import the default `togetherai` provider instance from the `@ai-sdk/togetherai` package in a TypeScript project. This instance acts as a factory function to configure specific Together.ai models for use with the Vercel AI SDK functions.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/togetherai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { togetherai } from '@ai-sdk/togetherai';\n```\n\n----------------------------------------\n\nTITLE: Creating a Redis-based Caching Middleware for Language Models\nDESCRIPTION: A TypeScript implementation of a LanguageModelV1Middleware that caches AI responses in Redis. It handles both direct generation and streaming, properly formatting timestamps and simulating streams for cached responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/122-caching-middleware.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { Redis } from '@upstash/redis';\nimport {\n  type LanguageModelV1,\n  type LanguageModelV1Middleware,\n  type LanguageModelV1StreamPart,\n  simulateReadableStream,\n} from 'ai';\n\nconst redis = new Redis({\n  url: process.env.KV_URL,\n  token: process.env.KV_TOKEN,\n});\n\nexport const cacheMiddleware: LanguageModelV1Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    const cacheKey = JSON.stringify(params);\n\n    const cached = (await redis.get(cacheKey)) as Awaited<\n      ReturnType<LanguageModelV1['doGenerate']>\n    > | null;\n\n    if (cached !== null) {\n      return {\n        ...cached,\n        response: {\n          ...cached.response,\n          timestamp: cached?.response?.timestamp\n            ? new Date(cached?.response?.timestamp)\n            : undefined,\n        },\n      };\n    }\n\n    const result = await doGenerate();\n\n    redis.set(cacheKey, result);\n\n    return result;\n  },\n  wrapStream: async ({ doStream, params }) => {\n    const cacheKey = JSON.stringify(params);\n\n    // Check if the result is in the cache\n    const cached = await redis.get(cacheKey);\n\n    // If cached, return a simulated ReadableStream that yields the cached result\n    if (cached !== null) {\n      // Format the timestamps in the cached response\n      const formattedChunks = (cached as LanguageModelV1StreamPart[]).map(p => {\n        if (p.type === 'response-metadata' && p.timestamp) {\n          return { ...p, timestamp: new Date(p.timestamp) };\n        } else return p;\n      });\n      return {\n        stream: simulateReadableStream({\n          initialDelayInMs: 0,\n          chunkDelayInMs: 10,\n          chunks: formattedChunks,\n        }),\n        rawCall: { rawPrompt: null, rawSettings: {} },\n      };\n    }\n\n    // If not cached, proceed with streaming\n    const { stream, ...rest } = await doStream();\n\n    const fullResponse: LanguageModelV1StreamPart[] = [];\n\n    const transformStream = new TransformStream<\n      LanguageModelV1StreamPart,\n      LanguageModelV1StreamPart\n    >({\n      transform(chunk, controller) {\n        fullResponse.push(chunk);\n        controller.enqueue(chunk);\n      },\n      flush() {\n        // Store the full response in the cache after streaming is complete\n        redis.set(cacheKey, fullResponse);\n      },\n    });\n\n    return {\n      stream: stream.pipeThrough(transformStream),\n      ...rest,\n    };\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing a Chat Interface using AI SDK UI in Next.js\nDESCRIPTION: Example of creating a chat interface using AI SDK UI in a Next.js application. It demonstrates the usage of the useChat hook to manage chat state and render messages.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/README.md#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, handleSubmit, handleInputChange, status } =\n    useChat();\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <strong>{`${message.role}: `}</strong>\n          {message.parts.map((part, index) => {\n            switch (part.type) {\n              case 'text':\n                return <span key={index}>{part.text}</span>;\n\n              // other cases can handle images, tool calls, etc\n            }\n          })}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          placeholder=\"Send a message...\"\n          onChange={handleInputChange}\n          disabled={status !== 'ready'}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Amazon Bedrock Provider\nDESCRIPTION: Example of creating a custom Amazon Bedrock provider instance with explicit credential configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAmazonBedrock } from '@ai-sdk/amazon-bedrock';\n\nconst bedrock = createAmazonBedrock({\n  region: 'us-east-1',\n  accessKeyId: 'xxxxxxxxx',\n  secretAccessKey: 'xxxxxxxxx',\n  sessionToken: 'xxxxxxxxx',\n});\n```\n\n----------------------------------------\n\nTITLE: Running the Svelte Development Server\nDESCRIPTION: Command to start the Svelte development server to run the AI chatbot application locally.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Image File Buffer - TypeScript\nDESCRIPTION: This example demonstrates how to generate text using GPT-4 Turbo model with an image provided as a file buffer. The code reads a local image file and uses the AI SDK to analyze it and answer questions about it.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/12-generate-text-with-image-prompt.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport fs from 'fs';\n\nconst result = await generateText({\n  model: openai('gpt-4-turbo'),\n  maxTokens: 512,\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'what are the red things in this image?',\n        },\n        {\n          type: 'image',\n          image: fs.readFileSync('./node/attachments/eclipse.jpg'),\n        },\n      ],\n    },\n  ],\n});\n\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_NoTranscriptGeneratedError in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates importing `NoTranscriptGeneratedError` from the 'ai' package and using its static `isInstance` method to verify if a caught error variable is of this specific type. This pattern allows for targeted error handling logic, typically within a try-catch block.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-no-transcript-generated-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nimport { NoTranscriptGeneratedError } from 'ai';\n\nif (NoTranscriptGeneratedError.isInstance(error)) {\n  // Handle the error\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Importing the Default Google Vertex Anthropic Provider in TypeScript\nDESCRIPTION: Shows how to import the default instance of the Google Vertex Anthropic provider, `vertexAnthropic`, from the `@ai-sdk/google-vertex/anthropic` package for use within the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic';\n```\n\n----------------------------------------\n\nTITLE: Adding Labels to Spans in Laminar\nDESCRIPTION: Add pre-defined labels to spans for easier filtering in Laminar.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/laminar.mdx#2025-04-23_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { withLabels } from '@lmnr-ai/lmnr';\n\nwithLabels({ myLabel: 'someValue' }, async () => {\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Google Generative AI Model with Safety Settings\nDESCRIPTION: Configures a Google Generative AI model with custom safety settings to control content filtering thresholds for specific harm categories.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = google('gemini-1.5-pro-latest', {\n  safetySettings: [\n    { category: 'HARM_CATEGORY_UNSPECIFIED', threshold: 'BLOCK_LOW_AND_ABOVE' },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Completion Cancellation\nDESCRIPTION: Implementation of a stop button to cancel ongoing completion requests using the stop function from useCompletion.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/05-completion.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\nconst { stop, isLoading, ... } = useCompletion()\n\nreturn (\n  <>\n    <button onClick={stop} disabled={!isLoading}>Stop</button>\n  </>\n)\n```\n\n----------------------------------------\n\nTITLE: Streaming Text with AI SDK using Stream Reader\nDESCRIPTION: This snippet shows an alternative approach to stream text using the reader interface from the AI SDK. It creates a stream reader from the text stream and manually processes chunks of text using a while loop with the reader's read() method, continuing until the stream is complete.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/20-stream-text.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst result = streamText({\n  model: openai('gpt-3.5-turbo'),\n  maxTokens: 512,\n  temperature: 0.3,\n  maxRetries: 5,\n  prompt: 'Invent a new holiday and describe its traditions.'\n});\n\nconst reader = result.textStream.getReader();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) {\n    break;\n  }\n  console.log(value);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Eager Stream Generation in JavaScript\nDESCRIPTION: This snippet demonstrates an eager approach to stream generation using an async generator function. It shows how to create a ReadableStream that continuously yields integers, highlighting potential issues with back-pressure and cancellation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/03-backpressure.mdx#2025-04-23_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n// A generator that will yield positive integers\nasync function* integers() {\n  let i = 1;\n  while (true) {\n    console.log(`yielding ${i}`);\n    yield i++;\n\n    await sleep(100);\n  }\n}\nfunction sleep(ms) {\n  return new Promise(resolve => setTimeout(resolve, ms));\n}\n\n// Wraps a generator into a ReadableStream\nfunction createStream(iterator) {\n  return new ReadableStream({\n    async start(controller) {\n      for await (const v of iterator) {\n        controller.enqueue(v);\n      }\n      controller.close();\n    },\n  });\n}\n\n// Collect data from stream\nasync function run() {\n  // Set up a stream of integers\n  const stream = createStream(integers());\n\n  // Read values from our stream\n  const reader = stream.getReader();\n  for (let i = 0; i < 10_000; i++) {\n    // we know our stream is infinite, so there's no need to check `done`.\n    const { value } = await reader.read();\n    console.log(`read ${value}`);\n\n    await sleep(1_000);\n  }\n}\nrun();\n```\n\n----------------------------------------\n\nTITLE: Passing Model-Specific Options to Replicate Models (TypeScript)\nDESCRIPTION: Demonstrates advanced usage by supplying model-specific options in the providerOptions.replicate object. In this example, the style parameter customizes the generated image's appearance. This allows fine-grained control for models that support extra options. The 'size' and provider-specific properties must be verified against the Replicate model's documentation for compatibility.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/60-replicate.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { replicate } from '@ai-sdk/replicate';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: replicate.image('recraft-ai/recraft-v3'),\n  prompt: 'The Loch Ness Monster getting a manicure',\n  size: '1365x1024',\n  providerOptions: {\n    replicate: {\n      style: 'realistic_image',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Custom Headers Configuration\nDESCRIPTION: Shows how to add custom headers to the speech generation request.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/37-speech.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\nimport { readFile } from 'fs/promises';\n\nconst audio = await generateSpeech({\n  model: openai.speech('tts-1'),\n  text: 'Hello, world!',\n  headers: { 'X-Custom-Header': 'custom-value' },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI Responses Model with Provider Options\nDESCRIPTION: Illustrates using the `generateText` function with an Azure OpenAI responses model, configuring provider-specific options like `parallelToolCalls`, `store`, and `user` via the `providerOptions.openai` field. It also shows using the `OpenAIResponsesProviderOptions` type for validation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { azure, OpenAIResponsesProviderOptions } from '@ai-sdk/azure';\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: azure.responses('your-deployment-name'),\n  providerOptions: {\n    openai: {\n      parallelToolCalls: false,\n      store: false,\n      user: 'user_123',\n      // ...\n    } satisfies OpenAIResponsesProviderOptions,\n  },\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Abort Signal Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `abortSignal` parameter as an `AbortSignal`. This allows for canceling the ongoing language model call.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nAbortSignal\n```\n\n----------------------------------------\n\nTITLE: simulateReadableStream without Delays\nDESCRIPTION: Shows how to create a stream that emits chunks immediately without any delays between emissions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/75-simulate-readable-stream.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst stream = simulateReadableStream({\n  chunks: ['Hello', ' ', 'World'],\n  initialDelayInMs: null, // No initial delay\n  chunkDelayInMs: null, // No delay between chunks\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Batch Embeddings with LM Studio and AI SDK in TSX\nDESCRIPTION: Illustrates how to generate embeddings for multiple text values simultaneously (batch embedding) using an LM Studio model. It imports `createOpenAICompatible` (note: original uses `@ai-sdk/openai`, potentially a typo) and `embedMany`, initializes the LM Studio provider, and calls the `embedMany` function from the 'ai' package. It provides the embedding model via `lmstudio.textEmbeddingModel()` and an array of strings in the `values` parameter. The output is an array of embedding vectors (`number[][]`), maintaining the order of the input values.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/30-lmstudio.mdx#2025-04-23_snippet_8\n\nLANGUAGE: tsx\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai';\nimport { embedMany } from 'ai';\n\nconst lmstudio = createOpenAICompatible({\n  name: 'lmstudio',\n  baseURL: 'https://localhost:1234/v1',\n});\n\n// 'embeddings' is an array of embedding objects (number[][]).\n// It is sorted in the same order as the input values.\nconst { embeddings } = await embedMany({\n  model: lmstudio.textEmbeddingModel('text-embedding-nomic-embed-text-v1.5'),\n  values: [\n    'sunny day at the beach',\n    'rainy afternoon in the city',\n    'snowy night in the mountains',\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing a Chat API Route using AI SDK Core in Next.js\nDESCRIPTION: Example of creating an API route for chat functionality using AI SDK Core in a Next.js application. It demonstrates how to stream text responses from OpenAI's GPT-4 model.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/README.md#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'You are a helpful assistant.',\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Installing workers-ai-provider with npm - Bash\nDESCRIPTION: This snippet provides the npm command to install the workers-ai-provider package, facilitating integration with Cloudflare Workers AI. Requires npm pre-installed. Execution installs the necessary node module for your project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/11-cloudflare-workers-ai.mdx#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install workers-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Configuring AI Instance for Chat Application\nDESCRIPTION: This code sets up the AI instance for the chat application, defining the initial state and actions. It uses the createAI function from the AI SDK to configure the AI behavior.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/90-render-visual-interface-in-chat.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAI } from 'ai/rsc';\nimport { ServerMessage, ClientMessage, continueConversation } from './actions';\n\nexport const AI = createAI<ServerMessage[], ClientMessage[]>({\n  actions: {\n    continueConversation,\n  },\n  initialAIState: [],\n  initialUIState: [],\n});\n```\n\n----------------------------------------\n\nTITLE: Checking for TooManyEmbeddingValuesForCallError Instance in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to check if a caught error object is an instance of `TooManyEmbeddingValuesForCallError` using its static `isInstance` method. This is useful for specific error handling logic when making embedding calls with the Vercel AI SDK. It requires importing the `TooManyEmbeddingValuesForCallError` class from the 'ai' package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-too-many-embedding-values-for-call-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TooManyEmbeddingValuesForCallError } from 'ai';\n\nif (TooManyEmbeddingValuesForCallError.isInstance(error)) {\n  // Handle the error\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Speech with Custom Voice\nDESCRIPTION: Demonstrates how to use the generateSpeech function with a Hume speech model, specifying a custom voice and provider options.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/150-hume.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\nimport { hume } from '@ai-sdk/hume';\n\nconst result = await generateSpeech({\n  model: hume.speech(),\n  text: 'Hello, world!',\n  voice: 'd8ab67c6-953d-4bd8-9370-8fa53a0f1453',\n  providerOptions: { hume: {} },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating AI Context in Svelte Layout\nDESCRIPTION: Demonstrates how to use createAIContext to enable state synchronization between AI hook instances in a Svelte application. This setup allows multiple instances of hooks with the same ID to share the same messages and status state.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_12\n\nLANGUAGE: svelte\nCODE:\n```\n<script>\n  import { createAIContext } from '@ai-sdk/svelte';\n\n  let { children } = $props();\n\n  createAIContext();\n  // all hooks created after this or in components that are children of this component\n  // will have synchronized state\n</script>\n\n{@render children()}\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Generation with ChromeAI - JavaScript\nDESCRIPTION: Demonstrates the use of the 'streamText' function from the SDK in combination with ChromeAI to perform streaming text generation. Requires the 'ai' SDK, 'chrome-ai' provider, and async iteration. The prompt is sent, and generated text is read in parts from an async iterator. Suitable for applications requiring immediate feedback of model outputs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/04-chrome-ai.mdx#2025-04-23_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { streamText } from 'ai';\\nimport { chromeai } from 'chrome-ai';\\n\\nconst { textStream } = streamText({\\n  model: chromeai(),\\n  prompt: 'Who are you?',\\n});\\n\\nlet result = '';\\nfor await (const textPart of textStream) {\\n  result = textPart;\\n}\\n\\nconsole.log(result);\\n//  I am a large language model, trained by Google.\n```\n\n----------------------------------------\n\nTITLE: Updating useAssistant Import\nDESCRIPTION: Shows the migration from experimental_useAssistant to useAssistant direct import.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_41\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_useAssistant } from '@ai-sdk/react';\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { useAssistant } from '@ai-sdk/react';\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for AI SDK with Fastify\nDESCRIPTION: Sets up the necessary environment variables for using the AI SDK, particularly the OpenAI API key. Additional settings may be required depending on the providers being used.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/fastify/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Importing the Default OpenAI Provider Instance in TypeScript\nDESCRIPTION: Demonstrates importing the default, pre-configured `openai` provider instance from the `@ai-sdk/openai` package in a TypeScript application. This instance can be used directly with AI SDK functions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\n```\n\n----------------------------------------\n\nTITLE: Advanced Image Generation with Additional Options\nDESCRIPTION: Example showing how to use additional provider-specific options when generating images, including style preferences and image size specifications\nSOURCE: https://github.com/vercel/ai/blob/main/packages/fal/README.md#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst { image } = await generateImage({\n  model: fal.image('fal-ai/recraft-v3'),\n  prompt: 'A cat wearing a intricate robe',\n  size: '1920x1080',\n  providerOptions: {\n    fal: {\n      style: 'digital_illustration',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Server-Side Tool Execution with Vercel AI Toolkit (TypeScript/React)\nDESCRIPTION: Demonstrates how to define a tool with an 'execute' function that handles server-side operations for tool calls within the Vercel AI toolkit. Dependencies include the 'tool' helper (presumably from Vercel AI) and Zod for type-safe parameter schemas. The 'weather' tool fetches weather data for a specified location and returns structured data to the conversation model; required parameters must be validated and provided. The function must return a result for conversation state consistency. Inputs: object with 'location' string parameter; output: weather data object.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/05-tool-invocation-missing-result.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst tools = {\n  weather: tool({\n    description: 'Get the weather in a location',\n    parameters: z.object({\n      location: z\n        .string()\n        .describe('The city and state, e.g. \"San Francisco, CA\"'),\n    }),\n    execute: async ({ location }) => {\n      // Fetch and return weather data\n      return { temperature: 72, conditions: 'sunny', location };\n    },\n  }),\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI Provider Options in generateText\nDESCRIPTION: Illustrates how to pass provider-specific options for OpenAI, such as image detail level within messages or reasoning effort level for the overall call, using the `providerOptions.openai` property within the `generateText` function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst messages = [\n  {\n    role: 'user',\n    content: [\n      {\n        type: 'text',\n        text: 'What is the capital of the moon?',\n      },\n      {\n        type: 'image',\n        image: 'https://example.com/image.png',\n        providerOptions: {\n          openai: { imageDetail: 'low' },\n        },\n      },\n    ],\n  },\n];\n\nconst { text } = await generateText({\n  model: azure('your-deployment-name'),\n  providerOptions: {\n    openai: {\n      reasoningEffort: 'low',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming plain text with Fastify and AI SDK\nDESCRIPTION: A simple implementation that uses the textStream property to stream AI-generated text directly to the client. This approach is more straightforward than the data stream methods but provides less flexibility.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/40-fastify.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport Fastify from 'fastify';\n\nconst fastify = Fastify({ logger: true });\n\nfastify.post('/', async function (request, reply) {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    prompt: 'Invent a new holiday and describe its traditions.',\n  });\n\n  reply.header('Content-Type', 'text/plain; charset=utf-8');\n\n  return reply.send(result.textStream);\n});\n\nfastify.listen({ port: 8080 });\n```\n\n----------------------------------------\n\nTITLE: Rendering Example Links with React JSX Component\nDESCRIPTION: This snippet utilizes a custom React component `<ExampleLinks />` to render a list of links. An array of objects, where each object contains a `title` (string describing the example) and a `link` (string representing the URL path), is passed to the `examples` prop. This component dynamically generates links based on the provided data, showcasing various Vercel AI SDK functionalities within Next.js applications like UI state management, routing, and streaming.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/01-stream-ui.mdx#2025-04-23_snippet_5\n\nLANGUAGE: jsx\nCODE:\n```\n<ExampleLinks\n  examples={[\n    {\n      title:\n        'Learn to render a React component as a function call using a language model in Next.js',\n      link: '/examples/next-app/state-management/ai-ui-states',\n    },\n    {\n      title: 'Learn to persist and restore states UI/AI states in Next.js',\n      link: '/examples/next-app/state-management/save-and-restore-states',\n    },\n    {\n      title:\n        'Learn to route React components using a language model in Next.js',\n      link: '/examples/next-app/interface/route-components',\n    },\n    {\n      title: 'Learn to stream component updates to the client in Next.js',\n      link: '/examples/next-app/interface/stream-component-updates',\n    },\n  ]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Implementing Cache Middleware for AI SDK\nDESCRIPTION: Implementation of language model middleware for caching both streaming and non-streaming responses using Redis. The middleware handles caching for both generate and stream operations with proper timestamp handling and stream simulation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/04-caching.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Redis } from '@upstash/redis';\nimport {\n  type LanguageModelV1,\n  type LanguageModelV1Middleware,\n  type LanguageModelV1StreamPart,\n  simulateReadableStream,\n} from 'ai';\n\nconst redis = new Redis({\n  url: process.env.KV_URL,\n  token: process.env.KV_TOKEN,\n});\n\nexport const cacheMiddleware: LanguageModelV1Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    const cacheKey = JSON.stringify(params);\n\n    const cached = (await redis.get(cacheKey)) as Awaited<\n      ReturnType<LanguageModelV1['doGenerate']>\n    > | null;\n\n    if (cached !== null) {\n      return {\n        ...cached,\n        response: {\n          ...cached.response,\n          timestamp: cached?.response?.timestamp\n            ? new Date(cached?.response?.timestamp)\n            : undefined,\n        },\n      };\n    }\n\n    const result = await doGenerate();\n\n    redis.set(cacheKey, result);\n\n    return result;\n  },\n  wrapStream: async ({ doStream, params }) => {\n    const cacheKey = JSON.stringify(params);\n\n    // Check if the result is in the cache\n    const cached = await redis.get(cacheKey);\n\n    // If cached, return a simulated ReadableStream that yields the cached result\n    if (cached !== null) {\n      // Format the timestamps in the cached response\n      const formattedChunks = (cached as LanguageModelV1StreamPart[]).map(p => {\n        if (p.type === 'response-metadata' && p.timestamp) {\n          return { ...p, timestamp: new Date(p.timestamp) };\n        } else return p;\n      });\n      return {\n        stream: simulateReadableStream({\n          initialDelayInMs: 0,\n          chunkDelayInMs: 10,\n          chunks: formattedChunks,\n        }),\n        rawCall: { rawPrompt: null, rawSettings: {} },\n      };\n    }\n\n    // If not cached, proceed with streaming\n    const { stream, ...rest } = await doStream();\n\n    const fullResponse: LanguageModelV1StreamPart[] = [];\n\n    const transformStream = new TransformStream<\n      LanguageModelV1StreamPart,\n      LanguageModelV1StreamPart\n    >({\n      transform(chunk, controller) {\n        fullResponse.push(chunk);\n        controller.enqueue(chunk);\n      },\n      flush() {\n        // Store the full response in the cache after streaming is complete\n        redis.set(cacheKey, fullResponse);\n      },\n    });\n\n    return {\n      stream: stream.pipeThrough(transformStream),\n      ...rest,\n    };\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Importing Deepgram Provider in TypeScript\nDESCRIPTION: Import the default Deepgram provider instance from the @ai-sdk/deepgram package to use with the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/110-deepgram.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepgram } from '@ai-sdk/deepgram';\n```\n\n----------------------------------------\n\nTITLE: Creating a Chat API Endpoint with Image Generation Tool in Next.js\nDESCRIPTION: This server-side code creates an API endpoint at '/api/chat' that processes chat messages and provides a tool for generating images using OpenAI's DALL-E 3 model. The code also filters out base64 image data from previous messages to avoid sending large payloads to the model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/12-generate-image-with-chat-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_generateImage, Message, streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nexport async function POST(request: Request) {\n  const { messages }: { messages: Message[] } = await request.json();\n\n  // filter through messages and remove base64 image data to avoid sending to the model\n  const formattedMessages = messages.map(m => {\n    if (m.role === 'assistant' && m.toolInvocations) {\n      m.toolInvocations.forEach(ti => {\n        if (ti.toolName === 'generateImage' && ti.state === 'result') {\n          ti.result.image = `redacted-for-length`;\n        }\n      });\n    }\n    return m;\n  });\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: formattedMessages,\n    tools: {\n      generateImage: tool({\n        description: 'Generate an image',\n        parameters: z.object({\n          prompt: z.string().describe('The prompt to generate the image from'),\n        }),\n        execute: async ({ prompt }) => {\n          const { image } = await experimental_generateImage({\n            model: openai.image('dall-e-3'),\n            prompt,\n          });\n          // in production, save this image to blob storage and return a URL\n          return { image: image.base64, prompt };\n        },\n      }),\n    },\n  });\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Using Cerebras Model with AI SDK in TypeScript\nDESCRIPTION: Example of using a Cerebras language model with the generateText function from the AI SDK to create text based on a prompt.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/40-cerebras.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { cerebras } from '@ai-sdk/cerebras';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: cerebras('llama3.1-8b'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Installing the Perplexity Provider Package\nDESCRIPTION: Command to install the Perplexity provider package for the AI SDK using npm. This is required before you can use the Perplexity provider in your application.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/perplexity/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/perplexity\n```\n\n----------------------------------------\n\nTITLE: Using FriendliAI Dedicated via OpenAI Compatibility\nDESCRIPTION: Illustrates how to use the OpenAI-compatible FriendliAI Dedicated API by configuring the `@ai-sdk/openai` provider. It sets the `baseURL` to the FriendliAI dedicated endpoint and uses the `FRIENDLI_TOKEN` as the `apiKey`. Dependencies: `@ai-sdk/openai`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst friendli = createOpenAI({\n  baseURL: 'https://api.friendli.ai/dedicated/v1',\n  apiKey: process.env.FRIENDLI_TOKEN,\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing Embedding Model Settings - TypeScript\nDESCRIPTION: This TypeScript snippet shows how to instantiate a Mixedbread embedding model with customized settings using the textEmbeddingModel method. Options like prompt (up to 256 characters) and dimensions (up to 1024 for specified models) can be set to tailor the embedding request. This is useful when specific embedding configurations or model tuning are necessary.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/60-mixedbread.mdx#2025-04-23_snippet_4\n\nLANGUAGE: ts\nCODE:\n```\nimport { mixedbread } from 'mixedbread-ai-provider';\n\nconst embeddingModel = mixedbread.textEmbeddingModel(\n  'mixedbread-ai/mxbai-embed-large-v1',\n  {\n    prompt: 'Generate embeddings for text', // Max 256 characters\n    dimensions: 512, // Max 1024 for embed-large-v1\n  },\n);\n```\n\n----------------------------------------\n\nTITLE: Using Text Embedding Models from Provider Registry\nDESCRIPTION: Demonstrates how to generate text embeddings using an embedding model accessed through the registry using the provider:model ID format.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/40-provider-registry.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { embed } from 'ai';\nimport { registry } from './registry';\n\nconst { embedding } = await embed({\n  model: registry.textEmbeddingModel('openai:text-embedding-3-small'),\n  value: 'sunny day at the beach',\n});\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Compatible SDK via pnpm\nDESCRIPTION: Installs the `@ai-sdk/openai-compatible` package using the pnpm package manager. This package is required to interact with LM Studio's OpenAI-compatible API via the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/30-lmstudio.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/openai-compatible\n```\n\n----------------------------------------\n\nTITLE: Installing Luma AI SDK Module via yarn (Shell)\nDESCRIPTION: Installs the @ai-sdk/luma package with yarn. This command sets up all dependencies for Luma AI's image functionality within a yarn-managed project. Yarn must be pre-installed.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/80-luma.mdx#2025-04-23_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nyarn add @ai-sdk/luma\n```\n\n----------------------------------------\n\nTITLE: Defining ImagePart Interface in TypeScript\nDESCRIPTION: Defines the structure for an image part in a user message. It includes a type of 'image', image data (which can be various formats), and an optional mime type.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/30-core-message.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nexport interface ImagePart {\n  type: 'image';\n\n  /**\n   * Image data. Can either be:\n   * - data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer\n   * - URL: a URL that points to the image\n   */\n  image: DataContent | URL;\n\n  /**\n   * Optional mime type of the image.\n   * We recommend leaving this out as it will be detected automatically.\n   */\n  mimeType?: string;\n}\n```\n\n----------------------------------------\n\nTITLE: Using useObject Hook for Simplified Stream Handling (After)\nDESCRIPTION: This refactored client component uses the useObject hook from @ai-sdk/react to handle stream decoding and state updates. It simplifies the implementation by removing the need for manual stream handling and state management, providing a more concise and maintainable solution.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_15\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useObject } from '@ai-sdk/react';\nimport { notificationSchema } from '@/utils/schemas';\n\nexport default function Page() {\n  const { object, submit } = useObject({\n    api: '/api/object',\n    schema: notificationSchema,\n  });\n\n  return (\n    <div>\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Assistant Thread Messages in Slack with TypeScript\nDESCRIPTION: This function handles the 'assistant_thread_started' event in Slack. It posts a welcome message to the thread and sets up suggested prompts to help users get started with the AI assistant.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/03-slackbot.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { AssistantThreadStartedEvent } from '@slack/web-api';\nimport { client } from './slack-utils';\n\nexport async function assistantThreadMessage(\n  event: AssistantThreadStartedEvent,\n) {\n  const { channel_id, thread_ts } = event.assistant_thread;\n  console.log(`Thread started: ${channel_id} ${thread_ts}`);\n  console.log(JSON.stringify(event));\n\n  await client.chat.postMessage({\n    channel: channel_id,\n    thread_ts: thread_ts,\n    text: \"Hello, I'm an AI assistant built with the AI SDK by Vercel!\",\n  });\n\n  await client.assistant.threads.setSuggestedPrompts({\n    channel_id: channel_id,\n    thread_ts: thread_ts,\n    prompts: [\n      {\n        title: 'Get the weather',\n        message: 'What is the current weather in London?',\n      },\n      {\n        title: 'Get the news',\n        message: 'What is the latest Premier League news from the BBC?',\n      },\n    ],\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Next.js App with npx for AI Chat Example\nDESCRIPTION: This command uses create-next-app to bootstrap a new Next.js application with the AI chat example using npx.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-langchain/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-langchain next-langchain-app\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI Embedding Models\nDESCRIPTION: Shows how to initialize and configure embedding models with optional parameters for dimensions and user identification.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = azure.embedding('your-embedding-deployment');\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = azure.embedding('your-embedding-deployment', {\n  dimensions: 512 // optional, number of dimensions for the embedding\n  user: 'test-user' // optional unique user identifier\n})\n```\n\n----------------------------------------\n\nTITLE: Importing useChat in Solid\nDESCRIPTION: Shows how to import the useChat hook from the AI SDK in a Solid application\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#2025-04-23_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { useChat } from '@ai-sdk/solid'\n```\n\n----------------------------------------\n\nTITLE: Implementing Stop Word Transformation with Stream Control\nDESCRIPTION: This snippet shows how to implement a custom transformation that stops the stream when a specific word is encountered. It demonstrates the use of stopStream function and simulates step-finish and finish events.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst stopWordTransform =\n  <TOOLS extends ToolSet>() =>\n  ({ stopStream }: { stopStream: () => void }) =>\n    new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({\n      // note: this is a simplified transformation for testing;\n      // in a real-world version more there would need to be\n      // stream buffering and scanning to correctly emit prior text\n      // and to detect all STOP occurrences.\n      transform(chunk, controller) {\n        if (chunk.type !== 'text-delta') {\n          controller.enqueue(chunk);\n          return;\n        }\n\n        if (chunk.textDelta.includes('STOP')) {\n          // stop the stream\n          stopStream();\n\n          // simulate the step-finish event\n          controller.enqueue({\n            type: 'step-finish',\n            finishReason: 'stop',\n            logprobs: undefined,\n            usage: {\n              completionTokens: NaN,\n              promptTokens: NaN,\n              totalTokens: NaN,\n            },\n            request: {},\n            response: {\n              id: 'response-id',\n              modelId: 'mock-model-id',\n              timestamp: new Date(0),\n            },\n            warnings: [],\n            isContinued: false,\n          });\n\n          // simulate the finish event\n          controller.enqueue({\n            type: 'finish',\n            finishReason: 'stop',\n            logprobs: undefined,\n            usage: {\n              completionTokens: NaN,\n              promptTokens: NaN,\n              totalTokens: NaN,\n            },\n            response: {\n              id: 'response-id',\n              modelId: 'mock-model-id',\n              timestamp: new Date(0),\n            },\n          });\n\n          return;\n        }\n\n        controller.enqueue(chunk);\n      },\n    });\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Provider for AI SDK\nDESCRIPTION: Installation command for the OpenAI provider package to use with the AI SDK core module.\nSOURCE: https://github.com/vercel/ai/blob/main/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @ai-sdk/openai\n```\n\n----------------------------------------\n\nTITLE: Customizing Image Generation with Provider Options\nDESCRIPTION: Shows how to pass provider-specific options when generating images with Together.ai models, such as setting the number of steps for the generation process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/24-togetherai.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { togetherai } from '@ai-sdk/togetherai';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { images } = await generateImage({\n  model: togetherai.image('black-forest-labs/FLUX.1-dev'),\n  prompt: 'A delighted resplendent quetzal mid flight amidst raindrops',\n  size: '512x512',\n  // Optional additional provider-specific request parameters\n  providerOptions: {\n    togetherai: {\n      steps: 40,\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Enabling Predicted Outputs for Latency Optimization\nDESCRIPTION: Illustrates how to use OpenAI's predicted outputs feature with `streamText` to potentially reduce latency. It involves providing the existing text that the model should modify within the `providerOptions.openai.prediction` object.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai'; // Assumed import\nimport { streamText } from 'ai'; // Assumed import\n\n// Assuming 'existingCode' is a string variable containing the base code\ndeclare const existingCode: string;\n\nconst result = streamText({\n  model: openai('gpt-4o'),\n  messages: [\n    {\n      role: 'user',\n      content: 'Replace the Username property with an Email property.',\n    },\n    {\n      role: 'user',\n      content: existingCode,\n    },\n  ],\n  providerOptions: {\n    openai: {\n      prediction: {\n        type: 'content',\n        content: existingCode,\n      },\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Manually Providing Tool Results with addToolResult in useChat (TypeScript/React)\nDESCRIPTION: Shows how to deliver tool results in response to user actions by calling 'addToolResult' from the 'useChat' hook in a React component. This approach lets users confirm tool results manually (e.g., clicking a button in the UI). Required dependencies: 'useChat' hook and UI code for capturing interactions. Parameters: 'toolCallId' (the ID of the tool call) and a 'result' object with response data. Returns: None, but the conversation state is updated accordingly. Limitations: the correct 'toolCallId' must be tracked and provided for each pending tool call.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/05-tool-invocation-missing-result.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// Option 2: Use addToolResult (e.g. with interactive UI elements)\nconst { messages, addToolResult } = useChat();\n\n// Inside your JSX, when rendering tool calls:\n<button\n  onClick={() =>\n    addToolResult({\n      toolCallId, // must provide tool call ID\n      result: {\n        /* your tool result */\n      },\n    })\n  }\n>\n  Confirm\n</button>;\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with LangDB and Vercel AI SDK (TypeScript)\nDESCRIPTION: Illustrates using the LangDB provider with the `embed` function from the Vercel AI SDK (`ai` package). It configures the text embedding model using `langdb.textEmbeddingModel('text-embedding-3-small')` and provides the text `value` to be embedded. The function returns the generated embedding vector.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/94-langdb.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { createLangDB } from '@langdb/vercel-provider';\nimport { embed } from 'ai';\n\nconst langdb = createLangDB({\n  apiKey: process.env.LANGDB_API_KEY,\n  projectId: 'your-project-id',\n});\n\nexport async function generateEmbeddings() {\n  const { embedding } = await embed({\n    model: langdb.textEmbeddingModel('text-embedding-3-small'),\n    value: 'sunny day at the beach',\n  });\n\n  console.log('Embedding:', embedding);\n}\n```\n\n----------------------------------------\n\nTITLE: Defining ToolCallPart Interface in TypeScript\nDESCRIPTION: Defines the structure for a tool call content part of a prompt, typically generated by the AI model. It includes a type of 'tool-call', a tool call ID, tool name, and arguments.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/30-core-message.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nexport interface ToolCallPart {\n  type: 'tool-call';\n\n  /**\n   * ID of the tool call. This ID is used to match the tool call with the tool result.\n   */\n  toolCallId: string;\n\n  /**\n   * Name of the tool that is being called.\n   */\n  toolName: string;\n\n  /**\n   * Arguments of the tool call. This is a JSON-serializable object that matches the tool's input schema.\n   */\n  args: unknown;\n}\n```\n\n----------------------------------------\n\nTITLE: Using AssemblyAI for Audio Transcription with Content Safety Filter in TypeScript\nDESCRIPTION: Demonstrates how to use the AssemblyAI provider with the AI SDK's transcribe function, including reading an audio file and enabling content safety filtering.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/100-assemblyai.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { assemblyai } from '@ai-sdk/assemblyai';\nimport { readFile } from 'fs/promises';\n\nconst result = await transcribe({\n  model: assemblyai.transcription('best'),\n  audio: await readFile('audio.mp3'),\n  providerOptions: { assemblyai: { contentSafety: true } },\n});\n```\n\n----------------------------------------\n\nTITLE: Handling Regular Errors in AI SDK Core (TypeScript)\nDESCRIPTION: This snippet demonstrates how to handle regular errors using a try/catch block when generating text with the AI SDK Core. It uses the generateText function and catches any errors that may occur during the text generation process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/50-error-handling.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\n\ntry {\n  const { text } = await generateText({\n    model: yourModel,\n    prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  });\n} catch (error) {\n  // handle error\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Fireworks Provider Instance\nDESCRIPTION: How to create a customized Fireworks provider instance with specific settings like API key.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/26-fireworks.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createFireworks } from '@ai-sdk/fireworks';\n\nconst fireworks = createFireworks({\n  apiKey: process.env.FIREWORKS_API_KEY ?? '',\n});\n```\n\n----------------------------------------\n\nTITLE: Handling Generation and Rendering in Server Action (RSC)\nDESCRIPTION: This snippet shows how to handle generation and rendering in a single server action using the streamUI function from AI SDK RSC.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { getMutableAIState, streamUI } from 'ai/rsc';\n\nexport async function sendMessage(message: string) {\n  'use server';\n\n  const messages = getMutableAIState('messages');\n\n  messages.update([...messages.get(), { role: 'user', content: message }]);\n\n  const { value: stream } = await streamUI({\n    model: openai('gpt-4o'),\n    system: 'you are a friendly assistant!',\n    messages: messages.get(),\n    text: async function* ({ content, done }) {\n      // process text\n    },\n    tools: {\n      // tool definitions\n    },\n  });\n\n  return stream;\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing streamText warnings Asynchronously in TypeScript\nDESCRIPTION: Demonstrates the change in accessing the `warnings` property of the `StreamTextResult` type in AI SDK 4.0. It is now a Promise and must be awaited.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_29\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await streamText({\n  // ...\n});\n\nconst warnings = result.warnings;\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = streamText({\n  // ...\n});\n\nconst warnings = await result.warnings;\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Provider Management Setup\nDESCRIPTION: Implements a complete provider management system combining custom providers, registry, and middleware with various configuration patterns.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/45-provider-management.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { xai } from '@ai-sdk/xai';\nimport { groq } from '@ai-sdk/groq';\nimport {\n  createProviderRegistry,\n  customProvider,\n  defaultSettingsMiddleware,\n  wrapLanguageModel,\n} from 'ai';\n\nexport const registry = createProviderRegistry(\n  {\n    // pass through a full provider with a namespace prefix\n    xai,\n\n    // access an OpenAI-compatible provider with custom setup\n    custom: createOpenAICompatible({\n      name: 'provider-name',\n      apiKey: process.env.CUSTOM_API_KEY,\n      baseURL: 'https://api.custom.com/v1',\n    }),\n\n    // setup model name aliases\n    anthropic: customProvider({\n      languageModels: {\n        fast: anthropic('claude-3-haiku-20240307'),\n\n        // simple model\n        writing: anthropic('claude-3-7-sonnet-20250219'),\n\n        // extended reasoning model configuration:\n        reasoning: wrapLanguageModel({\n          model: anthropic('claude-3-7-sonnet-20250219'),\n          middleware: defaultSettingsMiddleware({\n            settings: {\n              maxTokens: 100000, // example default setting\n              providerMetadata: {\n                anthropic: {\n                  thinking: {\n                    type: 'enabled',\n                    budgetTokens: 32000,\n                  },\n                } satisfies AnthropicProviderOptions,\n              },\n            },\n          }),\n        }),\n      },\n      fallbackProvider: anthropic,\n    }),\n\n    // limit a provider to certain models without a fallback\n    groq: customProvider({\n      languageModels: {\n        'gemma2-9b-it': groq('gemma2-9b-it'),\n        'qwen-qwq-32b': groq('qwen-qwq-32b'),\n      },\n    }),\n  },\n  { separator: ' > ' },\n);\n\n// usage:\nconst model = registry.languageModel('anthropic > reasoning');\n```\n\n----------------------------------------\n\nTITLE: Migrating maxSteps Parameter in useChat Hook\nDESCRIPTION: Shows the change from experimental_maxAutomaticRoundtrips/maxAutomaticRoundtrips/maxToolRoundtrips to maxSteps parameter in useChat hook. The new maxSteps value equals the old roundtrips value plus 1.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_36\n\nLANGUAGE: typescript\nCODE:\n```\nconst { messages } = useChat({\n  experimental_maxAutomaticRoundtrips: 2,\n  // or maxAutomaticRoundtrips\n  // or maxToolRoundtrips\n  // ...\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst { messages } = useChat({\n  maxSteps: 3, // 2 roundtrips + 1\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Spark AI Provider via npm - Bash\nDESCRIPTION: Installs the 'spark-ai-provider' npm module, a required dependency for accessing Spark's language models and provider functionality. This command must be run in your project directory prior to attempting any imports or provider creation. After installation, the Spark provider's functionalities become available within your Node.js or TypeScript project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/92-spark.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i spark-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Qwen Provider Instance - TypeScript\nDESCRIPTION: This snippet demonstrates how to import the 'createQwen' factory function and create a custom provider instance with optional settings, such as 'baseURL'. All options (baseURL, apiKey, headers, fetch) allow further customization of API interaction. Requires 'qwen-ai-provider' to be installed and allows advanced users to modify how requests are handled.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/02-qwen.mdx#2025-04-23_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { createQwen } from 'qwen-ai-provider';\n\nconst qwen = createQwen({\n  // optional settings, e.g.\n  // baseURL: 'https://qwen/api/v1',\n});\n```\n\n----------------------------------------\n\nTITLE: Advanced ID Generator Configuration Example\nDESCRIPTION: Shows a more detailed example of creating an ID generator with specific size parameter for generating shorter IDs, useful for user identification scenarios.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/91-create-id-generator.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Create a custom ID generator for user IDs\nconst generateUserId = createIdGenerator({\n  prefix: 'user',\n  separator: '_',\n  size: 8,\n});\n\n// Generate IDs\nconst id1 = generateUserId(); // e.g., \"user_1a2b3c4d\"\n```\n\n----------------------------------------\n\nTITLE: Installing Replicate Integration via pnpm (Shell)\nDESCRIPTION: Demonstrates how to install the main ai package and the Replicate provider module using the pnpm package manager. No additional dependencies are required beyond pnpm itself, and it prepares the project for TypeScript examples that follow. This command should be run in the project root.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/60-replicate.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add ai @ai-sdk/replicate\n```\n\n----------------------------------------\n\nTITLE: Installing AI SDK Chat Example with Next.js and OpenAI using npx\nDESCRIPTION: This command uses npx to create a new Next.js application with the AI SDK, OpenAI, and Sentry integration example.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry-sentry/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry-sentry next-openai-telemetry-sentry-app\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Fal AI\nDESCRIPTION: This snippet shows how to use Fal's image generation capabilities with the AI SDK's generateImage function, including saving the resulting image to a file.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/10-fal.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fal } from '@ai-sdk/fal';\nimport { experimental_generateImage as generateImage } from 'ai';\nimport fs from 'fs';\n\nconst { image } = await generateImage({\n  model: fal.image('fal-ai/fast-sdxl'),\n  prompt: 'A serene mountain landscape at sunset',\n});\n\nconst filename = `image-${Date.now()}.png`;\nfs.writeFileSync(filename, image.uint8Array);\nconsole.log(`Image saved to ${filename}`);\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Interface with React useChat Hook\nDESCRIPTION: Client-side chat interface implementation using the useChat hook to handle message display and user input.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n\n  return (\n    <>\n      {messages.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.content}\n        </div>\n      ))}\n      <form onSubmit={handleSubmit}>\n        <input name=\"prompt\" value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Submit</button>\n      </form>\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Importing useStreamableValue Hook\nDESCRIPTION: Shows how to import the useStreamableValue hook from the AI SDK RSC package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/11-use-streamable-value.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { useStreamableValue } from \"ai/rsc\"\n```\n\n----------------------------------------\n\nTITLE: Generating Multiple Slogans with Enhanced Prompt in Markdown\nDESCRIPTION: This snippet demonstrates a prompt that requests multiple slogans for a coffee shop with a specific feature (live music).\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/01-prompt-engineering.mdx#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n<InlinePrompt initialInput=\"Create three slogans for a coffee shop with live music.\" />\n```\n\n----------------------------------------\n\nTITLE: Testing Hono AI SDK Server with curl\nDESCRIPTION: A simple curl command to test the Hono server running the AI SDK by sending a POST request to the local endpoint.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/30-hono.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8080\n```\n\n----------------------------------------\n\nTITLE: Explicitly Specifying FriendliAI Dedicated Endpoint\nDESCRIPTION: Shows how to explicitly force requests to use a dedicated endpoint by setting the `endpoint` option to `\"dedicated\"` when creating the FriendliAI model instance, in addition to providing the endpoint ID. This is usually auto-detected but can be specified manually. Dependencies: `@friendliai/ai-provider`, `ai`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { friendli } from '@friendliai/ai-provider';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: friendli(\"YOUR_ENDPOINT_ID\", {\n    endpoint: \"dedicated\",\n  });\n  prompt: 'What is the meaning of life?',\n});\n\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Baseten Model using generateText - TypeScript\nDESCRIPTION: Demonstrates generating text with a Baseten-backed model using the generateText function from the AI SDK in TypeScript. Assumes a properly configured Baseten provider and model, and that generateText is imported from the SDK. The prompt and model are provided as parameters, and the function returns generated text which is then logged. Requires asynchronous execution (await).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/40-baseten.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { generateText } from 'ai';\n\nconst BASETEN_MODEL_ID = '<model-id>'; // e.g. 5q3z8xcw\nconst BASETEN_MODEL_URL = `https://model-${BASETEN_MODEL_ID}.api.baseten.co/environments/production/sync/v1`;\n\nconst baseten = createOpenAICompatible({\n  name: 'baseten',\n  baseURL: BASETEN_MODEL_URL,\n  headers: {\n    Authorization: `Bearer ${process.env.BASETEN_API_KEY ?? ''}`,\n  },\n});\n\nconst { text } = await generateText({\n  model: baseten('llama'),\n  prompt: 'Tell me about yourself in one sentence',\n});\n\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Generating Text with xAI Grok Model in TypeScript\nDESCRIPTION: Complete example showing how to generate text using the xAI Grok-3-Beta model. It demonstrates importing the provider, configuring the model with the xai function, and using the generateText function with a prompt.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/xai/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { xai } from '@ai-sdk/xai';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: xai('grok-3-beta'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing the Default Gladia Provider in TypeScript\nDESCRIPTION: Shows how to import the default Gladia provider instance for use with the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/120-gladia.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { gladia } from '@ai-sdk/gladia';\n```\n\n----------------------------------------\n\nTITLE: Importing useAIState Hook from AI SDK RSC\nDESCRIPTION: Code snippet showing how to import the useAIState hook from the ai/rsc package. This hook provides functionality for reading and updating the global AI state shared between components under the same AI provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/08-use-ai-state.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { useAIState } from \"ai/rsc\"\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Inflection AI and Vercel AI SDK in TypeScript\nDESCRIPTION: Demonstrates how to generate text using the Inflection AI provider within the Vercel AI SDK framework. It imports the `inflection` provider and the `generateText` function, then calls `generateText` with the specified model (`inflection('inflection_3_with_tools')`) and a prompt to get a text response. Requires both `inflection-ai-sdk-provider` and `ai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/93-inflection-ai.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { inflection } from 'inflection-ai-sdk-provider';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: inflection('inflection_3_with_tools'),\n  prompt: 'how can I make quick chicken pho?',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Route Handler for Chat Functionality in Next.js\nDESCRIPTION: This code snippet sets up a route handler for the chat functionality. It uses the AI SDK's OpenAI provider and streamText function to handle chat messages and stream responses from the GPT-4 model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing LlamaIndex Completion API Route in Next.js\nDESCRIPTION: This code snippet demonstrates how to create an API route in Next.js that uses LlamaIndex and the AI SDK to handle completion requests. It initializes an OpenAI model, creates a SimpleChatEngine, and uses the LlamaIndexAdapter to stream the response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/04-adapters/02-llamaindex.mdx#2025-04-23_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { OpenAI, SimpleChatEngine } from 'llamaindex';\nimport { LlamaIndexAdapter } from 'ai';\n\nexport const maxDuration = 60;\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const llm = new OpenAI({ model: 'gpt-4o' });\n  const chatEngine = new SimpleChatEngine({ llm });\n\n  const stream = await chatEngine.chat({\n    message: prompt,\n    stream: true,\n  });\n\n  return LlamaIndexAdapter.toDataStreamResponse(stream);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install required dependencies including AI SDK, OpenAI provider, and development tools.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npnpm add ai @ai-sdk/openai zod dotenv\npnpm add -D @types/node tsx typescript\n```\n\n----------------------------------------\n\nTITLE: Importing the Default Voyage AI Provider in TypeScript\nDESCRIPTION: Imports the default pre-configured instance of the Voyage AI provider from the `voyage-ai-provider` package. This instance can be used directly to access Voyage AI functionalities.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/61-voyage-ai.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { voyage } from 'voyage-ai-provider';\n```\n\n----------------------------------------\n\nTITLE: Importing streamToResponse in TypeScript\nDESCRIPTION: Basic import statement for the streamToResponse helper function from the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/05-stream-to-response.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamToResponse } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Installing Next.js Chat App with NPX\nDESCRIPTION: Command to create a new Next.js application using create-next-app with the OpenAI rate limits example template.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-upstash-rate-limits/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-rate-limits next-openai-rate-limits-app\n```\n\n----------------------------------------\n\nTITLE: Enabling Reasoning Support in Anthropic Model Requests (TypeScript)\nDESCRIPTION: This example demonstrates advanced usage of the generateText function with the Claudic model 'claude-3-7-sonnet@20250219', enabling reasoning support by passing a 'thinking' provider option with a token budget. The returned result object exposes 'reasoning', 'reasoningDetails', and 'text' fields. The snippet also logs these fields, showcasing debugging and explainability use-cases. Requires the AI SDK and '@ai-sdk/google-vertex/anthropic'.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_26\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic';\nimport { generateText } from 'ai';\n\nconst { text, reasoning, reasoningDetails } = await generateText({\n  model: vertexAnthropic('claude-3-7-sonnet@20250219'),\n  prompt: 'How many people will live in the world in 2040?',\n  providerOptions: {\n    anthropic: {\n      thinking: { type: 'enabled', budgetTokens: 12000 },\n    },\n  },\n});\n\nconsole.log(reasoning); // reasoning text\nconsole.log(reasoningDetails); // reasoning details including redacted reasoning\nconsole.log(text); // text response\n```\n\n----------------------------------------\n\nTITLE: Handling Loading State and Stream Stopping in React\nDESCRIPTION: This React component demonstrates how to handle loading states and provide a mechanism to stop the object generation stream. It uses the isLoading and stop properties from the useObject hook.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/40-stream-object.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\nimport { notificationSchema } from './api/use-object/schema';\n\nexport default function Page() {\n  const { object, submit, isLoading, stop } = useObject({\n    api: '/api/use-object',\n    schema: notificationSchema,\n  });\n\n  return (\n    <div>\n      <button\n        onClick={() => submit('Messages during finals week.')}\n        disabled={isLoading}\n      >\n        Generate notifications\n      </button>\n\n      {isLoading && (\n        <div>\n          <div>Loading...</div>\n          <button type=\"button\" onClick={() => stop()}>\n            Stop\n          </button>\n        </div>\n      )}\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Text with LangDB and Vercel AI SDK (TypeScript)\nDESCRIPTION: Demonstrates using the LangDB provider instance with the `generateText` function from the Vercel AI SDK (`ai` package). It configures the model by calling the `langdb` instance with the desired model name (e.g., 'openai/gpt-4o-mini') and provides a prompt. The function call returns the generated text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/94-langdb.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { createLangDB } from '@langdb/vercel-provider';\nimport { generateText } from 'ai';\n\nconst langdb = createLangDB({\n  apiKey: process.env.LANGDB_API_KEY,\n  projectId: 'your-project-id',\n});\n\nexport async function generateTextExample() {\n  const { text } = await generateText({\n    model: langdb('openai/gpt-4o-mini'),\n    prompt: 'Write a Python function that sorts a list:',\n  });\n\n  console.log(text);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing User Tracking with Helicone in JavaScript\nDESCRIPTION: This code demonstrates how to add user tracking to your AI SDK requests using Helicone. It includes a user ID in the request headers for monitoring individual user interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/helicone.mdx#2025-04-23_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst response = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'Hello world',\n  headers: {\n    'Helicone-User-Id': 'user@example.com',\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Provider-Specific Settings\nDESCRIPTION: Shows how to set provider-specific options for transcription using the providerOptions parameter, specifically for OpenAI's word-level timestamp granularity.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/36-transcription.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { readFile } from 'fs/promises';\n\nconst transcript = await transcribe({\n  model: openai.transcription('whisper-1'),\n  audio: await readFile('audio.mp3'),\n  providerOptions: {\n    openai: {\n      timestampGranularities: ['word'],\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Object with Promise Handling in TypeScript\nDESCRIPTION: Shows how to use the object Promise to handle the final streamed object. Includes error handling for type validation failures and demonstrates consuming the partial object stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/46-stream-object-record-final-object.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { z } from 'zod';\n\nconst result = streamObject({\n  model: openai('gpt-4-turbo'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.string()),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n\nresult.object\n  .then(({ recipe }) => {\n    // do something with the fully typed, final object:\n    console.log('Recipe:', JSON.stringify(recipe, null, 2));\n  })\n  .catch(error => {\n    // handle type validation failure\n    // (when the object does not match the schema):\n    console.error(error);\n  });\n\n// note: the stream needs to be consumed because of backpressure\nfor await (const partialObject of result.partialObjectStream) {\n}\n```\n\n----------------------------------------\n\nTITLE: Defining ToolResultContent Type in TypeScript\nDESCRIPTION: Defines the structure for the content of a tool result, which can be an array of text or image parts. Text parts include a type and text string, while image parts include a type, base64 encoded data, and an optional mime type.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/30-core-message.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nexport type ToolResultContent = Array<\n  | {\n      type: 'text';\n      text: string;\n    }\n  | {\n      type: 'image';\n      data: string; // base64 encoded png image, e.g. screenshot\n      mimeType?: string; // e.g. 'image/png';\n    }\n>;\n```\n\n----------------------------------------\n\nTITLE: Streaming Text with NVIDIA NIM and AI SDK in TypeScript\nDESCRIPTION: Illustrates how to stream text generated by an NVIDIA NIM model using the AI SDK's `streamText` function. It sets up the NIM provider, specifies the model ('deepseek-ai/deepseek-r1'), provides a prompt, and iterates through the asynchronous `textStream` to output text chunks to standard output as they arrive. Finally, it logs the total token usage and the finish reason after the stream completes. Requires `@ai-sdk/openai-compatible`, `ai` packages, and the `NIM_API_KEY` environment variable.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/35-nim.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { streamText } from 'ai';\n\nconst nim = createOpenAICompatible({\n  name: 'nim',\n  baseURL: 'https://integrate.api.nvidia.com/v1',\n  headers: {\n    Authorization: `Bearer ${process.env.NIM_API_KEY}`,\n  },\n});\n\nconst result = streamText({\n  model: nim.chatModel('deepseek-ai/deepseek-r1'),\n  prompt: 'Tell me the history of the Northern White Rhino.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n\nconsole.log();\nconsole.log('Token usage:', await result.usage);\nconsole.log('Finish reason:', await result.finishReason);\n```\n\n----------------------------------------\n\nTITLE: Adding Temperature Conversion Tool to API Route in TypeScript\nDESCRIPTION: This snippet demonstrates how to add a new tool for converting temperature from Fahrenheit to Celsius. It shows the updated API route with both the weather and temperature conversion tools, enabling more complex multi-step interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText, tool } from 'ai';\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nexport default defineLazyEventHandler(async () => {\n  const apiKey = useRuntimeConfig().openaiApiKey;\n  if (!apiKey) throw new Error('Missing OpenAI API key');\n  const openai = createOpenAI({\n    apiKey: apiKey,\n  });\n\n  return defineEventHandler(async (event: any) => {\n    const { messages } = await readBody(event);\n\n    const result = streamText({\n      model: openai('gpt-4o-preview'),\n      messages,\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (fahrenheit)',\n          parameters: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => {\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n            return {\n              location,\n              temperature,\n            };\n          },\n        }),\n        convertFahrenheitToCelsius: tool({\n          description: 'Convert a temperature in fahrenheit to celsius',\n          parameters: z.object({\n            temperature: z\n              .number()\n              .describe('The temperature in fahrenheit to convert'),\n          }),\n          execute: async ({ temperature }) => {\n            const celsius = Math.round((temperature - 32) * (5 / 9));\n            return {\n              celsius,\n            };\n          },\n        }),\n      },\n    });\n\n    return result.toDataStreamResponse();\n  });\n});\n```\n\n----------------------------------------\n\nTITLE: Image Generation with Model-specific Parameters using DeepInfra - TypeScript\nDESCRIPTION: Demonstrates passing provider-specific options (such as num_inference_steps for controlling the diffusion process) for image generation models via the providerOptions.deepinfra field. Supports advanced scenarios where underlying model configuration is required, in addition to basic parameters like prompt and aspectRatio.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/11-deepinfra.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepinfra } from '@ai-sdk/deepinfra';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: deepinfra.image('stabilityai/sd3.5'),\n  prompt: 'A futuristic cityscape at sunset',\n  aspectRatio: '16:9',\n  providerOptions: {\n    deepinfra: {\n      num_inference_steps: 30, // Control the number of denoising steps (1-50)\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Mistral AI Default Provider\nDESCRIPTION: Code snippet showing how to import the default Mistral AI provider instance from the SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/20-mistral.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { mistral } from '@ai-sdk/mistral';\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Chat Interface with AI SDK Hooks\nDESCRIPTION: Creates a client-side chat interface that manages the conversation state and allows users to send messages. Uses useUIState and useActions hooks from the AI SDK to manage the conversation flow.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/60-save-messages-to-database.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { ClientMessage } from './actions';\nimport { useActions, useUIState } from 'ai/rsc';\nimport { generateId } from 'ai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [input, setInput] = useState<string>('');\n  const [conversation, setConversation] = useUIState();\n  const { continueConversation } = useActions();\n\n  return (\n    <div>\n      <div>\n        {conversation.map((message: ClientMessage) => (\n          <div key={message.id}>\n            {message.role}: {message.display}\n          </div>\n        ))}\n      </div>\n\n      <div>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button\n          onClick={async () => {\n            setConversation((currentConversation: ClientMessage[]) => [\n              ...currentConversation,\n              { id: generateId(), role: 'user', display: input },\n            ]);\n\n            const message = await continueConversation(input);\n\n            setConversation((currentConversation: ClientMessage[]) => [\n              ...currentConversation,\n              message,\n            ]);\n          }}\n        >\n          Send Message\n        </button>\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Predicted Outputs Metadata\nDESCRIPTION: Shows how to retrieve metadata related to predicted outputs, specifically the count of accepted and rejected prediction tokens, from the `providerMetadata` object returned by the AI SDK call.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\n// Assuming 'result' is the response object from streamText or generateText\ndeclare const result: { providerMetadata?: { openai?: { acceptedPredictionTokens?: number; rejectedPredictionTokens?: number } } };\n\nconst openaiMetadata = (await result.providerMetadata)?.openai;\n\nconst acceptedPredictionTokens = openaiMetadata?.acceptedPredictionTokens;\nconst rejectedPredictionTokens = openaiMetadata?.rejectedPredictionTokens;\n```\n\n----------------------------------------\n\nTITLE: Customizing Run ID in LangSmith Tracing\nDESCRIPTION: Example of customizing the run ID when using AISDKExporter for tracing.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AISDKExporter } from 'langsmith/vercel';\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\nimport { v4 as uuidv4 } from 'uuid';\n\nawait generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  experimental_telemetry: AISDKExporter.getSettings({\n    runId: uuidv4(),\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Enabling Multi-Step Tool Calls in Vue Component\nDESCRIPTION: This snippet shows how to enable multi-step tool calls by adding the maxSteps option to the useChat hook. This allows the model to use multiple steps for complex interactions and process information over several steps if needed.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#2025-04-23_snippet_6\n\nLANGUAGE: vue\nCODE:\n```\n<script setup lang=\"ts\">\nimport { useChat } from '@ai-sdk/vue';\n\nconst { messages, input, handleSubmit } = useChat({ maxSteps: 5 });\n</script>\n\n<!-- ... rest of your component code -->\n```\n\n----------------------------------------\n\nTITLE: Importing getMutableAIState Function from AI SDK RSC\nDESCRIPTION: Shows how to import the getMutableAIState function from the ai/rsc package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/07-get-mutable-ai-state.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { getMutableAIState } from \"ai/rsc\"\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Multi-modal Tool Results in TypeScript\nDESCRIPTION: This example demonstrates how to generate text using an AI model with multi-modal tool results, including both text and image data in the tool response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  messages: [\n    // ...\n    {\n      role: 'tool',\n      content: [\n        {\n          type: 'tool-result',\n          toolCallId: '12345', // needs to match the tool call id\n          toolName: 'get-nutrition-data',\n          // for models that do not support multi-part tool results,\n          // you can include a regular result part:\n          result: {\n            name: 'Cheese, roquefort',\n            calories: 369,\n            fat: 31,\n            protein: 22,\n          },\n          // for models that support multi-part tool results,\n          // you can include a multi-part content part:\n          content: [\n            {\n              type: 'text',\n              text: 'Here is an image of the nutrition data for the cheese:',\n            },\n            {\n              type: 'image',\n              data: fs.readFileSync('./data/roquefort-nutrition-data.png'),\n              mimeType: 'image/png',\n            },\n          ],\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Migrating LangChain Adapter Stream Conversion - TypeScript\nDESCRIPTION: Updates stream conversion in LangChain Adapter usage. The deprecated toAIStream method is replaced with the new toDataStream method. Inputs: a readable stream object. Output: converted data stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_9\n\nLANGUAGE: ts\nCODE:\n```\nLangChainAdapter.toAIStream(stream);\n```\n\nLANGUAGE: ts\nCODE:\n```\nLangChainAdapter.toDataStream(stream);\n```\n\n----------------------------------------\n\nTITLE: Installing Luma AI SDK Module (Shell)\nDESCRIPTION: Installs the @ai-sdk/luma package using pnpm. This command adds the Luma AI provider to your project, which is required to use any subsequent code in this file. No additional parameters are needed and the package manager must be available.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/80-luma.mdx#2025-04-23_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npnpm add @ai-sdk/luma\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Caching with Helicone in JavaScript\nDESCRIPTION: This snippet shows how to enable request caching in Helicone to reduce costs by caching identical requests. It sets the 'Helicone-Cache-Enabled' header to 'true'.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/helicone.mdx#2025-04-23_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst response = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'What is the capital of France?',\n  headers: {\n    'Helicone-Cache-Enabled': 'true',\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Environment Variable File for API Keys\nDESCRIPTION: Command to create a .env.local file that will store the OpenAI API key for authentication.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/02-multi-modal-chatbot.mdx#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntouch .env.local\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-side Chat API with Tool Calls in Next.js\nDESCRIPTION: This code snippet demonstrates the server-side implementation of the chat API using the AI SDK. It defines tools for getting weather information and user location, and handles streaming of responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/90-render-visual-interface-in-chat.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport { z } from 'zod';\n\nexport default async function POST(request: Request) {\n  const { messages } = await request.json();\n\n  const result = streamText({\n    model: openai('gpt-4-turbo'),\n    messages,\n    tools: {\n      // server-side tool with execute function:\n      getWeatherInformation: {\n        description: 'show the weather in a given city to the user',\n        parameters: z.object({ city: z.string() }),\n        execute: async ({}: { city: string }) => {\n          return {\n            value: 24,\n            unit: 'celsius',\n            weeklyForecast: [\n              { day: 'Monday', value: 24 },\n              { day: 'Tuesday', value: 25 },\n              { day: 'Wednesday', value: 26 },\n              { day: 'Thursday', value: 27 },\n              { day: 'Friday', value: 28 },\n              { day: 'Saturday', value: 29 },\n              { day: 'Sunday', value: 30 },\n            ],\n          };\n        },\n      },\n      // client-side tool that starts user interaction:\n      askForConfirmation: {\n        description: 'Ask the user for confirmation.',\n        parameters: z.object({\n          message: z.string().describe('The message to ask for confirmation.'),\n        }),\n      },\n      // client-side tool that is automatically executed on the client:\n      getLocation: {\n        description:\n          'Get the user location. Always ask for confirmation before using this tool.',\n        parameters: z.object({}),\n      },\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Portkey Provider Instance - TypeScript\nDESCRIPTION: This TypeScript code example shows how to import the createPortkey function from the @portkey-ai/vercel-provider package and use it to create a new Portkey provider instance. The provider is configured with your Portkey API key and a provider configuration object that specifies the underlying LLM provider (e.g., 'openai'), the provider's API key, and any override parameters such as the model. All parameters such as apiKey and config must be provided. Prior installation of @portkey-ai/vercel-provider is required.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/10-portkey.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createPortkey } from '@portkey-ai/vercel-provider';\n\nconst portkeyConfig = {\n  provider: 'openai', //enter provider of choice\n  api_key: 'OPENAI_API_KEY', //enter the respective provider's api key\n  override_params: {\n    model: 'gpt-4', //choose from 250+ LLMs\n  },\n};\n\nconst portkey = createPortkey({\n  apiKey: 'YOUR_PORTKEY_API_KEY',\n  config: portkeyConfig,\n});\n```\n\n----------------------------------------\n\nTITLE: Using DeepSeek Language Model for Text Generation\nDESCRIPTION: Example of generating text with a DeepSeek language model using the AI SDK's generateText function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/30-deepseek.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepseek } from '@ai-sdk/deepseek';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: deepseek('deepseek-chat'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Default Google Generative AI Provider\nDESCRIPTION: Shows how to import the default Google Generative AI provider instance that can be used to create language models.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\n```\n\n----------------------------------------\n\nTITLE: Initializing ElevenLabs Transcription Model in TypeScript\nDESCRIPTION: Creates an ElevenLabs transcription model instance using the `.transcription()` factory method on the provider instance. The first argument specifies the model ID, such as 'scribe_v1'. This model object can then be used with AI SDK functions like `transcribe`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/90-elevenlabs.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = elevenlabs.transcription('scribe_v1');\n```\n\n----------------------------------------\n\nTITLE: Implementing a Chat Interface with useChat hook in Next.js\nDESCRIPTION: A client-side Next.js component that creates a simple chat interface using the useChat hook from @ai-sdk/react to handle user input and stream AI responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/122-caching-middleware.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit, error } = useChat();\n  if (error) return <div>{error.message}</div>;\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      <div className=\"space-y-4\">\n        {messages.map(m => (\n          <div key={m.id} className=\"whitespace-pre-wrap\">\n            <div>\n              <div className=\"font-bold\">{m.role}</div>\n              {m.toolInvocations ? (\n                <pre>{JSON.stringify(m.toolInvocations, null, 2)}</pre>\n              ) : (\n                <p>{m.content}</p>\n              )}\n            </div>\n          </div>\n        ))}\n      </div>\n\n      <form onSubmit={handleSubmit}>\n        <input\n          className=\"fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Importing AssistantResponse from Vercel AI SDK\nDESCRIPTION: Shows how to import the AssistantResponse class from the ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/21-assistant-response.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { AssistantResponse } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for AI SDK (Shell)\nDESCRIPTION: This snippet demonstrates how to set up the necessary environment variables in a `.env` file. Specifically, it shows setting the `OPENAI_API_KEY` which is required for interacting with the OpenAI API through the Vercel AI SDK. Additional provider keys might be needed depending on usage.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/express/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Experimental Telemetry in generateText\nDESCRIPTION: Enable experimental telemetry in the generateText function call to create spans for AI SDK tracing.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/laminar.mdx#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'What is Laminar flow?',\n  experimental_telemetry: {\n    isEnabled: true,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing @ai-sdk/elevenlabs using yarn\nDESCRIPTION: Installs the ElevenLabs provider package for the AI SDK using the yarn package manager. This command adds `@ai-sdk/elevenlabs` as a dependency to your project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/90-elevenlabs.mdx#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nyarn add @ai-sdk/elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Importing DeepInfra Provider\nDESCRIPTION: Shows how to import the default DeepInfra provider instance from the package\nSOURCE: https://github.com/vercel/ai/blob/main/packages/deepinfra/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepinfra } from '@ai-sdk/deepinfra';\n```\n\n----------------------------------------\n\nTITLE: Seeding the Postgres Database with Unicorn Data\nDESCRIPTION: Command to initialize and populate the database with unicorn company data that was downloaded from CB Insights. This creates the necessary data structure for running queries.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npnpm run seed\n```\n\n----------------------------------------\n\nTITLE: Running SvelteKit OpenAI Project in Development Mode\nDESCRIPTION: Command to start the SvelteKit OpenAI project in development mode. This uses pnpm's filtering feature to run the dev script specifically for the sveltekit-openai project.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/sveltekit-openai/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm -F sveltekit-openai dev\n```\n\n----------------------------------------\n\nTITLE: Implementing onStepFinish Callback for Tracking Generation Steps (TypeScript)\nDESCRIPTION: This snippet details the onStepFinish callback in TypeScript, enabling tracking of generation lifecycle stages within an AI streaming session. The callback receives a result object specifying the step type (e.g., initial, continue, tool-result), finish reason (e.g., stop, length, error), and token usage metadata. It is optional, expects an object conforming to onStepFinishResult structure, and returns void or a Promise, facilitating extensible tracking or analytics.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  name: 'onStepFinish',\n  type: '(result: onStepFinishResult) => Promise<void> | void',\n  isOptional: true,\n  description: 'Callback that is called when a step is finished.',\n  properties: [\n    {\n      type: 'onStepFinishResult',\n      parameters: [\n        {\n          name: 'stepType',\n          type: '\"initial\" | \"continue\" | \"tool-result\"',\n          description:\n            'The type of step. The first step is always an \"initial\" step, and subsequent steps are either \"continue\" steps or \"tool-result\" steps.',\n        },\n        {\n          name: 'finishReason',\n          type: '\"stop\" | \"length\" | \"content-filter\" | \"tool-calls\" | \"error\" | \"other\" | \"unknown\"',\n          description:\n            'The reason the model finished generating the text for the step.',\n        },\n        {\n          name: 'usage',\n          type: 'TokenUsage',\n          description: 'The token usage of the step.',\n          properties: [\n            {\n              type: 'TokenUsage',\n              parameters: [\n                {\n                  name: 'promptTokens',\n                  type: 'number',\n```\n\n----------------------------------------\n\nTITLE: Rendering Messages with Sources\nDESCRIPTION: Demonstrates how to render messages that include source information.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_19\n\nLANGUAGE: tsx\nCODE:\n```\nmessages.map(message => (\n  <div key={message.id}>\n    {message.role === 'user' ? 'User: ' : 'AI: '}\n    {message.parts\n      .filter(part => part.type !== 'source')\n      .map((part, index) => {\n        if (part.type === 'text') {\n          return <div key={index}>{part.text}</div>;\n        }\n      })}\n    {message.parts\n      .filter(part => part.type === 'source')\n      .map(part => (\n        <span key={`source-${part.source.id}`}>\n          [\n          <a href={part.source.url} target=\"_blank\">\n            {part.source.title ?? new URL(part.source.url).hostname}\n          </a>\n          ]\n        </span>\n      ))}\n  </div>\n));\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Metadata in Middleware\nDESCRIPTION: Demonstrates how to pass and access custom metadata in middleware for logging and debugging purposes.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText, wrapLanguageModel, LanguageModelV1Middleware } from 'ai';\n\nexport const yourLogMiddleware: LanguageModelV1Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    console.log('METADATA', params?.providerMetadata?.yourLogMiddleware);\n    const result = await doGenerate();\n    return result;\n  },\n};\n\nconst { text } = await generateText({\n  model: wrapLanguageModel({\n    model: openai('gpt-4o'),\n    middleware: yourLogMiddleware,\n  }),\n  prompt: 'Invent a new holiday and describe its traditions.',\n  providerOptions: {\n    yourLogMiddleware: {\n      hello: 'world',\n    },\n  },\n});\n\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Tracing AI SDK Tool Calls with LangSmith\nDESCRIPTION: Example of tracing tool calls using AISDKExporter with the AI SDK in a Next.js environment.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AISDKExporter } from 'langsmith/vercel';\nimport { generateText, tool } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nawait generateText({\n  model: openai('gpt-4o-mini'),\n  messages: [\n    {\n      role: 'user',\n      content: 'What are my orders and where are they? My user ID is 123',\n    },\n  ],\n  tools: {\n    listOrders: tool({\n      description: 'list all orders',\n      parameters: z.object({ userId: z.string() }),\n      execute: async ({ userId }) =>\n        `User ${userId} has the following orders: 1`,\n    }),\n    viewTrackingInformation: tool({\n      description: 'view tracking information for a specific order',\n      parameters: z.object({ orderId: z.string() }),\n      execute: async ({ orderId }) =>\n        `Here is the tracking information for ${orderId}`,\n    }),\n  },\n  experimental_telemetry: AISDKExporter.getSettings(),\n  maxSteps: 10,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating an Ollama Embedding Model Client (TypeScript)\nDESCRIPTION: Creates an embedding model client configured to use a specific Ollama embedding model (e.g., 'nomic-embed-text') via the Ollama Embeddings API. The client is created using the `.embedding()` factory method on the provider instance, passing the model ID as the argument.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/03-ollama.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = ollama.embedding('nomic-embed-text');\n```\n\n----------------------------------------\n\nTITLE: Setting Up LangWatch Exporter for Next.js\nDESCRIPTION: TypeScript code for registering OpenTelemetry with LangWatch exporter in Next.js application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langwatch.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { registerOTel } from '@vercel/otel';\nimport { LangWatchExporter } from 'langwatch';\n\nexport function register() {\n  registerOTel({\n    serviceName: 'next-app',\n    traceExporter: new LangWatchExporter(),\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Google Generative AI Provider with Package Managers\nDESCRIPTION: Installation commands for the Google Generative AI provider module using different package managers (pnpm, npm, and yarn).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/google\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/google\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/google\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Fireworks Language Model\nDESCRIPTION: Example demonstrating text generation using the Fireworks language model with the DeepSeek v3 model.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/fireworks/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fireworks } from '@ai-sdk/fireworks';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: fireworks('accounts/fireworks/models/deepseek-v3'),\n  prompt: 'Write a JavaScript function that sorts a list:',\n});\n```\n\n----------------------------------------\n\nTITLE: Recording Token Usage with onFinish Callback in TypeScript\nDESCRIPTION: Example showing how to use the onFinish callback to record token usage when streaming structured data. Uses the Zod schema for type validation and OpenAI's GPT-4 Turbo model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/45-stream-object-record-token-usage.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { z } from 'zod';\n\nconst result = streamObject({\n  model: openai('gpt-4-turbo'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.string()),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n  onFinish({ usage }) {\n    console.log('Token usage:', usage);\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Enabling Cache Point for Bedrock streamText in TypeScript\nDESCRIPTION: Illustrates enabling prompt caching for Amazon Bedrock within a streaming context using the `streamText` function from the Vercel AI SDK. A 'default' cache point is specified via `providerOptions` on the assistant message, and the code shows how to process the streamed text and access cache usage metadata afterwards. Requires `@ai-sdk/amazon-bedrock` and `ai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_13\n\nLANGUAGE: ts\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\nimport { streamText } from 'ai';\n\nconst cyberpunkAnalysis =\n  '... literary analysis of cyberpunk themes and concepts ...';\n\nconst result = streamText({\n  model: bedrock('anthropic.claude-3-5-sonnet-20241022-v2:0'),\n  messages: [\n    {\n      role: 'assistant',\n      content: [\n        { type: 'text', text: 'You are an expert on cyberpunk literature.' },\n        { type: 'text', text: `Academic analysis: ${cyberpunkAnalysis}` },\n      ],\n      providerOptions: { bedrock: { cachePoint: { type: 'default' } } },\n    },\n    {\n      role: 'user',\n      content:\n        'How does Gibson explore the relationship between humanity and technology?',\n    },\n  ],\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n\nconsole.log(\n  'Cache token usage:',\n  (await result.providerMetadata)?.bedrock?.usage,\n);\n// Shows cache read/write token usage, e.g.:\n// {\n//   cacheReadInputTokens: 1337,\n//   cacheWriteInputTokens: 42,\n// }\n```\n\n----------------------------------------\n\nTITLE: Configuring Error Handling in Data Stream Response\nDESCRIPTION: Demonstrates error handling configuration when using createDataStreamResponse with an onError callback.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#2025-04-23_snippet_8\n\nLANGUAGE: tsx\nCODE:\n```\nconst response = createDataStreamResponse({\n  // ...\n  async execute(dataStream) {\n    // ...\n  },\n  onError: error => `Custom error: ${error.message}`,\n});\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Compatible SDK with pnpm\nDESCRIPTION: Installs the `@ai-sdk/openai-compatible` package using the pnpm package manager. This package is required to interact with OpenAI-compatible APIs like NVIDIA NIM via the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/35-nim.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add @ai-sdk/openai-compatible\n```\n\n----------------------------------------\n\nTITLE: Starting Nest.js Development Server with AI SDK\nDESCRIPTION: This command starts the Nest.js development server with the AI SDK integration. It runs the server in development mode, allowing for real-time updates and debugging.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/nest/README.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npnpm run start:dev\n```\n\n----------------------------------------\n\nTITLE: Creating LMNT Speech Model in TypeScript\nDESCRIPTION: How to create a speech model by specifying the model ID (e.g. 'aurora') using the speech() factory method.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/140-lmnt.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = lmnt.speech('aurora');\n```\n\n----------------------------------------\n\nTITLE: Enabling Image Responses and Accessing Provider Metadata (TypeScript)\nDESCRIPTION: Demonstrates enabling Perplexity image response support (for Tier-2 users and up) by setting providerOptions.perplexity.return_images to true in generateText. The result exposes providerMetadata with usage statistics and image metadata. Useful for retrieving visual content alongside text completions. Requires correct API tier and both ai and @ai-sdk/perplexity packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/70-perplexity.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: perplexity('sonar-pro'),\n  prompt: 'What are the latest developments in quantum computing?',\n  providerOptions: {\n    perplexity: {\n      return_images: true, // Enable image responses (Tier-2 Perplexity users only)\n    },\n  },\n});\n\nconsole.log(result.providerMetadata);\n// Example output:\n// {\n//   perplexity: {\n//     usage: { citationTokens: 5286, numSearchQueries: 1 },\n//     images: [\n//       { imageUrl: \"https://example.com/image1.jpg\", originUrl: \"https://elsewhere.com/page1\", height: 1280, width: 720 },\n//       { imageUrl: \"https://example.com/image2.jpg\", originUrl: \"https://elsewhere.com/page2\", height: 1280, width: 720 }\n//     ]\n//   },\n// }\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Google Vertex Provider for Edge Runtime\nDESCRIPTION: Code to create a customized Google Vertex provider instance for Edge runtime environments. This allows specifying project ID and location when using the Edge-compatible provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createVertex } from '@ai-sdk/google-vertex/edge';\n\nconst vertex = createVertex({\n  project: 'my-project', // optional\n  location: 'us-central1', // optional\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Array Streaming in React\nDESCRIPTION: This React component demonstrates how to use the useObject hook for streaming an array of objects. It wraps the schema in z.array() to generate an array of notifications.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/40-stream-object.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\nimport { notificationSchema } from './api/use-object/schema';\n\nexport default function Page() {\n  const { object, submit, isLoading, stop } = useObject({\n    api: '/api/use-object',\n    schema: z.array(notificationSchema),\n  });\n\n  return (\n    <div>\n      <button\n        onClick={() => submit('Messages during finals week.')}\n        disabled={isLoading}\n      >\n        Generate notifications\n      </button>\n\n      {isLoading && (\n        <div>\n          <div>Loading...</div>\n          <button type=\"button\" onClick={() => stop()}>\n            Stop\n          </button>\n        </div>\n      )}\n\n      {object?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification.name}</p>\n          <p>{notification.message}</p>\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Building a Chat Interface with Next.js and AI SDK\nDESCRIPTION: This React component demonstrates how to create a chat interface using the useChat hook from the AI SDK. It renders a list of messages and a form for user input, handling submissions and updates to the chat state.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/22-gpt-4-5.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, handleInputChange, handleSubmit, error } = useChat();\n\n  return (\n    <>\n      {messages.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.content}\n        </div>\n      ))}\n      <form onSubmit={handleSubmit}>\n        <input name=\"prompt\" value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Submit</button>\n      </form>\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Installing and Building AI SDK for Hono Integration\nDESCRIPTION: These commands are used to install dependencies and build the AI SDK project. They should be run from the root directory of the AI SDK repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/hono/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npnpm install\npnpm build\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Language Models\nDESCRIPTION: This code snippet shows how to access generated images from language models like Google's gemini-2.0-flash-exp that support multimodal outputs. It uses the `generateText` function and the `files` property in the result to retrieve images. It also shows how to access different data formats of the image such as base64 and Uint8Array. Requires the `ai` and a model provider like `@ai-sdk/google` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_12\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: google('gemini-2.0-flash-exp'),\n  providerOptions: {\n    google: { responseModalities: ['TEXT', 'IMAGE'] },\n  },\n  prompt: 'Generate an image of a comic cat',\n});\n\nfor (const file of result.files) {\n  if (file.mimeType.startsWith('image/')) {\n    // The file object provides multiple data formats:\n    // Access images as base64 string, Uint8Array binary data, or check type\n    // - file.base64: string (data URL format)\n    // - file.uint8Array: Uint8Array (binary data)\n    // - file.mimeType: string (e.g. \"image/png\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Qwen Chat Model - TypeScript\nDESCRIPTION: This comprehensive example shows how to use the 'generateText' function from the AI SDK to create text completions with a Qwen language model. It demonstrates importing necessary modules, selecting a chat model, and providing a prompt. Expected output is an object with a 'text' property, which contains the generated response. Dependencies: 'qwen-ai-provider' and 'ai' packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/02-qwen.mdx#2025-04-23_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { qwen } from 'qwen-ai-provider';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: qwen('qwen-plus'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Object Schema Type for Output in TypeScript\nDESCRIPTION: Specifies the type for the `schema` parameter within the options for `Output.object()`. It requires a `Schema<OBJECT>` which defines the structure of the JSON object (`OBJECT`) to be generated.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_30\n\nLANGUAGE: typescript\nCODE:\n```\nSchema<OBJECT>\n```\n\n----------------------------------------\n\nTITLE: Importing wrapLanguageModel Function\nDESCRIPTION: Simple import statement showing how to import the wrapLanguageModel function from the ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/60-wrap-language-model.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { wrapLanguageModel } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Using Wrapped Language Model with StreamText\nDESCRIPTION: Shows how to use a wrapped language model with the streamText function for text generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = streamText({\n  model: wrappedLanguageModel,\n  prompt: 'What cities are in the United States?',\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Chat Models with Provider Options\nDESCRIPTION: Shows how to configure xAI chat models using `providerOptions`. This allows settings that are specific to the provider and are not part of the standard call settings.  The `reasoningEffort` parameter is set.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_8\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = xai('grok-3');\n\nawait generateText({\n  model,\n  providerOptions: {\n    xai: {\n      reasoningEffort: 'high',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Importing ElevenLabs Provider\nDESCRIPTION: Code to import the default ElevenLabs provider instance from the package.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/elevenlabs/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { elevenlabs } from '@ai-sdk/elevenlabs';\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_TypeValidationError in TypeScript Using the ai Library\nDESCRIPTION: This snippet demonstrates how to import and use the TypeValidationError from the 'ai' package to identify whether a given error is an instance of AI_TypeValidationError. The isInstance static method is called with the error object as its argument, returning true if the error is of the expected type. This requires that the 'ai' package is installed and available in the project. Inputs: error object. Output: boolean indicating if the error is an AI_TypeValidationError. There are no direct constraints, but the snippet assumes the error comes from an AI-related context where type validation may occur.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-type-validation-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TypeValidationError } from 'ai';\n\nif (TypeValidationError.isInstance(error)) {\n  // Handle the error\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Provider with baseURL Option - TypeScript\nDESCRIPTION: Shows how to replace the deprecated baseUrl option with baseURL in the createOpenAI function. Requires ai-sdk version 4.0 and that createOpenAI is correctly imported from the SDK. The baseURL parameter should point to the provider endpoint; the result is an instance for use in subsequent API calls.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_1\n\nLANGUAGE: ts\nCODE:\n```\nconst perplexity = createOpenAI({\n  // ...\n  baseUrl: 'https://api.perplexity.ai/',\n});\n```\n\nLANGUAGE: ts\nCODE:\n```\nconst perplexity = createOpenAI({\n  // ...\n  baseURL: 'https://api.perplexity.ai/',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing appendResponseMessages from Vercel AI SDK\nDESCRIPTION: Shows how to import the appendResponseMessages utility function from the AI package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/32-append-response-messages.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { appendResponseMessages } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Integrating LangSmith with Sentry\nDESCRIPTION: Example of attaching the LangSmith trace exporter to Sentry's default OpenTelemetry instrumentation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport * as Sentry from '@sentry/node';\nimport { BatchSpanProcessor } from '@opentelemetry/sdk-trace-base';\nimport { AISDKExporter } from 'langsmith/vercel';\n\nconst client = Sentry.init({\n  dsn: '[Sentry DSN]',\n  tracesSampleRate: 1.0,\n});\n\nclient?.traceProvider?.addSpanProcessor(\n  new BatchSpanProcessor(new AISDKExporter()),\n);\n```\n\n----------------------------------------\n\nTITLE: Helicone Proxy Configuration in JavaScript\nDESCRIPTION: This snippet shows a simplified Helicone proxy setup compared to a typical OpenTelemetry setup. It demonstrates the minimal configuration required for Helicone integration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/helicone.mdx#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst openai = createOpenAI({\n  baseURL: \"https://oai.helicone.ai/v1\",\n  headers: { \"Helicone-Auth\": `Bearer ${process.env.HELICONE_API_KEY}` },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring IndexCards for AI SDK Reference Links (JSX)\nDESCRIPTION: This JSX snippet demonstrates the configuration of an `IndexCards` component, likely within an MDX or React-based documentation framework. It passes an array of objects to the `cards` prop, where each object specifies the `title`, `description`, and navigation `href` for a card linking to a specific section of the Vercel AI SDK API reference (AI SDK Core, AI SDK RSC, AI SDK UI, Stream Helpers).\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/index.mdx#2025-04-23_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<IndexCards\n  cards={[\n    {\n      title: 'AI SDK Core',\n      description: 'Switch between model providers without changing your code.',\n      href: '/docs/reference/ai-sdk-core',\n    },\n    {\n      title: 'AI SDK RSC',\n      description:\n        'Use React Server Components to stream user interfaces to the client.',\n      href: '/docs/reference/ai-sdk-rsc',\n    },\n    {\n      title: 'AI SDK UI',\n      description:\n        'Use hooks to integrate user interfaces that interact with language models.',\n      href: '/docs/reference/ai-sdk-ui',\n    },\n    {\n      title: 'Stream Helpers',\n      description:\n        'Use special functions that help stream model generations from various providers.',\n      href: '/docs/reference/stream-helpers',\n    },\n  ]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Installing voyage-ai-provider using npm\nDESCRIPTION: Installs the `voyage-ai-provider` package using the npm package manager. This command adds the necessary dependency to your project's `package.json` and `node_modules`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/61-voyage-ai.mdx#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm install voyage-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Generating Text with FriendliAI Dedicated Endpoints\nDESCRIPTION: Demonstrates using a FriendliAI dedicated endpoint by passing the endpoint ID (instead of a standard model ID) to the `friendli` provider function when creating the model instance. Dependencies: `@friendliai/ai-provider`, `ai`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { friendli } from '@friendliai/ai-provider';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: friendli('YOUR_ENDPOINT_ID'),\n  prompt: 'What is the meaning of life?',\n});\n\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Generating Non-Streaming Text via AI Gateway - TypeScript\nDESCRIPTION: Provides a usage example of generating a single, non-streaming text result using the AI Gateway and Vercel AI SDK. The generateText function consumes a configured model, processes the input prompt, and returns a result containing a text property. Input parameters include a model and prompt; the function returns an object with a text property containing the AI output.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/12-cloudflare-ai-gateway.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAiGateway } from 'ai-gateway-provider';\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst aigateway = createAiGateway({\n  accountId: 'your-cloudflare-account-id',\n  gateway: 'your-gateway-name',\n  apiKey: 'your-cloudflare-api-key',\n});\n\nconst openai = createOpenAI({ apiKey: 'openai-api-key' });\n\nconst { text } = await generateText({\n  model: aigateway([openai('gpt-4o-mini')]),\n  prompt: 'Write a greeting.',\n});\n\nconsole.log(text); // Output: \"Hello\"\n```\n\n----------------------------------------\n\nTITLE: Override Max Images Per Call - AI SDK\nDESCRIPTION: This code snippet shows how to override the default batch size when generating multiple images. It uses the `maxImagesPerCall` setting within the model configuration. This is helpful when working with custom models or adjusting provider limits. This example sets `maxImagesPerCall` to 5, which causes the SDK to make two API calls (if n=10). This requires the `ai` and a model provider like `@ai-sdk/openai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = openai.image('dall-e-2', {\n  maxImagesPerCall: 5, // Override the default batch size\n});\n\nconst { images } = await generateImage({\n  model,\n  prompt: 'Santa Claus driving a Cadillac',\n  n: 10, // Will make 2 calls of 5 images each\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Default Settings Middleware in TypeScript\nDESCRIPTION: Basic example of importing and creating a middleware instance with default settings for temperature and token limits.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/68-default-settings-middleware.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { defaultSettingsMiddleware } from 'ai';\n\nconst middleware = defaultSettingsMiddleware({\n  settings: {\n    temperature: 0.7,\n    maxTokens: 1000,\n    // other settings...\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Checking AI_NoAudioGeneratedError Instance with ai Library (TypeScript)\nDESCRIPTION: This TypeScript snippet imports the NoAudioGeneratedError class from the 'ai' library and demonstrates how to determine if an error is specifically an instance of AI_NoAudioGeneratedError using the isInstance static method. Dependencies include the 'ai' library. The input is any error object, and the snippet checks its type to conditionally handle the error if it matches. The main output is the execution of a user-defined handler inside the conditional block if the error is the expected type. This approach ensures robust error handling for scenarios where expected audio output is missing.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-no-audio-generated-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { NoAudioGeneratedError } from 'ai';\n\nif (NoAudioGeneratedError.isInstance(error)) {\n  // Handle the error\n}\n\n```\n\n----------------------------------------\n\nTITLE: Installing AI SDK using npm\nDESCRIPTION: Command to install the AI SDK package using npm. This is the first step to start using the SDK in your project.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install ai\n```\n\n----------------------------------------\n\nTITLE: Using Groq Transcription with Provider Options\nDESCRIPTION: Example of using a Groq transcription model with provider-specific options like language settings to improve accuracy and latency.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/09-groq.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { groq } from '@ai-sdk/groq';\nimport { readFile } from 'fs/promises';\n\nconst result = await transcribe({\n  model: groq.transcription('whisper-large-v3'),\n  audio: await readFile('audio.mp3'),\n  providerOptions: { groq: { language: 'en' } },\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Generation with Computer Tool\nDESCRIPTION: Example of using the streamText function to receive real-time updates while the model performs computer operations.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/05-computer-use.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = streamText({\n  model: anthropic('claude-3-5-sonnet-20241022'),\n  prompt: 'Open the browser and navigate to vercel.com',\n  tools: { computer: computerTool },\n});\n\nfor await (const chunk of result.textStream) {\n  console.log(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Updated Flight List with useChat Hook\nDESCRIPTION: Updated implementation of flight list component using useChat hook with synchronized chat ID and message appending functionality.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_8\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport function ListFlights({ chatId, flights }) {\n  const { append } = useChat({\n    id: chatId,\n    body: { id: chatId },\n    maxSteps: 5,\n  });\n\n  return (\n    <div>\n      {flights.map(flight => (\n        <div\n          key={flight.id}\n          onClick={async () => {\n            await append({\n              role: 'user',\n              content: `I would like to choose flight ${flight.id}!`,\n            });\n          }}\n        >\n          {flight.name}\n        </div>\n      ))}\n    </div>\n  );\n```\n\n----------------------------------------\n\nTITLE: Implementing Sequential Text Generation Chain with OpenAI\nDESCRIPTION: Demonstrates a three-step sequential generation process using the AI SDK and OpenAI. The code generates blog post ideas about spaghetti, selects the best idea, and creates a detailed outline. Each step uses the output from the previous generation as input for the next step.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/09-sequential-generations.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nasync function sequentialActions() {\n  // Generate blog post ideas\n  const ideasGeneration = await generateText({\n    model: openai('gpt-4o'),\n    prompt: 'Generate 10 ideas for a blog post about making spaghetti.',\n  });\n\n  console.log('Generated Ideas:\\n', ideasGeneration);\n\n  // Pick the best idea\n  const bestIdeaGeneration = await generateText({\n    model: openai('gpt-4o'),\n    prompt: `Here are some blog post ideas about making spaghetti:\n${ideasGeneration}\n\nPick the best idea from the list above and explain why it's the best.`,\n  });\n\n  console.log('\\nBest Idea:\\n', bestIdeaGeneration);\n\n  // Generate an outline\n  const outlineGeneration = await generateText({\n    model: openai('gpt-4o'),\n    prompt: `We've chosen the following blog post idea about making spaghetti:\n${bestIdeaGeneration}\n\nCreate a detailed outline for a blog post based on this idea.`,\n  });\n\n  console.log('\\nBlog Post Outline:\\n', outlineGeneration);\n}\n\nsequentialActions().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Creating Flight Status Component in React\nDESCRIPTION: This component fetches and displays flight status information for a given flight number. It's used as a visual element in the chat interface when flight information is requested.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/90-render-visual-interface-in-chat.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nexport async function Flight({ flightNumber }) {\n  const data = await fetch(`https://api.example.com/flight/${flightNumber}`);\n\n  return (\n    <div>\n      <div>{flightNumber}</div>\n      <div>{data.status}</div>\n      <div>{data.source}</div>\n      <div>{data.destination}</div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Importing the Transcribe Function\nDESCRIPTION: This code snippet shows how to import the experimental transcribe function from the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/11-transcribe.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Importing the Default Luma Provider Instance (TypeScript)\nDESCRIPTION: Imports the default provider instance 'luma' from the '@ai-sdk/luma' module. This provider is used to access Luma AI model functionalities and does not require custom configuration. No parameters or prior setup beyond installing the module are necessary.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/80-luma.mdx#2025-04-23_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { luma } from '@ai-sdk/luma';\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Compatible SDK via npm\nDESCRIPTION: Installs the `@ai-sdk/openai-compatible` package using the npm package manager. This package enables communication between the Vercel AI SDK and LM Studio's local API server.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/30-lmstudio.mdx#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/openai-compatible\n```\n\n----------------------------------------\n\nTITLE: Wrapping Application with AI Component in React\nDESCRIPTION: This layout component wraps the application with the AI component, ensuring that AI functionality is available throughout the app.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/121-stream-assistant-response-with-tools.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { ReactNode } from 'react';\nimport { AI } from './ai';\n\nexport default function Layout({ children }: { children: ReactNode }) {\n  return <AI>{children}</AI>;\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Anthropic Model Instance with Google Vertex (TypeScript)\nDESCRIPTION: This snippet demonstrates initializing an Anthropic language model instance by invoking the anthropic provider function with a specific model identifier (e.g., 'claude-3-haiku-20240307'). The function returns a model object that can later be used for generating text and other interactions. No dependencies are shown, but usage requires the appropriate provider import and Google Vertex setup.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_24\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = anthropic('claude-3-haiku-20240307');\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Together.ai Model\nDESCRIPTION: Shows how to use a Together.ai language model to generate text with the generateText function from the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/24-togetherai.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { togetherai } from '@ai-sdk/togetherai';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: togetherai('meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with Rev.ai and AI SDK\nDESCRIPTION: Complete example demonstrating how to use the Rev.ai provider with the AI SDK's transcribe function to convert an audio file to text. It uses the 'machine' transcription model on a sample audio file.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/revai/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { revai } from '@ai-sdk/revai';\nimport { experimental_transcribe as transcribe } from 'ai';\n\nconst { text } = await transcribe({\n  model: revai.transcription('machine'),\n  audio: new URL(\n    'https://github.com/vercel/ai/raw/refs/heads/main/examples/ai-core/data/galileo.mp3',\n  ),\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Google Generative AI Embedding Model\nDESCRIPTION: Simple initialization of a Google Generative AI text embedding model using the .textEmbeddingModel() factory method with the text-embedding-004 model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = google.textEmbeddingModel('text-embedding-004');\n```\n\n----------------------------------------\n\nTITLE: Importing Deepgram Provider\nDESCRIPTION: TypeScript import statement for accessing the default Deepgram provider instance.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/deepgram/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepgram } from '@ai-sdk/deepgram';\n```\n\n----------------------------------------\n\nTITLE: Provider Specific Settings - AI SDK\nDESCRIPTION: Demonstrates how to pass provider-specific settings to the `generateImage` function via the `providerOptions` parameter. The options are placed inside the `providerOptions` object, making the options provider specific. For the OpenAI provider, this allows setting options such as style and quality. This requires the `ai` and a model provider like `@ai-sdk/openai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { image } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n  size: '1024x1024',\n  providerOptions: {\n    openai: { style: 'vivid', quality: 'hd' },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing @types/react to Resolve JSX Namespace Error - Shell - Bash\nDESCRIPTION: This snippet shows how to install the @types/react package using npm to resolve the TypeScript error 'Cannot find namespace \\'JSX\\'' in projects that use the AI SDK without React. The installation of @types/react provides the necessary JSX namespace typings, eliminating the compiler error. It requires Node.js and npm to be installed beforehand; after execution, TypeScript should recognize the JSX namespace without further configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/40-typescript-cannot-find-namespace-jsx.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @types/react\n```\n\n----------------------------------------\n\nTITLE: AWSBedrockStream API Parameters Type Definition\nDESCRIPTION: Demonstrates the TypeScript interface for AWSBedrockStream parameters including response object structure and callback options.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/09-aws-bedrock-stream.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ntype AWSBedrockResponse = {\n  body?: AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>\n}\n\ntype AIStreamCallbacksAndOptions = {\n  onStart?: () => Promise<void>;\n  onCompletion?: (completion: string) => Promise<void>;\n  onFinal?: (completion: string) => Promise<void>;\n  onToken?: (token: string) => Promise<void>;\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Cohere Provider Instance - TypeScript\nDESCRIPTION: Demonstrates how to import the default 'cohere' provider instance from the @ai-sdk/cohere package. This import grants access to Cohere's API through the AI SDK. Ensure @ai-sdk/cohere is installed as a dependency before importing.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/cohere/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { cohere } from '@ai-sdk/cohere';\n```\n\n----------------------------------------\n\nTITLE: Using Message Prompts for Conversation in TypeScript\nDESCRIPTION: This snippet shows how to implement a conversational interface using message prompts with an array of user and assistant messages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await streamUI({\n  model: yourModel,\n  messages: [\n    { role: 'user', content: 'Hi!' },\n    { role: 'assistant', content: 'Hello, how can I help?' },\n    { role: 'user', content: 'Where can I buy the best Currywurst in Berlin?' },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Querying LLM with AI SDK Core API in API Route Handler - Next.js TypeScript\nDESCRIPTION: This TypeScript snippet illustrates querying a language model using the AI SDK Core API and the OpenAI provider. It imports 'streamText' from 'ai' and 'openai' from '@ai-sdk/openai', and provides a POST handler that accepts JSON input with 'messages'. The handler calls streamText with the OpenAI model and returns the response as a stream suitable for client consumption. Dependencies: 'ai', '@ai-sdk/openai'. The approach abstracts the provider and improves portability.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/39-migration-guide-3-1.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = await streamText({\n    model: openai('gpt-4-turbo'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Message Parts Array for Multi-Modal Outputs in JavaScript\nDESCRIPTION: This snippet illustrates the new approach to representing message outputs where all message components (text, reasoning, tool invocation) are elements in an ordered parts array. This allows for precise sequencing and better support of multi-modal or interleaved model responses. The example uses a plain JavaScript object with explicit fields for each part. No external dependencies are required for this data structure, but it assumes the upgraded AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/27-migration-guide-4-2.mdx#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nmessage.parts = [\n  { type: \"text\", text: \"Final answer: 42\" },\n  { type: \"reasoning\", reasoning: \"First I'll calculate X, then Y...\" },\n  { type: \"tool-invocation\", toolInvocation: { toolName: \"calculator\", args: {...} } },\n];\n```\n\n----------------------------------------\n\nTITLE: Importing InkeepStream in React\nDESCRIPTION: Shows how to import the InkeepStream utility from the ai package in a React application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/19-inkeep-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { InkeepStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Client-Side Component for Displaying Generated Notifications in React\nDESCRIPTION: A React client component that calls a server action to generate notifications based on a prompt. It uses useState to store and display the generated notifications as formatted JSON.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/30-generate-object.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { getNotifications } from './actions';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport default function Home() {\n  const [generation, setGeneration] = useState<string>('');\n\n  return (\n    <div>\n      <button\n        onClick={async () => {\n          const { notifications } = await getNotifications(\n            'Messages during finals week.'\n          );\n\n          setGeneration(JSON.stringify(notifications, null, 2));\n        }}\n      >\n        View Notifications\n      </button>\n\n      <pre>{generation}</pre>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Building Stdio Transport Example\nDESCRIPTION: Command to build the Stdio transport example. This prepares the stdio-based implementation for execution.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npnpm stdio:build\n```\n\n----------------------------------------\n\nTITLE: Importing readStreamableValue from AI SDK RSC\nDESCRIPTION: Shows how to import the readStreamableValue function from the AI SDK RSC package\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/05-read-streamable-value.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { readStreamableValue } from \"ai/rsc\"\n```\n\n----------------------------------------\n\nTITLE: Detecting AI_NoImageGeneratedError in TypeScript Using ai Library\nDESCRIPTION: This snippet demonstrates how to use the ai library in TypeScript for image generation, with error handling that specifically detects the AI_NoImageGeneratedError by checking its instance via NoImageGeneratedError.isInstance(). The example uses an async function (generateImage) with a try-catch block, logging information about the error's cause and associated response metadata when the error is detected. Dependencies include the ai library and a properly initialized image generation model. The input is a model and prompt object; outputs are error logs if image generation fails due to known causes.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-no-image-generated-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateImage, NoImageGeneratedError } from 'ai';\\n\\ntry {\\n  await generateImage({ model, prompt });\\n} catch (error) {\\n  if (NoImageGeneratedError.isInstance(error)) {\\n    console.log('NoImageGeneratedError');\\n    console.log('Cause:', error.cause);\\n    console.log('Responses:', error.responses);\\n  }\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Using Deepgram Transcription with Provider Options in TypeScript\nDESCRIPTION: Transcribe audio using Deepgram with additional provider-specific options like summarization. This example reads audio from a file and processes it with the nova-3 model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/110-deepgram.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { deepgram } from '@ai-sdk/deepgram';\nimport { readFile } from 'fs/promises';\n\nconst result = await transcribe({\n  model: deepgram.transcription('nova-3'),\n  audio: await readFile('audio.mp3'),\n  providerOptions: { deepgram: { summarize: true } },\n});\n```\n\n----------------------------------------\n\nTITLE: Chat Message Storage Implementation\nDESCRIPTION: Implements the saveChat function to persist chat messages to the file system.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-message-persistence.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\nimport { Message } from 'ai';\nimport { writeFile } from 'fs/promises';\n\nexport async function saveChat({\n  id,\n  messages,\n}: {\n  id: string;\n  messages: Message[];\n}): Promise<void> {\n  const content = JSON.stringify(messages, null, 2);\n  await writeFile(getChatFile(id), content);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Nuxt Project with AI SDK and OpenAI Integration\nDESCRIPTION: This command uses create-nuxt to bootstrap a new project with the AI SDK and OpenAI integration. It clones the example repository from GitHub.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/nuxt-openai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx create-nuxt -t github:vercel/ai/examples/nuxt-openai nuxt-openai\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI with Helicone Proxy in JavaScript\nDESCRIPTION: This code snippet demonstrates how to configure the OpenAI client to use Helicone's proxy. It sets the base URL to Helicone's proxy and includes the Helicone API key in the headers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/helicone.mdx#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst openai = createOpenAI({\n  baseURL: 'https://oai.helicone.ai/v1',\n  headers: {\n    'Helicone-Auth': `Bearer ${process.env.HELICONE_API_KEY}`,\n  },\n});\n\n// Use normally with AI SDK\nconst response = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'Hello world',\n});\n```\n\n----------------------------------------\n\nTITLE: Using cache control with message parts\nDESCRIPTION: Demonstrates how to implement cache control for specific parts of a message to optimize token usage and performance. This example marks an error message as ephemeral for caching.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { generateText } from 'ai';\n\nconst errorMessage = '... long error message ...';\n\nconst result = await generateText({\n  model: anthropic('claude-3-5-sonnet-20240620'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'You are a JavaScript expert.' },\n        {\n          type: 'text',\n          text: `Error message: ${errorMessage}`,\n          providerOptions: {\n            anthropic: { cacheControl: { type: 'ephemeral' } },\n          },\n        },\n        { type: 'text', text: 'Explain the error message.' },\n      ],\n    },\n  ],\n});\n\nconsole.log(result.text);\nconsole.log(result.providerMetadata?.anthropic);\n// e.g. { cacheCreationInputTokens: 2118, cacheReadInputTokens: 0 }\n```\n\n----------------------------------------\n\nTITLE: Adding Headers to Data Stream Response in TypeScript/TSX\nDESCRIPTION: This snippet demonstrates how to configure the response headers when using the `toDataStreamResponse` method from the Vercel AI SDK. Specifically, it adds the 'Transfer-Encoding': 'chunked' and 'Connection': 'keep-alive' headers. This is suggested as a potential fix for issues where streaming does not work correctly in certain deployment environments, ensuring the server correctly signals chunked transfer and maintains the connection for streaming.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/06-streaming-not-working-when-deployed.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nreturn result.toDataStreamResponse({\n  headers: {\n    'Transfer-Encoding': 'chunked',\n    Connection: 'keep-alive',\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Retrieving OpenAI Provider Metadata\nDESCRIPTION: This snippet extracts provider-specific metadata from the response, including response ID and token counts for prompt and reasoning tokens. It provides insights into response provenance and token usage statistics.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_19\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst { providerMetadata } = await generateText({\n  model: openai.responses('gpt-4o-mini'),\n});\n\nconst openaiMetadata = providerMetadata?.openai;\n```\n\n----------------------------------------\n\nTITLE: Listing AI SDK Error Reference Links - Markdown\nDESCRIPTION: This Markdown snippet lists common AI SDK errors, each with a hyperlink to a corresponding detailed documentation page. It serves as a troubleshooting reference for developers working with the AI SDK. No external dependencies are required, and the links are intended for internal documentation navigation within the project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/index.mdx#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- [AI_APICallError](/docs/reference/ai-sdk-errors/ai-api-call-error)\\n- [AI_DownloadError](/docs/reference/ai-sdk-errors/ai-download-error)\\n- [AI_EmptyResponseBodyError](/docs/reference/ai-sdk-errors/ai-empty-response-body-error)\\n- [AI_InvalidArgumentError](/docs/reference/ai-sdk-errors/ai-invalid-argument-error)\\n- [AI_InvalidDataContent](/docs/reference/ai-sdk-errors/ai-invalid-data-content)\\n- [AI_InvalidDataContentError](/docs/reference/ai-sdk-errors/ai-invalid-data-content-error)\\n- [AI_InvalidMessageRoleError](/docs/reference/ai-sdk-errors/ai-invalid-message-role-error)\\n- [AI_InvalidPromptError](/docs/reference/ai-sdk-errors/ai-invalid-prompt-error)\\n- [AI_InvalidResponseDataError](/docs/reference/ai-sdk-errors/ai-invalid-response-data-error)\\n- [AI_InvalidToolArgumentsError](/docs/reference/ai-sdk-errors/ai-invalid-tool-arguments-error)\\n- [AI_JSONParseError](/docs/reference/ai-sdk-errors/ai-json-parse-error)\\n- [AI_LoadAPIKeyError](/docs/reference/ai-sdk-errors/ai-load-api-key-error)\\n- [AI_LoadSettingError](/docs/reference/ai-sdk-errors/ai-load-setting-error)\\n- [AI_MessageConversionError](/docs/reference/ai-sdk-errors/ai-message-conversion-error)\\n- [AI_NoContentGeneratedError](/docs/reference/ai-sdk-errors/ai-no-content-generated-error)\\n- [AI_NoImageGeneratedError](/docs/reference/ai-sdk-errors/ai-no-image-generated-error)\\n- [AI_NoObjectGeneratedError](/docs/reference/ai-sdk-errors/ai-no-object-generated-error)\\n- [AI_NoOutputSpecifiedError](/docs/reference/ai-sdk-errors/ai-no-output-specified-error)\\n- [AI_NoSuchModelError](/docs/reference/ai-sdk-errors/ai-no-such-model-error)\\n- [AI_NoSuchProviderError](/docs/reference/ai-sdk-errors/ai-no-such-provider-error)\\n- [AI_NoSuchToolError](/docs/reference/ai-sdk-errors/ai-no-such-tool-error)\\n- [AI_RetryError](/docs/reference/ai-sdk-errors/ai-retry-error)\\n- [AI_ToolCallRepairError](/docs/reference/ai-sdk-errors/ai-tool-call-repair-error)\\n- [AI_ToolExecutionError](/docs/reference/ai-sdk-errors/ai-tool-execution-error)\\n- [AI_TooManyEmbeddingValuesForCallError](/docs/reference/ai-sdk-errors/ai-too-many-embedding-values-for-call-error)\\n- [AI_TypeValidationError](/docs/reference/ai-sdk-errors/ai-type-validation-error)\\n- [AI_UnsupportedFunctionalityError](/docs/reference/ai-sdk-errors/ai-unsupported-functionality-error)\n```\n\n----------------------------------------\n\nTITLE: Import Statement for smoothStream\nDESCRIPTION: Shows how to import the smoothStream utility from the ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/80-smooth-stream.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { smoothStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: TypeScript Interface Documentation\nDESCRIPTION: Detailed type definitions for AI model response objects including text output, reasoning, sources, files, tool calls, and metadata. Covers both basic response properties and complex nested types for multi-step generation and provider-specific features.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_32\n\nLANGUAGE: typescript\nCODE:\n```\ninterface AIResponse {\n  text: string;                     // Generated text output\n  reasoning?: string;               // Model reasoning (if available)\n  reasoningDetails: ReasoningDetail[];  // Detailed reasoning steps\n  sources: Source[];                // Referenced sources used in generation\n  files: GeneratedFile[];           // Generated file outputs\n  toolCalls: any[];                 // Tool calls made by model\n  toolResults: any[];               // Results from tool calls\n  finishReason: 'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown';\n  usage: CompletionTokenUsage;      // Token usage statistics\n  request?: RequestMetadata;        // Request metadata\n  response?: ResponseMetadata;      // Response metadata\n  warnings?: Warning[];            // Provider warnings\n  providerMetadata?: Record<string,Record<string,JSONValue>>;\n  experimental_output?: Output;    // Experimental structured output\n  steps: StepResult[];             // Multi-step generation info\n}\n```\n\n----------------------------------------\n\nTITLE: Using onStepFinish Callback in AI SDK Core\nDESCRIPTION: This example shows how to use the onStepFinish callback in generateText. The callback is triggered when a step is finished, providing access to text, tool calls, tool results, finish reason, and usage information for each step.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  // ...\n  onStepFinish({ text, toolCalls, toolResults, finishReason, usage }) {\n    // your own logic, e.g. for saving the chat history or recording usage\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Nested Spans with Laminar's observe Wrapper\nDESCRIPTION: Use Laminar's observe wrapper to create nested spans and trace function calls beyond AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/laminar.mdx#2025-04-23_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { observe } from '@lmnr-ai/lmnr';\n\nconst result = await observe({ name: 'my-function' }, async () => {\n  // ... some work\n  await generateText({\n    //...\n  });\n  // ... some work\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Customized Google Generative AI Provider\nDESCRIPTION: Creates a customized Google Generative AI provider instance with specific settings such as custom baseURL, API key, headers, or fetch implementation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createGoogleGenerativeAI } from '@ai-sdk/google';\n\nconst google = createGoogleGenerativeAI({\n  // custom settings\n});\n```\n\n----------------------------------------\n\nTITLE: Installing the Google Generative AI Provider via npm\nDESCRIPTION: This command uses npm (Node Package Manager) to install the `@ai-sdk/google` package. This package provides the necessary components to interact with Google Generative AI models through the Vercel AI SDK and is a prerequisite for using the provider in a Node.js project.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/google\n```\n\n----------------------------------------\n\nTITLE: Getting Sources from Search Grounded Gemini Models (TypeScript)\nDESCRIPTION: Demonstrates how to extract source information from Gemini ground-truth responses by accessing the 'sources' property after text generation. Relies on '@ai-sdk/google' and 'ai', and requires the use of probabilistic models with Search Grounding enabled. Main parameters are the model and prompt; the output includes a list of sources used in forming the answer. Useful for transparency in information retrieval and reporting.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst { sources } = await generateText({\n  model: google('gemini-2.0-flash-exp', { useSearchGrounding: true }),\n  prompt: 'List the top 5 San Francisco news from the past week.',\n});\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat UI with Tool Invocations in React\nDESCRIPTION: React component using useChat hook to render messages and handle tool invocations. Includes form handling for user input and conditional rendering of weather components based on tool states.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\nimport { Weather } from '@/components/weather';\n\nexport default function Page() {\n  const { messages, input, setInput, handleSubmit } = useChat();\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>{message.role}</div>\n          <div>{message.content}</div>\n\n          <div>\n            {message.toolInvocations.map(toolInvocation => {\n              const { toolName, toolCallId, state } = toolInvocation;\n\n              if (state === 'result') {\n                const { result } = toolInvocation;\n\n                return (\n                  <div key={toolCallId}>\n                    {toolName === 'displayWeather' ? (\n                      <Weather weatherAtLocation={result} />\n                    ) : null}\n                  </div>\n                );\n              } else {\n                return (\n                  <div key={toolCallId}>\n                    {toolName === 'displayWeather' ? (\n                      <div>Loading weather...</div>\n                    ) : null}\n                  </div>\n                );\n              }\n            })}\n          </div>\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={event => {\n            setInput(event.target.value);\n          }}\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n```\n\n----------------------------------------\n\nTITLE: Rendering Index Cards for AI SDK Foundations in JSX\nDESCRIPTION: This code snippet creates an IndexCards component that displays a grid of cards, each representing a foundational topic in the AI SDK documentation. It includes titles, descriptions, and links for various subjects such as overview, providers and models, prompts, tools, streaming, and agents.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/index.mdx#2025-04-23_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<IndexCards\n  cards={[\n    {\n      title: 'Overview',\n      description: 'Learn about foundational concepts around AI and LLMs.',\n      href: '/docs/foundations/overview',\n    },\n    {\n      title: 'Providers and Models',\n      description:\n        'Learn about the providers and models that you can use with the AI SDK.',\n      href: '/docs/foundations/providers-and-models',\n    },\n    {\n      title: 'Prompts',\n      description:\n        'Learn about how Prompts are used and defined in the AI SDK.',\n      href: '/docs/foundations/prompts',\n    },\n    {\n      title: 'Tools',\n      description: 'Learn about tools in the AI SDK.',\n      href: '/docs/foundations/tools',\n    },\n    {\n      title: 'Streaming',\n      description: 'Learn why streaming is used for AI applications.',\n      href: '/docs/foundations/streaming',\n    },\n    {\n      title: 'Agents',\n      description: 'Learn how to build agents with the AI SDK.',\n      href: '/docs/foundations/agents',\n    },\n  ]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Text Generation with ChromeAI - JavaScript\nDESCRIPTION: Illustrates the use of ChromeAI with the SDK's 'generateText' utility to generate language model output. Requires the 'ai' SDK and the 'chrome-ai' provider. Takes a prompt string and returns generated text via destructuring the result. Useful for synchronous text generation scenarios.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/04-chrome-ai.mdx#2025-04-23_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { generateText } from 'ai';\\nimport { chromeai } from 'chrome-ai';\\n\\nconst { text } = await generateText({\\n  model: chromeai(),\\n  prompt: 'Who are you?',\\n});\\n\\nconsole.log(text); //  I am a large language model, trained by Google.\n```\n\n----------------------------------------\n\nTITLE: Converting Messages to CoreMessage Format in AI SDK\nDESCRIPTION: This code snippet demonstrates how to resolve the useChat maxSteps issue by converting incoming messages to the CoreMessage format using the convertToCoreMessages function before passing them to the model. This ensures proper handling of tool calls and responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/10-use-chat-tools-no-response.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { convertToCoreMessages, streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToCoreMessages(messages),\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Temperature Conversion Tool to API Route in SvelteKit\nDESCRIPTION: This snippet demonstrates how to add a new tool for converting temperature from Fahrenheit to Celsius in the API route. It extends the previous example with an additional tool definition.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nimport { OPENAI_API_KEY } from '$env/static/private';\n\nconst openai = createOpenAI({\n  apiKey: OPENAI_API_KEY,\n});\n\nexport async function POST({ request }) {\n  const { messages } = await request.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n      convertFahrenheitToCelsius: tool({\n        description: 'Convert a temperature in fahrenheit to celsius',\n        parameters: z.object({\n          temperature: z\n            .number()\n            .describe('The temperature in fahrenheit to convert'),\n        }),\n        execute: async ({ temperature }) => {\n          const celsius = Math.round((temperature - 32) * (5 / 9));\n          return {\n            celsius,\n          };\n        },\n      }),\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Experimental Structured Output Type in TypeScript\nDESCRIPTION: Specifies the type for the optional experimental setting `experimental_output` as `Output`. This setting enables generating structured outputs instead of just text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_28\n\nLANGUAGE: typescript\nCODE:\n```\nOutput\n```\n\n----------------------------------------\n\nTITLE: Initializing Wrapped Language Model with Middleware\nDESCRIPTION: Demonstrates how to wrap a language model with middleware using the wrapLanguageModel function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { wrapLanguageModel } from 'ai';\n\nconst wrappedLanguageModel = wrapLanguageModel({\n  model: yourModel,\n  middleware: yourLanguageModelMiddleware,\n});\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAIStream in React\nDESCRIPTION: Shows how to import the OpenAIStream helper function from the AI package in a React application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/07-openai-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { OpenAIStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Importing Crosshatch Provider Creator - TypeScript\nDESCRIPTION: Imports the 'createCrosshatch' factory function from the '@crosshatch/ai-provider' module. This allows you to instantiate a provider object for further configuration and model creation. The import assumes TypeScript or modern JavaScript module syntax and that '@crosshatch/ai-provider' has been installed as a project dependency.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/21-crosshatch.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport createCrosshatch from '@crosshatch/ai-provider';\n```\n\n----------------------------------------\n\nTITLE: Testing AI SDK with Fastify server using curl\nDESCRIPTION: A command line example showing how to test the Fastify server implementation using curl to make a POST request to the localhost endpoint.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/40-fastify.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8080\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for PDF Chat\nDESCRIPTION: Sets up the required environment variable for Anthropic API authentication in the application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/23-chat-with-pdf.mdx#2025-04-23_snippet_2\n\nLANGUAGE: env\nCODE:\n```\nANTHROPIC_API_KEY=xxxxxxxxx\n```\n\n----------------------------------------\n\nTITLE: Initializing Project with Package Manager\nDESCRIPTION: Commands to create a new directory for the AI app project and initialize it with pnpm package manager.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir my-ai-app\ncd my-ai-app\npnpm init\n```\n\n----------------------------------------\n\nTITLE: Importing pipeDataStreamToResponse Function - TypeScript\nDESCRIPTION: Shows how to import the pipeDataStreamToResponse function from the ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/42-pipe-data-stream-to-response.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { pipeDataStreamToResponse } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Installing DeepInfra SDK Module (pnpm) - Shell\nDESCRIPTION: Installs the @ai-sdk/deepinfra module using pnpm, which is required to access DeepInfra providers within the AI SDK. This is a prerequisite for working with any DeepInfra models in a JavaScript or TypeScript environment. Ensure pnpm is installed before running this command.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/11-deepinfra.mdx#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npnpm add @ai-sdk/deepinfra\n```\n\n----------------------------------------\n\nTITLE: Encoding Tool Call Result Stream Part - TypeScript\nDESCRIPTION: Describes the format for a TextStreamPart representing the result of a tool execution, including type, identifiers, arguments, and result payload. It assumes prior tool invocation and argument schema conformance, emitting the final output of a tool call. Result can be any data type; consumers must support parsing dynamic payloads.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_21\n\nLANGUAGE: TypeScript\nCODE:\n```\nparameters: [\n  {\n    name: 'type',\n    type: '\\'tool-result\\'',\n    description: 'The type to identify the object as tool result.',\n  },\n  {\n    name: 'toolCallId',\n    type: 'string',\n    description: 'The id of the tool call.',\n  },\n  {\n    name: 'toolName',\n    type: 'string',\n    description: 'The name of the tool, which typically would be the name of the function.',\n  },\n  {\n    name: 'args',\n    type: 'object based on zod schema',\n    description: 'Parameters generated by the model to be used by the tool.',\n  },\n  {\n    name: 'result',\n    type: 'any',\n    description: 'The result returned by the tool after execution has completed.',\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Max Duration via vercel.json (json)\nDESCRIPTION: This JSON configuration snippet within a `vercel.json` file sets the maximum execution duration for a specific Vercel Function, identified by its file path (e.g., `api/chat/route.ts`). By setting `maxDuration` to 30, the function is allowed to run for up to 30 seconds, which is useful for preventing timeouts in scenarios like AI response streaming. This method applies to various frameworks deployed on Vercel when not using the Next.js App Router's built-in configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/06-timeout-on-vercel.mdx#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"functions\": {\n    \"api/chat/route.ts\": {\n      \"maxDuration\": 30\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing @ai-sdk/openai-compatible using npm - Shell\nDESCRIPTION: Installs the @ai-sdk/openai-compatible package using npm. This package enables use of OpenAI-compatible API providers with the AI SDK. Requires npm to be available in your development environment.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/40-baseten.mdx#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @ai-sdk/openai-compatible\n```\n\n----------------------------------------\n\nTITLE: Enabling Telemetry in AI SDK for Text Generation (TypeScript)\nDESCRIPTION: This snippet demonstrates how to enable telemetry for a text generation function call in the AI SDK. It uses the experimental_telemetry option to turn on telemetry and specifies the OpenAI model to use.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/60-telemetry.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: openai('gpt-4-turbo'),\n  prompt: 'Write a short story about a cat.',\n  experimental_telemetry: { isEnabled: true },\n});\n```\n\n----------------------------------------\n\nTITLE: Appending Message Annotations\nDESCRIPTION: Demonstrates how to append message annotations to the stream data. Accepts any JSON-serializable annotation value.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/45-stream-data.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ndata.appendMessageAnnotation(annotation: JSONValue)\n```\n\n----------------------------------------\n\nTITLE: Handling UI Stream Errors with StreamableUI in TypeScript React\nDESCRIPTION: Demonstrates how to handle errors when streaming UI components using the createStreamableUI method. The code shows updating UI states, handling successful data fetching, and error cases.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/08-error-handling.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { createStreamableUI } from 'ai/rsc';\n\nexport async function getStreamedUI() {\n  const ui = createStreamableUI();\n\n  (async () => {\n    ui.update(<div>loading</div>);\n    const data = await fetchData();\n    ui.done(<div>{data}</div>);\n  })().catch(e => {\n    ui.error(<div>Error: {e.message}</div>);\n  });\n\n  return ui.value;\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Objects with generateObject in a Cloudflare Worker - TypeScript\nDESCRIPTION: This TypeScript snippet showcases generating structured objects (e.g. recipe with ingredients and description) using Cloudflare Workers AI via the generateObject function. It uses zod for schema definition and returns JSON responses. Requires 'workers-ai-provider', 'ai', and 'zod' packages and an Env type with an AI property. Input is an HTTP request/environment; output is a JSON Response containing the structured object.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/11-cloudflare-workers-ai.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createWorkersAI } from 'workers-ai-provider';\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\ntype Env = {\n  AI: Ai;\n};\n\nexport default {\n  async fetch(_: Request, env: Env) {\n    const workersai = createWorkersAI({ binding: env.AI });\n    const result = await generateObject({\n      model: workersai('@cf/meta/llama-3.1-8b-instruct'),\n      prompt: 'Generate a Lasagna recipe',\n      schema: z.object({\n        recipe: z.object({\n          ingredients: z.array(z.string()),\n          description: z.string(),\n        }),\n      }),\n    });\n\n    return Response.json(result.object);\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Importing and Initializing extractReasoningMiddleware in TypeScript\nDESCRIPTION: Example showing how to import and initialize the extractReasoningMiddleware with custom configuration options. The middleware extracts reasoning sections marked with XML tags from generated text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/66-extract-reasoning-middleware.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { extractReasoningMiddleware } from 'ai';\n\nconst middleware = extractReasoningMiddleware({\n  tagName: 'reasoning',\n  separator: '\\n',\n});\n```\n\n----------------------------------------\n\nTITLE: Repository Clone Commands\nDESCRIPTION: Commands to clone the starter repository and navigate to its directory.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_3\n\nLANGUAGE: txt\nCODE:\n```\ngit clone https://github.com/vercel/ai-sdk-rag-starter\ncd ai-sdk-rag-starter\n```\n\n----------------------------------------\n\nTITLE: Installing DeepInfra SDK Module (yarn) - Shell\nDESCRIPTION: Adds the @ai-sdk/deepinfra package to the current project using yarn. This prepares the codebase for integration with DeepInfra's AI model capabilities through the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/11-deepinfra.mdx#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nyarn add @ai-sdk/deepinfra\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_InvalidPromptError Instance in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to check if a caught error object is an instance of `AI_InvalidPromptError`. It utilizes the static `isInstance` method available on the `InvalidPromptError` class, which needs to be imported from the 'ai' library. This allows for specific error handling logic when an invalid prompt causes an error.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-invalid-prompt-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { InvalidPromptError } from 'ai';\n\nif (InvalidPromptError.isInstance(error)) {\n  // Handle the error\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced Image Generation with Additional Parameters\nDESCRIPTION: Example showing how to pass additional model-specific parameters to Replicate models through the providerOptions object. This demonstrates setting style and size parameters for the recraft-v3 model.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/replicate/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst { image } = await generateImage({\n  model: replicate.image('recraft-ai/recraft-v3'),\n  prompt: 'The Loch Ness Monster getting a manicure',\n  size: '1365x1024',\n  providerOptions: {\n    replicate: {\n      style: 'realistic_image',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Importing xAI Provider in TypeScript\nDESCRIPTION: Shows how to import the default xAI provider instance from the @ai-sdk/xai module.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/xai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { xai } from '@ai-sdk/xai';\n```\n\n----------------------------------------\n\nTITLE: Creating a Text Embedding Model Instance - TypeScript\nDESCRIPTION: This TypeScript code demonstrates how to create a text embedding model instance from the Mixedbread provider using the textEmbeddingModel factory method, targeting the 'mixedbread-ai/mxbai-embed-large-v1' model. It depends on a properly configured mixedbread instance. The function returns an embedding model client usable for generating embeddings; the model signature must match one supported by Mixedbread.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/60-mixedbread.mdx#2025-04-23_snippet_3\n\nLANGUAGE: ts\nCODE:\n```\nimport { mixedbread } from 'mixedbread-ai-provider';\n\nconst embeddingModel = mixedbread.textEmbeddingModel(\n  'mixedbread-ai/mxbai-embed-large-v1',\n);\n```\n\n----------------------------------------\n\nTITLE: Installing the Anthropic Vertex AI Provider\nDESCRIPTION: Demonstrates how to install the `anthropic-vertex-ai` package using pnpm, npm, or yarn. This package provides the necessary integration for using Anthropic models via Vertex AI with the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/91-anthropic-vertex-ai.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add anthropic-vertex-ai\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm install anthropic-vertex-ai\n```\n\nLANGUAGE: shell\nCODE:\n```\nyarn add anthropic-vertex-ai\n```\n\n----------------------------------------\n\nTITLE: Configuring Request Options for useObject Hook in React\nDESCRIPTION: This code snippet demonstrates how to configure custom API endpoints, headers, and credentials when using the useObject hook in React.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/08-object-generation.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\nconst { submit, object } = useObject({\n  api: '/api/use-object',\n  headers: {\n    'X-Custom-Header': 'CustomValue',\n  },\n  credentials: 'include',\n  schema: yourSchema,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Single Embedding with LM Studio and AI SDK in TSX\nDESCRIPTION: Shows how to generate an embedding vector for a single string value using an LM Studio embedding model. It imports `createOpenAICompatible` and `embed`, initializes the LM Studio provider, and calls the `embed` function from the 'ai' package. The `model` parameter is specified using `lmstudio.textEmbeddingModel()` with the desired model ID, and the `value` parameter holds the string to embed. The result is a single embedding object (an array of numbers).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/30-lmstudio.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { embed } from 'ai';\n\nconst lmstudio = createOpenAICompatible({\n  name: 'lmstudio',\n  baseURL: 'https://localhost:1234/v1',\n});\n\n// 'embedding' is a single embedding object (number[])\nconst { embedding } = await embed({\n  model: lmstudio.textEmbeddingModel('text-embedding-nomic-embed-text-v1.5'),\n  value: 'sunny day at the beach',\n});\n```\n\n----------------------------------------\n\nTITLE: Enabling Extended Thinking with Claude 3.7 Sonnet\nDESCRIPTION: Demonstrates how to enable Claude 3.7 Sonnet's extended thinking capability by configuring the thinking option with a token budget. This allows for step-by-step reasoning on complex problems.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/20-sonnet-3-7.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';\nimport { generateText } from 'ai';\n\nconst { text, reasoning, reasoningDetails } = await generateText({\n  model: anthropic('claude-3-7-sonnet-20250219'),\n  prompt: 'How many people will live in the world in 2040?',\n  providerOptions: {\n    anthropic: {\n      thinking: { type: 'enabled', budgetTokens: 12000 },\n    } satisfies AnthropicProviderOptions,\n  },\n});\n\nconsole.log(reasoning); // reasoning text\nconsole.log(reasoningDetails); // reasoning details including redacted reasoning\nconsole.log(text); // text response\n```\n\n----------------------------------------\n\nTITLE: Creating a Fal Transcription Model\nDESCRIPTION: This snippet demonstrates how to create a model that calls the Fal transcription API using the .transcription() factory method.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/10-fal.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = fal.transcription('wizper');\n```\n\n----------------------------------------\n\nTITLE: Instantiating a Qwen Chat Model - TypeScript\nDESCRIPTION: This code initializes a Qwen chat model instance by invoking the 'qwen' provider with a model identifier, such as 'qwen-plus'. The resulting model object can be used with AI SDK functions like 'generateText'. Requires prior initialization of 'qwen' provider instance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/02-qwen.mdx#2025-04-23_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = qwen('qwen-plus');\n```\n\n----------------------------------------\n\nTITLE: Implementing PDF Analysis API Route in Next.js\nDESCRIPTION: This server-side code snippet creates an API route that receives a PDF file, sends it to an LLM for analysis using the generateObject function, and returns a structured summary. It uses the Anthropic Claude model and Zod for schema validation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/31-generate-object-with-file-prompt.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateObject } from 'ai';\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { z } from 'zod';\n\nexport async function POST(request: Request) {\n  const formData = await request.formData();\n  const file = formData.get('pdf') as File;\n\n  const result = await generateObject({\n    model: anthropic('claude-3-5-sonnet-latest'),\n    messages: [\n      {\n        role: 'user',\n        content: [\n          {\n            type: 'text',\n            text: 'Analyze the following PDF and generate a summary.',\n          },\n          {\n            type: 'file',\n            data: await file.arrayBuffer(),\n            mimeType: 'application/pdf',\n          },\n        ],\n      },\n    ],\n    schema: z.object({\n      summary: z.string().describe('A 50 word summary of the PDF.'),\n    }),\n  });\n\n  return new Response(result.object.summary);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat API Route in Next.js\nDESCRIPTION: This code snippet creates a server-side API route in Next.js for handling chat requests. It uses the AI SDK to generate text responses based on the conversation history.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/11-generate-text-with-chat-prompt.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CoreMessage, generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: CoreMessage[] } = await req.json();\n\n  const { response } = await generateText({\n    model: openai('gpt-4'),\n    system: 'You are a helpful assistant.',\n    messages,\n  });\n\n  return Response.json({ messages: response.messages });\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Weather Tool Integration\nDESCRIPTION: Initial implementation of a weather tool in the chat application with basic temperature simulation functionality. Includes setup of the OpenAI model, terminal interface, and message handling.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { CoreMessage, streamText, tool } from 'ai';\nimport dotenv from 'dotenv';\nimport { z } from 'zod';\nimport * as readline from 'node:readline/promises';\n\ndotenv.config();\n\nconst terminal = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n});\n\nconst messages: CoreMessage[] = [];\n\nasync function main() {\n  while (true) {\n    const userInput = await terminal.question('You: ');\n\n    messages.push({ role: 'user', content: userInput });\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages,\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (in Celsius)',\n          parameters: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => ({\n            location,\n            temperature: Math.round((Math.random() * 30 + 5) * 10) / 10,\n          }),\n        }),\n      },\n    });\n\n    let fullResponse = '';\n    process.stdout.write('\\nAssistant: ');\n    for await (const delta of result.textStream) {\n      fullResponse += delta;\n      process.stdout.write(delta);\n    }\n    process.stdout.write('\\n\\n');\n\n    messages.push({ role: 'assistant', content: fullResponse });\n  }\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry - Feature Updates and Dependency Changes\nDESCRIPTION: Changelog documenting addition of stopSequences, topK, responseFormat settings, removal of grammar mode, and dependency updates.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/amazon-bedrock/CHANGELOG.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n### Patch Changes\n\n- 2b9da0f0: feat (core): support stopSequences setting.\n- a5b58845: feat (core): support topK setting\n- 4aa8deb3: feat (provider): support responseFormat setting in provider api\n- 13b27ec6: chore (ai/core): remove grammar mode\n- Updated dependencies [2b9da0f0]\n- Updated dependencies [a5b58845]\n- Updated dependencies [4aa8deb3]\n- Updated dependencies [13b27ec6]\n  - @ai-sdk/provider@0.0.13\n  - @ai-sdk/provider-utils@1.0.3\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Call Repair Options Type in TypeScript\nDESCRIPTION: Defines the structure (`ToolCallRepairOptions`) passed to the `experimental_repairToolCall` function. It includes `system` (string | undefined), `messages` (CoreMessage[]), the failed `toolCall` (LanguageModelV1FunctionToolCall), available `tools` (TOOLS), a `parameterSchema` function returning JSONSchema7 for a tool, and the parsing `error` (NoSuchToolError | InvalidToolArgumentsError).\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_26\n\nLANGUAGE: typescript\nCODE:\n```\nToolCallRepairOptions\n```\n\n----------------------------------------\n\nTITLE: Handling Errors in Streaming Text Generation with TypeScript\nDESCRIPTION: Demonstrates how to use the onError callback to log errors when streaming text. This is important because streamText suppresses errors to prevent server crashes.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { streamText } from 'ai';\n\nconst result = streamText({\n  model: yourModel,\n  prompt: 'Invent a new holiday and describe its traditions.',\n  onError({ error }) {\n    console.error(error); // your error logging logic here\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing @ai-sdk/perplexity via pnpm, npm, or yarn (Shell Commands)\nDESCRIPTION: These commands install the @ai-sdk/perplexity package using different JavaScript package managers: pnpm, npm, and yarn. Execute one of these in your project root to add Perplexity support. Required before importing and utilizing the Perplexity provider in your TypeScript or JavaScript code.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/70-perplexity.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add @ai-sdk/perplexity\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @ai-sdk/perplexity\n```\n\nLANGUAGE: shell\nCODE:\n```\nyarn add @ai-sdk/perplexity\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry with AISDKExporter in Node.js\nDESCRIPTION: Example of setting up OpenTelemetry with AISDKExporter in a Node.js environment.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AISDKExporter } from 'langsmith/vercel';\n\nimport { NodeSDK } from '@opentelemetry/sdk-node';\nimport { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';\n\nconst sdk = new NodeSDK({\n  traceExporter: new AISDKExporter(),\n  instrumentations: [getNodeAutoInstrumentations()],\n});\n\nsdk.start();\n```\n\n----------------------------------------\n\nTITLE: Creating Bedrock Model with Additional Parameters\nDESCRIPTION: Example of creating an Amazon Bedrock model with additional model-specific parameters like top_k.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = bedrock('anthropic.claude-3-sonnet-20240229-v1:0', {\n  additionalModelRequestFields: { top_k: 350 },\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing Image Generation Options\nDESCRIPTION: This example shows customizing image generation by passing model-specific settings to the image model instance.  The `maxImagesPerCall` setting is customized, which can affect the number of images generated.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_10\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { xai } from '@ai-sdk/xai';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { images } = await generateImage({\n  model: xai.image('grok-2-image', {\n    maxImagesPerCall: 5, // Default is 10\n  }),\n  prompt: 'A futuristic cityscape at sunset',\n  n: 2, // Generate 2 images\n});\n```\n\n----------------------------------------\n\nTITLE: Using Gladia Transcription with AI SDK\nDESCRIPTION: Complete example showing how to use the Gladia provider with the AI SDK's transcribe function to transcribe an audio file from a URL.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/gladia/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { gladia } from '@ai-sdk/gladia';\nimport { experimental_transcribe as transcribe } from 'ai';\n\nconst { text } = await transcribe({\n  model: gladia.transcription(),\n  audio: new URL(\n    'https://github.com/vercel/ai/raw/refs/heads/main/examples/ai-core/data/galileo.mp3',\n  ),\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Building SvelteKit OpenAI Project\nDESCRIPTION: Commands to install project dependencies and build the SvelteKit OpenAI integration project. These steps are performed at the root of the cloned repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/sveltekit-openai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm install\npnpm build\n```\n\n----------------------------------------\n\nTITLE: Starting the Development Server\nDESCRIPTION: Command to launch the Next.js development server, making the application accessible at localhost:3000 for testing and development.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Rendering Structured Data Example Links - React JSX\nDESCRIPTION: This snippet demonstrates how to use a React component, ExampleLinks, to render a list of example links for generating structured data with a language model in both Next.js and Node.js environments. It passes an array of objects, each specifying a title and a documentation link, to the component as the examples prop. There are no additional dependencies beyond React; the ExampleLinks component is presumed to be available in the project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/03-generate-object.mdx#2025-04-23_snippet_7\n\nLANGUAGE: jsx\nCODE:\n```\n<ExampleLinks\n  examples={[\n    {\n      title:\n        'Learn to generate structured data using a language model in Next.js',\n      link: '/examples/next-app/basics/generating-object',\n    },\n    {\n      title:\n        'Learn to generate structured data using a language model in Node.js',\n      link: '/examples/node/generating-structured-data/generate-object',\n    },\n  ]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Responses with File Prompts using Anthropic's Claude Model\nDESCRIPTION: This code demonstrates how to stream text responses from Anthropic's Claude 3.5 Sonnet model when providing a PDF file as part of the prompt. It reads a file from disk, includes it in the message content along with a text query, and then streams the model's response to standard output.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/23-stream-text-with-file-prompt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { streamText } from 'ai';\nimport 'dotenv/config';\nimport fs from 'node:fs';\n\nasync function main() {\n  const result = streamText({\n    model: anthropic('claude-3-5-sonnet-20241022'),\n    messages: [\n      {\n        role: 'user',\n        content: [\n          {\n            type: 'text',\n            text: 'What is an embedding model according to this document?',\n          },\n          {\n            type: 'file',\n            data: fs.readFileSync('./data/ai.pdf'),\n            mimeType: 'application/pdf',\n          },\n        ],\n      },\n    ],\n  });\n\n  for await (const textPart of result.textStream) {\n    process.stdout.write(textPart);\n  }\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Implementing a Chat Interface with DeepSeek R1 and AI SDK UI\nDESCRIPTION: This React component uses the useChat hook from AI SDK UI to create an interactive chat interface. It renders messages, including reasoning tokens, and provides an input form for user interactions with the DeepSeek R1 model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/25-r1.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, handleInputChange, handleSubmit, error } = useChat();\n\n  return (\n    <>\n      {messages.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.reasoning && <pre>{message.reasoning}</pre>}\n          {message.content}\n        </div>\n      ))}\n      <form onSubmit={handleSubmit}>\n        <input name=\"prompt\" value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Submit</button>\n      </form>\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Version 0.0.6 - AWS Bedrock Fix\nDESCRIPTION: Patch fixing tool parameters handling in AWS Bedrock provider.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/amazon-bedrock/CHANGELOG.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## 0.0.6\n\n### Patch Changes\n\n- 42b11b8e: fix (provider/aws-bedrock): pass tool parameters for object generation without stringify\n```\n\n----------------------------------------\n\nTITLE: Accessing LM Studio Embedding Model in TypeScript\nDESCRIPTION: Demonstrates how to access an embedding model (e.g., 'text-embedding-nomic-embed-text-v1.5') provided by LM Studio. It uses the `.embedding()` factory method on the LM Studio provider instance, passing the specific model ID as an argument. This model object can then be used with AI SDK embedding functions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/30-lmstudio.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = lmstudio.embedding('text-embedding-nomic-embed-text-v1.5');\n```\n\n----------------------------------------\n\nTITLE: Message Type Definitions for AI Communication\nDESCRIPTION: Detailed type definitions for different message types including system, user, assistant, and tool messages, with their respective properties and parameters.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/01-stream-ui.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  type: 'CoreSystemMessage',\n  parameters: [\n    {\n      name: 'role',\n      type: \"'system'\",\n      description: 'The role for the system message.',\n    },\n    {\n      name: 'content',\n      type: 'string',\n      description: 'The content of the message.',\n    },\n  ],\n}\n```\n\n----------------------------------------\n\nTITLE: Importing createStreamableValue from AI SDK RSC\nDESCRIPTION: Shows how to import the createStreamableValue function from the AI SDK RSC package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/04-create-streamable-value.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createStreamableValue } from \"ai/rsc\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Chat Models with Options\nDESCRIPTION: Demonstrates how to configure xAI chat models by passing an options argument.  The `user` parameter is an optional unique identifier, potentially used for abuse detection.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = xai('grok-3', {\n  user: 'test-user', // optional unique user identifier\n});\n```\n\n----------------------------------------\n\nTITLE: Installing the Azure AI SDK Provider (Bash)\nDESCRIPTION: This command uses the Node Package Manager (npm) to install the `@ai-sdk/azure` package. This package provides the necessary functions and instances to interact with Azure OpenAI services through the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/azure/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/azure\n```\n\n----------------------------------------\n\nTITLE: Installing Together.ai Provider with npm (Bash)\nDESCRIPTION: Shows how to install the `@ai-sdk/togetherai` package using the Node Package Manager (npm). This package is required to use the Together.ai provider with the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/togetherai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/togetherai\n```\n\n----------------------------------------\n\nTITLE: Defining TextStreamPart Type Structure for AI Response Handling\nDESCRIPTION: Type definition for TextStreamPart that handles both successful completions and errors in AI responses. Includes detailed type information for token usage, metadata, and response details.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_24\n\nLANGUAGE: typescript\nCODE:\n```\ntype: 'TextStreamPart',\nparameters: [\n  {\n    name: 'type',\n    type: \"'finish'\",\n    description: 'The type to identify the object as finish.',\n  },\n  {\n    name: 'finishReason',\n    type: \"'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'\",\n    description: 'The reason the model finished generating the text.',\n  },\n  {\n    name: 'usage',\n    type: 'TokenUsage',\n    description: 'The token usage of the generated text.',\n    properties: [...]\n  }\n  // Additional parameters omitted for brevity\n]\n```\n\n----------------------------------------\n\nTITLE: Tool Call Repair Using Re-ask Strategy\nDESCRIPTION: Shows how to implement tool call repair by re-asking the model with previous context and error information.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject, generateText, NoSuchToolError, tool } from 'ai';\n\nconst result = await generateText({\n  model,\n  tools,\n  prompt,\n\n  experimental_repairToolCall: async ({\n    toolCall,\n    tools,\n    error,\n    messages,\n    system,\n  }) => {\n    const result = await generateText({\n      model,\n      system,\n      messages: [\n        ...messages,\n        {\n          role: 'assistant',\n          content: [\n            {\n              type: 'tool-call',\n              toolCallId: toolCall.toolCallId,\n              toolName: toolCall.toolName,\n              args: toolCall.args,\n            },\n          ],\n        },\n        {\n          role: 'tool' as const,\n          content: [\n            {\n              type: 'tool-result',\n              toolCallId: toolCall.toolCallId,\n              toolName: toolCall.toolName,\n              result: error.message,\n            },\n          ],\n        },\n      ],\n      tools,\n    });\n\n    const newToolCall = result.toolCalls.find(\n      newToolCall => newToolCall.toolName === toolCall.toolName,\n    );\n\n    return newToolCall != null\n      ? {\n          toolCallType: 'function' as const,\n          toolCallId: toolCall.toolCallId,\n          toolName: toolCall.toolName,\n          args: JSON.stringify(newToolCall.args),\n        }\n      : null;\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Interactive Flights Component\nDESCRIPTION: Implements client-side Flights component with interactive flight selection and lookup functionality.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/04-multistep-interfaces.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useActions, useUIState } from 'ai/rsc';\nimport { ReactNode } from 'react';\n\ninterface FlightsProps {\n  flights: { id: string; flightNumber: string }[];\n}\n\nexport const Flights = ({ flights }: FlightsProps) => {\n  const { submitUserMessage } = useActions();\n  const [_, setMessages] = useUIState();\n\n  return (\n    <div>\n      {flights.map(result => (\n        <div key={result.id}>\n          <div\n            onClick={async () => {\n              const display = await submitUserMessage(\n                `lookupFlight ${result.flightNumber}`,\n              );\n\n              setMessages((messages: ReactNode[]) => [...messages, display]);\n            }}\n          >\n            {result.flightNumber}\n          </div>\n        </div>\n      ))}\n    </div>\n  );\n};\n```\n\n----------------------------------------\n\nTITLE: Chat Loading Implementation\nDESCRIPTION: React components for loading and displaying chat history with database integration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_11\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { Message } from 'ai';\nimport { useChat } from '@ai-sdk/react';\n\nexport function Chat({\n  id,\n  initialMessages,\n}: {\n  id;\n  initialMessages: Array<Message>;\n}) {\n  const { messages } = useChat({\n    id,\n    initialMessages,\n  });\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>{message.role}</div>\n          <div>{message.content}</div>\n        </div>\n      ))}\n    </div>\n  );\n```\n\n----------------------------------------\n\nTITLE: Creating Mistral AI Language Model Instance\nDESCRIPTION: Shows how to create a language model instance using Mistral's 'mistral-large-latest' model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/20-mistral.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = mistral('mistral-large-latest');\n```\n\n----------------------------------------\n\nTITLE: Creating Google Generative AI Language Model\nDESCRIPTION: Creates a language model instance using the Google Generative AI provider with a specified model ID.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = google('gemini-1.5-pro-latest');\n```\n\n----------------------------------------\n\nTITLE: Starting LLM Span in LangWatch Trace\nDESCRIPTION: TypeScript code for starting an LLM span within a LangWatch trace to capture input data.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langwatch.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst span = trace.startLLMSpan({\n  name: 'llm',\n  model: model,\n  input: {\n    type: 'chat_messages',\n    value: messages,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Modifying an Image via Luma Generation (TypeScript)\nDESCRIPTION: Illustrates modification of an existing image based on a text prompt using 'modify_image_ref'. The referenced input image and weight parameter are provided for content transformation. Weights near 1.0 enforce strict adherence to the original; lower values (0.0-0.1) are recommended for subtler changes, such as color adjustments. Constraints: requires a valid image URL.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/80-luma.mdx#2025-04-23_snippet_10\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Example: Modify existing image\nawait generateImage({\n  model: luma.image('photon-1'),\n  prompt: 'transform the bike to a boat',\n  providerOptions: {\n    luma: {\n      modify_image_ref: {\n        url: 'https://example.com/image.jpg',\n        weight: 1.0,\n      },\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Entry - Recent Patches\nDESCRIPTION: Documents recent patch changes including interface updates and bug fixes for Vue and Solid frameworks.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/CHANGELOG.md#2025-04-23_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n### Patch Changes\n\n- 5ed581d: Use interface instead of type for Message to allow declaration merging\n- 9adec1e: vue and solid: fix including `function_call` and `name` fields in subsequent requests\n```\n\n----------------------------------------\n\nTITLE: Using Cached Content with Google Generative AI\nDESCRIPTION: Shows how to cache content with Google Generative AI models to improve performance for multiple related queries, setting up a cache manager and applying the cached content to subsequent requests.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { GoogleAICacheManager } from '@google/generative-ai/server';\nimport { generateText } from 'ai';\n\nconst cacheManager = new GoogleAICacheManager(\n  process.env.GOOGLE_GENERATIVE_AI_API_KEY,\n);\n\n// As of August 23rd, 2024, these are the only models that support caching\ntype GoogleModelCacheableId =\n  | 'models/gemini-1.5-flash-001'\n  | 'models/gemini-1.5-pro-001';\n\nconst model: GoogleModelCacheableId = 'models/gemini-1.5-pro-001';\n\nconst { name: cachedContent } = await cacheManager.create({\n  model,\n  contents: [\n    {\n      role: 'user',\n      parts: [{ text: '1000 Lasanga Recipes...' }],\n    },\n  ],\n  ttlSeconds: 60 * 5,\n});\n\nconst { text: veggieLasangaRecipe } = await generateText({\n  model: google(model, { cachedContent }),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n\nconst { text: meatLasangaRecipe } = await generateText({\n  model: google(model, { cachedContent }),\n  prompt: 'Write a meat lasagna recipe for 12 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat API Route Handler in Next.js\nDESCRIPTION: Route handler implementation for chat functionality using OpenAI and AI SDK's streamText function. Handles POST requests and returns streamed responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Chat API Route Implementation\nDESCRIPTION: Implements the API route handler for processing chat messages with OpenAI and storing responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-message-persistence.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { appendResponseMessages, streamText } from 'ai';\nimport { saveChat } from '@tools/chat-store';\n\nexport async function POST(req: Request) {\n  const { messages, id } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o-mini'),\n    messages,\n    async onFinish({ response }) {\n      await saveChat({\n        id,\n        messages: appendResponseMessages({\n          messages,\n          responseMessages: response.messages,\n        }),\n      });\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Stream with Hono and AI SDK\nDESCRIPTION: This snippet demonstrates how to use the textStream property to directly stream AI-generated text to the client through a Hono server. It provides a simpler approach compared to data streams for plain text output.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/30-hono.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { serve } from '@hono/node-server';\nimport { streamText } from 'ai';\nimport { Hono } from 'hono';\nimport { stream } from 'hono/streaming';\n\nconst app = new Hono();\n\napp.post('/', async c => {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    prompt: 'Invent a new holiday and describe its traditions.',\n  });\n\n  c.header('Content-Type', 'text/plain; charset=utf-8');\n\n  return stream(c, stream => stream.pipe(result.textStream));\n});\n\nserve({ fetch: app.fetch, port: 8080 });\n```\n\n----------------------------------------\n\nTITLE: Testing the Node.js HTTP Server with curl\nDESCRIPTION: A simple curl command to test the Node.js HTTP server by sending a POST request to localhost:8080.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/10-node-http-server.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8080\n```\n\n----------------------------------------\n\nTITLE: Generating an Object Without Structured Outputs in Google Vertex (TypeScript)\nDESCRIPTION: Shows a workaround for schema limitations when generating objects from the model. Disables 'structuredOutputs' in the vertex model options to support unsupported schema features such as Zod unions or records. Snippet presents sample schema and prompt for generating test objects. Dependencies: 'ai', Zod, '@ai-sdk/google-vertex'.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateObject({\n  model: vertex('gemini-1.5-pro', {\n    structuredOutputs: false,\n  }),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    contact: z.union([\n      z.object({\n        type: z.literal('email'),\n        value: z.string(),\n      }),\n      z.object({\n        type: z.literal('phone'),\n        value: z.string(),\n      }),\n    ]),\n  }),\n  prompt: 'Generate an example person for testing.',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating an Anthropic Language Model Instance in TypeScript\nDESCRIPTION: Demonstrates how to create a specific language model instance by calling the `anthropicVertex` provider instance with the desired model ID (e.g., `claude-3-sonnet@20240229`). This model instance can then be used with Vercel AI SDK functions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/91-anthropic-vertex-ai.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = anthropicVertex('claude-3-sonnet@20240229');\n```\n\n----------------------------------------\n\nTITLE: Installing the AI SDK Google Vertex Provider using npm\nDESCRIPTION: Shows the npm command to install the `@ai-sdk/google-vertex` package. This package is required to use the Google Vertex AI provider with the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google-vertex/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/google-vertex\n```\n\n----------------------------------------\n\nTITLE: Enabling Cache Point for Bedrock generateText in TypeScript\nDESCRIPTION: Shows how to enable prompt caching for Amazon Bedrock using the `providerOptions` in the `generateText` function from the Vercel AI SDK. It sets a 'default' cache point for the system message and demonstrates retrieving cache usage statistics from the result's `providerMetadata`. Requires `@ai-sdk/amazon-bedrock` and `ai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_12\n\nLANGUAGE: ts\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\nimport { generateText } from 'ai';\n\nconst cyberpunkAnalysis =\n  '... literary analysis of cyberpunk themes and concepts ...';\n\nconst result = await generateText({\n  model: bedrock('anthropic.claude-3-5-sonnet-20241022-v2:0'),\n  messages: [\n    {\n      role: 'system',\n      content: `You are an expert on William Gibson's cyberpunk literature and themes. You have access to the following academic analysis: ${cyberpunkAnalysis}`,\n      providerOptions: {\n        bedrock: { cachePoint: { type: 'default' } },\n      },\n    },\n    {\n      role: 'user',\n      content:\n        'What are the key cyberpunk themes that Gibson explores in Neuromancer?',\n    },\n  ],\n});\n\nconsole.log(result.text);\nconsole.log(result.providerMetadata?.bedrock?.usage);\n// Shows cache read/write token usage, e.g.:\n// {\n//   cacheReadInputTokens: 1337,\n//   cacheWriteInputTokens: 42,\n// }\n```\n\n----------------------------------------\n\nTITLE: Customizing Request Headers for OpenAI Compatible Provider in TypeScript\nDESCRIPTION: Illustrates how to customize HTTP request headers when creating an OpenAI compatible provider instance using `createOpenAICompatible`. This example specifically shows passing an API key via the `Authorization` header (using a Bearer token) instead of the default `apiKey` parameter, which is useful for APIs requiring different authentication schemes.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/openai-compatible/README.md#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: createOpenAICompatible({\n    baseURL: 'https://api.example.com/v1',\n    name: 'example',\n    headers: {\n      Authorization: `Bearer ${process.env.MY_API_KEY}`,\n    },\n  }).chatModel('meta-llama/Llama-3-70b-chat-hf'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Reasoning Format for Groq Models\nDESCRIPTION: Configure how reasoning is displayed in Groq reasoning models like qwen-qwq-32b using the reasoningFormat option.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/09-groq.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { groq } from '@ai-sdk/groq';\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: groq('qwen-qwq-32b'),\n  providerOptions: {\n    groq: { reasoningFormat: 'parsed' },\n  },\n  prompt: 'How many \"r\"s are in the word \"strawberry\"?',\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring a Custom Google Vertex Provider in Node.js using AI SDK\nDESCRIPTION: Shows how to create a custom Google Vertex provider instance using `createVertex` from `@ai-sdk/google-vertex` in a Node.js environment. This allows specifying configuration options like `project`, `location`, and explicit `googleAuthOptions` including credentials (`client_email`, `private_key`), overriding default authentication mechanisms. The custom provider is then used with `generateText`.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google-vertex/README.md#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createVertex } from '@ai-sdk/google-vertex';\nimport { generateText } from 'ai';\n\nconst customProvider = createVertex({\n  project: 'your-project-id',\n  location: 'us-central1',\n  googleAuthOptions: {\n    credentials: {\n      client_email: 'your-client-email',\n      private_key: 'your-private-key',\n    },\n  },\n});\n\nconst { text } = await generateText({\n  model: customProvider('gemini-1.5-flash'),\n  prompt: 'Write a vegetarian lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Displaying Changelog Entry for Version 4.3.7\nDESCRIPTION: Shows the patch change for version 4.3.7, which refactors the toResponseMessages function to filter out empty string/content.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/CHANGELOG.md#2025-04-23_snippet_2\n\nLANGUAGE: Markdown\nCODE:\n```\n## 4.3.7\n\n### Patch Changes\n\n- f4f3945: fix (ai/core): refactor `toResponseMessages` to filter out empty string/content\n```\n\n----------------------------------------\n\nTITLE: Using Image Reference in Luma Generation (TypeScript)\nDESCRIPTION: Demonstrates guiding image generation using one or more reference images. This is achieved by passing URLs and optional weights (0-1) to the 'providerOptions.luma.image_ref' array when calling generateImage. Reference images influence the output for tasks like variation or style transfers. Requires reference image URLs and can specify up to four references with corresponding weights.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/80-luma.mdx#2025-04-23_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Example: Generate a salamander with reference\nawait generateImage({\n  model: luma.image('photon-1'),\n  prompt: 'A salamander at dusk in a forest pond, in the style of ukiyo-e',\n  providerOptions: {\n    luma: {\n      image_ref: [\n        {\n          url: 'https://example.com/reference.jpg',\n          weight: 0.85,\n        },\n      ],\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Example Bedrock API Response Headers in JavaScript\nDESCRIPTION: Provides a sample JavaScript object representing the typical HTTP response headers received from a Bedrock API call made via the AI SDK's `generateText`. It highlights the `x-amzn-requestid` header, useful for debugging and correlating requests.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_22\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  connection: 'keep-alive',\n  'content-length': '2399',\n  'content-type': 'application/json',\n  date: 'Fri, 07 Feb 2025 04:28:30 GMT',\n  'x-amzn-requestid': 'c9f3ace4-dd5d-49e5-9807-39aedfa47c8e'\n}\n```\n\n----------------------------------------\n\nTITLE: Importing useCompletion Hook - Svelte\nDESCRIPTION: Shows how to import the useCompletion hook in a Svelte application\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/02-use-completion.mdx#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { useCompletion } from '@ai-sdk/svelte'\n```\n\n----------------------------------------\n\nTITLE: Installing Zod Package for Schema Validation\nDESCRIPTION: Commands for installing the Zod schema validation library using different package managers (pnpm, npm, yarn).\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/04-tools.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add zod\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm install zod\n```\n\nLANGUAGE: shell\nCODE:\n```\nyarn add zod\n```\n\n----------------------------------------\n\nTITLE: Configuring Reasoning Effort for o3-mini\nDESCRIPTION: Shows how to adjust the reasoning effort parameter for o3-mini to control the balance between response speed and reasoning depth. The parameter can be set to low, medium, or high.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/24-o3.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n// Reduce reasoning effort for faster responses\nconst { text } = await generateText({\n  model: openai('o3-mini'),\n  prompt: 'Explain quantum entanglement briefly.',\n  providerOptions: {\n    openai: { reasoningEffort: 'low' },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing Custom Metadata from Provider Responses in TypeScript\nDESCRIPTION: Illustrates how to access the custom metadata extracted by a configured `MetadataExtractor`. The extracted data is available in the `providerMetadata` field of the result object returned by functions like `generateText`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/index.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst { text, providerMetadata } = await generateText({\n  model: provider('model-id'),\n  prompt: 'Hello',\n});\n\nconsole.log(providerMetadata.myProvider.customMetric);\n```\n\n----------------------------------------\n\nTITLE: Installing the OpenAI Compatible Provider using npm\nDESCRIPTION: Shows how to install the `@ai-sdk/openai-compatible` package using the Node Package Manager (npm). This package provides the necessary functions to interact with OpenAI-compatible APIs through the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/openai-compatible/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/openai-compatible\n```\n\n----------------------------------------\n\nTITLE: Creating a Bash Tool for computer use\nDESCRIPTION: Shows how to create and use the Bash Tool, which allows running bash commands through the Anthropic API. This tool enables models to interact with the command line.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst bashTool = anthropic.tools.bash_20241022({\n  execute: async ({ command, restart }) => {\n    // Implement your bash command execution logic here\n    // Return the result of the command execution\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Basic smoothStream Usage with streamText in TypeScript\nDESCRIPTION: Demonstrates basic usage of smoothStream utility with streamText function, showing configuration of delay and chunking options.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/80-smooth-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { smoothStream, streamText } from 'ai';\n\nconst result = streamText({\n  model,\n  prompt,\n  experimental_transform: smoothStream({\n    delayInMs: 20, // optional: defaults to 10ms\n    chunking: 'line', // optional: defaults to 'word'\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Cerebras Provider Instance in TypeScript\nDESCRIPTION: Demonstrates how to create a customized Cerebras provider instance with specific configuration options including API key.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/40-cerebras.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createCerebras } from '@ai-sdk/cerebras';\n\nconst cerebras = createCerebras({\n  apiKey: process.env.CEREBRAS_API_KEY ?? '',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Computer Tool for Keyboard and Mouse Control in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a Computer Tool using Anthropic's API. It allows control of keyboard and mouse actions on a computer, including taking screenshots and executing various input commands.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst computerTool = anthropic.tools.computer_20241022({\n  displayWidthPx: 1920,\n  displayHeightPx: 1080,\n  displayNumber: 0, // Optional, for X11 environments\n\n  execute: async ({ action, coordinate, text }) => {\n    // Implement your computer control logic here\n    // Return the result of the action\n\n    // Example code:\n    switch (action) {\n      case 'screenshot': {\n        // multipart result:\n        return {\n          type: 'image',\n          data: fs\n            .readFileSync('./data/screenshot-editor.png')\n            .toString('base64'),\n        };\n      }\n      default: {\n        console.log('Action:', action);\n        console.log('Coordinate:', coordinate);\n        console.log('Text:', text);\n        return `executed ${action}`;\n      }\n    }\n  },\n\n  // map to tool result content for LLM consumption:\n  experimental_toToolResultContent(result) {\n    return typeof result === 'string'\n      ? [{ type: 'text', text: result }]\n      : [{ type: 'image', data: result.data, mimeType: 'image/png' }];\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_NoOutputSpecifiedError in Vercel AI (TypeScript)\nDESCRIPTION: This code demonstrates how to check if a thrown error is an instance of AI_NoOutputSpecifiedError using the isInstance static method from the NoOutputSpecifiedError class provided by the Vercel AI library. The snippet assumes 'ai' is installed as a project dependency. The \"error\" object should be the caught error; if it is an instance of AI_NoOutputSpecifiedError, appropriate error handling logic (highlighted in comments) can be performed. The code requires the 'ai' package and the TypeScript runtime.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-no-output-specified-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { NoOutputSpecifiedError } from 'ai';\\n\\nif (NoOutputSpecifiedError.isInstance(error)) {\\n  // Handle the error\\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Module Name Mapping in Jest (JSON)\nDESCRIPTION: Configures Jest's moduleNameMapper to resolve imports of 'ai/rsc' to the correct dist directory in node_modules. This ensures that Jest can find and import the AI SDK RSC module during test execution. Place this snippet inside your jest.config.js to fix module resolution issues related to 'ai/rsc'. It expects that the AI SDK is installed in node_modules, and the path reflects the default distribution directory structure.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/60-jest-cannot-find-module-ai-rsc.mdx#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"moduleNameMapper\": {\n    \"^ai/rsc$\": \"<rootDir>/node_modules/ai/rsc/dist\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Rendering Index Cards for AI SDK Guides using JSX\nDESCRIPTION: This code snippet uses the IndexCards component to display a set of cards, each representing a guide for building AI applications with the AI SDK. Each card includes a title, description, and link to the full guide.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/index.mdx#2025-04-23_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<IndexCards\n  cards={[\n    {\n      title: 'RAG Chatbot',\n      description:\n        'Learn how to build a retrieval-augmented generation chatbot with the AI SDK.',\n      href: '/docs/guides/rag-chatbot',\n    },\n    {\n      title: 'Multimodal Chatbot',\n      description: 'Learn how to build a multimodal chatbot with the AI SDK.',\n      href: '/docs/guides/multi-modal-chatbot',\n    },\n    {\n      title: 'Get started with Llama 3.1',\n      description: 'Get started with Llama 3.1 using the AI SDK.',\n      href: '/docs/guides/llama-3_1',\n    },\n    {\n      title: 'Get started with OpenAI o1',\n      description: 'Get started with OpenAI o1 using the AI SDK.',\n      href: '/docs/guides/o1',\n    },\n  ]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Defining Frequency Penalty Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `frequencyPenalty` parameter as a number. This influences the model's likelihood to reuse the same words or phrases frequently. The valid range depends on the provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nnumber\n```\n\n----------------------------------------\n\nTITLE: Generating Text with an OpenAI Compatible Provider in TypeScript\nDESCRIPTION: A complete example showing the import of necessary functions, creation of an OpenAI compatible provider instance, and using it with the `generateText` function from the `ai` package to generate text based on a prompt. Requires API key and base URL configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/index.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { generateText } from 'ai';\n\nconst provider = createOpenAICompatible({\n  name: 'provider-name',\n  apiKey: process.env.PROVIDER_API_KEY,\n  baseURL: 'https://api.provider.com/v1',\n});\n\nconst { text } = await generateText({\n  model: provider('model-id'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Retries for Embedding in TypeScript\nDESCRIPTION: This example demonstrates how to set the maximum number of retries for the embedding process using the 'maxRetries' parameter. In this case, retries are disabled by setting the value to 0.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/30-embeddings.mdx#2025-04-23_snippet_4\n\nLANGUAGE: ts\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { embed } from 'ai';\n\nconst { embedding } = await embed({\n  model: openai.embedding('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n  maxRetries: 0, // Disable retries\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Llama 3.1 using DeepInfra Provider\nDESCRIPTION: This snippet demonstrates how to generate text using the Llama 3.1 405B model through DeepInfra provider with the AI SDK. It uses the generateText function to create a simple text completion based on a prompt.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText } from 'ai';\nimport { deepinfra } from '@ai-sdk/deepinfra';\n\nconst { text } = await generateText({\n  model: deepinfra('meta-llama/Meta-Llama-3.1-405B-Instruct'),\n  prompt: 'What is love?',\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata Type for Telemetry in TypeScript\nDESCRIPTION: Defines the complex type for the optional `metadata` field within `TelemetrySettings`. It's a record where keys are strings and values can be string, number, boolean, or arrays of nullable strings, numbers, or booleans.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_22\n\nLANGUAGE: typescript\nCODE:\n```\nRecord<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\n```\n\n----------------------------------------\n\nTITLE: Setting Timeout for Embedding Process in TypeScript\nDESCRIPTION: This snippet shows how to set a timeout for the embedding process using the 'abortSignal' parameter. It uses the AbortSignal.timeout() method to abort the embedding after a specified duration (1 second in this example).\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/30-embeddings.mdx#2025-04-23_snippet_5\n\nLANGUAGE: ts\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { embed } from 'ai';\n\nconst { embedding } = await embed({\n  model: openai.embedding('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Polyfills in Layout\nDESCRIPTION: TypeScript import statement for including polyfills in the root layout file.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport '@/polyfills';\n```\n\n----------------------------------------\n\nTITLE: Installing Rev.ai Provider for AI SDK\nDESCRIPTION: Command to install the Rev.ai provider module for the AI SDK using npm.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/revai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/revai\n```\n\n----------------------------------------\n\nTITLE: Converting LlamaIndex ChatEngine Stream to Data Stream Response\nDESCRIPTION: Demonstrates how to create a POST endpoint that uses LlamaIndex ChatEngine with OpenAI and converts its output stream to a data stream response. It initializes a chat engine with GPT-4 and processes prompt inputs with streaming enabled.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/16-llamaindex-adapter.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { OpenAI, SimpleChatEngine } from 'llamaindex';\nimport { LlamaIndexAdapter } from 'ai';\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const llm = new OpenAI({ model: 'gpt-4o' });\n  const chatEngine = new SimpleChatEngine({ llm });\n\n  const stream = await chatEngine.chat({\n    message: prompt,\n    stream: true,\n  });\n\n  return LlamaIndexAdapter.toDataStreamResponse(stream);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating xAI Model Instance\nDESCRIPTION: This code snippet shows how to create an instance of an xAI language model. The first argument is the model ID (e.g., 'grok-3'). This instance can then be used with the `generateText` function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = xai('grok-3');\n```\n\n----------------------------------------\n\nTITLE: Streaming JSON Without Schema\nDESCRIPTION: Demonstrates schema-less JSON streaming using streamObject for unstructured data generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/04-stream-object.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\n\nconst { partialObjectStream } = streamObject({\n  model: openai('gpt-4-turbo'),\n  output: 'no-schema',\n  prompt: 'Generate a lasagna recipe.',\n});\n\nfor await (const partialObject of partialObjectStream) {\n  console.clear();\n  console.log(partialObject);\n}\n```\n\n----------------------------------------\n\nTITLE: Processing PDF Files with Azure OpenAI\nDESCRIPTION: Shows how to pass PDF files as message content to Azure OpenAI for processing and analysis using the file type configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: azure.responses('your-deployment-name'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'What is an embedding model?',\n        },\n        {\n          type: 'file',\n          data: fs.readFileSync('./data/ai.pdf'),\n          mimeType: 'application/pdf',\n          filename: 'ai.pdf', // optional\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Object Response with File Buffer Image Input\nDESCRIPTION: Shows how to stream a structured object response using an image provided as a file buffer. Uses the same OpenAI model and Zod schema structure but reads the image from a local file.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/41-stream-object-with-image-prompt.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport dotenv from 'dotenv';\nimport { z } from 'zod';\n\ndotenv.config();\n\nasync function main() {\n  const { partialObjectStream } = streamObject({\n    model: openai('gpt-4-turbo'),\n    maxTokens: 512,\n    schema: z.object({\n      stamps: z.array(\n        z.object({\n          country: z.string(),\n          date: z.string(),\n        }),\n      ),\n    }),\n    messages: [\n      {\n        role: 'user',\n        content: [\n          {\n            type: 'text',\n            text: 'list all the stamps in these passport pages?',\n          },\n          {\n            type: 'image',\n            image: fs.readFileSync('./node/attachments/eclipse.jpg'),\n          },\n        ],\n      },\n    ],\n  });\n\n  for await (const partialObject of partialObjectStream) {\n    console.clear();\n    console.log(partialObject);\n  }\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Importing Internal API from AI SDK OpenAI Compatible Package in TypeScript\nDESCRIPTION: Shows how to import and potentially use internal, non-public functions from the `@ai-sdk/openai-compatible` package by importing from the `/internal` path. This example imports `convertToOpenAICompatibleChatMessages`, which might be necessary for advanced customization or specific compatibility needs not covered by the public API. Using internal APIs is generally discouraged as they can change without notice.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { convertToOpenAICompatibleChatMessages } from '@ai-sdk/openai-compatible/internal';\n```\n\n----------------------------------------\n\nTITLE: Implementing Client Disconnect Handling in Next.js API Route with Vercel AI SDK\nDESCRIPTION: This code snippet shows how to use the 'consumeStream' method from the Vercel AI SDK to handle client disconnects in a Next.js API route. It ensures that the stream from the language model is consumed and stored even if the client disconnects prematurely.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-message-persistence.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\nimport { appendResponseMessages, streamText } from 'ai';\nimport { saveChat } from '@tools/chat-store';\n\nexport async function POST(req: Request) {\n  const { messages, id } = await req.json();\n\n  const result = streamText({\n    model,\n    messages,\n    async onFinish({ response }) {\n      await saveChat({\n        id,\n        messages: appendResponseMessages({\n          messages,\n          responseMessages: response.messages,\n        }),\n      });\n    },\n  });\n\n  // consume the stream to ensure it runs to completion & triggers onFinish\n  // even when the client response is aborted:\n  result.consumeStream(); // no await\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Groq through AI SDK - TypeScript\nDESCRIPTION: This TypeScript example shows how to generate text using the Groq provider in combination with the AI SDK's generateText function. The snippet imports both 'groq' and 'generateText', then asynchronously requests a vegetarian lasagna recipe by calling generateText with the groq model ('gemma2-9b-it') and a prompt. Dependencies: '@ai-sdk/groq' and 'ai' modules must be installed. Expected input is a prompt string, and the output is a destructured object containing the generated text. Ensure API credentials are set if required.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/groq/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { groq } from '@ai-sdk/groq';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: groq('gemma2-9b-it'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing AWSBedrockLlama2Stream in React\nDESCRIPTION: Shows how to import the AWSBedrockLlama2Stream function from the ai package in a React application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/12-aws-bedrock-llama-2-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { AWSBedrockLlama2Stream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL Embeddings Schema with Drizzle ORM\nDESCRIPTION: Defines a PostgreSQL table schema for storing vector embeddings with HNSW indexing using Drizzle ORM. Includes columns for ID, resource reference, content, and vector embeddings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\nimport { nanoid } from '@/lib/utils';\nimport { index, pgTable, text, varchar, vector } from 'drizzle-orm/pg-core';\nimport { resources } from './resources';\n\nexport const embeddings = pgTable(\n  'embeddings',\n  {\n    id: varchar('id', { length: 191 })\n      .primaryKey()\n      .$defaultFn(() => nanoid()),\n    resourceId: varchar('resource_id', { length: 191 }).references(\n      () => resources.id,\n      { onDelete: 'cascade' },\n    ),\n    content: text('content').notNull(),\n    embedding: vector('embedding', { dimensions: 1536 }).notNull(),\n  },\n  table => ({\n    embeddingIndex: index('embeddingIndex').using(\n      'hnsw',\n      table.embedding.op('vector_cosine_ops'),\n    ),\n  }),\n);\n```\n\n----------------------------------------\n\nTITLE: Advanced Transcription with Provider Options\nDESCRIPTION: This example shows how to use Fal's transcription API with additional provider-specific options such as batchSize for parallel processing of audio chunks.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/10-fal.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { fal } from '@ai-sdk/fal';\nimport { readFile } from 'fs/promises';\n\nconst result = await transcribe({\n  model: fal.transcription('wizper'),\n  audio: await readFile('audio.mp3'),\n  providerOptions: { fal: { batchSize: 10 } },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Document OCR with Mistral AI Models\nDESCRIPTION: Example of utilizing document OCR capability with Mistral AI models to process PDF files, including optional image and page limit settings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/20-mistral.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: mistral('mistral-small-latest'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'What is an embedding model according to this document?',\n        },\n        {\n          type: 'file',\n          data: new URL(\n            'https://github.com/vercel/ai/blob/main/examples/ai-core/data/ai.pdf?raw=true',\n          ),\n          mimeType: 'application/pdf',\n        },\n      ],\n    },\n  ],\n  // optional settings:\n  providerOptions: {\n    mistral: {\n      documentImageLimit: 8,\n      documentPageLimit: 64,\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Database Migration Command\nDESCRIPTION: Command to copy the environment configuration file for database setup.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_2\n\nLANGUAGE: txt\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Creating a Groq Transcription Model\nDESCRIPTION: Initialize a Groq transcription model for converting speech to text using the transcription factory method.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/09-groq.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = groq.transcription('whisper-large-v3');\n```\n\n----------------------------------------\n\nTITLE: Implementing Reasoning with Together.ai Models\nDESCRIPTION: Demonstrates how to use middleware to extract reasoning from DeepSeek-R1 model using the <think> tag, enhancing the model with reasoning capabilities.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/24-togetherai.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { togetherai } from '@ai-sdk/togetherai';\nimport { wrapLanguageModel, extractReasoningMiddleware } from 'ai';\n\nconst enhancedModel = wrapLanguageModel({\n  model: togetherai('deepseek-ai/DeepSeek-R1'),\n  middleware: extractReasoningMiddleware({ tagName: 'think' }),\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Stock Display Component in TSX\nDESCRIPTION: React component for displaying stock information. Takes price and symbol as props and renders them in a structured format.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\ntype StockProps = {\n  price: number;\n  symbol: string;\n};\n\nexport const Stock = ({ price, symbol }: StockProps) => {\n  return (\n    <div>\n      <h2>Stock Information</h2>\n      <p>Symbol: {symbol}</p>\n      <p>Price: ${price}</p>\n    </div>\n  );\n};\n```\n\n----------------------------------------\n\nTITLE: Smoothing Azure OpenAI Streams with AI SDK\nDESCRIPTION: This code snippet demonstrates how to use the smoothStream transformation function from the AI SDK to improve streaming performance with Azure OpenAI. It applies a transformation to ensure words are streamed individually rather than in large chunks.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/01-azure-stream-slow.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { smoothStream, streamText } from 'ai';\n\nconst result = streamText({\n  model,\n  prompt,\n  experimental_transform: smoothStream(),\n});\n```\n\n----------------------------------------\n\nTITLE: Importing the Anthropic Provider in TypeScript\nDESCRIPTION: This TypeScript code snippet demonstrates how to import the default `anthropic` provider instance from the installed `@ai-sdk/anthropic` package. This imported instance serves as the entry point for configuring and interacting with Anthropic models via the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/anthropic/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\n```\n\n----------------------------------------\n\nTITLE: Initializing Azure OpenAI Completion Model\nDESCRIPTION: Demonstrates basic completion model initialization and configuration with optional parameters like echo, logitBias, suffix, and user identification.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = azure.completion('your-gpt-35-turbo-instruct-deployment');\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = azure.completion('your-gpt-35-turbo-instruct-deployment', {\n  echo: true, // optional, echo the prompt in addition to the completion\n  logitBias: {\n    // optional likelihood for specific tokens\n    '50256': -100,\n  },\n  suffix: 'some text', // optional suffix that comes after a completion of inserted text\n  user: 'test-user', // optional unique user identifier\n});\n```\n\n----------------------------------------\n\nTITLE: Converting LangChain Expression Language Stream to Data Stream Response\nDESCRIPTION: Demonstrates how to use LangChainAdapter to convert a basic LangChain chat stream into a data stream response. Uses ChatOpenAI model with GPT-3.5-turbo.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/16-langchain-adapter.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\nimport { LangChainAdapter } from 'ai';\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const model = new ChatOpenAI({\n    model: 'gpt-3.5-turbo-0125',\n    temperature: 0,\n  });\n\n  const stream = await model.stream(prompt);\n\n  return LangChainAdapter.toDataStreamResponse(stream);\n}\n```\n\n----------------------------------------\n\nTITLE: Updating UI Framework Hook Imports in TypeScript\nDESCRIPTION: Demonstrates the change in importing UI framework hooks (like `useChat` for Svelte) in AI SDK 4.0. Instead of importing from the main 'ai' package, framework-specific packages like '@ai-sdk/svelte', '@ai-sdk/vue', or '@ai-sdk/solid' must be used.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_33\n\nLANGUAGE: typescript\nCODE:\n```\nimport { useChat } from 'ai/svelte';\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { useChat } from '@ai-sdk/svelte';\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Responses Model with TypeScript\nDESCRIPTION: This snippet demonstrates how to instantiate a response model using the openai.responses() factory method with a specific model ID ('gpt-4o-mini'). It serves as a basis for generating responses or related tasks in a TypeScript environment.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_17\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = openai.responses('gpt-4o-mini');\n```\n\n----------------------------------------\n\nTITLE: Updating Token Usage Type Imports in TypeScript\nDESCRIPTION: Demonstrates the change in token usage type imports for AI SDK 4.0. The types `TokenUsage` and `CompletionTokenUsage` are consolidated into `LanguageModelUsage`, and `EmbeddingTokenUsage` is renamed to `EmbeddingModelUsage`. The import source remains the 'ai' package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_21\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TokenUsage, CompletionTokenUsage, EmbeddingTokenUsage } from 'ai';\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LanguageModelUsage, EmbeddingModelUsage } from 'ai';\n```\n\n----------------------------------------\n\nTITLE: Booking a Flight Using Multiple Tools in a Travel Assistant\nDESCRIPTION: This snippet illustrates tool composition in a flight booking assistant. It shows how searchFlights, lookupFlight, and bookFlight tools are used together to create a seamless booking experience, demonstrating how the model manages context and user inputs across multiple steps.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/09-multistep-interfaces.mdx#2025-04-23_snippet_1\n\nLANGUAGE: txt\nCODE:\n```\nUser: I want to book a flight from New York to London.\nTool: searchFlights(\"New York\", \"London\")\nModel: Here are the available flights from New York to London.\nUser: I want to book flight number BA123 on 12th December for myself and my wife.\nTool: lookupFlight(\"BA123\") -> \"4 seats available\"\nModel: Sure, there are seats available! Can you provide the names of the passengers?\nUser: John Doe and Jane Doe.\nTool: bookFlight(\"BA123\", \"12th December\", [\"John Doe\", \"Jane Doe\"])\nModel: Your flight has been booked!\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Callbacks for useObject Hook in React\nDESCRIPTION: This React component shows how to use the onFinish and onError callbacks provided by the useObject hook. These callbacks can be used for logging, analytics, or custom UI updates.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/08-object-generation.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\nimport { notificationSchema } from './api/notifications/schema';\n\nexport default function Page() {\n  const { object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n    onFinish({ object, error }) {\n      // typed object, undefined if schema validation fails:\n      console.log('Object generation completed:', object);\n\n      // error, undefined if schema validation succeeds:\n      console.log('Schema validation error:', error);\n    },\n    onError(error) {\n      // error during fetch request:\n      console.error('An error occurred:', error);\n    },\n  });\n\n  return (\n    <div>\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Installing xAI Provider for AI SDK via npm\nDESCRIPTION: Command to install the xAI Grok provider package for AI SDK using npm.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/xai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/xai\n```\n\n----------------------------------------\n\nTITLE: Checking AI_InvalidToolArgumentsError Instance - Vercel AI (TypeScript)\nDESCRIPTION: Demonstrates how to check if a caught error is an instance of AI_InvalidToolArgumentsError using the InvalidToolArgumentsError.isInstance static method. This TypeScript snippet requires importing InvalidToolArgumentsError from the 'ai' package. It enables conditionally handling this specific error type, with the input being an unknown error object and triggering custom behavior if it matches. Outputs depend on user-defined handling logic within the conditional block.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-invalid-tool-arguments-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { InvalidToolArgumentsError } from 'ai';\\n\\nif (InvalidToolArgumentsError.isInstance(error)) {\\n  // Handle the error\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Handling Slack Events in TypeScript\nDESCRIPTION: This code defines a POST function that handles incoming Slack events, including URL verification, app mentions, assistant thread messages, and direct messages. It uses the waitUntil function to process AI responses asynchronously.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/03-slackbot.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { SlackEvent } from '@slack/web-api';\nimport {\n  assistantThreadMessage,\n  handleNewAssistantMessage,\n} from '../lib/handle-messages';\nimport { waitUntil } from '@vercel/functions';\nimport { handleNewAppMention } from '../lib/handle-app-mention';\nimport { verifyRequest, getBotId } from '../lib/slack-utils';\n\nexport async function POST(request: Request) {\n  const rawBody = await request.text();\n  const payload = JSON.parse(rawBody);\n  const requestType = payload.type as 'url_verification' | 'event_callback';\n\n  // See https://api.slack.com/events/url_verification\n  if (requestType === 'url_verification') {\n    return new Response(payload.challenge, { status: 200 });\n  }\n\n  await verifyRequest({ requestType, request, rawBody });\n\n  try {\n    const botUserId = await getBotId();\n\n    const event = payload.event as SlackEvent;\n\n    if (event.type === 'app_mention') {\n      waitUntil(handleNewAppMention(event, botUserId));\n    }\n\n    if (event.type === 'assistant_thread_started') {\n      waitUntil(assistantThreadMessage(event));\n    }\n\n    if (\n      event.type === 'message' &&\n      !event.subtype &&\n      event.channel_type === 'im' &&\n      !event.bot_id &&\n      !event.bot_profile &&\n      event.bot_id !== botUserId\n    ) {\n      waitUntil(handleNewAssistantMessage(event, botUserId));\n    }\n\n    return new Response('Success!', { status: 200 });\n  } catch (error) {\n    console.error('Error generating response', error);\n    return new Response('Error generating response', { status: 500 });\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Registering Laminar in Next.js\nDESCRIPTION: Initialize Laminar in a Next.js application using the instrumentation.js file, preventing execution in edge runtime.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/laminar.mdx#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nexport async function register() {\n  // prevent this from running in the edge runtime\n  if (process.env.NEXT_RUNTIME === 'nodejs') {\n    const { Laminar } = await import('@lmnr-ai/lmnr');\n    Laminar.initialize({\n      projectApiKey: process.env.LMNR_API_KEY,\n    });\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating AssistantResponse Parameters\nDESCRIPTION: Shows the removal of threadId and messageId from AssistantResponse callback parameters.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_42\n\nLANGUAGE: typescript\nCODE:\n```\nreturn AssistantResponse(\n  { threadId: myThreadId, messageId: myMessageId },\n  async ({ forwardStream, sendDataMessage, threadId, messageId }) => {\n    // use threadId and messageId here\n  },\n);\n```\n\nLANGUAGE: typescript\nCODE:\n```\nreturn AssistantResponse(\n  { threadId: myThreadId, messageId: myMessageId },\n  async ({ forwardStream, sendDataMessage }) => {\n    // use myThreadId and myMessageId here\n  },\n);\n```\n\n----------------------------------------\n\nTITLE: Initializing Azure AI Provider Instance (TypeScript)\nDESCRIPTION: Imports the createAzure function and initializes a provider instance with endpoint and API key loaded from environment variables. This object (azure) is used as a model provider for subsequent text generation. Requires @quail-ai/azure-ai-provider and assumes environment variables are properly set. Outputs a configured azure provider instance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/14-azure-ai.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAzure } from '@quail-ai/azure-ai-provider';\n\nconst azure = createAzure({\n  endpoint: process.env.AZURE_API_ENDPOINT,\n  apiKey: process.env.AZURE_API_KEY,\n});\n```\n\n----------------------------------------\n\nTITLE: Text Chunking Function Implementation\nDESCRIPTION: Implements a basic text chunking function that splits input text by periods and filters empty chunks for embedding preparation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_7\n\nLANGUAGE: tsx\nCODE:\n```\nconst generateChunks = (input: string): string[] => {\n  return input\n    .trim()\n    .split('.')\n    .filter(i => i !== '');\n};\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Running Development Server\nDESCRIPTION: These commands install the required dependencies and start the development server for the Nuxt project with AI integration.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/nuxt-openai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npnpm install\npnpm dev\n```\n\n----------------------------------------\n\nTITLE: Text Streaming with AI SDK in Nest.js\nDESCRIPTION: This example demonstrates how to use the pipeTextStreamToResponse method to stream text-only output from an AI model to the client. This approach is simpler than data streaming when you only need to return the generated text without additional metadata.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/50-nest.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Controller, Post, Res } from '@nestjs/common';\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport { Response } from 'express';\n\n@Controller()\nexport class AppController {\n  @Post()\n  async example(@Res() res: Response) {\n    const result = streamText({\n      model: openai('gpt-4o'),\n      prompt: 'Invent a new holiday and describe its traditions.',\n    });\n\n    result.pipeTextStreamToResponse(res);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enhanced Flight Booking with Automatic Passenger Information Retrieval\nDESCRIPTION: This example shows an improved version of the flight booking process where the model automatically retrieves passenger information using a lookupContacts tool. This reduces the number of steps required from the user and demonstrates how tool composition can enhance the user experience.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/09-multistep-interfaces.mdx#2025-04-23_snippet_2\n\nLANGUAGE: txt\nCODE:\n```\nUser: I want to book a flight from New York to London.\nTool: searchFlights(\"New York\", \"London\")\nModel: Here are the available flights from New York to London.\nUser: I want to book flight number BA123 on 12th December for myself an my wife.\nTool: lookupContacts() -> [\"John Doe\", \"Jane Doe\"]\nTool: bookFlight(\"BA123\", \"12th December\", [\"John Doe\", \"Jane Doe\"])\nModel: Your flight has been booked!\n```\n\n----------------------------------------\n\nTITLE: Initialize ChromeAI with Custom Settings - TypeScript\nDESCRIPTION: Shows how to instantiate the ChromeAI provider with specific settings for temperature and topK, allowing fine-tuned control of generation randomness and token sampling. Requires the 'chrome-ai' module and supports passing customization options like 'modelId', 'temperature', and 'topK'. Outputs a model instance configured for advanced text generation tasks.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/04-chrome-ai.mdx#2025-04-23_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { chromeai } from 'chrome-ai';\\n\\nconst model = chromeai('generic', {\\n  // additional settings\\n  temperature: 0.5,\\n  topK: 5,\\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Slack Message Handler in TypeScript\nDESCRIPTION: Core function for handling new assistant messages in Slack, including message verification, status updates, conversation history retrieval, and response generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/03-slackbot.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { GenericMessageEvent } from '@slack/web-api';\nimport { client, getThread } from './slack-utils';\nimport { generateResponse } from './ai';\n\nexport async function handleNewAssistantMessage(\n  event: GenericMessageEvent,\n  botUserId: string,\n) {\n  if (\n    event.bot_id ||\n    event.bot_id === botUserId ||\n    event.bot_profile ||\n    !event.thread_ts\n  )\n    return;\n\n  const { thread_ts, channel } = event;\n  const updateStatus = updateStatusUtil(channel, thread_ts);\n  updateStatus('is thinking...');\n\n  const messages = await getThread(channel, thread_ts, botUserId);\n  const result = await generateResponse(messages, updateStatus);\n\n  await client.chat.postMessage({\n    channel: channel,\n    thread_ts: thread_ts,\n    text: result,\n    unfurl_links: false,\n    blocks: [\n      {\n        type: 'section',\n        text: {\n          type: 'mrkdwn',\n          text: result,\n        },\n      },\n    ],\n  });\n\n  updateStatus('');\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a React Client Component with useCompletion Hook for MCP Tools\nDESCRIPTION: This React component uses the useCompletion hook from the AI SDK to connect to the MCP-enabled API endpoint. It provides a simple interface for calling the API with a predefined prompt to schedule a call, and displays the resulting completion text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/73-mcp-tools.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useCompletion } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { completion, complete } = useCompletion({\n    api: '/api/completion',\n  });\n\n  return (\n    <div>\n      <div\n        onClick={async () => {\n          await complete(\n            'Please schedule a call with Sonny and Robby for tomorrow at 10am ET for me!',\n          );\n        }}\n      >\n        Schedule a call\n      </div>\n\n      {completion}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Step Tool Calls with OpenAI and AI SDK\nDESCRIPTION: Example showing how to set up multi-step tool calls using the AI SDK with OpenAI. The code demonstrates a weather tool implementation with configurable maxSteps to prevent infinite loops. The tool accepts a location parameter and returns simulated weather data.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/53-call-tools-multiple-steps.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText, tool } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\nconst { text } = await generateText({\n  model: openai('gpt-4-turbo'),\n  maxSteps: 5,\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }: { location: string }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Text with Portkey and Vercel AI SDK - JavaScript\nDESCRIPTION: This JavaScript snippet illustrates how to use the streamText function together with Portkey's completion model to asynchronously stream generated text chunks. The code sets up a Portkey provider and calls streamText with a prompt, then iterates over the streamed result. Useful for progressively rendering output. Dependencies: @portkey-ai/vercel-provider, ai. The provider must be properly configured.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/10-portkey.mdx#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPortkey } from '@portkey-ai/vercel-provider';\nimport { streamText } from 'ai';\n\nconst portkey = createPortkey({\n  apiKey: 'YOUR_PORTKEY_API_KEY',\n  config: portkeyConfig,\n});\n\nconst result = streamText({\n  model: portkey.completionModel(''),\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n\nfor await (const chunk of result) {\n  console.log(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Fireworks Text Embedding Model\nDESCRIPTION: How to initialize a text embedding model using the Fireworks provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/26-fireworks.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = fireworks.textEmbeddingModel(\n  'accounts/fireworks/models/nomic-embed-text-v1',\n);\n```\n\n----------------------------------------\n\nTITLE: Specifying AI Chat Hook Method Contracts with TypeScript\nDESCRIPTION: This snippet outlines comprehensive TypeScript property and function type definitions for the public API of the AI chat hook. It includes method signatures for messaging, input and data state management, error handling, and advanced actions like appending or reloading chat responses. Dependencies include the TypeScript language and type declarations for React (e.g., React.Dispatch) and JSON value representations. Parameters for each function are described in-line, ensuring correct developer usage with detailed contextual comments and examples of extensibility (such as attachments or custom headers). There are no runtime outputs, as this pure type-level documentation enforces contract correctness and API self-descriptiveness.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#2025-04-23_snippet_9\n\nLANGUAGE: TypeScript\nCODE:\n```\n    {\n      name: 'error',\n      type: 'Error | undefined',\n      description: 'An error object returned by SWR, if any.',\n    },\n    {\n      name: 'append',\n      type: '(message: Message | CreateMessage, options?: ChatRequestOptions) => Promise<string | undefined>',\n      description:\n        'Function to append a message to the chat, triggering an API call for the AI response. It returns a promise that resolves to full response message content when the API call is successfully finished, or throws an error when the API call fails.',\n      properties: [\n        {\n          type: 'ChatRequestOptions',\n          parameters: [\n            {\n              name: 'headers',\n              type: 'Record<string, string> | Headers',\n              description:\n                'Additional headers that should be to be passed to the API endpoint.',\n            },\n            {\n              name: 'body',\n              type: 'object',\n              description:\n                'Additional body JSON properties that should be sent to the API endpoint.',\n            },\n            {\n              name: 'data',\n              type: 'JSONValue',\n              description: 'Additional data to be sent to the API endpoint.',\n            },\n            {\n              name: 'experimental_attachments',\n              type: 'FileList | Array<Attachment>',\n              isOptional: true,\n              description:\n                'An array of attachments to be sent to the API endpoint.',\n              properties: [\n                {\n                  type: 'FileList',\n                  parameters: [\n                    {\n                      name: '',\n                      type: '',\n                      description:\n                        \"A list of files that have been selected by the user using an <input type='file'> element. It's also used for a list of files dropped into web content when using the drag and drop API.\",\n                    },\n                  ],\n                },\n                {\n                  type: 'Attachment',\n                  description:\n                    'An attachment object that can be used to describe the metadata of the file.',\n                  parameters: [\n                    {\n                      name: 'name',\n                      type: 'string',\n                      isOptional: true,\n                      description:\n                        'The name of the attachment, usually the file name.',\n                    },\n                    {\n                      name: 'contentType',\n                      type: 'string',\n                      isOptional: true,\n                      description:\n                        'A string indicating the media type of the file.',\n                    },\n                    {\n                      name: 'url',\n                      type: 'string',\n                      description:\n                        'The URL of the attachment. It can either be a URL to a hosted file or a Data URL.',\n                    },\n                  ],\n                },\n              ],\n            },\n          ],\n        },\n      ],\n    },\n    {\n      name: 'reload',\n      type: '(options?: ChatRequestOptions) => Promise<string | undefined>',\n      description:\n        \"Function to reload the last AI chat response for the given chat history. If the last message isn't from the assistant, it will request the API to generate a new response.\",\n      properties: [\n        {\n          type: 'ChatRequestOptions',\n          parameters: [\n            {\n              name: 'headers',\n              type: 'Record<string, string> | Headers',\n              description:\n                'Additional headers that should be to be passed to the API endpoint.',\n            },\n            {\n              name: 'body',\n              type: 'object',\n              description:\n                'Additional body JSON properties that should be sent to the API endpoint.',\n            },\n            {\n              name: 'data',\n              type: 'JSONValue',\n              description: 'Additional data to be sent to the API endpoint.',\n            },\n          ],\n        },\n      ],\n    },\n    {\n      name: 'stop',\n      type: '() => void',\n      description: 'Function to abort the current API request.',\n    },\n    {\n      name: 'setMessages',\n      type: '(messages: Message[] | ((messages: Message[]) => Message[]) => void',\n      description:\n        'Function to update the `messages` state locally without triggering an API call.',\n    },\n    {\n      name: 'input',\n      type: 'string',\n      description: 'The current value of the input field.',\n    },\n    {\n      name: 'setInput',\n      type: 'React.Dispatch<React.SetStateAction<string>>',\n      description: 'Function to update the `input` value.',\n    },\n    {\n      name: 'handleInputChange',\n      type: '(event: any) => void',\n      description:\n        \"Handler for the `onChange` event of the input field to control the input's value.\",\n    },\n    {\n      name: 'handleSubmit',\n      type: '(event?: { preventDefault?: () => void }, options?: ChatRequestOptions) => void',\n      description:\n        'Form submission handler that automatically resets the input field and appends a user message. You can use the `options` parameter to send additional data, headers and more to the server.',\n      properties: [\n        {\n          type: 'ChatRequestOptions',\n          parameters: [\n            {\n              name: 'headers',\n              type: 'Record<string, string> | Headers',\n              description:\n                'Additional headers that should be to be passed to the API endpoint.',\n            },\n            {\n              name: 'body',\n              type: 'object',\n              description:\n                'Additional body JSON properties that should be sent to the API endpoint.',\n            },\n            {\n              name: 'data',\n              type: 'JSONValue',\n              description: 'Additional data to be sent to the API endpoint.',\n            },\n            {\n              name: 'allowEmptySubmit',\n              type: 'boolean',\n              isOptional: true,\n              description:\n                'A boolean that determines whether to allow submitting an empty input that triggers a generation. Defaults to `false`.',\n            },\n            {\n              name: 'experimental_attachments',\n              type: 'FileList | Array<Attachment>',\n              isOptional: true,\n              description:\n                'An array of attachments to be sent to the API endpoint.',\n              properties: [\n                {\n                  type: 'FileList',\n                  parameters: [\n                    {\n                      name: '',\n                      type: '',\n                      description:\n                        \"A list of files that have been selected by the user using an <input type='file'> element. It's also used for a list of files dropped into web content when using the drag and drop API.\",\n                    },\n                  ],\n                },\n                {\n                  type: 'Attachment',\n                  description:\n                    'An attachment object that can be used to describe the metadata of the file.',\n                  parameters: [\n                    {\n                      name: 'name',\n                      type: 'string',\n                      isOptional: true,\n                      description:\n                        'The name of the attachment, usually the file name.',\n                    },\n                    {\n                      name: 'contentType',\n                      type: 'string',\n                      isOptional: true,\n                      description:\n                        'A string indicating the media type of the file.',\n                    },\n                    {\n                      name: 'url',\n                      type: 'string',\n                      description:\n                        'The URL of the attachment. It can either be a URL to a hosted file or a Data URL.',\n                    },\n                  ],\n                },\n              ],\n            },\n          ],\n        },\n      ],\n    },\n    {\n      name: 'status',\n      type: '\"submitted\" | \"streaming\" | \"ready\" | \"error\"',\n      description:\n        'Status of the chat request: submitted (message sent to API), streaming (receiving response chunks), ready (response complete), or error (request failed).',\n    },\n    {\n      name: 'id',\n      type: 'string',\n      description: 'The unique identifier of the chat.',\n    },\n    {\n      name: 'data',\n      type: 'JSONValue[]',\n      description: 'Data returned from StreamData.',\n    },\n    {\n      name: 'setData',\n      type: '(data: JSONValue[] | undefined | ((data: JSONValue[] | undefined) => JSONValue[] | undefined)) => void',\n      description:\n        'Function to update the `data` state which contains data from StreamData.',\n    },\n    {\n      name: 'addToolResult',\n      type: '({toolCallId: string; result: any;}) => void',\n      description:\n        'Function to add a tool result to the chat. This will update the chat messages with the tool result and call the API route if all tool results for the last message are available.',\n    },\n```\n\n----------------------------------------\n\nTITLE: Updating UI to Display Tool Invocations in Vue\nDESCRIPTION: This snippet demonstrates how to modify the Vue component to handle and display different message parts, including tool invocations. It shows how to iterate over message parts and conditionally render text or tool invocation results.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#2025-04-23_snippet_5\n\nLANGUAGE: vue\nCODE:\n```\n<script setup lang=\"ts\">\nimport { useChat } from '@ai-sdk/vue';\n\nconst { messages, input, handleSubmit } = useChat();\n</script>\n\n<template>\n  <div>\n    <div\n      v-for=\"m in messages\"\n      :key=\"m.id ? m.id : index\"\n    >\n      {{ m.role === 'user' ? 'User: ' : 'AI: ' }}\n      <div v-for=\"part in m.parts\" :key=\"part.id\">\n        <div v-if=\"part.type === 'text'\">{{ part.text }}</div>\n        <div v-if=\"part.type === 'tool-invocation'\">\n          {{ part.toolInvocation }}\n        </div>\n      </div>\n    </div>\n\n    <form @submit=\"handleSubmit\">\n      <input\n        v-model=\"input\"\n        placeholder=\"Say something...\"\n      />\n    </form>\n  </div>\n</template>\n```\n\n----------------------------------------\n\nTITLE: Creating Fireworks Language Model Instance\nDESCRIPTION: How to initialize a Fireworks language model by specifying the model ID.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/26-fireworks.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = fireworks('accounts/fireworks/models/firefunction-v1');\n```\n\n----------------------------------------\n\nTITLE: Using Versioned Replicate Models (TypeScript)\nDESCRIPTION: Shows how to call a versioned model on Replicate by specifying the model ID with a colon and a version hash. This is required when a specific version of a model is needed for reproducibility or to use certain model capabilities. Inputs include the fully-qualified model version string and a prompt; output is an image object.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/60-replicate.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { replicate } from '@ai-sdk/replicate';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: replicate.image(\n    'bytedance/sdxl-lightning-4step:5599ed30703defd1d160a25a63321b4dec97101d98b4674bcc56e41f62f35637',\n  ),\n  prompt: 'The Loch Ness Monster getting a manicure',\n});\n```\n\n----------------------------------------\n\nTITLE: Starting SSE Transport Server\nDESCRIPTION: Command to start the Server-Sent Events (SSE) transport server. This initializes the server-side component for the SSE-based example.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#2025-04-23_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npnpm sse:server\n```\n\n----------------------------------------\n\nTITLE: Marking Step Start Event in AI Stream - TypeScript\nDESCRIPTION: Specifies the event structure for the beginning of a new processing step, including the assistant message ID, request metadata, and any model warnings. It supports tracking complex multi-step interactions with external model providers, requiring metadata composition for requests and responses. Inputs originate from model orchestration logic; output is event emission for workflow stepping.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_22\n\nLANGUAGE: TypeScript\nCODE:\n```\nparameters: [\n  {\n    name: 'type',\n    type: '\\'step-start\\'',\n    description: 'Indicates the start of a new step in the stream.',\n  },\n  {\n    name: 'messageId',\n    type: 'string',\n    description: 'The ID of the assistant message that started the step.',\n  },\n  {\n    name: 'request',\n    type: 'RequestMetadata',\n    description: 'Information about the request that was sent to the language model provider.',\n    properties: [\n      {\n        type: 'RequestMetadata',\n        parameters: [\n          {\n            name: 'body',\n            type: 'string',\n            description: 'Raw request HTTP body that was sent to the provider API as a string.',\n          }\n        ]\n      }\n    ]\n  },\n  {\n    name: 'warnings',\n    type: 'Warning[]',\n    description: 'Warnings from the model provider (e.g. unsupported settings).',\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Vertex AI Imagen Generation with Provider Options in TypeScript\nDESCRIPTION: Demonstrates providing specific configuration options for the Google Vertex AI image generation model ('imagen-3.0-generate-001'). It uses the `providerOptions` field within the `generateImage` call, specifying Vertex-specific settings like `negativePrompt` under the `vertex` key. The `GoogleVertexImageProviderOptions` type is used for validation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertex } from '@ai-sdk/google-vertex';\nimport { GoogleVertexImageProviderOptions } from '@ai-sdk/google-vertex';\nimport { generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: vertex.image('imagen-3.0-generate-001'),\n  providerOptions: {\n    vertex: {\n      negativePrompt: 'pixelated, blurry, low-quality',\n    } satisfies GoogleVertexImageProviderOptions,\n  },\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Completion Model with openai.completion for GPT-3.5 Turbo-Instruct\nDESCRIPTION: This snippet illustrates how to instantiate a completion model targeting GPT-3.5-Turbo-Instruct using the openai.completion() factory method. It also demonstrates how to specify additional optional settings like 'echo', 'logitBias', 'suffix', and 'user' for customizing the completion request.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_25\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst model = openai.completion('gpt-3.5-turbo-instruct');\n\n// With specific options:\nconst model = openai.completion('gpt-3.5-turbo-instruct', {\n  echo: true, // optional, echo the prompt in addition to the completion\n  logitBias: {\n    // optional likelihood for specific tokens\n    '50256': -100,\n  },\n  suffix: 'some text', // optional suffix that comes after a completion of inserted text\n  user: 'test-user', // optional unique user identifier\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Baseten Provider Instance - TypeScript\nDESCRIPTION: Demonstrates how to create a custom Baseten provider instance using createOpenAICompatible from @ai-sdk/openai-compatible in TypeScript. Requires the BASETEN_API_KEY environment variable and a deployed BASETEN model ID. The BASETEN_MODEL_URL is constructed from the model ID, and bearer authentication is set up using the API key.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/40-baseten.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\n\nconst BASETEN_MODEL_ID = '<model-id>'; // e.g. 5q3z8xcw\nconst BASETEN_MODEL_URL = `https://model-${BASETEN_MODEL_ID}.api.baseten.co/environments/production/sync/v1`;\n\nconst baseten = createOpenAICompatible({\n  name: 'baseten',\n  baseURL: BASETEN_MODEL_URL,\n  headers: {\n    Authorization: `Bearer ${process.env.BASETEN_API_KEY ?? ''}`,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Speech with Hume Provider and Vercel AI SDK (TypeScript)\nDESCRIPTION: This TypeScript example demonstrates generating speech using the Vercel AI SDK's `experimental_generateSpeech` function and the Hume provider. It imports the `hume` instance and the speech generation function, then calls `generateSpeech` specifying the Hume speech model ('aurora') and the text ('Hello, world!') to be synthesized. Requires `@ai-sdk/hume` and `ai` packages installed.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/hume/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { hume } from '@ai-sdk/hume';\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\n\nconst result = await generateSpeech({\n  model: hume.speech('aurora'),\n  text: 'Hello, world!',\n});\n```\n\n----------------------------------------\n\nTITLE: Using Custom LangSmith Client with AISDKExporter\nDESCRIPTION: Example of passing a custom LangSmith client instance into the AISDKExporter constructor.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AISDKExporter } from 'langsmith/vercel';\nimport { Client } from 'langsmith';\n\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nimport { NodeSDK } from '@opentelemetry/sdk-node';\nimport { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';\n\nconst langsmithClient = new Client({});\n\nconst sdk = new NodeSDK({\n  traceExporter: new AISDKExporter({ client: langsmithClient }),\n  instrumentations: [getNodeAutoInstrumentations()],\n});\n\nsdk.start();\n\nawait generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  experimental_telemetry: AISDKExporter.getSettings(),\n});\n```\n\n----------------------------------------\n\nTITLE: Provider Options at Message Part Level in TypeScript\nDESCRIPTION: This snippet shows how to set provider-specific options at the message part level for both text and image content with OpenAI.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst messages = [\n  {\n    role: 'user',\n    content: [\n      {\n        type: 'text',\n        text: 'Describe the image in detail.',\n        providerOptions: {\n          openai: { imageDetail: 'low' },\n        },\n      },\n      {\n        type: 'image',\n        image:\n          'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',\n        // Sets image detail configuration for image part:\n        providerOptions: {\n          openai: { imageDetail: 'low' },\n        },\n      },\n    ],\n  },\n];\n```\n\n----------------------------------------\n\nTITLE: Database Migration Command\nDESCRIPTION: Command to run database migrations for setting up the initial schema.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_5\n\nLANGUAGE: txt\nCODE:\n```\npnpm db:migrate\n```\n\n----------------------------------------\n\nTITLE: Installing and Building Vercel AI SDK\nDESCRIPTION: These commands install dependencies and build the Vercel AI SDK project. They should be run from the root directory of the AI SDK repository before running any examples.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/ai-core/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npnpm install\npnpm build\n```\n\n----------------------------------------\n\nTITLE: Creating API Route for Chat with Tool Definitions in Next.js\nDESCRIPTION: This code snippet shows how to create an API route handler for a chat application using the AI SDK. It defines a getWeatherInformation tool without an execute function to allow frontend interception.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#2025-04-23_snippet_1\n\nLANGUAGE: ts\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { createDataStreamResponse, streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  return createDataStreamResponse({\n    execute: async dataStream => {\n      const result = streamText({\n        model: openai('gpt-4o'),\n        messages,\n        tools: {\n          getWeatherInformation: tool({\n            description: 'show the weather in a given city to the user',\n            parameters: z.object({ city: z.string() }),\n            // execute function removed to stop automatic execution\n          }),\n        },\n      });\n\n      result.mergeIntoDataStream(dataStream);\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Google Vertex Gemini in Node.js using AI SDK\nDESCRIPTION: Demonstrates basic text generation using the `gemini-1.5-flash` model via the default Google Vertex provider (`@ai-sdk/google-vertex`) within a Node.js environment. It imports `vertex` and `generateText`, then calls `generateText` with the specified model and prompt. Requires standard Google Cloud authentication, commonly set via the `GOOGLE_APPLICATION_CREDENTIALS` environment variable.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google-vertex/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertex } from '@ai-sdk/google-vertex';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: vertex('gemini-1.5-flash'),\n  prompt: 'Write a vegetarian lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing LMNT Provider Instance (TypeScript)\nDESCRIPTION: Imports the default LMNT provider instance named `lmnt` from the installed `@ai-sdk/lmnt` package. This instance is used to configure and interact with the LMNT API through the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/lmnt/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { lmnt } from '@ai-sdk/lmnt';\n```\n\n----------------------------------------\n\nTITLE: Handling Warnings - AI SDK\nDESCRIPTION: This code snippet shows how to access warnings returned by the image model.  Warnings are accessed via the `warnings` property in the response from `generateImage`. This allows developers to address potential issues that may affect image generation, such as unsupported parameters. This requires the `ai` and a model provider like `@ai-sdk/openai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_10\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst { image, warnings } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing the Default Cerebras Provider Instance (TypeScript)\nDESCRIPTION: This snippet shows how to import the default Cerebras provider instance named 'cerebras' from the '@ai-sdk/cerebras' package in a TypeScript codebase. The import statement is required to access the Cerebras models via the SDK and serves as a prerequisite for any further interaction, such as text generation. Ensure that the '@ai-sdk/cerebras' dependency is installed before using this import.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/cerebras/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { cerebras } from '@ai-sdk/cerebras';\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat UI with Tool Call Feedback\nDESCRIPTION: React component implementing the chat interface with support for displaying tool calls and user interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_15\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      <div className=\"space-y-4\">\n        {messages.map(m => (\n          <div key={m.id} className=\"whitespace-pre-wrap\">\n            <div>\n              <div className=\"font-bold\">{m.role}</div>\n              <p>\n                {m.content.length > 0 ? (\n                  m.content\n                ) : (\n                  <span className=\"italic font-light\">\n                    {'calling tool: ' + m?.toolInvocations?.[0].toolName}\n                  </span>\n                )}\n              </p>\n            </div>\n          </div>\n        ))}\n      </div>\n\n      <form onSubmit={handleSubmit}>\n        <input\n          className=\"fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing useObject Hook in React Client Component\nDESCRIPTION: Example demonstrating how to use the useObject hook to handle streaming JSON objects. The component sets up an API endpoint, defines a schema, and renders a button to trigger object generation with the result displayed as text.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/03-use-object.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { object, submit } = useObject({\n    api: '/api/use-object',\n    schema: z.object({ content: z.string() }),\n  });\n\n  return (\n    <div>\n      <button onClick={() => submit('example input')}>Generate</button>\n      {object?.content && <p>{object.content}</p>}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI provider for AI SDK Core\nDESCRIPTION: Command to install the OpenAI provider package for AI SDK Core. This is required to use OpenAI's models with the SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @ai-sdk/openai\n```\n\n----------------------------------------\n\nTITLE: Importing Gladia Provider in TypeScript\nDESCRIPTION: Code snippet demonstrating how to import the default Gladia provider instance from the @ai-sdk/gladia package.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/gladia/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { gladia } from '@ai-sdk/gladia';\n```\n\n----------------------------------------\n\nTITLE: Importing GoogleGenerativeAIStream in React\nDESCRIPTION: Shows how to import the GoogleGenerativeAIStream function from the AI package for use in React applications.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/14-google-generative-ai-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { GoogleGenerativeAIStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Basic Implementation of Claude 3.7 Sonnet with AI SDK Core\nDESCRIPTION: Demonstrates how to use the AI SDK to call Claude 3.7 Sonnet. This snippet shows the minimal code required to generate text responses using Anthropic's direct API.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/20-sonnet-3-7.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { generateText } from 'ai';\n\nconst { text, reasoning, reasoningDetails } = await generateText({\n  model: anthropic('claude-3-7-sonnet-20250219'),\n  prompt: 'How many people will live in the world in 2040?',\n});\nconsole.log(text); // text response\n```\n\n----------------------------------------\n\nTITLE: Recording Patch Changes for AssemblyAI SDK v0.0.1\nDESCRIPTION: Documents the addition of transcribe functionality to the AssemblyAI provider package through patch version 0.0.1\nSOURCE: https://github.com/vercel/ai/blob/main/packages/assemblyai/CHANGELOG.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# @ai-sdk/assemblyai\n\n## 0.0.1\n\n### Patch Changes\n\n- cb05e9c: feat(providers/assemblyai): add transcribe\n```\n\n----------------------------------------\n\nTITLE: Installing Qwen AI Provider using Shell Commands\nDESCRIPTION: These snippets show how to install the 'qwen-ai-provider' npm package using three popular package managers: pnpm, npm, and yarn. Installation is the prerequisite to using any code examples that require the provider. The commands should be run in your project root; no input or output is expected other than package installation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/02-qwen.mdx#2025-04-23_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npnpm add qwen-ai-provider\n```\n\nLANGUAGE: Shell\nCODE:\n```\nnpm install qwen-ai-provider\n```\n\nLANGUAGE: Shell\nCODE:\n```\nyarn add qwen-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Configuring Polyfills Setup\nDESCRIPTION: TypeScript implementation for setting up polyfills in non-web environments, including structured clone and text encoding streams.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Platform } from 'react-native';\nimport structuredClone from '@ungap/structured-clone';\n\nif (Platform.OS !== 'web') {\n  const setupPolyfills = async () => {\n    const { polyfillGlobal } = await import(\n      'react-native/Libraries/Utilities/PolyfillFunctions'\n    );\n\n    const { TextEncoderStream, TextDecoderStream } = await import(\n      '@stardazed/streams-text-encoding'\n    );\n\n    if (!('structuredClone' in global)) {\n      polyfillGlobal('structuredClone', () => structuredClone);\n    }\n\n    polyfillGlobal('TextEncoderStream', () => TextEncoderStream);\n    polyfillGlobal('TextDecoderStream', () => TextDecoderStream);\n  };\n\n  setupPolyfills();\n}\n\nexport {};\n```\n\n----------------------------------------\n\nTITLE: Tracing AI SDK Text Generation in Node.js\nDESCRIPTION: Example of using AISDKExporter to trace text generation with the AI SDK in a Node.js environment.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { AISDKExporter } from 'langsmith/vercel';\n\nconst result = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  experimental_telemetry: AISDKExporter.getSettings(),\n});\n\nawait sdk.shutdown();\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Provider: Facade Replacement - TypeScript\nDESCRIPTION: Replaces the OpenAI facade class instantiation with the factory function createOpenAI for provider initialization. Applies to AI SDK 4.0+ codebases. Expects configuration object input and returns an OpenAI provider instance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_8\n\nLANGUAGE: ts\nCODE:\n```\nconst openai = new OpenAI({\n  // ...\n});\n```\n\nLANGUAGE: ts\nCODE:\n```\nconst openai = createOpenAI({\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Installing AI Gateway Provider with Package Managers - bash\nDESCRIPTION: Demonstrates how to install the ai-gateway-provider module using pnpm, npm, or yarn. These commands are prerequisites for using the provider in your project. Each command line snippet adds the package to your project's dependencies, thus enabling further code examples and usage shown in the documentation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/12-cloudflare-ai-gateway.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add ai-gateway-provider\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install ai-gateway-provider\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add ai-gateway-provider\n```\n\n----------------------------------------\n\nTITLE: Checking Error Instance with Vercel AI_EmptyResponseBodyError in TypeScript\nDESCRIPTION: This TypeScript snippet shows how to check if a given error object is an instance of the AI_EmptyResponseBodyError using the Vercel AI package. It imports the EmptyResponseBodyError class and uses its static isInstance method to determine if error handling logic should be triggered for this specific case. Dependencies include the 'ai' library, and the key parameter is the error object to check. The snippet expects that the 'ai' package is installed and imported, and that errors occurring in the app may originate from empty server response bodies.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-empty-response-body-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { EmptyResponseBodyError } from 'ai';\n\nif (EmptyResponseBodyError.isInstance(error)) {\n  // Handle the error\n}\n```\n\n----------------------------------------\n\nTITLE: Defining File Upload Stream Part Schema - TypeScript\nDESCRIPTION: This snippet specifies the parameters for a TextStreamPart object that represents a file part in a streaming interaction. Dependencies include the Uint8Array type for binary data and requires MIME type information. Inputs are a file in Uint8Array format and a string MIME type; outputs are structured objects representing the file segment for streaming purposes. It ensures type safety for file-based interactions in the AI streaming API.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_17\n\nLANGUAGE: TypeScript\nCODE:\n```\nparameters: [\n  {\n    name: 'file',\n    type: 'Uint8Array',\n    description: 'File as a Uint8Array.',\n  },\n  {\n    name: 'mimeType',\n    type: 'string',\n    description: 'MIME type of the file.',\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Initializing StreamData Instance\nDESCRIPTION: Demonstrates how to create a new instance of the StreamData class.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/45-stream-data.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst data = new StreamData();\n```\n\n----------------------------------------\n\nTITLE: Initializing Cloudflare AI Gateway Provider with AI Binding - TypeScript\nDESCRIPTION: Shows how to initialize the AI Gateway Provider in a Cloudflare Worker context using an AI binding configured in the wrangler.toml file. The binding property references the AI gateway via env.AI.gateway(). Optional settings like skipCache can be specified. This method is only valid within Cloudflare Workers and assumes proper wrangler.toml configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/12-cloudflare-ai-gateway.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAiGateway } from 'ai-gateway-provider';\n\nconst aigateway = createAiGateway({\n  binding: env.AI.gateway('my-gateway'),\n  options: {\n    skipCache: true, // Optional request-level settings\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing @quail-ai/azure-ai-provider via npm (Bash)\nDESCRIPTION: Installs the @quail-ai/azure-ai-provider package using npm, which is required for accessing Azure AI models through the Quail-AI provider. This step must be performed prior to any code usage and assumes Node.js/npm are installed on the system. Outputs no value but adds the package to your node_modules directory.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/14-azure-ai.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @quail-ai/azure-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Zhipu Provider Example (TypeScript)\nDESCRIPTION: Demonstrates a basic usage example of the Zhipu provider with the Vercel AI SDK's `generateText` function. It imports the default `zhipu` instance, specifies a Zhipu model (e.g., 'glm-4-plus'), provides a prompt, and awaits the generated text response. The result (containing the generated text) is then logged to the console.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/95-zhipu.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { zhipu } from 'zhipu-ai-provider';\n\nconst { text } = await generateText({\n  model: zhipu('glm-4-plus'),\n  prompt: 'Why is the sky blue?',\n});\n\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Implementing AI-powered Completion UI with AI SDK in Next.js\nDESCRIPTION: This client-side code snippet shows how to create a simple UI for AI-powered completions using the AI SDK's useCompletion hook in a Next.js application. It handles user input and displays the completion result.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/04-adapters/01-langchain.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useCompletion } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { completion, input, handleInputChange, handleSubmit } =\n    useCompletion();\n\n  return (\n    <div>\n      {completion}\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n      </form>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Streaming Errors with Error Support (TypeScript)\nDESCRIPTION: This snippet illustrates how to handle streaming errors in full streams that support error parts. It uses a switch statement to handle different part types, including error parts, and includes a try/catch block for errors that occur outside of streaming.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/50-error-handling.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\n\ntry {\n  const { fullStream } = streamText({\n    model: yourModel,\n    prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  });\n\n  for await (const part of fullStream) {\n    switch (part.type) {\n      // ... handle other part types\n\n      case 'error': {\n        const error = part.error;\n        // handle error\n        break;\n      }\n    }\n  }\n} catch (error) {\n  // handle error\n}\n```\n\n----------------------------------------\n\nTITLE: Tracing AI SDK Function Calls in Next.js API Route\nDESCRIPTION: Enable tracing for AI SDK function calls in a Next.js API route by setting experimental_telemetry.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/laminar.mdx#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// /api/.../route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'What is Laminar flow?',\n  experimental_telemetry: {\n    isEnabled: true,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Importing and Using the Transcribe Function in TypeScript\nDESCRIPTION: This snippet demonstrates how to import the transcribe function from the AI SDK, use it with the OpenAI Whisper model, and generate a transcript from an audio file.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/11-transcribe.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { readFile } from 'fs/promises';\n\nconst { transcript } = await transcribe({\n  model: openai.transcription('whisper-1'),\n  audio: await readFile('audio.mp3'),\n});\n\nconsole.log(transcript);\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Google Vertex Anthropic in Edge Runtime using AI SDK\nDESCRIPTION: Demonstrates generating text with an Anthropic Claude model using the `vertexAnthropic` provider specifically for Edge runtimes (`@ai-sdk/google-vertex/anthropic/edge`). It imports `vertexAnthropic` from the `/anthropic/edge` submodule and uses `generateText`. Requires authentication via environment variables as per Application Default Credentials.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google-vertex/README.md#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic/edge';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: vertexAnthropic('claude-3-5-sonnet@20240620'),\n  prompt: 'Write a vegetarian lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Import and Usage of simulateReadableStream\nDESCRIPTION: Demonstrates how to import and create a basic ReadableStream with sequential chunks.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/75-simulate-readable-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { simulateReadableStream } from 'ai';\n\nconst stream = simulateReadableStream({\n  chunks: ['Hello', ' ', 'World'],\n  initialDelayInMs: 100,\n  chunkDelayInMs: 50,\n});\n```\n\n----------------------------------------\n\nTITLE: Closing StreamData Instance\nDESCRIPTION: Shows how to close the stream data instance when streaming is complete.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/45-stream-data.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ndata.close();\n```\n\n----------------------------------------\n\nTITLE: Using Perplexity Sonar Model for Web Search in TypeScript\nDESCRIPTION: This code shows how to use Perplexity's Sonar model which combines real-time web search with natural language processing. It implements the AI SDK to generate text that is grounded in current web data with detailed citations when querying about quantum computing developments.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/56-web-search-agent.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { perplexity } from '@ai-sdk/perplexity';\nimport { generateText } from 'ai';\n\nconst { text, sources } = await generateText({\n  model: perplexity('sonar-pro'),\n  prompt: 'What are the latest developments in quantum computing?',\n});\n\nconsole.log(text);\nconsole.log(sources);\n```\n\n----------------------------------------\n\nTITLE: Rendering Components within Server Action (RSC)\nDESCRIPTION: This snippet shows how to render components within a server action and stream them to the client using streamUI in AI SDK RSC.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { z } from 'zod';\nimport { streamUI } from 'ai/rsc';\nimport { openai } from '@ai-sdk/openai';\nimport { getWeather } from '@/utils/queries';\nimport { Weather } from '@/components/weather';\n\nconst { value: stream } = await streamUI({\n  model: openai('gpt-4o'),\n  system: 'you are a friendly assistant!',\n  messages,\n  text: async function* ({ content, done }) {\n    // process text\n  },\n  tools: {\n    displayWeather: {\n      description: 'Display the weather for a location',\n      parameters: z.object({\n        latitude: z.number(),\n        longitude: z.number(),\n      }),\n      generate: async function* ({ latitude, longitude }) {\n        yield <div>Loading weather...</div>;\n\n        const { value, unit } = await getWeather({ latitude, longitude });\n\n        return <Weather value={value} unit={unit} />;\n      },\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Processing Base64-encoded PDF with Claude 3.5 Sonnet Model in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the Claude 3.5 Sonnet model to process a locally stored PDF file. It shows how to read the file, encode it as Base64, and pass it as part of the message content for analysis.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: anthropic('claude-3-5-sonnet-20241022'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'What is an embedding model according to this document?',\n        },\n        {\n          type: 'file',\n          data: fs.readFileSync('./data/ai.pdf'),\n          mimeType: 'application/pdf',\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Streamable UIs with Stock Data\nDESCRIPTION: Shows how to implement nested streamable UI components where one streamable component is passed as a prop to another component. Includes handling of loading states and async data fetching.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/05-multiple-streamables.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nasync function getStockHistoryChart({ symbol: string }) {\n  'use server';\n\n  const ui = createStreamableUI(<Spinner />);\n\n  // We need to wrap this in an async IIFE to avoid blocking.\n  (async () => {\n    const price = await getStockPrice({ symbol });\n\n    // Show a spinner as the history chart for now.\n    const historyChart = createStreamableUI(<Spinner />);\n    ui.done(<StockCard historyChart={historyChart.value} price={price} />);\n\n    // Getting the history data and then update that part of the UI.\n    const historyData = await fetch('https://my-stock-data-api.com');\n    historyChart.done(<HistoryChart data={historyData} />);\n  })();\n\n  return ui;\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Object with MockLanguageModelV1 in TypeScript\nDESCRIPTION: This snippet demonstrates how to use MockLanguageModelV1 to test the generateObject function. It sets up a mock model that returns a JSON string, which is then parsed according to a Zod schema.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/55-testing.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateObject } from 'ai';\nimport { MockLanguageModelV1 } from 'ai/test';\nimport { z } from 'zod';\n\nconst result = await generateObject({\n  model: new MockLanguageModelV1({\n    defaultObjectGenerationMode: 'json',\n    doGenerate: async () => ({\n      rawCall: { rawPrompt: null, rawSettings: {} },\n      finishReason: 'stop',\n      usage: { promptTokens: 10, completionTokens: 20 },\n      text: `{\"content\":\"Hello, world!\"}`,\n    }),\n  }),\n  schema: z.object({ content: z.string() }),\n  prompt: 'Hello, test!',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Server Actions for Message Handling\nDESCRIPTION: This server component defines interfaces for both server-side and client-side message formats, and provides a function to retrieve saved messages from a database. It uses React Server Components' 'use server' directive for server-side execution.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/61-restore-messages-from-database.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { getAIState } from 'ai/rsc';\n\nexport interface ServerMessage {\n  role: 'user' | 'assistant' | 'function';\n  content: string;\n}\n\nexport interface ClientMessage {\n  id: string;\n  role: 'user' | 'assistant' | 'function';\n  display: ReactNode;\n}\n\n// Function to get saved messages from database\nexport async function getSavedMessages(): Promise<ServerMessage[]> {\n  'use server';\n\n  // Implement your database fetching logic here\n  return await fetchMessagesFromDatabase();\n}\n```\n\n----------------------------------------\n\nTITLE: Logging and Deleting Meals in a Food Tracking Application\nDESCRIPTION: This example demonstrates how application context is maintained across multiple steps in a meal logging application. It shows the use of log_meal and delete_meal tools, and how the model references previous interactions to generate appropriate responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/09-multistep-interfaces.mdx#2025-04-23_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nUser: Log a chicken shawarma for lunch.\nTool: log_meal(\"chicken shawarma\", \"250g\", \"12:00 PM\")\nModel: Chicken shawarma has been logged for lunch.\n...\n...\nUser: I skipped lunch today, can you update my log?\nTool: delete_meal(\"chicken shawarma\")\nModel: Chicken shawarma has been deleted from your log.\n```\n\n----------------------------------------\n\nTITLE: Generating Speech with LMNT Provider (TypeScript)\nDESCRIPTION: Demonstrates how to generate speech using the Vercel AI SDK with the LMNT provider. It imports the `lmnt` provider and the `experimental_generateSpeech` function, then calls `generateSpeech` specifying the LMNT speech model ('aurora') and the input text ('Hello, world!'). This requires the `@ai-sdk/lmnt` and `ai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/lmnt/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { lmnt } from '@ai-sdk/lmnt';\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\n\nconst result = await generateSpeech({\n  model: lmnt.speech('aurora'),\n  text: 'Hello, world!',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Mistral AI Embedding Model\nDESCRIPTION: Shows how to create an embedding model instance using Mistral's 'mistral-embed' model for generating vector embeddings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/20-mistral.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = mistral.embedding('mistral-embed');\n```\n\n----------------------------------------\n\nTITLE: Enabling Model Distillation Storage\nDESCRIPTION: Shows how to flag an AI generation for storage on the OpenAI platform for potential use in model distillation. This is done by setting `store: true` within the `providerOptions.openai` object when calling `generateText`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\nimport 'dotenv/config';\n\nasync function main() {\n  const { text, usage } = await generateText({\n    model: openai('gpt-4o-mini'),\n    prompt: 'Who worked on the original macintosh?',\n    providerOptions: {\n      openai: {\n        store: true,\n        metadata: {\n          custom: 'value',\n        },\n      },\n    },\n  });\n\n  console.log(text);\n  console.log();\n  console.log('Usage:', usage);\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Installing Crosshatch Provider via pnpm - Shell\nDESCRIPTION: Installs the '@crosshatch/ai-provider' NPM package using pnpm, which is a fast, disk space efficient package manager. This command ensures the Crosshatch provider is available for import in your TypeScript or JavaScript project. Requires pnpm to be installed and run in the root of your project directory.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/21-crosshatch.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add @crosshatch/ai-provider\n```\n\n----------------------------------------\n\nTITLE: Configuring a Custom Google Vertex Anthropic Provider in Node.js using AI SDK\nDESCRIPTION: Example of creating a custom Google Vertex Anthropic provider instance in Node.js using `createVertexAnthropic` from `@ai-sdk/google-vertex/anthropic`. This allows setting specific configuration like `project` and `location`. The resulting custom provider function is then used to specify the model (e.g., `claude-3-5-sonnet@20240620`) when calling `generateText`.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google-vertex/README.md#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createVertexAnthropic } from '@ai-sdk/google-vertex/anthropic';\nimport { generateText } from 'ai';\n\nconst customProvider = createVertexAnthropic({\n  project: 'your-project-id',\n  location: 'us-east5',\n});\n\nconst { text } = await generateText({\n  model: customProvider('claude-3-5-sonnet@20240620'),\n  prompt: 'Write a vegetarian lasagna recipe.',\n});\n```\n\n----------------------------------------\n\nTITLE: Using Image Models with Provider Registry\nDESCRIPTION: Demonstrates accessing image generation models through the provider registry.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/45-provider-management.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateImage } from 'ai';\nimport { registry } from './registry';\n\nconst { image } = await generateImage({\n  model: registry.imageModel('openai:dall-e-3'),\n  prompt: 'A beautiful sunset over a calm ocean',\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing the Replicate Provider Instance (TypeScript)\nDESCRIPTION: Illustrates how to create a custom Replicate provider instance using createReplicate, allowing injection of specific settings such as the API token from environment variables. This is useful when working with custom endpoints or headers. The apiToken must be provided for authenticated requests; the example reads it from process.env.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/60-replicate.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createReplicate } from '@ai-sdk/replicate';\n\nconst replicate = createReplicate({\n  apiToken: process.env.REPLICATE_API_TOKEN ?? '',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Custom ElevenLabs Provider Instance in TypeScript\nDESCRIPTION: Demonstrates how to create a customized ElevenLabs provider instance using the `createElevenLabs` factory function. This allows configuring options like a custom `fetch` implementation, specific API keys, or additional headers. The example shows setting a custom `fetch` function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/90-elevenlabs.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createElevenLabs } from '@ai-sdk/elevenlabs';\n\nconst elevenlabs = createElevenLabs({\n  // custom settings, e.g.\n  fetch: customFetch,\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Call Delta Stream Part - TypeScript\nDESCRIPTION: This segment defines the structure for a tool call delta event, used to represent incremental updates to tool call arguments during streaming. It includes fields for type discrimination, tool identifiers, and argument text deltas, supporting interactive or partial argument generation. Dependencies include string handling for delta computation; outputs enable real-time UI updates or stepwise execution.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_20\n\nLANGUAGE: TypeScript\nCODE:\n```\nparameters: [\n  {\n    name: 'type',\n    type: '\\'tool-call-delta\\'',\n    description: 'The type to identify the object as tool call delta. Only available when streaming tool calls.',\n  },\n  {\n    name: 'toolCallId',\n    type: 'string',\n    description: 'The id of the tool call.',\n  },\n  {\n    name: 'toolName',\n    type: 'string',\n    description: 'The name of the tool, which typically would be the name of the function.',\n  },\n  {\n    name: 'argsTextDelta',\n    type: 'string',\n    description: 'The text delta of the tool call arguments.',\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Streaming Text with MockLanguageModelV1 in TypeScript\nDESCRIPTION: This example shows how to use MockLanguageModelV1 and simulateReadableStream to test the streamText function. It simulates a stream of text chunks with delays.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/55-testing.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText, simulateReadableStream } from 'ai';\nimport { MockLanguageModelV1 } from 'ai/test';\n\nconst result = streamText({\n  model: new MockLanguageModelV1({\n    doStream: async () => ({\n      stream: simulateReadableStream({\n        chunks: [\n          { type: 'text-delta', textDelta: 'Hello' },\n          { type: 'text-delta', textDelta: ', ' },\n          { type: 'text-delta', textDelta: `world!` },\n          {\n            type: 'finish',\n            finishReason: 'stop',\n            logprobs: undefined,\n            usage: { completionTokens: 10, promptTokens: 3 },\n          },\n        ],\n      }),\n      rawCall: { rawPrompt: null, rawSettings: {} },\n    }),\n  }),\n  prompt: 'Hello, test!',\n});\n```\n\n----------------------------------------\n\nTITLE: Enabling Multi-Step Tool Calls\nDESCRIPTION: Updated chat component configuration to enable multi-step tool calls with maxSteps parameter.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    maxSteps: 5,\n  });\n\n  // ... rest of your component code\n}\n```\n\n----------------------------------------\n\nTITLE: Importing the Groq Provider Instance - TypeScript\nDESCRIPTION: This snippet demonstrates how to import the default Groq provider instance from the '@ai-sdk/groq' package in a TypeScript file. The import is necessary to access Groq-powered models through the AI SDK. No additional parameters or configuration are required at this stage beyond having installed the dependency. This step should be performed before using the provider in further code.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/groq/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { groq } from '@ai-sdk/groq';\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_MessageConversionError with AI SDK in TypeScript\nDESCRIPTION: Demonstrates how to import the MessageConversionError class from the 'ai' SDK and check if an encountered error is an instance of AI_MessageConversionError using the static isInstance method. Requires the 'ai' package to be installed as a dependency. The key input is the error object, and output is conditional logic flow if the error matches. This snippet is foundational for robust error handling when converting messages using the SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-message-conversion-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MessageConversionError } from 'ai';\\n\\nif (MessageConversionError.isInstance(error)) {\\n  // Handle the error\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Migrating Provider-Specific Options to Responses API\nDESCRIPTION: Shows how to migrate provider-specific options when switching from the Completions API to the Responses API, where options now go in the providerOptions object.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/19-openai-responses.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n// Completions API\nconst { text } = await generateText({\n  model: openai('gpt-4o', { parallelToolCalls: false }),\n  prompt: 'Explain the concept of quantum entanglement.',\n});\n\n// Responses API\nconst { text } = await generateText({\n  model: openai.responses('gpt-4o'),\n  prompt: 'Explain the concept of quantum entanglement.',\n  providerOptions: {\n    openai: {\n      parallelToolCalls: false,\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Replicate Provider for AI SDK\nDESCRIPTION: Command to install the Replicate provider module for the AI SDK using npm.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/replicate/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/replicate\n```\n\n----------------------------------------\n\nTITLE: Importing the Luma Provider Instance (TypeScript)\nDESCRIPTION: Imports the default Luma provider instance (`luma`) from the `@ai-sdk/luma` package. This instance is used to configure and interact with the Luma AI models via the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/luma/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { luma } from '@ai-sdk/luma';\n```\n\n----------------------------------------\n\nTITLE: Creating Customized Ollama Provider Instance (TypeScript)\nDESCRIPTION: Demonstrates how to create a customized Ollama provider instance using the `createOllama` factory function from `ollama-ai-provider`. This allows specifying optional settings like a custom `baseURL` for the Ollama API endpoint or custom `headers` for requests. The default `baseURL` is `http://localhost:11434/api`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/03-ollama.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOllama } from 'ollama-ai-provider';\n\nconst ollama = createOllama({\n  // optional settings, e.g.\n  baseURL: 'https://api.ollama.com',\n});\n```\n\n----------------------------------------\n\nTITLE: Specifying Tool Choice Strategy Type in TypeScript\nDESCRIPTION: Defines the type for the optional `toolChoice` setting, controlling how tools are selected. It accepts literal strings 'auto' (default), 'none' (disable tools), 'required' (force tool use), or an object specifying a particular tool: `{ \"type\": \"tool\", \"toolName\": string }`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n\"auto\" | \"none\" | \"required\" | { \"type\": \"tool\", \"toolName\": string }\n```\n\n----------------------------------------\n\nTITLE: Custom Headers - AI SDK\nDESCRIPTION: This code snippet shows how to add custom headers to the image generation request by using the `headers` parameter in the `generateImage` function. This allows you to customize the request with your specific needs.  This requires the `ai` and a model provider like `@ai-sdk/openai` packages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_9\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: openai.image('dall-e-3'),\n  value: 'sunny day at the beach',\n  headers: { 'X-Custom-Header': 'custom-value' },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring AI Provider with Initial State in React Layout Component\nDESCRIPTION: This code sets up the AI provider component in the root layout, fetching saved messages from a database and passing them as the initial AI state. This enables the application to restore previous conversation history.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/61-restore-messages-from-database.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { ServerMessage } from './actions';\nimport { AI } from './ai';\n\nexport default function RootLayout({\n  children,\n}: Readonly<{\n  children: React.ReactNode;\n}>) {\n  // Fetch stored messages from your database\n  const savedMessages: ServerMessage[] = getSavedMessages();\n\n  return (\n    <html lang=\"en\">\n      <body>\n        <AI initialAIState={savedMessages} initialUIState={[]}>\n          {children}\n        </AI>\n      </body>\n    </html>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Nesting Runs with LangSmith Tracing\nDESCRIPTION: Example of nesting runs within other traced functions to create a hierarchy of associated runs using the traceable method.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AISDKExporter } from 'langsmith/vercel';\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nimport { traceable } from 'langsmith/traceable';\n\nconst wrappedGenerateText = traceable(\n  async (content: string) => {\n    const { text } = await generateText({\n      model: openai('gpt-4o-mini'),\n      messages: [{ role: 'user', content }],\n      experimental_telemetry: AISDKExporter.getSettings(),\n    });\n\n    const reverseText = traceable(\n      async (text: string) => {\n        return text.split('').reverse().join('');\n      },\n      {\n        name: 'reverseText',\n      },\n    );\n\n    const reversedText = await reverseText(text);\n    return { text, reversedText };\n  },\n  { name: 'parentTraceable' },\n);\n\nconst result = await wrappedGenerateText(\n  'What color is the sky? Respond with one word.',\n);\n```\n\n----------------------------------------\n\nTITLE: Tool Execution Type Definition in TypeScript\nDESCRIPTION: Defines the function signature for tool execution with parameters and options. Includes detailed type definitions for tool execution options including toolCallId, messages, and abort signal.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ntype: 'async (parameters: T, options: ToolExecutionOptions) => RESULT',\nproperties: [\n  {\n    type: 'ToolExecutionOptions',\n    parameters: [\n      {\n        name: 'toolCallId',\n        type: 'string',\n        description: 'The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.'\n      },\n      {\n        name: 'messages',\n        type: 'CoreMessage[]',\n        description: 'Messages that were sent to the language model to initiate the response that contained the tool call.'\n      },\n      {\n        name: 'abortSignal',\n        type: 'AbortSignal',\n        description: 'An optional abort signal that indicates that the overall operation should be aborted.'\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI Provider in TypeScript\nDESCRIPTION: Imports the default `openai` provider instance from the installed `@ai-sdk/openai` package. This instance is required to interact with OpenAI models through the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/openai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\n```\n\n----------------------------------------\n\nTITLE: Accessing Source References from Google Vertex Search Grounding (TypeScript)\nDESCRIPTION: Demonstrates how to obtain source citations from model output when search grounding is enabled, using the sources property of the generated text response. The model is configured with the useSearchGrounding flag. Requires '@ai-sdk/google-vertex' and 'ai'.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertex } from '@ai-sdk/google-vertex';\nimport { generateText } from 'ai';\n\nconst { sources } = await generateText({\n  model: vertex('gemini-1.5-pro', { useSearchGrounding: true }),\n  prompt: 'List the top 5 San Francisco news from the past week.',\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Cohere Provider Package - Bash\nDESCRIPTION: Installs the @ai-sdk/cohere package using npm, making Cohere provider modules available for use in a TypeScript or JavaScript project. The command should be executed from your project directory. No additional parameters are required; ensure that npm is installed and initialized in the current repository before running.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/cohere/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/cohere\n```\n\n----------------------------------------\n\nTITLE: Using Groq's DeepSeek R1 Model with AI SDK Core\nDESCRIPTION: This snippet demonstrates how to use Groq's implementation of the DeepSeek R1 model with AI SDK Core. It includes middleware for extracting reasoning tokens and shows how to generate text using the enhanced model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/25-r1.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { groq } from '@ai-sdk/groq';\nimport {\n  generateText,\n  wrapLanguageModel,\n  extractReasoningMiddleware,\n} from 'ai';\n\n// middleware to extract reasoning tokens\nconst enhancedModel = wrapLanguageModel({\n  model: groq('deepseek-r1-distill-llama-70b'),\n  middleware: extractReasoningMiddleware({ tagName: 'think' }),\n});\n\nconst { reasoning, text } = await generateText({\n  model: enhancedModel,\n  prompt: 'Explain quantum entanglement.',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Smooth Streaming for Chinese Text in React with AI SDK\nDESCRIPTION: This code snippet demonstrates how to use the 'smoothStream' function from the AI SDK to implement smooth streaming for Chinese text. It utilizes a specific regex pattern that splits on either words or Chinese characters, enabling proper chunking for a smoother streaming experience.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/23-smooth-stream-chinese.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { smoothStream } from 'ai';\nimport { useChat } from '@ai-sdk/react';\n\nconst { data } = useChat({\n  experimental_transform: smoothStream({\n    chunking: /[\\u4E00-\\u9FFF]|\\S+\\s+/,\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Converting Messages for OpenAI Stream Response in TypeScript\nDESCRIPTION: Example showing how to use convertToCoreMessages with OpenAI's streaming API in a Next.js route handler.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/31-convert-to-core-messages.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { convertToCoreMessages, streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToCoreMessages(messages),\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Top-P (Nucleus Sampling) Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `topP` parameter as a number, used for nucleus sampling. The valid range depends on the provider. It's recommended to set either `temperature` or `topP`, but not both.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nnumber\n```\n\n----------------------------------------\n\nTITLE: Installing LangWatch with Package Managers\nDESCRIPTION: Commands for installing the LangWatch package using different package managers (pnpm, npm, yarn).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langwatch.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add langwatch\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install langwatch\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add langwatch\n```\n\n----------------------------------------\n\nTITLE: Tracking Version History in Changelog - Markdown\nDESCRIPTION: This snippet provides a maintenance log using markdown syntax to document version changes and detailed patch notes for the AI SDK. The content is structured with headings for versions and bullet points for specific changes, including references to feature additions, fixes, and updated dependencies. The snippet assumes familiarity with markdown and does not require external dependencies; it serves as a read-only record and does not take direct input or output, besides visual representation in markdown-supporting tools.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/CHANGELOG.md#2025-04-23_snippet_5\n\nLANGUAGE: Markdown\nCODE:\n```\n### Patch Changes\n\n- 6ac355e: feat (provider/anthropic): add cache control support\n- b56dee1: chore (ai): deprecate prompt helpers\n- Updated dependencies [6ac355e]\n  - @ai-sdk/provider@0.0.20\n  - @ai-sdk/provider-utils@1.0.13\n  - @ai-sdk/ui-utils@0.0.33\n  - @ai-sdk/react@0.0.45\n  - @ai-sdk/solid@0.0.36\n  - @ai-sdk/svelte@0.0.38\n  - @ai-sdk/vue@0.0.37\n\n## 3.3.8\n\n### Patch Changes\n\n- Updated dependencies [dd712ac]\n  - @ai-sdk/provider-utils@1.0.12\n  - @ai-sdk/ui-utils@0.0.32\n  - @ai-sdk/react@0.0.44\n  - @ai-sdk/solid@0.0.35\n  - @ai-sdk/svelte@0.0.37\n  - @ai-sdk/vue@0.0.36\n\n## 3.3.7\n\n### Patch Changes\n\n- eccbd8e: feat (ai/core): add onChunk callback to streamText\n- Updated dependencies [dd4a0f5]\n  - @ai-sdk/provider@0.0.19\n  - @ai-sdk/provider-utils@1.0.11\n  - @ai-sdk/ui-utils@0.0.31\n  - @ai-sdk/react@0.0.43\n  - @ai-sdk/solid@0.0.34\n  - @ai-sdk/svelte@0.0.36\n  - @ai-sdk/vue@0.0.35\n\n## 3.3.6\n\n### Patch Changes\n\n- e9c891d: feat (ai/react): useObject supports non-Zod schemas\n- 3719e8a: chore (ai/core): provider registry code improvements\n- Updated dependencies [e9c891d]\n- Updated dependencies [4bd27a9]\n- Updated dependencies [845754b]\n  - @ai-sdk/ui-utils@0.0.30\n  - @ai-sdk/react@0.0.42\n  - @ai-sdk/provider-utils@1.0.10\n  - @ai-sdk/provider@0.0.18\n  - @ai-sdk/solid@0.0.33\n  - @ai-sdk/svelte@0.0.35\n  - @ai-sdk/vue@0.0.34\n\n## 3.3.5\n\n### Patch Changes\n\n- 9ada023: feat (ai/core): mask data stream error messages with streamText\n- Updated dependencies [e5b58f3]\n  - @ai-sdk/ui-utils@0.0.29\n  - @ai-sdk/react@0.0.41\n  - @ai-sdk/solid@0.0.32\n  - @ai-sdk/svelte@0.0.34\n  - @ai-sdk/vue@0.0.33\n\n## 3.3.4\n\n### Patch Changes\n\n- 029af4c: feat (ai/core): support schema name & description in generateObject & streamObject\n- 3806c0c: chore (ai/ui): increase stream data warning timeout to 15 seconds\n- db0118a: feat (ai/core): export Schema type\n- Updated dependencies [029af4c]\n  - @ai-sdk/provider@0.0.17\n  - @ai-sdk/provider-utils@1.0.9\n  - @ai-sdk/ui-utils@0.0.28\n  - @ai-sdk/react@0.0.40\n  - @ai-sdk/solid@0.0.31\n  - @ai-sdk/svelte@0.0.33\n  - @ai-sdk/vue@0.0.32\n\n## 3.3.3\n\n### Patch Changes\n\n- d58517b: feat (ai/openai): structured outputs\n- Updated dependencies [d58517b]\n  - @ai-sdk/provider@0.0.16\n  - @ai-sdk/provider-utils@1.0.8\n  - @ai-sdk/ui-utils@0.0.27\n  - @ai-sdk/react@0.0.39\n  - @ai-sdk/solid@0.0.30\n  - @ai-sdk/svelte@0.0.32\n  - @ai-sdk/vue@0.0.31\n\n## 3.3.2\n\n### Patch Changes\n\n- Updated dependencies [96aed25]\n  - @ai-sdk/provider@0.0.15\n  - @ai-sdk/provider-utils@1.0.7\n  - @ai-sdk/ui-utils@0.0.26\n  - @ai-sdk/react@0.0.38\n  - @ai-sdk/solid@0.0.29\n  - @ai-sdk/svelte@0.0.31\n  - @ai-sdk/vue@0.0.30\n\n## 3.3.1\n\n### Patch Changes\n\n- 9614584: fix (ai/core): use Symbol.for\n- 0762a22: feat (ai/core): support zod transformers in generateObject & streamObject\n- Updated dependencies [9614584]\n- Updated dependencies [0762a22]\n  - @ai-sdk/provider-utils@1.0.6\n  - @ai-sdk/react@0.0.37\n  - @ai-sdk/solid@0.0.28\n  - @ai-sdk/svelte@0.0.30\n  - @ai-sdk/ui-utils@0.0.25\n  - @ai-sdk/vue@0.0.29\n\n## 3.3.0\n\n### Minor Changes\n\n- dbc3afb7: chore (ai): release AI SDK 3.3\n\n### Patch Changes\n\n- b9827186: feat (ai/core): update operation.name telemetry attribute to include function id and detailed name\n\n## 3.2.45\n\n### Patch Changes\n\n- Updated dependencies [5be25124]\n  - @ai-sdk/ui-utils@0.0.24\n  - @ai-sdk/react@0.0.36\n  - @ai-sdk/solid@0.0.27\n  - @ai-sdk/svelte@0.0.29\n  - @ai-sdk/vue@0.0.28\n\n## 3.2.44\n\n### Patch Changes\n\n- Updated dependencies [a147d040]\n  - @ai-sdk/react@0.0.35\n\n## 3.2.43\n\n### Patch Changes\n\n- Updated dependencies [b68fae4f]\n  - @ai-sdk/react@0.0.34\n\n## 3.2.42\n\n### Patch Changes\n\n- f63c99e7: feat (ai/core): record OpenTelemetry gen_ai attributes\n- Updated dependencies [fea7b604]\n  - @ai-sdk/ui-utils@0.0.23\n  - @ai-sdk/react@0.0.33\n  - @ai-sdk/solid@0.0.26\n  - @ai-sdk/svelte@0.0.28\n  - @ai-sdk/vue@0.0.27\n\n## 3.2.41\n\n### Patch Changes\n\n- a12044c7: feat (ai/core): add recordInputs / recordOutputs setting to telemetry options\n- Updated dependencies [1d93d716]\n  - @ai-sdk/ui-utils@0.0.22\n  - @ai-sdk/react@0.0.32\n  - @ai-sdk/solid@0.0.25\n  - @ai-sdk/svelte@0.0.27\n  - @ai-sdk/vue@0.0.26\n\n## 3.2.40\n\n### Patch Changes\n\n- f56b7e66: feat (ai/ui): add toDataStreamResponse to LangchainAdapter.\n\n## 3.2.39\n\n### Patch Changes\n\n- b694f2f9: feat (ai/svelte): add tool calling support to useChat\n- Updated dependencies [b694f2f9]\n  - @ai-sdk/svelte@0.0.26\n\n## 3.2.38\n\n### Patch Changes\n\n- 5c4b8cfc: chore (ai/core): rename ai stream methods to data stream (in streamText, LangChainAdapter).\n- c450fcf7: feat (ui): invoke useChat onFinish with finishReason and tokens\n- e4a1719f: chore (ai/ui): rename streamMode to streamProtocol\n- 10158bf2: fix (ai/core): generateObject.doGenerate sets object telemetry attribute\n- Updated dependencies [c450fcf7]\n- Updated dependencies [e4a1719f]\n  - @ai-sdk/ui-utils@0.0.21\n  - @ai-sdk/svelte@0.0.25\n  - @ai-sdk/react@0.0.31\n  - @ai-sdk/solid@0.0.24\n  - @ai-sdk/vue@0.0.25\n\n## 3.2.37\n\n### Patch Changes\n\n- b2bee4c5: fix (ai/ui): send data, body, headers in useChat().reload\n- Updated dependencies [b2bee4c5]\n  - @ai-sdk/svelte@0.0.24\n  - @ai-sdk/react@0.0.30\n  - @ai-sdk/solid@0.0.23\n\n## 3.2.36\n\n### Patch Changes\n\n- a8d1c9e9: feat (ai/core): parallel image download\n- cfa360a8: feat (ai/core): add telemetry support to embedMany function.\n- 49808ca5: feat (ai/core): add telemetry to streamObject\n- Updated dependencies [a8d1c9e9]\n  - @ai-sdk/provider-utils@1.0.5\n  - @ai-sdk/provider@0.0.14\n  - @ai-sdk/react@0.0.29\n  - @ai-sdk/svelte@0.0.23\n  - @ai-sdk/ui-utils@0.0.20\n  - @ai-sdk/vue@0.0.24\n  - @ai-sdk/solid@0.0.22\n\n## 3.2.35\n\n### Patch Changes\n\n- 1be014b7: feat (ai/core): add telemetry support for embed function.\n- 4f88248f: feat (core): support json schema\n- 0d545231: chore (ai/svelte): change sswr into optional peer dependency\n- Updated dependencies [4f88248f]\n  - @ai-sdk/provider-utils@1.0.4\n  - @ai-sdk/react@0.0.28\n  - @ai-sdk/svelte@0.0.22\n  - @ai-sdk/ui-utils@0.0.19\n  - @ai-sdk/vue@0.0.23\n  - @ai-sdk/solid@0.0.21\n\n## 3.2.34\n\n### Patch Changes\n\n- 2b9da0f0: feat (core): support stopSequences setting.\n- a5b58845: feat (core): support topK setting\n- 420f170f: chore (ai/core): use interfaces for core function results\n- 13b27ec6: chore (ai/core): remove grammar mode\n- 644f6582: feat (ai/core): add telemetry to generateObject\n- Updated dependencies [2b9da0f0]\n- Updated dependencies [a5b58845]\n- Updated dependencies [4aa8deb3]\n- Updated dependencies [13b27ec6]\n  - @ai-sdk/provider@0.0.13\n  - @ai-sdk/provider-utils@1.0.3\n  - @ai-sdk/react@0.0.27\n  - @ai-sdk/svelte@0.0.21\n  - @ai-sdk/ui-utils@0.0.18\n  - @ai-sdk/solid@0.0.20\n  - @ai-sdk/vue@0.0.22\n\n## 3.2.33\n\n### Patch Changes\n\n- 4b2c09d9: feat (ai/ui): add mutator function support to useChat / setMessages\n- 281e7662: chore: add description to ai package\n- Updated dependencies [f63829fe]\n- Updated dependencies [4b2c09d9]\n  - @ai-sdk/ui-utils@0.0.17\n  - @ai-sdk/svelte@0.0.20\n  - @ai-sdk/react@0.0.26\n  - @ai-sdk/solid@0.0.19\n  - @ai-sdk/vue@0.0.21\n\n## 3.2.32\n\n### Patch Changes\n\n- Updated dependencies [5b7b3bbe]\n  - @ai-sdk/ui-utils@0.0.16\n  - @ai-sdk/react@0.0.25\n  - @ai-sdk/solid@0.0.18\n  - @ai-sdk/svelte@0.0.19\n  - @ai-sdk/vue@0.0.20\n\n## 3.2.31\n\n### Patch Changes\n\n- b86af092: feat (ai/core): add langchain stream event v2 support to LangChainAdapter\n\n## 3.2.30\n\n### Patch Changes\n\n- Updated dependencies [19c3d50f]\n  - @ai-sdk/react@0.0.24\n  - @ai-sdk/vue@0.0.19\n\n## 3.2.29\n\n### Patch Changes\n\n- e710b388: fix (ai/core): race condition in mergeStreams\n- 6078a690: feat (ai/core): introduce stream data support in toAIStreamResponse\n\n## 3.2.28\n\n### Patch Changes\n\n- 68d1f78c: fix (ai/core): do not construct object promise in streamObject result until requested\n- f0bc1e79: feat (ai/ui): add system message support to convertToCoreMessages\n- 1f67fe49: feat (ai/ui): stream tool calls with streamText and useChat\n- Updated dependencies [1f67fe49]\n  - @ai-sdk/ui-utils@0.0.15\n  - @ai-sdk/react@0.0.23\n  - @ai-sdk/solid@0.0.17\n  - @ai-sdk/svelte@0.0.18\n  - @ai-sdk/vue@0.0.18\n\n## 3.2.27\n\n### Patch Changes\n\n- 811f4493: fix (ai/core): generateText token usage is sum over all roundtrips\n\n## 3.2.26\n\n### Patch Changes\n\n- 8f545ce9: fix (ai/core): forward request headers in generateObject and streamObject\n\n## 3.2.25\n\n### Patch Changes\n\n- 99ddbb74: feat (ai/react): add experimental support for managing attachments to useChat\n- Updated dependencies [99ddbb74]\n  - @ai-sdk/ui-utils@0.0.14\n  - @ai-sdk/react@0.0.22\n  - @ai-sdk/solid@0.0.16\n  - @ai-sdk/svelte@0.0.17\n  - @ai-sdk/vue@0.0.17\n\n## 3.2.24\n\n### Patch Changes\n\n- f041c056: feat (ai/core): add roundtrips property to generateText result\n\n## 3.2.23\n\n### Patch Changes\n\n- a6cb2c8b: feat (ai/ui): add keepLastMessageOnError option to useChat\n- Updated dependencies [a6cb2c8b]\n  - @ai-sdk/ui-utils@0.0.13\n  - @ai-sdk/svelte@0.0.16\n  - @ai-sdk/react@0.0.21\n  - @ai-sdk/solid@0.0.15\n  - @ai-sdk/vue@0.0.16\n\n## 3.2.22\n\n### Patch Changes\n\n- 53fccf1c: fix (ai/core): report error on controller\n- dd0d854e: feat (ai/vue): add useAssistant\n- Updated dependencies [dd0d854e]\n  - @ai-sdk/vue@0.0.15\n\n## 3.2.21\n\n### Patch Changes\n\n- 56bbc2a7: feat (ai/ui): set body and headers directly on options for handleSubmit and append\n- Updated dependencies [56bbc2a7]\n  - @ai-sdk/ui-utils@0.0.12\n  - @ai-sdk/svelte@0.0.15\n  - @ai-sdk/react@0.0.20\n  - @ai-sdk/solid@0.0.14\n  - @ai-sdk/vue@0.0.14\n\n## 3.2.20\n\n### Patch Changes\n\n- 671331b6: feat (core): add experimental OpenTelemetry support for generateText and streamText\n\n## 3.2.19\n\n### Patch Changes\n\n- b7290943: chore (ai/core): rename TokenUsage type to CompletionTokenUsage\n- b7290943: feat (ai/core): add token usage to embed and embedMany\n- Updated dependencies [b7290943]\n  - @ai-sdk/provider@0.0.12\n  - @ai-sdk/provider-utils@1.0.2\n  - @ai-sdk/react@0.0.19\n  - @ai-sdk/svelte@0.0.14\n  - @ai-sdk/ui-utils@0.0.11\n  - @ai-sdk/solid@0.0.13\n  - @ai-sdk/vue@0.0.13\n\n## 3.2.18\n\n### Patch Changes\n\n- Updated dependencies [70d18003]\n  - @ai-sdk/react@0.0.18\n\n## 3.2.17\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Google Vertex Anthropic Provider Instance in TypeScript\nDESCRIPTION: Illustrates creating a customized Google Vertex Anthropic provider instance using the `createVertexAnthropic` factory function. This allows specifying optional settings like the Google Cloud `project` ID and `location`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_20\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createVertexAnthropic } from '@ai-sdk/google-vertex/anthropic';\n\nconst vertexAnthropic = createVertexAnthropic({\n  project: 'my-project', // optional\n  location: 'us-central1', // optional\n});\n```\n\n----------------------------------------\n\nTITLE: Ending LLM Span and Capturing Metrics\nDESCRIPTION: TypeScript example of ending an LLM span in LangWatch, capturing output and token metrics.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langwatch.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nspan.end({\n  output: {\n    type: 'chat_messages',\n    value: [chatCompletion.choices[0]!.message],\n  },\n  metrics: {\n    promptTokens: chatCompletion.usage?.prompt_tokens,\n    completionTokens: chatCompletion.usage?.completion_tokens,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Top-K Sampling Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `topK` parameter as a number. This restricts sampling to the top K most likely tokens, useful for removing low-probability 'long tail' options. Recommended for advanced use cases.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nnumber\n```\n\n----------------------------------------\n\nTITLE: Cancelling Server-Side Streams with AI SDK Core\nDESCRIPTION: Demonstrates how to use the abortSignal parameter to cancel streams from the server side to the LLM API. The code shows implementation of a POST handler that forwards the request's abort signal to the stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/02-stopping-streams.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4-turbo'),\n    prompt,\n    // forward the abort signal:\n    abortSignal: req.signal,\n  });\n\n  return result.toTextStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Using Chat Persistence with OpenAI Responses API\nDESCRIPTION: Shows how to persist chat history with OpenAI across multiple requests, allowing you to send just the user's last message while maintaining context from previous interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/19-openai-responses.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst result1 = await generateText({\n  model: openai.responses('gpt-4o-mini'),\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n\nconst result2 = await generateText({\n  model: openai.responses('gpt-4o-mini'),\n  prompt: 'Summarize in 2 sentences',\n  providerOptions: {\n    openai: {\n      previousResponseId: result1.providerMetadata?.openai.responseId as string,\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing OpenRouter AI SDK Provider (Shell Commands)\nDESCRIPTION: Provides install commands for setting up the @openrouter/ai-sdk-provider package using pnpm, npm, or yarn. Each command should be executed in a terminal within the desired project directory. Ensure your package manager is installed and project initialized before running these commands.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/13-openrouter.mdx#2025-04-23_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npnpm add @openrouter/ai-sdk-provider\n```\n\nLANGUAGE: Shell\nCODE:\n```\nnpm install @openrouter/ai-sdk-provider\n```\n\nLANGUAGE: Shell\nCODE:\n```\nyarn add @openrouter/ai-sdk-provider\n```\n\n----------------------------------------\n\nTITLE: Accessing Guardrails Trace Information\nDESCRIPTION: Conditional check to access trace information from guardrails in the provider metadata.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nif (result.providerMetadata?.bedrock.trace) {\n  // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Google Vertex AI Imagen using Vercel AI SDK in TypeScript\nDESCRIPTION: Illustrates how to generate an image using the Google Vertex AI Imagen model ('imagen-3.0-generate-001') via the Vercel AI SDK. It imports the `vertex` provider and the `generateImage` function, then calls `generateImage` with the model instance, a prompt, and an aspect ratio.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertex } from '@ai-sdk/google-vertex';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: vertex.image('imagen-3.0-generate-001'),\n  prompt: 'A futuristic cityscape at sunset',\n  aspectRatio: '16:9',\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Groq Provider Dependency via npm - Bash\nDESCRIPTION: This snippet shows the command to install the Groq provider for the AI SDK using npm. It must be run in the terminal before using any Groq-related features. The '@ai-sdk/groq' package is a required dependency for interacting with Groq models in the SDK. No parameters are needed beyond a working npm environment.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/groq/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/groq\n```\n\n----------------------------------------\n\nTITLE: Handling Streaming Errors in Simple Streams (TypeScript)\nDESCRIPTION: This code snippet shows how to handle streaming errors in simple streams that don't support error chunks. It uses a try/catch block to catch errors that may occur during the text streaming process using the streamText function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/50-error-handling.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\n\ntry {\n  const { textStream } = streamText({\n    model: yourModel,\n    prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  });\n\n  for await (const textPart of textStream) {\n    process.stdout.write(textPart);\n  }\n} catch (error) {\n  // handle error\n}\n```\n\n----------------------------------------\n\nTITLE: Basic AI Response Generator Implementation\nDESCRIPTION: Initial implementation of the generateResponse function using AI SDK to process messages and generate responses using OpenAI's GPT model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/03-slackbot.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { CoreMessage, generateText } from 'ai';\n\nexport const generateResponse = async (\n  messages: CoreMessage[],\n  updateStatus?: (status: string) => void,\n) => {\n  const { text } = await generateText({\n    model: openai('gpt-4o-mini'),\n    system: `You are a Slack bot assistant. Keep your responses concise and to the point.\n    - Do not tag users.\n    - Current date is: ${new Date().toISOString().split('T')[0]}`,\n    messages,\n  });\n\n  // Convert markdown to Slack mrkdwn format\n  return text.replace(/\\[(.*?)\\]\\((.*?)\\)/g, '<$2|$1>').replace(/\\*\\*/g, '*');\n};\n```\n\n----------------------------------------\n\nTITLE: Tool Calling Implementation with o3-mini\nDESCRIPTION: Shows how to implement tool calling functionality with o3-mini, allowing the model to interact with external systems. Includes an example of a weather information tool.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/24-o3.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText, tool } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { text } = await generateText({\n  model: openai('o3-mini'),\n  prompt: 'What is the weather like today in San Francisco?',\n  tools: {\n    getWeather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Logs for AISDKExporter\nDESCRIPTION: Example of enabling debug logs for the AISDKExporter by passing the debug argument to the constructor.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst traceExporter = new AISDKExporter({ debug: true });\n```\n\n----------------------------------------\n\nTITLE: Using Streamable Values to Stream Object Generation (Before)\nDESCRIPTION: This server action generates sample notifications using streamObject with openai model and updates a streamable value. It demonstrates the previous approach of creating a streamable value, updating it with partial objects from the stream, and returning the value for client consumption.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { createStreamableValue } from 'ai/rsc';\nimport { notificationsSchema } from '@/utils/schemas';\n\nexport async function generateSampleNotifications() {\n  'use server';\n\n  const stream = createStreamableValue();\n\n  (async () => {\n    const { partialObjectStream } = streamObject({\n      model: openai('gpt-4o'),\n      system: 'generate sample ios messages for testing',\n      prompt: 'messages from a family group chat during diwali, max 4',\n      schema: notificationsSchema,\n    });\n\n    for await (const partialObject of partialObjectStream) {\n      stream.update(partialObject);\n    }\n  })();\n\n  stream.done();\n\n  return { partialNotificationsStream: stream.value };\n}\n```\n\n----------------------------------------\n\nTITLE: Simulating Data Stream Protocol Responses in Next.js\nDESCRIPTION: This snippet demonstrates how to simulate Data Stream Protocol responses for testing or debugging purposes in a Next.js route handler. It uses simulateReadableStream to create a stream of chunks with delays.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/55-testing.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { simulateReadableStream } from 'ai';\n\nexport async function POST(req: Request) {\n  return new Response(\n    simulateReadableStream({\n      initialDelayInMs: 1000, // Delay before the first chunk\n      chunkDelayInMs: 300, // Delay between chunks\n      chunks: [\n        `0:\"This\"\\n`,\n        `0:\" is an\"\\n`,\n        `0:\"example.\"\\n`,\n        `e:{\"finishReason\":\"stop\",\"usage\":{\"promptTokens\":20,\"completionTokens\":50},\"isContinued\":false}\\n`,\n        `d:{\"finishReason\":\"stop\",\"usage\":{\"promptTokens\":20,\"completionTokens\":50}}\\n`,\n      ],\n    }).pipeThrough(new TextEncoderStream()),\n    {\n      status: 200,\n      headers: {\n        'X-Vercel-AI-Data-Stream': 'v1',\n        'Content-Type': 'text/plain; charset=utf-8',\n      },\n    },\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Replacing nanoid Export with generateId - TypeScript\nDESCRIPTION: Updates import statements to replace the removed nanoid export with generateId from the ai package. The generateId function now generates 16-character unique IDs by default. Code must be updated in all files using nanoid.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_13\n\nLANGUAGE: ts\nCODE:\n```\nimport { nanoid } from 'ai';\n```\n\nLANGUAGE: ts\nCODE:\n```\nimport { generateId } from 'ai';\n```\n\n----------------------------------------\n\nTITLE: Enabling Prompt Caching for Google Vertex Anthropic Models in Node.js\nDESCRIPTION: Illustrates how to enable prompt caching for Anthropic models via the Google Vertex provider in Node.js. It sets `cacheControl: true` in the model options and demonstrates providing structured messages, including potentially reading content from a file using `fs`. It also shows how to access cache metadata (e.g., `cacheCreationInputTokens`, `cacheReadInputTokens`) from the result's `experimental_providerMetadata`.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google-vertex/README.md#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic';\nimport { generateText } from 'ai';\nimport fs from 'node:fs';\n\nconst errorMessage = fs.readFileSync('data/error-message.txt', 'utf8');\n\nasync function main() {\n  const result = await generateText({\n    model: vertexAnthropic('claude-3-5-sonnet-v2@20241022', {\n      cacheControl: true,\n    }),\n    messages: [\n      {\n        role: 'user',\n        content: [\n          {\n            type: 'text',\n            text: 'You are a JavaScript expert.',\n          },\n          {\n            type: 'text',\n            text: `Error message: ${errorMessage}`,\n            providerOptions: {\n              anthropic: {\n                cacheControl: { type: 'ephemeral' },\n              },\n            },\n          },\n          {\n            type: 'text',\n            text: 'Explain the error message.',\n          },\n        ],\n      },\n    ],\n  });\n\n  console.log(result.text);\n  console.log(result.experimental_providerMetadata?.anthropic);\n  // e.g. { cacheCreationInputTokens: 2118, cacheReadInputTokens: 0 }\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Initializing Google Vertex Model with Safety Settings (TypeScript)\nDESCRIPTION: Shows how to instantiate a Google Vertex model with specific safety settings, customizing the model's content filtering behaviors. The 'safetySettings' key is provided as an options object to vertex, allowing fine-grained control over model output risks (e.g., blocking low and above harm).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = vertex('gemini-1.5-pro', {\n  safetySettings: [\n    { category: 'HARM_CATEGORY_UNSPECIFIED', threshold: 'BLOCK_LOW_AND_ABOVE' },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing AI Component with History in Root Layout (React Server Components)\nDESCRIPTION: Sets up the app layout with an AI provider component that initializes with chat history retrieved from a database. This allows persisting conversations across sessions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/60-save-messages-to-database.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { ServerMessage } from './actions';\nimport { AI } from './ai';\n\nexport default function RootLayout({\n  children,\n}: Readonly<{\n  children: React.ReactNode;\n}>) {\n  // get chat history from database\n  const history: ServerMessage[] = getChat();\n\n  return (\n    <html lang=\"en\">\n      <body>\n        <AI initialAIState={history} initialUIState={[]}>\n          {children}\n        </AI>\n      </body>\n    </html>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Tool Choice in AI SDK Core\nDESCRIPTION: This example shows how to use the toolChoice setting to influence when a tool is selected. It demonstrates setting toolChoice to 'required', forcing the model to call a tool when generating a response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from 'zod';\nimport { generateText, tool } from 'ai';\n\nconst result = await generateText({\n  model: yourModel,\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n  toolChoice: 'required', // force the model to call a tool\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n----------------------------------------\n\nTITLE: Defining CoreToolMessage and ToolContent Types in TypeScript\nDESCRIPTION: Defines the structure for a tool message that contains the result of one or more tool calls. It includes a role of 'tool' and a ToolContent type.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/30-core-message.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ntype CoreToolMessage = {\n  role: 'tool';\n  content: ToolContent;\n};\n\ntype ToolContent = Array<ToolResultPart>;\n```\n\n----------------------------------------\n\nTITLE: Defining PropertiesTable Content Structure for AI Parameters\nDESCRIPTION: TypeScript interface definition showing the structure of parameters used for language model configuration, including message types, model settings, and generation controls.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/01-stream-ui.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  name: 'model',\n  type: 'LanguageModel',\n  description: 'The language model to use. Example: openai(\"gpt-4-turbo\")',\n},\n{\n  name: 'initial',\n  isOptional: true,\n  type: 'ReactNode',\n  description: 'The initial UI to render.',\n}\n```\n\n----------------------------------------\n\nTITLE: Using DeepSeek R1 via Fireworks with AI SDK Core\nDESCRIPTION: This code shows how to use DeepSeek R1 through the Fireworks provider. It includes middleware for extracting reasoning tokens and demonstrates how to switch providers with minimal code changes.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/25-r1.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fireworks } from '@ai-sdk/fireworks';\nimport {\n  generateText,\n  wrapLanguageModel,\n  extractReasoningMiddleware,\n} from 'ai';\n\n// middleware to extract reasoning tokens\nconst enhancedModel = wrapLanguageModel({\n  model: fireworks('accounts/fireworks/models/deepseek-r1'),\n  middleware: extractReasoningMiddleware({ tagName: 'think' }),\n});\n\nconst { reasoning, text } = await generateText({\n  model: enhancedModel,\n  prompt: 'Explain quantum entanglement.',\n});\n```\n\n----------------------------------------\n\nTITLE: Logging Anthropic API Error Response in TypeScript\nDESCRIPTION: This snippet shows the error response received from the Anthropic API. It includes details about the error type, message, and the token count that caused the error. The API requires at least 1024 tokens in the initial message, but only 939 were provided.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/ai-core/data/error-message.txt#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nFetched {\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"The message up to and including the first cache-control block must be at least 1024 tokens. Found: 939.\"}}\n```\n\n----------------------------------------\n\nTITLE: Running Stdio Client Example\nDESCRIPTION: Command to run the Stdio client example. This executes the client-side implementation using the stdio transport.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#2025-04-23_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npnpm stdio:client\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_NoSuchToolError with the ai Library in TypeScript\nDESCRIPTION: This snippet demonstrates how to import and use the NoSuchToolError class from the 'ai' package to verify if a caught error is specifically an AI_NoSuchToolError. Dependencies include the 'ai' library providing the NoSuchToolError utility. The key parameter is 'error', representing the error object, and the code conditionally executes error-handling logic if the error is an instance of this specific error class. The code expects 'error' to be an Exception or Error type, and requires TypeScript support in the runtime environment.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-no-such-tool-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { NoSuchToolError } from 'ai';\\n\\nif (NoSuchToolError.isInstance(error)) {\\n  // Handle the error\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Mistral AI Model with Additional Settings\nDESCRIPTION: Demonstrates how to create a Mistral AI model with additional model-specific settings like safePrompt for safety prompt injection.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/20-mistral.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = mistral('mistral-large-latest', {\n  safePrompt: true, // optional safety prompt injection\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing streamObject warnings Asynchronously in TypeScript\nDESCRIPTION: Shows the change in accessing the `warnings` property of the `StreamObjectResult` type in AI SDK 4.0. Similar to `streamText`, it is now a Promise and requires `await`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_30\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await streamObject({\n  // ...\n});\n\nconst warnings = result.warnings;\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = streamObject({\n  // ...\n});\n\nconst warnings = await result.warnings;\n```\n\n----------------------------------------\n\nTITLE: Installing voyage-ai-provider using yarn\nDESCRIPTION: Installs the `voyage-ai-provider` package using the yarn package manager. This command adds the necessary dependency to your project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/61-voyage-ai.mdx#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nyarn add voyage-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Creating a Language Model Instance from a Provider in TypeScript\nDESCRIPTION: Demonstrates accessing a specific language model from a previously configured provider instance by calling the provider function with the desired model ID string. This model object is then used for generation tasks.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/index.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = provider('model-id');\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom AI SDK Provider Logic in TypeScript\nDESCRIPTION: Shows the core implementation of a custom AI SDK provider factory function (`createExample`). It defines provider settings (`ExampleProviderSettings`), the provider interface (`ExampleProvider`), and internal functions to create chat, completion, text embedding, and image models using classes from the `@ai-sdk/openai-compatible` package. It handles API key loading (`loadApiKey`), base URL configuration, headers, query parameters, and allows for a custom `fetch` implementation. Dependencies include `@ai-sdk/provider`, `@ai-sdk/openai-compatible`, and `@ai-sdk/provider-utils`. The function returns a provider instance configured with the provided options.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LanguageModelV1, EmbeddingModelV1 } from '@ai-sdk/provider';\nimport {\n  OpenAICompatibleChatLanguageModel,\n  OpenAICompatibleCompletionLanguageModel,\n  OpenAICompatibleEmbeddingModel,\n  OpenAICompatibleImageModel,\n} from '@ai-sdk/openai-compatible';\nimport {\n  FetchFunction,\n  loadApiKey,\n  withoutTrailingSlash,\n} from '@ai-sdk/provider-utils';\n// Import your model id and settings here.\n\nexport interface ExampleProviderSettings {\n  /**\nExample API key.\n*/\n  apiKey?: string;\n  /**\nBase URL for the API calls.\n*/\n  baseURL?: string;\n  /**\nCustom headers to include in the requests.\n*/\n  headers?: Record<string, string>;\n  /**\nOptional custom url query parameters to include in request urls.\n*/\n  queryParams?: Record<string, string>;\n  /**\nCustom fetch implementation. You can use it as a middleware to intercept requests,\nor to provide a custom fetch implementation for e.g. testing.\n*/\n  fetch?: FetchFunction;\n}\n\nexport interface ExampleProvider {\n  /**\nCreates a model for text generation.\n*/\n  (\n    modelId: ExampleChatModelId,\n    settings?: ExampleChatSettings,\n  ): LanguageModelV1;\n\n  /**\nCreates a chat model for text generation.\n*/\n  chatModel(\n    modelId: ExampleChatModelId,\n    settings?: ExampleChatSettings,\n  ): LanguageModelV1;\n\n  /**\nCreates a completion model for text generation.\n*/\n  completionModel(\n    modelId: ExampleCompletionModelId,\n    settings?: ExampleCompletionSettings,\n  ): LanguageModelV1;\n\n  /**\nCreates a text embedding model for text generation.\n*/\n  textEmbeddingModel(\n    modelId: ExampleEmbeddingModelId,\n    settings?: ExampleEmbeddingSettings,\n  ): EmbeddingModelV1<string>;\n\n  /**\nCreates an image model for image generation.\n*/\n  imageModel(\n    modelId: ExampleImageModelId,\n    settings?: ExampleImageSettings,\n  ): ImageModelV1;\n}\n\nexport function createExample(\n  options: ExampleProviderSettings = {},\n): ExampleProvider {\n  const baseURL = withoutTrailingSlash(\n    options.baseURL ?? 'https://api.example.com/v1',\n  );\n  const getHeaders = () => ({\n    Authorization: `Bearer ${loadApiKey({\n      apiKey: options.apiKey,\n      environmentVariableName: 'EXAMPLE_API_KEY',\n      description: 'Example API key',\n    })}`,\n    ...options.headers,\n  });\n\n  interface CommonModelConfig {\n    provider: string;\n    url: ({ path }: { path: string }) => string;\n    headers: () => Record<string, string>;\n    fetch?: FetchFunction;\n  }\n\n  const getCommonModelConfig = (modelType: string): CommonModelConfig => ({\n    provider: `example.${modelType}`,\n    url: ({ path }) => {\n      const url = new URL(`${baseURL}${path}`);\n      if (options.queryParams) {\n        url.search = new URLSearchParams(options.queryParams).toString();\n      }\n      return url.toString();\n    },\n    headers: getHeaders,\n    fetch: options.fetch,\n  });\n\n  const createChatModel = (\n    modelId: ExampleChatModelId,\n    settings: ExampleChatSettings = {},\n  ) => {\n    return new OpenAICompatibleChatLanguageModel(modelId, settings, {\n      ...getCommonModelConfig('chat'),\n      defaultObjectGenerationMode: 'tool',\n    });\n  };\n\n  const createCompletionModel = (\n    modelId: ExampleCompletionModelId,\n    settings: ExampleCompletionSettings = {},\n  ) =>\n    new OpenAICompatibleCompletionLanguageModel(\n      modelId,\n      settings,\n      getCommonModelConfig('completion'),\n    );\n\n  const createTextEmbeddingModel = (\n    modelId: ExampleEmbeddingModelId,\n    settings: ExampleEmbeddingSettings = {},\n  ) =>\n    new OpenAICompatibleEmbeddingModel(\n      modelId,\n      settings,\n      getCommonModelConfig('embedding'),\n    );\n\n  const createImageModel = (\n    modelId: ExampleImageModelId,\n    settings: ExampleImageSettings = {},\n  ) =>\n    new OpenAICompatibleImageModel(\n      modelId,\n      settings,\n      getCommonModelConfig('image'),\n    );\n\n  const provider = (\n    modelId: ExampleChatModelId,\n    settings?: ExampleChatSettings,\n  ) => createChatModel(modelId, settings);\n\n  provider.completionModel = createCompletionModel;\n  provider.chatModel = createChatModel;\n  provider.textEmbeddingModel = createTextEmbeddingModel;\n  provider.imageModel = createImageModel;\n\n  return provider;\n}\n\n// Export default instance\nexport const example = createExample();\n```\n\n----------------------------------------\n\nTITLE: UI State Restoration with onGetUIState\nDESCRIPTION: Example demonstrating UI state restoration using onGetUIState callback. Compares database history with application history and returns UI state based on database state when they're out of sync.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-saving-and-restoring-states.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nexport const AI = createAI<ServerMessage[], ClientMessage[]>({\n  actions: {\n    continueConversation,\n  },\n  onGetUIState: async () => {\n    'use server';\n\n    const historyFromDB: ServerMessage[] = await loadChatFromDB();\n    const historyFromApp: ServerMessage[] = getAIState();\n\n    // If the history from the database is different from the\n    // history in the app, they're not in sync so return the UIState\n    // based on the history from the database\n\n    if (historyFromDB.length !== historyFromApp.length) {\n      return historyFromDB.map(({ role, content }) => ({\n        id: generateId(),\n        role,\n        display:\n          role === 'function' ? (\n            <Component {...JSON.parse(content)} />\n          ) : (\n            content\n          ),\n      }));\n    }\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Image Size - AI SDK\nDESCRIPTION: This code snippet demonstrates how to specify the size of the generated image. It uses the `size` parameter in `generateImage` and sets the size as a string with width and height in the format `{width}x{height}`. Requires appropriate model support.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { image } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n  size: '1024x1024',\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing Provider-Specific Metadata from Azure OpenAI Responses\nDESCRIPTION: Shows how to retrieve provider-specific metadata returned by the Azure OpenAI responses provider after a call to `generateText`. The example accesses the `openai` specific metadata nested within the `providerMetadata` object of the result.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst { providerMetadata } = await generateText({\n  model: azure.responses('your-deployment-name'),\n});\n\nconst openaiMetadata = providerMetadata?.openai;\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Stream Protocol in Backend API Route\nDESCRIPTION: Next.js API route implementation for handling text stream completion requests. Shows how to use streamText with OpenAI and convert the result to a text stream response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { prompt }: { prompt: string } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    prompt,\n  });\n\n  return result.toTextStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Checking for AI_JSONParseError Instance in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to import the `JSONParseError` class from the 'ai' library and use its static `isInstance` method to check if a caught error object is specifically an instance of `JSONParseError`. This allows for targeted error handling when JSON parsing fails.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-json-parse-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSONParseError } from 'ai';\n\nif (JSONParseError.isInstance(error)) {\n  // Handle the error\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing API Route for Chat in Next.js\nDESCRIPTION: Server-side implementation of a chat API endpoint using AI SDK with OpenAI in a Next.js application. Creates a streaming text response that can be consumed by the client-side chat interface.\nSOURCE: https://github.com/vercel/ai/blob/main/README.md#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'You are a helpful assistant.',\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Interface Component with Next.js\nDESCRIPTION: Implements a React client component using the useChat hook to create an interactive chat interface with message history and input form.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/23-o1.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, handleInputChange, handleSubmit, error } = useChat();\n\n  return (\n    <>\n      {messages.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.content}\n        </div>\n      ))}\n      <form onSubmit={handleSubmit}>\n        <input name=\"prompt\" value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Submit</button>\n      </form>\n    </>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Defining getRoomTemperature Function in JSON for OpenAI Assistant\nDESCRIPTION: This JSON structure defines the 'getRoomTemperature' function for the OpenAI assistant. It specifies the function name, description, and required parameters including the room name as an enumerated string.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai/app/api/assistant/assistant-setup.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \"name\": \"getRoomTemperature\",\\n  \"description\": \"Get the temperature in a room\",\\n  \"parameters\": {\\n    \"type\": \"object\",\\n    \"properties\": {\\n      \"room\": {\\n        \"type\": \"string\",\\n        \"enum\": [\"bedroom\", \"home office\", \"living room\", \"kitchen\", \"bathroom\"]\\n      }\\n    },\\n    \"required\": [\"room\"]\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Loading and Error States\nDESCRIPTION: Example showing how to handle loading states and error conditions in the completion UI using the useCompletion hook.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/05-completion.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nconst { isLoading, ... } = useCompletion()\n\nreturn(\n  <>\n    {isLoading ? <Spinner /> : null}\n  </>\n)\n```\n\n----------------------------------------\n\nTITLE: Chat Persistence Implementation\nDESCRIPTION: Route handler implementation for saving chat history using streamText and database integration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { saveChat } from '@/utils/queries';\nimport { streamText, convertToCoreMessages } from 'ai';\n\nexport async function POST(request) {\n  const { id, messages } = await request.json();\n\n  const coreMessages = convertToCoreMessages(messages);\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'you are a friendly assistant!',\n    messages: coreMessages,\n    onFinish: async ({ response }) => {\n      try {\n        await saveChat({\n          id,\n          messages: [...coreMessages, ...response.messages],\n        });\n      } catch (error) {\n        console.error('Failed to save chat');\n      }\n    },\n  });\n\n  return result.toDataStreamResponse();\n```\n\n----------------------------------------\n\nTITLE: Integrating Chart Generation into React Component Submit Handler in TypeScript\nDESCRIPTION: This snippet shows an update to the `handleSubmit` asynchronous function within a React component (`app/page.tsx`). After successfully generating and executing a SQL query (`generateQuery`, `runGeneratedSQLQuery`) and receiving the results (`companies`), it calls the `generateChartConfig` Server Action. This action is passed the query results and the original user query (`question`) to generate a chart configuration, which is then stored in the component's state using `setChartConfig`. This ensures the chart visualization is generated automatically after the data loads.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\n/* ...other imports... */\nimport { getCompanies, generateQuery, generateChartConfig } from './actions';\n\n/* ...rest of the file... */\nconst handleSubmit = async (suggestion?: string) => {\n  clearExistingData();\n\n  const question = suggestion ?? inputValue;\n  if (inputValue.length === 0 && !suggestion) return;\n\n  if (question.trim()) {\n    setSubmitted(true);\n  }\n\n  setLoading(true);\n  setLoadingStep(1);\n  setActiveQuery('');\n\n  try {\n    const query = await generateQuery(question);\n\n    if (query === undefined) {\n      toast.error('An error occurred. Please try again.');\n      setLoading(false);\n      return;\n    }\n\n    setActiveQuery(query);\n    setLoadingStep(2);\n\n    const companies = await runGeneratedSQLQuery(query);\n    const columns = companies.length > 0 ? Object.keys(companies[0]) : [];\n    setResults(companies);\n    setColumns(columns);\n\n    setLoading(false);\n\n    const { config } = await generateChartConfig(companies, question);\n    setChartConfig(config);\n  } catch (e) {\n    toast.error('An error occurred. Please try again.');\n    setLoading(false);\n  }\n};\n\n/* ...rest of the file... */\n```\n\n----------------------------------------\n\nTITLE: Implementing throttling with useChat hook in React\nDESCRIPTION: This code snippet demonstrates how to use the experimental_throttle option with the useChat hook to limit UI updates to every 50ms. This prevents the \"Maximum update depth exceeded\" error by reducing the frequency of re-renders during streaming AI responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/50-react-maximum-update-depth-exceeded.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nconst { messages, ... } = useChat({\n  // Throttle the messages and data updates to 50ms:\n  experimental_throttle: 50\n})\n```\n\n----------------------------------------\n\nTITLE: Enabling LLM Security with Helicone in JavaScript\nDESCRIPTION: This snippet shows how to enable LLM security features in Helicone to protect against prompt injection, jailbreaking, and other LLM-specific threats. It demonstrates both basic and advanced protection modes.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/helicone.mdx#2025-04-23_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst response = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: userInput,\n  headers: {\n    // Basic protection (Prompt Guard model)\n    'Helicone-LLM-Security-Enabled': 'true',\n\n    // Optional: Advanced protection (Llama Guard model)\n    'Helicone-LLM-Security-Advanced': 'true',\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Using FriendliAI Serverless via OpenAI Compatibility\nDESCRIPTION: Illustrates how to use the OpenAI-compatible FriendliAI Serverless API by configuring the `@ai-sdk/openai` provider. It sets the `baseURL` to the FriendliAI serverless endpoint and uses the `FRIENDLI_TOKEN` as the `apiKey`. Dependencies: `@ai-sdk/openai`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst friendli = createOpenAI({\n  baseURL: 'https://api.friendli.ai/serverless/v1',\n  apiKey: process.env.FRIENDLI_TOKEN,\n});\n```\n\n----------------------------------------\n\nTITLE: Reading Streamable UI on Client-Side in React\nDESCRIPTION: Demonstrates how to read and render a streamable UI on the client-side. The component calls the getWeather server action and updates the UI state as the streamable UI is updated.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/05-streaming-values.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { readStreamableValue } from 'ai/rsc';\nimport { getWeather } from '@/actions';\n\nexport default function Page() {\n  const [weather, setWeather] = useState<React.ReactNode | null>(null);\n\n  return (\n    <div>\n      <button\n        onClick={async () => {\n          const weatherUI = await getWeather();\n          setWeather(weatherUI);\n        }}\n      >\n        What&apos;s the weather?\n      </button>\n\n      {weather}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: IndexCards Navigation Component in React\nDESCRIPTION: A React component that renders a grid of navigation cards linking to different sections of the AI SDK RSC documentation. Each card contains a title, description and href link.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/index.mdx#2025-04-23_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<IndexCards\n  cards={[\n    {\n      title: 'Overview',\n      description: 'Learn about AI SDK RSC.',\n      href: '/docs/ai-sdk-rsc/overview',\n    },\n    {\n      title: 'Streaming React Components',\n      description: 'Learn how to stream React components.',\n      href: '/docs/ai-sdk-rsc/streaming-react-components',\n    },\n    {\n      title: 'Managing Generative UI State',\n      description: 'Learn how to manage generative UI state.',\n      href: '/docs/ai-sdk-rsc/generative-ui-state',\n    },\n    {\n      title: 'Saving and Restoring States',\n      description: 'Learn how to save and restore states.',\n      href: '/docs/ai-sdk-rsc/saving-and-restoring-states',\n    },\n    {\n      title: 'Multi-step Interfaces',\n      description: 'Learn how to build multi-step interfaces.',\n      href: '/docs/ai-sdk-rsc/multistep-interfaces',\n    },\n    {\n      title: 'Streaming Values',\n      description: 'Learn how to stream values with AI SDK RSC.',\n      href: '/docs/ai-sdk-rsc/streaming-values',\n    },\n    {\n      title: 'Error Handling',\n      description: 'Learn how to handle errors.',\n      href: '/docs/ai-sdk-rsc/error-handling',\n    },\n    {\n      title: 'Authentication',\n      description: 'Learn how to authenticate users.',\n      href: '/docs/ai-sdk-rsc/authentication',\n    },\n  ]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Importing Default xAI Provider Instance\nDESCRIPTION: This snippet imports the pre-configured xAI provider instance from the `@ai-sdk/xai` module. It allows direct use of the provider with default settings.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { xai } from '@ai-sdk/xai';\n```\n\n----------------------------------------\n\nTITLE: Replacing AI-stream Methods with Data-stream Methods in TypeScript\nDESCRIPTION: Demonstrates replacing the deprecated `toAIStream`, `pipeAIStreamToResponse`, and `toAIStreamResponse` methods from the result of `streamText` with their replacements: `toDataStream`, `pipeDataStreamToResponse`, and `toDataStreamResponse` respectively, as required for AI SDK 4.0.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await streamText({\n  // ...\n});\n\nresult.toAIStream();\nresult.pipeAIStreamToResponse(response);\nresult.toAIStreamResponse();\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = streamText({\n  // ...\n});\n\nresult.toDataStream();\nresult.pipeDataStreamToResponse(response);\nresult.toDataStreamResponse();\n```\n\n----------------------------------------\n\nTITLE: Retrieving Token Usage Information for Embeddings in TypeScript\nDESCRIPTION: This code snippet shows how to retrieve token usage information when embedding a single value using the AI SDK. The 'usage' property in the result object provides the number of tokens used for the embedding process.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/30-embeddings.mdx#2025-04-23_snippet_3\n\nLANGUAGE: ts\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { embed } from 'ai';\n\nconst { embedding, usage } = await embed({\n  model: openai.embedding('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n});\n\nconsole.log(usage); // { tokens: 10 }\n```\n\n----------------------------------------\n\nTITLE: Migrating streamText API to Synchronous Return - TypeScript\nDESCRIPTION: Adjusts streamText calls from awaiting a Promise to direct synchronous call usage. In AI SDK 4.0, streamText returns its result immediately. Clients must not include 'await'. Input: configuration object. Output: streaming text result.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_10\n\nLANGUAGE: ts\nCODE:\n```\nconst result = await streamText({\n  // ...\n});\n```\n\nLANGUAGE: ts\nCODE:\n```\nconst result = streamText({\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Together.ai Provider Instance\nDESCRIPTION: Demonstrates how to create a customized Together.ai provider instance with specific settings like API key configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/24-togetherai.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createTogetherAI } from '@ai-sdk/togetherai';\n\nconst togetherai = createTogetherAI({\n  apiKey: process.env.TOGETHER_AI_API_KEY ?? '',\n});\n```\n\n----------------------------------------\n\nTITLE: Regex-based Chunking Configuration\nDESCRIPTION: Examples of configuring smoothStream with regex-based chunking patterns for splitting text on underscores.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/80-smooth-stream.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// To split on underscores:\nsmoothStream({\n  chunking: /_+/,\n});\n\n// Also can do it like this, same behavior\nsmoothStream({\n  chunking: /[^_]*_/,\n});\n```\n\n----------------------------------------\n\nTITLE: Handling Date Objects with Zod Schema in TypeScript\nDESCRIPTION: Demonstrates how to handle date objects in Zod schemas when generating structured data with AI models. Uses string-to-Date transformation to properly handle date formats.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/20-prompt-engineering.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateObject({\n  model: openai('gpt-4-turbo'),\n  schema: z.object({\n    events: z.array(\n      z.object({\n        event: z.string(),\n        date: z\n          .string()\n          .date()\n          .transform(value => new Date(value)),\n      }),\n    ),\n  }),\n  prompt: 'List 5 important events from the year 2000.'\n});\n```\n\n----------------------------------------\n\nTITLE: Updating and Clearing Data with useChat Hook in React\nDESCRIPTION: This snippet shows how to update and clear the data object of the useChat hook using the setData function. It includes examples of clearing existing data, setting new data, and transforming existing data.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/20-streaming-data.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nconst { setData } = useChat();\n\n// clear existing data\nsetData(undefined);\n\n// set new data\nsetData([{ test: 'value' }]);\n\n// transform existing data, e.g. adding additional values:\nsetData(currentData => [...currentData, { test: 'value' }]);\n```\n\n----------------------------------------\n\nTITLE: Sending Audio Files in User Messages with OpenAI in TypeScript\nDESCRIPTION: This example demonstrates how to send an MP3 audio file as part of a user message using a Buffer with the OpenAI gpt-4o-audio-preview model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: openai('gpt-4o-audio-preview'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'What is the audio saying?' },\n        {\n          type: 'file',\n          mimeType: 'audio/mpeg',\n          data: fs.readFileSync('./data/galileo.mp3'),\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Stream Handling Utility Function Definitions\nDESCRIPTION: Definitions for utility functions that handle stream processing, including consumeStream, pipeDataStreamToResponse, and pipeTextStreamToResponse with their respective options and parameters.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_25\n\nLANGUAGE: typescript\nCODE:\n```\nname: 'consumeStream',\ntype: '(options?: ConsumeStreamOptions) => Promise<void>',\ndescription: 'Consumes the stream without processing the parts. This is useful to force the stream to finish. If an error occurs, it is passed to the optional `onError` callback.',\nproperties: [\n  {\n    type: 'ConsumeStreamOptions',\n    parameters: [\n      {\n        name: 'onError',\n        type: '(error: unknown) => void',\n        isOptional: true,\n        description: 'The error callback.',\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Building AI SDK Project (Shell)\nDESCRIPTION: These commands use `pnpm` (Performant npm) to install the project dependencies (`pnpm install`) and then build the project (`pnpm build`) from the root directory of the AI SDK repository. These steps are necessary to prepare the application for execution.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/express/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npnpm install\npnpm build\n```\n\n----------------------------------------\n\nTITLE: Basic Image Generation with Replicate Provider\nDESCRIPTION: Example showing how to generate an image using the Replicate provider with the AI SDK. This snippet demonstrates calling a Flux model and saving the resulting image to a file.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/replicate/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { replicate } from '@ai-sdk/replicate';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: replicate.image('black-forest-labs/flux-schnell'),\n  prompt: 'The Loch Ness Monster getting a manicure',\n});\n\nconst filename = `image-${Date.now()}.png`;\nfs.writeFileSync(filename, image.uint8Array);\nconsole.log(`Image saved to ${filename}`);\n```\n\n----------------------------------------\n\nTITLE: Defining FilePart Interface in TypeScript\nDESCRIPTION: Defines the structure for a file part in a user message. It includes a type of 'file', file data (which can be various formats), and a mime type.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/30-core-message.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nexport interface FilePart {\n  type: 'file';\n\n  /**\n   * File data. Can either be:\n   * - data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer\n   * - URL: a URL that points to the file\n   */\n  data: DataContent | URL;\n\n  /**\n   * Mime type of the file.\n   */\n  mimeType: string;\n}\n```\n\n----------------------------------------\n\nTITLE: Sending PDF Files in User Messages with Google AI in TypeScript\nDESCRIPTION: This example shows how to send a PDF file as part of a user message using a Buffer with the Google Generative AI provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nconst result = await generateText({\n  model: google('gemini-1.5-flash'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'What is the file about?' },\n        {\n          type: 'file',\n          mimeType: 'application/pdf',\n          data: fs.readFileSync('./data/example.pdf'),\n          filename: 'example.pdf', // optional, not used by all providers\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Updating Vercel AI SDK using pnpm\nDESCRIPTION: This shell command uses the `pnpm` package manager to add or update the `ai` package (Vercel AI SDK) to its latest version. This is the primary step recommended for upgrading to AI SDK 3.2. Requires `pnpm` to be installed and executed within a Node.js project directory.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/38-migration-guide-3-2.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add ai@latest\n```\n\n----------------------------------------\n\nTITLE: Importing the default Anthropic provider instance\nDESCRIPTION: Shows how to import the default Anthropic provider instance from the @ai-sdk/anthropic package. This is the simplest way to get started with the Anthropic provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\n```\n\n----------------------------------------\n\nTITLE: Configuring AI SDK with createAI for State Management\nDESCRIPTION: This code initializes the AI configuration using createAI from the AI SDK. It sets up the actions, initial AI state, and UI state for the application, connecting the server-side actions with the client-side UI state management.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/91-stream-updates-to-visual-interfaces.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAI } from 'ai/rsc';\nimport { ServerMessage, ClientMessage, continueConversation } from './actions';\n\nexport const AI = createAI<ServerMessage[], ClientMessage[]>({\n  actions: {\n    continueConversation,\n  },\n  initialAIState: [],\n  initialUIState: [],\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with OpenAI using AI SDK Core in Node.js\nDESCRIPTION: Example of using AI SDK Core to generate text with OpenAI's GPT-4 model. It demonstrates how to set up the model, provide a system message and prompt, and generate text.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai'; // Ensure OPENAI_API_KEY environment variable is set\n\nconst { text } = await generateText({\n  model: openai('gpt-4o'),\n  system: 'You are a friendly assistant!',\n  prompt: 'Why is the sky blue?',\n});\n\nconsole.log(text);\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Perplexity Sonar Pro Model\nDESCRIPTION: Complete example showing how to use the Perplexity provider with the AI SDK's generateText function. This demonstrates configuring the Sonar Pro model and sending a query about quantum computing.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/perplexity/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { perplexity } from '@ai-sdk/perplexity';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: perplexity('sonar-pro'),\n  prompt: 'What are the latest developments in quantum computing?',\n});\n```\n\n----------------------------------------\n\nTITLE: Testing the Express.js Endpoint with Curl (Shell)\nDESCRIPTION: This command uses `curl` to send an HTTP POST request to the running Express.js server, assumed to be listening on `http://localhost:8080`. This is a basic way to test if the server is running and responding to requests on the configured endpoint.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/express/README.md#2025-04-23_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncurl -X POST http://localhost:8080\n```\n\n----------------------------------------\n\nTITLE: Implementing Server Actions for AI Conversation Processing\nDESCRIPTION: Defines server-side actions that handle AI processing using OpenAI, including message history management and tool usage. Uses the getMutableAIState and streamUI functions to manage conversation state and stream responses to the client.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/60-save-messages-to-database.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\n'use server';\n\nimport { getAIState, getMutableAIState, streamUI } from 'ai/rsc';\nimport { openai } from '@ai-sdk/openai';\nimport { ReactNode } from 'react';\nimport { z } from 'zod';\nimport { generateId } from 'ai';\nimport { Stock } from '@ai-studio/components/stock';\n\nexport interface ServerMessage {\n  role: 'user' | 'assistant' | 'function';\n  content: string;\n}\n\nexport interface ClientMessage {\n  id: string;\n  role: 'user' | 'assistant' | 'function';\n  display: ReactNode;\n}\n\nexport async function continueConversation(\n  input: string,\n): Promise<ClientMessage> {\n  'use server';\n\n  const history = getMutableAIState();\n\n  const result = await streamUI({\n    model: openai('gpt-3.5-turbo'),\n    messages: [...history.get(), { role: 'user', content: input }],\n    text: ({ content, done }) => {\n      if (done) {\n        history.done([\n          ...history.get(),\n          { role: 'user', content: input },\n          { role: 'assistant', content },\n        ]);\n      }\n\n      return <div>{content}</div>;\n    },\n    tools: {\n      showStockInformation: {\n        description:\n          'Get stock information for symbol for the last numOfMonths months',\n        parameters: z.object({\n          symbol: z\n            .string()\n            .describe('The stock symbol to get information for'),\n          numOfMonths: z\n            .number()\n            .describe('The number of months to get historical information for'),\n        }),\n        generate: async ({ symbol, numOfMonths }) => {\n          history.done([\n            ...history.get(),\n            {\n              role: 'function',\n              name: 'showStockInformation',\n              content: JSON.stringify({ symbol, numOfMonths }),\n            },\n          ]);\n\n          return <Stock symbol={symbol} numOfMonths={numOfMonths} />;\n        },\n      },\n    },\n  });\n\n  return {\n    id: generateId(),\n    role: 'assistant',\n    display: result.value,\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Multi-Step Tool Calls in SvelteKit Chat Component\nDESCRIPTION: This snippet shows how to enable multi-step tool calls by adding the 'maxSteps' option to the Chat class instantiation. This allows the model to use multiple steps for more complex interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_10\n\nLANGUAGE: svelte\nCODE:\n```\n<script>\n  import { Chat } from '@ai-sdk/svelte';\n\n  const chat = new Chat({ maxSteps: 5 });\n</script>\n\n<!-- ... rest of your component code -->\n```\n\n----------------------------------------\n\nTITLE: Implementing Amazon Bedrock Guardrails\nDESCRIPTION: Example of using Amazon Bedrock Guardrails with providerOptions to control model output and access trace information.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  bedrock('anthropic.claude-3-sonnet-20240229-v1:0'),\n  providerOptions: {\n    bedrock: {\n      guardrailConfig: {\n        guardrailIdentifier: '1abcd2ef34gh',\n        guardrailVersion: '1',\n        trace: 'enabled' as const,\n        streamProcessingMode: 'async',\n      },\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Displaying Changelog Entry for Version 4.3.6\nDESCRIPTION: Details multiple patch changes for version 4.3.6, including adding speech generation, handling custom separators, and updating dependencies.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/CHANGELOG.md#2025-04-23_snippet_3\n\nLANGUAGE: Markdown\nCODE:\n```\n## 4.3.6\n\n### Patch Changes\n\n- beef951: feat: add speech with experimental_generateSpeech\n- bd41167: fix(ai/core): properly handle custom separator in provider registry\n- Updated dependencies [beef951]\n  - @ai-sdk/provider@1.1.3\n  - @ai-sdk/provider-utils@2.2.7\n  - @ai-sdk/ui-utils@1.2.8\n  - @ai-sdk/react@1.2.9\n```\n\n----------------------------------------\n\nTITLE: Enabling Telemetry in AI SDK Text Generation (TypeScript)\nDESCRIPTION: This snippet demonstrates how to enable experimental telemetry for a text generation request using the AI SDK. It sets the 'experimental_telemetry' option to true, allowing the collection of traces.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langfuse.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  prompt: 'Write a short story about a cat.',\n  experimental_telemetry: { isEnabled: true },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Anthropic topK Setting Migration - TypeScript\nDESCRIPTION: Illustrates transitioning the Anthropic provider's model-specific topK setting to the standard topK argument in generateText. Code using anthropic factory and generateText in AI SDK 4.0 must pass topK at the root function call. Inputs: model provider and topK parameter. Output: Generated text object.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_3\n\nLANGUAGE: ts\nCODE:\n```\nconst result = await generateText({\n  model: anthropic('claude-3-5-sonnet-latest', {\n    topK: 0.5,\n  }),\n});\n```\n\nLANGUAGE: ts\nCODE:\n```\nconst result = await generateText({\n  model: anthropic('claude-3-5-sonnet-latest'),\n  topK: 0.5,\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing an MCP Client with SSE Transport in AI SDK using TypeScript\nDESCRIPTION: This snippet demonstrates initializing an MCP client with a Server-Sent Events (SSE) transport within the AI SDK for accessing MCP-native tools. It includes authorization headers for authenticated requests. The only dependencies are the AI SDK and a running MCP server exposing an SSE endpoint. Primary parameters are the transport 'type', 'url', and optional 'headers'. The resulting 'mcpClient' instance is used to interact with remote tools.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#2025-04-23_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_createMCPClient as createMCPClient } from 'ai';\n\nconst mcpClient = await createMCPClient({\n  transport: {\n    type: 'sse',\n    url: 'https://my-server.com/sse',\n\n    // optional: configure HTTP headers, e.g. for authentication\n    headers: {\n      Authorization: 'Bearer my-api-key',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for AI SDK in Hono\nDESCRIPTION: This snippet shows the content of the .env file required for setting up the AI SDK with Hono. It includes the OpenAI API key and mentions that additional settings may be needed depending on the providers used.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/hono/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Adding System Instructions to Chat API\nDESCRIPTION: Enhanced version of the chat API with system instructions to control model behavior and responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#2025-04-23_snippet_13\n\nLANGUAGE: tsx\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: `You are a helpful assistant. Check your knowledge base before answering any questions.\n    Only respond to questions using information from tool calls.\n    if no relevant information is found in the tool calls, respond, \"Sorry, I don't know.\"`,\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: API Parameters Type Definition\nDESCRIPTION: Defines the core parameters and types for interacting with language models through Vercel's AI API. Includes model configuration, system prompts, and message handling structures.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ntype Parameters = {\n  model: LanguageModel,\n  system?: string,\n  prompt?: string,\n  messages?: Array<CoreSystemMessage | CoreUserMessage | CoreAssistantMessage | CoreToolMessage> | Array<UIMessage>,\n  tools?: ToolSet\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Experimental Active Tools Type in TypeScript\nDESCRIPTION: Specifies the type for the optional experimental feature `experimental_activeTools` as an array of tool names (`Array<TOOLNAME>`). This determines which tools are currently active. Defaults to all tools being active. Can be `undefined`.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_24\n\nLANGUAGE: typescript\nCODE:\n```\nArray<TOOLNAME> | undefined\n```\n\n----------------------------------------\n\nTITLE: Configuring Web Crawler Access in robots.txt\nDESCRIPTION: This snippet defines the rules for web crawlers accessing the website. It allows all user agents ('*') and does not disallow any paths, effectively granting full access to crawlers.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/sveltekit-openai/static/robots.txt#2025-04-23_snippet_0\n\nLANGUAGE: robotstxt\nCODE:\n```\nUser-agent: *\nDisallow:\n```\n\n----------------------------------------\n\nTITLE: Configuring Assistant Instructions for Home Automation\nDESCRIPTION: These instructions define the assistant's role in managing a home automation system, specifically for temperature control in various rooms. It includes guidelines for handling temperature conversions between Celsius and Fahrenheit.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai/app/api/assistant/assistant-setup.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nYou are an assistant with access to a home automation system. You can get and set the temperature in the bedroom, home office, living room, kitchen and bathroom.\\n\\nThe system uses temperature in Celsius. If the user requests Fahrenheit, you should convert the temperature to Fahrenheit.\n```\n\n----------------------------------------\n\nTITLE: Processing Streamed Text Chunks with AI SDK in TypeScript\nDESCRIPTION: Shows how to use the onChunk callback to process individual chunks of the streamed text. This example logs text delta chunks to the console.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#2025-04-23_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\nimport { streamText } from 'ai';\n\nconst result = streamText({\n  model: yourModel,\n  prompt: 'Invent a new holiday and describe its traditions.',\n  onChunk({ chunk }) {\n    // implement your own logic here, e.g.:\n    if (chunk.type === 'text-delta') {\n      console.log(chunk.text);\n    }\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Message Types Definition\nDESCRIPTION: Defines the structure for different types of messages including system, user, assistant, and tool messages, along with their respective content formats and parameters.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ntype CoreMessage = {\n  role: 'system' | 'user' | 'assistant' | 'tool',\n  content: string | Array<TextPart | ImagePart | FilePart | ReasoningPart | RedactedReasoningPart | ToolCallPart | ToolResultPart>\n}\n```\n\n----------------------------------------\n\nTITLE: Creating xAI Image Model Instance\nDESCRIPTION: This snippet shows how to create an instance of an xAI image model using the `.imageModel()` factory method. This instance is used for image generation. It uses experimental_generateImage from ai.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_9\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { xai } from '@ai-sdk/xai';\nimport { experimental_generateImage as generateImage } from 'ai';\n\nconst { image } = await generateImage({\n  model: xai.image('grok-2-image'),\n  prompt: 'A futuristic cityscape at sunset',\n});\n```\n\n----------------------------------------\n\nTITLE: Using streamText function to send raw text stream in Next.js API route\nDESCRIPTION: Example of a Next.js API route handler that uses the streamText function from AI Core to send a raw text stream response without protocol prefixes. This approach allows direct text streaming when working with AI SDK version 3.0.20 or newer.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/04-strange-stream-output.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const result = streamText({\n    model: openai.completion('gpt-3.5-turbo-instruct'),\n    maxTokens: 2000,\n    prompt,\n  });\n\n  return result.toTextStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Next.js App with npx for AI SDK and OpenAI Integration\nDESCRIPTION: This command uses npx to create a new Next.js application with the AI SDK and OpenAI integration example. It sets up the project structure and installs necessary dependencies.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry/app/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app\n```\n\n----------------------------------------\n\nTITLE: Basic Audio Transcription with OpenAI Whisper\nDESCRIPTION: Demonstrates how to use the transcribe function to convert an audio file to text using OpenAI's Whisper model. Shows basic setup and accessing transcript properties.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/36-transcription.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { readFile } from 'fs/promises';\n\nconst transcript = await transcribe({\n  model: openai.transcription('whisper-1'),\n  audio: await readFile('audio.mp3'),\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst text = transcript.text; // transcript text e.g. \"Hello, world!\"\nconst segments = transcript.segments; // array of segments with start and end times, if available\nconst language = transcript.language; // language of the transcript e.g. \"en\", if available\nconst durationInSeconds = transcript.durationInSeconds; // duration of the transcript in seconds, if available\n```\n\n----------------------------------------\n\nTITLE: Importing the Google Provider Instance in TypeScript\nDESCRIPTION: This TypeScript code demonstrates how to import the default `google` provider instance from the installed `@ai-sdk/google` package. This instance is required to configure and interact with Google's Generative AI models within the Vercel AI SDK framework.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { google } from '@ai-sdk/google';\n```\n\n----------------------------------------\n\nTITLE: Importing the AssemblyAI Provider Instance (TypeScript)\nDESCRIPTION: Imports the default `assemblyai` provider instance from the installed `@ai-sdk/assemblyai` package. This instance serves as the entry point for configuring and interacting with the AssemblyAI transcription API through the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/assemblyai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { assemblyai } from '@ai-sdk/assemblyai';\n```\n\n----------------------------------------\n\nTITLE: Replacing Server Action with Route Handler (UI)\nDESCRIPTION: This snippet shows how to replace the server action with a route handler using the streamText function from AI SDK UI.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nexport async function POST(request) {\n  const { messages } = await request.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'you are a friendly assistant!',\n    messages,\n    tools: {\n      // tool definitions\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Next.js App with AI SDK and OpenAI Telemetry Example (npm)\nDESCRIPTION: This command uses npx create-next-app to bootstrap a new Next.js application with the AI SDK and OpenAI telemetry example. It clones the example from the Vercel AI repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry next-openai-telemetry-app\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Headers to AI SDK Text Generation Request in TypeScript\nDESCRIPTION: This snippet illustrates how to add custom HTTP headers to the text generation request. It shows the usage of the headers option to include a 'Prompt-Id' header.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/25-settings.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  prompt: 'Invent a new holiday and describe its traditions.',\n  headers: {\n    'Prompt-Id': 'my-prompt-id',\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Assistant File Content in TypeScript\nDESCRIPTION: This example demonstrates how to generate text using an AI model with an assistant message containing file content, specifically an image file.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  messages: [\n    { role: 'user', content: 'Generate an image of a roquefort cheese!' },\n    {\n      role: 'assistant',\n      content: [\n        {\n          type: 'file',\n          mimeType: 'image/png',\n          data: fs.readFileSync('./data/roquefort.jpg'),\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Temperature Setting Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `temperature` parameter as a number. This controls the randomness of the model's output; the valid range depends on the provider. It's recommended to set either `temperature` or `topP`, but not both.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nnumber\n```\n\n----------------------------------------\n\nTITLE: Documenting Version Changes for @ai-sdk/gladia Package\nDESCRIPTION: A changelog entry documenting the addition of a transcribe feature to the Gladia provider in version 0.0.1. The entry includes the version number, change type (Patch), and a short description of the feature addition.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/gladia/CHANGELOG.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# @ai-sdk/gladia\n\n## 0.0.1\n\n### Patch Changes\n\n- e6e1cd9: feat(providers/gladia): add transcribe\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat UI Component in Svelte\nDESCRIPTION: Svelte component that creates a chat interface using AI SDK's Svelte package. Handles messages display and user input submission.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_6\n\nLANGUAGE: svelte\nCODE:\n```\n<script>\n  import { Chat } from '@ai-sdk/svelte';\n\n  const chat = new Chat();\n</script>\n\n<main>\n  <ul>\n    {#each chat.messages as message, messageIndex (messageIndex)}\n      <li>\n        <div>{message.role}</div>\n        <div>\n          {#each message.parts as part, partIndex (partIndex)}\n            {#if part.type === 'text'}\n              <div>{part.text}</div>\n            {/if}\n          {/each}\n        </div>\n      </li>\n    {/each}\n  </ul>\n  <form onsubmit={chat.handleSubmit}>\n    <input bind:value={chat.input} />\n    <button type=\"submit\">Send</button>\n  </form>\n</main>\n```\n\n----------------------------------------\n\nTITLE: Creating Interactive Chat Interface Page\nDESCRIPTION: Implements the main chat interface page component with message input and conversation display using AI SDK hooks.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/04-multistep-interfaces.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { AI } from './ai';\nimport { useActions, useUIState } from 'ai/rsc';\n\nexport default function Page() {\n  const [input, setInput] = useState<string>('');\n  const [conversation, setConversation] = useUIState<typeof AI>();\n  const { submitUserMessage } = useActions();\n\n  const handleSubmit = async (e: React.FormEvent<HTMLFormElement>) => {\n    e.preventDefault();\n    setInput('');\n    setConversation(currentConversation => [\n      ...currentConversation,\n      <div>{input}</div>,\n    ]);\n    const message = await submitUserMessage(input);\n    setConversation(currentConversation => [...currentConversation, message]);\n  };\n\n  return (\n    <div>\n      <div>\n        {conversation.map((message, i) => (\n          <div key={i}>{message}</div>\n        ))}\n      </div>\n      <div>\n        <form onSubmit={handleSubmit}>\n          <input\n            type=\"text\"\n            value={input}\n            onChange={e => setInput(e.target.value)}\n          />\n          <button>Send Message</button>\n        </form>\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Next.js App with pnpm for AI Chat Example\nDESCRIPTION: This command uses create-next-app to bootstrap a new Next.js application with the AI chat example using pnpm.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-langchain/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-langchain next-langchain-app\n```\n\n----------------------------------------\n\nTITLE: Creating a Chat Interface with Next.js App Router\nDESCRIPTION: Client-side React component using the useChat hook from AI SDK UI to create a chat interface in a Next.js application. Handles messages, user input, and rendering different content types.\nSOURCE: https://github.com/vercel/ai/blob/main/README.md#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, handleSubmit, handleInputChange, status } =\n    useChat();\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <strong>{`${message.role}: `}</strong>\n          {message.parts.map((part, index) => {\n            switch (part.type) {\n              case 'text':\n                return <span key={index}>{part.text}</span>;\n\n              // other cases can handle images, tool calls, etc\n            }\n          })}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          placeholder=\"Send a message...\"\n          onChange={handleInputChange}\n          disabled={status !== 'ready'}\n        />\n      </form>\n    </div>\n  );\n```\n\n----------------------------------------\n\nTITLE: Limiting Available AI Models Across Providers\nDESCRIPTION: Creates a custom provider that limits available models from multiple providers and configures embedding models.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/45-provider-management.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { openai } from '@ai-sdk/openai';\nimport { customProvider } from 'ai';\n\nexport const myProvider = customProvider({\n  languageModels: {\n    'text-medium': anthropic('claude-3-5-sonnet-20240620'),\n    'text-small': openai('gpt-4o-mini'),\n    'structure-medium': openai('gpt-4o', { structuredOutputs: true }),\n    'structure-fast': openai('gpt-4o-mini', { structuredOutputs: true }),\n  },\n  embeddingModels: {\n    emdedding: openai.textEmbeddingModel('text-embedding-3-small'),\n  },\n  // no fallback provider\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Parameter Schema Function Type in TypeScript\nDESCRIPTION: Specifies the type for the `parameterSchema` function within `ToolCallRepairOptions`. This function takes an object with `toolName` (string) and returns the JSON Schema (`JSONSchema7`) for that tool's parameters.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_27\n\nLANGUAGE: typescript\nCODE:\n```\n(options: { toolName: string }) => JSONSchema7\n```\n\n----------------------------------------\n\nTITLE: simulateReadableStream with Configured Delays\nDESCRIPTION: Demonstrates using simulateReadableStream with specific initial and chunk delays for controlled emission timing.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/75-simulate-readable-stream.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst stream = simulateReadableStream({\n  chunks: ['Hello', ' ', 'World'],\n  initialDelayInMs: 1000, // Wait 1 second before first chunk\n  chunkDelayInMs: 500, // Wait 0.5 seconds between chunks\n});\n```\n\n----------------------------------------\n\nTITLE: Wrapping AI SDK Models with Braintrust for Automatic Logging\nDESCRIPTION: Shows how to wrap AI SDK models using Braintrust to automatically log requests, responses, and metrics. This example uses the openai.chat model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/braintrust.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { initLogger, wrapAISDKModel } from 'braintrust';\nimport { openai } from '@ai-sdk/openai';\n\nconst logger = initLogger({\n  projectName: 'My Project',\n  apiKey: process.env.BRAINTRUST_API_KEY,\n});\n\nconst model = wrapAISDKModel(openai.chat('gpt-3.5-turbo'));\n\nasync function main() {\n  // This will automatically log the request, response, and metrics to Braintrust\n  const response = await model.doGenerate({\n    inputFormat: 'messages',\n    mode: {\n      type: 'regular',\n    },\n    prompt: [\n      {\n        role: 'user',\n        content: [{ type: 'text', text: 'What is the capital of France?' }],\n      },\n    ],\n  });\n  console.log(response);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Client Component for Streaming UI\nDESCRIPTION: Client-side implementation for rendering streamed UI components using React state management.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#2025-04-23_snippet_9\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { streamComponent } from './actions';\n\nexport default function Page() {\n  const [component, setComponent] = useState<React.ReactNode>();\n\n  return (\n    <div>\n      <form\n        onSubmit={async e => {\n          e.preventDefault();\n          setComponent(await streamComponent());\n        }}\n      >\n        <button>Stream Component</button>\n      </form>\n      <div>{component}</div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Bedrock Provider\nDESCRIPTION: Example showing how to use the bedrock provider with the generateText function to create a recipe using the Meta Llama model\nSOURCE: https://github.com/vercel/ai/blob/main/packages/amazon-bedrock/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: bedrock('meta.llama3-8b-instruct-v1:0'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Layout Component with AI Provider\nDESCRIPTION: Updates the layout component to wrap all children with the AI provider. This ensures all components in the application have access to the AI context and actions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/120-stream-assistant-response.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { ReactNode } from 'react';\nimport { AI } from './ai';\n\nexport default function Layout({ children }: { children: ReactNode }) {\n  return <AI>{children}</AI>;\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Provider Registry with Multiple Providers\nDESCRIPTION: Creates a registry instance with multiple AI providers including Anthropic and OpenAI. Shows both default and custom provider configuration setup.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/40-provider-registry.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { createProviderRegistry } from 'ai';\n\nexport const registry = createProviderRegistry({\n  // register provider with prefix and default setup:\n  anthropic,\n\n  // register provider with prefix and custom setup:\n  openai: createOpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Note in Markdown Using Custom Component\nDESCRIPTION: This snippet demonstrates the use of a custom 'Note' component in markdown to provide additional information about the compatibility table's content and where to find more models.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/02-providers-and-models.mdx#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<Note>\n  This table is not exhaustive. Additional models can be found in the provider\n  documentation pages and on the provider websites.\n</Note>\n```\n\n----------------------------------------\n\nTITLE: Setting Helicone API Key in Environment Variables\nDESCRIPTION: This snippet shows how to set the Helicone API key as an environment variable in a .env file. This is a prerequisite for using Helicone with your AI SDK application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/helicone.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nHELICONE_API_KEY=your-helicone-api-key\n```\n\n----------------------------------------\n\nTITLE: Implementing Root Layout with AI Provider\nDESCRIPTION: Creates root layout component that wraps the application with AI context provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/04-multistep-interfaces.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { type ReactNode } from 'react';\nimport { AI } from './ai';\n\nexport default function RootLayout({\n  children,\n}: Readonly<{ children: ReactNode }>) {\n  return (\n    <AI>\n      <html lang=\"en\">\n        <body>{children}</body>\n      </html>\n    </AI>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Controlling Reasoning Effort in OpenAI o1\nDESCRIPTION: Demonstrates how to adjust the reasoning effort parameter for o1 model to control computation time and response generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/23-o1.mdx#2025-04-23_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n// Reduce reasoning effort for faster responses\nconst { text } = await generateText({\n  model: openai('o1'),\n  prompt: 'Explain quantum entanglement briefly.',\n  providerOptions: {\n    openai: { reasoningEffort: 'low' },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Default DeepSeek Provider in TypeScript\nDESCRIPTION: Basic import of the default DeepSeek provider instance that can be used directly with AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/30-deepseek.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepseek } from '@ai-sdk/deepseek';\n```\n\n----------------------------------------\n\nTITLE: Renaming simulateReadableStream Parameter from values to chunks in TypeScript\nDESCRIPTION: Illustrates the renaming of the `values` parameter to `chunks` within the `simulateReadableStream` test utility function (`ai/test`) for AI SDK 4.0.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_31\n\nLANGUAGE: typescript\nCODE:\n```\nimport { simulateReadableStream } from 'ai/test';\n\nconst stream = simulateReadableStream({\n  values: [1, 2, 3],\n  chunkDelayInMs: 100,\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { simulateReadableStream } from 'ai/test';\n\nconst stream = simulateReadableStream({\n  chunks: [1, 2, 3],\n  chunkDelayInMs: 100,\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Enum Value in TypeScript\nDESCRIPTION: Example showing how to generate a specific enum value from a predefined list of options for movie genre classification.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/03-generate-object.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateObject } from 'ai';\n\nconst { object } = await generateObject({\n  model: yourModel,\n  output: 'enum',\n  enum: ['action', 'comedy', 'drama', 'horror', 'sci-fi'],\n  prompt:\n    'Classify the genre of this movie plot: ' +\n    '\"A group of astronauts travel through a wormhole in search of a ' +\n    'new habitable planet for humanity.\"',\n});\n```\n\n----------------------------------------\n\nTITLE: Installing the AI SDK Mistral Provider via npm\nDESCRIPTION: This command installs the necessary package for the Mistral provider using the Node Package Manager (npm). This package contains the language model support for the Mistral chat API within the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/mistral/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/mistral\n```\n\n----------------------------------------\n\nTITLE: Creating a Rev.ai transcription model\nDESCRIPTION: Shows how to create a transcription model using the Rev.ai provider. The example uses the 'machine' model ID, which is one of the available transcription models.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/160-revai.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = revai.transcription('machine');\n```\n\n----------------------------------------\n\nTITLE: Implementing Caching Middleware\nDESCRIPTION: Example of a caching middleware that stores and retrieves generated text based on input parameters.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { LanguageModelV1Middleware } from 'ai';\n\nconst cache = new Map<string, any>();\n\nexport const yourCacheMiddleware: LanguageModelV1Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    const cacheKey = JSON.stringify(params);\n\n    if (cache.has(cacheKey)) {\n      return cache.get(cacheKey);\n    }\n\n    const result = await doGenerate();\n\n    cache.set(cacheKey, result);\n\n    return result;\n  },\n\n  // here you would implement the caching logic for streaming\n};\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Fal Provider Instance\nDESCRIPTION: This snippet demonstrates how to create a customized Fal provider instance using the createFal function with options for API key, baseURL, and custom headers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/10-fal.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createFal } from '@ai-sdk/fal';\n\nconst fal = createFal({\n  apiKey: 'your-api-key', // optional, defaults to FAL_API_KEY environment variable, falling back to FAL_KEY\n  baseURL: 'custom-url', // optional\n  headers: {\n    /* custom headers */\n  }, // optional\n});\n```\n\n----------------------------------------\n\nTITLE: Importing `createOpenAICompatible` Function in TypeScript\nDESCRIPTION: Demonstrates how to import the `createOpenAICompatible` factory function from the `@ai-sdk/openai-compatible` module in a TypeScript project. This function is used to instantiate an AI provider compatible with the OpenAI API structure.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/openai-compatible/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\n```\n\n----------------------------------------\n\nTITLE: Creating Custom SambaNova Provider in TypeScript\nDESCRIPTION: Code to create a customized SambaNova provider instance using createSambaNova function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/96-sambanova.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createSambaNova } from 'sambanova-ai-provider';\n\nconst sambanova = createSambaNova({\n  // Optional settings\n});\n```\n\n----------------------------------------\n\nTITLE: Wrapping Model with Reasoning Middleware\nDESCRIPTION: Shows how to enhance an Azure OpenAI model (specifically for models like DeepSeek-R1) using `wrapLanguageModel` and `extractReasoningMiddleware` from the 'ai' package. This extracts reasoning steps tagged with '<think>' from the model's output.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-azure.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { azure } from '@ai-sdk/azure';\nimport { wrapLanguageModel, extractReasoningMiddleware } from 'ai';\n\nconst enhancedModel = wrapLanguageModel({\n  model: azure('your-deepseek-r1-deployment-name'),\n  middleware: extractReasoningMiddleware({ tagName: 'think' }),\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Experimental Message ID Generator Type in TypeScript\nDESCRIPTION: Specifies the type for the optional experimental feature `experimental_generateMessageId` as a function that returns a string (`() => string`). This function is used to generate unique IDs for each message.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\n() => string\n```\n\n----------------------------------------\n\nTITLE: Text Parts in User Messages for TypeScript\nDESCRIPTION: This example demonstrates how to structure user messages with explicit text parts using the type and text properties in the content array.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'Where can I buy the best Currywurst in Berlin?',\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Running All End-to-end Provider Integration Tests\nDESCRIPTION: This command runs all end-to-end provider integration tests located in the src/e2e directory. These tests are designed for manual execution and are not part of the CI pipeline.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/ai-core/README.md#2025-04-23_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npnpm run test:e2e:all\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key in Environment File\nDESCRIPTION: Instructions for creating a .env.local file in the sveltekit-openai example directory and adding the OpenAI API key. This step is crucial for authenticating with the OpenAI service.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/sveltekit-openai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=<your key>\n```\n\n----------------------------------------\n\nTITLE: Initializing Next.js Project with AI SDK and FastAPI Example using pnpm\nDESCRIPTION: This command uses pnpm to set up a new Next.js project with the AI SDK and FastAPI example from the Vercel AI repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-fastapi/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-fastapi next-fastapi-app\n```\n\n----------------------------------------\n\nTITLE: Multi-modal Tool Response Implementation in TypeScript\nDESCRIPTION: Demonstrates how to implement multi-modal tool responses using convertToCoreMessages with a screenshot tool example.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/31-convert-to-core-messages.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { tool } from 'ai';\nimport { z } from 'zod';\n\nconst screenshotTool = tool({\n  parameters: z.object({}),\n  execute: async () => 'imgbase64',\n  experimental_toToolResultContent: result => [{ type: 'image', data: result }],\n});\n\nconst result = streamText({\n  model: openai('gpt-4'),\n  messages: convertToCoreMessages(messages, {\n    tools: {\n      screenshot: screenshotTool,\n    },\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Installing the Luma Provider (Bash)\nDESCRIPTION: Installs the Luma provider package (@ai-sdk/luma) using npm. This package is required to integrate Luma AI's image generation capabilities into applications using the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/luma/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/luma\n```\n\n----------------------------------------\n\nTITLE: Replacing responseMessages with response.messages Property in TypeScript\nDESCRIPTION: Illustrates the removal of the `responseMessages` property from `generateText` and `streamText` results in AI SDK 4.0. The messages should now be accessed via the `messages` property within the `response` object.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_26\n\nLANGUAGE: typescript\nCODE:\n```\nconst { text, responseMessages } = await generateText({\n  // ...\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst { text, response } = await generateText({\n  // ...\n});\n\nconst responseMessages = response.messages;\n```\n\n----------------------------------------\n\nTITLE: Installing AI SDK Chat Example with Next.js and OpenAI using yarn\nDESCRIPTION: This command uses yarn to create a new Next.js application with the AI SDK, OpenAI, and Sentry integration example.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry-sentry/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry-sentry next-openai-telemetry-sentry-app\n```\n\n----------------------------------------\n\nTITLE: Importing Default DeepInfra Provider - TypeScript\nDESCRIPTION: Imports the default instance of the DeepInfra provider from the @ai-sdk/deepinfra module. This instance uses default settings, including any API key from the DEEPINFRA_API_KEY environment variable, and is typically used as a starting point for all requests to DeepInfra models via the SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/11-deepinfra.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepinfra } from '@ai-sdk/deepinfra';\n```\n\n----------------------------------------\n\nTITLE: Installing Zhipu AI Provider Package\nDESCRIPTION: Installs the `zhipu-ai-provider` package using common JavaScript package managers (pnpm, npm, yarn). This package is required to interact with Zhipu AI models via the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/95-zhipu.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add zhipu-ai-provider\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm i zhipu-ai-provider\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add zhipu-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Installing Next.js Project with AI SDK and Google Vertex AI Example\nDESCRIPTION: This command uses create-next-app to bootstrap a new Next.js project with the AI SDK and Google Vertex AI example. It clones the example from the Vercel AI repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-google-vertex/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-google-vertex-edge next-vertex-edge-app\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Compatible SDK via yarn\nDESCRIPTION: Installs the `@ai-sdk/openai-compatible` package using the yarn package manager. This dependency allows the Vercel AI SDK to connect to the OpenAI-compatible API provided by LM Studio.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/30-lmstudio.mdx#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/openai-compatible\n```\n\n----------------------------------------\n\nTITLE: Basic Text Generation with OpenAI o1-mini\nDESCRIPTION: Demonstrates basic text generation using the AI SDK with OpenAI o1-mini model. Shows how to initialize the SDK and generate text responses.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/23-o1.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { text } = await generateText({\n  model: openai('o1-mini'),\n  prompt: 'Explain the concept of quantum entanglement.',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Default Ollama Provider Instance (TypeScript)\nDESCRIPTION: Imports the default pre-configured instance of the Ollama provider from the `ollama-ai-provider` package. This instance can be used directly to interact with Ollama models.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/03-ollama.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ollama } from 'ollama-ai-provider';\n```\n\n----------------------------------------\n\nTITLE: Setting Up Basic Provider Registry\nDESCRIPTION: Initializes a provider registry with multiple AI providers and custom configuration options.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/45-provider-management.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { createProviderRegistry } from 'ai';\n\nexport const registry = createProviderRegistry({\n  // register provider with prefix and default setup:\n  anthropic,\n\n  // register provider with prefix and custom setup:\n  openai: createOpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Running AI Core Examples\nDESCRIPTION: This command demonstrates how to run any AI Core example script using pnpm and tsx. The path to the example script should be specified relative to the examples/ai-core directory.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/ai-core/README.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npnpm tsx src/path/to/example.ts\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Compatible SDK with npm\nDESCRIPTION: Installs the `@ai-sdk/openai-compatible` package using the npm package manager. This package enables interaction with OpenAI-compatible APIs such as NVIDIA NIM through the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/35-nim.mdx#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @ai-sdk/openai-compatible\n```\n\n----------------------------------------\n\nTITLE: Installing Next.js Chat App with PNPM\nDESCRIPTION: Command to create a new Next.js application using PNPM with the OpenAI rate limits example template.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-upstash-rate-limits/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-rate-limits next-openai-rate-limits-app\n```\n\n----------------------------------------\n\nTITLE: Accessing Prompt Caching Metadata\nDESCRIPTION: Illustrates how to access information about prompt caching, specifically the number of cached prompt tokens, from the `providerMetadata` returned by `generateText`. Prompt caching is automatic for supported models and long prompts.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-openai.mdx#_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\nconst { text, usage, providerMetadata } = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: `A 1024-token or longer prompt...`, // Example long prompt\n});\n\nconsole.log(`usage:`, {\n  ...usage,\n  cachedPromptTokens: providerMetadata?.openai?.cachedPromptTokens,\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json for Custom AI SDK Provider in JavaScript\nDESCRIPTION: Provides a sample `package.json` structure for the custom provider NPM package. It specifies the package name (e.g., `@company-name/example`), version, and essential dependencies like `@ai-sdk/openai-compatible`, `@ai-sdk/provider`, and `@ai-sdk/provider-utils` needed for the provider to function. Additional dependencies, scripts, and build configurations would typically be included for a complete package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"name\": \"@company-name/example\",\n  \"version\": \"0.0.1\",\n  \"dependencies\": {\n    \"@ai-sdk/openai-compatible\": \"^0.0.7\",\n    \"@ai-sdk/provider\": \"^1.0.2\",\n    \"@ai-sdk/provider-utils\": \"^2.0.4\",\n    // ...additional dependencies\n  },\n  // ...additional scripts and module build configuration\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing DeepSeek Provider Initialization in TypeScript\nDESCRIPTION: This code snippet demonstrates the setup of the DeepSeek provider, a core component that enables interaction with DeepSeek's AI services within a TypeScript environment. It relies on dependencies like '@ai-sdk/provider' and '@ai-sdk/openai-compatible' for provider management and compatibility layers.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/deepseek/CHANGELOG.md#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { deepseekProvider } from \"@ai-sdk/deepseek\";\nimport { createProvider } from \"@ai-sdk/provider\";\nimport { getOpenAICompatibility } from \"@ai-sdk/openai-compatible\";\n\n// Initialize DeepSeek provider with necessary dependencies\nconst provider = createProvider({\n  name: \"DeepSeekAI\",\n  createProviderFn: () => deepseekProvider\n});\n\n// Setup OpenAI compatibility layer for smooth integration\nconst openAICompat = getOpenAICompatibility({\n  provider,\n  options: {}\n});\n\n// Export configured provider for use across modules\nexport { provider, openAICompat }\n```\n\n----------------------------------------\n\nTITLE: Installing @ai-sdk/elevenlabs using npm\nDESCRIPTION: Installs the ElevenLabs provider package for the AI SDK using the npm package manager. This command adds `@ai-sdk/elevenlabs` as a dependency to your project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/90-elevenlabs.mdx#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @ai-sdk/elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Installing Hume Provider for Vercel AI SDK (Bash)\nDESCRIPTION: This command installs the `@ai-sdk/hume` package using npm. This package is required to utilize the Hume provider functionalities within the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/hume/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/hume\n```\n\n----------------------------------------\n\nTITLE: Enabling Telemetry in AI SDK Calls\nDESCRIPTION: TypeScript example of enabling experimental telemetry in AI SDK generateText function call.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langwatch.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt:\n    'Explain why a chicken would make a terrible astronaut, be creative and humorous about it.',\n  experimental_telemetry: {\n    isEnabled: true,\n    // optional metadata\n    metadata: {\n      userId: 'myuser-123',\n      threadId: 'mythread-123',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Intercepting Fetch Requests with SambaNova Provider in TypeScript\nDESCRIPTION: Example of intercepting fetch requests when using the SambaNova provider, demonstrating how to log request details before sending.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/96-sambanova.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createSambaNova } from 'sambanova-ai-provider';\nimport { generateText } from 'ai';\n\nconst sambanovaProvider = createSambaNova({\n  apiKey: 'YOUR_API_KEY',\n  fetch: async (url, options) => {\n    console.log('URL', url);\n    console.log('Headers', JSON.stringify(options.headers, null, 2));\n    console.log(`Body ${JSON.stringify(JSON.parse(options.body), null, 2)}`);\n    return await fetch(url, options);\n  },\n});\n\nconst model = sambanovaProvider('Meta-Llama-3.1-70B-Instruct');\n\nconst { text } = await generateText({\n  model,\n  prompt: 'Hello, nice to meet you.',\n});\n```\n\n----------------------------------------\n\nTITLE: Reading Streamable Value and Updating Component State (Before)\nDESCRIPTION: This client component reads the streamable value returned by the server action and updates component state. It shows how to consume the stream using readStreamableValue and update the UI as partial notifications are received.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#2025-04-23_snippet_13\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useState } from 'react';\nimport { readStreamableValue } from 'ai/rsc';\nimport { generateSampleNotifications } from '@/app/actions';\n\nexport default function Page() {\n  const [notifications, setNotifications] = useState(null);\n\n  return (\n    <div>\n      <button\n        onClick={async () => {\n          const { partialNotificationsStream } =\n            await generateSampleNotifications();\n\n          for await (const partialNotifications of readStreamableValue(\n            partialNotificationsStream,\n          )) {\n            if (partialNotifications) {\n              setNotifications(partialNotifications.notifications);\n            }\n          }\n        }}\n      >\n        Generate\n      </button>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a FriendliAI Model Instance in TypeScript\nDESCRIPTION: Creates a FriendliAI language model instance using the imported `friendli` provider. The model ID (e.g., 'meta-llama-3.1-8b-instruct') is passed as the first argument.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = friendli('meta-llama-3.1-8b-instruct');\n```\n\n----------------------------------------\n\nTITLE: Installing @ai-sdk/elevenlabs using pnpm\nDESCRIPTION: Installs the ElevenLabs provider package for the AI SDK using the pnpm package manager. This command adds `@ai-sdk/elevenlabs` as a dependency to your project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/90-elevenlabs.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add @ai-sdk/elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Creating AI Context Configuration\nDESCRIPTION: Sets up AI context configuration using createAI to manage UI and AI state with submitUserMessage action.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/04-multistep-interfaces.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createAI } from 'ai/rsc';\nimport { submitUserMessage } from './actions';\n\nexport const AI = createAI<any[], React.ReactNode[]>({\n  initialUIState: [],\n  initialAIState: [],\n  actions: {\n    submitUserMessage,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing the AssemblyAI Provider Package (Bash)\nDESCRIPTION: Installs the `@ai-sdk/assemblyai` npm package using the Node Package Manager (npm). This package is required to utilize the AssemblyAI transcription provider within the Vercel AI SDK framework.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/assemblyai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/assemblyai\n```\n\n----------------------------------------\n\nTITLE: Importing zodSchema Function in TypeScript\nDESCRIPTION: This code snippet shows how to import the zodSchema function from the 'ai' package. This import is necessary to use the zodSchema function in your TypeScript code.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/26-zod-schema.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { zodSchema } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Tracing AI SDK Text Generation with LangSmith\nDESCRIPTION: Example of using AISDKExporter to trace text generation with the AI SDK in a Next.js environment.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AISDKExporter } from 'langsmith/vercel';\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nawait streamText({\n  model: openai('gpt-4o-mini'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  experimental_telemetry: AISDKExporter.getSettings(),\n});\n```\n\n----------------------------------------\n\nTITLE: Renaming useChat streamMode Option to streamProtocol in TypeScript\nDESCRIPTION: Shows the renaming of the `streamMode` option to `streamProtocol` within the `useChat` hook configuration for AI SDK 4.0.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_35\n\nLANGUAGE: typescript\nCODE:\n```\nconst { messages } = useChat({\n  streamMode: 'text',\n  // ...\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst { messages } = useChat({\n  streamProtocol: 'text',\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Installing LangDB Vercel Provider (Shell)\nDESCRIPTION: Installs the `@langdb/vercel-provider` package using pnpm, npm, or yarn package managers. This package is required to use LangDB with the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/94-langdb.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add @langdb/vercel-provider\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @langdb/vercel-provider\n```\n\nLANGUAGE: shell\nCODE:\n```\nyarn add @langdb/vercel-provider\n```\n\n----------------------------------------\n\nTITLE: Installing Inflection AI Provider via npm\nDESCRIPTION: Installs the unofficial Inflection AI provider package using the npm command-line interface. This package is required to use Inflection AI models with the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/93-inflection-ai.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i inflection-ai-sdk-provider\n```\n\n----------------------------------------\n\nTITLE: Updating keepLastMessageOnError Default in useChat Hook\nDESCRIPTION: Demonstrates the removal of explicit keepLastMessageOnError parameter as it now defaults to true.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_39\n\nLANGUAGE: typescript\nCODE:\n```\nconst { messages } = useChat({\n  keepLastMessageOnError: true,\n  // ...\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst { messages } = useChat({\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Speech Generation with OpenAI\nDESCRIPTION: Demonstrates basic usage of generateSpeech function with OpenAI's TTS model to convert text to audio.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/37-speech.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { readFile } from 'fs/promises';\n\nconst audio = await generateSpeech({\n  model: openai.speech('tts-1'),\n  text: 'Hello, world!',\n  voice: 'alloy',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing embedMany Function in TypeScript\nDESCRIPTION: This snippet shows how to import the embedMany function from the 'ai' package. It's a simple import statement that allows you to use the function in your code.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/06-embed-many.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { embedMany } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Installing Next.js Chat App with Yarn\nDESCRIPTION: Command to create a new Next.js application using Yarn with the OpenAI rate limits example template.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-upstash-rate-limits/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-rate-limits next-openai-rate-limits-app\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Prompt in TypeScript\nDESCRIPTION: This example shows how to use a basic text prompt with the generateText function to create content with a simple string instruction.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: yourModel,\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Headers to Transcription Requests\nDESCRIPTION: Shows how to add custom headers to transcription requests using the headers parameter.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/36-transcription.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { readFile } from 'fs/promises';\n\nconst transcript = await transcribe({\n  model: openai.transcription('whisper-1'),\n  audio: await readFile('audio.mp3'),\n  headers: { 'X-Custom-Header': 'custom-value' },\n});\n```\n\n----------------------------------------\n\nTITLE: Defining CoreSystemMessage Type in TypeScript\nDESCRIPTION: Defines the structure for a system message containing system information. It includes a role of 'system' and a content string.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/30-core-message.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ntype CoreSystemMessage = {\n  role: 'system';\n  content: string;\n};\n```\n\n----------------------------------------\n\nTITLE: Installing Next.js OpenAI App with yarn\nDESCRIPTION: Command to create a new Next.js application using the AI SDK and OpenAI example template with yarn.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-pages/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-pages next-openai-app\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Groq Language Models\nDESCRIPTION: Example of generating text using a Groq language model with the AI SDK generateText function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/09-groq.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { groq } from '@ai-sdk/groq';\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: groq('gemma2-9b-it'),\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Mistral AI Provider Instance\nDESCRIPTION: Example of creating a customized Mistral AI provider instance with specific settings using the createMistral function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/20-mistral.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createMistral } from '@ai-sdk/mistral';\n\nconst mistral = createMistral({\n  // custom settings\n});\n```\n\n----------------------------------------\n\nTITLE: Installing xAI Grok Package (pnpm)\nDESCRIPTION: This snippet demonstrates installing the xAI Grok provider using pnpm. It's a prerequisite for using the xAI Grok models.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\npnpm add @ai-sdk/xai\n```\n\n----------------------------------------\n\nTITLE: Installing Mixedbread Provider with Package Managers - Shell\nDESCRIPTION: These shell commands install the mixedbread-ai-provider into your project using pnpm, npm, or yarn. No dependencies except a working Node.js environment and the relevant package manager are required. Run any one of these in your project directory to add the Mixedbread provider for further usage in your TypeScript/JavaScript code.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/60-mixedbread.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add mixedbread-ai-provider\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm install mixedbread-ai-provider\n```\n\nLANGUAGE: shell\nCODE:\n```\nyarn add mixedbread-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Provider-Specific Settings Configuration\nDESCRIPTION: Example of setting provider-specific options using the providerOptions parameter.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/37-speech.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { readFile } from 'fs/promises';\n\nconst audio = await generateSpeech({\n  model: openai.speech('tts-1'),\n  text: 'Hello, world!',\n  providerOptions: {\n    openai: {\n      // ...\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Importing the Perplexity Provider\nDESCRIPTION: Code snippet showing how to import the default perplexity provider instance from the package. This provider instance is used to create model configurations.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/perplexity/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { perplexity } from '@ai-sdk/perplexity';\n```\n\n----------------------------------------\n\nTITLE: Chat Message Loading Implementation\nDESCRIPTION: Implements the loadChat function to retrieve messages from the file system storage.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-message-persistence.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nimport { Message } from 'ai';\nimport { readFile } from 'fs/promises';\n\nexport async function loadChat(id: string): Promise<Message[]> {\n  return JSON.parse(await readFile(getChatFile(id), 'utf8'));\n}\n```\n\n----------------------------------------\n\nTITLE: Replacing LanguageModelResponseMetadataWithHeaders Type in TypeScript\nDESCRIPTION: Illustrates the removal of the `LanguageModelResponseMetadataWithHeaders` type in AI SDK 4.0. The `LanguageModelResponseMetadata` type should be used instead.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_28\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LanguageModelResponseMetadataWithHeaders } from 'ai';\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LanguageModelResponseMetadata } from 'ai';\n```\n\n----------------------------------------\n\nTITLE: Running SSE Client Example\nDESCRIPTION: Command to run the SSE client example. This executes the client-side implementation using the SSE transport, connecting to the previously started server.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#2025-04-23_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npnpm sse:client\n```\n\n----------------------------------------\n\nTITLE: Installing Luma AI SDK Module via npm (Shell)\nDESCRIPTION: Installs the @ai-sdk/luma package using npm. Provides the Luma AI provider dependency, enabling access to Luma's image generation APIs in JavaScript/TypeScript projects. Requires npm installed and a project context.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/80-luma.mdx#2025-04-23_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nnpm install @ai-sdk/luma\n```\n\n----------------------------------------\n\nTITLE: Inspecting HTTP Request Bodies in TypeScript\nDESCRIPTION: Demonstrates how to inspect raw HTTP request bodies for debugging purposes when working with model providers like OpenAI.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/20-prompt-engineering.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  prompt: 'Hello, world!',\n});\n\nconsole.log(result.request.body);\n```\n\n----------------------------------------\n\nTITLE: Configuring UI Update Throttling\nDESCRIPTION: Example showing how to implement throttling for UI updates during completion streaming.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/05-completion.mdx#2025-04-23_snippet_6\n\nLANGUAGE: tsx\nCODE:\n```\nconst { completion, ... } = useCompletion({\n  // Throttle the completion and data updates to 50ms:\n  experimental_throttle: 50\n})\n```\n\n----------------------------------------\n\nTITLE: Manual Trace Creation with LangWatch\nDESCRIPTION: TypeScript example of manually creating a trace using LangWatch for capturing messages.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langwatch.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LangWatch } from 'langwatch';\n\nconst langwatch = new LangWatch();\n\nconst trace = langwatch.getTrace({\n  metadata: { threadId: 'mythread-123', userId: 'myuser-123' },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Provider Registry with Custom Separator\nDESCRIPTION: Creates a provider registry with a custom separator for provider and model IDs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/45-provider-management.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { openai } from '@ai-sdk/openai';\n\nexport const customSeparatorRegistry = createProviderRegistry(\n  {\n    anthropic,\n    openai,\n  },\n  { separator: ' > ' },\n);\n```\n\n----------------------------------------\n\nTITLE: Initializing LangWatch Client with API Key\nDESCRIPTION: TypeScript code snippet showing how to initialize the LangWatch client with an API key.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langwatch.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LangWatch } from 'langwatch';\n\nconst langwatch = new LangWatch({\n  apiKey: 'your_api_key_here',\n});\n```\n\n----------------------------------------\n\nTITLE: Importing useChat in React\nDESCRIPTION: Shows how to import the useChat hook from the AI SDK in a React application\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { useChat } from '@ai-sdk/react'\n```\n\n----------------------------------------\n\nTITLE: Importing generateSpeech Function in TypeScript\nDESCRIPTION: This code snippet shows how to import the generateSpeech function from the 'ai' package. The function is imported as an experimental feature.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/12-generate-speech.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_generateSpeech as generateSpeech } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Nest.js AI SDK Project\nDESCRIPTION: This command installs the necessary dependencies for the Nest.js AI SDK project. It should be run from the root directory of the AI SDK repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/nest/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSeek Provider with Package Managers\nDESCRIPTION: Installation commands for adding the DeepSeek provider to your project using different package managers (pnpm, npm, or yarn).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/30-deepseek.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/deepseek\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/deepseek\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/deepseek\n```\n\n----------------------------------------\n\nTITLE: Installing Gladia Provider Package\nDESCRIPTION: Shows how to install the Gladia provider package using different package managers (pnpm, npm, yarn).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/120-gladia.mdx#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/gladia\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/gladia\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/gladia\n```\n\n----------------------------------------\n\nTITLE: Installing React Components for AI SDK\nDESCRIPTION: Installation command for the React-specific package of AI SDK UI to build chatbots and generative user interfaces in React applications.\nSOURCE: https://github.com/vercel/ai/blob/main/README.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @ai-sdk/react\n```\n\n----------------------------------------\n\nTITLE: Defining Attachment Metadata for Attachments in Messages (TypeScript)\nDESCRIPTION: This snippet defines the 'Attachment' interface for describing metadata associated with a message attachment, such as the file name. The structure is intended to support optional metadata only (name is optional), and is suitable for extensibility to richer metadata objects in the future. No dependencies, simple object input and output, and very limited constraints or fields at this level.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#2025-04-23_snippet_8\n\nLANGUAGE: TypeScript\nCODE:\n```\ninterface Attachment {\n  name?: string; // The name of the attachment, usually the file name (optional).\n}\n\n```\n\n----------------------------------------\n\nTITLE: Querying OpenAI with Legacy Provider in API Route Handler - Next.js TypeScript\nDESCRIPTION: This TypeScript/Next.js snippet demonstrates how to handle a POST API request by querying the OpenAI model using their SDK directly, streaming the output via OpenAIStream, and returning it with StreamingTextResponse. It requires the 'openai' and 'ai' NPM packages, as well as access to an OpenAI API key in an environment variable. The function expects a JSON body containing 'messages', initiates a completion request, and streams the result. Limitation: tied directly to OpenAI's SDK and not provider-agnostic.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/39-migration-guide-3-1.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nimport OpenAI from 'openai';\nimport { OpenAIStream, StreamingTextResponse } from 'ai';\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY!,\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const response = await openai.chat.completions.create({\n    model: 'gpt-4-turbo',\n    stream: true,\n    messages,\n  });\n\n  const stream = OpenAIStream(response);\n\n  return new StreamingTextResponse(stream);\n}\n```\n\n----------------------------------------\n\nTITLE: Adding experimental_onFunctionCall Stub to OpenAIStream in React\nDESCRIPTION: A code snippet demonstrating how to add the required experimental_onFunctionCall stub to OpenAIStream to enable correct forwarding of function calls to the client. This is necessary after upgrading the AI SDK to v3.0.20 or newer.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/02-client-side-function-calls-not-invoked.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nconst stream = OpenAIStream(response, {\n  async experimental_onFunctionCall() {\n    return;\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Simulated Streaming Middleware\nDESCRIPTION: Demonstrates how to use simulateStreamingMiddleware to add streaming capabilities to non-streaming models.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { wrapLanguageModel, simulateStreamingMiddleware } from 'ai';\n\nconst model = wrapLanguageModel({\n  model: yourModel,\n  middleware: simulateStreamingMiddleware(),\n});\n```\n\n----------------------------------------\n\nTITLE: Installing xAI Grok Package (yarn)\nDESCRIPTION: This snippet demonstrates installing the xAI Grok provider using yarn. It's a prerequisite for using the xAI Grok models.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nyarn add @ai-sdk/xai\n```\n\n----------------------------------------\n\nTITLE: Version 0.0.1 - Amazon Bedrock Provider\nDESCRIPTION: Initial release of Amazon Bedrock provider and related utility updates.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/amazon-bedrock/CHANGELOG.md#2025-04-23_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n## 0.0.1\n\n### Patch Changes\n\n- 02f6a088: feat (@ai-sdk/amazon-bedrock): add Amazon Bedrock provider\n- Updated dependencies [02f6a088]\n  - @ai-sdk/provider-utils@0.0.16\n```\n\n----------------------------------------\n\nTITLE: Environment Variables Configuration\nDESCRIPTION: Required environment variables setup for deploying the Slack chatbot application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/03-slackbot.mdx#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nSLACK_BOT_TOKEN=your_slack_bot_token\nSLACK_SIGNING_SECRET=your_slack_signing_secret\nOPENAI_API_KEY=your_openai_api_key\nEXA_API_KEY=your_exa_api_key\n```\n\n----------------------------------------\n\nTITLE: Installing AI SDK UI for React\nDESCRIPTION: Command to install the AI SDK UI package for React. This is necessary to use the SDK's UI components and hooks in a React application.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/README.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @ai-sdk/react\n```\n\n----------------------------------------\n\nTITLE: Installing Crosshatch Provider via yarn - Shell\nDESCRIPTION: Installs the '@crosshatch/ai-provider' NPM package with yarn, an alternative JavaScript package manager. This command registers Crosshatch as a dependency, making it available for import and use in your project code. yarn must be installed and the command run from your project root.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/21-crosshatch.mdx#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nyarn add @crosshatch/ai-provider\n```\n\n----------------------------------------\n\nTITLE: Using Binary Images in User Messages in TypeScript\nDESCRIPTION: This example shows how to include binary image data (Buffer) in a user message for multimodal AI processing.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model,\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'Describe the image in detail.' },\n        {\n          type: 'image',\n          image: fs.readFileSync('./data/comic-cat.png'),\n        },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Importing cosineSimilarity Function\nDESCRIPTION: Code snippet showing how to import the cosineSimilarity function from the ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/50-cosine-similarity.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { cosineSimilarity } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Installing AI SDK OpenAI Provider and LangSmith Client\nDESCRIPTION: Commands for installing the AI SDK OpenAI provider and LangSmith client using different package managers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langsmith.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/openai langsmith\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/openai langsmith\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/openai langsmith\n```\n\n----------------------------------------\n\nTITLE: Reinitializing Git Repository\nDESCRIPTION: Commands to reset Git repository when encountering 'remote origin already exists' error.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/10-vercel-deployment-guide.mdx#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf .git\ngit init\ngit add .\ngit commit -m \"init\"\n```\n\n----------------------------------------\n\nTITLE: Importing the Default AssemblyAI Provider Instance in TypeScript\nDESCRIPTION: Shows how to import the default provider instance from the AssemblyAI module for use with the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/100-assemblyai.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { assemblyai } from '@ai-sdk/assemblyai';\n```\n\n----------------------------------------\n\nTITLE: Importing useActions from AI SDK RSC\nDESCRIPTION: Shows how to import the useActions hook from the AI SDK RSC package for use in client components.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/09-use-actions.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { useActions } from \"ai/rsc\"\n```\n\n----------------------------------------\n\nTITLE: Running Development Server for AI SDK Examples\nDESCRIPTION: Command for starting the development server for framework examples in the AI SDK, which allows testing the examples in a browser.\nSOURCE: https://github.com/vercel/ai/blob/main/CONTRIBUTING.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm dev\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Environment in Nuxt\nDESCRIPTION: Sets up the OpenAI API key in the environment file for authentication with OpenAI services.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=xxxxxxxxx\n```\n\n----------------------------------------\n\nTITLE: Running the Fastify Server with AI SDK\nDESCRIPTION: Command to start the development server, which runs the Fastify application integrated with the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/fastify/README.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npnpm dev\n```\n\n----------------------------------------\n\nTITLE: Installing AI SDK Dependencies with Package Managers\nDESCRIPTION: Commands for installing AI SDK and related packages using different package managers. Includes options for pnpm, npm, and yarn.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm add -D ai @ai-sdk/openai @ai-sdk/svelte zod\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -D ai @ai-sdk/openai @ai-sdk/svelte zod\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add -D ai @ai-sdk/openai @ai-sdk/svelte zod\n```\n\n----------------------------------------\n\nTITLE: Initializing Google Generative AI Provider: Facade Replacement - TypeScript\nDESCRIPTION: Conveys the change from the deprecated Google facade instantiation to the createGoogleGenerativeAI factory function for Google provider clients. Requires ai-sdk version 4.0+ and proper function import. Intended for API authentication and configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_4\n\nLANGUAGE: ts\nCODE:\n```\nconst google = new Google({\n  // ...\n});\n```\n\nLANGUAGE: ts\nCODE:\n```\nconst google = createGoogleGenerativeAI({\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing Generated Audio Data\nDESCRIPTION: Shows how to access the audio data from the generated speech output as a Uint8Array.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/37-speech.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst audio = audio.audioData; // audio data e.g. Uint8Array\n```\n\n----------------------------------------\n\nTITLE: Importing AWSBedrockCohereStream in React\nDESCRIPTION: Import statement for the AWSBedrockCohereStream function from the ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/11-aws-bedrock-cohere-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { AWSBedrockCohereStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Importing DeepSeek Provider\nDESCRIPTION: Shows how to import the default DeepSeek provider instance from the package.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/deepseek/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepseek } from '@ai-sdk/deepseek';\n```\n\n----------------------------------------\n\nTITLE: Installing ElevenLabs Provider Package\nDESCRIPTION: Command to install the ElevenLabs provider package using npm package manager.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/elevenlabs/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Importing render Function from AI SDK RSC\nDESCRIPTION: Import statement for the deprecated render function from the AI SDK RSC package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/20-render.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { render } from \"ai/rsc\"\n```\n\n----------------------------------------\n\nTITLE: Importing LanguageModelV1Middleware from AI Package\nDESCRIPTION: Shows how to import the LanguageModelV1Middleware class from the ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/65-language-model-v1-middleware.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { LanguageModelV1Middleware } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Version Entry 1.2.9\nDESCRIPTION: Changelog entry noting addition of URL-based PDF document support\nSOURCE: https://github.com/vercel/ai/blob/main/packages/anthropic/CHANGELOG.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## 1.2.9\n\n### Patch Changes\n\n- aeba38e: Add support for URL-based PDF documents in the Anthropic provider\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Entry - Version 2.2.16\nDESCRIPTION: Documents patches for version 2.2.16, focusing on interface resynchronization.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/CHANGELOG.md#2025-04-23_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n## 2.2.16\n\n### Patch Changes\n\n- e569688: Fix for #637, resync interfaces\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Hume Speech Model\nDESCRIPTION: Shows how to initialize a basic Hume speech model using the factory method.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/150-hume.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = hume.speech();\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install all required dependencies using pnpm package manager.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/node-http-server/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Installing workers-ai-provider with yarn - Bash\nDESCRIPTION: This yarn command installs the workers-ai-provider dependency, enabling use of Cloudflare Workers AI models. Yarn must be available in your environment. The result is the addition of the package to your project dependencies.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/11-cloudflare-workers-ai.mdx#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn add workers-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Provider Options at Function Call Level in TypeScript\nDESCRIPTION: This example demonstrates how to add provider-specific options at the function call level for the Azure OpenAI provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst { text } = await generateText({\n  model: azure('your-deployment-name'),\n  providerOptions: {\n    openai: {\n      reasoningEffort: 'low',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Importing experimental_createMCPClient from AI SDK\nDESCRIPTION: Shows how to import the experimental_createMCPClient function from the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/21-create-mcp-client.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_createMCPClient } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Installing pnpm for AI SDK Development\nDESCRIPTION: Command for installing pnpm v9 globally using npm, which is required for AI SDK development.\nSOURCE: https://github.com/vercel/ai/blob/main/CONTRIBUTING.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g pnpm@9\n```\n\n----------------------------------------\n\nTITLE: Importing useAssistant in React\nDESCRIPTION: Import statement for using the useAssistant hook in a React application\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/20-use-assistant.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { useAssistant } from '@ai-sdk/react'\n```\n\n----------------------------------------\n\nTITLE: Importing Fal Provider\nDESCRIPTION: How to import the default fal provider instance from the package\nSOURCE: https://github.com/vercel/ai/blob/main/packages/fal/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fal } from '@ai-sdk/fal';\n```\n\n----------------------------------------\n\nTITLE: Importing Chat in Svelte\nDESCRIPTION: Shows how to import the Chat component from the AI SDK in a Svelte application\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Chat } from '@ai-sdk/svelte'\n```\n\n----------------------------------------\n\nTITLE: Importing CohereStream in React\nDESCRIPTION: Shows how to import the CohereStream utility from the AI package in a React application\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/13-cohere-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CohereStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Creating a new Solid.js project using npm\nDESCRIPTION: This snippet demonstrates how to create a new Solid.js project using npm, either in the current directory or in a specified directory.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/solidstart-openai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# create a new project in the current directory\nnpm init solid@latest\n\n# create a new project in my-app\nnpm init solid@latest my-app\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Deletion in Chatbot UI\nDESCRIPTION: This example demonstrates how to add a delete functionality to individual messages in the chat interface using the setMessages function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nconst { messages, setMessages, ... } = useChat()\n\nconst handleDelete = (id) => {\n  setMessages(messages.filter(message => message.id !== id))\n}\n\nreturn <>\n  {messages.map(message => (\n    <div key={message.id}>\n      {message.role === 'user' ? 'User: ' : 'AI: '}\n      {message.content}\n      <button onClick={() => handleDelete(message.id)}>Delete</button>\n    </div>\n  ))}\n  ...\n```\n\n----------------------------------------\n\nTITLE: Version 0.0.2 - Bedrock Configuration\nDESCRIPTION: Addition of custom Bedrock configuration support.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/amazon-bedrock/CHANGELOG.md#2025-04-23_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n## 0.0.2\n\n### Patch Changes\n\n- 542a2b28: feat (@ai-sdk/bedrock): support custom bedrock configuration\n```\n\n----------------------------------------\n\nTITLE: Installing DeepInfra Provider Package\nDESCRIPTION: Command to install the DeepInfra provider package using npm\nSOURCE: https://github.com/vercel/ai/blob/main/packages/deepinfra/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/deepinfra\n```\n\n----------------------------------------\n\nTITLE: Custom Provider Import Statement\nDESCRIPTION: Basic import statement for the customProvider function from the AI package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/42-custom-provider.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport {  customProvider } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Package Version Header\nDESCRIPTION: Package name and version identifier for the Anthropic provider module\nSOURCE: https://github.com/vercel/ai/blob/main/packages/anthropic/CHANGELOG.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# @ai-sdk/anthropic\n```\n\n----------------------------------------\n\nTITLE: Importing Fireworks Provider\nDESCRIPTION: Example showing how to import the default Fireworks provider instance.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/fireworks/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fireworks } from '@ai-sdk/fireworks';\n```\n\n----------------------------------------\n\nTITLE: Importing useCompletion Hook - Vue\nDESCRIPTION: Shows how to import the useCompletion hook in a Vue application\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/02-use-completion.mdx#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { useCompletion } from '@ai-sdk/vue'\n```\n\n----------------------------------------\n\nTITLE: Configuring Next.js for OpenTelemetry\nDESCRIPTION: JavaScript configuration for enabling instrumentationHook in Next.js config file.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langwatch.mdx#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  experimental: {\n    instrumentationHook: true,\n  },\n};\n\nmodule.exports = nextConfig;\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Slogan Generator Prompt in Markdown\nDESCRIPTION: This snippet demonstrates a simple prompt for generating a coffee shop slogan using an inline prompt component.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/01-prompt-engineering.mdx#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<InlinePrompt initialInput=\"Create a slogan for a coffee shop.\" />\n```\n\n----------------------------------------\n\nTITLE: Importing LlamaIndexAdapter Module\nDESCRIPTION: Shows how to import the LlamaIndexAdapter module from the ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/16-llamaindex-adapter.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LlamaIndexAdapter } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Importing appendClientMessage from AI Library\nDESCRIPTION: Shows how to import the appendClientMessage function from the AI package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/33-append-client-message.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { appendClientMessage } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Starting Development Server\nDESCRIPTION: Command to start the development server using pnpm.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/node-http-server/README.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npnpm dev\n```\n\n----------------------------------------\n\nTITLE: Defining Stop Sequences Type in TypeScript\nDESCRIPTION: Specifies the type for the optional `stopSequences` parameter as an array of strings (`string[]`). If the model generates any of these sequences, it will halt further text generation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nstring[]\n```\n\n----------------------------------------\n\nTITLE: Basic Image Generation with Fal Provider\nDESCRIPTION: Complete example of generating and saving an image using the fal provider with the flux/schnell model. Shows how to generate an image from a prompt and save it to a file.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/fal/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fal } from '@ai-sdk/fal';\nimport { experimental_generateImage as generateImage } from 'ai';\nimport fs from 'fs';\nconst { image } = await generateImage({\n  model: fal.image('fal-ai/flux/schnell'),\n  prompt: 'A cat wearing a intricate robe',\n});\n\nconst filename = `image-${Date.now()}.png`;\nfs.writeFileSync(filename, image.uint8Array);\nconsole.log(`Image saved to ${filename}`);\n```\n\n----------------------------------------\n\nTITLE: Adding Metadata to Traces in Laminar\nDESCRIPTION: Add key-value pair metadata to traces for filtering in Laminar.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/laminar.mdx#2025-04-23_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Laminar } from '@lmnr-ai/lmnr';\nconst { text } = await generateText({\n  model: openai('gpt-4o-mini'),\n  prompt: `Write a poem about Laminar flow.`,\n  experimental_telemetry: {\n    isEnabled: true,\n    metadata: {\n      'my-key': 'my-value',\n      'another-key': 'another-value',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Fireworks Provider Package\nDESCRIPTION: Command to install the Fireworks provider package using npm.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/fireworks/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/fireworks\n```\n\n----------------------------------------\n\nTITLE: Example Structure of Grounding Metadata Response (JSON)\nDESCRIPTION: Provides a JSON example of the structure returned for grounding metadata. This response includes search queries, entry points, and granular support mappings showing which response segments are grounded by which search results. Key fields include 'webSearchQueries', 'searchEntryPoint', and 'groundingSupports'. Primarily used as a reference for expected response formats when implementing or parsing Gemini model outputs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#2025-04-23_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"groundingMetadata\": {\n    \"webSearchQueries\": [\"What's the weather in Chicago this weekend?\"],\n    \"searchEntryPoint\": {\n      \"renderedContent\": \"...\"\n    },\n    \"groundingSupports\": [\n      {\n        \"segment\": {\n          \"startIndex\": 0,\n          \"endIndex\": 65,\n          \"text\": \"Chicago weather changes rapidly, so layers let you adjust easily.\"\n        },\n        \"groundingChunkIndices\": [0],\n        \"confidenceScores\": [0.99]\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Vercel Function Duration\nDESCRIPTION: TypeScript configuration to set maximum duration for Vercel serverless functions using route segment config.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/10-vercel-deployment-guide.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nexport const maxDuration = 30;\n```\n\n----------------------------------------\n\nTITLE: Creating a Bash Command for SvelteKit Project Initialization\nDESCRIPTION: Command to create a new SvelteKit application named 'my-ai-app' using npx.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx sv create my-ai-app\n```\n\n----------------------------------------\n\nTITLE: Defining LLM Request Parameter Types - Vercel AI (TypeScript)\nDESCRIPTION: This snippet defines the available options and their associated types for configuring LLM requests in the Vercel AI SDK via TypeScript type or interface declarations. It covers settings such as retry management, HTTP abort control, HTTP headers, provider/toolset integration, tool choice logic, callback handlers for token streaming, provider-specific metadata, and post-response events. Dependencies may include zod (for schema validation), React for UI types, and possibly Fetch API abort mechanisms. Inputs are objects matching the type definitions; outputs are validated by TypeScript's type system.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/01-stream-ui.mdx#2025-04-23_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\n// LLM Request option types for the Vercel AI framework\n{\n  name: 'maxRetries',\n  type: 'number',\n  isOptional: true,\n  description:\n    'Maximum number of retries. Set to 0 to disable retries. Default: 2.',\n},\n{\n  name: 'abortSignal',\n  type: 'AbortSignal',\n  isOptional: true,\n  description:\n    'An optional abort signal that can be used to cancel the call.',\n},\n{\n  name: 'headers',\n  type: 'Record<string, string>',\n  isOptional: true,\n  description:\n    'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.',\n},\n{\n  name: 'tools',\n  type: 'ToolSet',\n  description:\n    'Tools that are accessible to and can be called by the model.',\n  properties: [\n    {\n      type: 'Tool',\n      parameters: [\n        {\n          name: 'description',\n          isOptional: true,\n          type: 'string',\n          description:\n            'Information about the purpose of the tool including details on how and when it can be used by the model.',\n        },\n        {\n          name: 'parameters',\n          type: 'zod schema',\n          description:\n            'The typed schema that describes the parameters of the tool that can also be used to validation and error handling.',\n        },\n        {\n          name: 'generate',\n          isOptional: true,\n          type: '(async (parameters) => ReactNode) | AsyncGenerator<ReactNode, ReactNode, void>',\n          description:\n            'A function or a generator function that is called with the arguments from the tool call and yields React nodes as the UI.',\n        },\n      ],\n    },\n  ],\n},\n{\n  name: 'toolChoice',\n  isOptional: true,\n  type: '\"auto\" | \"none\" | \"required\" | { \"type\": \"tool\", \"toolName\": string }',\n  description:\n    'The tool choice setting. It specifies how tools are selected for execution. The default is \"auto\". \"none\" disables tool execution. \"required\" requires tools to be executed. { \"type\": \"tool\", \"toolName\": string } specifies a specific tool to execute.',\n},\n{\n  name: 'text',\n  isOptional: true,\n  type: '(Text) => ReactNode',\n  description: 'Callback to handle the generated tokens from the model.',\n  properties: [\n    {\n      type: 'Text',\n      parameters: [\n        {\n          name: 'content',\n          type: 'string',\n          description: 'The full content of the completion.',\n        },\n        { name: 'delta', type: 'string', description: 'The delta.' },\n        { name: 'done', type: 'boolean', description: 'Is it done?' },\n      ],\n    },\n  ],\n},\n{\n  name: 'providerOptions',\n  type: 'Record<string,Record<string,JSONValue>> | undefined',\n  isOptional: true,\n  description:\n    'Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.',\n},\n{\n  name: 'onFinish',\n  type: '(result: OnFinishResult) => void',\n  isOptional: true,\n  description:\n    'Callback that is called when the LLM response and all request tool executions (for tools that have a `generate` function) are finished.',\n  properties: [\n    {\n      type: 'OnFinishResult',\n      parameters: [\n        {\n          name: 'usage',\n          type: 'TokenUsage',\n          description: 'The token usage of the generated text.',\n          properties: [\n            {\n              type: 'TokenUsage',\n              parameters: [\n                {\n                  name: 'promptTokens',\n                  type: 'number',\n                  description: 'The total number of tokens in the prompt.',\n                },\n                {\n                  name: 'completionTokens',\n                  type: 'number',\n                  description:\n                    'The total number of tokens in the completion.',\n                },\n                {\n                  name: 'totalTokens',\n                  type: 'number',\n                  description: 'The total number of tokens generated.',\n                },\n              ],\n            },\n          ],\n        },\n        {\n          name: 'value',\n          type: 'ReactNode',\n          description: 'The final ui node that was generated.',\n        },\n        {\n          name: 'warnings',\n          type: 'Warning[] | undefined',\n          description:\n            'Warnings from the model provider (e.g. unsupported settings).',\n        },\n        {\n          name: 'rawResponse',\n          type: 'RawResponse',\n          description: 'Optional raw response data.',\n          properties: [\n            {\n              type: 'RawResponse',\n              parameters: [\n                {\n                  name: 'headers',\n                  isOptional: true,\n                  type: 'Record<string, string>',\n                  description: 'Response headers.',\n                },\n              ],\n            },\n          ],\n        },\n      ],\n    },\n  ],\n},\n]\n\n```\n\n----------------------------------------\n\nTITLE: Initializing SambaNova Model in TypeScript\nDESCRIPTION: Example of initializing a SambaNova model using the provider instance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/96-sambanova.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = sambanova('Meta-Llama-3.1-70B-Instruct');\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key in Next.js Environment\nDESCRIPTION: This code snippet shows how to set up the OpenAI API key in a .env.local file for use with the AI SDK's OpenAI provider in a Next.js application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#2025-04-23_snippet_2\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=xxxxxxxxx\n```\n\n----------------------------------------\n\nTITLE: Importing Default SambaNova Provider in TypeScript\nDESCRIPTION: Import statement for the default SambaNova provider instance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/96-sambanova.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { sambanova } from 'sambanova-ai-provider';\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Deepgram Provider Instance in TypeScript\nDESCRIPTION: Create a customized Deepgram provider instance with specific settings such as a custom fetch implementation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/110-deepgram.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createDeepgram } from '@ai-sdk/deepgram';\n\nconst deepgram = createDeepgram({\n  // custom settings, e.g.\n  fetch: customFetch,\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Rate Limiting for API Protection in TypeScript with Vercel KV and Upstash Ratelimit\nDESCRIPTION: This code snippet demonstrates how to implement rate limiting for an API endpoint using Vercel KV and Upstash Ratelimit. It sets a limit of 5 requests per 30 seconds for each IP address and handles streaming responses from OpenAI.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/06-rate-limiting.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport kv from '@vercel/kv';\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport { Ratelimit } from '@upstash/ratelimit';\nimport { NextRequest } from 'next/server';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n// Create Rate limit\nconst ratelimit = new Ratelimit({\n  redis: kv,\n  limiter: Ratelimit.fixedWindow(5, '30s'),\n});\n\nexport async function POST(req: NextRequest) {\n  // call ratelimit with request ip\n  const ip = req.ip ?? 'ip';\n  const { success, remaining } = await ratelimit.limit(ip);\n\n  // block the request if unsuccessfull\n  if (!success) {\n    return new Response('Ratelimited!', { status: 429 });\n  }\n\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-3.5-turbo'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI API Key in .env File\nDESCRIPTION: This code shows how to set up the .env.local file with the OpenAI API key that will be used for authentication with OpenAI services.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#2025-04-23_snippet_3\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=xxxxxxxxx\n```\n\n----------------------------------------\n\nTITLE: Configuring Next.js for Instrumentation Hook\nDESCRIPTION: Enable the experimental instrumentation hook in Next.js configuration for versions 13.4 â¤ Next.js < 15.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/laminar.mdx#2025-04-23_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nmodule.exports = {\n  experimental: {\n    instrumentationHook: true,\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Importing and Using generateId in TypeScript\nDESCRIPTION: Demonstrates how to import and use the generateId function to create a unique identifier. The function can optionally take a size parameter to specify the length of the generated ID.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/90-generate-id.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateId } from 'ai';\n\nconst id = generateId();\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateId } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Traceloop in Next.js\nDESCRIPTION: Sets up the OpenTelemetry exporter endpoint and authorization header for Traceloop in a Next.js application's .env file.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/traceloop.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOTEL_EXPORTER_OTLP_ENDPOINT=https://api.traceloop.com\nOTEL_EXPORTER_OTLP_HEADERS=\"Authorization=Bearer <Your API Key>\"\n```\n\n----------------------------------------\n\nTITLE: Generating Images with LangDB and Vercel AI SDK (TypeScript)\nDESCRIPTION: Shows how to use the LangDB provider with the experimental `generateImage` function from the Vercel AI SDK (`ai` package). It specifies the image model using `langdb.image('openai/dall-e-3')` and provides a prompt. The generated image data (as a Uint8Array) is then saved to a file using Node.js `fs` module.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/94-langdb.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nimport { createLangDB } from '@langdb/vercel-provider';\nimport { experimental_generateImage as generateImage } from 'ai';\nimport fs from 'fs';\nimport path from 'path';\n\nconst langdb = createLangDB({\n  apiKey: process.env.LANGDB_API_KEY,\n  projectId: 'your-project-id',\n});\n\nexport async function generateImageExample() {\n  const { images } = await generateImage({\n    model: langdb.image('openai/dall-e-3'),\n    prompt: 'A delighted resplendent quetzal mid-flight amidst raindrops',\n  });\n\n  const imagePath = path.join(__dirname, 'generated-image.png');\n  fs.writeFileSync(imagePath, images[0].uint8Array);\n  console.log(`Image saved to: ${imagePath}`);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Step Tool Calls\nDESCRIPTION: Final version that enables multi-step tool interactions using maxSteps and adds step completion logging, allowing the AI to process and respond with weather information over multiple steps.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#2025-04-23_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { CoreMessage, streamText, tool } from 'ai';\nimport dotenv from 'dotenv';\nimport { z } from 'zod';\nimport * as readline from 'node:readline/promises';\n\ndotenv.config();\n\nconst terminal = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n});\n\nconst messages: CoreMessage[] = [];\n\nasync function main() {\n  while (true) {\n    const userInput = await terminal.question('You: ');\n\n    messages.push({ role: 'user', content: userInput });\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages,\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (in Celsius)',\n          parameters: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => ({\n            location,\n            temperature: Math.round((Math.random() * 30 + 5) * 10) / 10,\n          }),\n        }),\n      },\n      maxSteps: 5,\n      onStepFinish: step => {\n        console.log(JSON.stringify(step, null, 2));\n      },\n    });\n\n    let fullResponse = '';\n    process.stdout.write('\\nAssistant: ');\n    for await (const delta of result.textStream) {\n      fullResponse += delta;\n      process.stdout.write(delta);\n    }\n    process.stdout.write('\\n\\n');\n\n    messages.push({ role: 'assistant', content: fullResponse });\n  }\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Updating Options Parameter in useChat Hook\nDESCRIPTION: Demonstrates the migration from the options parameter to direct headers and body parameters in useChat hook configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_37\n\nLANGUAGE: typescript\nCODE:\n```\nconst { messages } = useChat({\n  options: {\n    headers: {\n      'X-Custom-Header': 'value',\n    },\n  },\n  // ...\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst { messages } = useChat({\n  headers: {\n    'X-Custom-Header': 'value',\n  },\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Transcription\nDESCRIPTION: Shows how to implement error handling for transcription failures, including accessing error metadata and causes.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/36-transcription.mdx#2025-04-23_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  experimental_transcribe as transcribe,\n  NoTranscriptGeneratedError,\n} from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { readFile } from 'fs/promises';\n\ntry {\n  await transcribe({\n    model: openai.transcription('whisper-1'),\n    audio: await readFile('audio.mp3'),\n  });\n} catch (error) {\n  if (NoTranscriptGeneratedError.isInstance(error)) {\n    console.log('NoTranscriptGeneratedError');\n    console.log('Cause:', error.cause);\n    console.log('Responses:', error.responses);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic OpenAI o3-mini Text Generation with AI SDK\nDESCRIPTION: Demonstrates how to generate text using OpenAI o3-mini model through AI SDK Core. Shows the minimal setup required to make API calls to the model.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/24-o3.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nconst { text } = await generateText({\n  model: openai('o3-mini'),\n  prompt: 'Explain the concept of quantum entanglement.',\n});\n```\n\n----------------------------------------\n\nTITLE: Documenting Version Changes for Google Provider - Markdown\nDESCRIPTION: This code snippet uses Markdown syntax to record version history and patch notes for the @ai-sdk/google package. The changelog includes version headers, lists of features, fixes, and updated dependencies, along with commit hashes and brief descriptions of each change. It requires no external dependencies to render as documentation but assumes a Markdown viewer or IDE plugin for optimal readability. Inputs are version numbers, commit hashes, and free-form text describing each change; the output is a formatted changelog entry delineating the project's evolution. There are no parameterized constraints, though entries should adhere to semantic versioning conventions and be comprehensive for auditability.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/google/CHANGELOG.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# @ai-sdk/google\n\n## 1.2.13\n\n### Patch Changes\n\n- 6183b08: feat(providers/google): Add taskType support for Text Embedding Models\n\n## 1.2.12\n\n### Patch Changes\n\n- c56331d: feat (providers/google): add thinking config to provider options\n\n## 1.2.11\n\n### Patch Changes\n\n- Updated dependencies [beef951]\n  - @ai-sdk/provider@1.1.3\n  - @ai-sdk/provider-utils@2.2.7\n\n## 1.2.10\n\n### Patch Changes\n\n- Updated dependencies [013faa8]\n  - @ai-sdk/provider@1.1.2\n  - @ai-sdk/provider-utils@2.2.6\n\n## 1.2.9\n\n### Patch Changes\n\n- Updated dependencies [c21fa6d]\n  - @ai-sdk/provider-utils@2.2.5\n  - @ai-sdk/provider@1.1.1\n\n## 1.2.8\n\n### Patch Changes\n\n- 1e8e66d: fix (provider/google): allow \"OFF\" for Google HarmBlockThreshold\n\n## 1.2.7\n\n### Patch Changes\n\n- 1789884: feat: add provider option schemas for vertex imagegen and google genai\n\n## 1.2.6\n\n### Patch Changes\n\n- Updated dependencies [2c19b9a]\n  - @ai-sdk/provider-utils@2.2.4\n\n## 1.2.5\n\n### Patch Changes\n\n- Updated dependencies [28be004]\n  - @ai-sdk/provider-utils@2.2.3\n\n## 1.2.4\n\n### Patch Changes\n\n- Updated dependencies [b01120e]\n  - @ai-sdk/provider-utils@2.2.2\n\n## 1.2.3\n\n### Patch Changes\n\n- 871df87: feat (provider/google): add gemini-2.5-pro-exp-03-25 to model list\n\n## 1.2.2\n\n### Patch Changes\n\n- Updated dependencies [f10f0fa]\n  - @ai-sdk/provider-utils@2.2.1\n\n## 1.2.1\n\n### Patch Changes\n\n- 994a13b: feat (provider/google): support IMAGE_SAFETY finish reason\n\n## 1.2.0\n\n### Minor Changes\n\n- 5bc638d: AI SDK 4.2\n\n### Patch Changes\n\n- Updated dependencies [5bc638d]\n  - @ai-sdk/provider@1.1.0\n  - @ai-sdk/provider-utils@2.2.0\n\n## 1.1.27\n\n### Patch Changes\n\n- d0c4659: feat (provider-utils): parseProviderOptions function\n- Updated dependencies [d0c4659]\n  - @ai-sdk/provider-utils@2.1.15\n\n## 1.1.26\n\n### Patch Changes\n\n- 0bd5bc6: feat (ai): support model-generated files\n- Updated dependencies [0bd5bc6]\n  - @ai-sdk/provider@1.0.12\n  - @ai-sdk/provider-utils@2.1.14\n\n## 1.1.25\n\n### Patch Changes\n\n- Updated dependencies [2e1101a]\n  - @ai-sdk/provider@1.0.11\n  - @ai-sdk/provider-utils@2.1.13\n\n## 1.1.24\n\n### Patch Changes\n\n- 5261762: fix (provider/google): ensure correct finishReason for tool calls in streaming response\n\n## 1.1.23\n\n### Patch Changes\n\n- 413f5a7: feat (providers/google): add gemma 3 model id\n\n## 1.1.22\n\n### Patch Changes\n\n- 62f46fd: feat (provider/google): add support for dynamic retrieval\n\n## 1.1.21\n\n### Patch Changes\n\n- Updated dependencies [1531959]\n  - @ai-sdk/provider-utils@2.1.12\n\n## 1.1.20\n\n### Patch Changes\n\n- e1d3d42: feat (ai): expose raw response body in generateText and generateObject\n- Updated dependencies [e1d3d42]\n  - @ai-sdk/provider@1.0.10\n  - @ai-sdk/provider-utils@2.1.11\n\n## 1.1.19\n\n### Patch Changes\n\n- 2c27583: fix (provider/google): support empty content in malformed function call responses\n\n## 1.1.18\n\n### Patch Changes\n\n- 5c8f512: feat (provider/google): add seed support\n\n## 1.1.17\n\n### Patch Changes\n\n- Updated dependencies [ddf9740]\n  - @ai-sdk/provider@1.0.9\n  - @ai-sdk/provider-utils@2.1.10\n\n## 1.1.16\n\n### Patch Changes\n\n- 1b2e2a0: fix (provider/google): add resilience against undefined parts\n\n## 1.1.15\n\n### Patch Changes\n\n- Updated dependencies [2761f06]\n  - @ai-sdk/provider@1.0.8\n  - @ai-sdk/provider-utils@2.1.9\n\n## 1.1.14\n\n### Patch Changes\n\n- 08a3641: fix (provider/google): support nullable enums in schema\n\n## 1.1.13\n\n### Patch Changes\n\n- Updated dependencies [2e898b4]\n  - @ai-sdk/provider-utils@2.1.8\n\n## 1.1.12\n\n### Patch Changes\n\n- Updated dependencies [3ff4ef8]\n  - @ai-sdk/provider-utils@2.1.7\n\n## 1.1.11\n\n### Patch Changes\n\n- 6eb7fc4: feat (ai/core): url source support\n\n## 1.1.10\n\n### Patch Changes\n\n- e5567f7: feat (provider/google): update model ids\n\n## 1.1.9\n\n### Patch Changes\n\n- b2573de: fix (provider/google): remove reasoning text following removal from Gemini API\n\n## 1.1.8\n\n### Patch Changes\n\n- Updated dependencies [d89c3b9]\n  - @ai-sdk/provider@1.0.7\n  - @ai-sdk/provider-utils@2.1.6\n\n## 1.1.7\n\n### Patch Changes\n\n- d399f25: feat (provider/google-vertex): support public file urls in messages\n\n## 1.1.6\n\n### Patch Changes\n\n- e012cd8: feat (provider/google): add reasoning support\n\n## 1.1.5\n\n### Patch Changes\n\n- Updated dependencies [3a602ca]\n  - @ai-sdk/provider-utils@2.1.5\n\n## 1.1.4\n\n### Patch Changes\n\n- Updated dependencies [066206e]\n  - @ai-sdk/provider-utils@2.1.4\n\n## 1.1.3\n\n### Patch Changes\n\n- Updated dependencies [39e5c1f]\n  - @ai-sdk/provider-utils@2.1.3\n\n## 1.1.2\n\n### Patch Changes\n\n- Updated dependencies [ed012d2]\n- Updated dependencies [3a58a2e]\n  - @ai-sdk/provider-utils@2.1.2\n  - @ai-sdk/provider@1.0.6\n\n## 1.1.1\n\n### Patch Changes\n\n- Updated dependencies [e7a9ec9]\n- Updated dependencies [0a699f1]\n  - @ai-sdk/provider-utils@2.1.1\n  - @ai-sdk/provider@1.0.5\n\n## 1.1.0\n\n### Minor Changes\n\n- 62ba5ad: release: AI SDK 4.1\n\n### Patch Changes\n\n- Updated dependencies [62ba5ad]\n  - @ai-sdk/provider-utils@2.1.0\n\n## 1.0.17\n\n### Patch Changes\n\n- Updated dependencies [00114c5]\n  - @ai-sdk/provider-utils@2.0.8\n\n## 1.0.16\n\n### Patch Changes\n\n- 4eb9b41: fix (provider/google): support conversion of string enums to openapi spec\n\n## 1.0.15\n\n### Patch Changes\n\n- 7611964: feat (provider/xai): Support structured output for latest models.\n\n## 1.0.14\n\n### Patch Changes\n\n- Updated dependencies [90fb95a]\n- Updated dependencies [e6dfef4]\n- Updated dependencies [6636db6]\n  - @ai-sdk/provider-utils@2.0.7\n\n## 1.0.13\n\n### Patch Changes\n\n- Updated dependencies [19a2ce7]\n- Updated dependencies [19a2ce7]\n- Updated dependencies [6337688]\n  - @ai-sdk/provider@1.0.4\n  - @ai-sdk/provider-utils@2.0.6\n\n## 1.0.12\n\n### Patch Changes\n\n- 5ed5e45: chore (config): Use ts-library.json tsconfig for no-UI libs.\n- Updated dependencies [5ed5e45]\n  - @ai-sdk/provider-utils@2.0.5\n  - @ai-sdk/provider@1.0.3\n\n## 1.0.11\n\n### Patch Changes\n\n- db31e74: feat: adding audioTimestamp support to GoogleGenerativeAISettings\n\n## 1.0.10\n\n### Patch Changes\n\n- e07439a: feat (provider/google): Include safety ratings response detail.\n- 4017b0f: feat (provider/google-vertex): Enhance grounding metadata response detail.\n- a9df182: feat (provider/google): Add support for search grounding.\n\n## 1.0.9\n\n### Patch Changes\n\n- c0b1c7e: feat (provider/google): Add Gemini 2.0 model.\n\n## 1.0.8\n\n### Patch Changes\n\n- b7372dc: feat (provider/google): Include optional response grounding metadata.\n\n## 1.0.7\n\n### Patch Changes\n\n- Updated dependencies [09a9cab]\n  - @ai-sdk/provider@1.0.2\n  - @ai-sdk/provider-utils@2.0.4\n\n## 1.0.6\n\n### Patch Changes\n\n- 9e54403: fix (provider/google-vertex): support empty object as usage metadata\n\n## 1.0.5\n\n### Patch Changes\n\n- 0984f0b: feat (provider/google-vertex): Rewrite for Edge runtime support.\n- Updated dependencies [0984f0b]\n  - @ai-sdk/provider-utils@2.0.3\n\n## 1.0.4\n\n### Patch Changes\n\n- 6373c60: fix (provider/google): send json schema into provider\n\n## 1.0.3\n\n### Patch Changes\n\n- Updated dependencies [b446ae5]\n  - @ai-sdk/provider@1.0.1\n  - @ai-sdk/provider-utils@2.0.2\n\n## 1.0.2\n\n### Patch Changes\n\n- b748dfb: feat (providers): update model lists\n\n## 1.0.1\n\n### Patch Changes\n\n- Updated dependencies [c3ab5de]\n  - @ai-sdk/provider-utils@2.0.1\n\n## 1.0.0\n\n### Major Changes\n\n- 66060f7: chore (release): bump major version to 4.0\n- 0d3d3f5: chore (providers): remove baseUrl option\n- 36b03b0: chore (provider/google): remove topK model setting\n- 277fc6b: chore (provider/google): remove facade\n\n### Patch Changes\n\n- c38a0db: fix (provider/google): allow empty candidates array when streaming\n- 0509c34: fix (provider/google): add name/content details to tool responses\n- Updated dependencies [b469a7e]\n- Updated dependencies [dce4158]\n- Updated dependencies [c0ddc24]\n- Updated dependencies [b1da952]\n- Updated dependencies [dce4158]\n- Updated dependencies [8426f55]\n- Updated dependencies [db46ce5]\n  - @ai-sdk/provider-utils@2.0.0\n  - @ai-sdk/provider@1.0.0\n\n## 1.0.0-canary.6\n\n### Patch Changes\n\n- c38a0db: fix (provider/google): allow empty candidates array when streaming\n\n## 1.0.0-canary.5\n\n### Patch Changes\n\n- 0509c34: fix (provider/google): add name/content details to tool responses\n\n## 1.0.0-canary.4\n\n### Major Changes\n\n- 36b03b0: chore (provider/google): remove topK model setting\n- 277fc6b: chore (provider/google): remove facade\n\n## 1.0.0-canary.3\n\n### Patch Changes\n\n- Updated dependencies [8426f55]\n  - @ai-sdk/provider-utils@2.0.0-canary.3\n\n## 1.0.0-canary.2\n\n### Patch Changes\n\n- Updated dependencies [dce4158]\n- Updated dependencies [dce4158]\n  - @ai-sdk/provider-utils@2.0.0-canary.2\n\n## 1.0.0-canary.1\n\n### Major Changes\n\n- 0d3d3f5: chore (providers): remove baseUrl option\n\n### Patch Changes\n\n- Updated dependencies [b1da952]\n  - @ai-sdk/provider-utils@2.0.0-canary.1\n\n## 1.0.0-canary.0\n\n### Major Changes\n\n- 66060f7: chore (release): bump major version to 4.0\n\n### Patch Changes\n\n- Updated dependencies [b469a7e]\n- Updated dependencies [c0ddc24]\n- Updated dependencies [db46ce5]\n  - @ai-sdk/provider-utils@2.0.0-canary.0\n  - @ai-sdk/provider@1.0.0-canary.0\n\n## 0.0.55\n\n### Patch Changes\n\n- 11beb9d: fix (provider/google): support tool calls without parameters\n\n## 0.0.54\n\n### Patch Changes\n\n- 1486128: feat: add supportsUrl to language model specification\n- 1486128: feat (provider/google): support native file URLs without download\n- 3b1b69a: feat: provider-defined tools\n- Updated dependencies [aa98cdb]\n- Updated dependencies [1486128]\n- Updated dependencies [7b937c5]\n- Updated dependencies [3b1b69a]\n- Updated dependencies [811a317]\n  - @ai-sdk/provider-utils@1.0.22\n  - @ai-sdk/provider@0.0.26\n\n## 0.0.53\n\n### Patch Changes\n\n- b9b0d7b: feat (ai): access raw request body\n- Updated dependencies [b9b0d7b]\n  - @ai-sdk/provider@0.0.25\n  - @ai-sdk/provider-utils@1.0.21\n\n## 0.0.52\n\n### Patch Changes\n\n- 7944e61: feat (provider/google): enable frequencyPenalty and presencePenalty settings\n\n## 0.0.51\n\n### Patch Changes\n\n- d595d0d: feat (ai/core): file content parts\n- Updated dependencies [d595d0d]\n  - @ai-sdk/provider@0.0.24\n  - @ai-sdk/provider-utils@1.0.20\n\n## 0.0.50\n\n```\n\n----------------------------------------\n\nTITLE: Installing Google Vertex AI Provider with Package Managers\nDESCRIPTION: Commands to install the Google Vertex AI provider package using different package managers. This installs the @ai-sdk/google-vertex package which is required to use Google Vertex AI with the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/google-vertex\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/google-vertex\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/google-vertex @google-cloud/vertexai\n```\n\n----------------------------------------\n\nTITLE: Configuring Error Message Handling\nDESCRIPTION: Shows how to customize error message handling in the streamText response using getErrorMessage function.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_12\n\nLANGUAGE: ts\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse({\n    getErrorMessage: error => {\n      if (error == null) {\n        return 'unknown error';\n      }\n\n      if (typeof error === 'string') {\n        return error;\n      }\n\n      if (error instanceof Error) {\n        return error.message;\n      }\n\n      return JSON.stringify(error);\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Migrating Stream Mode in useCompletion Hook\nDESCRIPTION: Shows the change from streamMode to streamProtocol parameter in the useCompletion hook.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_40\n\nLANGUAGE: typescript\nCODE:\n```\nconst { text } = useCompletion({\n  streamMode: 'text',\n  // ...\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst { text } = useCompletion({\n  streamProtocol: 'text',\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Refining Slogan Generator Prompt with Specificity in Markdown\nDESCRIPTION: This snippet shows how to make the slogan generator prompt more specific by adding a descriptive term.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/01-prompt-engineering.mdx#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<InlinePrompt initialInput=\"Create a slogan for an organic coffee shop.\" />\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables in Nuxt Config\nDESCRIPTION: Adds the OpenAI API key to the Nuxt runtime configuration to make it accessible in server routes.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nexport default defineNuxtConfig({\n  // rest of your nuxt config\n  runtimeConfig: {\n    openaiApiKey: process.env.OPENAI_API_KEY,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Importing the default Rev.ai provider instance\nDESCRIPTION: Shows how to import the default Rev.ai provider instance from the SDK module. This is the simplest way to get started with the Rev.ai provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/160-revai.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { revai } from '@ai-sdk/revai';\n```\n\n----------------------------------------\n\nTITLE: Adding Patch Changeset for AI SDK Updates\nDESCRIPTION: Command for creating a patch changeset in the AI SDK workspace, which is required when updating packages to ensure they're released.\nSOURCE: https://github.com/vercel/ai/blob/main/CONTRIBUTING.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npnpm changeset\n```\n\n----------------------------------------\n\nTITLE: Implementing Deepgram Transcription\nDESCRIPTION: Complete example showing how to use the Deepgram provider with the AI SDK's transcribe function to process an audio file using the nova-3 model.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/deepgram/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { deepgram } from '@ai-sdk/deepgram';\nimport { experimental_transcribe as transcribe } from 'ai';\n\nconst { text } = await transcribe({\n  model: deepgram.transcription('nova-3'),\n  audio: new URL(\n    'https://github.com/vercel/ai/raw/refs/heads/main/examples/ai-core/data/galileo.mp3',\n  ),\n});\n```\n\n----------------------------------------\n\nTITLE: Renaming parseStreamPart to parseDataStreamPart in TypeScript\nDESCRIPTION: Illustrates the renaming of the `parseStreamPart` function to `parseDataStreamPart` in AI SDK 4.0. This function is used to parse a line from the data stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_20\n\nLANGUAGE: typescript\nCODE:\n```\nconst part = parseStreamPart(line);\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst part = parseDataStreamPart(line);\n```\n\n----------------------------------------\n\nTITLE: Testing Server Endpoint\nDESCRIPTION: cURL command to test the HTTP server endpoint with a POST request.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/node-http-server/README.md#2025-04-23_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncurl -X POST http://localhost:8080\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Vertex topK Setting Migration - TypeScript\nDESCRIPTION: Demonstrates updating usage of the model-specific topK parameter for the Google Vertex provider to set topK on the generateText call instead. Input: vertex model provider, topK value. Output: generated text result.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_6\n\nLANGUAGE: ts\nCODE:\n```\nconst result = await generateText({\n  model: vertex('gemini-1.5-flash', {\n    topK: 0.5,\n  }),\n});\n```\n\nLANGUAGE: ts\nCODE:\n```\nconst result = await generateText({\n  model: vertex('gemini-1.5-flash'),\n  topK: 0.5,\n});\n```\n\n----------------------------------------\n\nTITLE: Importing LangChainAdapter in TypeScript\nDESCRIPTION: Shows how to import the LangChainAdapter from the ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/16-langchain-adapter.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LangChainAdapter } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Creating Next.js App with Yarn for AI SDK and OpenAI Integration\nDESCRIPTION: This command uses Yarn to create a new Next.js application with the AI SDK and OpenAI integration example. It initializes the project and installs required packages.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry/app/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app\n```\n\n----------------------------------------\n\nTITLE: Implementing the Text Generation API Route in Next.js\nDESCRIPTION: A server-side route handler for the /api/completion endpoint that processes incoming POST requests. It extracts the prompt from the request body, uses the generateText function from the AI SDK with OpenAI's GPT-4 model, and returns the generated text as a JSON response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/10-generate-text.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nexport async function POST(req: Request) {\n  const { prompt }: { prompt: string } = await req.json();\n\n  const { text } = await generateText({\n    model: openai('gpt-4'),\n    system: 'You are a helpful assistant.',\n    prompt,\n  });\n\n  return Response.json({ text });\n}\n```\n\n----------------------------------------\n\nTITLE: Building and Deploying to Vercel\nDESCRIPTION: These commands build the Nuxt project and deploy it to Vercel. The project is configured to use Vercel Edge Functions by default.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/nuxt-openai/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm run build\nvercel deploy\n```\n\n----------------------------------------\n\nTITLE: Importing Default ElevenLabs Provider in TypeScript\nDESCRIPTION: Imports the default pre-configured instance of the ElevenLabs provider from the `@ai-sdk/elevenlabs` package. This instance uses default settings, including reading the API key from the `ELEVENLABS_API_KEY` environment variable.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/90-elevenlabs.mdx#2025-04-23_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { elevenlabs } from '@ai-sdk/elevenlabs';\n```\n\n----------------------------------------\n\nTITLE: Creating Streamable UI with render() and OpenAI SDK - Next.js TypeScript\nDESCRIPTION: This Server Action (TypeScript/Next.js) uses the legacy 'render' function from the AI SDK RSC API to stream a UI response powered by OpenAI. It imports and configures the OpenAI SDK, utility, and UI components, and enables a tool (get_city_weather) with Zod for input validation and async generator for handling asynchronous UI updates. Requires 'ai/rsc', 'openai', 'zod', and custom components/utilities. The function is specific to OpenAI SDK and is less flexible than new Core-based APIs.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/39-migration-guide-3-1.mdx#2025-04-23_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nimport { render } from 'ai/rsc';\nimport OpenAI from 'openai';\nimport { z } from 'zod';\nimport { Spinner, Weather } from '@/components';\nimport { getWeather } from '@/utils';\n\nconst openai = new OpenAI();\n\nasync function submitMessage(userInput = 'What is the weather in SF?') {\n  'use server';\n\n  return render({\n    provider: openai,\n    model: 'gpt-4-turbo',\n    messages: [\n      { role: 'system', content: 'You are a helpful assistant' },\n      { role: 'user', content: userInput },\n    ],\n    text: ({ content }) => <p>{content}</p>,\n    tools: {\n      get_city_weather: {\n        description: 'Get the current weather for a city',\n        parameters: z\n          .object({\n            city: z.string().describe('the city'),\n          })\n          .required(),\n        render: async function* ({ city }) {\n          yield <Spinner />;\n          const weather = await getWeather(city);\n          return <Weather info={weather} />;\n        },\n      },\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Metadata Extractor in TypeScript\nDESCRIPTION: Provides an example implementation of a `MetadataExtractor` object. It includes an `extractMetadata` function for processing complete responses and a `createStreamExtractor` function factory for handling streaming responses, allowing extraction of custom or non-standard data returned by the provider API.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/index.mdx#2025-04-23_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst myMetadataExtractor: MetadataExtractor = {\n  // Process complete, non-streaming responses\n  extractMetadata: ({ parsedBody }) => {\n    // You have access to the complete raw response\n    // Extract any fields the provider includes\n    return {\n      myProvider: {\n        standardUsage: parsedBody.usage,\n        experimentalFeatures: parsedBody.beta_features,\n        customMetrics: {\n          processingTime: parsedBody.server_timing?.total_ms,\n          modelVersion: parsedBody.model_version,\n          // ... any other provider-specific data\n        },\n      },\n    };\n  },\n\n  // Process streaming responses\n  createStreamExtractor: () => {\n    let accumulatedData = {\n      timing: [],\n      customFields: {},\n    };\n\n    return {\n      // Process each chunk's raw data\n      processChunk: parsedChunk => {\n        if (parsedChunk.server_timing) {\n          accumulatedData.timing.push(parsedChunk.server_timing);\n        }\n        if (parsedChunk.custom_data) {\n          Object.assign(accumulatedData.customFields, parsedChunk.custom_data);\n        }\n      },\n      // Build final metadata from accumulated data\n      buildMetadata: () => ({\n        myProvider: {\n          streamTiming: accumulatedData.timing,\n          customData: accumulatedData.customFields,\n        },\n      }),\n    };\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Checking for NoSuchModelError Instance in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to determine if a given error object is an instance of `NoSuchModelError` imported from the `ai` library. It utilizes the static `isInstance` method for type-safe error checking, typically within a conditional block to handle scenarios where a specified AI model ID was not found.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/05-ai-sdk-errors/ai-no-such-model-error.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { NoSuchModelError } from 'ai';\n\nif (NoSuchModelError.isInstance(error)) {\n  // Handle the error\n}\n```\n\n----------------------------------------\n\nTITLE: Installing AI SDK Dependencies with Package Managers\nDESCRIPTION: Commands for installing the required AI SDK packages using different package managers (pnpm, npm, yarn). These commands install the core AI package, React integration, and OpenAI provider.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/02-multi-modal-chatbot.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add ai @ai-sdk/react @ai-sdk/openai\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install ai @ai-sdk/react @ai-sdk/openai\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add ai @ai-sdk/react @ai-sdk/openai\n```\n\n----------------------------------------\n\nTITLE: Installing Mistral AI SDK Package\nDESCRIPTION: Command line instructions for installing the Mistral AI provider package using different package managers (pnpm, npm, or yarn).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/20-mistral.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/mistral\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/mistral\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/mistral\n```\n\n----------------------------------------\n\nTITLE: Creating Next.js App with AI SDK and OpenAI Telemetry Example (Yarn)\nDESCRIPTION: This command uses Yarn to create a new Next.js application with the AI SDK and OpenAI telemetry example. It clones the example from the Vercel AI repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry next-openai-telemetry-app\n```\n\n----------------------------------------\n\nTITLE: Creating React Component with useChat Hook for Parallel Tool Calling\nDESCRIPTION: This snippet shows how to create a React component that uses the useChat hook from the AI SDK. It sets up a chat interface with parallel tool calling capability, limiting the maximum steps to 2 for multiple tool calls.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/71-call-tools-in-parallel.mdx#2025-04-23_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useChat } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { messages, input, setInput, append } = useChat({\n    api: '/api/chat',\n    maxSteps: 2,\n  });\n\n  return (\n    <div>\n      <input\n        value={input}\n        onChange={event => {\n          setInput(event.target.value);\n        }}\n        onKeyDown={async event => {\n          if (event.key === 'Enter') {\n            append({ content: input, role: 'user' });\n          }\n        }}\n      />\n\n      {messages.map((message, index) => (\n        <div key={index}>{message.content}</div>\n      ))}\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LMNT Provider via npm (Bash)\nDESCRIPTION: Installs the `@ai-sdk/lmnt` package using the Node Package Manager (npm). This package is required to use the LMNT text-to-speech capabilities within the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/lmnt/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/lmnt\n```\n\n----------------------------------------\n\nTITLE: Installing @ai-sdk/openai-compatible using yarn - Shell\nDESCRIPTION: Installs the @ai-sdk/openai-compatible package using yarn. This is an alternative to pnpm and npm, and achieves the same result of adding the required dependency for OpenAI-compatible API interactions.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/40-baseten.mdx#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nyarn add @ai-sdk/openai-compatible\n```\n\n----------------------------------------\n\nTITLE: Installing Ollama AI Provider (Shell)\nDESCRIPTION: Commands to install the `ollama-ai-provider` package using different JavaScript package managers (pnpm, npm, yarn). This package provides the necessary integration for using Ollama with the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/03-ollama.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add ollama-ai-provider\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm install ollama-ai-provider\n```\n\nLANGUAGE: shell\nCODE:\n```\nyarn add ollama-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Running TypeScript Examples in AI SDK Core\nDESCRIPTION: Command for executing TypeScript examples in the AI SDK Core package using tsx, which allows running TypeScript files directly.\nSOURCE: https://github.com/vercel/ai/blob/main/CONTRIBUTING.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npnpm tsx src/stream-text/openai.ts\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSeek Provider Package\nDESCRIPTION: Command to install the DeepSeek provider package using npm package manager.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/deepseek/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/deepseek\n```\n\n----------------------------------------\n\nTITLE: Importing Rev.ai Provider in TypeScript\nDESCRIPTION: Code snippet showing how to import the default provider instance from the Rev.ai package.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/revai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { revai } from '@ai-sdk/revai';\n```\n\n----------------------------------------\n\nTITLE: Running the Hono Server with AI SDK Integration\nDESCRIPTION: This command starts the development server for the Hono + AI SDK example project.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/hono/README.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npnpm dev\n```\n\n----------------------------------------\n\nTITLE: Initializing Next.js Project with AI SDK and FastAPI Example using npx\nDESCRIPTION: This command uses create-next-app to bootstrap a new Next.js project with the AI SDK and FastAPI example from the Vercel AI repository.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-fastapi/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-fastapi next-fastapi-app\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for AI Core Examples\nDESCRIPTION: This snippet shows how to set up the necessary environment variables in a .env file for running AI Core examples. It includes the OpenAI API key and mentions that additional settings may be required depending on the providers being used.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/ai-core/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n...\n```\n\n----------------------------------------\n\nTITLE: Installing @ai-sdk/valibot Package using npm\nDESCRIPTION: This shell command utilizes the Node Package Manager (npm) install command (`i` is shorthand for `install`) to download and add the `@ai-sdk/valibot` package to the current Node.js project's dependencies. This package enables the use of Valibot schemas with the Vercel AI SDK. Requires Node.js and npm to be installed.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/valibot/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/valibot\n```\n\n----------------------------------------\n\nTITLE: Version 0.0.4 - Provider Utils Update\nDESCRIPTION: Update to provider-utils dependency.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/amazon-bedrock/CHANGELOG.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n## 0.0.4\n\n### Patch Changes\n\n- Updated dependencies [d481729f]\n  - @ai-sdk/provider-utils@1.0.1\n```\n\n----------------------------------------\n\nTITLE: Copying Environment Variables Template\nDESCRIPTION: Command to create a .env file from the provided template, which will store configuration settings and API keys.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Credentials in Environment Variables\nDESCRIPTION: Environment variable setup for AWS authentication using access key, secret key, and region in a .env file.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_1\n\nLANGUAGE: makefile\nCODE:\n```\nAWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY\nAWS_REGION=YOUR_REGION\n```\n\n----------------------------------------\n\nTITLE: Environment Variables Configuration for Postgres and OpenAI\nDESCRIPTION: Example .env file configuration showing required environment variables for OpenAI API access and Postgres database connection. These credentials are necessary for the app's core functionalities.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=\"your_api_key_here\"\nPOSTGRES_URL=\"...\"\nPOSTGRES_PRISMA_URL=\"...\"\nPOSTGRES_URL_NO_SSL=\"...\"\nPOSTGRES_URL_NON_POOLING=\"...\"\nPOSTGRES_USER=\"...\"\nPOSTGRES_HOST=\"...\"\nPOSTGRES_PASSWORD=\"...\"\nPOSTGRES_DATABASE=\"...\"\n```\n\n----------------------------------------\n\nTITLE: Using useStreamableValue in React Component\nDESCRIPTION: Demonstrates how to use the useStreamableValue hook in a React component to handle streamable values, including loading and error states.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/11-use-streamable-value.mdx#2025-04-23_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nfunction MyComponent({ streamableValue }) {\n  const [data, error, pending] = useStreamableValue(streamableValue);\n\n  if (pending) return <div>Loading...</div>;\n  if (error) return <div>Error: {error.message}</div>;\n\n  return <div>Data: {data}</div>;\n}\n```\n\n----------------------------------------\n\nTITLE: Updating AssistantResponse Import\nDESCRIPTION: Shows the migration from experimental_AssistantResponse to AssistantResponse direct import.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_43\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_AssistantResponse } from 'ai';\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AssistantResponse } from 'ai';\n```\n\n----------------------------------------\n\nTITLE: Creating an API URL Generator for Expo\nDESCRIPTION: This utility function generates API URLs that work correctly across development and production environments in Expo. It handles the proper formatting of URLs based on the current environment configuration.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport Constants from 'expo-constants';\n\nexport const generateAPIUrl = (relativePath: string) => {\n  const origin = Constants.experienceUrl.replace('exp://', 'http://');\n\n  const path = relativePath.startsWith('/') ? relativePath : `/${relativePath}`;\n\n  if (process.env.NODE_ENV === 'development') {\n    return origin.concat(path);\n  }\n\n  if (!process.env.EXPO_PUBLIC_API_BASE_URL) {\n    throw new Error(\n      'EXPO_PUBLIC_API_BASE_URL environment variable is not defined',\n    );\n  }\n\n  return process.env.EXPO_PUBLIC_API_BASE_URL.concat(path);\n};\n```\n\n----------------------------------------\n\nTITLE: Installing Deepgram Provider Package\nDESCRIPTION: NPM installation command for the Deepgram provider package in the AI SDK ecosystem.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/deepgram/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/deepgram\n```\n\n----------------------------------------\n\nTITLE: Example Bedrock Streaming API Response Headers in JavaScript\nDESCRIPTION: Provides a sample JavaScript object representing the typical HTTP response headers received from a Bedrock API call when using the AI SDK's `streamText` function. Note the `content-type` indicates an event stream.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#2025-04-23_snippet_24\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  connection: 'keep-alive',\n  'content-type': 'application/vnd.amazon.eventstream',\n  date: 'Fri, 07 Feb 2025 04:33:37 GMT',\n  'transfer-encoding': 'chunked',\n  'x-amzn-requestid': 'a976e3fc-0e45-4241-9954-b9bdd80ab407'\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Google Vertex Provider for Edge Runtime\nDESCRIPTION: Code to import the Google Vertex provider for Edge runtime environments from the @ai-sdk/google-vertex/edge sub-module. Edge runtimes include Vercel Edge Functions and Cloudflare Workers.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertex } from '@ai-sdk/google-vertex/edge';\n```\n\n----------------------------------------\n\nTITLE: Importing AWSBedrockAnthropicMessagesStream in React\nDESCRIPTION: Shows how to import the AWSBedrockAnthropicMessagesStream utility from the AI package for use in React applications.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/10-aws-bedrock-messages-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { AWSBedrockAnthropicMessagesStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Importing Default Google Vertex Provider Instance\nDESCRIPTION: Code to import the default Google Vertex provider instance from the @ai-sdk/google-vertex package. This provides access to Google's Gemini models through the Vertex AI API.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { vertex } from '@ai-sdk/google-vertex';\n```\n\n----------------------------------------\n\nTITLE: Setting LangWatch API Key in Environment Variables\nDESCRIPTION: Example of setting the LangWatch API key in the .env file.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langwatch.mdx#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nLANGWATCH_API_KEY='your_api_key_here'\n```\n\n----------------------------------------\n\nTITLE: Installing Next.js OpenAI App with npx\nDESCRIPTION: Command to create a new Next.js application using the AI SDK and OpenAI example template with npx.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-pages/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-pages next-openai-app\n```\n\n----------------------------------------\n\nTITLE: Version 0.0.3 - Custom Header Support\nDESCRIPTION: Addition of custom request header support and related dependency updates.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/amazon-bedrock/CHANGELOG.md#2025-04-23_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n## 0.0.3\n\n### Patch Changes\n\n- 5edc6110: feat (ai/core): add custom request header support\n- Updated dependencies [5edc6110]\n- Updated dependencies [5edc6110]\n- Updated dependencies [5edc6110]\n  - @ai-sdk/provider@0.0.11\n  - @ai-sdk/provider-utils@1.0.0\n```\n\n----------------------------------------\n\nTITLE: Creating a Groq Language Model Instance\nDESCRIPTION: Initialize a Groq language model by passing the model ID to the provider instance.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/09-groq.mdx#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = groq('gemma2-9b-it');\n```\n\n----------------------------------------\n\nTITLE: Updating API Route with Weather Tool in SvelteKit\nDESCRIPTION: This snippet shows how to modify the API route to include a custom weather tool. It uses the 'ai' package to create a tool for fetching weather information and the 'zod' library for parameter validation.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#2025-04-23_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nimport { OPENAI_API_KEY } from '$env/static/private';\n\nconst openai = createOpenAI({\n  apiKey: OPENAI_API_KEY,\n});\n\nexport async function POST({ request }) {\n  const { messages } = await request.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n    },\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n----------------------------------------\n\nTITLE: Importing the Default Mixedbread Provider - TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to import the default mixedbread provider instance from the mixedbread-ai-provider module, making it ready to use for embedding requests. This import requires the mixedbread-ai-provider package to be installed and can be used in any TypeScript or modern JavaScript project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/60-mixedbread.mdx#2025-04-23_snippet_1\n\nLANGUAGE: ts\nCODE:\n```\nimport { mixedbread } from 'mixedbread-ai-provider';\n```\n\n----------------------------------------\n\nTITLE: Installing workers-ai-provider with pnpm - Bash\nDESCRIPTION: This snippet shows the pnpm command to install the workers-ai-provider module, which grants access to Cloudflare Workers AI models. Ensure pnpm is installed. No input parameters are required; output is the installation of the npm package. Run this at your project's root.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/11-cloudflare-workers-ai.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add workers-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Importing Bedrock Provider Instance\nDESCRIPTION: How to import the default bedrock provider instance from the package\nSOURCE: https://github.com/vercel/ai/blob/main/packages/amazon-bedrock/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bedrock } from '@ai-sdk/amazon-bedrock';\n```\n\n----------------------------------------\n\nTITLE: Importing useCompletion Hook - React\nDESCRIPTION: Shows how to import the useCompletion hook in a React application\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/02-use-completion.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { useCompletion } from '@ai-sdk/react'\n```\n\n----------------------------------------\n\nTITLE: Importing getAIState from AI SDK RSC\nDESCRIPTION: Shows how to import the getAIState function from the AI SDK RSC package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/06-get-ai-state.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { getAIState } from \"ai/rsc\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Separator in Provider Registry\nDESCRIPTION: Demonstrates how to customize the separator used between provider and model IDs in the registry. Instead of the default ':' separator, it uses a custom ' > ' separator.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/40-provider-registry.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst registry = createProviderRegistry(\n  {\n    anthropic,\n    openai,\n  },\n  { separator: ' > ' },\n);\n\n// Now you can use the custom separator\nconst model = registry.languageModel('anthropic > claude-3-opus-20240229');\n```\n\n----------------------------------------\n\nTITLE: Importing useChat in Vue\nDESCRIPTION: Shows how to import the useChat hook from the AI SDK in a Vue application\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { useChat } from '@ai-sdk/vue'\n```\n\n----------------------------------------\n\nTITLE: Importing the Default Fal Provider Instance\nDESCRIPTION: This snippet shows how to import the default Fal provider instance from the @ai-sdk/fal package for use with the AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/10-fal.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fal } from '@ai-sdk/fal';\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for OpenTelemetry Integration\nDESCRIPTION: Bash command to install necessary dependencies for OpenTelemetry integration with LangWatch.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langwatch.mdx#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @vercel/otel langwatch @opentelemetry/api-logs @opentelemetry/instrumentation @opentelemetry/sdk-logs\n```\n\n----------------------------------------\n\nTITLE: Setting FriendliAI Token Environment Variable\nDESCRIPTION: Sets the required `FRIENDLI_TOKEN` environment variable in a bash shell. This token is necessary for authenticating requests to the FriendliAI API.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport FRIENDLI_TOKEN=\"YOUR_FRIENDLI_TOKEN\"\n```\n\n----------------------------------------\n\nTITLE: Building AI SDK Project\nDESCRIPTION: Commands to install dependencies and build the AI SDK project. These steps are necessary to set up the development environment and compile the project.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npnpm install\npnpm build\n```\n\n----------------------------------------\n\nTITLE: Setting Langfuse Environment Variables (Bash)\nDESCRIPTION: This code snippet shows how to set the necessary environment variables for Langfuse integration. It includes the secret key, public key, and base URL for the Langfuse service.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langfuse.mdx#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nLANGFUSE_SECRET_KEY=\"sk-lf-...\"\nLANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\nLANGFUSE_BASEURL=\"https://cloud.langfuse.com\" # ðªðº EU region, use \"https://us.cloud.langfuse.com\" for US region\n```\n\n----------------------------------------\n\nTITLE: Importing AWSBedrockAnthropicStream in React\nDESCRIPTION: Code snippet showing how to import the AWSBedrockAnthropicStream utility from the AI package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/10-aws-bedrock-anthropic-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { AWSBedrockAnthropicStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Importing the Tool Helper Function\nDESCRIPTION: This snippet shows how to import the 'tool' helper function from the 'ai' package.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/20-tool.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { tool } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Replacing RSC render Function with streamUI in TypeScript\nDESCRIPTION: Shows the replacement of the deprecated AI SDK RSC 3.0 `render` function with the `streamUI` function in AI SDK 4.0 for rendering UI components from server components.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_32\n\nLANGUAGE: typescript\nCODE:\n```\nimport { render } from 'ai/rsc';\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamUI } from 'ai/rsc';\n```\n\n----------------------------------------\n\nTITLE: Configuring Kasada API URL\nDESCRIPTION: Example of the Kasada API URL format to be updated in kasada-server.ts and kasada-client.ts files based on the Kasada dashboard.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-kasada-bot-protection/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nhttps://${kasadaAPIHostname}/149e9513-01fa-4fb0-aad4-566afd725d1b/2d206a39-8ed7-437e-a3be-862e0f06eea3/api/${kasadaAPIVersion}/classification\n```\n\n----------------------------------------\n\nTITLE: Installing Next.js OpenAI App with pnpm\nDESCRIPTION: Command to create a new Next.js application using the AI SDK and OpenAI example template with pnpm.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-pages/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-pages next-openai-app\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Creating Environment File\nDESCRIPTION: Commands to install project dependencies using pnpm and create a local environment file from a template.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-kasada-bot-protection/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npnpm i\ncp .env.local.example .env.local # and fill in the required values\n```\n\n----------------------------------------\n\nTITLE: Importing useAssistant in Svelte\nDESCRIPTION: Import statement for using the useAssistant hook in a Svelte application\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/20-use-assistant.mdx#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { useAssistant } from '@ai-sdk/svelte'\n```\n\n----------------------------------------\n\nTITLE: Importing the generateImage Function in TypeScript\nDESCRIPTION: This code snippet shows how to import the experimental generateImage function from the 'ai' library. The function is renamed to 'generateImage' for ease of use.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/10-generate-image.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { experimental_generateImage as generateImage } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Importing Experimental_StdioMCPTransport in JavaScript\nDESCRIPTION: Import the Experimental_StdioMCPTransport function from the ai/mcp-stdio module. This function is used to create a transport for MCP clients to communicate with MCP servers using standard input and output streams.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/22-mcp-stdio-transport.mdx#2025-04-23_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { Experimental_StdioMCPTransport } from \"ai/mcp-stdio\"\n```\n\n----------------------------------------\n\nTITLE: Importing Hume Provider Instance in TypeScript\nDESCRIPTION: This TypeScript snippet shows how to import the default Hume provider instance, named `hume`, from the installed `@ai-sdk/hume` package. This instance is necessary to interact with the Hume API via the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/hume/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { hume } from '@ai-sdk/hume';\n```\n\n----------------------------------------\n\nTITLE: Running a Single End-to-end Test File\nDESCRIPTION: This command demonstrates how to run a specific end-to-end test file. In this example, it runs the Google provider integration tests.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/ai-core/README.md#2025-04-23_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npnpm run test:file src/e2e/google.test.ts\n```\n\n----------------------------------------\n\nTITLE: Renaming experimental_continuationSteps Option in TypeScript\nDESCRIPTION: Shows the renaming of the `experimental_continuationSteps` option to `experimental_continueSteps` within the `generateText` function for AI SDK 4.0.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/29-migration-guide-4-0.mdx#2025-04-23_snippet_27\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  experimental_continuationSteps: true,\n  // ...\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  experimental_continueSteps: true,\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Installing the OpenAI Compatible Provider Package using Bash\nDESCRIPTION: Shows commands to install the '@ai-sdk/openai-compatible' module using pnpm, npm, and yarn package managers. This package is necessary for integrating OpenAI-compatible LLM providers with the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/index.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/openai-compatible\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/openai-compatible\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/openai-compatible\n```\n\n----------------------------------------\n\nTITLE: Installing the Anthropic Provider Package via npm\nDESCRIPTION: This command uses npm (Node Package Manager) to install the `@ai-sdk/anthropic` library. This package is required to utilize Anthropic language models within applications built with the Vercel AI SDK. Execute this command in your project's terminal.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/anthropic/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/anthropic\n```\n\n----------------------------------------\n\nTITLE: Installing LMNT Provider with Package Managers\nDESCRIPTION: Command line instructions for installing the LMNT provider module using different package managers (pnpm, npm, and yarn).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/140-lmnt.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/lmnt\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/lmnt\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/lmnt\n```\n\n----------------------------------------\n\nTITLE: Importing MistralStream in React\nDESCRIPTION: Example of importing the MistralStream helper function from the AI package in a React application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/17-mistral-stream.mdx#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { MistralStream } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Installing Fal Provider Package\nDESCRIPTION: Command to install the fal provider package for the AI SDK via npm\nSOURCE: https://github.com/vercel/ai/blob/main/packages/fal/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/fal\n```\n\n----------------------------------------\n\nTITLE: Importing StreamData in React\nDESCRIPTION: Shows how to import the StreamData class from the ai package in a React application.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/45-stream-data.mdx#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { StreamData } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Usage Information Settings\nDESCRIPTION: Demonstrates how to disable usage information in the stream response.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#2025-04-23_snippet_13\n\nLANGUAGE: ts\nCODE:\n```\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse({\n    sendUsage: false,\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Installing DeepInfra SDK Module (npm) - Shell\nDESCRIPTION: Installs the @ai-sdk/deepinfra package using npm, the standard Node.js package manager. This step is necessary to add DeepInfra model support to your project before importing or utilizing provider features.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/11-deepinfra.mdx#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnpm install @ai-sdk/deepinfra\n```\n\n----------------------------------------\n\nTITLE: Installing SambaNova Provider with Package Managers\nDESCRIPTION: Commands to install the sambanova-ai-provider package using different package managers (pnpm, npm, yarn).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/96-sambanova.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add sambanova-ai-provider\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install sambanova-ai-provider\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add sambanova-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Installing Gladia Provider for AI SDK\nDESCRIPTION: Command to install the Gladia provider package for the Vercel AI SDK using npm.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/gladia/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @ai-sdk/gladia\n```\n\n----------------------------------------\n\nTITLE: Installing AI SDK with npm\nDESCRIPTION: Basic installation command for the AI SDK using npm package manager. Requires Node.js 18+ and pnpm installed on the local development machine.\nSOURCE: https://github.com/vercel/ai/blob/main/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install ai\n```\n\n----------------------------------------\n\nTITLE: Installing FriendliAI Provider with npm\nDESCRIPTION: Installs the necessary FriendliAI provider package using the npm package manager. This package enables using FriendliAI models within the Vercel AI SDK.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @friendliai/ai-provider\n```\n\n----------------------------------------\n\nTITLE: Installing voyage-ai-provider using pnpm\nDESCRIPTION: Installs the `voyage-ai-provider` package using the pnpm package manager. This command adds the necessary dependency to your project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/61-voyage-ai.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add voyage-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Running the Express.js Development Server (Shell)\nDESCRIPTION: This command uses `pnpm` to start the development server (`pnpm dev`). This command typically runs the Express.js application defined in the project, making it accessible for testing.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/express/README.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npnpm dev\n```\n\n----------------------------------------\n\nTITLE: Rendering Example Links using a JSX Component\nDESCRIPTION: This JSX snippet uses an `ExampleLinks` component to display a list of links to usage examples. Each example includes a title and a corresponding URL, likely pointing to different documentation pages or code examples within the project (e.g., generating text, using tools in Next.js and Node.js).\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#2025-04-23_snippet_34\n\nLANGUAGE: jsx\nCODE:\n```\n<ExampleLinks\n  examples={[\n    {\n      title: 'Learn to generate text using a language model in Next.js',\n      link: '/examples/next-app/basics/generating-text',\n    },\n    {\n      title:\n        'Learn to generate a chat completion using a language model in Next.js',\n      link: '/examples/next-app/basics/generating-text',\n    },\n    {\n      title: 'Learn to call tools using a language model in Next.js',\n      link: '/examples/next-app/tools/call-tool',\n    },\n    {\n      title:\n        'Learn to render a React component as a tool call using a language model in Next.js',\n      link: '/examples/next-app/tools/render-interface-during-tool-call',\n    },\n    {\n      title: 'Learn to generate text using a language model in Node.js',\n      link: '/examples/node/generating-text/generate-text',\n    },\n    {\n      title:\n        'Learn to generate chat completions using a language model in Node.js',\n      link: '/examples/node/generating-text/generate-text-with-chat-prompt',\n    },\n  ]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Installing Fireworks Provider with Package Managers\nDESCRIPTION: Commands to install the Fireworks provider module using different package managers (pnpm, npm, or yarn).\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/26-fireworks.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @ai-sdk/fireworks\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @ai-sdk/fireworks\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @ai-sdk/fireworks\n```\n\n----------------------------------------\n\nTITLE: Installing Portkey Provider Package - Shell\nDESCRIPTION: These shell command snippets demonstrate how to install the @portkey-ai/vercel-provider module using three common Node.js package managers: pnpm, npm, and yarn. This step is required before importing and utilizing Portkey as a provider in the Vercel AI SDK. Users must choose the package manager that matches their project setup.\nSOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/10-portkey.mdx#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm add @portkey-ai/vercel-provider\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @portkey-ai/vercel-provider\n```\n\nLANGUAGE: shell\nCODE:\n```\nyarn add @portkey-ai/vercel-provider\n```\n\n----------------------------------------\n\nTITLE: Testing the Hono + AI SDK Endpoint with Curl\nDESCRIPTION: This curl command demonstrates how to test the endpoint of the Hono server integrated with AI SDK. It sends a POST request to the local server running on port 8080.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/hono/README.md#2025-04-23_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncurl -i -X POST http://localhost:8080\n```\n\n----------------------------------------\n\nTITLE: Importing valibotSchema Function in TypeScript\nDESCRIPTION: This snippet shows how to import the valibotSchema function from the AI SDK. It's a simple import statement that brings in the function for use in your project.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/27-valibot-schema.mdx#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { valibotSchema } from \"ai\"\n```\n\n----------------------------------------\n\nTITLE: Documenting Version Changes in @ai-sdk/elevenlabs Package\nDESCRIPTION: Documents patch version updates including dependency changes and feature additions for the @ai-sdk/elevenlabs package. Shows changes across versions 0.0.2 and 0.0.1.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/elevenlabs/CHANGELOG.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# @ai-sdk/elevenlabs\n\n## 0.0.2\n\n### Patch Changes\n\n- Updated dependencies [beef951]\n  - @ai-sdk/provider@1.1.3\n  - @ai-sdk/provider-utils@2.2.7\n\n## 0.0.1\n\n### Patch Changes\n\n- 01888d9: feat (provider/elevenlabs): add transcription provider\n```\n\n----------------------------------------\n\nTITLE: Declaring Page Title in Markdown Frontmatter\nDESCRIPTION: Markdown frontmatter block that defines the page title for a Next.js documentation page.\nSOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/index.mdx#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Next.js\n---\n```\n\n----------------------------------------\n\nTITLE: Filtering End-to-end Tests\nDESCRIPTION: This command shows how to filter and run a subset of end-to-end tests. It runs the Google provider integration tests and filters for test cases containing the word 'stream'.\nSOURCE: https://github.com/vercel/ai/blob/main/examples/ai-core/README.md#2025-04-23_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npnpm run test:file src/e2e/google.test.ts -t stream\n```\n\n----------------------------------------\n\nTITLE: Version 0.0.5 - Dependency Updates\nDESCRIPTION: Update of provider and provider-utils dependencies.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/amazon-bedrock/CHANGELOG.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## 0.0.5\n\n### Patch Changes\n\n- Updated dependencies [b7290943]\n  - @ai-sdk/provider@0.0.12\n  - @ai-sdk/provider-utils@1.0.2\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure\nDESCRIPTION: The structure and metadata for the AI SDK RSC documentation page, including frontmatter configuration and component layout\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/index.mdx#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: AI SDK RSC\ndescription: Reference documentation for the AI SDK UI\ncollapsed: true\n---\n```\n\n----------------------------------------\n\nTITLE: Rendering Return Properties Documentation with JSX\nDESCRIPTION: This JSX snippet utilizes a `PropertiesTable` component to display detailed documentation for the return properties of a Vercel AI SDK function or method. The `content` prop is an array of objects, where each object defines a return property, including its name, type, description, and potentially nested properties or parameters. This allows for a structured presentation of complex return types like Promises, AsyncIterableStreams, and metadata objects.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/04-stream-object.mdx#2025-04-23_snippet_6\n\nLANGUAGE: jsx\nCODE:\n```\n<PropertiesTable\n  content={[\n    {\n      name: 'usage',\n      type: 'Promise<CompletionTokenUsage>',\n      description:\n        'The token usage of the generated text. Resolved when the response is finished.',\n      properties: [\n        {\n          type: 'CompletionTokenUsage',\n          parameters: [\n            {\n              name: 'promptTokens',\n              type: 'number',\n              description: 'The total number of tokens in the prompt.',\n            },\n            {\n              name: 'completionTokens',\n              type: 'number',\n              description: 'The total number of tokens in the completion.',\n            },\n            {\n              name: 'totalTokens',\n              type: 'number',\n              description: 'The total number of tokens generated.',\n            },\n          ],\n        },\n      ],\n    },\n    {\n      name: 'providerMetadata',\n      type: 'Promise<Record<string,Record<string,JSONValue>> | undefined>',\n      description:\n        'Optional metadata from the provider. Resolved whe the response is finished. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.',\n    },\n    {\n      name: 'object',\n      type: 'Promise<T>',\n      description:\n        'The generated object (typed according to the schema). Resolved when the response is finished.',\n    },\n    {\n      name: 'partialObjectStream',\n      type: 'AsyncIterableStream<DeepPartial<T>>',\n      description:\n        'Stream of partial objects. It gets more complete as the stream progresses. Note that the partial object is not validated. If you want to be certain that the actual content matches your schema, you need to implement your own validation for partial results.',\n    },\n    {\n      name: 'elementStream',\n      type: 'AsyncIterableStream<ELEMENT>',\n      description: 'Stream of array elements. Only available in \\\"array\\\" mode.',\n    },\n    {\n      name: 'textStream',\n      type: 'AsyncIterableStream<string>',\n      description:\n        'Text stream of the JSON representation of the generated object. It contains text chunks. When the stream is finished, the object is valid JSON that can be parsed.',\n    },\n    {\n      name: 'fullStream',\n      type: 'AsyncIterableStream<ObjectStreamPart<T>>',\n      description:\n        'Stream of different types of events, including partial objects, errors, and finish events. Only errors that stop the stream, such as network errors, are thrown.',\n      properties: [\n        {\n          type: 'ObjectPart',\n          parameters: [\n            {\n              name: 'type',\n              type: \"'object'\",\n            },\n            {\n              name: 'object',\n              type: 'DeepPartial<T>',\n              description: 'The partial object that was generated.',\n            },\n          ],\n        },\n        {\n          type: 'TextDeltaPart',\n          parameters: [\n            {\n              name: 'type',\n              type: \"'text-delta'\",\n            },\n            {\n              name: 'textDelta',\n              type: 'string',\n              description: 'The text delta for the underlying raw JSON text.',\n            },\n          ],\n        },\n        {\n          type: 'ErrorPart',\n          parameters: [\n            {\n              name: 'type',\n              type: \"'error'\",\n            },\n            {\n              name: 'error',\n              type: 'unknown',\n              description: 'The error that occurred.',\n            },\n          ],\n        },\n        {\n          type: 'FinishPart',\n          parameters: [\n            {\n              name: 'type',\n              type: \"'finish'\",\n            },\n            {\n              name: 'finishReason',\n              type: 'FinishReason',\n            },\n            {\n              name: 'logprobs',\n              type: 'Logprobs',\n              isOptional: true,\n            },\n            {\n              name: 'usage',\n              type: 'Usage',\n              description: 'Token usage.',\n            },\n            {\n              name: 'response',\n              type: 'Response',\n              isOptional: true,\n              description: 'Response metadata.',\n              properties: [\n                {\n                  type: 'Response',\n                  parameters: [\n                    {\n                      name: 'id',\n                      type: 'string',\n                      description:\n                        'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.',\n                    },\n                    {\n                      name: 'model',\n                      type: 'string',\n                      description:\n                        'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.',\n                    },\n                    {\n                      name: 'timestamp',\n                      type: 'Date',\n                      description:\n                        'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.',\n                    },\n                  ],\n                },\n              ],\n            },\n          ],\n        },\n      ],\n    },\n    {\n      name: 'request',\n      type: 'Promise<RequestMetadata>',\n      isOptional: true,\n      description: 'Request metadata.',\n      properties: [\n        {\n          type: 'RequestMetadata',\n          parameters: [\n            {\n              name: 'body',\n              type: 'string',\n              description:\n                'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).',\n            },\n          ],\n        },\n      ],\n    },\n    {\n      name: 'response',\n      type: 'Promise<ResponseMetadata>',\n      isOptional: true,\n      description: 'Response metadata. Resolved when the response is finished.',\n      properties: [\n        {\n          type: 'ResponseMetadata',\n          parameters: [\n            {\n              name: 'id',\n              type: 'string',\n              description:\n                'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.',\n            },\n            {\n              name: 'model',\n              type: 'string',\n              description:\n                'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.',\n            },\n            {\n              name: 'timestamp',\n              type: 'Date',\n              description:\n                'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.',\n            },\n            {\n              name: 'headers',\n              isOptional: true,\n              type: 'Record<string, string>',\n              description: 'Optional response headers.',\n            },\n          ],\n        },\n      ],\n    },\n    {\n      name: 'warnings',\n      type: 'Warning[] | undefined',\n      description:\n        'Warnings from the model provider (e.g. unsupported settings).',\n    },\n    {\n      name: 'pipeTextStreamToResponse',\n      type: '(response: ServerResponse, init?: ResponseInit => void',\n      description:\n        'Writes text delta output to a Node.js response-like object. It sets a `Content-Type` header to `text/plain; charset=utf-8` and writes each text delta as a separate chunk.',\n      properties: [\n        {\n          type: 'ResponseInit',\n          parameters: [\n            {\n              name: 'status',\n              type: 'number',\n              isOptional: true,\n              description: 'The response status code.',\n            },\n            {\n              name: 'statusText',\n              type: 'string',\n              isOptional: true,\n              description: 'The response status text.',\n            },\n            {\n              name: 'headers',\n              type: 'Record<string, string>',\n              isOptional: true,\n              description: 'The response headers.',\n            },\n          ],\n        },\n      ],\n    },\n    {\n      name: 'toTextStreamResponse',\n      type: '(init?: ResponseInit) => Response',\n      description:\n        'Creates a simple text stream response. Each text delta is encoded as UTF-8 and sent as a separate chunk. Non-text-delta events are ignored.',\n      properties: [\n        {\n          type: 'ResponseInit',\n          parameters: [\n            {\n              name: 'status',\n              type: 'number',\n              isOptional: true,\n              description: 'The response status code.',\n            },\n            {\n              name: 'statusText',\n              type: 'string',\n              isOptional: true,\n              description: 'The response status text.',\n            },\n            {\n              name: 'headers',\n              type: 'Record<string, string>',\n              isOptional: true,\n              description: 'The response headers.',\n            },\n          ],\n        },\n      ],\n    },\n  ]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Displaying Changelog Entry for Version 4.3.5\nDESCRIPTION: Shows the patch change for version 4.3.5, which improves support for zero-argument MCP tools.\nSOURCE: https://github.com/vercel/ai/blob/main/packages/ai/CHANGELOG.md#2025-04-23_snippet_4\n\nLANGUAGE: Markdown\nCODE:\n```\n## 4.3.5\n\n### Patch Changes\n\n- 452bf12: fix (ai/mcp): better support for zero-argument MCP tools\n```\n\n----------------------------------------\n\nTITLE: Rendering Index Cards for AI SDK Core Documentation in Markdown\nDESCRIPTION: This code snippet uses a custom IndexCards component to display a set of cards, each representing a different section of the AI SDK Core documentation. Each card includes a title, description, and link to the corresponding documentation page.\nSOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/index.mdx#2025-04-23_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n<IndexCards\n  cards={[\n    {\n      title: 'Overview',\n      description:\n        'Learn about AI SDK Core and how to work with Large Language Models (LLMs).',\n      href: '/docs/ai-sdk-core/overview',\n    },\n    {\n      title: 'Generating Text',\n      description: 'Learn how to generate text.',\n      href: '/docs/ai-sdk-core/generating-text',\n    },\n    {\n      title: 'Generating Structured Data',\n      description: 'Learn how to generate structured data.',\n      href: '/docs/ai-sdk-core/generating-structured-data',\n    },\n    {\n      title: 'Tool Calling',\n      description: 'Learn how to do tool calling with AI SDK Core.',\n      href: '/docs/ai-sdk-core/tools-and-tool-calling',\n    },\n    {\n      title: 'Prompt Engineering',\n      description: 'Learn how to write prompts with AI SDK Core.',\n      href: '/docs/ai-sdk-core/prompt-engineering',\n    },\n    {\n      title: 'Settings',\n      description:\n        'Learn how to set up settings for language models generations.',\n      href: '/docs/ai-sdk-core/settings',\n    },\n    {\n      title: 'Embeddings',\n      description: 'Learn how to use embeddings with AI SDK Core.',\n      href: '/docs/ai-sdk-core/embeddings',\n    },\n    {\n      title: 'Image Generation',\n      description: 'Learn how to generate images with AI SDK Core.',\n      href: '/docs/ai-sdk-core/image-generation',\n    },\n    {\n      title: 'Transcription',\n      description: 'Learn how to transcribe audio with AI SDK Core.',\n      href: '/docs/ai-sdk-core/transcription',\n    },\n    {\n      title: 'Speech',\n      description: 'Learn how to generate speech with AI SDK Core.',\n      href: '/docs/ai-sdk-core/speech',\n    },\n    {\n      title: 'Provider Management',\n      description: 'Learn how to work with multiple providers.',\n      href: '/docs/ai-sdk-core/provider-management',\n    },\n    {\n      title: 'Middleware',\n      description: 'Learn how to use middleware with AI SDK Core.',\n      href: '/docs/ai-sdk-core/middleware',\n    },\n    {\n      title: 'Error Handling',\n      description: 'Learn how to handle errors with AI SDK Core.',\n      href: '/docs/ai-sdk-core/error-handling',\n    },\n    {\n      title: 'Testing',\n      description: 'Learn how to test with AI SDK Core.',\n      href: '/docs/ai-sdk-core/testing',\n    },\n    {\n      title: 'Telemetry',\n      description: 'Learn how to use telemetry with AI SDK Core.',\n      href: '/docs/ai-sdk-core/telemetry',\n    },\n  ]}\n/>\n```"
  }
]
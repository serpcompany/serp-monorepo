[
  {
    "owner": "nvidia",
    "repo": "dali",
    "content": "TITLE: Creating a DALI Dataset Input Function for Estimator Training\nDESCRIPTION: This code defines a function `train_data_fn` that returns a DALI dataset for use with the TensorFlow Estimator API. The dataset is created within a `tf.device('/gpu:0')` context to ensure it resides on the GPU. The `mnist_set.map` function transforms the dataset to match the expected input format for the Estimator model (a dictionary of features and labels).\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef train_data_fn():\n    with tf.device(\"/gpu:0\"):\n        mnist_set = dali_tf.DALIDataset(\n            fail_on_device_mismatch=False,\n            pipeline=mnist_pipeline(device=\"gpu\"),\n            batch_size=BATCH_SIZE,\n            output_shapes=shapes,\n            output_dtypes=dtypes,\n            device_id=0,\n        )\n        mnist_set = mnist_set.map(\n            lambda features, labels: ({\"images\": features}, labels)\n        )\n\n    return mnist_set\n```\n\n----------------------------------------\n\nTITLE: Define sharded DALI iterator for MNIST dataset on multiple GPUs\nDESCRIPTION: This code defines a sharded DALI iterator for the MNIST dataset, specifically designed for training on multiple GPUs. It uses the `data_iterator` decorator with the `devices` argument set to `jax.devices()` to distribute the data across all available GPUs. The iterator utilizes `fn.readers.caffe2` with `num_shards` and `shard_id` to shard the dataset across the GPUs. It performs image decoding, normalization, and reshaping, and also applies one-hot encoding to the labels during training. This ensures efficient data loading and preprocessing for distributed training with JAX.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/flax-basic_example.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nbatch_size = 200\nimage_size = 28\nnum_classes = 10\n\n\n@data_iterator(\n    output_map=[\"images\", \"labels\"],\n    reader_name=\"mnist_caffe2_reader\",\n    devices=jax.devices(),\n)\ndef mnist_sharded_iterator(data_path, is_training, num_shards, shard_id):\n    jpegs, labels = fn.readers.caffe2(\n        path=data_path,\n        random_shuffle=is_training,\n        name=\"mnist_caffe2_reader\",\n        num_shards=num_shards,\n        shard_id=shard_id,\n    )\n    images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.GRAY)\n    images = fn.crop_mirror_normalize(\n        images, dtype=types.FLOAT, std=[255.0], output_layout=\"CHW\"\n    )\n    images = fn.reshape(images, shape=[-1])  # Flatten the output image\n\n    labels = labels.gpu()\n\n    if is_training:\n        labels = fn.one_hot(labels, num_classes=num_classes)\n\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Running SSD Training with DALI (PyTorch)\nDESCRIPTION: This command runs the SSD training script on 8 GPUs using half-precision. It specifies the number of processes per node, enables warmup iterations, sets the batch size, and points to the COCO 2017 dataset directory. This is the primary command used to initiate the training process.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/single_stage_detector/pytorch_ssd.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nproc_per_node=8 ./main.py --warmup 300 --bs 64 --fp16 --data /coco/\n```\n\n----------------------------------------\n\nTITLE: GPU Operator Implementation CUDA\nDESCRIPTION: This CUDA file provides the GPU implementation of the `CustomDummy` operator. It overrides the `RunImpl` method, retrieves the batch data, and uses `cudaMemcpyAsync` to copy data from input to output on the CUDA stream provided by the workspace. The GPU version of the operator is registered using the `DALI_REGISTER_OPERATOR` macro.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/create_a_custom_operator.ipynb#_snippet_2\n\nLANGUAGE: CUDA\nCODE:\n```\n#include \"customdummy/dummy.h\"\n\n#include <vector>\n#include <numeric>\n\n#include \"dali/core/kernel_params.h\"\n#include \"dali/core/tensor_view.h\"\n#include \"dali/pipeline/workspace/workspace.h\"\n\nnamespace dali {\n\ntemplate <typename Backend>\nclass CustomDummyGpu : public CustomDummy {\n public:\n  explicit CustomDummyGpu(const OpSpec& spec) : CustomDummy(spec) {}\n\n  void RunImpl(Workspace& ws) override {\n    const auto& input = ws.Input<Backend>(0);\n    auto& output = ws.Output<Backend>(0);\n    output.ResizeLike(input);\n\n    int n = input.num_samples();\n\n    cudaStream_t stream = ws.stream();\n\n    for (int i = 0; i < n; ++i) {\n        auto in_tensor = input.tensor_view(i);\n        auto out_tensor = output.mutable_tensor_view(i);\n\n        cudaMemcpyAsync(out_tensor.data(), in_tensor.data(), in_tensor.shape().num_elements() * in_tensor.type().size(), cudaMemcpyDeviceToDevice, stream);\n    }\n  }\n};\n\nDALI_REGISTER_OPERATOR(CustomDummy, CustomDummyGpu<GPU>, GPU);\n\n}\n\n```\n\n----------------------------------------\n\nTITLE: Train DALILitMNIST Model using PyTorch Lightning\nDESCRIPTION: This code snippet instantiates the `DALILitMNIST` model and trains it using PyTorch Lightning Trainer. It configures the trainer to use a GPU and sets the number of epochs. It showcases the integration of DALI with PyTorch Lightning for accelerated training.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-lightning.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Even if previous Trainer finished his work it still keeps the GPU booked,\n# force it to release the device.\nif \"PL_TRAINER_GPUS\" in os.environ:\n    os.environ.pop(\"PL_TRAINER_GPUS\")\nmodel = DALILitMNIST()\ntrainer = Trainer(\n    max_epochs=5, devices=1, accelerator=\"gpu\", num_sanity_val_steps=0\n)\n# ddp work only in no-interactive mode, to test it unncoment and run as a script\n# trainer = Trainer(devices=8, accelerator=\"gpu\", strategy=\"ddp\", max_epochs=5)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Training the Flax model on multiple GPUs\nDESCRIPTION: This code trains the Flax model on multiple GPUs using `jax.pmap` for parallel execution. It iterates over a specified number of epochs and batches from the training iterator. For each batch, it calls the `parallel_train_step` function to update the model state on each GPU. After each epoch, it calculates the accuracy on the validation set using the `accuracy` function, but only on one replica of the model. The model is unreplicated to run validation on a single GPU. Finally, the accuracy result is printed.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/flax-basic_example.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport flax\n\nparallel_train_step = jax.pmap(train_step)\n\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch}\")\n    for batch in training_iterator:\n        model_state = parallel_train_step(model_state, batch)\n\n    acc = accuracy(flax.jax_utils.unreplicate(model_state), validation_iterator)\n    print(f\"Accuracy = {acc}\")\n```\n\n----------------------------------------\n\nTITLE: TFRecord Reader Pipeline Definition\nDESCRIPTION: This snippet defines a DALI pipeline for reading data from TFRecord files. It uses `fn.readers.tfrecord` to load images and labels, specifying the data types and shapes of the features. It supports shuffling and sharding.  The `common_pipeline` function is then applied to the decoded images and labels.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-various-readers.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali.tfrecord as tfrec\n\n\n@pipeline_def\ndef tfrecord_reader_pipeline(num_gpus):\n    inputs = fn.readers.tfrecord(\n        path=tfrecord,\n        index_path=tfrecord_idx,\n        features={\n            \"image/encoded\": tfrec.FixedLenFeature((), tfrec.string, \"\"),\n            \"image/class/label\": tfrec.FixedLenFeature([1], tfrec.int64, -1),\n        },\n        random_shuffle=True,\n        shard_id=Pipeline.current().device_id,\n        num_shards=num_gpus,\n        name=\"Reader\",\n    )\n\n    return common_pipeline(inputs[\"image/encoded\"], inputs[\"image/class/label\"])\n```\n\n----------------------------------------\n\nTITLE: Creating Training and Validation Iterators (Python)\nDESCRIPTION: This snippet defines the paths to the training and validation data, sets the batch size and number of epochs, and instantiates the DALI iterators for both training and validation. It also prints the number of batches in each iterator.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-multigpu_example.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\ntraining_data_path = os.path.join(\n    os.environ[\"DALI_EXTRA_PATH\"], \"db/MNIST/training/\"\n)\nvalidation_data_path = os.path.join(\n    os.environ[\"DALI_EXTRA_PATH\"], \"db/MNIST/testing/\"\n)\n\nbatch_size = 200\nnum_epochs = 5\n\n\ntraining_iterator = mnist_training_iterator(\n    batch_size=batch_size, data_path=training_data_path\n)\nprint(f\"Number of batches in training iterator = {len(training_iterator)}\")\n\nvalidation_iterator = mnist_validation_iterator(\n    batch_size=batch_size, data_path=validation_data_path\n)\nprint(f\"Number of batches in validation iterator = {len(validation_iterator)}\")\n```\n\n----------------------------------------\n\nTITLE: Using the DALI Pipeline with PyTorch Iterator\nDESCRIPTION: This snippet demonstrates how to use the defined DALI pipeline with the `PyTorchIterator`. It creates instances of the `ExternalInputIterator` and `ExternalSourcePipeline`, then creates a `PyTorchIterator` with `last_batch_padded=True` and `last_batch_policy=LastBatchPolicy.PARTIAL`. Finally, it iterates through the data using the PyTorch iterator.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-external_input.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali.plugin.pytorch import (\n    DALIClassificationIterator as PyTorchIterator,\n)\nfrom nvidia.dali.plugin.pytorch import LastBatchPolicy\n\neii = ExternalInputIterator(batch_size, 0, 1)\npipe = ExternalSourcePipeline(\n    batch_size=batch_size, num_threads=2, device_id=0, external_data=eii\n)\npii = PyTorchIterator(\n    pipe, last_batch_padded=True, last_batch_policy=LastBatchPolicy.PARTIAL\n)\n\nfor e in range(epochs):\n    for i, data in enumerate(pii):\n        real_batch_size = len(data[0][\"data\"])\n        print(f\"epoch: {e}, iter {i}, real batch size: {real_batch_size}\")\n    pii.reset()\n```\n\n----------------------------------------\n\nTITLE: Defining External Input Iterator\nDESCRIPTION: This snippet defines a custom iterator class `ExternalInputIterator` that yields batches of images and labels from an external data source. The iterator reads image file names and labels from a file, shuffles them, and returns batches of numpy arrays (images) and PyTorch tensors (labels).\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-external_input.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ExternalInputIterator(object):\n    def __init__(self, batch_size, device_id, num_gpus):\n        self.images_dir = \"../../data/images/\"\n        self.batch_size = batch_size\n        with open(self.images_dir + \"file_list.txt\", \"r\") as f:\n            self.files = [line.rstrip() for line in f if line is not \"\"]\n        # whole data set size\n        self.data_set_len = len(self.files)\n        # based on the device_id and total number of GPUs - world size\n        # get proper shard\n        self.files = self.files[\n            self.data_set_len\n            * device_id\n            // num_gpus : self.data_set_len\n            * (device_id + 1)\n            // num_gpus\n        ]\n        self.n = len(self.files)\n\n    def __iter__(self):\n        self.i = 0\n        shuffle(self.files)\n        return self\n\n    def __next__(self):\n        batch = []\n        labels = []\n\n        if self.i >= self.n:\n            self.__iter__()\n            raise StopIteration\n\n        for _ in range(self.batch_size):\n            jpeg_filename, label = self.files[self.i % self.n].split(\" \")\n            batch.append(\n                np.fromfile(self.images_dir + jpeg_filename, dtype=np.uint8)\n            )  # we can use numpy\n            labels.append(\n                torch.tensor([int(label)], dtype=torch.uint8)\n            )  # or PyTorch's native tensors\n            self.i += 1\n        return (batch, labels)\n\n    def __len__(self):\n        return self.data_set_len\n\n    next = __next__\n```\n\n----------------------------------------\n\nTITLE: Define DALI Pipeline for MNIST Data Loading\nDESCRIPTION: This code defines a DALI pipeline for loading and preprocessing the MNIST dataset. It utilizes DALI operators for reading data from a Caffe2 format, decoding images, normalizing pixel values, and casting labels to the correct data type for PyTorch. The pipeline is defined using the `@pipeline_def` decorator.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-lightning.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali as dali\nfrom nvidia.dali import pipeline_def\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\nfrom nvidia.dali.plugin.pytorch import (\n    DALIClassificationIterator,\n    LastBatchPolicy,\n)\n\n# Path to MNIST dataset\ndata_path = os.path.join(os.environ[\"DALI_EXTRA_PATH\"], \"db/MNIST/training/\")\n\n\n@pipeline_def\ndef GetMnistPipeline(device, shard_id=0, num_shards=1):\n    jpegs, labels = fn.readers.caffe2(\n        path=data_path,\n        shard_id=shard_id,\n        num_shards=num_shards,\n        random_shuffle=True,\n        name=\"Reader\",\n    )\n    images = fn.decoders.image(\n        jpegs,\n        device=\"mixed\" if device == \"gpu\" else \"cpu\",\n        output_type=types.GRAY,\n    )\n    images = fn.crop_mirror_normalize(\n        images,\n        dtype=types.FLOAT,\n        std=[0.3081 * 255],\n        mean=[0.1307 * 255],\n        output_layout=\"CHW\",\n    )\n    if device == \"gpu\":\n        labels = labels.gpu()\n    # PyTorch expects labels as INT64\n    labels = fn.cast(labels, dtype=types.INT64)\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Define common pipeline (Python)\nDESCRIPTION: This function defines the common part of the image processing pipeline used by all readers. It decodes the JPEG images, resizes them, and performs crop, mirror, and normalization operations. It uses the `nvidia.dali.fn` module for DALI operations.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-various-readers.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, Pipeline\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\n\n\ndef common_pipeline(jpegs, labels):\n    images = fn.decoders.image(jpegs, device=\"mixed\")\n    images = fn.resize(\n        images,\n        resize_shorter=fn.random.uniform(range=(256, 480)),\n        interp_type=types.INTERP_LINEAR,\n    )\n    images = fn.crop_mirror_normalize(\n        images,\n        crop_pos_x=fn.random.uniform(range=(0.0, 1.0)),\n        crop_pos_y=fn.random.uniform(range=(0.0, 1.0)),\n        dtype=types.FLOAT,\n        crop=(227, 227),\n        mean=[128.0, 128.0, 128.0],\n        std=[1.0, 1.0, 1.0],\n    )\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Run EfficientNet Training with Standard Configuration (DGX-A100)\nDESCRIPTION: This command runs EfficientNet training with a standard configuration for DGX-A100, using AMP and DALI with AutoAugment.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython multiproc.py --nproc_per_node 8 ./main.py --amp --static-loss-scale 128 --batch-size 256 $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Defining a DALI pipeline with ExternalSource (GPU)\nDESCRIPTION: This snippet defines a DALI pipeline that uses the `ExternalSource` operator with the `device` argument set to \"gpu\" to read data directly from the GPU. It enhances the contrast of the images and sets the outputs of the pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/external_input.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipe_gpu = Pipeline(batch_size=batch_size, num_threads=2, device_id=0)\nwith pipe_gpu:\n    images, labels = fn.external_source(\n        source=eii_gpu, num_outputs=2, device=\"gpu\", dtype=types.UINT8\n    )\n    enhance = fn.brightness_contrast(images, contrast=2)\n    pipe_gpu.set_outputs(enhance, labels)\n\npipe_gpu.build()\n```\n\n----------------------------------------\n\nTITLE: Wrapping DALI Pipeline with DALIDataset for TensorFlow\nDESCRIPTION: This code wraps a DALI pipeline instance with a `DALIDataset` object from the DALI TensorFlow plugin, creating a `tf.data.Dataset` compatible object.  It defines the output shapes and data types of the DALI pipeline. The `tf.device` context ensures that the dataset is created on the specified device.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali.plugin.tf as dali_tf\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tf_v1\nimport logging\n\ntf.get_logger().setLevel(logging.ERROR)\n\n# Create pipeline\npipeline = mnist_pipeline(device=\"cpu\")\n\n# Define shapes and types of the outputs\nshapes = ((BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE), (BATCH_SIZE))\ndtypes = (tf.float32, tf.int32)\n\n# Create dataset\nwith tf.device(\"/cpu:0\"):\n    mnist_set = dali_tf.DALIDataset(\n        pipeline=pipeline,\n        batch_size=BATCH_SIZE,\n        output_shapes=shapes,\n        output_dtypes=dtypes,\n        device_id=0,\n    )\n```\n\n----------------------------------------\n\nTITLE: Define Input Data for DALI Pipeline\nDESCRIPTION: Defines input data using NumPy arrays and helper functions to prepare the data for use in the DALI pipeline.  The function `get_data()` converts the numpy arrays into a format suitable for use with the `ExternalSource` operator.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nleft_magic_values = [[[42, 7, 0], [0, 0, 0]], [[5, 10, 15], [10, 100, 1000]]]\n\nright_magic_values = [[[3, 3, 3], [1, 3, 5]], [[1, 5, 5], [1, 1, 1]]]\n\nbatch_size = len(left_magic_values)\n\n\ndef convert_batch(batch):\n    return [np.int32(tensor) for tensor in batch]\n\n\ndef get_data():\n    return (convert_batch(left_magic_values), convert_batch(right_magic_values))\n```\n\n----------------------------------------\n\nTITLE: Creating DALI Pipeline and Integrating with PyTorch Iterator\nDESCRIPTION: This snippet creates multiple instances of the `caffe_pipeline`, one for each GPU. It then wraps these pipelines with `DALIGenericIterator` to be used in a PyTorch training loop. It also checks the correctness of labels by ensuring they are integers within a predefined range.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-basic_example.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom nvidia.dali.plugin.pytorch import DALIGenericIterator\n\n\nlabel_range = (0, 999)\npipes = [\n    caffe_pipeline(\n        batch_size=BATCH_SIZE, num_threads=2, device_id=device_id, num_gpus=N\n    )\n    for device_id in range(N)\n]\n\nfor pipe in pipes:\n    pipe.build()\n\ndali_iter = DALIGenericIterator(pipes, [\"data\", \"label\"], reader_name=\"Reader\")\n\nfor i, data in enumerate(dali_iter):\n    # Testing correctness of labels\n    for d in data:\n        label = d[\"label\"]\n        image = d[\"data\"]\n        ## labels need to be integers\n        assert np.equal(np.mod(label, 1), 0).all()\n        ## labels need to be in range pipe_name[2]\n        assert (label >= label_range[0]).all()\n        assert (label <= label_range[1]).all()\n\nprint(\"OK\")\n```\n\n----------------------------------------\n\nTITLE: DALI Server and Proxy Usage\nDESCRIPTION: This snippet demonstrates how to initialize the DALI server with a pipeline and use the proxy to get processed samples. It shows how to create future samples by passing data to the proxy callable, within a `with` statement to manage server lifecycle.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/plugins/pytorch_dali_proxy.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali.plugin.pytorch.experimental import proxy as dali_proxy\nwith dali_proxy.DALIServer(pipeline) as dali_server:\n   future_samples = [dali_server.proxy(image) for image in images]\n```\n\n----------------------------------------\n\nTITLE: Defining DALI Pipeline with Caffe Reader\nDESCRIPTION: This code defines a DALI pipeline named `caffe_pipeline` using the `@pipeline_def` decorator. It reads data from a Caffe LMDB database using `fn.readers.caffe`, decodes images, resizes them with random shorter side length, and applies crop, mirror, and normalization. The pipeline outputs the processed images and labels.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-basic_example.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, Pipeline\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\n\n\n@pipeline_def\ndef caffe_pipeline(num_gpus):\n    device_id = Pipeline.current().device_id\n    jpegs, labels = fn.readers.caffe(\n        name=\"Reader\",\n        path=lmdb_folder,\n        random_shuffle=True,\n        shard_id=device_id,\n        num_shards=num_gpus,\n    )\n    images = fn.decoders.image(jpegs, device=\"mixed\")\n    images = fn.resize(\n        images,\n        resize_shorter=fn.random.uniform(range=(256, 480)),\n        interp_type=types.INTERP_LINEAR,\n    )\n    images = fn.crop_mirror_normalize(\n        images,\n        crop_pos_x=fn.random.uniform(range=(0.0, 1.0)),\n        crop_pos_y=fn.random.uniform(range=(0.0, 1.0)),\n        dtype=types.FLOAT,\n        crop=(227, 227),\n        mean=[128.0, 128.0, 128.0],\n        std=[1.0, 1.0, 1.0],\n    )\n\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Defining a CNN model using Flax\nDESCRIPTION: This code defines a simple Convolutional Neural Network (CNN) using the Flax library. It includes the model definition, a function to create the model state, and functions for training and evaluation. The model consists of three dense layers with ReLU activations, followed by a final dense layer with 10 output features.  The training process involves computing the loss, calculating gradients, and updating the model parameters.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/flax-basic_example.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\nfrom flax import linen as nn\nfrom flax.training import train_state\n\nimport optax\n\n\nclass CNN(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(features=784)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=1024)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=1024)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=10)(x)\n        return x\n\n\ndef create_model_state(rng, learning_rate, momentum):\n    cnn = CNN()\n    params = cnn.init(rng, jnp.ones([784]))[\"params\"]\n    tx = optax.sgd(learning_rate, momentum)\n    return train_state.TrainState.create(\n        apply_fn=cnn.apply, params=params, tx=tx\n    )\n\n\n@jax.jit\ndef train_step(model_state, batch):\n    def loss_fn(params):\n        logits = model_state.apply_fn({\"params\": params}, batch[\"images\"])\n        loss = optax.softmax_cross_entropy(\n            logits=logits, labels=batch[\"labels\"]\n        ).mean()\n        return loss\n\n    grad_fn = jax.grad(loss_fn)\n    grads = grad_fn(model_state.params)\n    model_state = model_state.apply_gradients(grads=grads)\n    return model_state\n\n\ndef accuracy(model_state, iterator):\n    correct_predictions = 0\n    for batch in iterator:\n        logits = model_state.apply_fn(\n            {\"params\": model_state.params}, batch[\"images\"]\n        )\n        correct_predictions = correct_predictions + jnp.sum(\n            batch[\"labels\"].ravel() == jnp.argmax(logits, axis=-1)\n        )\n\n    return correct_predictions / iterator.size\n```\n\n----------------------------------------\n\nTITLE: Training a Keras Model with DALI Dataset on CPU\nDESCRIPTION: This code demonstrates how to train a Keras model using the DALI dataset as input. The model is a simple sequential model with a flatten layer, a dense layer with ReLU activation, a dropout layer, and a dense layer with softmax activation. The model is compiled with the Adam optimizer and sparse categorical crossentropy loss. `model.fit` is used for training.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create the model\nmodel = tf.keras.models.Sequential(\n    [\n        tf.keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE), name=\"images\"),\n        tf.keras.layers.Flatten(input_shape=(IMAGE_SIZE, IMAGE_SIZE)),\n        tf.keras.layers.Dense(HIDDEN_SIZE, activation=\"relu\"),\n        tf.keras.layers.Dropout(DROPOUT),\n        tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n    ]\n)\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\n# Train using DALI dataset\nmodel.fit(mnist_set, epochs=EPOCHS, steps_per_epoch=ITERATIONS_PER_EPOCH)\n```\n\n----------------------------------------\n\nTITLE: Creating Pipelines and Running with Paddle Iterator\nDESCRIPTION: This snippet creates instances of different DALI pipelines (MXNet, Caffe, File, and TFRecord) and uses them with the `DALIGenericIterator` to feed data into a Paddle training loop.  It iterates through the data, performs basic label validation, and prints the pipeline name to track progress.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-various-readers.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom nvidia.dali.plugin.paddle import DALIGenericIterator\n\n\npipe_types = [\n    [mxnet_reader_pipeline, (0, 999)],\n    [caffe_reader_pipeline, (0, 999)],\n    [file_reader_pipeline, (0, 1)],\n    [tfrecord_reader_pipeline, (1, 1000)],\n]\n\nfor pipe_t in pipe_types:\n    pipe_name, label_range = pipe_t\n    print(\"RUN: \" + pipe_name.__name__)\n    pipes = [\n        pipe_name(\n            batch_size=BATCH_SIZE,\n            num_threads=2,\n            device_id=device_id,\n            num_gpus=N,\n        )\n        for device_id in range(N)\n    ]\n    dali_iter = DALIGenericIterator(\n        pipes, [\"data\", \"label\"], reader_name=\"Reader\"\n    )\n\n    for i, data in enumerate(dali_iter):\n        # Testing correctness of labels\n        for d in data:\n            label = d[\"label\"]\n            image = d[\"data\"]\n            ## labels need to be integers\n            assert np.equal(np.mod(label, 1), 0).all()\n            ## labels need to be in range pipe_name[2]\n            assert (np.array(label) >= label_range[0]).all()\n            assert (np.array(label) <= label_range[1]).all()\n    print(\"OK : \" + pipe_name.__name__)\n```\n\n----------------------------------------\n\nTITLE: Evaluating an Estimator Model with DALI Dataset\nDESCRIPTION: This code shows how to evaluate a TensorFlow Estimator model using the DALI dataset through the `test_data_fn` input function. The `model.evaluate` function is called with the input function and the number of steps for evaluation.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel.evaluate(input_fn=test_data_fn, steps=ITERATIONS_PER_EPOCH)\n```\n\n----------------------------------------\n\nTITLE: Initializing Global Constants for DALI PyTorch Integration\nDESCRIPTION: This snippet defines global constants used in the DALI and PyTorch integration. It retrieves the `DALI_EXTRA_PATH` environment variable, calculates the number of GPUs using `nvidia-smi`, and defines batch size, number of iterations, and image size.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-basic_example.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os.path\nimport subprocess\n\ntest_data_root = os.environ[\"DALI_EXTRA_PATH\"]\n\n# Caffe LMDB\nlmdb_folder = os.path.join(test_data_root, \"db\", \"lmdb\")\n\nres = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE, text=True)\nN = res.stdout.count(\"\\n\")  # number of GPUs\nBATCH_SIZE = 128  # batch size per GPU\nITERATIONS = 32\nIMAGE_SIZE = 3\n```\n\n----------------------------------------\n\nTITLE: Visualize DALI Color Space Conversion Results (Python)\nDESCRIPTION: This snippet visualizes the original and converted images using matplotlib. It extracts the converted images from the pipeline output and displays them in a grid.  It handles grayscale images by reshaping them before displaying. Requires matplotlib and synsets.py.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/image_processing/color_space_conversion.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport math\nfrom synsets import imagenet_synsets\nimport matplotlib.gridspec as gridspec\n\nn = 1\nlen_outputs = len(pipe_out)\noriginal_images = pipe_out[0]\nn_conversions = len(conversions)\n\nconversions_cpu = [elem for elem in pipe_out[1 : n_conversions + 1]]\nconversions_gpu = [elem for elem in pipe_out[n_conversions + 1 :]]\n\ntitles = [title for title, _, _ in conversions]\n\n\ndef show_images(original, conversions, titles_conversions, device):\n    outputs = conversions\n    titles = [elem + \" \" + device for elem in titles_conversions]\n    fig = plt.figure(figsize=(16, 16))\n    columns = 4\n    rows = 1 + int(math.ceil(1.0 * len_outputs / columns))\n    gs = gridspec.GridSpec(rows, columns)\n\n    plt.subplot(gs[0])\n    plt.axis(\"off\")\n    plt.title(\"original\")\n    plt.imshow(original.at(n))\n\n    for i in range(len(conversions)):\n        plt.subplot(gs[i + 1])\n        plt.axis(\"off\")\n        plt.title(titles[i])\n        img_chw = (\n            outputs[i].as_cpu().at(n) if device == \"gpu\" else outputs[i].at(n)\n        )\n        shape = img_chw.shape\n        if shape[2] == 1:\n            plt.imshow(img_chw.reshape(shape[0], shape[1]), cmap=\"gray\")\n        else:\n            plt.imshow(img_chw)\n\n\nshow_images(\n    original=original_images,\n    conversions=conversions_cpu,\n    titles_conversions=titles,\n    device=\"cpu\",\n)\nshow_images(\n    original=original_images,\n    conversions=conversions_gpu,\n    titles_conversions=titles,\n    device=\"gpu\",\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Max Reduction in DALI (Python)\nDESCRIPTION: This code snippet demonstrates how to use the `fn.reductions.max` operator in DALI. It initializes a DALI pipeline, feeds the input data using `fn.external_source`, and applies the `max` reduction operator to find the maximum value within each sample.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reductions.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\nwith pipe:\n    input = fn.external_source(source=get_batch, dtype=types.INT64)\n    max = fn.reductions.max(input)\n\n    pipe.set_outputs(max)\n\nrun_and_print(pipe)\n```\n\n----------------------------------------\n\nTITLE: Training loop with DALI iterators and JAX\nDESCRIPTION: This code snippet implements the training loop. It initializes the model, iterates through the training data using the DALI iterator, performs a training step using the `update` function, and calculates the accuracy on the validation set after each epoch.  The training progress is printed to the console.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-basic_example.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Starting training\")\n\nmodel = init_model()\nnum_epochs = 5\n\nfor epoch in range(num_epochs):\n    for batch in training_iterator:\n        model = update(model, batch)\n\n    test_acc = accuracy(model, validation_iterator)\n    print(f\"Epoch {epoch} sec\")\n    print(f\"Test set accuracy {test_acc}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Common Pipeline Function\nDESCRIPTION: This snippet defines a common pipeline function `common_pipeline` that performs image decoding, resizing, and normalization. It uses DALI operators `fn.decoders.image`, `fn.resize`, and `fn.crop_mirror_normalize`.  The resizing uses a random uniform distribution for the shorter edge, and the cropping position is also randomized. The output consists of the processed images and labels.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-various-readers.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, Pipeline\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\n\n\ndef common_pipeline(jpegs, labels):\n    images = fn.decoders.image(jpegs, device=\"mixed\")\n    images = fn.resize(\n        images,\n        resize_shorter=fn.random.uniform(range=(256, 480)),\n        interp_type=types.INTERP_LINEAR,\n    )\n    images = fn.crop_mirror_normalize(\n        images,\n        crop_pos_x=fn.random.uniform(range=(0.0, 1.0)),\n        crop_pos_y=fn.random.uniform(range=(0.0, 1.0)),\n        dtype=types.FLOAT,\n        crop=(227, 227),\n        mean=[128.0, 128.0, 128.0],\n        std=[1.0, 1.0, 1.0],\n    )\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline Definition in Python\nDESCRIPTION: This Python code defines a DALI pipeline for image loading, decoding, resizing, and normalization. It uses DALI's functional API and integrates with PyTorch via DALIGenericIterator for training a deep learning model. The pipeline reads images from a directory, performs random cropping, resizes the images to 256x256, and applies crop, mirror, and normalization.\nSOURCE: https://github.com/nvidia/dali/blob/main/README.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali.pipeline import pipeline_def\nimport nvidia.dali.types as types\nimport nvidia.dali.fn as fn\nfrom nvidia.dali.plugin.pytorch import DALIGenericIterator\nimport os\n\n# To run with different data, see documentation of nvidia.dali.fn.readers.file\n# points to https://github.com/NVIDIA/DALI_extra\ndata_root_dir = os.environ['DALI_EXTRA_PATH']\nimages_dir = os.path.join(data_root_dir, 'db', 'single', 'jpeg')\n\n\ndef loss_func(pred, y):\n    pass\n\n\ndef model(x):\n    pass\n\n\ndef backward(loss, model):\n    pass\n\n\n@pipeline_def(num_threads=4, device_id=0)\ndef get_dali_pipeline():\n    images, labels = fn.readers.file(\n        file_root=images_dir, random_shuffle=True, name=\"Reader\")\n    # decode data on the GPU\n    images = fn.decoders.image_random_crop(\n        images, device=\"mixed\", output_type=types.RGB)\n    # the rest of processing happens on the GPU as well\n    images = fn.resize(images, resize_x=256, resize_y=256)\n    images = fn.crop_mirror_normalize(\n        images,\n        crop_h=224,\n        crop_w=224,\n        mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n        std=[0.229 * 255, 0.224 * 255, 0.225 * 255],\n        mirror=fn.random.coin_flip())\n    return images, labels\n\n\ntrain_data = DALIGenericIterator(\n    [get_dali_pipeline(batch_size=16)],\n    ['data', 'label'],\n    reader_name='Reader'\n)\n\n\nfor i, data in enumerate(train_data):\n    x, y = data[0]['data'], data[0]['label']\n    pred = model(x)\n    loss = loss_func(pred, y)\n    backward(loss, model)\n```\n\n----------------------------------------\n\nTITLE: Creating DALI iterator for MNIST data with JAX plugin\nDESCRIPTION: This code defines a DALI iterator using the `nvidia.dali.plugin.jax.data_iterator` decorator. The iterator loads data from Caffe2 format, decodes images, normalizes them, reshapes them, and converts labels to one-hot encoding.  The output map specifies the names of the tensors to be returned by the iterator.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-basic_example.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali.plugin.jax import data_iterator\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\n\n\nbatch_size = 200\nimage_size = 28\nnum_classes = 10\n\n\n@data_iterator(output_map=[\"images\", \"labels\"], reader_name=\"caffe2_reader\")\ndef mnist_iterator(data_path, random_shuffle):\n    jpegs, labels = fn.readers.caffe2(\n        path=data_path, random_shuffle=random_shuffle, name=\"caffe2_reader\"\n    )\n    images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.GRAY)\n    images = fn.crop_mirror_normalize(\n        images, dtype=types.FLOAT, std=[255.0], output_layout=\"CHW\"\n    )\n    images = fn.reshape(images, shape=[image_size * image_size])\n\n    labels = labels.gpu()\n\n    if random_shuffle:\n        labels = fn.one_hot(labels, num_classes=num_classes)\n\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Downloading and Preparing COCO Dataset\nDESCRIPTION: This bash script automates the process of downloading and extracting the COCO 2017 dataset. It creates a directory, downloads the necessary zip files (training images, validation images, and annotations), extracts them, and then returns to the original directory. This ensures that the dataset is correctly set up for training.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/single_stage_detector/pytorch_ssd.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndir=$(pwd)\nmkdir /coco; cd /coco\ncurl -O http://images.cocodataset.org/zips/train2017.zip; unzip train2017.zip\ncurl -O http://images.cocodataset.org/zips/val2017.zip; unzip val2017.zip\ncurl -O http://images.cocodataset.org/annotations/annotations_trainval2017.zip\nunzip annotations_trainval2017.zip\ncd $dir\n```\n\n----------------------------------------\n\nTITLE: DALI Iterator for Training with JAX pmap (Python)\nDESCRIPTION: This snippet defines a DALI iterator for training data, configured for use with JAX's `pmap` function.  It reads data using `fn.readers.caffe2`, decodes images, applies normalization, and converts labels to one-hot encoding. The `devices` argument specifies the GPUs to use.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-multigpu_example.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@data_iterator(\n    output_map=[\"images\", \"labels\"],\n    reader_name=\"mnist_caffe2_reader\",\n    devices=jax.devices(),\n)\ndef mnist_training_iterator(data_path, num_shards, shard_id):\n    jpegs, labels = fn.readers.caffe2(\n        path=data_path,\n        random_shuffle=True,\n        name=\"mnist_caffe2_reader\",\n        num_shards=num_shards,\n        shard_id=shard_id,\n    )\n    images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.GRAY)\n    images = fn.crop_mirror_normalize(\n        images, dtype=types.FLOAT, std=[255.0], output_layout=\"CHW\"\n    )\n    images = fn.reshape(images, shape=[image_size * image_size])\n\n    labels = labels.gpu()\n    labels = fn.one_hot(labels, num_classes=num_classes)\n\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Integration with DALI Proxy DataLoader\nDESCRIPTION: Shows how to integrate DALI with a PyTorch DataLoader using the `dali_proxy.DataLoader` wrapper. It initializes the DALI server and creates a `CustomDataset` that uses the DALI proxy as a transform function.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/plugins/pytorch_dali_proxy.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali.plugin.pytorch.experimental import proxy as dali_proxy\n\nwith dali_proxy.DALIServer(pipeline) as dali_server:\n   dataset = CustomDataset(dali_server.proxy, data=images)\n   loader = dali_proxy.DataLoader(dali_server, dataset, batch_size=32, num_workers=4)\n   for data, _ in loader:\n      print(data.shape)  # Ready-to-use processed batch\n```\n\n----------------------------------------\n\nTITLE: Defining a DALI Pipeline\nDESCRIPTION: This code defines a DALI pipeline using the @pipeline_def decorator. It uses the fn.readers.video operator to read video data from a file list, configure parameters such as sequence length, shuffling, image type, and enabling frame number/timestamp extraction. The file_list_frame_num parameter dictates whether labels in the file list are to be interpreted as frame numbers or timestamps.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/video/video_file_list_outputs.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef video_pipe(file_list):\n    video, label, start_frame_num, timestamps = fn.readers.video(\n        device=\"gpu\",\n        file_list=file_list,\n        sequence_length=COUNT,\n        shard_id=0,\n        num_shards=1,\n        random_shuffle=True,\n        initial_fill=10,\n        image_type=types.RGB,\n        dtype=types.FLOAT,\n        file_list_frame_num=frame_num_based_labels,\n        enable_frame_num=True,\n        enable_timestamps=True,\n        file_list_include_preceding_frame=True,\n    )\n    return video, label, start_frame_num, timestamps\n```\n\n----------------------------------------\n\nTITLE: Define global constants for DALI readers (Python)\nDESCRIPTION: This snippet sets global constants used for configuring the different DALI readers. It defines paths to MXNet RecordIO, Caffe LMDB, image directories, and TFRecord datasets. It also determines the number of GPUs available and sets the batch size and image size.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-various-readers.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os.path\nimport subprocess\n\ntest_data_root = os.environ[\"DALI_EXTRA_PATH\"]\n\n# MXNet RecordIO\ndb_folder = os.path.join(test_data_root, \"db\", \"recordio/\")\n\n# Caffe LMDB\nlmdb_folder = os.path.join(test_data_root, \"db\", \"lmdb\")\n\n# image dir with plain jpeg files\nimage_dir = \"../../data/images\"\n\n# TFRecord\ntfrecord = os.path.join(test_data_root, \"db\", \"tfrecord\", \"train\")\ntfrecord_idx = \"idx_files/train.idx\"\ntfrecord2idx_script = \"tfrecord2idx\"\n\nres = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE, text=True)\nN = res.stdout.count(\"\\n\")  # number of GPUs\nBATCH_SIZE = 128  # batch size per GPU\nIMAGE_SIZE = 3\n```\n\n----------------------------------------\n\nTITLE: Invoking AutoAugment with predefined ImageNet policy in DALI\nDESCRIPTION: This code snippet demonstrates how to use the predefined ImageNet AutoAugment policy within a DALI pipeline. It showcases the necessary imports, pipeline definition with the `@pipeline_def` decorator and `enable_conditionals=True`, and the use of `auto_augment.auto_augment` function. This function takes image data and its shape as input and returns the augmented images.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/auto_augment.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, fn, types\nfrom nvidia.dali.auto_aug import auto_augment\n\n@pipeline_def(enable_conditionals=True)\ndef training_pipe(data_dir, image_size):\n\n    jpegs, labels = fn.readers.file(file_root=data_dir, ...)\n    shapes = fn.peek_image_shape(jpegs)\n    images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.RGB)\n\n    augmented_images = auto_augment.auto_augment(images, shape=shapes)\n\n    resized_images = fn.resize(augmented_images, size=[image_size, image_size])\n\n    return resized_images, labels\n```\n\n----------------------------------------\n\nTITLE: Define DALI Color Space Conversion Pipeline (Python)\nDESCRIPTION: This snippet defines a DALI pipeline that reads images from a directory, decodes them, and converts them to different color spaces. It uses `nvidia.dali.fn.color_space_conversion` to perform the color space transformations. The conversions are performed on both CPU and GPU.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/image_processing/color_space_conversion.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\n\n\nimage_dir = \"../data/images\"\n\nconversions = [\n    (\"RGB to BGR\", types.RGB, types.BGR),\n    (\"RGB to YCbCR\", types.RGB, types.YCbCr),\n    (\"RGB to Gray\", types.RGB, types.GRAY),\n]\n\n\n@pipeline_def()\ndef conversion_pipeline():\n    files, labels = fn.readers.file(file_root=image_dir)\n    images_cpu = fn.decoders.image(files)\n    images_gpu = images_cpu.gpu()\n    converted_cpu = [\n        fn.color_space_conversion(\n            images_cpu, image_type=inp_t, output_type=out_t\n        )\n        for _, inp_t, out_t in conversions\n    ]\n    converted_gpu = [\n        fn.color_space_conversion(\n            images_gpu, image_type=inp_t, output_type=out_t\n        )\n        for _, inp_t, out_t in conversions\n    ]\n    return tuple([images_cpu] + converted_cpu + converted_gpu)\n```\n\n----------------------------------------\n\nTITLE: Building and running the DALI pipeline (CPU)\nDESCRIPTION: This snippet builds and runs the DALI pipeline to process the data from the external source. It retrieves the output tensors from the pipeline run.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/external_input.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe.build()\npipe_out = pipe.run()\n```\n\n----------------------------------------\n\nTITLE: Executing Keras Model Training on GPU\nDESCRIPTION: This code demonstrates how to execute the `model.fit` training loop on the GPU using a `tf.device` context. This ensures that the training process utilizes the GPU and avoids intermediate CPU buffers.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Train on the GPU\nwith tf.device(\"/gpu:0\"):\n    model.fit(mnist_set, epochs=EPOCHS, steps_per_epoch=ITERATIONS_PER_EPOCH)\n```\n\n----------------------------------------\n\nTITLE: File Reader Pipeline Definition\nDESCRIPTION: This snippet defines a DALI pipeline that reads image files directly from a directory. It utilizes `fn.readers.file` to load images and their corresponding labels (based on directory structure or a separate label file).  Shuffling and sharding are enabled.  The images are then preprocessed using the `common_pipeline` function.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-various-readers.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef file_reader_pipeline(num_gpus):\n    jpegs, labels = fn.readers.file(\n        file_root=image_dir,\n        random_shuffle=True,\n        shard_id=Pipeline.current().device_id,\n        num_shards=num_gpus,\n        name=\"Reader\",\n    )\n\n    return common_pipeline(jpegs, labels)\n```\n\n----------------------------------------\n\nTITLE: Creating DALI iterators for training and validation\nDESCRIPTION: This code creates instances of the DALI iterators for both training and validation datasets.  It calls the `mnist_iterator` function with the appropriate data paths and `is_training` flags. The `batch_size` is passed as well. Finally, the code prints the iterator objects and the number of batches in each iterator.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/flax-basic_example.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Creating iterators\")\ntraining_iterator = mnist_iterator(\n    data_path=training_data_path, is_training=True, batch_size=batch_size\n)\nvalidation_iterator = mnist_iterator(\n    data_path=validation_data_path, is_training=False, batch_size=batch_size\n)\n\nprint(training_iterator)\nprint(validation_iterator)\n\nprint(f\"Number of batches in training iterator = {len(training_iterator)}\")\nprint(f\"Number of batches in validation iterator = {len(validation_iterator)}\")\n```\n\n----------------------------------------\n\nTITLE: Define LitMNIST LightningModule for MNIST Classification\nDESCRIPTION: This code defines a PyTorch Lightning Module for MNIST classification. It includes the neural network architecture (three linear layers), the forward pass, training step definition, optimizer configuration, and data preparation and setup using the native PyTorch MNIST dataset.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-lightning.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass LitMNIST(LightningModule):\n    def __init__(self):\n        super().__init__()\n\n        # mnist images are (1, 28, 28) (channels, width, height)\n        self.layer_1 = torch.nn.Linear(28 * 28, 128)\n        self.layer_2 = torch.nn.Linear(128, 256)\n        self.layer_3 = torch.nn.Linear(256, 10)\n\n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n\n        # (b, 1, 28, 28) -> (b, 1*28*28)\n        x = x.view(batch_size, -1)\n        x = self.layer_1(x)\n        x = F.relu(x)\n        x = self.layer_2(x)\n        x = F.relu(x)\n        x = self.layer_3(x)\n\n        x = F.log_softmax(x, dim=1)\n        return x\n\n    def process_batch(self, batch):\n        return batch\n\n    def training_step(self, batch, batch_idx):\n        x, y = self.process_batch(batch)\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        return loss\n\n    def cross_entropy_loss(self, logits, labels):\n        return F.nll_loss(logits, labels)\n\n    def configure_optimizers(self):\n        return Adam(self.parameters(), lr=1e-3)\n\n    def prepare_data(self):\n        # download data only\n        self.mnist_train = MNIST(\n            os.getcwd(),\n            train=True,\n            download=True,\n            transform=transforms.ToTensor(),\n        )\n\n    def setup(self, stage=None):\n        # transforms for images\n        transform = transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))] \n        )\n        self.mnist_train = MNIST(\n            os.getcwd(), train=True, download=False, transform=transform\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.mnist_train, batch_size=64, num_workers=8, pin_memory=True\n        )\n```\n\n----------------------------------------\n\nTITLE: Using Mean, StdDev, and Variance Reductions in DALI (Python)\nDESCRIPTION: This code snippet demonstrates the use of `mean`, `std_dev`, and `variance` reduction operators. `StdDev` and `Variance` reductions require an externally provided mean, which is calculated using the `Mean` reduction operator.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reductions.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\nwith pipe:\n    input = fn.external_source(source=get_batch, dtype=types.INT32)\n    mean = fn.reductions.mean(input)\n    std_dev = fn.reductions.std_dev(input, mean)\n    variance = fn.reductions.variance(input, mean)\n\n    pipe.set_outputs(mean, std_dev, variance)\n\nrun_and_print(pipe)\n```\n\n----------------------------------------\n\nTITLE: Train BetterDALILitMNIST Model with Custom Iterator\nDESCRIPTION: This code snippet instantiates the `BetterDALILitMNIST` model and trains it using the PyTorch Lightning Trainer. This uses the custom DALI iterator defined earlier, resulting in cleaner code since no custom batch processing is required in the LightningModule.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-lightning.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Even if previous Trainer finished his work it still keeps the GPU booked,\n# force it to release the device.\nif \"PL_TRAINER_GPUS\" in os.environ:\n    os.environ.pop(\"PL_TRAINER_GPUS\")\nmodel = BetterDALILitMNIST()\ntrainer = Trainer(\n    max_epochs=5, devices=1, accelerator=\"gpu\", num_sanity_val_steps=0\n)\n# ddp work only in no-interactive mode, to test it uncomment and run as a script\n# trainer = Trainer(devices=8, accelerator=\"gpu\", strategy=\"ddp\", max_epochs=5)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Create DALI TensorFlow Dataset\nDESCRIPTION: Creates a DALI TensorFlow dataset using `dali_tf.DALIDataset`. The `dataset_fn` function creates a dataset for each device (GPU), taking into account the `input_pipeline_id` to correctly shard the data. The `strategy.distribute_datasets_from_function` distributes the datasets to each replica.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset-multigpu.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef dataset_fn(input_context):\n    with tf.device(\"/gpu:{}\".format(input_context.input_pipeline_id)):\n        device_id = input_context.input_pipeline_id\n        return dali_tf.DALIDataset(\n            pipeline=mnist_pipeline(device_id=device_id, shard_id=device_id),\n            batch_size=BATCH_SIZE,\n            output_shapes=shapes,\n            output_dtypes=dtypes,\n            device_id=device_id,\n        )\n\n\ninput_options = tf.distribute.InputOptions(\n    experimental_place_dataset_on_device=True,\n    experimental_fetch_to_device=False,\n    experimental_replication_mode=tf.distribute.InputReplicationMode.PER_REPLICA,\n)\n\ntrain_dataset = strategy.distribute_datasets_from_function(\n    dataset_fn, input_options\n)\n```\n\n----------------------------------------\n\nTITLE: Concatenate Tensors with DALI\nDESCRIPTION: This snippet demonstrates how to concatenate tensors along different axes (0, 1, and 2) using the `fn.cat` operator in NVIDIA DALI. It initializes three constant tensors (`src1`, `src2`, `src3`) with NumPy arrays and then uses `fn.cat` to concatenate them along different axes within a DALI pipeline. The output tensors `cat_outer`, `cat_middle`, and `cat_inner` represent the concatenated tensors along axes 0, 1, and 2, respectively. This requires the `nvidia.dali` and `numpy` libraries.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/tensor_join.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali as dali\nimport nvidia.dali.fn as fn\nimport numpy as np\n\nnp.random.seed(1234)\n\narr = np.array(\n    [\n        [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n        [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]],\n    ]\n)\n\nsrc1 = dali.types.Constant(arr)\nsrc2 = dali.types.Constant(arr + 100)\nsrc3 = dali.types.Constant(arr + 200)\n\npipe_cat = dali.pipeline.Pipeline(batch_size=1, num_threads=3, device_id=0)\nwith pipe_cat:\n    cat_outer = fn.cat(src1, src2, src3, axis=0)\n    cat_middle = fn.cat(src1, src2, src3, axis=1)\n    cat_inner = fn.cat(src1, src2, src3, axis=2)\n    pipe_cat.set_outputs(cat_outer, cat_middle, cat_inner)\n\npipe_cat.build()\no = pipe_cat.run()\n```\n\n----------------------------------------\n\nTITLE: Defining an Estimator Model\nDESCRIPTION: This snippet shows how to define a TensorFlow Estimator model using `tf.estimator.DNNClassifier`. It defines feature columns, a run configuration (specifying GPU usage), and the model itself with hidden units, number of classes, dropout, and the Adam optimizer.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Define the feature columns for Estimator\nfeature_columns = [\n    tf.feature_column.numeric_column(\"images\", shape=[IMAGE_SIZE, IMAGE_SIZE])\n]\n\n# And the run config\nrun_config = tf.estimator.RunConfig(\n    model_dir=\"/tmp/tensorflow-checkpoints\", device_fn=lambda op: \"/gpu:0\"\n)\n\n# Finally create the model based on `DNNClassifier`\nmodel = tf.estimator.DNNClassifier(\n    feature_columns=feature_columns,\n    hidden_units=[HIDDEN_SIZE],\n    n_classes=NUM_CLASSES,\n    dropout=DROPOUT,\n    config=run_config,\n    optimizer=\"Adam\",\n)\n```\n\n----------------------------------------\n\nTITLE: Build and Run DALI Pipeline Python\nDESCRIPTION: Builds and runs the defined DALI pipeline. Retrieves the output, which includes the decoded audio data and the sampling rate.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/audio_processing/audio_decoder.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipe = audio_decoder_pipe(batch_size=batch_size, num_threads=1, device_id=0)\npipe.build()\ncpu_output = pipe.run()\n```\n\n----------------------------------------\n\nTITLE: Defining DALI Pipeline with ExternalSource\nDESCRIPTION: This snippet defines a DALI pipeline `ExternalSourcePipeline` that uses the `fn.external_source` operator to fetch data from the external iterator. The pipeline decodes images, resizes them, casts them to `UINT8`, and sets the outputs to the processed images and labels.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-external_input.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef ExternalSourcePipeline(batch_size, num_threads, device_id, external_data):\n    pipe = Pipeline(batch_size, num_threads, device_id)\n    with pipe:\n        jpegs, labels = fn.external_source(\n            source=external_data, num_outputs=2, dtype=types.UINT8\n        )\n        images = fn.decoders.image(jpegs, device=\"mixed\")\n        images = fn.resize(images, resize_x=240, resize_y=240)\n        output = fn.cast(images, dtype=types.UINT8)\n        pipe.set_outputs(output, labels)\n    return pipe\n```\n\n----------------------------------------\n\nTITLE: Using RandAugment with NVIDIA DALI pipeline in Python\nDESCRIPTION: This code snippet demonstrates how to use the `rand_augment` function within a NVIDIA DALI pipeline to apply random augmentations to images. It defines a pipeline that reads images, applies RandAugment with n=3 and m=17, resizes the images, and returns the resized images and labels. The pipeline definition requires the `enable_conditionals` parameter to be set to `True`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/rand_augment.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, fn, types\nfrom nvidia.dali.auto_aug import rand_augment\n\n@pipeline_def(enable_conditionals=True)\ndef training_pipe(data_dir, image_size):\n\n    jpegs, labels = fn.readers.file(file_root=data_dir, ...)\n    shapes = fn.peek_image_shape(jpegs)\n    images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.RGB)\n\n    augmented_images = rand_augment.rand_augment(images, shape=shapes, n=3, m=17)\n\n    resized_images = fn.resize(augmented_images, size=[image_size, image_size])\n\n    return resized_images, labels\n```\n\n----------------------------------------\n\nTITLE: Using DALI Pipeline with Paddle Iterator (Python)\nDESCRIPTION: This code demonstrates how to use the defined DALI pipeline with the Paddle DALI iterator (`DALIClassificationIterator`).  It creates an instance of the custom iterator and the DALI pipeline, then wraps the pipeline with the Paddle iterator.  The example iterates through the data for a specified number of epochs, printing the real batch size for each iteration. `last_batch_padded` and `last_batch_policy` are used to handle the last batch in each epoch.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-external_input.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali.plugin.paddle import (\n    DALIClassificationIterator as PaddleIterator,\n)\nfrom nvidia.dali.plugin.paddle import LastBatchPolicy\n\neii = ExternalInputIterator(batch_size, 0, 1)\npipe = ExternalSourcePipeline(\n    batch_size=batch_size, num_threads=2, device_id=0, external_data=eii\n)\npii = PaddleIterator(\n    pipe, last_batch_padded=True, last_batch_policy=LastBatchPolicy.PARTIAL\n)\n\nfor e in range(epochs):\n    for i, data in enumerate(pii):\n        real_batch_size = len(np.array(data[0][\"data\"]))\n        print(f\"epoch: {e}, iter {i}, real batch size: {real_batch_size}\")\n    pii.reset()\n```\n\n----------------------------------------\n\nTITLE: Define Output Shapes and Types\nDESCRIPTION: Defines the output shapes and data types for the DALI pipeline, which are required for creating the TensorFlow dataset.  Specifies the shape for the image and labels tensors.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset-multigpu.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nshapes = ((BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE), (BATCH_SIZE))\ndtypes = (tf.float32, tf.int32)\n```\n\n----------------------------------------\n\nTITLE: Define DALI Pipeline with Broadcasting\nDESCRIPTION: Defines a DALI pipeline that demonstrates broadcasting by adding a tensor of shape (1, 3) to a tensor of shape (2, 3) using constants.  The `Constant` operator is used to define the input tensors.  This highlights DALI's ability to handle operations between tensors with different shapes.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(batch_size=1, num_threads=4, device_id=0)\ndef pipeline_3():\n    left = Constant(np.float32([[1.0, 2.0, 3.0]]))\n    right = Constant(np.float32([[-5.0, -6.0, -7.0], [10.0, 20.0, 30.0]]))\n    return left + right\n```\n\n----------------------------------------\n\nTITLE: Define DALI pipeline for optical flow\nDESCRIPTION: This snippet defines a DALI pipeline using `@pipeline_def` decorator that loads a video using `fn.readers.video` and calculates the optical flow using `fn.optical_flow`. The `output_grid` parameter specifies the grid size for optical flow calculation.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/optical_flow_example.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef optical_flow_pipe():\n    video = fn.readers.video(\n        device=\"gpu\", filenames=video_filename, sequence_length=sequence_length\n    )\n    of = fn.optical_flow(video, output_grid=4)\n    return of\n```\n\n----------------------------------------\n\nTITLE: Implement Custom DALI Iterator Wrapper\nDESCRIPTION: This code defines a `BetterDALILitMNIST` class that inherits from `LitMNIST` and implements a custom DALI iterator wrapper (`LightningWrapper`).  This wrapper modifies the output of the DALIClassificationIterator to be directly compatible with the `LitMNIST` class, eliminating the need for custom batch processing.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-lightning.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass BetterDALILitMNIST(LitMNIST):\n    def __init__(self):\n        super().__init__()\n\n    def prepare_data(self):\n        # no preparation is needed in DALI\n        pass\n\n    def setup(self, stage=None):\n        device_id = self.local_rank\n        shard_id = self.global_rank\n        num_shards = self.trainer.world_size\n        mnist_pipeline = GetMnistPipeline(\n            batch_size=BATCH_SIZE,\n            device=\"gpu\",\n            device_id=device_id,\n            shard_id=shard_id,\n            num_shards=num_shards,\n            num_threads=8,\n        )\n\n        class LightningWrapper(DALIClassificationIterator):\n            def __init__(self, *kargs, **kvargs):\n                super().__init__(*kargs, **kvargs)\n\n            def __next__(self):\n                out = super().__next__()\n                # DDP is used so only one pipeline per process\n                # also we need to transform dict returned by\n                # DALIClassificationIterator to iterable and squeeze the lables\n                out = out[0]\n                return [\n                    out[k] if k != \"label\" else torch.squeeze(out[k])\n                    for k in self.output_map\n                ]\n\n        self.train_loader = LightningWrapper(\n            mnist_pipeline,\n            reader_name=\"Reader\",\n            last_batch_policy=LastBatchPolicy.PARTIAL,\n        )\n\n    def train_dataloader(self):\n        return self.train_loader\n```\n\n----------------------------------------\n\nTITLE: Loading the DALI Plugin with Python\nDESCRIPTION: This Python code loads the custom DALI plugin library using `nvidia.dali.plugin_manager.load_library`. This makes the custom operator defined in the plugin available for use in DALI pipelines.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/create_a_custom_operator.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport nvidia.dali.plugin_manager as plugin_manager\n\nplugin_manager.load_library(\"./customdummy/build/libdali_customdummy.so\")\n```\n\n----------------------------------------\n\nTITLE: Define a DALI Pipeline using pipeline_def\nDESCRIPTION: This code defines a DALI pipeline that reads data from MXNet recordIO format, decodes images, resizes them, and then performs crop, mirror, and normalize operations. The pipeline is defined using the `pipeline_def` decorator for simplified pipeline creation.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/advanced/serialization.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, Pipeline\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os.path\n\ntest_data_root = os.environ[\"DALI_EXTRA_PATH\"]\nbase = os.path.join(test_data_root, \"db\", \"recordio\")\n\nidx_files = [base + \"/train.idx\"]\nrec_files = [base + \"/train.rec\"]\n\n\n@pipeline_def\ndef example_pipe():\n    encoded, labels = fn.readers.mxnet(path=rec_files, index_path=idx_files)\n    images = fn.decoders.image(encoded, device=\"mixed\", output_type=types.RGB)\n    images = fn.resize(\n        images,\n        interp_type=types.INTERP_LINEAR,\n        resize_shorter=fn.random.uniform(range=(256.0, 480.0)),\n    )\n    images = fn.crop_mirror_normalize(\n        images,\n        dtype=types.FLOAT,\n        crop=(224, 224),\n        mean=[0.0, 0.0, 0.0],\n        std=[1.0, 1.0, 1.0],\n    )\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Caffe Reader Pipeline Definition\nDESCRIPTION: This snippet defines a DALI pipeline to read data from Caffe LMDB files. It uses `fn.readers.caffe` to load images and labels, enabling shuffling and sharding for distributed processing. The loaded data is then passed through the `common_pipeline` for preprocessing.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-various-readers.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef caffe_reader_pipeline(num_gpus):\n    jpegs, labels = fn.readers.caffe(\n        path=lmdb_folder,\n        random_shuffle=True,\n        shard_id=Pipeline.current().device_id,\n        num_shards=num_gpus,\n        name=\"Reader\",\n    )\n\n    return common_pipeline(jpegs, labels)\n```\n\n----------------------------------------\n\nTITLE: Define function to convert flow to color image\nDESCRIPTION: This snippet defines a function `flow_to_color` that converts a 2D optical flow field (H,W,2) into a color image for visualization. It normalizes the flow vectors and then uses the `flow_compute_color` function to map the flow vectors to colors.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/optical_flow_example.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef flow_to_color(flow_uv, clip_flow=None, convert_to_bgr=False):\n    \"\"\"\n    Expects a two dimensional flow image of shape [H,W,2]\n    According to the C++ source code of Daniel Scharstein\n    According to the Matlab source code of Deqing Sun\n    :param flow_uv: np.ndarray of shape [H,W,2]\n    :param clip_flow: float, maximum clipping value for flow\n    :return:\n    \"\"\"\n\n    assert flow_uv.ndim == 3, \"input flow must have three dimensions\"\n    assert flow_uv.shape[2] == 2, \"input flow must have shape [H,W,2]\"\n\n    if clip_flow is not None:\n        flow_uv = np.clip(flow_uv, 0, clip_flow)\n\n    u = flow_uv[:, :, 0]\n    v = flow_uv[:, :, 1]\n\n    rad = np.sqrt(np.square(u) + np.square(v))\n    rad_max = np.max(rad)\n\n    epsilon = 1e-5\n    u = u / (rad_max + epsilon)\n    v = v / (rad_max + epsilon)\n\n    return flow_compute_color(u, v, convert_to_bgr)\n```\n\n----------------------------------------\n\nTITLE: Creating a DALI Dataset Input Function for Estimator Evaluation\nDESCRIPTION: This code defines a function `test_data_fn` that returns a DALI dataset for evaluating a TensorFlow Estimator model. The dataset is created within a `tf.device('/cpu:0')` context to ensure it resides on the CPU. The `mnist_set.map` function transforms the dataset to match the expected input format for the Estimator model (a dictionary of features and labels).\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef test_data_fn():\n    with tf.device(\"/cpu:0\"):\n        mnist_set = dali_tf.DALIDataset(\n            fail_on_device_mismatch=False,\n            pipeline=mnist_pipeline(device=\"cpu\"),\n            batch_size=BATCH_SIZE,\n            output_shapes=shapes,\n            output_dtypes=dtypes,\n            device_id=0,\n        )\n        mnist_set = mnist_set.map(\n            lambda features, labels: ({\"images\": features}, labels)\n        )\n\n    return mnist_set\n```\n\n----------------------------------------\n\nTITLE: Data Collation with DALI Proxy\nDESCRIPTION: Illustrates data collation using the `default_collate` function from `torch.utils.data.dataloader`. The code shows how to collate processed samples into a single batch run reference, and subsequently replace the run reference with actual data using `dali_server.produce_data()`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/plugins/pytorch_dali_proxy.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.data.dataloader import default_collate as default_collate\n\nwith dali_proxy.DALIServer(example_pipeline2(...)) as dali_server:\n   outs = []\n   for _ in range(10):\n      a = np.array(np.random.rand(3, 3), dtype=np.float32)\n      b = np.array(np.random.rand(3, 3), dtype=np.float32)\n      a_plus_b, b_minus_a = dali_server.proxy(a, b)\n      outs.append((a_plus_b, b_minus_a))\n\n   # Collate into a single batch run reference\n   outs = default_collate(outs)\n\n   # And we can now replace the run reference with actual data\n   outs = dali_server.produce_data(outs)\n```\n\n----------------------------------------\n\nTITLE: Stack Tensors with DALI\nDESCRIPTION: This snippet demonstrates how to stack tensors along different axes (0, 1, 2, and 3) using the `fn.stack` operator in NVIDIA DALI. It uses the same constant tensors (`src1`, `src2`, `src3`) as the concatenation example and stacks them along different axes within a DALI pipeline. The output tensors `st_outermost`, `st_1`, `st_2`, and `st_new_inner` represent the stacked tensors along axes 0, 1, 2, and 3, respectively.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/tensor_join.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe_stack = dali.pipeline.Pipeline(batch_size=1, num_threads=3, device_id=0)\nwith pipe_stack:\n    st_outermost = fn.stack(src1, src2, src3, axis=0)\n    st_1 = fn.stack(src1, src2, src3, axis=1)\n    st_2 = fn.stack(src1, src2, src3, axis=2)\n    st_new_inner = fn.stack(src1, src2, src3, axis=3)\n    pipe_stack.set_outputs(st_outermost, st_1, st_2, st_new_inner)\n\npipe_stack.build()\no = pipe_stack.run()\n```\n\n----------------------------------------\n\nTITLE: Numpy Reader with Out-of-Bounds Handling\nDESCRIPTION: This snippet showcases how to handle out-of-bounds access when extracting ROIs. It defines a DALI pipeline (`pipe_roi_oob`) that uses `roi_start` and `roi_end` values that cause the ROI to extend beyond the array boundaries. The `out_of_bounds_policy` is set to \"pad\", and `fill_value` is set to 0, causing the out-of-bounds regions to be filled with zeros.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(batch_size=batch_size, num_threads=3, device_id=0)\ndef pipe_roi_oob():\n    data = fn.readers.numpy(\n        device=\"cpu\",\n        file_root=data_dir,\n        files=files,\n        roi_start=(-150, -150),\n        roi_end=(400, 400),\n        out_of_bounds_policy=\"pad\",\n        fill_value=0,\n        shard_id=0,\n        num_shards=1,\n    )\n    return data\n\n\ndata_oob = run(pipe_roi_oob())\nplot_batch(data_oob)\n```\n\n----------------------------------------\n\nTITLE: Creating DALI Iterator for Tensorflow\nDESCRIPTION: This snippet defines a function `get_batch_test_dali` that creates DALI pipelines and a `DALIIterator` for Tensorflow.  It creates a pipeline for each GPU, then uses the `DALIIterator` to fetch data from the pipelines and expose it as Tensorflow tensors. It also places the tensors on the correct GPU device.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-various-readers.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nimport nvidia.dali.plugin.tf as dali_tf\n\nfrom tensorflow.compat.v1 import GPUOptions\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import Session\nfrom tensorflow.compat.v1 import placeholder\n\ntf.compat.v1.disable_eager_execution()\n\n\ndef get_batch_test_dali(batch_size, pipe_type):\n    pipe_name, label_type, _ = pipe_type\n    pipes = [\n        pipe_name(\n            batch_size=BATCH_SIZE,\n            num_threads=2,\n            device_id=device_id,\n            num_gpus=N,\n        )\n        for device_id in range(N)\n    ]\n\n    daliop = dali_tf.DALIIterator()\n    images = []\n    labels = []\n    for d in range(N):\n        with tf.device(\"/gpu:%i\" % d):\n            image, label = daliop(\n                pipeline=pipes[d],\n                shapes=[(BATCH_SIZE, 3, 227, 227), ()],\n                dtypes=[tf.int32, label_type],\n                device_id=d,\n            )\n            images.append(image)\n            labels.append(label)\n\n    return [images, labels]\n```\n\n----------------------------------------\n\nTITLE: Defining Common Pipeline for Image Processing\nDESCRIPTION: This code defines a common image processing pipeline using DALI operations. It decodes JPEG images, resizes them, and then performs crop, mirror, and normalization. This pipeline is used by all reader-specific pipelines to ensure consistent data preprocessing across different data sources.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-various-readers.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, Pipeline\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\n\n\ndef common_pipeline(jpegs, labels):\n    images = fn.decoders.image(jpegs, device=\"mixed\")\n    images = fn.resize(\n        images,\n        resize_shorter=fn.random.uniform(range=(256, 480)),\n        interp_type=types.INTERP_LINEAR,\n    )\n    images = fn.crop_mirror_normalize(\n        images,\n        crop_pos_x=fn.random.uniform(range=(0.0, 1.0)),\n        crop_pos_y=fn.random.uniform(range=(0.0, 1.0)),\n        dtype=types.FLOAT,\n        crop=(227, 227),\n        mean=[128.0, 128.0, 128.0],\n        std=[1.0, 1.0, 1.0],\n    )\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Define function to compute color from flow components\nDESCRIPTION: This snippet defines a function `flow_compute_color` that takes the horizontal (u) and vertical (v) components of the optical flow and maps them to a color using the color wheel.  It normalizes and scales the flow components to determine the appropriate color from the color wheel.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/optical_flow_example.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef flow_compute_color(u, v, convert_to_bgr=False):\n    \"\"\"\n    Applies the flow color wheel to (possibly clipped) flow components u and v.\n    According to the C++ source code of Daniel Scharstein\n    According to the Matlab source code of Deqing Sun\n    :param u: np.ndarray, input horizontal flow\n    :param v: np.ndarray, input vertical flow\n    :param convert_to_bgr: bool, whether to change ordering and output BGR\n                                 instead of RGB\n    :return:\n    \"\"\"\n\n    flow_image = np.zeros((u.shape[0], u.shape[1], 3), np.uint8)\n\n    colorwheel = make_colorwheel()  # shape [55x3]\n    ncols = colorwheel.shape[0]\n\n    rad = np.sqrt(np.square(u) + np.square(v))\n    a = np.arctan2(-v, -u) / np.pi\n\n    fk = (a + 1) / 2 * (ncols - 1)\n    k0 = np.floor(fk).astype(np.int32)\n    k1 = k0 + 1\n    k1[k1 == ncols] = 0\n    f = fk - k0\n\n    for i in range(colorwheel.shape[1]):\n        tmp = colorwheel[:, i]\n        col0 = tmp[k0] / 255.0\n        col1 = tmp[k1] / 255.0\n        col = (1 - f) * col0 + f * col1\n\n        idx = rad <= 1\n        col[idx] = 1 - rad[idx] * (1 - col[idx])\n        col[~idx] = col[~idx] * 0.75  # out of range?\n\n        # Note the 2-i => BGR instead of RGB\n        ch_idx = 2 - i if convert_to_bgr else i\n        flow_image[:, :, ch_idx] = np.floor(255 * col)\n\n    return flow_image\n```\n\n----------------------------------------\n\nTITLE: Build and run the GPU pipeline\nDESCRIPTION: This code builds and runs the previously defined GPU pipeline with the specified batch size, number of threads, and device ID. It then retrieves the output from the pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/image_processing/brightness_contrast_example.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe_gpu = bc_gpu_pipeline(batch_size=batch_size, num_threads=1, device_id=0)\npipe_gpu.build()\n```\n\nLANGUAGE: python\nCODE:\n```\ngpu_output = pipe_gpu.run()\n```\n\nLANGUAGE: python\nCODE:\n```\ndisplay(gpu_output, cpu=False)\n```\n\n----------------------------------------\n\nTITLE: Visualize the Optical Flow Result\nDESCRIPTION: This snippet visualizes the calculated optical flow by converting the flow vectors to a color image using the `flow_to_color` function and displaying it using matplotlib.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/optical_flow_example.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nof_result = flow_to_color(flow_vector[sequence_length // 2])\nplt.imshow(of_result)\n```\n\n----------------------------------------\n\nTITLE: Defining Global Constants for DALI and PaddlePaddle\nDESCRIPTION: This snippet defines global constants for the DALI pipeline, including the path to the test data, the number of GPUs available, the batch size per GPU, and the image size.  It retrieves the DALI_EXTRA_PATH environment variable and uses nvidia-smi to determine the number of available GPUs.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-basic_example.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os.path\nimport subprocess\n\ntest_data_root = os.environ[\"DALI_EXTRA_PATH\"]\n\n# Caffe LMDB\nlmdb_folder = os.path.join(test_data_root, \"db\", \"lmdb\")\n\nres = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE, text=True)\nN = res.stdout.count(\"\\n\")  # number of GPUs\nBATCH_SIZE = 128  # batch size per GPU\nIMAGE_SIZE = 3\n```\n\n----------------------------------------\n\nTITLE: Applying Min and Sum Reductions in DALI (Python)\nDESCRIPTION: This code snippet showcases the usage of `fn.reductions.min` and `fn.reductions.sum` operators. It initializes a DALI pipeline, provides input data using `fn.external_source`, and performs both minimum and sum reductions on each sample. The results are then printed.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reductions.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\nwith pipe:\n    input = fn.external_source(source=get_batch, dtype=types.INT64)\n    min = fn.reductions.min(input)\n    sum = fn.reductions.sum(input)\n\n    pipe.set_outputs(min, sum)\n\nrun_and_print(pipe)\n```\n\n----------------------------------------\n\nTITLE: Examine the Output with Constants\nDESCRIPTION: Defines a function to print the results of the `pipeline_2`, displaying the original tensors and the results of adding 200, multiplying by 0.75, and subtracting from 15. The output is formatted for readability.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef examine_output(pipe_out):\n    l = pipe_out[0].as_array()\n    r = pipe_out[1].as_array()\n    add_200 = pipe_out[2].as_array()\n    mul_075 = pipe_out[3].as_array()\n    sub_15 = pipe_out[4].as_array()\n    print(\"{}\\n+ 200 =\\n{}\\n\\n\".format(l, add_200))\n    print(\"{}\\n* 0.75 =\\n{}\\n\\n\".format(l, mul_075))\n    print(\"15 -\\n{}\\n=\\n{}\\n\\n\".format(r, sub_15))\n\n\nexamine_output(out)\n```\n\n----------------------------------------\n\nTITLE: ResNet Training with AMP (8 GPUs) - Bash\nDESCRIPTION: This command trains the ResNet50 model on 8 GPUs using Automatic Mixed Precision (AMP). It sets the number of epochs to 90, enables AMP, sets the loss scaling factor, enables dynamic loss scaling, and uses the NHWC data layout. It requires PaddlePaddle and the paddle.distributed module to be installed.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/paddle/resnet50/paddle-resnet50.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nFLAGS_apply_pass_to_program=1 python -m paddle.distributed.launch \\\n  --gpus=0,1,2,3,4,5,6,7 train.py \\\n  --epochs 90 \\\n  --amp \\\n  --scale-loss 128.0 \\\n  --use-dynamic-loss-scaling \\\n  --data-layout NHWC\n```\n\n----------------------------------------\n\nTITLE: Training a Keras Model with DALI Dataset on GPU\nDESCRIPTION: This code snippet shows how to move both the DALI dataset and the Keras model to the GPU for training. The DALI pipeline is configured to use the GPU, and both the dataset and the model are created within a `tf.device('/gpu:0')` context. This allows TensorFlow to use the GPU instance of the DALI dataset.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define the model and place it on the GPU\nwith tf.device(\"/gpu:0\"):\n    mnist_set = dali_tf.DALIDataset(\n        pipeline=mnist_pipeline(device=\"gpu\"),\n        batch_size=BATCH_SIZE,\n        output_shapes=shapes,\n        output_dtypes=dtypes,\n        device_id=0,\n    )\n    model = tf.keras.models.Sequential(\n        [\n            tf.keras.layers.Input(\n                shape=(IMAGE_SIZE, IMAGE_SIZE), name=\"images\"\n            ),\n            tf.keras.layers.Flatten(input_shape=(IMAGE_SIZE, IMAGE_SIZE)),\n            tf.keras.layers.Dense(HIDDEN_SIZE, activation=\"relu\"),\n            tf.keras.layers.Dropout(DROPOUT),\n            tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n        ]\n    )\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Build and run the CPU pipeline\nDESCRIPTION: This code builds and runs the previously defined CPU pipeline with the specified batch size, number of threads, and device ID. It retrieves the output from the pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/image_processing/brightness_contrast_example.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipe_cpu = bc_cpu_pipeline(batch_size=batch_size, num_threads=1, device_id=0)\npipe_cpu.build()\ncpu_output = pipe_cpu.run()\n```\n\nLANGUAGE: python\nCODE:\n```\ndisplay(cpu_output)\n```\n\n----------------------------------------\n\nTITLE: Define DALI Pipeline with Scalar Broadcasting\nDESCRIPTION: Defines a DALI pipeline that demonstrates broadcasting a batch of scalars against a batch of tensors using `ExternalSource`.  This shows how scalar values can be added element-wise to tensors within a DALI pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(batch_size=batch_size, num_threads=4, device_id=0)\ndef pipeline_4():\n    tensors = fn.external_source(lambda: get_data()[0], dtype=types.INT32)\n    scalars = fn.external_source(\n        lambda: np.arange(1, batch_size + 1), dtype=types.INT64\n    )\n    return tensors, scalars, tensors + scalars\n```\n\n----------------------------------------\n\nTITLE: Training Loop with JAX Sharding (Python)\nDESCRIPTION: This snippet implements the training loop. It iterates through the training data, updates the model using the `update` function, and calculates the accuracy on the validation set. The model is moved to a single device for validation to simplify the example.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-multigpu_example.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom model import init_model, accuracy\nfrom model import update\n\nmodel = init_model()\n\nfor epoch in range(num_epochs):\n    for it, batch in enumerate(training_iterator):\n        model = update(model, batch)\n\n    model_on_one_device = jax.tree_map(\n        lambda x: jax.device_put(x, jax.devices()[0]), model\n    )\n    test_acc = accuracy(model_on_one_device, validation_iterator)\n\n    print(f\"Epoch {epoch} sec\")\n    print(f\"Test set accuracy {test_acc}\")\n```\n\n----------------------------------------\n\nTITLE: CPU Operator Implementation C++\nDESCRIPTION: This C++ file provides the CPU implementation of the `CustomDummy` operator. It overrides the `RunImpl` method, retrieves the batch data, and uses the CPU thread pool to copy data from input to output in parallel. The operator schema and CPU version are registered using `DALI_SCHEMA` and `DALI_REGISTER_OPERATOR` macros.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/create_a_custom_operator.ipynb#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include \"customdummy/dummy.h\"\n\n#include <numeric>\n#include <vector>\n\n#include \"dali/core/kernel_params.h\"\n#include \"dali/core/tensor_view.h\"\n#include \"dali/pipeline/workspace/workspace.h\"\n\nnamespace dali {\n\nDALI_SCHEMA(CustomDummy)\n    .DocStr(\"Dummy operator that copies the input to the output.\")\n    .NumInput(1)\n    .NumOutput(1)\n    .AllowMultipleInputSets();\n\n\ntemplate <typename Backend>\nclass CustomDummyCpu : public CustomDummy {\n public:\n  explicit CustomDummyCpu(const OpSpec& spec) : CustomDummy(spec) {}\n\n  void RunImpl(Workspace& ws) override {\n    const auto& input = ws.Input<Backend>(0);\n    auto& output = ws.Output<Backend>(0);\n    output.ResizeLike(input);\n\n    auto& thread_pool = ws.GetThreadPool();\n    int n = input.num_samples();\n\n    for (int i = 0; i < n; ++i) {\n      thread_pool.AddWork([input, output, i](int thread_id) {\n          auto in_tensor = input.tensor_view(i);\n          auto out_tensor = output.mutable_tensor_view(i);\n\n          std::memcpy(out_tensor.data(), in_tensor.data(), in_tensor.shape().num_elements() * in_tensor.type().size());\n      }, in_tensor.shape().num_elements() * in_tensor.type().size());\n    }\n    thread_pool.RunAll();\n  }\n};\n\n\nDALI_REGISTER_OPERATOR(CustomDummy, CustomDummyCpu<CPU>, CPU);\n\n}\n\n```\n\n----------------------------------------\n\nTITLE: DALI Iterator for Training with JAX Sharding (Python)\nDESCRIPTION: This snippet defines a DALI iterator for training data, configured for use with JAX's sharding functionality.  It reads data using `fn.readers.caffe2`, decodes images, applies normalization, and converts labels to one-hot encoding.  The `sharding` argument ensures that the data is properly distributed across multiple GPUs.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-multigpu_example.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali.plugin.jax import data_iterator\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\n\n\nimage_size = 28\nnum_classes = 10\n\n\n@data_iterator(\n    output_map=[\"images\", \"labels\"],\n    reader_name=\"mnist_caffe2_reader\",\n    sharding=sharding,\n)\ndef mnist_training_iterator(data_path, num_shards, shard_id):\n    jpegs, labels = fn.readers.caffe2(\n        path=data_path,\n        random_shuffle=True,\n        name=\"mnist_caffe2_reader\",\n        num_shards=num_shards,\n        shard_id=shard_id,\n    )\n    images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.GRAY)\n    images = fn.crop_mirror_normalize(\n        images, dtype=types.FLOAT, std=[255.0], output_layout=\"CHW\"\n    )\n    images = fn.reshape(images, shape=[image_size * image_size])\n\n    labels = labels.gpu()\n    labels = fn.one_hot(labels, num_classes=num_classes)\n\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Train LitMNIST Model using PyTorch Lightning\nDESCRIPTION: This code instantiates the LitMNIST model and trains it using the PyTorch Lightning Trainer.  It sets training configurations like maximum epochs, devices (GPU), and uses native PyTorch dataloaders. The fit method starts the training process.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-lightning.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = LitMNIST()\ntrainer = Trainer(max_epochs=5, devices=1, accelerator=\"gpu\")\n# ddp work only in no-interactive mode, to test it unncoment and run as a script\n# trainer = Trainer(devices=8, accelerator=\"gpu\", strategy=\"ddp\", max_epochs=5)\n## MNIST data set is not always available to download due to network issues\n## to run this part of example either uncomment below line\n# trainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Defining a DALI pipeline with ExternalSource (CPU)\nDESCRIPTION: This snippet defines a DALI pipeline that uses the `ExternalSource` operator to read data from the custom iterator. It then decodes the images, enhances the contrast, and sets the outputs of the pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/external_input.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(batch_size=batch_size, num_threads=2, device_id=0)\nwith pipe:\n    jpegs, labels = fn.external_source(\n        source=eii, num_outputs=2, dtype=types.UINT8\n    )\n    decode = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.RGB)\n    enhance = fn.brightness_contrast(decode, contrast=2)\n    pipe.set_outputs(enhance, labels)\n```\n\n----------------------------------------\n\nTITLE: Performing Reductions Along Specific Axes in DALI (Python)\nDESCRIPTION: This snippet demonstrates how to perform reductions along specific axes using the `axes` argument. It calculates the minimum value along axis 0 and axis 1 using `fn.reductions.min`, showcasing the ability to reduce along different dimensions of the input tensor.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reductions.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\nwith pipe:\n    input = fn.external_source(source=get_batch, dtype=types.INT64)\n    min_axis_0 = fn.reductions.min(input, axes=0)\n    min_axis_1 = fn.reductions.min(input, axes=1)\n\n    pipe.set_outputs(min_axis_0, min_axis_1)\n\nrun_and_print(pipe)\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline with GPUDirect Storage (GDS) in Python\nDESCRIPTION: This code snippet defines a DALI pipeline that utilizes GPUDirect Storage (GDS) by setting the `device` parameter of the `fn.readers.numpy` operator to \"gpu\". This allows DALI to directly read data from storage into GPU memory, bypassing the CPU. The data is then copied back to CPU memory using `as_cpu()` for processing.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(batch_size=batch_size, num_threads=3, device_id=0)\ndef pipe_gds():\n    data = fn.readers.numpy(device=\"gpu\", file_root=data_dir, files=files)\n    return data\n\n\np = pipe_gds()\np.build()\npipe_out = p.run()\n\n# as_cpu() to copy the data back to CPU memory\ndata_gds = pipe_out[0].as_cpu().as_array()\nprint(data_gds.shape)\nplot_batch(data_gds)\n```\n\n----------------------------------------\n\nTITLE: Defining a custom data iterator (GPU)\nDESCRIPTION: This snippet defines a custom iterator class `ExternalInputGpuIterator` to feed data directly from the GPU into the DALI pipeline. It loads images using `imageio`, transfers them to the GPU using `cupy`, and returns them as cupy arrays in batches.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/external_input.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport cupy as cp\nimport imageio\n\n\nclass ExternalInputGpuIterator(object):\n    def __init__(self, batch_size):\n        self.images_dir = \"../../data/images/\"\n        self.batch_size = batch_size\n        with open(self.images_dir + \"file_list.txt\", \"r\") as f:\n            self.files = [line.rstrip() for line in f if line != \"\"]\n        shuffle(self.files)\n\n    def __iter__(self):\n        self.i = 0\n        self.n = len(self.files)\n        return self\n\n    def __next__(self):\n        batch = []\n        labels = []\n        for _ in range(self.batch_size):\n            jpeg_filename, label = self.files[self.i].split(\" \")\n            im = imageio.imread(self.images_dir + jpeg_filename)\n            im = cp.asarray(im)\n            im = im * 0.6\n            batch.append(im.astype(cp.uint8))\n            labels.append(cp.array([label], dtype=np.uint8))\n            self.i = (self.i + 1) % self.n\n        return (batch, labels)\n```\n\n----------------------------------------\n\nTITLE: Numpy Reader with List of Files\nDESCRIPTION: This snippet defines a DALI pipeline (`pipe3`) that uses the `fn.readers.numpy` operator with a list of file paths provided directly via the `files` argument.  It verifies that the loaded data is the same as in the previous examples.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Defining a pipeline taking a list of files directly as a keyword argument\n@pipeline_def(batch_size=batch_size, num_threads=3, device_id=0)\ndef pipe3():\n    data = fn.readers.numpy(device=\"cpu\", file_root=data_dir, files=files)\n    return data\n\n\ndata3 = run(pipe3())\nassert_all_equal(data1, data3)\n```\n\n----------------------------------------\n\nTITLE: Train AlexNet on ImageNet (PyTorch)\nDESCRIPTION: Trains an AlexNet model on the ImageNet dataset using PyTorch with a specified learning rate of 0.01. It utilizes the specified [imagenet-folder with train and val folders] which is required for the script to locate the training and validation datasets. The learning rate is reduced compared to ResNet due to the absence of batch normalization.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/resnet50/pytorch-resnet50.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py -a alexnet --lr 0.01 [imagenet-folder with train and val folders]\n```\n\n----------------------------------------\n\nTITLE: Process and Display Audio Data Python\nDESCRIPTION: Processes the output from the DALI pipeline, extracts the audio data and sampling rate, flattens the audio data, and plots the audio waveform using matplotlib.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/audio_processing/audio_decoder.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\naudio_data = cpu_output[0].at(0)\nsampling_rate = cpu_output[1].at(0)\nprint(\"Sampling rate:\", sampling_rate, \"[Hz]\")\nprint(\"Audio data:\", audio_data)\naudio_data = audio_data.flatten()\nprint(\"Audio data flattened:\", audio_data)\nplt.plot(audio_data)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Buffered Shuffle Generator - Python\nDESCRIPTION: Defines a function `buffered_shuffle` that randomizes the output from a given generator. It uses a prefetch buffer to store samples and randomly yields them, replacing the yielded samples with new ones from the generator. The `initial_fill` parameter controls the buffer size, and `seed` sets the random seed for reproducibility.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef buffered_shuffle(generator_factory, initial_fill, seed):\n    def buffered_shuffle_generator():\n        nonlocal generator_factory, initial_fill, seed\n        generator = generator_factory()\n        # The buffer size must be positive\n        assert(initial_fill > 0)\n\n        # The buffer that will hold the randomized samples\n        buffer = []\n\n        # The random context for preventing side effects\n        random_context = random.Random(seed)\n\n        try:\n            while len(buffer) < initial_fill: # Fills in the random buffer\n                buffer.append(next(generator))\n\n            # Selects a random sample from the buffer and then fills it back\n            # in with a new one\n            while True:\n                idx = random_context.randint(0, initial_fill-1)\n\n                yield buffer[idx]\n                buffer[idx] = None\n                buffer[idx] = next(generator)\n\n        # When the generator runs out of the samples flushes our the buffer\n        except StopIteration:\n            random_context.shuffle(buffer)\n\n            while buffer:\n                # Prevents the one sample that was not filled from being duplicated\n                if buffer[-1] != None:\n                    yield buffer[-1]\n                buffer.pop()\n    return buffered_shuffle_generator\n\n```\n\n----------------------------------------\n\nTITLE: Defining a DALI Pipeline with Mathematical Expressions (Python)\nDESCRIPTION: This code snippet demonstrates how to define a DALI pipeline that reads and decodes images, scales the channels by broadcasting, and clamps the result to a specified range using mathematical expressions. It uses DALI operators for reading and decoding, and then applies arithmetic and clamping operations directly on the output of those operators. The example utilizes `nvidia.dali.types.Constant` to wrap NumPy arrays for correct handling within DALI's mathematical expressions.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/math.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef my_pipeline():\n    \"\"\"Create a pipeline which reads and decodes the images, scales channels by\n    broadcasting and clamps the result to [128, 255) range.\"\"\"\n    img_files, _ = fn.readers.file(file_root=\"image_dir\")\n    images = fn.decoders.image(img_files, device=\"mixed\")\n    red_highlight = images * nvidia.dali.types.Constant(np.float32([1.25, 0.75, 0.75]))\n    result = nvidia.dali.math.clamp(red_highlight, 128, 255)\n    return result\n```\n\n----------------------------------------\n\nTITLE: Arithmetic and Bitwise Operations\nDESCRIPTION: This code defines lists of arithmetic and bitwise operations, along with lists of NumPy types. It iterates through these lists to create and run DALI pipelines with various type combinations, demonstrating the type promotion rules. It demonstrates different arithmetic and bitwise operations on different combinations of input types. The `arithmetic_pipeline` and `build_and_run` functions are used to execute these pipelines and print their results.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\narithmetic_operations = [\n    ((lambda x, y: x + y), \"+\"),\n    ((lambda x, y: x - y), \"-\"),\n    ((lambda x, y: x * y), \"*\"),\n    ((lambda x, y: x / y), \"/\"),\n    ((lambda x, y: x // y), \"//\"),\n]\n\nbitwise_operations = [\n    ((lambda x, y: x | y), \"|\"),\n    ((lambda x, y: x & y), \"&\"),\n    ((lambda x, y: x ^ y), \"^\"),\n]\n\nnp_types = [\n    np.int8,\n    np.int16,\n    np.int32,\n    np.int64,\n    np.uint8,\n    np.uint16,\n    np.uint32,\n    np.uint64,\n    np.float32,\n    np.float64,\n]\n\nfor op, op_name in arithmetic_operations:\n    for left_type in [np.uint8]:\n        for right_type in [np.uint8, np.int32, np.float32]:\n            pipe = arithmetic_pipeline(op, left_type, right_type)\n            build_and_run(pipe, op_name)\n\nfor op, op_name in bitwise_operations:\n    for left_type in [np.uint8]:\n        for right_type in [np.uint8, np.int32]:\n            pipe = arithmetic_pipeline(op, left_type, right_type)\n            build_and_run(pipe, op_name)\n```\n\n----------------------------------------\n\nTITLE: Define Custom External Input Iterator (Python)\nDESCRIPTION: This code defines a custom iterator class, `ExternalInputIterator`, to provide data to the DALI pipeline.  It reads a list of image filenames and labels from a file, shuffles them, and returns batches of image data and labels as NumPy arrays. The iterator also shards the data based on `device_id` and `num_gpus` for multi-GPU training.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-external_input.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ExternalInputIterator(object):\n    def __init__(self, batch_size, device_id, num_gpus):\n        self.images_dir = \"../../data/images/\"\n        self.batch_size = batch_size\n        with open(self.images_dir + \"file_list.txt\", \"r\") as f:\n            self.files = [line.rstrip() for line in f if line is not \"\"]\n        # whole data set size\n        self.data_set_len = len(self.files)\n        # based on the device_id and total number of GPUs - world size\n        # get proper shard\n        self.files = self.files[\n            self.data_set_len\n            * device_id\n            // num_gpus : self.data_set_len\n            * (device_id + 1)\n            // num_gpus\n        ]\n        self.n = len(self.files)\n\n    def __iter__(self):\n        self.i = 0\n        shuffle(self.files)\n        return self\n\n    def __next__(self):\n        batch = []\n        labels = []\n\n        if self.i >= self.n:\n            self.__iter__()\n            raise StopIteration\n\n        for _ in range(self.batch_size):\n            jpeg_filename, label = self.files[self.i % self.n].split(\" \")\n            batch.append(\n                np.fromfile(self.images_dir + jpeg_filename, dtype=np.uint8)\n            )  # we can use numpy\n            labels.append(np.uint8([int(label)]))\n            self.i += 1\n        return (batch, labels)\n\n    def __len__(self):\n        return self.data_set_len\n\n    next = __next__\n```\n\n----------------------------------------\n\nTITLE: Define DALI Pipeline with External Source (Python)\nDESCRIPTION: This snippet defines a DALI pipeline, `ExternalSourcePipeline`, that utilizes the `ExternalSource` operator to ingest data from the custom iterator. It specifies `num_outputs=2` to unpack the two outputs (images and labels) provided by the iterator. The pipeline then decodes the images, resizes them, casts them to `UINT8`, and sets the outputs for the pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-external_input.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef ExternalSourcePipeline(batch_size, num_threads, device_id, external_data):\n    pipe = Pipeline(batch_size, num_threads, device_id)\n    with pipe:\n        jpegs, labels = fn.external_source(\n            source=external_data, num_outputs=2, dtype=dali.types.UINT8\n        )\n        images = fn.decoders.image(jpegs, device=\"mixed\")\n        images = fn.resize(images, resize_x=240, resize_y=240)\n        output = fn.cast(images, dtype=dali.types.UINT8)\n        pipe.set_outputs(output, labels)\n    return pipe\n```\n\n----------------------------------------\n\nTITLE: Setting Current Pipeline with Context Manager (Python)\nDESCRIPTION: This example shows how to set the current pipeline using a context manager (`with` statement). This is necessary when using operators with side effects or when the graph is not defined inside a derived pipeline's `define_graph` method. An external source operator is used to generate two outputs, which are then set as the pipeline's outputs.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/pipeline.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipe = dali.Pipeline(batch_size=N, num_threads=3, device_id=0)\nwith pipe:\n    src = dali.ops.ExternalSource(my_source, num_outputs=2)\n    a, b = src()\n    pipe.set_outputs(a, b)\n```\n\n----------------------------------------\n\nTITLE: Integrate DALI into PyTorch Lightning with DALIClassificationIterator\nDESCRIPTION: This code defines a `DALILitMNIST` class, inheriting from `LitMNIST`, to integrate DALI for data loading within the PyTorch Lightning training process. It overrides the `prepare_data`, `setup`, `train_dataloader`, and `process_batch` methods to use the DALI pipeline and DALIClassificationIterator for efficient data fetching and preprocessing.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-lightning.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass DALILitMNIST(LitMNIST):\n    def __init__(self):\n        super().__init__()\n\n    def prepare_data(self):\n        # no preparation is needed in DALI\n        pass\n\n    def setup(self, stage=None):\n        device_id = self.local_rank\n        shard_id = self.global_rank\n        num_shards = self.trainer.world_size\n        mnist_pipeline = GetMnistPipeline(\n            batch_size=BATCH_SIZE,\n            device=\"gpu\",\n            device_id=device_id,\n            shard_id=shard_id,\n            num_shards=num_shards,\n            num_threads=8,\n        )\n        self.train_loader = DALIClassificationIterator(\n            mnist_pipeline,\n            reader_name=\"Reader\",\n            last_batch_policy=LastBatchPolicy.PARTIAL,\n        )\n\n    def train_dataloader(self):\n        return self.train_loader\n\n    def process_batch(self, batch):\n        x = batch[0][\"data\"]\n        y = batch[0][\"label\"].squeeze(-1)\n        return (x, y)\n```\n\n----------------------------------------\n\nTITLE: Integrating DALI Pipeline with PaddlePaddle Iterator\nDESCRIPTION: This snippet creates the DALI pipeline instances and integrates them with a PaddlePaddle generic iterator. It initializes the pipelines for each GPU, builds them, and then creates a `DALIGenericIterator` to feed data into the PaddlePaddle training loop.  It also includes assertions to check the correctness of labels.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-basic_example.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom nvidia.dali.plugin.paddle import DALIGenericIterator\n\n\nlabel_range = (0, 999)\npipes = [\n    caffe_pipeline(\n        batch_size=BATCH_SIZE, num_threads=2, device_id=device_id, num_gpus=N\n    )\n    for device_id in range(N)\n]\n\nfor pipe in pipes:\n    pipe.build()\n\ndali_iter = DALIGenericIterator(pipes, [\"data\", \"label\"], reader_name=\"Reader\")\n\nfor i, data in enumerate(dali_iter):\n    # Testing correctness of labels\n    for d in data:\n        label = d[\"label\"]\n        image = d[\"data\"]\n        ## labels need to be integers\n        assert np.equal(np.mod(label, 1), 0).all()\n        ## labels need to be in range pipe_name[2]\n        assert (np.array(label) >= label_range[0]).all()\n        assert (np.array(label) <= label_range[1]).all()\n\nprint(\"OK\")\n```\n\n----------------------------------------\n\nTITLE: Plotting Utility Function\nDESCRIPTION: This defines a utility function `plot_batch` which takes a batch of numpy arrays and visualizes them using matplotlib.  It creates a subplot for each array and displays it as an image using PIL.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# misc python stuff\nimport numpy as np\nfrom glob import glob\nimport shutil\nimport tempfile\n\n# visualization\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n\ndef plot_batch(np_arrays, nsamples=None):\n    if nsamples is None:\n        nsamples = len(np_arrays)\n    fig, axvec = plt.subplots(\n        nrows=1, ncols=nsamples, figsize=(10, 10 * nsamples)\n    )\n    for i in range(nsamples):\n        ax = axvec[i]\n        ax.tick_params(\n            left=False, bottom=False, labelleft=False, labelbottom=False\n        )\n        ax.imshow(Image.fromarray(np_arrays[i]))\n    plt.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Running and visualizing the results (GPU)\nDESCRIPTION: This snippet runs the DALI pipeline with GPU input, retrieves the processed image and label batches, transfers the image batch to the CPU, and displays an image from the batch using Matplotlib, printing its shape and label.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/external_input.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npipe_out_gpu = pipe_gpu.run()\nbatch_gpu = pipe_out_gpu[0].as_cpu()\nlabels_gpu = pipe_out_gpu[1].as_cpu()\n\nimg = batch_gpu.at(2)\nprint(img.shape)\nprint(labels_cpu.at(2))\nplt.axis(\"off\")\nplt.imshow(img)\n```\n\n----------------------------------------\n\nTITLE: Reinterpret Data Type - DALI (Python)\nDESCRIPTION: This code uses the `fn.reinterpret` operation in DALI to change the data type of a tensor. It takes an input tensor of type UINT8 and reinterprets it as UINT32. The innermost dimension is resized automatically to accommodate the new data type, given no new shape is specified.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reinterpret.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(device_id=0, num_threads=4, batch_size=3)\ndef example_reinterpret(input_data):\n    np.random.seed(1234)\n    inp = fn.external_source(input_data, batch=False, dtype=types.UINT8)\n    return inp, fn.reinterpret(inp, dtype=dali.types.UINT32)\n\n\npipe_reinterpret = example_reinterpret(\n    lambda: np.random.randint(\n        0,\n        255,\n        [np.random.randint(1, 7), 4 * np.random.randint(1, 5)],\n        dtype=np.uint8,\n    )\n)\n\npipe_reinterpret.build()\n\n\ndef hex_bytes(x):\n    f = f\"0x{{:0{2*x.nbytes}x}}\"\n    return f.format(x)\n\n\nshow_result(pipe_reinterpret.run(), formatter={\"int\": hex_bytes})\n```\n\n----------------------------------------\n\nTITLE: Performing Reductions Using Axis Names in DALI (Python)\nDESCRIPTION: This example shows how to use axis names for reductions instead of axis indices. It uses the `axis_names` argument with `fn.reductions.min` and requires specifying the layout of the input data via the `layout` argument in `fn.external_source`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reductions.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\nwith pipe:\n    input = fn.external_source(source=get_batch, layout=\"AB\", dtype=types.INT64)\n    min_axis_0 = fn.reductions.min(input, axis_names=\"A\")\n    min_axis_1 = fn.reductions.min(input, axis_names=\"B\")\n\n    pipe.set_outputs(min_axis_0, min_axis_1)\n\nrun_and_print(pipe)\n```\n\n----------------------------------------\n\nTITLE: Read WebDataset with External Source - Python\nDESCRIPTION: Defines a function `read_webdataset` that configures and returns a DALI `fn.external_source` node for reading WebDataset archives. It handles data loading, shuffling, padding, and batching, including support for variable-length batches. It uses the previously defined generator functions to implement these features.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef read_webdataset(\n    paths,\n    extensions=None,\n    random_shuffle=False,\n    initial_fill=256,\n    seed=0,\n    pad_last_batch=False,\n    read_ahead=False,\n    cycle=\"quiet\"\n):\n    # Parsing the input data\n    assert(cycle in {\"quiet\", \"raise\", \"no\"})\n    if extensions == None:\n        # All supported image formats\n        extensions = ';'.join([\"jpg\", \"jpeg\", \"img\", \"image\", \"pbm\", \"pgm\", \"png\"])\n    if type(extensions) == str:\n        extensions = (extensions,)\n\n    # For later information for batch collection and padding\n    max_batch_size = dali.pipeline.Pipeline.current().max_batch_size\n\n    def webdataset_generator():\n        bytes_np_mapper = (lambda data: np.frombuffer(data, dtype=np.uint8),\n                           )*len(extensions)\n        dataset_instance = (wds.WebDataset(paths)\n                            .to_tuple(*extensions)\n                            .map_tuple(*bytes_np_mapper))\n\n        for sample in dataset_instance:\n            yield sample\n\n    dataset = webdataset_generator\n\n    # Adding the buffered shuffling\n    if random_shuffle:\n        dataset = buffered_shuffle(dataset, initial_fill, seed)\n\n    # Adding the batch padding\n    if pad_last_batch:\n        dataset = last_batch_padding(dataset, max_batch_size)\n\n    # Collecting the data into batches (possibly undefull)\n    # Handled by a custom function only when `silent_cycle` is False\n    if cycle != \"quiet\":\n        dataset = collect_batches(dataset, max_batch_size)\n\n    # Prefetching the data\n    if read_ahead:\n        dataset=list(dataset())\n\n    return fn.external_source(\n        source=dataset,\n        num_outputs=len(extensions),\n        # If `cycle` is \"quiet\" then batching is handled by the external source\n        batch=(cycle != \"quiet\"),\n        cycle=cycle,\n        dtype=types.UINT8\n    )\n```\n\n----------------------------------------\n\nTITLE: Numpy Reader with File List\nDESCRIPTION: This snippet demonstrates how to use the `fn.readers.numpy` operator with a file list. It first creates a temporary file containing a list of numpy file paths.  It then defines a DALI pipeline (`pipe2`) that reads from this file list, using the `file_list` argument.  Finally, it asserts that the data loaded using the file list is identical to the data loaded using the glob filter in the previous example.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Used to verify that two batches are identical\ndef assert_all_equal(batch1, batch2):\n    assert len(batch1) == len(batch2)\n    batch_size = len(batch1)\n    for i in range(batch_size):\n        np.testing.assert_array_equal(batch1[i], batch2[i])\n\n\n# Listing all *.npy files in data_dir\nfiles = sorted([f for f in os.listdir(data_dir) if \".npy\" in f])\n\n\n# Defining a pipeline taking the file list file path as an argument\n@pipeline_def(batch_size=batch_size, num_threads=3, device_id=0)\ndef pipe2(file_list_path):\n    data = fn.readers.numpy(\n        device=\"cpu\", file_root=data_dir, file_list=filelist_path\n    )\n    return data\n\n\n# Creating a temporary directory to contain the \"file list\" file.\nwith tempfile.TemporaryDirectory() as tmp_dir:\n    # Creating a file_list text file, which contains the names of the data files\n    # to be loaded\n    filelist_path = os.path.join(tmp_dir, \"test_file_list.txt\")\n    with open(filelist_path, \"w\") as f:\n        f.writelines(\"\\n\".join(files))\n\n    # Displaying the contents of the file we just created\n    print(\"File list contents:\")\n    with open(filelist_path, \"r\") as f:\n        print(f.read())\n    print(\"\\n\")\n\n    data2 = run(pipe2(file_list_path=filelist_path))\n    assert_all_equal(data1, data2)\n```\n\n----------------------------------------\n\nTITLE: Offloading Reductions to GPU in DALI (Python)\nDESCRIPTION: This code snippet demonstrates how to offload reduction operations to the GPU by calling `.gpu()` on the input tensors. It showcases various reduction operators with different parameters, all running on the GPU.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reductions.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\nwith pipe:\n    input = fn.external_source(\n        source=get_batch, layout=\"ABC\", dtype=types.INT32\n    )\n    min = fn.reductions.min(input.gpu(), axis_names=\"AC\", keep_dims=True)\n    max = fn.reductions.max(input.gpu(), keep_dims=True)\n    sum = fn.reductions.sum(input.gpu(), dtype=types.INT64)\n    mean = fn.reductions.mean(input.gpu(), axes=0)\n    mean_square = fn.reductions.mean_square(input.gpu())\n    rms = fn.reductions.rms(input.gpu(), axes=(), dtype=types.FLOAT)\n    std_dev = fn.reductions.std_dev(input.gpu(), mean, axes=0)\n    variance = fn.reductions.variance(\n        input.gpu(), mean.gpu(), axes=0, keep_dims=True\n    )\n\n    pipe.set_outputs(min, max, sum, mean, mean_square, rms, std_dev, variance)\n\nrun_and_print(pipe)\n```\n\n----------------------------------------\n\nTITLE: Pipeline Usage Example (Addition)\nDESCRIPTION: This code snippet demonstrates how to use the defined functions to create and run a DALI pipeline that adds two tensors of type `uint8`. It defines a pipeline that adds two tensors of type `np.uint8` and then calls the `build_and_run` function to execute the pipeline and print the results using the '+' operator symbol.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe = arithmetic_pipeline((lambda x, y: x + y), np.uint8, np.uint8)\nbuild_and_run(pipe, \"+\")\n```\n\n----------------------------------------\n\nTITLE: Collect Batches Generator - Python\nDESCRIPTION: Defines a function `collect_batches` that collects data from a generator into batches of the specified `batch_size`. It handles potentially incomplete batches when the generator is exhausted and converts tuples of samples into tuples of batches of samples.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef collect_batches(generator_factory, batch_size):\n    def collect_batches_generator():\n        nonlocal generator_factory, batch_size\n        generator = generator_factory()\n        batch = []\n        try:\n            while True:\n                batch.append(next(generator))\n                if len(batch) == batch_size:\n                    # Converts tuples of samples into tuples of batches of samples\n                    yield tuple(map(list, zip(*batch)))\n                    batch = []\n        except StopIteration:\n            if batch is not []:\n                # Converts tuples of samples into tuples of batches of samples\n                yield tuple(map(list, zip(*batch)))\n    return collect_batches_generator\n```\n\n----------------------------------------\n\nTITLE: Train ResNet18 on ImageNet (PyTorch)\nDESCRIPTION: Trains a ResNet18 model on the ImageNet dataset using PyTorch. It utilizes the specified [imagenet-folder with train and val folders] which is required for the script to locate the training and validation datasets. The default learning rate schedule starts at 0.1 and decays by a factor of 10 every 30 epochs.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/resnet50/pytorch-resnet50.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py -a resnet18 [imagenet-folder with train and val folders]\n```\n\n----------------------------------------\n\nTITLE: DALI Proxy with Multiple Inputs\nDESCRIPTION: This demonstrates the usage of DALI proxy with multiple input tensors. The example showcases how to pass inputs to the proxy via positional or keyword arguments. The pipeline is defined to perform element-wise addition and subtraction on two input tensors.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/plugins/pytorch_dali_proxy.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom nvidia.dali import pipeline_def, fn, types\nfrom nvidia.dali.plugin.pytorch.experimental import proxy as dali_proxy\n\n@pipeline_def\ndef example_pipeline2(device):\n   a = fn.external_source(name=\"a\", no_copy=True)\n   b = fn.external_source(name=\"b\", no_copy=True)\n   return a + b, b - a\n\nwith dali_proxy.DALIServer(example_pipeline2(...)) as dali_server:\n   a = np.array(...)\n   b = np.array(...)\n\n   # Option 1: positional arguments\n   a_plus_b, b_minus_a = dali_server.proxy(a, b)\n\n   # Option 2: named arguments\n   a_plus_b, b_minus_a = dali_server.proxy(b=b, a=a)\n```\n\n----------------------------------------\n\nTITLE: Define TensorFlow Model\nDESCRIPTION: Defines a simple TensorFlow model using Keras, consisting of a flatten layer, a dense layer with ReLU activation, a dropout layer, and a final dense layer with softmax activation. The model is compiled with the Adam optimizer and sparse categorical crossentropy loss, using accuracy as a metric. The model is defined within the scope of `tf.distribute.MirroredStrategy` to distribute the training across multiple GPUs.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset-multigpu.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstrategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n\nwith strategy.scope():\n    model = tf.keras.models.Sequential(\n        [\n            tf.keras.layers.Input(\n                shape=(IMAGE_SIZE, IMAGE_SIZE), name=\"images\"\n            ),\n            tf.keras.layers.Flatten(input_shape=(IMAGE_SIZE, IMAGE_SIZE)),\n            tf.keras.layers.Dense(HIDDEN_SIZE, activation=\"relu\"),\n            tf.keras.layers.Dropout(DROPOUT),\n            tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n        ]\n    )\n\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Define CPU pipeline for brightness and contrast adjustment\nDESCRIPTION: This code defines a DALI pipeline that reads image files from a specified directory, decodes them, and applies brightness and contrast adjustments using the `fn.brightness_contrast` operator. The operations are performed on the CPU.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/image_processing/brightness_contrast_example.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def()\ndef bc_cpu_pipeline():\n    files, labels = fn.readers.file(file_root=image_filename)\n    images = fn.decoders.image(files)\n    converted = fn.brightness_contrast(\n        images, brightness_shift=0.3, contrast=0.4, contrast_center=100\n    )\n    return images, converted\n```\n\n----------------------------------------\n\nTITLE: Controlling Dimension Keeping with Reductions in DALI (Python)\nDESCRIPTION: This code snippet shows how to control whether unnecessary dimensions are removed after a reduction using the `keep_dims` argument. Setting `keep_dims=True` will retain the reduced dimensions with a size of 1.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reductions.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\nwith pipe:\n    input = fn.external_source(source=get_batch, dtype=types.INT32)\n    mean = fn.reductions.mean(input)\n    std_dev = fn.reductions.std_dev(input, mean, keep_dims=True)\n    variance = fn.reductions.variance(input, mean)\n\n    pipe.set_outputs(mean, std_dev, variance)\n\nrun_and_print(pipe)\n```\n\n----------------------------------------\n\nTITLE: Defining WebDataset Pipeline with external_source in DALI (Python)\nDESCRIPTION: This code defines a DALI pipeline using the `@dali.pipeline_def` decorator and an `external_source`-based loader (read_webdataset). It configures the pipeline for reading data from a webdataset, specifying paths, extensions, shuffling, initial fill, seed, padding, and reading behavior. The `decode_augment` function is used to decode and augment the images.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dali.pipeline_def(batch_size=batch_size, num_threads=4, device_id=0)\ndef webdataset_pipeline(\n    paths,\n    random_shuffle=False,\n    initial_fill=256,\n    seed=0,\n    pad_last_batch=False,\n    read_ahead=False,\n    cycle=\"quiet\"\n):\n    img, label = read_webdataset(paths=paths,\n                                 extensions=(\"jpg\", \"cls\"),\n                                 random_shuffle=random_shuffle,\n                                 initial_fill=initial_fill,\n                                 seed=seed,\n                                 pad_last_batch=pad_last_batch,\n                                 read_ahead=read_ahead,\n                                 cycle=cycle)\n    return decode_augment(img, seed=seed), label\n```\n\n----------------------------------------\n\nTITLE: Building and Running the Pipeline\nDESCRIPTION: This function builds and runs a DALI pipeline, then prints the inputs, operation, output, and their respective data types. It takes a DALI `pipe` and `op_name` as input, runs the pipeline, extracts the inputs and output tensors, and prints their values and data types using NumPy's `dtype` attribute. The pipeline outputs (l, r, and out) are converted to NumPy arrays using `.as_array()` before printing.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef build_and_run(pipe, op_name):\n    pipe.build()\n    pipe_out = pipe.run()\n    l = pipe_out[0].as_array()\n    r = pipe_out[1].as_array()\n    out = pipe_out[2].as_array()\n    print(\n        \"{} {} {} = {}; \\n\\twith types {} {} {} -> {}\\n\".format(\n            l, op_name, r, out, l.dtype, op_name, r.dtype, out.dtype\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Run both DALI Pipelines\nDESCRIPTION: This code builds and runs both the original pipeline (`pipe`) and the deserialized pipeline (`pipe2`), capturing their outputs for comparison. The `build()` method is called to prepare the pipelines, and then the `run()` method executes them.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/advanced/serialization.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipe.build()\noriginal_pipe_out = pipe.run()\nserialized_pipe_out = pipe2.run()\n```\n\n----------------------------------------\n\nTITLE: Color Augmentation Definition\nDESCRIPTION: This code snippet defines the `color` augmentation using the `@augmentation` decorator. It adjusts the color saturation of the image using `nvidia.dali.fn.saturation`. The magnitude is mapped to a [0, 2] parameter range. It takes the data and a parameter which control the color adjustment as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0, 0.9), randomly_negate=True, ...)\ndef color(data, parameter)\n```\n\n----------------------------------------\n\nTITLE: Reductions on Higher Dimensionality Data in DALI (Python)\nDESCRIPTION: This snippet showcases reductions on higher-dimensional data (2x2x2). It applies minimum reductions with empty axes, specific axes (0, 1), and specific layout names (\"AC\"), demonstrating flexibility in handling multi-dimensional inputs.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reductions.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\nwith pipe:\n    input = fn.external_source(\n        source=get_batch, layout=\"ABC\", dtype=types.INT32\n    )\n    min_axes_empty = fn.reductions.min(input, axes=())\n    min_axes_0_1 = fn.reductions.min(input, axes=(0, 1))\n    min_layout_A_C = fn.reductions.min(input, axis_names=\"AC\")\n\n    pipe.set_outputs(min_axes_empty, min_axes_0_1, min_layout_A_C)\n\nrun_and_print(pipe)\n```\n\n----------------------------------------\n\nTITLE: Loading and Plotting 2D Slices\nDESCRIPTION: This snippet loads a series of 2D numpy arrays representing MRI slices from the specified directory, and then plots them using the `plot_batch` function. It uses `np.load` to load each `.npy` file.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nser00001_2d = [\n    np.load(os.path.join(data_dir_2d, \"SER00001\", f\"{i}.npy\")) for i in range(4)\n]\nplot_batch(ser00001_2d)\n```\n\n----------------------------------------\n\nTITLE: Import and Define Paths\nDESCRIPTION: This code snippet imports the `os` module and defines the paths to the numpy array files used for testing. It also sets the batch size for the DALI pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nbatch_size = 4  # to be used in pipelines\ndali_extra_dir = os.environ[\"DALI_EXTRA_PATH\"]\ndata_dir_2d = os.path.join(\n    dali_extra_dir, \"db\", \"3D\", \"MRI\", \"Knee\", \"npy_2d_slices\", \"STU00001\"\n)\ndata_dir_3d = os.path.join(\n    dali_extra_dir, \"db\", \"3D\", \"MRI\", \"Knee\", \"npy_3d\", \"STU00001\"\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing the custom data iterator (GPU)\nDESCRIPTION: This snippet initializes an instance of the `ExternalInputGpuIterator` with a specified batch size and prints the type of the first image in the first batch returned by the iterator.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/external_input.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\neii_gpu = ExternalInputGpuIterator(batch_size)\n\nprint(type(next(iter(eii_gpu))[0][0]))\n```\n\n----------------------------------------\n\nTITLE: Strided Slices in DALI\nDESCRIPTION: This snippet illustrates how to use strided slices in DALI, similar to NumPy. Striding can be done in both positive and negative directions, and across multiple dimensions.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/indexing.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nreversed = array[::-1]\nevery_second = array[::2]\nevery_second_reversed = array[::-2]\nquarter_resolution = image[::2, ::2]\n```\n\n----------------------------------------\n\nTITLE: DALI Iterator for Validation (Python)\nDESCRIPTION: This snippet defines a DALI iterator for validation data. It reads data using `fn.readers.caffe2`, decodes images, applies normalization, and prepares labels. This iterator is designed to run on a single GPU for simplicity in this example.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-multigpu_example.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@data_iterator(\n    output_map=[\"images\", \"labels\"], reader_name=\"mnist_caffe2_reader\"\n)\ndef mnist_validation_iterator(data_path):\n    jpegs, labels = fn.readers.caffe2(\n        path=data_path, random_shuffle=False, name=\"mnist_caffe2_reader\"\n    )\n    images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.GRAY)\n    images = fn.crop_mirror_normalize(\n        images, dtype=types.FLOAT, std=[255.0], output_layout=\"CHW\"\n    )\n    images = fn.reshape(images, shape=[image_size * image_size])\n\n    labels = labels.gpu()\n\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Generating Python Stubs\nDESCRIPTION: This snippet generates Python stubs (.pyi files) for the DALI Python backend.  It defines a custom target `dali_python_generate_stubs` which depends on the DALI Python library and executes a Python script to generate the stubs. This improves IDE support and type checking.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/python/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT ${CMAKE_CROSSCOMPILING} AND NOT ${BUILD_WITH_ASAN})\n  # Prepate PYTHONPATH targetting the pre-packed wheel produced in this build\n  set(PYTHON_TARGET_PATH ${PROJECT_BINARY_DIR}/dali/python)\n  if($ENV{PYTHONPATH})\n    set(PYTHONPATH \"${PYTHON_TARGET_PATH}:$ENV{PYTHONPATH}\")\n  else()\n    set(PYTHONPATH \"${PYTHON_TARGET_PATH}\")\n  endif()\n\n  if (PREBUILD_DALI_LIBS)\n    add_custom_target(dali_python_generate_stubs ALL\n                      DEPENDS dali_python ${dali_python_function_lib})\n  else (PREBUILD_DALI_LIBS)\n    add_custom_target(dali_python_generate_stubs ALL\n                      DEPENDS dali_python ${dali_python_function_lib} dali dali_operators copy_post_build_target)\n  endif (PREBUILD_DALI_LIBS)\n\n  # Build the .pyi stubs, adjusting the PYTHONPATH for the invocation, allowing to use the\n  # backend from the current build.\n  add_custom_command(\n    TARGET dali_python_generate_stubs\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}\n    COMMAND PYTHONPATH=${PYTHONPATH} ${PYTHON_STUBGEN_INTERPRETER} ${PROJECT_SOURCE_DIR}/internal_tools/python_stub_generator.py --wheel_path ${PROJECT_BINARY_DIR}/dali/python/nvidia/dali\n  )\n\n  # Format the bindings, so they are somewhat readable.\n  if (${FORMAT_PYTHON_STUB_FILES})\n    add_custom_target(dali_python_format_stubs ALL DEPENDS dali_python dali_python_generate_stubs)\n    add_custom_command(\n      TARGET dali_python_format_stubs\n      WORKING_DIRECTORY ${PROJECT_BINARY_DIR}\n      COMMAND find dali/python/nvidia/dali -name '*.pyi' -print0 | xargs -0 black --line-length 100 --target-version py38 --verbose\n    )\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline with Constant Wrapper and Explicit Type (Float)\nDESCRIPTION: This example demonstrates how to use the DALI `Constant` wrapper with an explicitly specified data type (`UINT8`) using float value. It defines a DALI `Constant` with a float and `DALIDataType.UINT8`, creates a pipeline that multiplies it with a `uint8` tensor, and runs the pipeline, printing the result.  `is_const_left` flag is used, to specify that constant is on the left side of the equation.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nconstant = Constant(10.0, types.DALIDataType.UINT8)\npipe = arithmetic_constant_pipeline((lambda x: constant * x), np.uint8)\nbuild_and_run_with_const(pipe, \"*\", constant, True)\n```\n\n----------------------------------------\n\nTITLE: Custom PyTorch Dataset with DALI Proxy\nDESCRIPTION: Demonstrates the creation of a custom PyTorch Dataset that utilizes the DALI proxy as a transform function. The `__getitem__` method returns the processed sample and the original label.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/plugins/pytorch_dali_proxy.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, transform_fn, data):\n        self.data = data\n        self.transform_fn = transform_fn\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        filename, label = self.data[idx]\n        return self.transform_fn(filename), label  # Returns processed sample and the original label\n```\n\n----------------------------------------\n\nTITLE: Defining a Rotate Augmentation in DALI\nDESCRIPTION: This code snippet demonstrates how to define a custom rotate augmentation using the `@augmentation` decorator in NVIDIA DALI. It takes an input batch, an angle, and optional fill_value and rotate_keep_size parameters. The core functionality is implemented using `fn.rotate` from DALI.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali.auto_aug.core import augmentation\nfrom nvidia.dali import fn\n\n@augmentation(mag_range=(0, 30), randomly_negate=True)\ndef rotate_aug(data, angle, fill_value=128, rotate_keep_size=True):\n   return fn.rotate(data, angle=angle, fill_value=fill_value, keep_size=True)\n```\n\n----------------------------------------\n\nTITLE: Running Paxml Training with DALI Input\nDESCRIPTION: This code snippet demonstrates how to run a Paxml training job using the `paxml.main` module. It specifies the job log directory and the experiment configuration file. The output is redirected to `/dev/null` to suppress verbose output during training.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/pax-basic_example.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!python -m paxml.main --job_log_dir=/tmp/dali_pax_logs --exp pax_examples.dali_pax_example.MnistExperiment 2>/dev/null\n```\n\n----------------------------------------\n\nTITLE: Defining a DALI Pipeline for Caffe Data\nDESCRIPTION: This snippet defines a DALI pipeline using the `@pipeline_def` decorator.  It reads data from a Caffe LMDB database using `fn.readers.caffe`, decodes the images, resizes them, and applies crop-mirror-normalize transformations. The pipeline outputs images and labels.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-basic_example.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, Pipeline\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\n\n\n@pipeline_def\ndef caffe_pipeline(num_gpus):\n    device_id = Pipeline.current().device_id\n    jpegs, labels = fn.readers.caffe(\n        name=\"Reader\",\n        path=lmdb_folder,\n        random_shuffle=True,\n        shard_id=device_id,\n        num_shards=num_gpus,\n    )\n    images = fn.decoders.image(jpegs, device=\"mixed\")\n    images = fn.resize(\n        images,\n        resize_shorter=fn.random.uniform(range=(256, 480)),\n        interp_type=types.INTERP_LINEAR,\n    )\n    images = fn.crop_mirror_normalize(\n        images,\n        crop_pos_x=fn.random.uniform(range=(0.0, 1.0)),\n        crop_pos_y=fn.random.uniform(range=(0.0, 1.0)),\n        dtype=types.FLOAT,\n        crop=(227, 227),\n        mean=[128.0, 128.0, 128.0],\n        std=[1.0, 1.0, 1.0],\n    )\n\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Creating TFrecord Data from COCO Dataset\nDESCRIPTION: This script converts the COCO dataset into TFrecord format, which is a binary file format used for efficient data storage and retrieval in TensorFlow. The script takes the image directory, annotation file, and output file prefix as input.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/efficientdet/README.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npython3 ./dataset/create_coco_tfrecord.py \\\n            --image_dir ./coco/train2017 \\\n            --object_annotations_file ./coco/annotations/instances_train2017.json \\\n            --output_file_prefix ./tfrecords/train\n```\n\n----------------------------------------\n\nTITLE: Print Concatenated Tensor (Outer Axis)\nDESCRIPTION: This snippet prints the concatenated tensor along the outer axis (axis=0) and its shape. It retrieves the output from the DALI pipeline (`pipe_cat`) and prints the first element of the output tensor, along with its shape, using `o[0].at(0)`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/tensor_join.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Concatenation along outer axis:\")\nprint(o[0].at(0))\nprint(\"Shape: \", o[0].at(0).shape)\n```\n\n----------------------------------------\n\nTITLE: Initializing the custom data iterator (CPU)\nDESCRIPTION: This snippet initializes an instance of the `ExternalInputIterator` with a specified batch size. This iterator will be used as the data source for the DALI pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/external_input.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\neii = ExternalInputIterator(batch_size)\n```\n\n----------------------------------------\n\nTITLE: Define DALI Pipeline for MNIST\nDESCRIPTION: Defines a DALI pipeline to read and preprocess the MNIST dataset. It uses the `readers.caffe2` operator to read data from LMDB format, `decoders.image` to decode images, and `crop_mirror_normalize` for preprocessing. The pipeline shards the dataset across multiple devices.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset-multigpu.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(batch_size=BATCH_SIZE)\ndef mnist_pipeline(shard_id):\n    jpegs, labels = fn.readers.caffe2(\n        path=data_path,\n        random_shuffle=True,\n        shard_id=shard_id,\n        num_shards=NUM_DEVICES,\n    )\n    images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.GRAY)\n    images = fn.crop_mirror_normalize(\n        images, dtype=types.FLOAT, std=[255.0], output_layout=\"CHW\"\n    )\n\n    return images, labels.gpu()\n```\n\n----------------------------------------\n\nTITLE: Reshape with Relative Shape - DALI (Python)\nDESCRIPTION: This code demonstrates using relative shapes in DALI's `fn.reshape` to define output dimensions as multiples of input dimensions. It reinterprets the input tensor to have twice as many columns as the input had rows. The src_dims argument specifies the input dimension used for the relative calculation.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reinterpret.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(device_id=0, num_threads=4, batch_size=3)\ndef example_rel_shape(input_data):\n    np.random.seed(1234)\n    inp = fn.external_source(input_data, batch=False, dtype=types.INT32)\n    return inp, fn.reshape(inp, rel_shape=[0.5, 2], src_dims=[1, 0])\n\n\npipe_rel_shape = example_rel_shape(\n    lambda: np.random.randint(\n        0,\n        10,\n        [np.random.randint(1, 7), 2 * np.random.randint(1, 5)],\n        dtype=np.int32,\n    )\n)\n\npipe_rel_shape.build()\nshow_result(pipe_rel_shape.run())\n```\n\n----------------------------------------\n\nTITLE: Extract images from pipeline outputs\nDESCRIPTION: This code extracts the image tensors from the outputs of both pipelines. The `original_images` and `serialized_images` variables now hold the image data produced by the original and deserialized pipelines respectively.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/advanced/serialization.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\noriginal_images, _ = original_pipe_out\nserialized_images, _ = serialized_pipe_out\n```\n\n----------------------------------------\n\nTITLE: Benchmark EfficientNet with DALI and TrivialAugment\nDESCRIPTION: This command benchmarks EfficientNet training using DALI for data loading with TrivialAugment enabled. The `automatic-augmentation` flag is set to `trivialaugment`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# DALI with TrivialAugment\npython multiproc.py --nproc_per_node 8 ./main.py --amp --static-loss-scale 128 \\\n                    --batch-size 128 --epochs 4 --no-checkpoints --training-only \\\n                    --data-backend dali --automatic-augmentation trivialaugment \\\n                    --workspace $RESULT_WORKSPACE \\\n                    --report-file bench_report_dali_ta.json $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Caffe reader pipeline (Python)\nDESCRIPTION: This snippet defines a DALI pipeline for reading data from Caffe LMDB files. It uses `fn.readers.caffe` to read the data, sharding it across multiple GPUs. The data is then passed to the `common_pipeline` function for image processing.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-various-readers.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef caffe_reader_pipeline(num_gpus):\n    jpegs, labels = fn.readers.caffe(\n        path=lmdb_folder,\n        random_shuffle=True,\n        shard_id=Pipeline.current().device_id,\n        num_shards=num_gpus,\n        name=\"Reader\",\n    )\n\n    return common_pipeline(jpegs, labels)\n```\n\n----------------------------------------\n\nTITLE: Shear Y Augmentation Definition\nDESCRIPTION: This code snippet defines the `shear_y` augmentation using the `@augmentation` decorator. It applies a shear transformation along the y-axis using `nvidia.dali.fn.transforms.shear` and `nvidia.dali.fn.warp_affine`.  It takes shear and optional fill_value and interp_type as input parameters.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0, 0.3), randomly_negate=True, ...)\ndef shear_y(data, shear, fill_value=128, interp_type=None)\n```\n\n----------------------------------------\n\nTITLE: Solarize Add Augmentation Definition\nDESCRIPTION: This code snippet defines the `solarize_add` augmentation using the `@augmentation` decorator. It applies the shift to the pixels of value lower than 128.  It takes the data and a shift parameter as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0, 110), ...)\ndef solarize_add(data, shift)\n```\n\n----------------------------------------\n\nTITLE: Instantiating DALI iterators for training and validation\nDESCRIPTION: This snippet instantiates DALI iterators for training and validation using the `mnist_iterator` function defined previously. It passes the data paths, shuffle flag, and batch size to the iterator constructor.  The resulting iterators are then printed to the console for verification.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-basic_example.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Creating iterators\")\n\ntraining_iterator = mnist_iterator(\n    data_path=training_data_path, random_shuffle=True, batch_size=batch_size\n)\n\nvalidation_iterator = mnist_iterator(\n    data_path=validation_data_path, random_shuffle=False, batch_size=batch_size\n)\n\nprint(training_iterator)\nprint(validation_iterator)\n```\n\n----------------------------------------\n\nTITLE: Run EfficientNet Training on a Single GPU (AMP)\nDESCRIPTION: This command runs EfficientNet training on a single GPU using Automatic Mixed Precision (AMP). It includes the `--amp` flag and sets a static loss scale.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython ./main.py --batch-size 64 --amp --static-loss-scale 128 $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Initializing DALI Pipeline with External Source (Python)\nDESCRIPTION: This code snippet initializes a DALI pipeline using `ExternalSource` to provide input data. It defines a `get_batch` function to generate sample data as numpy arrays. It then creates a pipeline, feeds the data using `fn.external_source`, and prints the output.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reductions.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\nimport nvidia.dali.backend as backend\nfrom nvidia.dali.pipeline import Pipeline\nimport numpy as np\n\nbatch_size = 2\n\n\ndef get_batch():\n    return [\n        np.reshape(np.arange(9), (3, 3)) * (i + 1) for i in range(batch_size)\n    ]\n\n\ndef run_and_print(pipe):\n    pipe.build()\n    output = pipe.run()\n    for i, out in enumerate(output):\n        if type(out) == backend.TensorListGPU:\n            out = out.as_cpu()\n        output_array = out.as_array()\n        print(\"Output {}:\\n{} \\n\".format(i, output_array))\n\n\npipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\nwith pipe:\n    input = fn.external_source(source=get_batch, dtype=types.INT64)\n\n    pipe.set_outputs(input)\n\nrun_and_print(pipe)\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline with Constant Wrapper and uint8() function\nDESCRIPTION: This example demonstrates using the DALI `Constant` wrapper with the `.uint8()` conversion function.  It defines a DALI `Constant` with an integer and converts it to `uint8` using `.uint8()`, constructs a pipeline that multiplies it with a `uint8` tensor, and executes the pipeline. `is_const_left` flag is used, to specify that constant is on the left side of the equation.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nconstant = Constant(10).uint8()\npipe = arithmetic_constant_pipeline((lambda x: constant * x), np.uint8)\nbuild_and_run_with_const(pipe, \"*\", constant, True)\n```\n\n----------------------------------------\n\nTITLE: Solarize Augmentation Definition\nDESCRIPTION: This code snippet defines the `solarize` augmentation using the `@augmentation` decorator. It inverts the pixels that lie below a threshold. It takes the data and a threshold parameter as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(256, 0), ...)\ndef solarize(data, threshold)\n```\n\n----------------------------------------\n\nTITLE: Loading and Plotting 3D Volume Slices\nDESCRIPTION: This loads a 3D numpy array representing an MRI volume from the specified directory.  It then extracts the first four slices along the first axis and plots them using `plot_batch`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nser00001_3d = np.load(os.path.join(data_dir_3d, \"SER00001.npy\"))\nplot_batch([ser00001_3d[0], ser00001_3d[1], ser00001_3d[2], ser00001_3d[3]])\n```\n\n----------------------------------------\n\nTITLE: Evaluate the Model\nDESCRIPTION: Evaluates the trained TensorFlow model using the distributed DALI dataset. The `model.evaluate` method is used to evaluate the model's performance.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset-multigpu.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel.evaluate(train_dataset, steps=ITERATIONS)\n```\n\n----------------------------------------\n\nTITLE: Saving Checkpoints during Training\nDESCRIPTION: This command adds a flag to the training command to save checkpoints after each epoch. The checkpoints are saved to the specified directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/yolov4/readme.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n--ckpt_dir /ckpt\n```\n\n----------------------------------------\n\nTITLE: Eval.py Usage Arguments\nDESCRIPTION: This shows the command-line arguments for the eval.py script. The arguments configure evaluation parameters, data paths, pipeline type, and model weights.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/efficientdet/README.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nusage: eval.py [-h] --input_type {tfrecord,coco} [--images_path IMAGES_PATH]\n                 [--annotations_path ANNOTATIONS_PATH]\n                 [--eval_file_pattern EVAL_FILE_PATTERN]\n                 [--eval_steps EVAL_STEPS]\n                 --pipeline_type {synthetic,tensorflow,dali_cpu,dali_gpu}\n                 [--weights WEIGHTS] [--model_name MODEL_NAME] [--hparams HPARAMS]\n\n  optional arguments:\n    -h, --help            show this help message and exit\n    --input_type {tfrecord,coco}\n                          Input type.\n    --images_path IMAGES_PATH\n                          Path to COCO images.\n    --annotations_path ANNOTATIONS_PATH\n                          Path to COCO annotations.\n    --eval_file_pattern EVAL_FILE_PATTERN\n                          TFrecord files glob pattern for files with evaluation data.\n    --eval_steps EVAL_STEPS\n                          Number of examples to evaluate.\n    --pipeline_type {synthetic,tensorflow,dali_cpu,dali_gpu}\n                          Pipeline type used while loading and preprocessing data.\n                          One of: tensorflow  pipeline used in original\n                          EfficientDet implementation on\n                          https://github.com/google/automl/tree/master/efficientdet\n                          synthetic  like `tensorflow` pipeline type but repeats\n                          one batch endlessly dali_gpu  pipeline which uses\n                          Nvidia Data Loading Library (DALI) to run part of data\n                          preprocessing on GPUs to improve efficiency dali_cpu \n                          like `dali_gpu` pipeline type but restricted to run\n                          only on CPU\n    --weights WEIGHTS     Name of the file with model weights.\n    --model_name MODEL_NAME\n    --hparams HPARAMS     String or filename with parameters.\n```\n\n----------------------------------------\n\nTITLE: Creating a custom AutoAugment policy in DALI\nDESCRIPTION: This code snippet illustrates how to create a custom AutoAugment policy using the `Policy` class and available augmentations. The `my_custom_policy` function defines a policy with three sub-policies, each containing two augmentation operations.  The augmentations are defined using functions such as `shear_x.augmentation` and `shear_y.augmentation`, specifying magnitude ranges. The `Policy` constructor defines the policy name, number of magnitude bins, and the list of sub-policies.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/auto_augment.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali.auto_aug import augmentations\nfrom nvidia.dali.auto_aug.core import Policy\n\ndef my_custom_policy() -> Policy:\n    \"\"\"\n    Creates a simple AutoAugment policy with 3 sub-policies using custom\n    magnitude ranges.\n    \"\"\"\n\n    shear_x = augmentations.shear_x.augmentation((0, 0.5), True)\n    shear_y = augmentations.shear_y.augmentation((0, 0.5), True)\n    rotate = augmentations.rotate.augmentation((0, 40), True)\n    invert = augmentations.invert\n    return Policy(\n        name=\"SimplePolicy\", num_magnitude_bins=11, sub_policies=[\n            [(shear_x, 0.8, 7), (shear_y, 0.8, 4)],\n            [(invert, 0.4, None), (rotate, 0.6, 8)],\n            [(rotate, 0.6, 7), (shear_y, 0.6, 3)],\n        ])\n```\n\n----------------------------------------\n\nTITLE: Extracting Model Weights from Checkpoint\nDESCRIPTION: This python script extracts model weights from a checkpoint file. The `--checkpoint-path` argument specifies the location of the checkpoint, and `--weight-path` specifies where the extracted weights will be stored.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npython checkpoint2model.py --checkpoint-path <path to checkpoint>\n                             --weight-path <path where weights will be stored>\n```\n\n----------------------------------------\n\nTITLE: Argument Input in Pipeline (Python)\nDESCRIPTION: This example demonstrates using argument inputs in a DALI pipeline. The `flip_param` is generated by a CPU operator and passed as a named keyword argument to the `fn.flip` operator, establishing a connection in the processing graph. The labels are explicitly marked for transfer to the GPU using `.gpu()`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/pipeline.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef my_pipeline():\n    img_files, labels = fn.readers.file(file_root='image_dir', device='cpu')\n    # `images` is GPU data (result of Mixed operator)\n    images = fn.decoders.image(img_files, device='mixed')\n    # `coin_flip` must be on CPU so the `flip_params` can be used as argument input\n    flip_param = fn.random.coin_flip(device='cpu')\n    # `images` is input (GPU) and `flip_param` is argument input (CPU)\n    flipped = fn.flip(images, horizontal=flip_param, device='gpu')\n    # `labels` is explicitly marked for transfer to GPU, `flipped` is already GPU\n    return flipped, labels.gpu()\n\npipe = my_pipeline(batch_size=4, num_threads=2, device_id=0)\n```\n\n----------------------------------------\n\nTITLE: Define DALI iterator for MNIST dataset using jax plugin\nDESCRIPTION: This snippet defines a DALI iterator function using the `data_iterator` decorator from `nvidia.dali.plugin.jax`. It configures the pipeline to read data in Caffe2 format, decode images, normalize pixel values, and reshape the output tensors. The function also moves labels to GPU memory and applies one-hot encoding if `is_training` is true. This iterator is designed to work with the JAX framework.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/flax-basic_example.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\n\nfrom nvidia.dali.plugin.jax import data_iterator\n\n\nbatch_size = 50\nimage_size = 28\nnum_classes = 10\n\n\n@data_iterator(\n    output_map=[\"images\", \"labels\"], reader_name=\"mnist_caffe2_reader\"\n)\ndef mnist_iterator(data_path, is_training):\n    jpegs, labels = fn.readers.caffe2(\n        path=data_path, random_shuffle=is_training, name=\"mnist_caffe2_reader\"\n    )\n    images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.GRAY)\n    images = fn.crop_mirror_normalize(images, dtype=types.FLOAT, std=[255.0])\n    images = fn.reshape(images, shape=[-1])  # Flatten the output image\n\n    labels = labels.gpu()\n\n    if is_training:\n        labels = fn.one_hot(labels, num_classes=num_classes)\n\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Define DALI Pipeline with Arithmetic Operators\nDESCRIPTION: Defines a DALI pipeline that uses the `ExternalSource` operator to ingest data and performs element-wise addition, multiplication, and floor division on the input tensors.  The pipeline returns the original tensors and the results of the arithmetic operations.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(batch_size=batch_size, num_threads=4, device_id=0)\ndef pipeline():\n    l, r = fn.external_source(source=get_data, num_outputs=2, dtype=types.INT32)\n    sum_result = l + r\n    mul_result = l * r\n    div_result = l // r\n    return l, r, sum_result, mul_result, div_result\n```\n\n----------------------------------------\n\nTITLE: Creating a File List\nDESCRIPTION: This code creates a temporary file containing a file list of video paths and label information. It constructs a string (file_list_txt) with video paths, labels, and frame number ranges, then writes this string to a temporary file using tempfile.NamedTemporaryFile.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/video/video_file_list_outputs.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndali_extra_path = os.environ[\"DALI_EXTRA_PATH\"]\nfile_list_txt = dali_extra_path + \"/db/video/test_label.mp4 0 0 100\\n\"\nfile_list_txt += dali_extra_path + \"/db/video/test_label.mp4 1 100 200\\n\"\nfile_list_txt += dali_extra_path + \"/db/video/test_label.mp4 2 200 250\\n\"\ntf = tempfile.NamedTemporaryFile()\ntf.write(str.encode(file_list_txt))\ntf.flush()\n```\n\n----------------------------------------\n\nTITLE: Rearrange Tensor Dimensions - DALI (Python)\nDESCRIPTION: This code reorders the dimensions of an input tensor using `fn.reshape` with the `src_dims` argument in DALI. It swaps the first and second dimensions of the input tensor, effectively transposing it. The input is sourced from an external source with an arbitrary shape.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reinterpret.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(device_id=0, num_threads=4, batch_size=3)\ndef example_reorder(input_data):\n    np.random.seed(4321)\n    inp = fn.external_source(input_data, batch=False, dtype=types.INT32)\n    return inp, fn.reshape(inp, src_dims=[1, 0])\n\n\npipe_reorder = example_reorder(\n    lambda: np.random.randint(0, 10, size=rand_shape(2, 1, 7), dtype=np.int32)\n)\npipe_reorder.build()\nshow_result(pipe_reorder.run())\n```\n\n----------------------------------------\n\nTITLE: Benchmark EfficientNet with DALI Proxy and TrivialAugment\nDESCRIPTION: This command benchmarks EfficientNet training using DALI Proxy for data loading with TrivialAugment enabled. The `data-backend` flag is set to `dali_proxy` and the `automatic-augmentation` flag is set to `trivialaugment`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n# DALI proxy with TrivialAugment\npython multiproc.py --nproc_per_node 8 ./main.py --amp --static-loss-scale 128 \\\n                    --batch-size 128 --epochs 4 --no-checkpoints --training-only \\\n                    --data-backend dali_proxy --automatic-augmentation trivialaugment \\\n                    --workspace $RESULT_WORKSPACE \\\n                    --report-file bench_report_dali_proxy_ta.json $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Setting Current Pipeline with Decorator (Python)\nDESCRIPTION: This example demonstrates an alternative way to set the current pipeline using the `@dali.pipeline_def` decorator. The function defining the pipeline is executed within the scope of the newly created pipeline. An external source operator is used to generate outputs.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/pipeline.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dali.pipeline_def(batch_size=N, num_threads=3, device_id=0)\ndef my_pipe(my_source):\n    return dali.fn.external_source(my_source, num_outputs=2)\n\npipe = my_pipe(my_source)\n```\n\n----------------------------------------\n\nTITLE: Import DALI and Utils Python\nDESCRIPTION: Imports necessary DALI modules and other utilities like matplotlib and numpy. Defines the batch size and the path to the audio files.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/audio_processing/audio_decoder.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nbatch_size = 1\naudio_files = \"../data/audio\"\n```\n\n----------------------------------------\n\nTITLE: Examine the Output with Scalar Broadcasting\nDESCRIPTION: Defines a function to print the results of the scalar broadcasting pipeline, displaying the original tensors, the scalars, and the results of the element-wise addition. The output is formatted for readability.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef examine_output(pipe_out):\n    t = pipe_out[0].as_array()\n    scalar = pipe_out[1].as_array()\n    result = pipe_out[2].as_array()\n    print(\"{}\\n+\\n{}\\n=\\n{}\".format(t, scalar, result))\n\n\nexamine_output(out)\n```\n\n----------------------------------------\n\nTITLE: Import Libraries\nDESCRIPTION: This snippet imports necessary libraries including `numpy`, `nvidia.dali.pipeline`, `nvidia.dali.types`, `nvidia.dali.fn`, and `torch`. It also defines the `batch_size` and `epochs` variables.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-external_input.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport types\nimport collections\nimport numpy as np\nfrom random import shuffle\nfrom nvidia.dali.pipeline import Pipeline\nimport nvidia.dali.types as types\nimport nvidia.dali.fn as fn\nimport torch\n\nbatch_size = 3\nepochs = 3\n```\n\n----------------------------------------\n\nTITLE: Build and Run DALI Color Space Conversion Pipeline (Python)\nDESCRIPTION: This snippet builds and runs the DALI pipeline defined in the previous snippet. It sets the batch size, number of threads, and device ID. The `pipe.run()` method executes the pipeline and returns the results.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/image_processing/color_space_conversion.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbatch_size = 32\n\npipe = conversion_pipeline(batch_size=batch_size, num_threads=2, device_id=0)\npipe.build()\npipe_out = pipe.run()\n```\n\n----------------------------------------\n\nTITLE: Print Stacked Tensor (Axis Before 2)\nDESCRIPTION: This snippet prints the stacked tensor with the new axis inserted before axis 2 and its shape. It retrieves the output from the DALI pipeline (`pipe_stack`) and prints the first element of the output tensor, along with its shape, using `o[2].at(0)`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/tensor_join.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Stacking - new axis before 2:\")\nprint(o[2].at(0))\nprint(\"Shape: \", o[2].at(0).shape)\n```\n\n----------------------------------------\n\nTITLE: Saving a DALI Pipeline Checkpoint (Python)\nDESCRIPTION: This snippet shows how to save a DALI pipeline's checkpoint using the `Pipeline.checkpoint()` method.  The checkpoint can be saved directly to a file by passing the filename as an argument or returned as a serialized string for later saving.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/advanced_topics_checkpointing.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfor _ in range(iters):\n    output = p.run()\n\n# Write the checkpoint to file:\ncheckpoint = p.checkpoint()\nopen('checkpoint_file.cpt', 'wb')\n\n# Or simply:\ncheckpoint = p.checkpoint('checkpoint_file.cpt')\n```\n\n----------------------------------------\n\nTITLE: Defining TFRecord Reader Pipeline\nDESCRIPTION: This snippet defines a DALI pipeline `tfrecord_reader_pipeline` that uses the `fn.readers.tfrecord` operator to read data from a TFRecord file. It specifies the path to the TFRecord file and its index file, defines the features to be extracted (image encoded as a string and class label as an integer), enables random shuffling, and sets the shard ID and number of shards for distributed data loading. It then calls the `common_pipeline` function to process the loaded images and labels.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-various-readers.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali.tfrecord as tfrec\n\n\n@pipeline_def\ndef tfrecord_reader_pipeline(num_gpus):\n    inputs = fn.readers.tfrecord(\n        path=tfrecord,\n        index_path=tfrecord_idx,\n        features={\n            \"image/encoded\": tfrec.FixedLenFeature((), tfrec.string, \"\"),\n            \"image/class/label\": tfrec.FixedLenFeature([1], tfrec.int64, -1),\n        },\n        random_shuffle=True,\n        shard_id=Pipeline.current().device_id,\n        num_shards=num_gpus,\n        name=\"Reader\",\n    )\n\n    return common_pipeline(inputs[\"image/encoded\"], inputs[\"image/class/label\"])\n```\n\n----------------------------------------\n\nTITLE: Define GPU pipeline for brightness and contrast adjustment\nDESCRIPTION: This code defines a DALI pipeline that reads image files from a specified directory, decodes them on a mixed device (GPU), and applies brightness and contrast adjustments using the `fn.brightness_contrast` operator.  This demonstrates processing images on the GPU.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/image_processing/brightness_contrast_example.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def()\ndef bc_gpu_pipeline():\n    files, labels = fn.readers.file(file_root=image_filename)\n    images = fn.decoders.image(files, device=\"mixed\")\n    converted = fn.brightness_contrast(\n        # increase contrast  # invert...\n        images,\n        contrast=1.5,\n        brightness_shift=1,\n        brightness=-1,\n    )  # ...colors\n    return images, converted\n```\n\n----------------------------------------\n\nTITLE: Defining MXNet Reader Pipeline\nDESCRIPTION: This snippet defines a DALI pipeline `mxnet_reader_pipeline` that uses the `fn.readers.mxnet` operator to read data from an MXNet RecordIO database. It specifies the path to the database and index files, enables random shuffling, and sets the shard ID and number of shards for distributed data loading. It then calls the `common_pipeline` function to process the loaded images and labels.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-various-readers.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef mxnet_reader_pipeline(num_gpus):\n    jpegs, labels = fn.readers.mxnet(\n        path=[db_folder + \"train.rec\"],\n        index_path=[db_folder + \"train.idx\"],\n        random_shuffle=True,\n        shard_id=Pipeline.current().device_id,\n        num_shards=num_gpus,\n        name=\"Reader\",\n    )\n\n    return common_pipeline(jpegs, labels)\n```\n\n----------------------------------------\n\nTITLE: Installing DALI (CUDA 11.0) via pip\nDESCRIPTION: Installs the latest official release of NVIDIA DALI built for CUDA 11.0 using pip. This command fetches the package from NVIDIA's PyPI index and upgrades if an older version is present. Requires pip and a compatible Python environment.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --extra-index-url https://pypi.nvidia.com --upgrade nvidia-dali-cuda110\n```\n\n----------------------------------------\n\nTITLE: Print Stacked Tensor (Axis Before 1)\nDESCRIPTION: This snippet prints the stacked tensor with the new axis inserted before axis 1 and its shape. It retrieves the output from the DALI pipeline (`pipe_stack`) and prints the first element of the output tensor, along with its shape, using `o[1].at(0)`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/tensor_join.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Stacking - new axis before 1:\")\nprint(o[1].at(0))\nprint(\"Shape: \", o[1].at(0).shape)\n```\n\n----------------------------------------\n\nTITLE: Set DALI Affinity Mask (Bash)\nDESCRIPTION: This example demonstrates how to set the DALI CPU thread affinity using the DALI_AFFINITY_MASK environment variable. It sets thread 0 to CPU 3, thread 1 to CPU 5, thread 2 to CPU 6, thread 3 to CPU 10, and thread 4 to the CPU ID that is returned by nvmlDeviceGetCpuAffinity, assuming DALI uses num_threads=5.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/advanced_topics_performance_tuning.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# assuming that DALI uses num_threads=5\nDALI_AFFINITY_MASK=\"3,5,6,10\"\n```\n\n----------------------------------------\n\nTITLE: Examine the Output of the DALI Pipeline\nDESCRIPTION: Defines a function to display the results of the DALI pipeline, printing the original tensors and the results of the element-wise addition, multiplication, and floor division.  It uses the `as_array()` method to convert DALI tensors to NumPy arrays for printing.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef examine_output(pipe_out):\n    l = pipe_out[0].as_array()\n    r = pipe_out[1].as_array()\n    sum_out = pipe_out[2].as_array()\n    mul_out = pipe_out[3].as_array()\n    div_out = pipe_out[4].as_array()\n    print(\"{}\\n+\\n{}\\n=\\n{}\\n\\n\".format(l, r, sum_out))\n    print(\"{}\\n*\\n{}\\n=\\n{}\\n\\n\".format(l, r, mul_out))\n    print(\"{}\\n//\\n{}\\n=\\n{}\\n\\n\".format(l, r, div_out))\n\n\nexamine_output(out)\n```\n\n----------------------------------------\n\nTITLE: Remove and Add Dimensions with Reshape - DALI (Python)\nDESCRIPTION: This code demonstrates how to remove and add dimensions to a tensor in DALI using the `fn.reshape` operation. It removes the first dimension (channel) and adds a new trailing dimension, effectively reinterpreting the data from CHW to HWC layout. The `layout` argument specifies the output layout.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reinterpret.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(device_id=0, num_threads=4, batch_size=3)\ndef example_remove_add(input_data):\n    np.random.seed(4321)\n    inp = fn.external_source(\n        input_data, batch=False, layout=\"CHW\", dtype=types.INT32\n    )\n    return inp, fn.reshape(\n        inp,\n        src_dims=[1, 2, -1],\n        layout=\"HWC\",  # select HW and add a new one at the end\n    )  # specify the layout string\n\n\npipe_remove_add = example_remove_add(\n    lambda: np.random.randint(0, 10, [1, 4, 3], dtype=np.int32)\n)\npipe_remove_add.build()\nshow_result(pipe_remove_add.run())\n```\n\n----------------------------------------\n\nTITLE: Indexing with run-time values in DALI\nDESCRIPTION: This snippet shows how to use a result of other computations (run-time defined index) to access tensor elements.  It extracts a random byte from raw files using a random index. The index must be a result of a CPU operator.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/indexing.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nraw_files = fn.readers.file(...)\nlength = raw_files.shape()[0]\n\n# calculate a random index from 0 to file_length-1\nrandom_01 = fn.random.uniform(range=(0, 1))  # random numbers in range [0..1)\nindex = fn.floor(random_01 * length)  # calculate indices from [0..length)\n# cast the index to integer - required for indexing\nindex = fn.cast(index, dtype=dali.types.INT64)\n\n# extract a random byte\nrandom_byte = raw_files[index]\n```\n\n----------------------------------------\n\nTITLE: Creating DALI sharded iterator for training\nDESCRIPTION: This code creates an instance of the sharded DALI iterator for training data, using the `mnist_sharded_iterator` function. It passes the data path, training flag, and batch size as arguments. It prints the number of batches contained in the training iterator to verify data distribution across devices.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/flax-basic_example.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Creating training iterator\")\ntraining_iterator = mnist_sharded_iterator(\n    data_path=training_data_path, is_training=True, batch_size=batch_size\n)\n\nprint(f\"Number of batches in training iterator = {len(training_iterator)}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Parameters for the Pipeline\nDESCRIPTION: This snippet defines key parameters for the video processing pipeline, such as the batch size (BATCH_SIZE), count (COUNT), number of iterations (ITER), and a boolean flag (frame_num_based_labels) to indicate whether labels are based on frame numbers.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/video/video_file_list_outputs.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nBATCH_SIZE = 4\nCOUNT = 1\nITER = 10\nframe_num_based_labels = True\n```\n\n----------------------------------------\n\nTITLE: Defining Caffe Reader Pipeline\nDESCRIPTION: This snippet defines a DALI pipeline `caffe_reader_pipeline` that uses the `fn.readers.caffe` operator to read data from a Caffe LMDB database. It specifies the path to the LMDB folder, enables random shuffling, and sets the shard ID and number of shards for distributed data loading. It then calls the `common_pipeline` function to process the loaded images and labels.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-various-readers.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef caffe_reader_pipeline(num_gpus):\n    jpegs, labels = fn.readers.caffe(\n        path=lmdb_folder,\n        random_shuffle=True,\n        shard_id=Pipeline.current().device_id,\n        num_shards=num_gpus,\n        name=\"Reader\",\n    )\n\n    return common_pipeline(jpegs, labels)\n```\n\n----------------------------------------\n\nTITLE: Wrapping DALI Iterator for Pax Input\nDESCRIPTION: This code defines a `MnistDaliInput` class that wraps the DALI iterator for use as a data source in Paxml. It initializes the DALI iterator using the `DALIGenericIterator` from `nvidia.dali.plugin.jax` and overrides the `get_next` and `reset` methods to provide data to the Paxml model. It uses the `training_data_path` and `validation_data_path` defined earlier to configure the iterator based on whether it's training or validation.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/pax-basic_example.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom praxis import base_input\nfrom nvidia.dali.plugin import jax as dax\n\n\nclass MnistDaliInput(base_input.BaseInput):\n    def __post_init__(self):\n        super().__post_init__()\n\n        data_path = (\n            training_data_path if self.is_training else validation_data_path\n        )\n\n        training_pipeline = mnist_iterator(\n            data_path=data_path,\n            random_shuffle=self.is_training,\n            batch_size=self.batch_size,\n        )\n        self._iterator = dax.DALIGenericIterator(\n            training_pipeline,\n            output_map=[\"inputs\", \"labels\"],\n            reader_name=\"mnist_caffe2_reader\",\n            auto_reset=True,\n        )\n\n    def get_next(self):\n        try:\n            return next(self._iterator)\n        except StopIteration:\n            self._iterator.reset()\n            return next(self._iterator)\n\n    def reset(self) -> None:\n        super().reset()\n        self._iterator = self._iterator.reset()\n```\n\n----------------------------------------\n\nTITLE: Helper Function for Data Generation (NumPy)\nDESCRIPTION: This function creates two NumPy arrays of specified types with predefined magic values. It takes two NumPy types as input (`left_type` and `right_type`) and returns a tuple containing two lists, each containing a NumPy array of the specified type.  The arrays are initialized with `left_magic_values` and `right_magic_values` respectively. This is to simulate data for the pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nleft_magic_values = [42, 8]\nright_magic_values = [9, 2]\n\n\ndef get_data(left_type, right_type):\n    return ([left_type(left_magic_values)], [right_type(right_magic_values)])\n\nbatch_size = 1\n```\n\n----------------------------------------\n\nTITLE: Brightness Augmentation Definition\nDESCRIPTION: This code snippet defines the `brightness` augmentation using the `@augmentation` decorator. It adjusts the brightness of the image with `nvidia.dali.fn.brightness`. The magnitude is mapped to a [0, 2] parameter range. It takes the data and a parameter which control the brightness adjustment as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0, 0.9), randomly_negate=True, ...)\ndef brightness(data, parameter)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model during Training\nDESCRIPTION: These flags enable evaluation of the model every second epoch during training. It specifies the validation data path, annotations file, evaluation frequency, and evaluation steps.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/yolov4/readme.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n--eval_file_root /coco/val2017 \\\n--eval_annotations /coco/annotations/instances_val2017.json \\\n--eval_frequency 2 --eval_steps 500\n```\n\n----------------------------------------\n\nTITLE: Verify DALI Audio Decoder Python\nDESCRIPTION: Verifies the DALI audio decoder by comparing its output with the output from `simpleaudio`. It loads an audio file using `simpleaudio`, extracts the audio data, and then compares it with the audio data decoded by DALI. Finally it plots both audio signals side by side.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/audio_processing/audio_decoder.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport simpleaudio as sa\n\nwav = sa.WaveObject.from_wave_file(\"../data/audio/wav/three.wav\")\nthree_audio = np.frombuffer(wav.audio_data, dtype=np.int16)\n\nprint(\"src: simpleaudio\")\nprint(\"shape: \", three_audio.shape)\nprint(\"data: \", three_audio)\nprint(\"\\n\")\nprint(\"src: DALI\")\nprint(\"shape: \", audio_data.shape)\nprint(\"data: \", audio_data)\nprint(\n    \"\\nAre the arrays equal?\",\n    \"YES\" if np.all(audio_data == three_audio) else \"NO\",\n)\n\nfig, ax = plt.subplots(1, 2)\nax[0].plot(three_audio)\nax[1].plot(audio_data)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Testing Pipelines with Tensorflow Session\nDESCRIPTION: This snippet tests the defined DALI pipelines by creating a Tensorflow session and running the pipelines for a specified number of iterations.  It fetches data using the `get_batch_test_dali` function and checks the labels to ensure they are integers and within the expected range for each dataset. It validates that the integration with Tensorflow is working correctly.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-various-readers.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\npipe_types = [\n    [mxnet_reader_pipeline, tf.float32, (0, 999)],\n    [caffe_reader_pipeline, tf.int32, (0, 999)],\n    [file_reader_pipeline, tf.int32, (0, 1)],\n    [tfrecord_reader_pipeline, tf.int64, (1, 1000)],\n]\n\nfor pipe_name in pipe_types:\n    print(\"RUN: \" + pipe_name[0].__name__)\n    test_batch = get_batch_test_dali(BATCH_SIZE, pipe_name)\n    x = placeholder(\n        tf.float32, shape=[BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3], name=\"x\"\n    )\n    gpu_options = GPUOptions(per_process_gpu_memory_fraction=0.8)\n    config = ConfigProto(gpu_options=gpu_options)\n\n    with Session(config=config) as sess:\n        for i in range(ITERATIONS):\n            imgs, labels = sess.run(test_batch)\n            # Testing correctness of labels\n            for label in labels:\n                ## labels need to be integers\n                assert np.equal(np.mod(label, 1), 0).all()\n                ## labels need to be in range pipe_name[2]\n                assert (label >= pipe_name[2][0]).all()\n                assert (label <= pipe_name[2][1]).all()\n    print(\"OK : \" + pipe_name[0].__name__)\n```\n\n----------------------------------------\n\nTITLE: Set metaparameters for DALI pipeline\nDESCRIPTION: This snippet sets the metaparameters such as batch size, sequence length, and the path to the video file used for calculating optical flow. The video file path is constructed using the environment variable DALI_EXTRA_PATH.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/optical_flow_example.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbatch_size = 1\nsequence_length = 10\ndali_extra_path = os.environ[\"DALI_EXTRA_PATH\"]\nvideo_filename = (\n    dali_extra_path + \"/db/optical_flow/sintel_trailer/sintel_trailer_short.mp4\"\n)\n```\n\n----------------------------------------\n\nTITLE: Import DALI and NumPy for Pipeline\nDESCRIPTION: Imports necessary modules from NumPy and DALI to create and define a DALI pipeline that can use external data sources.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom nvidia.dali.pipeline import pipeline_def\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\nfrom nvidia.dali.types import Constant\n```\n\n----------------------------------------\n\nTITLE: Contrast Augmentation Definition\nDESCRIPTION: This code snippet defines the `contrast` augmentation using the `@augmentation` decorator. It adjusts the contrast of the image using a channel-weighted mean as a contrast center. The magnitude is mapped to a [0, 2] parameter range. It takes the data and a parameter which control the contrast adjustment as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0, 0.9), randomly_negate=True, ...)\ndef contrast(data, parameter)\n```\n\n----------------------------------------\n\nTITLE: Reshape Tensor with Fixed Shape - DALI (Python)\nDESCRIPTION: This code defines a DALI pipeline that reshapes an input tensor to a fixed shape of [5, 5] and sets the layout to 'HW'. It uses `fn.reshape` to change the tensor's shape without modifying the underlying data. The `input_data` is supplied via an external source.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reinterpret.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(device_id=0, num_threads=4, batch_size=3)\ndef example1(input_data):\n    np.random.seed(1234)\n    inp = fn.external_source(input_data, batch=False, dtype=types.INT32)\n    return inp, fn.reshape(inp, shape=[5, 5], layout=\"HW\")\n\n\npipe1 = example1(lambda: np.random.randint(0, 10, size=[25], dtype=np.int32))\npipe1.build()\nshow_result(pipe1.run())\n```\n\n----------------------------------------\n\nTITLE: Defining Arithmetic Constant Pipeline in DALI\nDESCRIPTION: This Python function defines a DALI pipeline that performs an arithmetic operation between a tensor and a constant. It uses `fn.external_source` to obtain tensor data and applies the provided operation. The pipeline outputs the original tensor and the result of the operation. It requires `batch_size`, `tensor_data_type`, `dali_type` and a function representing the arithmetic operation.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef arithmetic_constant_pipeline(operation, tensor_data_type):\n    pipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\n    with pipe:\n        t = fn.external_source(\n            source=lambda: get_data(tensor_data_type, tensor_data_type)[0],\n            dtype=dali_type(tensor_data_type),\n        )\n        pipe.set_outputs(t, operation(t))\n\n    return pipe\n```\n\n----------------------------------------\n\nTITLE: Installing DALI (CUDA 12.0) via pip - shorthand\nDESCRIPTION: Installs the latest official release of NVIDIA DALI built for CUDA 12.0 using pip. This is a shorthand command that assumes the package is available in the default PyPI index or a configured custom index. Requires pip and a compatible Python environment.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install nvidia-dali-cuda120\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Building File and WebDataset Pipelines (Python)\nDESCRIPTION: This snippet instantiates both the `webdataset_pipeline` and `file_pipeline` with their respective data paths and then builds both pipelines using the `build()` method, preparing them for execution.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwebdataset_pipeline_instance = webdataset_pipeline(tar_dataset_paths)\nwebdataset_pipeline_instance.build()\nfile_pipeline_instance = file_pipeline(folder_dataset_files)\nfile_pipeline_instance.build()\n```\n\n----------------------------------------\n\nTITLE: Rotate Augmentation Definition\nDESCRIPTION: This code snippet defines the `rotate` augmentation using the `@augmentation` decorator. It rotates the image using `nvidia.dali.fn.rotate`. It takes the data, angle, fill_value, interpolation type, and rotate_keep_size as parameters.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0, 30), randomly_negate=True)\ndef rotate(data, angle, fill_value=128, interp_type=None, rotate_keep_size=True)\n```\n\n----------------------------------------\n\nTITLE: Import Libraries for PyTorch Lightning MNIST Example\nDESCRIPTION: This code snippet imports necessary libraries for creating a simple MNIST classification network using PyTorch Lightning. It includes PyTorch modules, PyTorch Lightning components, and torchvision for dataset handling.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-lightning.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.nn import functional as F\nfrom torch import nn\nfrom pytorch_lightning import Trainer, LightningModule\nfrom torch.optim import Adam\nfrom torchvision.datasets import MNIST\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nimport os\n\nBATCH_SIZE = 64\n\n# workaround for https://github.com/pytorch/vision/issues/1938 - error 403 when\n# downloading mnist dataset\nimport urllib\n\nopener = urllib.request.build_opener()\nopener.addheaders = [(\"User-agent\", \"Mozilla/5.0\")]\nurllib.request.install_opener(opener)\n```\n\n----------------------------------------\n\nTITLE: Running a Custom TensorFlow Training Loop with DALI\nDESCRIPTION: This code shows how to run a custom TensorFlow training loop using a DALI dataset and `tf.Session`. It initializes global variables and the iterator, then iterates through epochs and iterations, running the training step and printing the accuracy. Finally, it calculates and prints the final accuracy.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwith tf_v1.Session() as sess:\n    sess.run(tf_v1.global_variables_initializer())\n    sess.run(iterator.initializer)\n\n    for i in range(EPOCHS * ITERATIONS_PER_EPOCH):\n        sess.run(train_step)\n        if i % ITERATIONS_PER_EPOCH == 0:\n            train_accuracy = sess.run(accuracy)\n            print(\"Step %d, accuracy: %g\" % (i, train_accuracy))\n\n    final_accuracy = 0\n    for _ in range(ITERATIONS_PER_EPOCH):\n        final_accuracy = final_accuracy + sess.run(accuracy)\n    final_accuracy = final_accuracy / ITERATIONS_PER_EPOCH\n\n    print(\"Final accuracy: \", final_accuracy)\n```\n\n----------------------------------------\n\nTITLE: Defining File Reader Pipeline\nDESCRIPTION: This snippet defines a DALI pipeline `file_reader_pipeline` that uses the `fn.readers.file` operator to read data from a directory containing image files. It specifies the path to the image directory, enables random shuffling, and sets the shard ID and number of shards for distributed data loading. It then calls the `common_pipeline` function to process the loaded images and labels.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-various-readers.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef file_reader_pipeline(num_gpus):\n    jpegs, labels = fn.readers.file(\n        file_root=image_dir,\n        random_shuffle=True,\n        shard_id=Pipeline.current().device_id,\n        num_shards=num_gpus,\n        name=\"Reader\",\n    )\n\n    return common_pipeline(jpegs, labels)\n```\n\n----------------------------------------\n\nTITLE: Run EfficientNet Training on a Single GPU (FP32)\nDESCRIPTION: This command runs EfficientNet training on a single GPU using FP32 precision. It requires the `$PATH_TO_IMAGENET` environment variable to be set to the location of the ImageNet dataset.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ./main.py --batch-size 64 $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Train.py Usage Arguments\nDESCRIPTION: This shows the command-line arguments for the train.py script.  The arguments configure training parameters, data paths, pipeline type (TensorFlow or DALI), and model configuration.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/efficientdet/README.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nusage: train.py [-h] [--initial_epoch INITIAL_EPOCH] [--epochs EPOCHS]\n                  --input_type {tfrecord,coco} [--images_path IMAGES_PATH]\n                  [--annotations_path ANNOTATIONS_PATH]\n                  [--train_file_pattern TRAIN_FILE_PATTERN]\n                  [--batch_size BATCH_SIZE] [--train_steps TRAIN_STEPS]\n                  [--eval_file_pattern EVAL_FILE_PATTERN]\n                  [--eval_steps EVAL_STEPS] [--eval_freq EVAL_FREQ]\n                  [--eval_during_training] [--eval_after_training]\n                  --pipeline_type {synthetic,tensorflow,dali_cpu,dali_gpu}\n                  [--multi_gpu [MULTI_GPU [MULTI_GPU ...]]] [--seed SEED]\n                  [--hparams HPARAMS] [--model_name MODEL_NAME]\n                  [--output_filename OUTPUT_FILENAME]\n                  [--start_weights START_WEIGHTS] [--log_dir LOG_DIR]\n                  [--ckpt_dir CKPT_DIR]\n\n  optional arguments:\n    -h, --help            show this help message and exit\n    --initial_epoch INITIAL_EPOCH\n                          Epoch from which to start training.\n    --epochs EPOCHS       Epoch on which training should finish.\n    --input_type {tfrecord,coco}\n                          Input type.\n    --images_path IMAGES_PATH\n                          Path to COCO images.\n    --annotations_path ANNOTATIONS_PATH\n                          Path to COCO annotations.\n    --train_file_pattern TRAIN_FILE_PATTERN\n                          TFrecord files glob pattern for files with training data.\n    --batch_size BATCH_SIZE\n    --train_steps TRAIN_STEPS\n                          Number of steps (iterations) in each epoch.\n    --eval_file_pattern EVAL_FILE_PATTERN\n                          TFrecord files glob pattern for files with evaluation data,\n                          defaults to `train_file_pattern` if not given.\n    --eval_steps EVAL_STEPS\n                          Number of examples to evaluate during each evaluation.\n    --eval_freq EVAL_FREQ\n                          During training evaluation frequency.\n    --eval_during_training\n                          Whether to run evaluation every `eval_freq` epochs.\n    --eval_after_training\n                          Whether to run evaluation after finished training.\n    --pipeline_type {synthetic,tensorflow,dali_cpu,dali_gpu}\n                          Pipeline type used while loading and preprocessing data.\n                          One of: tensorflow  pipeline used in original\n                          EfficientDet implementation on\n                          https://github.com/google/automl/tree/master/efficientdet\n                          synthetic  like `tensorflow` pipeline type but repeats\n                          one batch endlessly dali_gpu  pipeline which uses\n                          Nvidia Data Loading Library (DALI) to run part of data\n                          preprocessing on GPUs to improve efficiency\n                          dali_cpu  like `dali_gpu` pipeline type but restricted\n                          to run only on CPU\n    --multi_gpu [MULTI_GPU [MULTI_GPU ...]]\n                          List of GPUs to use, if empty defaults to all visible GPUs.\n    --seed SEED\n    --hparams HPARAMS     String or filename with parameters.\n    --model_name MODEL_NAME\n    --output_filename OUTPUT_FILENAME\n                          Filename for final weights to save.\n    --start_weights START_WEIGHTS\n    --log_dir LOG_DIR     Directory for tensorboard logs.\n    --ckpt_dir CKPT_DIR   Directory for saving weights each step.\n```\n\n----------------------------------------\n\nTITLE: Run the Broadcasting Pipeline\nDESCRIPTION: Executes the pipeline defined in `pipeline_3` and prints the result of the broadcasting operation.  The output shows how the (1, 3) tensor is added to each row of the (2, 3) tensor.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipe = pipeline_3()\npipe.build()\n(out,) = pipe.run()\nprint(out)\n```\n\n----------------------------------------\n\nTITLE: Adjusting an Existing Augmentation in DALI\nDESCRIPTION: This code snippet shows how to create a new augmentation based on an existing one, with adjusted parameters. It uses the `augmentation` method of an `Augmentation` instance to modify the `mag_range` and `randomly_negate` properties.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nrotate_aug_60 = rotate_aug.augmentation(mag_range=(0, 60), randomly_negate=False)\n```\n\n----------------------------------------\n\nTITLE: Define color wheel generation function\nDESCRIPTION: This snippet defines a function `make_colorwheel` that generates a color wheel for visualizing optical flow. The color wheel is a standard representation used in optical flow visualization, where different colors correspond to different flow directions and magnitudes.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/optical_flow_example.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef make_colorwheel():\n    \"\"\"\n    Generates a color wheel for optical flow visualization as presented in:\n        Baker et al. \"A Database and Evaluation Methodology for Optical Flow\"\n        (ICCV, 2007)\n        URL: http://vision.middlebury.edu/flow/flowEval-iccv07.pdf\n    According to the C++ source code of Daniel Scharstein\n    According to the Matlab source code of Deqing Sun\n    \"\"\"\n\n    RY = 15\n    YG = 6\n    GC = 4\n    CB = 11\n    BM = 13\n    MR = 6\n\n    ncols = RY + YG + GC + CB + BM + MR\n    colorwheel = np.zeros((ncols, 3))\n    col = 0\n\n    # RY\n    colorwheel[0:RY, 0] = 255\n    colorwheel[0:RY, 1] = np.floor(255 * np.arange(0, RY) / RY)\n    col = col + RY\n    # YG\n    colorwheel[col : col + YG, 0] = 255 - np.floor(255 * np.arange(0, YG) / YG)\n    colorwheel[col : col + YG, 1] = 255\n    col = col + YG\n    # GC\n    colorwheel[col : col + GC, 1] = 255\n    colorwheel[col : col + GC, 2] = np.floor(255 * np.arange(0, GC) / GC)\n    col = col + GC\n    # CB\n    colorwheel[col : col + CB, 1] = 255 - np.floor(255 * np.arange(CB) / CB)\n    colorwheel[col : col + CB, 2] = 255\n    col = col + CB\n    # BM\n    colorwheel[col : col + BM, 2] = 255\n    colorwheel[col : col + BM, 0] = np.floor(255 * np.arange(0, BM) / BM)\n    col = col + BM\n    # MR\n    colorwheel[col : col + MR, 2] = 255 - np.floor(255 * np.arange(MR) / MR)\n    colorwheel[col : col + MR, 0] = 255\n    return colorwheel\n```\n\n----------------------------------------\n\nTITLE: Print Stacked Tensor (New Innermost Axis)\nDESCRIPTION: This snippet prints the stacked tensor with the new axis inserted as the innermost axis (axis=3) and its shape. It retrieves the output from the DALI pipeline (`pipe_stack`) and prints the first element of the output tensor, along with its shape, using `o[3].at(0)`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/tensor_join.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Stacking - new innermost axis:\")\nprint(o[3].at(0))\nprint(\"Shape: \", o[3].at(0).shape)\n```\n\n----------------------------------------\n\nTITLE: Setting data paths for MNIST dataset with DALI\nDESCRIPTION: This code snippet retrieves the paths to the training and validation datasets for the MNIST dataset stored in the Caffe2 format. It uses the DALI_EXTRA_PATH environment variable to locate the dataset directories.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/flax-basic_example.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\ntraining_data_path = os.path.join(\n    os.environ[\"DALI_EXTRA_PATH\"], \"db/MNIST/training/\"\n)\nvalidation_data_path = os.path.join(\n    os.environ[\"DALI_EXTRA_PATH\"], \"db/MNIST/testing/\"\n)\n```\n\n----------------------------------------\n\nTITLE: Installing DALI TensorFlow Plugin (CUDA 11.0) via pip\nDESCRIPTION: Installs the latest official release of the NVIDIA DALI TensorFlow plugin built for CUDA 11.0 using pip. This command fetches the package from NVIDIA's PyPI index and upgrades if an older version is present. It requires `tensorflow-gpu` to be installed beforehand and `nvidia-dali-cuda110`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install --extra-index-url https://pypi.nvidia.com --upgrade nvidia-dali-tf-plugin-cuda110\n```\n\n----------------------------------------\n\nTITLE: Combining Functional and Object APIs in DALI\nDESCRIPTION: This code snippet demonstrates how to combine the functional and legacy operator object APIs in a single DALI pipeline. It initializes operator objects using the `dali.ops` module and uses functions from the `dali.fn` module within the same pipeline definition.  This example showcases the flexibility of DALI in using both APIs together.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/supported_ops_legacy.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipe = dali.pipeline.Pipeline(batch_size = 3, num_threads = 2, device_id = 0)\nreader = dali.ops.readers.File(file_root = \".\")\nresize = dali.ops.Resize(device = \"gpu\", resize_x = 300, resize_y = 300)\n\nwith pipe:\n    files, labels = reader()\n    images = dali.fn.decoders.image(files, device = \"mixed\")\n    images = dali.fn.rotate(images, angle = dali.fn.random.uniform(range=(-45,45)))\n    images = resize(images)\n    pipe.set_outputs(images, labels)\n\noutputs = pipe.run()\n```\n\n----------------------------------------\n\nTITLE: Download and Preprocess Videos (Bash)\nDESCRIPTION: This bash script downloads videos from YouTube and preprocesses them using ffmpeg. It downloads videos, resizes them to 300p, clips them to 10 seconds, and saves them in the 'demo' directory.  Requires youtube-dl and ffmpeg to be installed.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/paddle/tsm/paddle-tsm.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir demo\nyoutube-dl --quiet --no-warnings -f mp4 -o demo/tmp.mp4 \\\n              'https://www.youtube.com/watch?v=iU3ByohkPaM'\nffmpeg -y -i demo/tmp.mp4 -filter:v scale=-1:300 -ss 0 -t 10 -c:a copy demo/1.mp4\nyoutube-dl --quiet --no-warnings -f mp4 -o demo/tmp.mp4 \\\n              'https://www.youtube.com/watch?v=C0J6EQYYLzI'\nffmpeg -y -i demo/tmp.mp4 -filter:v scale=-1:300 -ss 0 -t 10 -c:a copy demo/2.mp4\nrm demo/tmp.mp4\n```\n\n----------------------------------------\n\nTITLE: Shear X Augmentation Definition\nDESCRIPTION: This code snippet defines the `shear_x` augmentation using the `@augmentation` decorator. It applies a shear transformation along the x-axis using `nvidia.dali.fn.transforms.shear` and `nvidia.dali.fn.warp_affine`.  It takes shear and optional fill_value and interp_type as input parameters.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0, 0.3), randomly_negate=True, ...)\ndef shear_x(data, shear, fill_value=128, interp_type=None)\n```\n\n----------------------------------------\n\nTITLE: Transcode Video Scenes\nDESCRIPTION: This script transcodes video scenes to a smaller keyframe interval and/or a lower resolution. The script requires specifying the main data directory and a desired resolution.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/video_superres/README.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ./tools/transcode_scenes.py --main_data <data_dir> --resolution <resolution>\n```\n\n----------------------------------------\n\nTITLE: Run the Pipeline with Constants\nDESCRIPTION: Instantiates, builds, and runs the `pipeline_2` which utilizes constant values in arithmetic operations. The output will reflect the use of these constants in conjunction with the input data.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipe = pipeline_2()\npipe.build()\nout = pipe.run()\n```\n\n----------------------------------------\n\nTITLE: MXNet reader pipeline (Python)\nDESCRIPTION: This snippet defines a DALI pipeline for reading data from MXNet RecordIO files. It uses `fn.readers.mxnet` to read the data, sharding it across multiple GPUs. The data is then passed to the `common_pipeline` function for image processing.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-various-readers.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef mxnet_reader_pipeline(num_gpus):\n    jpegs, labels = fn.readers.mxnet(\n        path=[db_folder + \"train.rec\"],\n        index_path=[db_folder + \"train.idx\"],\n        random_shuffle=True,\n        shard_id=Pipeline.current().device_id,\n        num_shards=num_gpus,\n        name=\"Reader\",\n    )\n\n    return common_pipeline(jpegs, labels)\n```\n\n----------------------------------------\n\nTITLE: Defining Pax Experiment Datasets\nDESCRIPTION: This code snippet shows how to define the datasets method of a Pax `Experiment` class to use the `MnistDaliInput` as a source of training data. It creates a `pax_fiddle.Config` object for `MnistDaliInput`, specifying the batch size and setting `is_training` to `True`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/pax-basic_example.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef datasets(self) -> list[pax_fiddle.Config[base_input.BaseInput]]:\n    return [\n        pax_fiddle.Config(\n            MnistDaliInput, batch_size=self.BATCH_SIZE, is_training=True\n        )\n    ]\n```\n\n----------------------------------------\n\nTITLE: Explicit DALI Server Start/Stop\nDESCRIPTION: This example demonstrates how to explicitly start and stop the DALI server thread instead of relying on the `with` statement.  It highlights the usage of `dali_server.start_thread()` and `dali_server.stop_thread()` to manage the server's lifecycle.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/plugins/pytorch_dali_proxy.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndali_server = dali_proxy.DALIServer(example_pipeline2(...))\ndataset = datasets.ImageFolder(\"/path/to/images\", transform=dali_server.proxy)\nloader = dali_proxy.DataLoader(dali_server, dataset, batch_size=64, num_workers=8, drop_last=True)\n\n# Optional, it will be started on first attempt to get data from the loader anyway\ndali_server.start_thread()\n\nfor data in loader:\n   ...\n\n# This is needed to make sure we have stopped the thread\ndali_server.stop_thread()\n```\n\n----------------------------------------\n\nTITLE: Building the Python bindings\nDESCRIPTION: This snippet includes the `python` subdirectory and sets up a `check-python` target for building the DALI Python bindings, contingent upon the `BUILD_PYTHON` option. The `PYTHONPATH` environment variable is also configured.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_PYTHON)\n  # Get all python srcs\n  add_subdirectory(python)\n\n  # prepare check-python target\n  add_custom_target(check-python)\n  add_dependencies(check check-python)\n\n  set(PYTHON_TARGET_PATH ${PROJECT_BINARY_DIR}/dali/python)\n  if($ENV{PYTHONPATH})\n    set(PYTHONPATH \"${PYTHON_TARGET_PATH}:$ENV{PYTHONPATH}\")\n  else()\n    set(PYTHONPATH \"${PYTHON_TARGET_PATH}\")\n  endif()\n\n  add_dependencies(check-python dali_python)\n  add_dependencies(check-python dali)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Clone FlowNet2-pytorch\nDESCRIPTION: This code snippet clones the FlowNet2-SD repository, checks out a specific commit, and then navigates back to the parent directory. This is required for using the pre-trained optical flow network FlowNetSD.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/video_superres/README.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/NVIDIA/flownet2-pytorch.git\npushd flownet2-pytorch\ngit checkout 6a0d9e70a5dcc37ef5577366a5163584fd7b4375\npopd\n```\n\n----------------------------------------\n\nTITLE: Model Validation\nDESCRIPTION: This snippet shows how to evaluate a trained model during validation. The `--evaluate` flag is used to indicate validation mode. It uses the checkpoint file to load the model state.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npython ./main.py --evaluate --epochs 1 --resume <path to checkpoint>\n                   -b <batch size> $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Run Inference Script (Python)\nDESCRIPTION: This command executes the `infer.py` script to perform inference on the preprocessed videos. It sets the top-k predictions to 1 and the frame stride to 30. It requires the `infer.py` script to be available in the current directory and the directory 'demo' containing the video files.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/paddle/tsm/paddle-tsm.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython infer.py -k 1 -s 30 demo\n```\n\n----------------------------------------\n\nTITLE: Importing DALI and other necessary libraries\nDESCRIPTION: This snippet imports the required libraries for using DALI, including `numpy` for array manipulation, `random` for shuffling data, and DALI modules for pipeline construction and data processing.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/external_input.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport types\nimport collections\nimport numpy as np\nfrom random import shuffle\nfrom nvidia.dali.pipeline import Pipeline\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\n\nbatch_size = 16\n```\n\n----------------------------------------\n\nTITLE: Defining File-Based Pipeline in DALI (Python)\nDESCRIPTION: This code defines a standard file-based DALI pipeline, using the `fn.readers.file` reader. It takes a list of files as input, reads images, and applies the `decode_augment` function for decoding and augmentation.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@dali.pipeline_def(batch_size=batch_size, num_threads=4, device_id=0)\ndef file_pipeline(files):\n    img, _ = fn.readers.file(files=files)\n    return decode_augment(img)\n```\n\n----------------------------------------\n\nTITLE: Building DALI with specific CUDA Version - Bash\nDESCRIPTION: This code snippet shows how to set a specific CUDA version for building DALI. By setting the `CUDA_VERSION` environment variable to `11.1` before executing the `build.sh` script, the resulting DALI build will be based on CUDA 11.1. The compiled Python wheel will then be placed inside the DALI_root/wheelhouse directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/compilation.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VERSION=11.1 ./build.sh\n```\n\n----------------------------------------\n\nTITLE: Split video into scenes\nDESCRIPTION: This script splits a raw video file into smaller scenes and removes the audio track. The script takes the path to the MP4 file and an output directory as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/video_superres/README.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython ./tools/split_scenes.py --raw_data <path_to_mp4_file> --out_data <data_dir>\n```\n\n----------------------------------------\n\nTITLE: Audio Decoder Pipeline Definition Python\nDESCRIPTION: Defines a DALI pipeline that reads audio files from disk using `fn.readers.file` and decodes them using `fn.decoders.audio`. The `dtype` parameter for `fn.decoders.audio` is explicitly set to `types.INT16`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/audio_processing/audio_decoder.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef audio_decoder_pipe():\n    encoded, _ = fn.readers.file(file_root=audio_files)\n    audio, sr = fn.decoders.audio(encoded, dtype=types.INT16)\n    return audio, sr\n```\n\n----------------------------------------\n\nTITLE: Creating TFRecord Index File\nDESCRIPTION: This snippet creates an index file for the TFRecord dataset if it doesn't already exist. It uses the `tfrecord2idx` script to generate the index file, which is used by the DALI TFRecord reader for efficient data access. It ensures that the `idx_files` directory exists before creating the index file.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-various-readers.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom subprocess import call\nimport os.path\n\nif not os.path.exists(\"idx_files\"):\n    os.mkdir(\"idx_files\")\n\nif not os.path.isfile(tfrecord_idx):\n    call([tfrecord2idx_script, tfrecord, tfrecord_idx])\n```\n\n----------------------------------------\n\nTITLE: Functional API Pipeline in DALI\nDESCRIPTION: This code snippet demonstrates how to define a DALI pipeline using the recommended functional API. It reads files and labels, decodes images, rotates and resizes them, and sets the outputs of the pipeline. The pipeline uses functions from the `dali.fn` module, such as `dali.fn.readers.file`, `dali.fn.decoders.image`, `dali.fn.rotate`, and `dali.fn.resize`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/supported_ops_legacy.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali as dali\n\npipe = dali.pipeline.Pipeline(batch_size = 3, num_threads = 2, device_id = 0)\nwith pipe:\n    files, labels = dali.fn.readers.file(file_root = \"./my_file_root\")\n    images = dali.fn.decoders.image(files, device = \"mixed\")\n    images = dali.fn.rotate(images, angle = dali.fn.random.uniform(range=(-45,45)))\n    images = dali.fn.resize(images, resize_x = 300, resize_y = 300)\n    pipe.set_outputs(images, labels)\n\noutputs = pipe.run()\n```\n\n----------------------------------------\n\nTITLE: Image Classification\nDESCRIPTION: This python script classifies a JPEG image using pretrained weights. The `--pretrained-from-file` argument specifies the path to the extracted weights, `--precision` specifies the precision (AMP or FP32), and `--image` specifies the path to the JPEG image.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\npython classify.py --pretrained-from-file <path to weights from previous step>\n                     --precision AMP|FP32 --image <path to JPEG image>\n```\n\n----------------------------------------\n\nTITLE: Numpy Reader with ROI Axes\nDESCRIPTION: This snippet demonstrates using `roi_axes` to specify the axes along which the region of interest is extracted. The code defines a pipeline (`pipe_roi3`) that takes `rel_roi_start`, `rel_roi_end`, and `roi_axes` as arguments, allowing for flexible ROI extraction along specific dimensions.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(batch_size=batch_size, num_threads=3, device_id=0)\ndef pipe_roi3(rel_roi_start, rel_roi_end, roi_axes):\n    data = fn.readers.numpy(\n        device=\"cpu\",\n        file_root=data_dir,\n        files=files,\n        rel_roi_start=rel_roi_start,\n        rel_roi_end=rel_roi_end,\n        roi_axes=roi_axes,\n    )\n    return data\n\n\np1 = pipe_roi3(rel_roi_start=[0.1], rel_roi_end=[0.4], roi_axes=(0,))\ndata_roi3_1 = run(p1)\nplot_batch(data_roi3_1)\n\np2 = pipe_roi3(rel_roi_start=[0.1], rel_roi_end=[0.4], roi_axes=(1,))\ndata_roi3_2 = run(p2)\nplot_batch(data_roi3_2)\n```\n\n----------------------------------------\n\nTITLE: Reshape Tensor with Wildcard - DALI (Python)\nDESCRIPTION: This code demonstrates reshaping a tensor using a wildcard dimension (-1) in DALI. The wildcard allows the number of rows to vary while maintaining a fixed number of columns (5). The shape is adjusted to keep the total number of elements constant, adapting to the input's size.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reinterpret.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(device_id=0, num_threads=4, batch_size=3)\ndef example2(input_data):\n    np.random.seed(12345)\n    inp = fn.external_source(input_data, batch=False, dtype=types.INT32)\n    return inp, fn.reshape(inp, shape=[-1, 5])\n\n\npipe2 = example2(\n    lambda: np.random.randint(\n        0, 10, size=[5 * np.random.randint(3, 10)], dtype=np.int32\n    )\n)\npipe2.build()\nshow_result(pipe2.run())\n```\n\n----------------------------------------\n\nTITLE: Run EfficientNet Training with DALI and TrivialAugment\nDESCRIPTION: This command runs EfficientNet training using DALI as the data backend and TrivialAugment for automatic augmentation. It also utilizes Automatic Mixed Precision (AMP).\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython ./main.py --amp --static-loss-scale 128 --batch-size 128 --data-backend dali --automatic-augmentation trivialaugment $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: MXNet Reader Pipeline Definition\nDESCRIPTION: This snippet defines a DALI pipeline that reads data from MXNet RecordIO files. It uses `fn.readers.mxnet` to load images and labels, sharding the dataset across multiple GPUs for distributed training. The `common_pipeline` function is then applied for image preprocessing.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-various-readers.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef mxnet_reader_pipeline(num_gpus):\n    jpegs, labels = fn.readers.mxnet(\n        path=[db_folder + \"train.rec\"],\n        index_path=[db_folder + \"train.idx\"],\n        random_shuffle=True,\n        shard_id=Pipeline.current().device_id,\n        num_shards=num_gpus,\n        name=\"Reader\",\n    )\n\n    return common_pipeline(jpegs, labels)\n```\n\n----------------------------------------\n\nTITLE: Extract Files from Tar Archives - Python\nDESCRIPTION: Extracts files from the specified tar archives into temporary directories. These extracted files are then used for comparing the file reader with the custom WebDataset loader. The paths to the extracted files are stored in the `folder_dataset_files` list.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfolder_dataset_root_dir = tempfile.TemporaryDirectory()\nfolder_dataset_dirs = [tempfile.TemporaryDirectory(dir=folder_dataset_root_dir.name)\n                     for dataset in tar_dataset_paths]\nfolder_dataset_tars = [tarfile.open(dataset) for dataset in tar_dataset_paths]\n\nfor folder_dataset_tar, folder_dataset_subdir in zip(folder_dataset_tars,\n                                                     folder_dataset_dirs):\n    folder_dataset_tar.extractall(path=folder_dataset_subdir.name)\n\nfolder_dataset_files = [\n    filepath\n    for folder_dataset_subdir in folder_dataset_dirs\n    for filepath in sorted(\n        glob.glob(os.path.join(folder_dataset_subdir.name, \"*.jpg\")),\n        key=lambda s: int(s[s.rfind('/') + 1:s.rfind(\".jpg\")])\n    )\n]\n```\n\n----------------------------------------\n\nTITLE: Translate X Augmentation Definition\nDESCRIPTION: This code snippet defines the `translate_x` augmentation using the `@augmentation` decorator. It applies a translation along the x-axis with a shape-relative offset using `nvidia.dali.fn.transforms.translation` and `nvidia.dali.fn.warp_affine`. It accepts the data, relative offset, shape of the image, fill_value, and interpolation type as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0., 1.), randomly_negate=True, ...)\ndef translate_x(data, rel_offset, shape, fill_value=128, interp_type=None)\n```\n\n----------------------------------------\n\nTITLE: Generating Python Stubs\nDESCRIPTION: Adds a custom target to generate Python stubs for the DALI video plugin, preloading the CUDA library for proper shared object loading.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(dali_${PLUGIN_NAME}_generate_stubs ALL\nDEPENDS dali_${PLUGIN_NAME}\nBYPRODUCTS ${CMAKE_CURRENT_BINARY_DIR}/fn/plugin/video/\nCOMMAND /bin/bash -c\n    \"LD_PRELOAD=\\\"${NEW_LD_PRELOAD}\\\" \\\n    ${PYTHON_EXECUTABLE} ${PROJECT_SOURCE_DIR}/generate_plugin_stubs.py \\\n    ${CMAKE_CURRENT_BINARY_DIR} ${PROJECT_BINARY_DIR}/libdali_${PLUGIN_NAME}.so\"\n)\n```\n\n----------------------------------------\n\nTITLE: Installing DALI with pip\nDESCRIPTION: These commands are used to install the latest DALI release for the latest CUDA version (12.x) using pip. The first command installs the package directly, while the second uses an extra index URL to ensure the latest version from NVIDIA's PyPI repository is installed and upgrades the package if necessary.\nSOURCE: https://github.com/nvidia/dali/blob/main/README.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install nvidia-dali-cuda120\n# or\npip install --extra-index-url https://pypi.nvidia.com  --upgrade nvidia-dali-cuda120\n```\n\n----------------------------------------\n\nTITLE: Integrating DALI with Custom TensorFlow Training Loops\nDESCRIPTION: This code demonstrates how to integrate a DALI dataset with a custom TensorFlow model and training loop using `tf.Session`.  It disables eager execution, resets the default graph, creates a DALI dataset on the GPU, creates an iterator, defines a simple neural network model, defines a loss function, creates a training step, and defines an accuracy metric.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntf.compat.v1.disable_eager_execution()\ntf_v1.reset_default_graph()\n\nwith tf.device(\"/gpu:0\"):\n    mnist_set = dali_tf.DALIDataset(\n        pipeline=mnist_pipeline(device=\"gpu\"),\n        batch_size=BATCH_SIZE,\n        output_shapes=shapes,\n        output_dtypes=dtypes,\n        device_id=0,\n    )\n\n    iterator = tf_v1.data.make_initializable_iterator(mnist_set)\n    images, labels = iterator.get_next()\n\n    labels = tf_v1.reshape(\n        tf_v1.one_hot(labels, NUM_CLASSES), [BATCH_SIZE, NUM_CLASSES]\n    )\n\n    with tf_v1.variable_scope(\"mnist_net\", reuse=False):\n        images = tf_v1.layers.flatten(images)\n        images = tf_v1.layers.dense(\n            images, HIDDEN_SIZE, activation=tf_v1.nn.relu\n        )\n        images = tf_v1.layers.dropout(images, rate=DROPOUT, training=True)\n        images = tf_v1.layers.dense(\n            images, NUM_CLASSES, activation=tf_v1.nn.softmax\n        )\n\n    logits_train = images\n    loss_op = tf_v1.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(\n            logits=logits_train, labels=labels\n        )\n    )\n    train_step = tf_v1.train.AdamOptimizer().minimize(loss_op)\n\n    correct_pred = tf_v1.equal(\n        tf_v1.argmax(logits_train, 1), tf_v1.argmax(labels, 1)\n    )\n    accuracy = tf_v1.reduce_mean(tf_v1.cast(correct_pred, tf_v1.float32))\n```\n\n----------------------------------------\n\nTITLE: Indexing from the end in DALI\nDESCRIPTION: This snippet demonstrates how to use negative indices to index a tensor starting from the end. The index of -1 denotes the last element.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/indexing.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nchannels = sizes[-1]   # channels go last\nwidths = sizes[-2]     # widths are the innermost dimension after channels\n```\n\n----------------------------------------\n\nTITLE: Compare the outputs of the DALI Pipelines\nDESCRIPTION: This code compares the outputs of the original and deserialized pipelines by calling the `check_difference` function on the CPU versions of the image tensors. This verifies that the deserialization process produced an identical pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/advanced/serialization.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncheck_difference(original_images.as_cpu(), serialized_images.as_cpu())\n```\n\n----------------------------------------\n\nTITLE: Decode and Augment Image - Python\nDESCRIPTION: Defines a data augmentation function `decode_augment` that decodes an image, applies jitter, and resizes it to 224x224. It uses DALI's `fn.decoders.image`, `fn.jitter`, and `fn.resize` operators.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef decode_augment(img, seed=0):\n    img = fn.decoders.image(img)\n    img = fn.jitter(img.gpu(), seed=seed)\n    img = fn.resize(img, size=(224, 224))\n    return img\n```\n\n----------------------------------------\n\nTITLE: Create Symbolic Links for ImageNet Data (Bash)\nDESCRIPTION: Creates symbolic links for the ImageNet training and validation datasets. This allows the training script to access the data without requiring the user to modify the script itself. The script assumes the ImageNet dataset is stored as raw JPEGs.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/resnet50/pytorch-resnet50.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nln -s /path/to/train/jpeg/ train\nln -s /path/to/validation/jpeg/ val\ntorchrun --nproc_per_node=NUM_GPUS main.py -a resnet50 --dali_cpu --b 128 \\\n            --loss-scale 128.0 --workers 4 --lr=0.4 --fp16-mode ./\n```\n\n----------------------------------------\n\nTITLE: Initializing Sharding for JAX Parallelization (Python)\nDESCRIPTION: This snippet initializes sharding for automatic parallelization in JAX. It creates a device mesh based on the number of available GPUs and defines a positional sharding strategy. This sharding will be used to distribute data and computation across the GPUs.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-multigpu_example.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom jax.sharding import PositionalSharding, Mesh\nfrom jax.experimental import mesh_utils\n\n\nmesh = mesh_utils.create_device_mesh((jax.device_count(), 1))\nsharding = PositionalSharding(mesh)\n\nprint(sharding)\n```\n\n----------------------------------------\n\nTITLE: Setting Data Loader Source Files (CMake)\nDESCRIPTION: This snippet sets the source files for the data loader operators. It includes various loader implementations such as filesystem, file label, COCO, sequence, and numpy loaders, as well as utility functions. These files are compiled to create the data loading functionality of DALI.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/loader/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_OPERATOR_SRCS ${DALI_OPERATOR_SRCS}\n  \"${CMAKE_CURRENT_SOURCE_DIR}/filesystem.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/discover_files.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/file_label_loader.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/coco_loader.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/loader.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/sequence_loader.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/numpy_loader.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/utils.cc\")\n```\n\n----------------------------------------\n\nTITLE: Print Concatenated Tensor (Middle Axis)\nDESCRIPTION: This snippet prints the concatenated tensor along the middle axis (axis=1) and its shape. It retrieves the output from the DALI pipeline (`pipe_cat`) and prints the first element of the output tensor, along with its shape, using `o[1].at(0)`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/tensor_join.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Concatenation along middle axis:\")\nprint(o[1].at(0))\nprint(\"Shape: \", o[1].at(0).shape)\n```\n\n----------------------------------------\n\nTITLE: Extract and Prepare ImageNet Validation Data\nDESCRIPTION: This bash script extracts the ImageNet validation data and moves the images to subfolders using a helper script. It creates a `val` directory, moves the validation archive into it, extracts the archive, and then downloads and executes the `valprep.sh` script to organize the validation images into subfolders.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir val && mv ILSVRC2012_img_val.tar val/ && cd val && tar -xvf ILSVRC2012_img_val.tar\nwget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash\n```\n\n----------------------------------------\n\nTITLE: Verify Custom Operator After Plugin Load Python\nDESCRIPTION: After loading the plugin, this Python code uses `help(fn.custom_dummy)` to verify that the custom operator is now available and to display its documentation.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/create_a_custom_operator.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nhelp(fn.custom_dummy)\n```\n\n----------------------------------------\n\nTITLE: Conditional Inclusion of WebDataset Loader (CMake)\nDESCRIPTION: This snippet conditionally includes the WebDataset loader source file (`webdataset_loader.cc`) if the `BUILD_LIBTAR` flag is enabled. This allows DALI to load data from WebDataset format when LIBTAR is available.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/loader/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_LIBTAR)\n  set(DALI_OPERATOR_SRCS ${DALI_OPERATOR_SRCS}\n     \"${CMAKE_CURRENT_SOURCE_DIR}/webdataset_loader.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Clone DALI Extra repository and set environment variable\nDESCRIPTION: This code snippet clones the DALI_extra repository and sets the `DALI_EXTRA_PATH` environment variable. This allows the example to access test images from the DALI_extra repository. The `-d` option in git clone sets the directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone -d /path/to/dali/extra https://github.com/NVIDIA/DALI_extra\nexport DALI_EXTRA_PATH=/path/to/dali/extra\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers with CMake\nDESCRIPTION: This snippet uses a custom CMake function `collect_headers` to find and store a list of header files related to the DALI library. The `DALI_INST_HDRS` variable will contain the list of header files, and it's passed to the parent scope for use in other parts of the build system.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/mm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Indexing with constant index in DALI\nDESCRIPTION: This snippet demonstrates how to extract width and height from a 3-element tensor representing image size using constant indexing. The indices are broadcast to the whole batch.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/indexing.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimages = fn.decoders.image(...)\nsizes = fn.sizes(images)  # height, width, channels\n\nheight = sizes[0]\nwidth  = sizes[1]\n```\n\n----------------------------------------\n\nTITLE: Numpy Reader with Absolute ROI\nDESCRIPTION: This snippet demonstrates the use of the Numpy reader with Region-of-Interest (ROI) extraction, using absolute coordinates. It defines a DALI pipeline (`pipe_roi1`) that reads numpy arrays and extracts a region of interest specified by `roi_start` and `roi_end`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# One file is enough for this example\nfiles = [\"0.npy\"]\n\n\n@pipeline_def(batch_size=batch_size, num_threads=3, device_id=0)\ndef pipe_roi1():\n    data = fn.readers.numpy(\n        device=\"cpu\",\n        file_root=data_dir,\n        files=files,\n        roi_start=[30, 30],\n        roi_end=[230, 230],\n    )\n    return data\n\n\ndata_roi1 = run(pipe_roi1())\nplot_batch(data_roi1)\n```\n\n----------------------------------------\n\nTITLE: Run EfficientNet Training with Standard Configuration (DGX1V-16G)\nDESCRIPTION: This command runs EfficientNet training with a standard configuration for DGX1V-16G, using AMP and DALI with AutoAugment.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython multiproc.py --nproc_per_node 8 ./main.py --amp --static-loss-scale 128 --batch-size 128 $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Import DALI modules and set parameters\nDESCRIPTION: This code snippet imports the necessary modules from the NVIDIA DALI library and sets the batch size and image filename for the example.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/image_processing/brightness_contrast_example.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\nimport matplotlib.pyplot as plt\n\nbatch_size = 10\nimage_filename = \"../data/images\"\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers CMake\nDESCRIPTION: Collects all header files needed for the DALI library. It uses the `collect_headers` CMake function, passing the variable `DALI_INST_HDRS` to store the collected headers and `PARENT_SCOPE` to make the headers available in the parent scope. This ensures that the headers are available during the compilation process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/c_api/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with YOLOv4\nDESCRIPTION: This command performs inference on a single image using a trained YOLOv4 model. It specifies the input image path, trained weights file, and the labels file. The results are displayed on the screen.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/yolov4/readme.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython src/main.py infer image.png -w output.h5 -c coco-labels.txt\n```\n\n----------------------------------------\n\nTITLE: Download and Prepare COCO Validation Data (Shell)\nDESCRIPTION: This snippet downloads, extracts, and prepares the COCO validation dataset.  It uses `wget` to download the image and annotation zip files, and `unzip` to extract their contents. These commands create the necessary directory structure expected by the COCO to TFRecord conversion script. It assumes that `wget` and `unzip` are available in the environment.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/efficientdet/dataset/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n!wget http://images.cocodataset.org/zips/val2017.zip\n!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n!unzip val2017.zip\n!unzip annotations_trainval2017.zip\n```\n\n----------------------------------------\n\nTITLE: Benchmark EfficientNet with DALI (No Augmentations)\nDESCRIPTION: This command benchmarks EfficientNet training using DALI for data loading without any automatic augmentations. The `automatic-augmentation` flag is set to `disabled`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# DALI without automatic augmentations\npython multiproc.py --nproc_per_node 8 ./main.py --amp --static-loss-scale 128 \\\n                    --batch-size 128 --epochs 4 --no-checkpoints --training-only \\\n                    --data-backend dali --automatic-augmentation disabled \\\n                    --workspace $RESULT_WORKSPACE \\\n                    --report-file bench_report_dali.json $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Installing DALI (CUDA 11.0) via pip - shorthand\nDESCRIPTION: Installs the latest official release of NVIDIA DALI built for CUDA 11.0 using pip. This is a shorthand command that assumes the package is available in the default PyPI index or a configured custom index. Requires pip and a compatible Python environment.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install nvidia-dali-cuda110\n```\n\n----------------------------------------\n\nTITLE: Generating Caffe/Caffe2 Protobuf Files with CMake\nDESCRIPTION: This snippet uses the `protobuf_generate_cpp` CMake function to generate C++ source and header files from the specified protobuf definition files (`caffe.proto` and `caffe2.proto`).  The resulting source and header files are stored in `CAFFE_PROTO_SRCS`, `CAFFE_PROTO_HEADERS`, `CAFFE2_PROTO_SRCS`, and `CAFFE2_PROTO_HEADERS` variables, respectively.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/parser/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nprotobuf_generate_cpp(CAFFE_PROTO_SRCS CAFFE_PROTO_HEADERS proto/caffe.proto)\nprotobuf_generate_cpp(CAFFE2_PROTO_SRCS CAFFE2_PROTO_HEADERS proto/caffe2.proto)\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline and Proxy Example in PyTorch\nDESCRIPTION: This example demonstrates a complete workflow of defining a DALI pipeline, initializing the DALI server, creating a PyTorch Dataset using the DALI proxy, and using the DALI proxy DataLoader to consume processed data.  It requires torchvision, nvidia.dali and numpy.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/plugins/pytorch_dali_proxy.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision import datasets, transforms\nfrom nvidia.dali import pipeline_def, fn, types\nfrom nvidia.dali.plugin.pytorch.experimental import proxy as dali_proxy\n\n# Step 1: Define a DALI pipeline\n@pipeline_def\ndef my_dali_pipeline():\n    images = fn.external_source(name=\"images\", no_copy=True)\n    images = fn.resize(images, size=[224, 224])\n    return fn.crop_mirror_normalize(\n        images, dtype=types.FLOAT, output_layout=\"CHW\",\n        mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n        std=[0.229 * 255, 0.224 * 255, 0.225 * 255],\n    )\n\n# Step 2: Initialize DALI server. The scope makes sure to start and stop the background thread\nwith dali_proxy.DALIServer(my_dali_pipeline(batch_size=64, num_threads=3, device_id=0)) as dali_server:\n    # Step 3: Define a PyTorch Dataset using the DALI proxy\n    dataset = datasets.ImageFolder(\"/path/to/images\", transform=dali_server.proxy)\n    \n    # Step 4: Use DALI proxy DataLoader\n    loader = dali_proxy.DataLoader(dali_server, dataset, batch_size=64, num_workers=8, drop_last=True)\n    \n    # Step 5: Consume data\n    for data, target in loader:\n        print(data.shape)  # Processed data ready\n```\n\n----------------------------------------\n\nTITLE: Display image manipulation result\nDESCRIPTION: This code defines a function to display the original and manipulated images. It retrieves the images from the pipeline output and uses matplotlib to display them in a subplot.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/image_processing/brightness_contrast_example.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef display(output, cpu=True):\n    i = 2  # Tweak that to have various images from batch\n    img1 = output[0].at(i) if cpu else output[0].as_cpu().at(i)\n    img2 = output[1].at(i) if cpu else output[1].as_cpu().at(i)\n    fig, ax = plt.subplots(1, 2)\n    ax[0].imshow(img1)\n    ax[1].imshow(img2);\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements\nDESCRIPTION: This command installs the necessary Python packages for the project, as listed in the requirements.txt file. It is a crucial step to ensure that all dependencies are met before running the training or evaluation scripts.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/efficientdet/README.rst#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Build and Run the DALI pipeline\nDESCRIPTION: This snippet builds and runs the defined DALI pipeline. It instantiates the `optical_flow_pipe` with specified batch size, number of threads, and device ID, then builds the pipeline. Finally, it runs the pipeline and retrieves the optical flow vectors as a numpy array.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/optical_flow_example.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipe = optical_flow_pipe(batch_size=batch_size, num_threads=1, device_id=0)\npipe.build()\npipe_out = pipe.run()\nflow_vector = np.array(pipe_out[0][0].as_cpu())\nprint(flow_vector.shape)\n```\n\n----------------------------------------\n\nTITLE: Last Batch Padding Generator - Python\nDESCRIPTION: Defines a function `last_batch_padding` that pads the last batch with the last sample from the generator to match the specified `batch_size`. It ensures that all batches have the same size, which can be important for certain operations.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef last_batch_padding(generator_factory, batch_size):\n    def last_batch_padding_generator():\n        nonlocal generator_factory, batch_size\n        generator = generator_factory()\n        in_batch_idx = 0\n        last_item = None\n        try:\n            # Keeps track of the last sample and the sample number mod batch_size\n            while True:\n                if in_batch_idx >= batch_size:\n                    in_batch_idx -= batch_size\n                last_item = next(generator)\n                in_batch_idx += 1\n                yield last_item\n        # Repeats the last sample the necessary number of times\n        except StopIteration:\n            while in_batch_idx < batch_size:\n                yield last_item\n                in_batch_idx += 1\n    return last_batch_padding_generator\n```\n\n----------------------------------------\n\nTITLE: Building and Linking Test Executable\nDESCRIPTION: This conditional block builds a test executable `dali_kernel_test` if `BUILD_TEST` is enabled. It links the test executable with the `dali_kernels` library, gtest, CUDA, and other DALI libraries, as well as handling library exclusions.  It also sets output name and directory for the executable and defines gtest check targets.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_TEST)\n  # TODO(janton): create a test_utils_lib with dali_test_config.cc and other common utilities\n  adjust_source_file_language_property(\"${DALI_KERNEL_TEST_SRCS}\")\n  add_executable(dali_kernel_test\n    ${DALI_KERNEL_TEST_SRCS}\n    ${DALI_ROOT}/dali/test/dali_test_config.cc)\n\n  # TODO(janton): Remove dependency with target `dali`\n  target_link_libraries(dali_kernel_test PUBLIC dali_kernels dali)\n  target_link_libraries(dali_kernel_test PRIVATE gtest dynlink_cuda ${DALI_LIBS})\n  target_link_libraries(dali_kernel_test PRIVATE \"-Wl,--exclude-libs,${exclude_libs}\")\n  if (WITH_DYNAMIC_NPP)\n    target_link_libraries(dali_kernel_test PRIVATE dynlink_npp)\n  endif ()\n  target_link_libraries(dali_kernel_test PRIVATE \"-pie\")\n  set_target_properties(dali_kernel_test PROPERTIES POSITION_INDEPENDENT_CODE ON)\n  set_target_properties(dali_kernel_test PROPERTIES OUTPUT_NAME \"dali_kernel_test.bin\")\n  set_target_properties(dali_kernel_test PROPERTIES\n    RUNTIME_OUTPUT_DIRECTORY ${TEST_BINARY_DIR})\n\n  add_check_gtest_target(\"check-kernel-gtest\" dali_kernel_test ${TEST_BINARY_DIR})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Sources in CMake\nDESCRIPTION: This CMake macro function collects source files related to DALI operators and stores them in the `DALI_OPERATOR_SRCS` variable. It utilizes the `collect_sources` function to find and add source files to the specified variable. The `PARENT_SCOPE` argument ensures that the variable is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/host/fused/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Training the Flax model\nDESCRIPTION: This code snippet implements the training loop for the Flax model. It iterates over a specified number of epochs and batches from the training iterator. For each batch, it calls the `train_step` function to update the model state. After each epoch, it calculates the accuracy on the validation set using the `accuracy` function and prints the result.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/flax-basic_example.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Starting training\")\n\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch}\")\n    for batch in training_iterator:\n        model_state = train_step(model_state, batch)\n\n    acc = accuracy(model_state, validation_iterator)\n    print(f\"Accuracy = {acc}\")\n```\n\n----------------------------------------\n\nTITLE: Create DALI pipelines and iterator (Python)\nDESCRIPTION: This snippet creates DALI pipelines for each reader type and passes them to a DALIGenericIterator. It then iterates through the data, validating the labels to ensure they fall within the expected range. It tests the pipelines using different label ranges.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-various-readers.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom nvidia.dali.plugin.pytorch import DALIGenericIterator\n\n\npipe_types = [\n    [mxnet_reader_pipeline, (0, 999)],\n    [caffe_reader_pipeline, (0, 999)],\n    [file_reader_pipeline, (0, 1)],\n    [tfrecord_reader_pipeline, (1, 1000)],\n]\n\nfor pipe_t in pipe_types:\n    pipe_name, label_range = pipe_t\n    print(\"RUN: \" + pipe_name.__name__)\n    pipes = [\n        pipe_name(\n            batch_size=BATCH_SIZE,\n            num_threads=2,\n            device_id=device_id,\n            num_gpus=N,\n        )\n        for device_id in range(N)\n    ]\n    dali_iter = DALIGenericIterator(\n        pipes, [\"data\", \"label\"], reader_name=\"Reader\"\n    )\n\n    for i, data in enumerate(dali_iter):\n        # Testing correctness of labels\n        for d in data:\n            label = d[\"label\"]\n            image = d[\"data\"]\n            ## labels need to be integers\n            assert np.equal(np.mod(label, 1), 0).all()\n            ## labels need to be in range pipe_name[2]\n            assert (label >= label_range[0]).all()\n            assert (label <= label_range[1]).all()\n    print(\"OK : \" + pipe_name.__name__)\n```\n\n----------------------------------------\n\nTITLE: Benchmark EfficientNet with DALI and AutoAugment\nDESCRIPTION: This command benchmarks EfficientNet training using DALI for data loading with AutoAugment enabled. The `automatic-augmentation` flag is set to `autoaugment`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# DALI with AutoAugment\npython multiproc.py --nproc_per_node 8 ./main.py --amp --static-loss-scale 128 \\\n                    --batch-size 128 --epochs 4 --no-checkpoints --training-only \\\n                    --data-backend dali --automatic-augmentation autoaugment \\\n                    --workspace $RESULT_WORKSPACE \\\n                    --report-file bench_report_dali_aa.json $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Installing DALI TensorFlow Plugin (CUDA 11.0) via pip - shorthand\nDESCRIPTION: Installs the latest official release of the NVIDIA DALI TensorFlow plugin built for CUDA 11.0 using pip. This is a shorthand command that assumes the package is available in the default PyPI index or a configured custom index. It requires `tensorflow-gpu` to be installed beforehand and `nvidia-dali-cuda110`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install nvidia-dali-tf-plugin-cuda110\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers in CMake\nDESCRIPTION: This CMake macro function collects header files and stores them in the `DALI_INST_HDRS` variable. It utilizes the `collect_headers` function to find and add header files to the specified variable. The `PARENT_SCOPE` argument ensures that the variable is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/host/fused/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline Definition Example\nDESCRIPTION: This code defines a DALI pipeline using the pipeline_def decorator. It uses `fn.external_source` to define the input, reads image data, decodes it to RGB format, and resizes the image. The pipeline expects `batch_size`, `num_threads`, and `device_id` as parameters.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/plugins/pytorch_dali_proxy.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, fn, types\n\n@pipeline_def\ndef example_pipeline():\n    images = fn.external_source(name=\"images\", no_copy=True)\n    images = fn.io.file.read(images)\n    images = fn.decoders.image(images, device=\"mixed\", output_type=types.RGB)\n    return fn.resize(images, size=[224, 224])\n\npipeline = example_pipeline(batch_size=32, num_threads=2, device_id=0)\n```\n\n----------------------------------------\n\nTITLE: Defining data paths for MNIST dataset\nDESCRIPTION: This code snippet defines the file paths for the MNIST training and validation datasets. It uses the `os.path.join` function to construct the paths by combining the value of the `DALI_EXTRA_PATH` environment variable with the relative paths to the MNIST data.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-basic_example.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\ntraining_data_path = os.path.join(\n    os.environ[\"DALI_EXTRA_PATH\"], \"db/MNIST/training/\"\n)\nvalidation_data_path = os.path.join(\n    os.environ[\"DALI_EXTRA_PATH\"], \"db/MNIST/testing/\"\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Trained YOLOv4 Model\nDESCRIPTION: This command evaluates a trained YOLOv4 model on the validation dataset. It specifies the validation data path, annotations file, trained weights file, batch size, and number of steps.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/yolov4/readme.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython src/main.py eval /coco/val2017 /coco/annotations/instances_val2017.json \\\n  -w output.h5 -b 1 -s 5000\n```\n\n----------------------------------------\n\nTITLE: Run EfficientNet Training on Multiple GPUs with DALI and AutoAugment\nDESCRIPTION: This command runs EfficientNet training on multiple GPUs using `multiproc.py`.  It uses DALI as the data backend with AutoAugment, and Automatic Mixed Precision (AMP). The number of GPUs is specified with `--nproc_per_node`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython ./multiproc.py --nproc_per_node 8 ./main.py --amp --static-loss-scale 128 --batch-size 128 --data-backend dali --automatic-augmentation autoaugment $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Creating DALI Iterator for MNIST Data\nDESCRIPTION: This code defines a DALI iterator function `mnist_iterator` that reads data in Caffe2 format, decodes images, normalizes them, and reshapes the labels. It uses `fn.readers.caffe2` for reading, `fn.decoders.image` for decoding, `fn.crop_mirror_normalize` for normalization, and `fn.reshape` for label reshaping. The `@data_iterator` decorator automatically handles the creation of DALI iterators for JAX.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/pax-basic_example.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\nfrom nvidia.dali.plugin.jax import data_iterator\n\n\n@data_iterator(\n    output_map=[\"inputs\", \"labels\"],\n    reader_name=\"mnist_caffe2_reader\",\n    auto_reset=True,\n)\ndef mnist_iterator(data_path, random_shuffle):\n    jpegs, labels = fn.readers.caffe2(\n        path=data_path,\n        random_shuffle=random_shuffle,\n        name=\"mnist_caffe2_reader\",\n    )\n    images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.GRAY)\n    images = fn.crop_mirror_normalize(\n        images, dtype=types.FLOAT, std=[255.0], output_layout=\"HWC\"\n    )\n\n    labels = labels.gpu()\n    labels = fn.reshape(labels, shape=[])\n\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Defining a DALI Pipeline for MNIST Data\nDESCRIPTION: This code defines a DALI pipeline to read, decode, and normalize MNIST images and their corresponding labels from a Caffe2 LMDB dataset. The pipeline uses DALI's `fn.readers.caffe2`, `fn.decoders.image`, and `fn.crop_mirror_normalize` operators. The device argument specifies whether to run the pipeline on CPU or GPU.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, Pipeline\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\nimport os\n\nBATCH_SIZE = 64\nDROPOUT = 0.2\nIMAGE_SIZE = 28\nNUM_CLASSES = 10\nHIDDEN_SIZE = 128\nEPOCHS = 5\nITERATIONS_PER_EPOCH = 100\n\n\n# Path to MNIST dataset\ndata_path = os.path.join(os.environ[\"DALI_EXTRA_PATH\"], \"db/MNIST/training/\")\n\n\n@pipeline_def(device_id=0, batch_size=BATCH_SIZE)\ndef mnist_pipeline(device):\n    jpegs, labels = fn.readers.caffe2(path=data_path, random_shuffle=True)\n    images = fn.decoders.image(\n        jpegs,\n        device=\"mixed\" if device == \"gpu\" else \"cpu\",\n        output_type=types.GRAY,\n    )\n    images = fn.crop_mirror_normalize(\n        images,\n        device=device,\n        dtype=types.FLOAT,\n        std=[255.0],\n        output_layout=\"CHW\",\n    )\n\n    if device == \"gpu\":\n        labels = labels.gpu()\n\n    return images, labels\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Sources with CMake\nDESCRIPTION: This snippet employs the `collect_sources` CMake function to identify and store DALI operator source files.  The `DALI_OPERATOR_SRCS` variable will hold the list of collected source files, and `PARENT_SCOPE` makes this variable available to the parent scope of the current CMake context. This allows other CMake scripts to use these source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/resize/experimental/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setup AFL on Ubuntu 18\nDESCRIPTION: This script installs AFL and its dependencies on a clean Ubuntu 18 installation. It includes installing clang, build-essential, llvm, gnuplot, setting compiler alternatives, configuring core dumps, downloading, extracting, building, and installing AFL.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/fuzzing/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install clang-6.0 build-essential llvm-6.0-dev gnuplot-nox\n\nsudo update-alternatives --install /usr/bin/clang clang `which clang-6.0` 1\nsudo update-alternatives --install /usr/bin/clang++ clang++ `which clang++-6.0` 1\nsudo update-alternatives --install /usr/bin/llvm-config llvm-config `which llvm-config-6.0` 1\nsudo update-alternatives --install /usr/bin/llvm-symbolizer llvm-symbolizer `which llvm-symbolizer-6.0` 1\n\necho core | sudo tee /proc/sys/kernel/core_pattern\n\nwget http://lcamtuf.coredump.cx/afl/releases/afl-latest.tgz\ntar xvf afl-latest.tgz\ncd afl-2.52b   # replace with whatever the current version is\nmake && make -C llvm_mode CXX=g++\nmake install\n```\n\n----------------------------------------\n\nTITLE: Building and Running DALI Pipeline with Constant\nDESCRIPTION: This function builds and runs a DALI pipeline with a specified constant. It retrieves the input and output tensors, then prints a formatted string showing the operation performed, the input and output tensors, and their respective data types. The `is_const_left` parameter controls the order of the constant and tensor in the output string. Requires a built DALI pipeline, an operation name, and a constant value.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef build_and_run_with_const(pipe, op_name, constant, is_const_left=False):\n    pipe.build()\n    pipe_out = pipe.run()\n    t_in = pipe_out[0].as_array()\n    t_out = pipe_out[1].as_array()\n    if is_const_left:\n        print(\n            \"{} {} {} = \\n{}; \\n\\twith types {} {} {} -> {}\\n\".format(\n                constant,\n                op_name,\n                t_in,\n                t_out,\n                type(constant),\n                op_name,\n                t_in.dtype,\n                t_out.dtype,\n            )\n        )\n    else:\n        print(\n            \"{} {} {} = \\n{}; \\n\\twith types {} {} {} -> {}\\n\".format(\n                t_in,\n                op_name,\n                constant,\n                t_out,\n                t_in.dtype,\n                op_name,\n                type(constant),\n                t_out.dtype,\n            )\n        )\n```\n\n----------------------------------------\n\nTITLE: Installing Files and Directories\nDESCRIPTION: Installs various files and directories into the build directory. This section specifies the source files/directories and their destination within the build structure. It ensures all the necessary components are in the correct place for building the plugin.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali_tf_plugin/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES \"${PROJECT_SOURCE_DIR}/nvidia/dali_tf_plugin/dali_tf_plugin.py\" DESTINATION \"${PROJECT_BINARY_DIR}/nvidia/dali_tf_plugin/\")\n\ninstall(DIRECTORY \"${DALI_ROOT}/include\" DESTINATION \"${PROJECT_BINARY_DIR}\")\ninstall(FILES \"${DALI_ROOT}/tools/stubgen.py\" DESTINATION \"${PROJECT_BINARY_DIR}\")\ninstall(FILES \"dali_dataset.h\" DESTINATION \"${PROJECT_BINARY_DIR}/dali_tf_plugin\")\ninstall(FILES \"dali_helper.h\" DESTINATION \"${PROJECT_BINARY_DIR}/dali_tf_plugin\")\ninstall(FILES \"daliop.cc\" DESTINATION \"${PROJECT_BINARY_DIR}\")\ninstall(FILES \"dali_dataset_op.cc\" DESTINATION \"${PROJECT_BINARY_DIR}\")\ninstall(FILES \"dali_tf_plugin_install_tool.py\" DESTINATION \"${PROJECT_BINARY_DIR}\")\ninstall(FILES \"dali_tf_plugin_utils.py\" DESTINATION \"${PROJECT_BINARY_DIR}\")\ninstall(FILES \"${PROJECT_SOURCE_DIR}/MANIFEST.in\" DESTINATION \"${PROJECT_BINARY_DIR}\")\ninstall(FILES \"${DALI_ROOT}/Acknowledgements.txt\" DESTINATION \"${PROJECT_BINARY_DIR}/nvidia/dali_tf_plugin\")\ninstall(FILES \"${DALI_ROOT}/COPYRIGHT\" DESTINATION \"${PROJECT_BINARY_DIR}/nvidia/dali_tf_plugin\")\ninstall(FILES \"${DALI_ROOT}/LICENSE\" DESTINATION \"${PROJECT_BINARY_DIR}/nvidia/dali_tf_plugin\")\ninstall(DIRECTORY \"${PROJECT_SOURCE_DIR}/prebuilt\" DESTINATION \"${PROJECT_BINARY_DIR}\" OPTIONAL)\n```\n\n----------------------------------------\n\nTITLE: Retrieving results from the DALI pipeline (CPU)\nDESCRIPTION: This snippet retrieves the processed image and label batches from the pipeline output and transfers the image batch to the CPU.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/external_input.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbatch_cpu = pipe_out[0].as_cpu()\nlabels_cpu = pipe_out[1]\n```\n\n----------------------------------------\n\nTITLE: Run AFL fuzzing on DALI target\nDESCRIPTION: This command executes the AFL fuzzer on a specified DALI binary. It uses an input directory with example files, lifts the memory limit, specifies an output directory for results, and passes the path to the target binary with '@@' as a placeholder for generated inputs.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/fuzzing/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nafl-fuzz -i /DALI_extra/db/fuzzing/bmp/ -m none -o fuzz_results ./build/dali/python/nvidia/dali/test/dali_rn50_fuzzing_target.bin @@\n```\n\n----------------------------------------\n\nTITLE: Check Custom Operator Availability Python\nDESCRIPTION: This Python code attempts to access the custom operator through `nvidia.dali.fn`. If the operator is not yet loaded, it catches the resulting exception and prints an error message.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/create_a_custom_operator.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport nvidia.dali.fn as fn\n\ntry:\n    help(fn.custom_dummy)\nexcept Exception as e:\n    print(\"Error: \" + str(e))\n```\n\n----------------------------------------\n\nTITLE: Setting Data Loader Test Source Files (CMake)\nDESCRIPTION: This snippet sets the source files for the data loader operator tests. It includes tests for loader, sequence loader, filesystem, and discover files functionalities. These tests verify the correctness of the data loading implementations.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/loader/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_OPERATOR_TEST_SRCS ${DALI_OPERATOR_TEST_SRCS}\n  \"${CMAKE_CURRENT_SOURCE_DIR}/loader_test.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/sequence_loader_test.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/filesystem_test.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/discover_files_test.cc\")\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline with Float Constant and Uint8 Tensor\nDESCRIPTION: This example showcases using a floating-point constant with a DALI pipeline and a `uint8` tensor. It defines a float constant, creates a pipeline that adds the constant to a `uint8` tensor, and then runs the pipeline, printing the result. It shows how a float will affect the type of the output tensor.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nconstant = 42.3\npipe = arithmetic_constant_pipeline((lambda x: x + constant), np.uint8)\nbuild_and_run_with_const(pipe, \"+\", constant)\n```\n\n----------------------------------------\n\nTITLE: Comparing WebDataset and File-Based Pipelines (Python)\nDESCRIPTION: This code snippet compares the outputs of the webdataset pipeline and the file-based pipeline. It runs both pipelines for a specified number of batches, checks for `StopIteration` exceptions, and compares the resulting images using `np.testing.assert_equal`. It asserts that both pipelines either throw a StopIteration exception, or that their images match.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# The number of batches to sample between the two pipelines\nnum_batches = 10\n\nfor _ in range(num_batches):\n    webdataset_pipeline_threw_exception = False\n    file_pipeline_threw_exception = False\n\n    # Try running the webdataset pipeline and check if it has run out of\n    # the samples\n    try:\n        web_img, _ = webdataset_pipeline_instance.run()\n    except StopIteration:\n        webdataset_pipeline_threw_exception = True\n\n    # Try running the file pipeline and check if it has run out of the samples\n    try:\n        (file_img,) = file_pipeline_instance.run()\n    except StopIteration:\n        file_pipeline_threw_exception = True\n\n    # In case of different number of batches\n    assert(webdataset_pipeline_threw_exception==file_pipeline_threw_exception)\n\n    web_img = web_img.as_cpu().as_array()\n    file_img = file_img.as_cpu().as_array()\n\n    # In case the pipelines give different outputs\n    np.testing.assert_equal(web_img, file_img)\nelse:\n    print(\"No difference found!\")\n```\n\n----------------------------------------\n\nTITLE: Instantiate the DALI Pipeline\nDESCRIPTION: This code instantiates the `example_pipe` pipeline with a specified batch size, number of threads, device ID, and random seed.  The `batch_size`, `num_threads`, `device_id`, and `seed` parameters are passed to control the pipeline's execution.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/advanced/serialization.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbatch_size = 16\n\npipe = example_pipe(batch_size=batch_size, num_threads=2, device_id=0, seed=12)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Source Files in CMake\nDESCRIPTION: This snippet utilizes a custom CMake function, 'collect_test_sources', to gather test source files for the project. The detected test source files are stored in the 'DALI_TEST_SRCS' variable within the parent scope, facilitating the integration of testing into the build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/executor/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources and Headers for DALI Operators\nDESCRIPTION: These CMake commands are used to collect all header files and source files related to DALI operators. The `collect_headers` command retrieves header files and sets them in the `DALI_INST_HDRS` variable in the parent scope, while `collect_sources` retrieves source files and sets them in `DALI_OPERATOR_SRCS`.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS)\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline with Constant Wrapper and Integer\nDESCRIPTION: This example demonstrates using the `Constant` wrapper with an integer value in a DALI pipeline. It defines a DALI `Constant` with an integer, creates a pipeline that multiplies it with a `uint8` tensor, and runs the pipeline, printing the results. It shows how `Constant` treats integers as `int32` by default.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nconstant = Constant(10)\npipe = arithmetic_constant_pipeline((lambda x: x * constant), np.uint8)\nbuild_and_run_with_const(pipe, \"*\", constant)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source and Header Files with CMake\nDESCRIPTION: This snippet uses custom CMake functions (`collect_headers`, `collect_sources`, `collect_test_sources`) to gather header files, operator source files, and test source files for the DALI project. The collected file lists are stored in the `DALI_INST_HDRS`, `DALI_OPERATOR_SRCS`, and `DALI_OPERATOR_TEST_SRCS` variables respectively, with `PARENT_SCOPE` making these visible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/remap/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline with Integer Constant\nDESCRIPTION: This example shows how to use an integer constant with a DALI pipeline. It defines a constant, creates a pipeline that adds the constant to a `uint8` tensor, and then runs the pipeline, printing the result. It demonstrates the implicit type promotion of Python integers to `int32` within DALI.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nconstant = 10\npipe = arithmetic_constant_pipeline((lambda x: x + constant), np.uint8)\nbuild_and_run_with_const(pipe, \"+\", constant)\n```\n\n----------------------------------------\n\nTITLE: Importing JAX model training utilities\nDESCRIPTION: This code imports the necessary functions for training the JAX model.  Specifically, it imports `init_model` to initialize the model, `update` to perform a single training step, and `accuracy` to calculate the accuracy on the validation set. These functions are assumed to be defined in a separate module named `model.py`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-basic_example.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom model import init_model, update, accuracy\n```\n\n----------------------------------------\n\nTITLE: Creating Training Iterator for pmap (Python)\nDESCRIPTION: This snippet creates the DALI training iterator using the `mnist_training_iterator` function defined for `pmap` and prints the number of batches. It reuses the validation iterator from the sharding example.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-multigpu_example.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Creating training iterator\")\ntraining_iterator = mnist_training_iterator(\n    batch_size=batch_size, data_path=training_data_path\n)\n\nprint(f\"Number of batches in training iterator = {len(training_iterator)}\")\n```\n\n----------------------------------------\n\nTITLE: Compiling the DALI Plugin with CMake\nDESCRIPTION: This shell script demonstrates how to compile the DALI plugin using CMake. It removes any existing build directory, creates a new build directory, navigates to it, runs CMake to generate the build files, and then uses Make to compile the plugin.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/create_a_custom_operator.ipynb#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\n! rm -rf customdummy/build\n! mkdir -p customdummy/build\n! cd customdummy/build && \\\n  cmake .. && \\\n  make\n```\n\n----------------------------------------\n\nTITLE: Conditional Operator Source Appending (LIBTAR) in CMake\nDESCRIPTION: Conditionally appends the source file for the WebDataset reader operator to the `DALI_OPERATOR_SRCS` list, based on the `BUILD_LIBTAR` build flag.  This allows inclusion of WebDataset support when libtar is available.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_LIBTAR)\n  list(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/webdataset_reader_op.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline with Float Constant and Float32 Tensor\nDESCRIPTION: This example showcases using a floating-point constant with a DALI pipeline and a `float32` tensor. It defines a float constant, creates a pipeline that adds the constant to a `float32` tensor, and then runs the pipeline, printing the result. It showcases the case when float and float tensor are added.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nconstant = 42.3\npipe = arithmetic_constant_pipeline((lambda x: x + constant), np.float32)\nbuild_and_run_with_const(pipe, \"+\", constant)\n```\n\n----------------------------------------\n\nTITLE: Create TFRecord index file (Python)\nDESCRIPTION: This snippet creates an index file for the TFRecord dataset using the `tfrecord2idx` script. It checks if the `idx_files` directory and the index file already exist, and creates them if they don't.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-various-readers.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom subprocess import call\nimport os.path\n\nif not os.path.exists(\"idx_files\"):\n    os.mkdir(\"idx_files\")\n\nif not os.path.isfile(tfrecord_idx):\n    call([tfrecord2idx_script, tfrecord, tfrecord_idx])\n```\n\n----------------------------------------\n\nTITLE: Running WebDataset Pipeline and Displaying Image (Python)\nDESCRIPTION: This code shows how to run the DALI pipeline, retrieve the image and label, convert the image to CPU memory, and display it using matplotlib. It also includes a check for `StopIteration` to handle the end of an epoch and suggests using `pipeline.reset()` to start a new epoch.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# If StopIteration is raised, use pipeline.reset() to start a new epoch\nimg, c = pipeline.run()\nimg = img.as_cpu()\n# Conversion from an array of bytes back to bytes and then to int\nprint(int(bytes(c.as_array()[0])))\nplt.imshow(img.as_array()[0])\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Tests with CMake\nDESCRIPTION: This snippet uses custom CMake functions to collect header files, source files, and test source files into specified variables for use in the DALI project build process. `collect_headers`, `collect_sources`, and `collect_test_sources` are custom functions, likely defined elsewhere in the project's CMake infrastructure. The PARENT_SCOPE argument makes these variables available to the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/parser/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers for DALI\nDESCRIPTION: This snippet uses the `collect_headers` CMake function to gather all relevant header files for DALI and make them available in the parent scope. This ensures that other parts of the project can access these headers.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/sequence/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline with Float Constant\nDESCRIPTION: This example demonstrates using a floating-point constant in a DALI pipeline.  It defines a float constant, constructs a pipeline to add it to a `float32` tensor, and executes the pipeline, printing the output. It illustrates the implicit conversion of Python floats to `float32` in DALI.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nconstant = 10\npipe = arithmetic_constant_pipeline((lambda x: x + constant), np.float32)\nbuild_and_run_with_const(pipe, \"+\", constant)\n```\n\n----------------------------------------\n\nTITLE: Arithmetic Pipeline Definition\nDESCRIPTION: This function defines a DALI pipeline that performs an arithmetic operation on two external source inputs. It takes an `operation` (a lambda function), a `left_type`, and a `right_type` as input. It uses `fn.external_source` to read data from the `get_data` function and then applies the given `operation` to the inputs. The resulting pipeline processes batches of data using the specified operation, configured for CPU execution with 4 threads.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef dali_type(np_type):\n    return types.to_dali_type(np.dtype(np_type).name)\n\n\ndef arithmetic_pipeline(operation, left_type, right_type):\n    pipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\n    with pipe:\n        l, r = fn.external_source(\n            source=lambda: get_data(left_type, right_type),\n            num_outputs=2,\n            dtype=[dali_type(left_type), dali_type(right_type)],\n        )\n        pipe.set_outputs(l, r, operation(l, r))\n\n    return pipe\n```\n\n----------------------------------------\n\nTITLE: Defining Global Constants\nDESCRIPTION: This snippet defines global constants used throughout the example, including paths to data directories for different reader types (MXNet RecordIO, Caffe LMDB, image directory, TFRecord), the number of GPUs, batch size, number of iterations, and image size. It also uses `subprocess` to determine the number of available GPUs.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-various-readers.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os.path\nimport subprocess\n\ntest_data_root = os.environ[\"DALI_EXTRA_PATH\"]\n\n# MXNet RecordIO\ndb_folder = os.path.join(test_data_root, \"db\", \"recordio/\")\n\n# Caffe LMDB\nlmdb_folder = os.path.join(test_data_root, \"db\", \"lmdb\")\n\n# image dir with plain jpeg files\nimage_dir = \"../../data/images\"\n\n# TFRecord\ntfrecord = os.path.join(test_data_root, \"db\", \"tfrecord\", \"train\")\ntfrecord_idx = \"idx_files/train.idx\"\ntfrecord2idx_script = \"tfrecord2idx\"\n\nres = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE, text=True)\nN = res.stdout.count(\"\\n\")  # number of GPUs\nBATCH_SIZE = 128  # batch size per GPU\nITERATIONS = 32\nIMAGE_SIZE = 3\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Sources (CMake)\nDESCRIPTION: This CMake command uses a macro (`collect_sources`) to gather source files for DALI operators. The `DALI_OPERATOR_SRCS` variable stores the collected source files, and `PARENT_SCOPE` ensures the variable is available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/audio/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Multidimensional selection in DALI\nDESCRIPTION: This snippet shows how to specify multiple, comma-separated selections for multidimensional data. When a selection is an index, the corresponding dimension is removed from the output. Slicing keeps the sliced dimensions even if the length of the slice is 1.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/indexing.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimages = fn.decoders.image(jpegs, device=\"mixed\")  # RGB images in HWC layout\nred =   images[:,:,0]\ngreen = images[:,:,1]\nblue =  images[:,:,2]\n\ngreen_with_channel = images[:,:,1:2]  # the last dimension is kept\n```\n\n----------------------------------------\n\nTITLE: Running with ASAN Sanitizers\nDESCRIPTION: This snippet shows how to run a DALI application with AddressSanitizer (ASAN) enabled, including setting environment variables for library paths, ASAN options, and the symbolizer path. It's important to use compatible versions of GCC, CUDA, and libasan.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/compilation.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nLD_LIBRARY_PATH=. ASAN_OPTIONS=symbolize=1:protect_shadow_gap=0 ASAN_SYMBOLIZER_PATH=$(shell which llvm-symbolizer)\nLD_PRELOAD=PATH_TO_LIB_ASAN/libasan.so.X PATH_TO_LIB_STDC/libstdc++.so.STDC_VERSION*PATH_TO_BINARY*\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: This snippet utilizes the `collect_sources` CMake macro to collect source files for the NVIDIA DALI project.  The collected source files are stored in the `DALI_KERNEL_SRCS` variable, and are scoped to the parent scope, ensuring availability throughout the project build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/jpeg/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline with Constant Wrapper and Float\nDESCRIPTION: This example shows how to use the DALI `Constant` wrapper with a float value in a pipeline.  It defines a DALI `Constant` with a float, constructs a pipeline that multiplies it with a `uint8` tensor, and executes the pipeline. `is_const_left` flag is used, to specify that constant is on the left side of the equation.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nconstant = Constant(10.0)\npipe = arithmetic_constant_pipeline((lambda x: constant * x), np.uint8)\nbuild_and_run_with_const(pipe, \"*\", constant, True)\n```\n\n----------------------------------------\n\nTITLE: ResNet Training with AMP (Single GPU) - Bash\nDESCRIPTION: This command trains the ResNet50 model on a single GPU using Automatic Mixed Precision (AMP). It sets the number of epochs to 90, enables AMP, sets the loss scaling factor, enables dynamic loss scaling, and uses the NHWC data layout. It requires PaddlePaddle to be installed.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/paddle/resnet50/paddle-resnet50.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nFLAGS_apply_pass_to_program=1 python -m paddle.distributed.launch \\\n  --gpus=0 train.py \\\n  --epochs 90 \\\n  --amp \\\n  --scale-loss 128.0 \\\n  --use-dynamic-loss-scaling \\\n  --data-layout NHWC\n```\n\n----------------------------------------\n\nTITLE: Defining a custom data iterator (CPU)\nDESCRIPTION: This snippet defines a custom iterator class `ExternalInputIterator` to feed data into the DALI pipeline. It reads image filenames and labels from a file, loads images from disk, and returns them as NumPy arrays in batches.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/external_input.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ExternalInputIterator(object):\n    def __init__(self, batch_size):\n        self.images_dir = \"../../data/images/\"\n        self.batch_size = batch_size\n        with open(self.images_dir + \"file_list.txt\", \"r\") as f:\n            self.files = [line.rstrip() for line in f if line != \"\"]\n        shuffle(self.files)\n\n    def __iter__(self):\n        self.i = 0\n        self.n = len(self.files)\n        return self\n\n    def __next__(self):\n        batch = []\n        labels = []\n        for _ in range(self.batch_size):\n            jpeg_filename, label = self.files[self.i].split(\" \")\n            f = open(self.images_dir + jpeg_filename, \"rb\")\n            batch.append(np.frombuffer(f.read(), dtype=np.uint8))\n            labels.append(np.array([label], dtype=np.uint8))\n            self.i = (self.i + 1) % self.n\n        return (batch, labels)\n```\n\n----------------------------------------\n\nTITLE: Creating TensorFlow Protobuf Library (Conditional) with CMake\nDESCRIPTION: This snippet conditionally creates an object library (`TF_PROTO`) from the generated TensorFlow C++ source and header files. The library creation only occurs if `BUILD_PROTO3` is true, indicating that the protobuf files were generated.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/parser/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(TF_PROTO OBJECT ${TF_PROTO_HEADERS} ${TF_PROTO_SRCS})\n```\n\n----------------------------------------\n\nTITLE: Collecting Files with CMake\nDESCRIPTION: This snippet collects header files, source files, and test source files for the DALI project. It uses custom CMake functions `collect_headers`, `collect_sources`, and `collect_test_sources`. The collected files are stored in `DALI_INST_HDRS`, `DALI_OPERATOR_SRCS`, and `DALI_OPERATOR_TEST_SRCS` variables respectively, with parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/nvcvop/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Slicing in DALI\nDESCRIPTION: This snippet demonstrates basic slicing of a DALI data node. It extracts the first 16 bytes (header) from files in the batch. If the start of the slice is omitted, the slice starts at 0. If the end is omitted, the slice ends at the end of the given axis.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/indexing.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nheader = raw_files[:16]  # extract 16-byte headers from files in the batch\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: This snippet uses a custom CMake function `collect_sources` to find all source files within the current scope (including subdirectories). The collected sources are stored in the `DALI_SRCS` variable with `PARENT_SCOPE`, making them available in the parent CMakeLists.txt.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/operator/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Test Sources with CMake\nDESCRIPTION: This snippet uses CMake functions to collect header, source, and test source files for DALI operators. It calls `collect_headers`, `collect_sources`, and `collect_test_sources` to populate the CMake lists `DALI_INST_HDRS`, `DALI_OPERATOR_SRCS`, and `DALI_OPERATOR_TEST_SRCS`, respectively, in the parent scope. These lists are intended for use in subsequent build and test configurations.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/color/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Run VSRNet Training\nDESCRIPTION: This bash script initiates the training process for the VSRNet.  The script is located in `docs/examples/use_cases/video_superres/run.sh` and allows for configuration of various training parameters. It is expected that data paths will be modified appropriately.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/video_superres/README.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./run.sh\n```\n\n----------------------------------------\n\nTITLE: Visualizing Results\nDESCRIPTION: This code visualizes the output of the DALI pipeline using matplotlib. It iterates through the pipeline, retrieves video frames and associated labels, converts the frame data to a numpy array, and then displays the frames with corresponding labels using matplotlib's pyplot module. PIL is used if available to convert the numpy array into a displayable image.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/video/video_file_list_outputs.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe = video_pipe(\n    batch_size=BATCH_SIZE, num_threads=2, device_id=0, file_list=tf.name\n)\npipe.build()\ngs = gridspec.GridSpec(ITER, 1)\n\nfor i in range(ITER):\n    sequences_out, label, start_frame_num, timestamps = pipe.run()\n    sequences_out = sequences_out.as_cpu().as_array()\n    label = label.as_cpu().as_array()\n    start_frame_num = start_frame_num.as_cpu().as_array()\n    timestamps = timestamps.as_cpu().as_array()\n    batch_sequences = sequences_out[0]\n    sample_frame = batch_sequences[0]\n    if has_PIL:\n        im = Image.fromarray(sample_frame.astype(\"uint8\"))\n        fig = plt.figure(figsize=(16, 64), facecolor=\"#76b900\")\n        plt.subplot(gs[i])\n        plt.axis(\"off\")\n        plt.title(\n            \"label=\" + str(label[0][0]) + \"\\n\"\n            \"frame number=\" + str(start_frame_num[0][0]) + \"\\n\"\n            \"timestamp=\" + str(round(timestamps[0][0], 2)),\n            fontsize=20,\n        )\n        plt.imshow(im)\nplt.close()\ntf.close()\n```\n\n----------------------------------------\n\nTITLE: Adding DALI Video Plugin Library\nDESCRIPTION: Adds a shared library for the DALI video plugin, sets its properties, and links it with necessary libraries.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(dali_${PLUGIN_NAME} SHARED ${VIDEO_PLUGIN_SOURCES})\nset_target_properties(dali_${PLUGIN_NAME} PROPERTIES POSITION_INDEPENDENT_CODE ON)\nset_target_properties(dali_${PLUGIN_NAME} PROPERTIES CUDA_ARCHITECTURES OFF)\ntarget_include_directories(dali_${PLUGIN_NAME} PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/src)\ntarget_link_directories(dali_${PLUGIN_NAME} PUBLIC ${DALI_LIB_DIR})\ntarget_link_libraries(dali_${PLUGIN_NAME} PUBLIC dali dali_core dali_kernels VideoCodecSDKUtils)\n```\n\n----------------------------------------\n\nTITLE: Image Decoder Fuzzing Target in CMake\nDESCRIPTION: This snippet creates a fuzzing target for the image decoder in DALI. It calls the `dali_add_fuzzing_target` function, specifying the target name, binary name, and source file for the image decoder fuzzing target.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/fuzzing/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ndali_add_fuzzing_target(dali_decoder_fuzzing dali_image_decoder_fuzzing_target.bin image_decoder_target.cc)\n```\n\n----------------------------------------\n\nTITLE: Appending MXNet and COCO Reader Sources in CMake\nDESCRIPTION: Appends source files for MXNet and COCO reader operators to the `DALI_OPERATOR_SRCS` list. These source files provide implementations for reading data from MXNet and COCO formats, respectively.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/mxnet_reader_op.cc\")\n\nlist(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/coco_reader_op.cc\")\n```\n\n----------------------------------------\n\nTITLE: Setting DALI Variables to Parent Scope with CMake\nDESCRIPTION: This CMake snippet sets the `DALI_INST_HDRS`, `DALI_SRCS`, and `DALI_TEST_SRCS` variables to the PARENT_SCOPE, making them available in the parent directory's CMake scope.  This enables the inclusion of these files in a higher-level build configuration.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/util/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_INST_HDRS ${DALI_INST_HDRS} PARENT_SCOPE)\nset(DALI_SRCS ${DALI_SRCS} PARENT_SCOPE)\nset(DALI_TEST_SRCS ${DALI_TEST_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Import DALI and NumPy\nDESCRIPTION: This snippet imports necessary libraries, including `types`, `collections`, `numpy` for data manipulation, `random` for shuffling data, and DALI modules for pipeline definition and operators. It also defines the `batch_size` and number of `epochs`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-external_input.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport types\nimport collections\nimport numpy as np\nfrom random import shuffle\nfrom nvidia.dali.pipeline import Pipeline\nimport nvidia.dali as dali\nimport nvidia.dali.fn as fn\n\nbatch_size = 3\nepochs = 3\n```\n\n----------------------------------------\n\nTITLE: Posterize Augmentation Definition\nDESCRIPTION: This code snippet defines the `posterize` augmentation using the `@augmentation` decorator. It posterizes the image by masking out the lower input bits. It takes the data and a mask parameter as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0, 4), ...)\ndef posterize(data, mask)\n```\n\n----------------------------------------\n\nTITLE: Configuring Version Script and NVML Linking in CMake\nDESCRIPTION: Configures a version script for the `dali_core` library and links against NVML if `BUILD_NVML` is enabled. The version script controls which symbols are exported from the library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nset(lib_exports \"libdali_core.map\")\nconfigure_file(\"${DALI_ROOT}/cmake/${lib_exports}.in\" \"${CMAKE_BINARY_DIR}/${lib_exports}\")\ntarget_link_libraries(dali_core PRIVATE -Wl,--version-script=${CMAKE_BINARY_DIR}/${lib_exports})\nif (BUILD_NVML)\n  target_link_libraries(dali_core PRIVATE dynlink_nvml)\nendif(BUILD_NVML)\n```\n\n----------------------------------------\n\nTITLE: Conditional Test Source Appending (BUILD_TEST) in CMake\nDESCRIPTION: Conditionally appends test source files to the `DALI_OPERATOR_TEST_SRCS` list, based on the `BUILD_TEST` build flag. Includes reader_op_test.cc and coco_reader_op_test.cc.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_TEST)\n  # get all the test srcs\n  list(APPEND DALI_OPERATOR_TEST_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/reader_op_test.cc\")\n  list(APPEND DALI_OPERATOR_TEST_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/coco_reader_op_test.cc\")\n  if(BUILD_CUFILE)\n    list(APPEND DALI_OPERATOR_TEST_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/gds_mem_test.cu\")\n  endif()\n  set(DALI_OPERATOR_TEST_SRCS ${DALI_OPERATOR_TEST_SRCS} PARENT_SCOPE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating TFrecord Index Files\nDESCRIPTION: This script creates index files for TFrecords, which are necessary for DALI pipelines to efficiently access the data.  It requires the tfrecord2idx tool from DALI. It takes the TFrecord file pattern and the path to the tfrecord2idx script as input.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/efficientdet/README.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npython3 ./dataset/create_tfrecord_indexes.py \\\n            --tfrecord_file_pattern './tfrecords/*.tfrecord' \\\n            --tfrecord2idx_script ../../../../../tools/tfrecord2idx\n```\n\n----------------------------------------\n\nTITLE: Defining Global Constants for DALI Readers\nDESCRIPTION: This snippet defines global constants and environment variables required for accessing datasets used by various DALI readers (MXNet, Caffe, File, and TFRecord). It retrieves the path to the DALI extra repository, the number of GPUs available, and sets batch size and image size parameters.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-various-readers.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os.path\nimport subprocess\n\ntest_data_root = os.environ[\"DALI_EXTRA_PATH\"]\n\n# MXNet RecordIO\ndb_folder = os.path.join(test_data_root, \"db\", \"recordio/\")\n\n# Caffe LMDB\nlmdb_folder = os.path.join(test_data_root, \"db\", \"lmdb\")\n\n# image dir with plain jpeg files\nimage_dir = \"../../data/images\"\n\n# TFRecord\ntfrecord = os.path.join(test_data_root, \"db\", \"tfrecord\", \"train\")\ntfrecord_idx = \"idx_files/train.idx\"\ntfrecord2idx_script = \"tfrecord2idx\"\n\nres = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE, text=True)\nN = res.stdout.count(\"\\n\")  # number of GPUs\nBATCH_SIZE = 128  # batch size per GPU\nIMAGE_SIZE = 3\n```\n\n----------------------------------------\n\nTITLE: Conditional Operator Source Appending (LIBSND) in CMake\nDESCRIPTION: Conditionally appends the source file for the Nemo ASR reader operator to the `DALI_OPERATOR_SRCS` list, based on the `BUILD_LIBSND` build flag. This provides NeMo ASR data support when libsnd is available.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_LIBSND)\n   list(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/nemo_asr_reader_op.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files with CMake\nDESCRIPTION: This snippet utilizes a CMake function (collect_headers) to gather all header files within the DALI project and stores them in the DALI_INST_HDRS variable. The PARENT_SCOPE option makes the variable available in the parent scope, ensuring accessibility across different CMake contexts.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/plugin/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Squeeze and Expand Dimensions - DALI (Python)\nDESCRIPTION: This code illustrates using `fn.squeeze` and `fn.expand_dims` in DALI to remove and add dimensions with unit extent. It removes a redundant dimension from the input tensor and then adds two new dimensions with names 'F' and 'C'. The input is expected to have a 'CHW' layout.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reinterpret.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(device_id=0, num_threads=4, batch_size=3)\ndef example_squeeze_expand(input_data):\n    np.random.seed(4321)\n    inp = fn.external_source(\n        input_data, batch=False, layout=\"CHW\", dtype=types.INT32\n    )\n    squeezed = fn.squeeze(inp, axes=[0])\n    expanded = fn.expand_dims(squeezed, axes=[0, 3], new_axis_names=\"FC\")\n    return inp, fn.squeeze(inp, axes=[0]), expanded\n\n\ndef single_channel_generator():\n    return np.random.randint(\n        0, 10, size=[1] + rand_shape(2, 1, 7), dtype=np.int32\n    )\n\n\npipe_squeeze_expand = example_squeeze_expand(single_channel_generator)\npipe_squeeze_expand.build()\nshow_result(pipe_squeeze_expand.run())\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: This CMake function collects source files and stores them in the `DALI_OPERATOR_SRCS` variable. The `PARENT_SCOPE` option ensures that this variable is accessible in the parent scope for use in other parts of the build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/host/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Numpy Reader with Glob Filter\nDESCRIPTION: This snippet defines a DALI pipeline (`pipe1`) that uses the `fn.readers.numpy` operator to read numpy files from a directory specified by `file_root`. The `file_filter` argument is used to specify a glob pattern to match files, in this case \"*.npy\". The pipeline is then executed, and the loaded data is plotted.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, fn\n\ndata_dir = os.path.join(data_dir_2d, \"SER00001\")\n\n\n@pipeline_def(batch_size=batch_size, num_threads=3, device_id=0)\ndef pipe1():\n    data = fn.readers.numpy(\n        device=\"cpu\", file_root=data_dir, file_filter=\"*.npy\"\n    )\n    return data\n\n\ndef run(p):\n    p.build()  # build the pipeline\n    outputs = p.run()  # Run once\n    # Getting the batch as a list of numpy arrays, for displaying\n    batch = [np.array(outputs[0][s]) for s in range(batch_size)]\n    return batch\n\n\ndata1 = run(pipe1())\nplot_batch(data1)\n```\n\n----------------------------------------\n\nTITLE: Retrieving DALI Build Configuration with Python\nDESCRIPTION: This Python code snippet demonstrates how to retrieve the DALI build configuration parameters using the `nvidia.dali.sysconfig` module. It prints the include directory, library directory, compile flags, and link flags, which are essential for building custom DALI operators and plugins.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/create_a_custom_operator.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport nvidia.dali.sysconfig as sysconfig\n```\n\nLANGUAGE: Python\nCODE:\n```\nprint(sysconfig.get_include_dir())\n```\n\nLANGUAGE: Python\nCODE:\n```\nprint(sysconfig.get_lib_dir())\n```\n\nLANGUAGE: Python\nCODE:\n```\nprint(sysconfig.get_compile_flags())\n```\n\nLANGUAGE: Python\nCODE:\n```\nprint(sysconfig.get_link_flags())\n```\n\n----------------------------------------\n\nTITLE: Appending Mixed Video Decoder Source File with CMake\nDESCRIPTION: This snippet conditionally appends the `video_decoder_mixed.cc` source file to the `DALI_OPERATOR_SRCS` list if the `BUILD_NVDEC` flag is set. The `BUILD_NVDEC` flag determines whether the mixed (CPU/GPU) NVDEC-based video decoder implementation should be included in the build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/decoder/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_NVDEC)\n  list(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/video_decoder_mixed.cc\")\nendif (BUILD_NVDEC)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Test Sources with CMake\nDESCRIPTION: This CMake command collects the test source files for the NVIDIA DALI operators. The `DALI_OPERATOR_TEST_SRCS` variable will contain the list of test source files. The `PARENT_SCOPE` option ensures the variable is accessible in the parent CMake scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/cache/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Pipeline Definition with Decorator (Python)\nDESCRIPTION: This example demonstrates how to define a DALI pipeline using the `@pipeline_def` decorator. It reads images and masks, decodes them, and returns the processed data along with labels. The file root and seed are defined for the file reader operators, and device placement is specified for the decoder operators.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/pipeline.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def  # create a pipeline with processing graph defined by the function below\ndef my_pipeline():\n    \"\"\" Create a pipeline which reads images and masks, decodes the images and\n        returns them. \"\"\"\n    img_files, labels = fn.readers.file(file_root=\"image_dir\", seed=1)\n    mask_files, _ = fn.readers.file(file_root=\"mask_dir\", seed=1)\n    images = fn.decoders.image(img_files, device=\"mixed\")\n    masks  = fn.decoders.image(mask_files, device=\"mixed\")\n    return images, masks, labels\n\npipe = my_pipeline(batch_size=4, num_threads=2, device_id=0)\n```\n\n----------------------------------------\n\nTITLE: SSD Training Script Usage\nDESCRIPTION: This section outlines the command-line arguments available for the SSD training script. It provides a comprehensive list of options, including data paths, training epochs, batch size, seed values, evaluation intervals, learning rate settings, optimizer parameters, and data pipeline selections. The script uses argparse to handle these command-line arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/single_stage_detector/pytorch_ssd.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nusage: main.py [-h] --data DATA [--epochs EPOCHS] [--batch-size BATCH_SIZE]\n               [--eval-batch-size EVAL_BATCH_SIZE] [--seed SEED]\n               [--evaluation [EVALUATION [EVALUATION ...]]]\n               [--multistep [MULTISTEP [MULTISTEP ...]]] [--target TARGET]\n               [--learning-rate LEARNING_RATE] [--momentum MOMENTUM]\n               [--weight-decay WEIGHT_DECAY] [--warmup WARMUP]\n               [--backbone {resnet18,resnet34,resnet50,resnet101,resnet152}]\n               [--num-workers NUM_WORKERS] [--fp16-mode {off,static,amp}]\n               [--data_pipeline {dali,no_dali}]\n```\n\n----------------------------------------\n\nTITLE: Benchmark EfficientNet with Synthetic Data\nDESCRIPTION: This command benchmarks EfficientNet training using synthetic data. It sets the data backend to `synthetic` and configures other parameters for a benchmarking run.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Adjust the following variable to control where to store the results of the benchmark runs\nexport RESULT_WORKSPACE=./\n\n# synthetic benchmark\npython multiproc.py --nproc_per_node 8 ./main.py --amp --static-loss-scale 128 \\\n                    --batch-size 128 --epochs 1 --prof 1000 --no-checkpoints \\\n                    --training-only --data-backend synthetic \\\n                    --workspace $RESULT_WORKSPACE \\\n                    --report-file bench_report_synthetic.json $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Standard\nDESCRIPTION: These CMake commands set the CUDA standard to CUDA 17 and makes it required.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CUDA_STANDARD 17)\nset(CMAKE_CUDA_STANDARD_REQUIRED ON)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources for DALI Operators (CMake)\nDESCRIPTION: This CMake command utilizes the `collect_sources` macro to accumulate source files related to DALI operators. The collected sources are stored in the `DALI_OPERATOR_SRCS` variable and are accessible in the parent scope. These source files likely contain the implementation of various DALI operators.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/imgcodec/util/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Correct Constant Usage in Mathematical Expressions (Python)\nDESCRIPTION: This snippet highlights the correct and incorrect ways to use constants (NumPy arrays) within DALI mathematical expressions. It demonstrates that constants should be wrapped using `nvidia.dali.types.Constant` to ensure proper handling and avoid undefined behavior. Failure to do so may result in the operator implementation from the tensor's library being incorrectly invoked.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/math.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Correct approach:\nred_highlight_0 = images *\n                  nvidia.dali.types.Constant(np.float32([1.25, 0.75, 0.75]))\nred_highlight_1 = nvidia.dali.types.Constant(np.float32([1.25, 0.75, 0.75])) *\n                  images\n# Wrong approach:\n# red_highlight_2 = np.float32([1.25, 0.75, 0.75]) * images\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Source Files with CMake\nDESCRIPTION: This snippet uses a CMake function (collect_test_sources) to gather all test source files within the DALI project and stores them in the DALI_TEST_SRCS variable. The PARENT_SCOPE option enables access to this variable from the parent scope, allowing its use in test-related build configurations.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/plugin/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files with CMake\nDESCRIPTION: This CMake function call collects all source files and stores them in the `DALI_KERNEL_SRCS` variable. The `PARENT_SCOPE` argument ensures that the variable is accessible in the parent scope, allowing other CMake scripts to utilize the collected source files for compilation.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/common/join/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Import Modules and Define Dataset Paths - Python\nDESCRIPTION: Imports necessary modules like `nvidia.dali`, `webdataset`, and `numpy`. Defines paths to the MNIST dataset shards stored in the DALI extra repository, setting the `batch_size` for subsequent data loading operations. The `DALI_EXTRA_PATH` environment variable must be set.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali.fn as fn\nimport nvidia.dali as dali\nimport nvidia.dali.types as types\nimport webdataset as wds\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nimport os\nimport random\nimport tempfile\nimport tarfile\n\nroot_path = os.path.join(os.environ[\"DALI_EXTRA_PATH\"], \"db\", \"webdataset\",\n                         \"MNIST\")\ntar_dataset_paths = [os.path.join(root_path, data_file)\n                        for data_file in [\"devel-0.tar\", \"devel-1.tar\",\n                                          \"devel-2.tar\"]]\nbatch_size = 16\n```\n\n----------------------------------------\n\nTITLE: AutoAugment policy application in DALI pipeline (Python)\nDESCRIPTION: This code snippet demonstrates how to apply the AutoAugment policy for ImageNet within a DALI pipeline. It defines a pipeline using `@pipeline_def`, enables conditional execution, and uses `auto_augment.auto_augment_image_net` to augment the images. The augmented images are then resized.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/auto_aug.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, fn, types\nfrom nvidia.dali.auto_aug import auto_augment\n\n\n@pipeline_def(enable_conditionals=True)\ndef training_pipe(data_dir, image_size):\n\n    jpegs, labels = fn.readers.file(file_root=data_dir, ...)\n    shapes = fn.peek_image_shape(jpegs)\n    images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.RGB)\n\n    # Applies the AutoAugment policy for ImageNet\n    augmented_images = auto_augment.auto_augment_image_net(images, shape=shapes)\n\n    resized_images = fn.resize(augmented_images, size=[image_size, image_size])\n\n    return resized_images, labels\n```\n\n----------------------------------------\n\nTITLE: Conditional Operator Source Appending (PROTO3) in CMake\nDESCRIPTION: Conditionally appends the source file for the TFRecord reader operator to the `DALI_OPERATOR_SRCS` list, based on the `BUILD_PROTO3` build flag. This ensures TFRecord support is included only when the protobuf library is available.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_PROTO3)\n  list(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/tfrecord_reader_op.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: Collects source files and stores their paths in the DALI_OPERATOR_SRCS variable. The PARENT_SCOPE option makes the variable available in the parent scope, allowing other parts of the CMake configuration to access the list of source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/segmentation/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Generating nvimgcodec Stub using Python in CMake\nDESCRIPTION: This CMake snippet defines a custom command to generate a stub file (`dynlink_nvimgcodec_gen.cc`) for the nvimgcodec library.  It uses a Python script (`stub_codegen.py`) to generate the stub based on the nvimgcodec header file and JSON configuration. The script takes include directories, compiler options and input/output paths as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/nvimgcodec/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(WITH_DYNAMIC_NVIMGCODEC)\n  set(NVIMGCODEC_GENERATED_STUB \"${CMAKE_CURRENT_BINARY_DIR}/dynlink_nvimgcodec_gen.cc\")\n  add_custom_command(\n    OUTPUT ${NVIMGCODEC_GENERATED_STUB}\n    COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/stub_codegen.py --unique_prefix=Nvimgcodec --\n    \"${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/nvimgcodec.json\" ${NVIMGCODEC_GENERATED_STUB}\n    \"${nvimgcodec_INCLUDE_DIR}/nvimgcodec.h\" \"-I${nvimgcodec_INCLUDE_DIR}\" \"-I${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}\"\n\n    # for some reason QNX fails with 'too many errors emitted' if this is not set\n    \"-ferror-limit=0\"\n    ${DEFAULT_COMPILER_INCLUDE}\n    DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/stub_codegen.py\n    \"${nvimgcodec_INCLUDE_DIR}/nvimgcodec.h\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/nvimgcodec.json\"\n    COMMENT \"Running nvimgcodec.h stub generator\"\n    VERBATIM)\n\n  set_source_files_properties(${NVIMGCODEC_GENERATED_STUB} PROPERTIES GENERATED TRUE)\n  add_library(dynlink_nvimgcodec STATIC nvimgcodec_wrap.cc ${NVIMGCODEC_GENERATED_STUB})\n  target_include_directories(dynlink_nvimgcodec PUBLIC \"${nvimgcodec_INCLUDE_DIR}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Project Definition and Subdirectory Inclusion in CMake\nDESCRIPTION: This snippet initializes the CMake project `dali_operator` with CUDA and C++ support. It then includes subdirectories, each representing a different category of operators, contributing to the overall DALI operator library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nproject(dali_operator CUDA CXX C)\n\nadd_subdirectory(audio)\nadd_subdirectory(bbox)\nadd_subdirectory(geometry)\nadd_subdirectory(debug)\nadd_subdirectory(decoder)\nadd_subdirectory(generic)\nadd_subdirectory(image)\nadd_subdirectory(imgcodec)\nadd_subdirectory(io)\nadd_subdirectory(math)\nadd_subdirectory(random)\nadd_subdirectory(reader)\nadd_subdirectory(segmentation)\nadd_subdirectory(sequence)\nadd_subdirectory(signal)\nadd_subdirectory(ssd)\nadd_subdirectory(util)\nadd_subdirectory(numba_function)\nif (BUILD_CVCUDA)\n  add_subdirectory(nvcvop)\nendif()\nif (BUILD_PYTHON)\n  add_subdirectory(python_function)\nendif()\nif (BUILD_NVDEC)\n  add_subdirectory(video)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Test Sources with CMake\nDESCRIPTION: This command collects the source files for testing DALI operators and makes them available in the parent scope for building test executables.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/geometry/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Language Property\nDESCRIPTION: This code adjusts the source file language properties and adds the `dali_kernels` library.  A dummy CUDA file is used to ensure CUDA is properly configured. It also links the `dali_core` library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadjust_source_file_language_property(\"${DALI_KERNEL_SRCS}\")\nset_source_files_properties(\"dummy.cu\" PROPERTIES LANGUAGE CUDA)\nadd_library(dali_kernels ${LIBTYPE} ${DALI_KERNEL_SRCS})\ntarget_link_libraries(dali_kernels PUBLIC dali_core)\n```\n\n----------------------------------------\n\nTITLE: Creating TFRecord Index File\nDESCRIPTION: This snippet creates an index file for a TFRecord dataset using the `tfrecord2idx` script. It first checks if the `idx_files` directory and the index file already exist. If not, it creates the directory and executes the script to generate the index file, which is required for efficient data access with the DALI TFRecord reader.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/paddle/paddle-various-readers.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom subprocess import call\nimport os.path\n\nif not os.path.exists(\"idx_files\"):\n    os.mkdir(\"idx_files\")\n\nif not os.path.isfile(tfrecord_idx):\n    call([tfrecord2idx_script, tfrecord, tfrecord_idx])\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Source Files with CMake\nDESCRIPTION: This CMake function call collects source files specifically for tests and stores them in the `DALI_KERNEL_TEST_SRCS` variable. The `PARENT_SCOPE` argument ensures that the variable is accessible in the parent scope, enabling the utilization of these source files within the testing framework.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/common/join/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Git Workflow for Feature Development\nDESCRIPTION: This snippet outlines a typical git workflow for developing a new feature within the DALI project. It includes creating a branch, writing code, and rebasing to stay up-to-date with the main DALI branch, ensuring a clean history and minimizing merge conflicts.\nSOURCE: https://github.com/nvidia/dali/blob/main/guided_contribution_tutorial.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Create a branch with your feature\ngit checkout -b my_awesome_feature\n\n# Write code, write tests, debug, develop\n\n# From time to time you should get up to date with the main DALI branch:\ngit checkout main\ngit pull upstream main\ngit checkout -b my_awesome_feature\ngit rebase main\n```\n\n----------------------------------------\n\nTITLE: Helper Function to Print Paxml Logs\nDESCRIPTION: This Python code defines a helper function, `print_logs`, which reads training accuracy from log files generated by Paxml and prints it to the console. It uses the `tensorflow` library to parse the log files, extract the accuracy metric, and display the iteration number and corresponding accuracy value.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/pax-basic_example.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorflow.core.util import event_pb2\nfrom tensorflow.python.lib.io import tf_record\nfrom tensorflow.python.framework import tensor_util\n\n\ndef print_logs(path):\n    \"Helper function to print logs from logs directory created by paxml example\"\n\n    def summary_iterator():\n        for r in tf_record.tf_record_iterator(path):\n            yield event_pb2.Event.FromString(r)\n\n    for summary in summary_iterator():\n        for value in summary.summary.value:\n            if value.tag == \"Metrics/accuracy\":\n                t = tensor_util.MakeNdarray(value.tensor)\n                print(f\"Iteration: {summary.step}, accuracy: {t}\")\n```\n\n----------------------------------------\n\nTITLE: Using TrivialAugment Wide in DALI Pipeline (Python)\nDESCRIPTION: This code snippet demonstrates how to integrate TrivialAugment Wide into a DALI pipeline using the `trivial_augment_wide` function. It requires the DALI library and defines a pipeline with image decoding, augmentation, and resizing. The `enable_conditionals=True` argument in `@pipeline_def` is crucial for using automatic augmentations.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/trivial_augment.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nvidia.dali import pipeline_def, fn, types\nfrom nvidia.dali.auto_aug import trivial_augment\n\n@pipeline_def(enable_conditionals=True)\ndef training_pipe(data_dir, image_size):\n\n    jpegs, labels = fn.readers.file(file_root=data_dir, ...)\n    shapes = fn.peek_image_shape(jpegs)\n    images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.RGB)\n\n    augmented_images = trivial_augment.trivial_augment_wide(images, shape=shapes)\n\n    resized_images = fn.resize(augmented_images, size=[image_size, image_size])\n\n    return resized_images, labels\n```\n\n----------------------------------------\n\nTITLE: Inferring Compiler Include Directories using CMake\nDESCRIPTION: This snippet uses the `DETERMINE_GCC_SYSTEM_INCLUDE_DIRS` macro to infer the system include directories used by the C++ compiler. It takes the compiler path and flags as input, storing the inferred directories in the `INFERED_COMPILER_INCLUDE` variable. This information is then transformed into a list of `-I` directives.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/npp/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nDETERMINE_GCC_SYSTEM_INCLUDE_DIRS(\"c++\" \"${CMAKE_CXX_COMPILER}\" \"${CMAKE_CXX_FLAGS}\" INFERED_COMPILER_INCLUDE)\n\n# transform a list of paths into a list of include directives\nset(DEFAULT_COMPILER_INCLUDE)\nforeach(incl_dir ${INFERED_COMPILER_INCLUDE})\n  set(DEFAULT_COMPILER_INCLUDE \"${DEFAULT_COMPILER_INCLUDE} -I${incl_dir}\")\nendforeach(incl_dir)\nseparate_arguments(DEFAULT_COMPILER_INCLUDE UNIX_COMMAND  \"${DEFAULT_COMPILER_INCLUDE}\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources and Test Sources in CMake\nDESCRIPTION: This snippet demonstrates how to collect header, source, and test source files in a CMake project using custom functions. The functions populate lists of file paths that can be used in subsequent build steps.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/operator/builtin/conditional/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers in CMake\nDESCRIPTION: This snippet defines a macro to collect header files for the DALI library. The `collect_headers` function is used to aggregate the DALI header files and make them available within a specified scope, typically used for installation or packaging purposes.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/fuzzing/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: This CMake function collects test source files and stores them in the `DALI_OPERATOR_TEST_SRCS` variable. The `PARENT_SCOPE` option ensures that these test sources are available in the parent scope for use in defining and running tests.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/host/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Filtering Source Files Based on Patterns in CMake\nDESCRIPTION: This snippet filters the source files for the DALI operators based on specified patterns and exclusions. It uses a custom `custom_filter` function to include only the necessary files based on the `OPERATOR_SRCS_PATTERN` and `OPERATOR_SRCS_PATTERN_EXCLUDE` variables.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT OPERATOR_SRCS_PATTERN STREQUAL \"\" OR\n    NOT OPERATOR_SRCS_PATTERN_EXCLUDE STREQUAL \"\")\n  # Needed for a usable build\n  set(EXTRA_FILES \"\")\n  list(APPEND EXTRA_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/operators.cc\")\n  # Those are needed for operators.cc to have all symbols\n  list(APPEND EXTRA_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/util/npp.cc\")\n  list(APPEND EXTRA_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/decoder/nvjpeg/nvjpeg_helper.cc\")\n  list(APPEND EXTRA_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/video/dynlink_nvcuvid/dynlink_nvcuvid.cc\")\n\n  list(APPEND OPERATOR_SRCS_PATTERN_EXCLUDE \"*test*\")\n  custom_filter(CMAKE_CURRENT_SOURCE_DIR\n                DALI_OPERATOR_SRCS\n                EXTRA_FILES\n                OPERATOR_SRCS_PATTERN\n                OPERATOR_SRCS_PATTERN_EXCLUDE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Filtering Source Files\nDESCRIPTION: This conditional block filters the source files based on patterns.  If `KERNEL_SRCS_PATTERN` or `KERNEL_SRCS_PATTERN_EXCLUDE` are set, it uses a custom filter to include only the desired files and adds specific files using `list(APPEND)`. It excludes test files by default.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT KERNEL_SRCS_PATTERN STREQUAL \"\" OR\n    NOT KERNEL_SRCS_PATTERN_EXCLUDE STREQUAL \"\")\n  # Needed for a usable build\n  set(EXTRA_FILES \"\")\n  list(APPEND EXTRA_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/kernel.cc\")\n  list(APPEND EXTRA_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/common/scatter_gather.cu\")\n  list(APPEND EXTRA_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/signal/fft/cufft_helper.cc\")\n\n  list(APPEND KERNEL_SRCS_PATTERN_EXCLUDE \"*test*\")\n  custom_filter(CMAKE_CURRENT_SOURCE_DIR\n                DALI_KERNEL_SRCS\n                EXTRA_FILES\n                KERNEL_SRCS_PATTERN\n                KERNEL_SRCS_PATTERN_EXCLUDE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Dummy Operator Header Definition C++\nDESCRIPTION: This C++ header file defines the `CustomDummy` operator. It includes necessary DALI headers and declares the operator class, including SetupImpl and HasContiguousOutputs functions.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/create_a_custom_operator.ipynb#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#pragma once\n\n#include <vector>\n#include <numeric>\n\n#include \"dali/core/tensor_shape.h\"\n#include \"dali/pipeline/operator/operator.h\"\n#include \"dali/pipeline/operator/output_layout.h\"\n\nnamespace dali {\n\nclass CustomDummy : public Operator {\n public:\n  explicit CustomDummy(const OpSpec& spec) : Operator(spec) {}\n\n  ~CustomDummy() override = default;\n\n  bool SetupImpl(const std::vector<OutputShape>& in_shape, const std::string& in_type,\n                 std::vector<OutputShape>* out_shape, std::string* out_type) override {\n    *out_shape = in_shape;\n    *out_type = in_type;\n    return true;\n  }\n\n  bool HasContiguousOutputs() const override { return true; }\n};\n\n}\n\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: This CMake command collects all test source files and makes them available in the parent scope. It likely uses CMake's file globbing to identify test files and assigns them to the `DALI_OPERATOR_TEST_SRCS` variable for use in testing the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/convolution/gaussian_blur_gpu/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Test Sources with CMake\nDESCRIPTION: This snippet uses the `collect_test_sources` CMake function to gather test source files for DALI operators, storing the results in the `DALI_OPERATOR_TEST_SRCS` variable. The `PARENT_SCOPE` option ensures that the variable is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/resize/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Building the fuzzing suite\nDESCRIPTION: This snippet includes the `fuzzing` subdirectory if the `BUILD_FUZZING` option is enabled, which allows the DALI fuzzing suite to be built.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_FUZZING)\n  add_subdirectory(fuzzing)\nendif()\n```\n\n----------------------------------------\n\nTITLE: PyTorch ImageNet Training Script Usage (Bash)\nDESCRIPTION: Displays the usage information for the `main.py` training script.  It lists available command-line arguments for configuring the training process, including model architecture, batch size, learning rate, data loader type (PyTorch, DALI, or DALI proxy), and other training parameters. The parameters control various aspects of the training process, from model selection to data loading and optimization strategies.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/resnet50/pytorch-resnet50.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmain.py [-h] [--arch ARCH] [-j N] [--epochs N] [--start-epoch N] [-b N] [--lr LR] [--momentum M] [--weight-decay W] [--print-freq N] [--resume PATH]\n                  [-e] [--pretrained] [--dali_cpu] [--data_loader {pytorch,dali,dali_proxy}] [--prof PROF] [--deterministic] [--fp16-mode]\n                  [--loss-scale LOSS_SCALE] [--channels-last CHANNELS_LAST] [-t]\n                  [DIR ...]\n\n  PyTorch ImageNet Training\n\n  positional arguments:\n    DIR                   path(s) to dataset (if one path is provided, it is assumed to have subdirectories named \"train\" and \"val\"; alternatively, train and val paths can\n                          be specified directly by providing both paths as arguments)\n\n  options:\n    -h, --help            show this help message and exit\n    --arch ARCH, -a ARCH  model architecture: alexnet | convnext_base | convnext_large | convnext_small | convnext_tiny | densenet121 | densenet161 | densenet169 |\n                          densenet201 | efficientnet_b0 | efficientnet_b1 | efficientnet_b2 | efficientnet_b3 | efficientnet_b4 | efficientnet_b5 | efficientnet_b6 |\n                          efficientnet_b7 | efficientnet_v2_l | efficientnet_v2_m | efficientnet_v2_s | get_model | get_model_builder | get_model_weights | get_weight |\n                          googlenet | inception_v3 | list_models | maxvit_t | mnasnet0_5 | mnasnet0_75 | mnasnet1_0 | mnasnet1_3 | mobilenet_v2 | mobilenet_v3_large |\n                          mobilenet_v3_small | regnet_x_16gf | regnet_x_1_6gf | regnet_x_32gf | regnet_x_3_2gf | regnet_x_400mf | regnet_x_800mf | regnet_x_8gf |\n                          regnet_y_128gf | regnet_y_16gf | regnet_y_1_6gf | regnet_y_32gf | regnet_y_3_2gf | regnet_y_400mf | regnet_y_800mf | regnet_y_8gf | resnet101 |\n                          resnet152 | resnet18 | resnet34 | resnet50 | resnext101_32x8d | resnext101_64x4d | resnext50_32x4d | shufflenet_v2_x0_5 | shufflenet_v2_x1_0 |\n                          shufflenet_v2_x1_5 | shufflenet_v2_x2_0 | squeezenet1_0 | squeezenet1_1 | swin_b | swin_s | swin_t | swin_v2_b | swin_v2_s | swin_v2_t | vgg11 |\n                          vgg11_bn | vgg13 | vgg13_bn | vgg16 | vgg16_bn | vgg19 | vgg19_bn | vit_b_16 | vit_b_32 | vit_h_14 | vit_l_16 | vit_l_32 | wide_resnet101_2 |\n                          wide_resnet50_2 (default: resnet18)\n    -j N, --workers N     number of data loading workers (default: 4)\n    --epochs N            number of total epochs to run\n    --start-epoch N       manual epoch number (useful on restarts)\n    -b N, --batch-size N  mini-batch size per process (default: 256)\n    --lr LR, --learning-rate LR\n                          Initial learning rate. Will be scaled by <global batch size>/256: args.lr = args.lr*float(args.batch_size*args.world_size)/256. A warmup schedule\n                          will also be applied over the first 5 epochs.\n    --momentum M          momentum\n    --weight-decay W, --wd W\n                          weight decay (default: 1e-4)\n    --print-freq N, -p N  print frequency (default: 10)\n    --resume PATH         path to latest checkpoint (default: none)\n    -e, --evaluate        evaluate model on validation set\n    --pretrained          use pre-trained model\n    --dali_cpu            Runs CPU based version of DALI pipeline.\n    --data_loader {pytorch,dali,dali_proxy}\n                          Select data loader: \"pytorch\" for native PyTorch data loader, \"dali\" for DALI data loader, or \"dali_proxy\" for PyTorch dataloader with DALI proxy\n                          preprocessing.\n    --prof PROF           Only run 10 iterations for profiling.\n    --deterministic       Enable deterministic behavior for reproducibility\n    --fp16-mode           Enable half precision mode.\n    --loss-scale LOSS_SCALE\n                          Scaling factor for loss to prevent underflow in FP16 mode.\n    --channels-last CHANNELS_LAST\n                          Use channels last memory format for tensors.\n    -t, --test            Launch test mode with preset arguments\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in DALI with CMake\nDESCRIPTION: This CMake snippet utilizes the `collect_sources` macro to find and aggregate all source files in the current directory and its subdirectories. The collected file paths are stored in the `DALI_OPERATOR_SRCS` variable, making them available in the parent scope. The macro automatically searches the current directory and its subdirectories for source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/distortion/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in DALI with CMake\nDESCRIPTION: This CMake snippet collects test source files and makes them available within the parent scope. It uses the `collect_test_sources` function, which is presumably defined elsewhere in the DALI build system. `DALI_KERNEL_TEST_SRCS` is the variable that will store the collected test source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/normalize/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional Execution in Pipeline (Python)\nDESCRIPTION: This example showcases conditional execution within a DALI pipeline using an `if` statement. The `@pipeline_def` decorator is used with `enable_conditionals=True`.  Each image is rotated with a 25% probability by a random angle.  The `do_rotate` variable is a DALI DataNode condition controlling whether the `rotate` operator is applied.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/pipeline.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(enable_conditionals=True)\ndef random_rotate():\n    jpegs, _ = fn.readers.file(device=\"cpu\", file_root=images_dir)\n    images = fn.decoders.image(jpegs, device=\"mixed\")\n    do_rotate = fn.random.coin_flip(probability=0.25, dtype=DALIDataType.BOOL)\n    if do_rotate:\n        result = fn.rotate(images, angle=fn.random.uniform(range=(10, 30)), fill_value=0)\n    else:\n        result = images\n    return result\n```\n\n----------------------------------------\n\nTITLE: TFRecord reader pipeline (Python)\nDESCRIPTION: This snippet defines a DALI pipeline for reading data from TFRecord files. It uses `fn.readers.tfrecord` to read the data, specifying the features to extract from the TFRecord.  The data is then passed to the `common_pipeline` function for image processing.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-various-readers.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali.tfrecord as tfrec\n\n\n@pipeline_def\ndef tfrecord_reader_pipeline(num_gpus):\n    inputs = fn.readers.tfrecord(\n        path=tfrecord,\n        index_path=tfrecord_idx,\n        features={\n            \"image/encoded\": tfrec.FixedLenFeature((), tfrec.string, \"\"),\n            \"image/class/label\": tfrec.FixedLenFeature([1], tfrec.int64, -1),\n        },\n        random_shuffle=True,\n        shard_id=Pipeline.current().device_id,\n        num_shards=num_gpus,\n        name=\"Reader\",\n    )\n\n    return common_pipeline(inputs[\"image/encoded\"], inputs[\"image/class/label\"])\n```\n\n----------------------------------------\n\nTITLE: CMake Project Setup and Linking\nDESCRIPTION: This CMake code snippet sets the minimum required CMake version, defines the project name and supported languages, sets the C++ standard, adds an executable named `nodeps_test` from `main.cc`, and links it against the `dali_core` and `dali_kernels` libraries. These libraries are presumably part of the DALI (Data Loading Library) framework.\nSOURCE: https://github.com/nvidia/dali/blob/main/qa/TL1_nodeps_build/CMakeLists_system.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\nproject(nodeps_test CUDA CXX)\nset(CMAKE_CXX_STANDARD 17)\n\nadd_executable(nodeps_test main.cc)\ntarget_link_libraries(nodeps_test dali_core dali_kernels)\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files with CMake\nDESCRIPTION: This CMake macro collects header files and makes them available in the parent scope.  It searches for header files based on project conventions and adds them to the `DALI_INST_HDRS` variable. The PARENT_SCOPE option ensures that the variable is accessible in the calling scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/convolution/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Inclusion CMake\nDESCRIPTION: Conditionally includes the `audio` subdirectory if the `BUILD_LIBSND` variable is set. This allows for selective building of components based on configuration options.  No specific inputs or outputs other than the presence or absence of the audio component in the build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_LIBSND)\n  add_subdirectory(audio)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers with CMake\nDESCRIPTION: This snippet uses the `collect_headers` CMake function to gather header files, storing the results in the `DALI_INST_HDRS` variable. The `PARENT_SCOPE` option ensures that the variable is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/resize/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Define DALI Pipeline with Constants\nDESCRIPTION: Defines a DALI pipeline that adds a constant (200) to one input tensor, multiplies another input tensor by a constant (0.75), and subtracts a constant (15) from the second input tensor using `nvidia.dali.types.Constant`.  This demonstrates how to use constants in arithmetic operations within DALI.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(batch_size=batch_size, num_threads=4, device_id=0)\ndef pipeline_2():\n    l, r = fn.external_source(source=get_data, num_outputs=2, dtype=types.INT32)\n    add_200 = l + 200\n    mul_075 = l * 0.75\n    sub_15 = Constant(15).float32() - r\n    return l, r, add_200, mul_075, sub_15\n```\n\n----------------------------------------\n\nTITLE: Installing Legacy DALI (CUDA 11.0) via pip\nDESCRIPTION: Installs an older version of NVIDIA DALI and the TensorFlow plugin built for CUDA 11.0 using pip. This command fetches the package from NVIDIA's CUDA 11.0 index and upgrades if an older version is present.  This targets DALI versions starting from 0.22.0 and requires setting the correct pip index.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/11.0 --upgrade nvidia-dali\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/11.0 --upgrade nvidia-dali-tf-plugin\n```\n\n----------------------------------------\n\nTITLE: Setting Object Files Based on Build Options in CMake\nDESCRIPTION: This code snippet appends target object files to the `DALI_OPERATOR_OBJ` list based on conditional build options. This allows for including different protocol buffer implementations and NVDEC support depending on what features are enabled during the build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_OPERATOR_OBJ)\nif (BUILD_LMDB)\n  list(APPEND DALI_OPERATOR_OBJ $<TARGET_OBJECTS:CAFFE_PROTO> $<TARGET_OBJECTS:CAFFE2_PROTO>)\nendif()\nif (BUILD_PROTO3)\n  list(APPEND DALI_OPERATOR_OBJ $<TARGET_OBJECTS:TF_PROTO>)\nendif()\n\nif (BUILD_NVDEC)\n  list(APPEND DALI_OPERATOR_OBJ $<TARGET_OBJECTS:NVCUVID_GEN>)\nendif(BUILD_NVDEC)\n```\n\n----------------------------------------\n\nTITLE: Inference Script Usage (Bash)\nDESCRIPTION: This section displays the usage information for the `infer.py` script.  It describes the positional argument DIR for the path to video files and the optional arguments -k/--topk for the number of top predictions and -s/--stride for the distance between frames.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/paddle/tsm/paddle-tsm.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nusage: infer.py [-h] [--topk K] [--stride S] DIR\n\nPaddle Temporal Shift Module Inference\n\npositional arguments:\n  DIR               Path to video files\n\noptional arguments:\n  -h, --help        show this help message and exit\n  --topk K, -k K    Top k results (default: 1)\n  --stride S, -s S  Distance between frames (default: 30)\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries and Setting Output Directory\nDESCRIPTION: These lines link the `dali_kernels` library with other DALI libraries, CUDA, and handles library exclusions.  It also sets the library output directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(dali_kernels PRIVATE ${DALI_LIBS} dynlink_cuda)\ntarget_link_libraries(dali_kernels PRIVATE \"-Wl,--exclude-libs,${exclude_libs}\")\nset_target_properties(dali_kernels PROPERTIES\n    LIBRARY_OUTPUT_DIRECTORY \"${DALI_LIBRARY_OUTPUT_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Generating NPP Stub using Python and CMake\nDESCRIPTION: This snippet uses `add_custom_command` to generate a C++ stub file `dynlink_npp_gen.cc` from `npp.json` and `npp.h` using a Python script `stub_codegen.py`.  The stub generation is conditional based on `WITH_DYNAMIC_NPP`. The generated file is added as a source to the `dynlink_npp` static library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/npp/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (WITH_DYNAMIC_NPP)\n    set(NPP_GENERATED_STUB \"${CMAKE_CURRENT_BINARY_DIR}/dynlink_npp_gen.cc\")\n    add_custom_command(\n        OUTPUT ${NPP_GENERATED_STUB}\n        COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/stub_codegen.py --unique_prefix=Npp --\n                    \"${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/npp.json\" ${NPP_GENERATED_STUB}\n                    \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}/npp.h\" \"-I${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}\"\n                    # for some reason QNX fails with 'too many errors emitted' is this is not set\n                    \"-ferror-limit=0\"\n                    ${DEFAULT_COMPILER_INCLUDE}\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/stub_codegen.py\n                \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}/npp.h\"\n                \"${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/npp.json\"\n        COMMENT \"Running npp.h stub generator\"\n        VERBATIM)\n\n    set_source_files_properties(${NPP_GENERATED_STUB} PROPERTIES GENERATED TRUE)\n    add_library(dynlink_npp STATIC npp_wrap.cc ${NPP_GENERATED_STUB})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in DALI with CMake\nDESCRIPTION: This CMake snippet collects header files and makes them available within the parent scope. It uses the `collect_headers` function which is presumably defined elsewhere in the DALI build system.  `DALI_INST_HDRS` is the variable that will store the collected header files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/normalize/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in DALI with CMake\nDESCRIPTION: This CMake snippet collects the necessary header files for the DALI kernel. The `collect_headers` function is called with `DALI_INST_HDRS` and `PARENT_SCOPE` as arguments. This ensures that the collected headers are available for use in the parent scope during the build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/geom/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Linking Prebuilt DALI Libraries\nDESCRIPTION: This code snippet finds and links prebuilt DALI libraries if `PREBUILD_DALI_LIBS` is enabled. It uses `find_library` to locate the prebuilt libraries and then uses `build_per_python_lib` to link them with the DALI Python backend.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/python/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (PREBUILD_DALI_LIBS)\n# find prebuild DALI libs\n  find_library(PREBUILD_DALI_LIB NAMES dali)\n  find_library(PREBUILD_DALI_OPERATORS_LIB NAMES dali_operators)\n  find_library(PREBUILD_DALI_KERNELES_LIB NAMES dali_kernels)\n  find_library(PREBUILD_DALI_CORE_LIB NAMES dali_core)\n  build_per_python_lib(dali_python\n                       OUTPUT_NAME backend_impl\n                       OUTPUT_DIR ${DALI_LIBRARY_OUTPUT_DIR}\n                       PUBLIC_LIBS ${PREBUILD_DALI_LIB} ${PREBUILD_DALI_OPERATORS_LIB} ${PREBUILD_DALI_KERNELES_LIB} ${PREBUILD_DALI_CORE_LIB} ${CUDART_LIB}\n                       PRIV_LIBS ${CUDA_LIBRARIES} dynlink_cuda\n                       EXCLUDE_LIBS ${exclude_libs}\n                       SRC ${DALI_PYTHON_BACKEND_SRCS})\nelse (PREBUILD_DALI_LIBS)\n  build_per_python_lib(dali_python\n                       OUTPUT_NAME backend_impl\n                       OUTPUT_DIR ${DALI_LIBRARY_OUTPUT_DIR}\n                       PUBLIC_LIBS dali dali_operators dali_kernels dali_core ${CUDART_LIB}\n                       PRIV_LIBS ${CUDA_LIBRARIES} dynlink_cuda\n                       EXCLUDE_LIBS ${exclude_libs}\n                       SRC ${DALI_PYTHON_BACKEND_SRCS})\nendif (PREBUILD_DALI_LIBS)\n```\n\n----------------------------------------\n\nTITLE: Equalize Augmentation Definition\nDESCRIPTION: This code snippet defines the `equalize` augmentation using the `@augmentation` decorator. It applies histogram equalization using `nvidia.dali.fn.experimental.equalize`. It takes the data and an underscore as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@augmentation\ndef equalize(data, _)\n    \"\"\"\n    DALI's equalize follows OpenCV's histogram equalization.\n    The PIL uses slightly different formula when transforming histogram's\n    cumulative sum into lookup table.\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in DALI with CMake\nDESCRIPTION: This CMake snippet uses the `collect_headers` macro to find and aggregate all header files in the current directory and its subdirectories. The results are stored in the `DALI_INST_HDRS` variable, which is accessible in the parent scope. The macro probably takes care of recursively traversing the directories and populating the variable with the paths of the headers.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/distortion/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources and Test Sources using CMake\nDESCRIPTION: This CMake snippet collects header files, source files, and test source files and assigns them to specified variables. The `collect_headers`, `collect_sources`, and `collect_test_sources` are custom CMake functions. `PARENT_SCOPE` ensures the variables are available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/operator/checkpointing/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Download and Prepare Pascal VOC 2012 (Shell)\nDESCRIPTION: This snippet downloads and extracts the Pascal VOC 2012 dataset. It uses `wget` to download the `VOCtrainval_11-May-2012.tar` archive and `tar xf` to extract the contents. It assumes that `wget` and `tar` are available in the environment.  This creates the `VOCdevkit` directory structure expected by the Pascal to TFRecord conversion script.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/efficientdet/dataset/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n!tar xf VOCtrainval_11-May-2012.tar\n```\n\n----------------------------------------\n\nTITLE: Training Loop with JAX pmap (Python)\nDESCRIPTION: This snippet implements the training loop using JAX's `pmap` function.  It iterates through the training data, updates the model using the `update_parallel` function (which synchronizes gradients across devices), and calculates the accuracy on the validation set using one replica of the model.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-multigpu_example.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom model import update_parallel\n\n\nfor epoch in range(num_epochs):\n    for it, batch in enumerate(training_iterator):\n        model = update_parallel(model, batch)\n\n    test_acc = accuracy(\n        jax.tree_map(lambda x: x[0], model), validation_iterator\n    )\n\n    print(f\"Epoch {epoch} sec\")\n    print(f\"Test set accuracy {test_acc}\")\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Building WebDataset Pipeline (Python)\nDESCRIPTION: This snippet demonstrates how to instantiate the `webdataset_pipeline` with specific arguments, such as the paths to the sharded dataset, shuffling settings, padding, reading behavior and cycling options. The `pipeline.build()` method is then called to build the pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/webdataset-externalsource.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipeline = webdataset_pipeline(\n    tar_dataset_paths,   # Paths for the sharded dataset\n    random_shuffle=True, # Random buffered shuffling on\n    pad_last_batch=False, # Last batch is filled to the full size\n    read_ahead=False,\n    cycle=\"raise\")     # All the data is preloaded into the memory\npipeline.build()\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources with CMake\nDESCRIPTION: This snippet utilizes the `collect_sources` CMake function to collect source files. The gathered source file paths are stored in the `DALI_OPERATOR_SRCS` variable, accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/nvjpeg/fused/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Benchmark Source Definition (CMake)\nDESCRIPTION: This snippet defines the source files for the DALI benchmark executable.  It uses `${CMAKE_CURRENT_SOURCE_DIR}` and `${PROJECT_SOURCE_DIR}` to specify the location of the source files. The benchmark sources include various DALI operator benchmarks and utility files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/benchmark/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_BENCHMARK_SRCS\n    \"${PROJECT_SOURCE_DIR}/dali/test/dali_test_config.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/resnet50_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/resnet50_nvjpeg_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/dali_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/file_reader_alexnet_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/decoder_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/displacement_cpu_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/crop_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/crop_mirror_normalize_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/warp_affine_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/transpose_cpu_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/color_twist_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/slice_kernel_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/slice_kernel_bench.cu\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/preemphasis_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/thread_pool_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/normal_distribution_gpu_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/file_reader_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/copy_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/one_hot_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/gaussian_blur_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/flip_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/cast_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/coin_flip_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/transpose_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/file_reader_fast_forward_bench.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/checkpointing_bench.cc\"\n  )\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources with CMake\nDESCRIPTION: This CMake code uses the `collect_sources` function to collect all the source files necessary for the DALI project's kernel. These source files are assigned to the `DALI_KERNEL_SRCS` variable and passed to the parent scope, enabling the use of these source files for building the DALI kernel.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/signal/decibel/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Determining GCC System Include Directories with CMake\nDESCRIPTION: This CMake snippet uses a custom macro `DETERMINE_GCC_SYSTEM_INCLUDE_DIRS` to infer the GCC system include directories. It then transforms these paths into a list of include directives and stores them in the `DEFAULT_COMPILER_INCLUDE` variable, which is used for compilation.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/util/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\n# transform a list of paths into a list of include directives\nDETERMINE_GCC_SYSTEM_INCLUDE_DIRS(\"c++\" \"${CMAKE_CXX_COMPILER}\" \"${CMAKE_CXX_FLAGS}\" INFERED_COMPILER_INCLUDE)\nset(DEFAULT_COMPILER_INCLUDE)\nforeach(incl_dir ${INFERED_COMPILER_INCLUDE})\n  set(DEFAULT_COMPILER_INCLUDE \"${DEFAULT_COMPILER_INCLUDE} -I${incl_dir}\")\nendforeach(incl_dir)\nseparate_arguments(DEFAULT_COMPILER_INCLUDE UNIX_COMMAND  \"${DEFAULT_COMPILER_INCLUDE}\")\n```\n\n----------------------------------------\n\nTITLE: Installing DALI Nightly (CUDA 12.0) via pip\nDESCRIPTION: Installs the latest nightly build of NVIDIA DALI and the TensorFlow plugin built for CUDA 12.0 using pip. This command fetches the package from NVIDIA's nightly build index and upgrades if an older version is present. It requires `tensorflow-gpu` to be installed beforehand.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist/nightly --upgrade nvidia-dali-nightly-cuda120\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist/nightly --upgrade nvidia-dali-tf-plugin-nightly-cuda120\n```\n\n----------------------------------------\n\nTITLE: Building Benchmarks Conditionally in CMake\nDESCRIPTION: This snippet conditionally includes benchmark source files if the `BUILD_BENCHMARK` option is enabled. It uses `file(GLOB)` to find all files ending in `_bench.cc` and adds them to the `DALI_BENCHMARK_SRCS` variable, which is also set in the parent scope. This allows building benchmark executables only when explicitly requested.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_BENCHMARK)\n  # Get all the benchmark srcs\n  file(GLOB tmp *_bench.cc)\n  set(DALI_BENCHMARK_SRCS ${DALI_BENCHMARK_SRCS} ${tmp} PARENT_SCOPE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources for DALI Operators (CMake)\nDESCRIPTION: This line uses a custom CMake function `collect_test_sources` to find and collect all source files related to DALI operator tests, storing their paths into the variable `DALI_OPERATOR_TEST_SRCS`. The `PARENT_SCOPE` option makes the variable available in the parent scope, so the test source files can be used to create test executables.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/random/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding Dimensions with Layout in DALI\nDESCRIPTION: This snippet demonstrates how to add a name to the newly created dimension by passing it to ``dali.newaxis``.  This allows the layout specifier to be updated accordingly.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/indexing.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimage = ... # layout is HWC\nsingle_frame_video = image[dali.newaxis(\"F\")]  # layout is FHWC\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This CMake snippet adds subdirectories 'dct', 'decibel', and 'window' to the current build. If the `BUILD_FFTS` variable is true, it also adds the 'fft' subdirectory. This allows CMake to recursively process the CMakeLists.txt files in those directories and include their targets in the overall build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/signal/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(dct)\nadd_subdirectory(decibel)\nif (BUILD_FFTS)\n  add_subdirectory(fft)\nendif()\nadd_subdirectory(window)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Kernel Sources in CMake\nDESCRIPTION: This snippet employs the `collect_sources` CMake function to collect source files. The collected file list is stored in the `DALI_KERNEL_SRCS` variable, with its scope extended to the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/color_manipulation/debayer/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in DALI with CMake\nDESCRIPTION: This CMake macro collects source files and stores them in the `DALI_SRCS` variable within the parent scope.  It is used for managing the source files required for building the DALI library.  The macro requires CMake and the `collect_sources` function to be defined elsewhere.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/data/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: CMake Project Configuration\nDESCRIPTION: This snippet sets the minimum CMake version, declares the project name with CUDA and CXX support, and specifies the C++ standard to be used. It also sets options to build DALI without dependencies and to create static libraries.\nSOURCE: https://github.com/nvidia/dali/blob/main/qa/TL1_nodeps_build/CMakeLists_submodule.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\nproject(nodeps_test CUDA CXX)\nset(CMAKE_CXX_STANDARD 17)\n\nset(BUILD_DALI_NODEPS ON)\nset(STATIC_LIBS ON)\n```\n\n----------------------------------------\n\nTITLE: Installing dllogger from Git Repository\nDESCRIPTION: This snippet installs the `dllogger` package directly from the specified GitHub repository and version.  It uses pip's VCS support to fetch and install the package from the specified commit hash. The `@v1.0.0` indicates the tag/version and `#egg=dllogger` specifies the name to use when installing the package.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/requirements.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ngit+https://github.com/NVIDIA/dllogger@v1.0.0#egg=dllogger\n```\n\n----------------------------------------\n\nTITLE: Collecting Source and Header Files with CMake Macros\nDESCRIPTION: This snippet uses custom CMake macros to collect header files, source files, and test source files related to DALI operators. The `PARENT_SCOPE` option makes the collected file lists available to the calling CMake scope for use in building the library and running tests.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/debug/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers with CMake\nDESCRIPTION: This snippet uses the `collect_headers` CMake function to gather header files. It stores the collected header file paths in the `DALI_INST_HDRS` variable, making them available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/nvjpeg/fused/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in DALI using CMake\nDESCRIPTION: This CMake command uses the `collect_headers` function (presumably defined elsewhere in the CMake infrastructure) to gather header files and store them in the `DALI_INST_HDRS` variable. The `PARENT_SCOPE` argument makes the variable available to the parent scope, which is typically the calling CMakeLists.txt file. This is used during installation to copy header files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/random/noise/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Creating the Flax model state\nDESCRIPTION: This code initializes the Flax model state. It generates a random number generator (RNG) key, sets the learning rate and momentum, and then calls `create_model_state` to create the model state with the specified parameters. This model state holds the model parameters and optimizer state required for training.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/flax-basic_example.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrng = jax.random.PRNGKey(0)\nrng, init_rng = jax.random.split(rng)\n\nlearning_rate = 0.1\nmomentum = 0.9\n\nmodel_state = create_model_state(init_rng, learning_rate, momentum)\n```\n\n----------------------------------------\n\nTITLE: Pipeline Debug Mode (Python)\nDESCRIPTION: This example demonstrates how to use the experimental pipeline debug mode. The `@nvidia.dali.pipeline.experimental.pipeline_def` decorator is used with `debug=True`.  It shows how to access data inside the pipeline using `.get()` and how to use non-DALI data types directly with DALI operators.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/pipeline.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@nvidia.dali.pipeline.experimental.pipeline_def(debug=True)\ndef my_pipe():\n    data, _ = fn.readers.file(file_root=images_dir)\n    img = fn.decoders.image(data)\n    print(np.array(img.get()[0]))\n    ...\n\n@nvidia.dali.pipeline.experimental.pipeline_def(batch_size=8, debug=True)\ndef my_pipe():\n    img = [np.random.rand(640, 480, 3) for _ in range(8)]\n    output = fn.flip(img)\n    ...\n```\n\n----------------------------------------\n\nTITLE: Training EfficientDet with DALI (GPU)\nDESCRIPTION: This command trains the EfficientDet model using the DALI GPU pipeline. It utilizes multiple GPUs for faster training. Key parameters include the training data file pattern, batch size, number of training steps, and output filename for the trained weights.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/efficientdet/README.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npython3 train.py \\\n            --multi_gpu \\\n            --pipeline dali_gpu \\\n            --epochs 50 \\\n\t    --input_type tfrecord \\\n            --train_file_pattern './tfrecords/train*.tfrecord' \\\n            --batch_size 16 \\\n            --train_steps 2000 \\\n            --output_filename final_weights.h5\n```\n\n----------------------------------------\n\nTITLE: Linking DALI Library Directory\nDESCRIPTION: This CMake command specifies the directory where the DALI library is located. This allows the linker to find the DALI library during the linking stage.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nlink_directories(\"${DALI_LIB_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Test Sources with CMake\nDESCRIPTION: This snippet gathers all `.cc`, `.cu`, and `.h` files in the current directory and adds them to the `DALI_TEST_SRCS` variable.  If `BUILD_CFITSIO` is not set, `cfitsio_test.cc` is excluded. `PARENT_SCOPE` makes the variable available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/test/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB tmp *.cc *.cu *.h)\n  if (NOT BUILD_CFITSIO)\n    list(REMOVE_ITEM tmp \"${CMAKE_CURRENT_SOURCE_DIR}/cfitsio_test.cc\")\n  endif()\n\n  set(DALI_TEST_SRCS ${DALI_TEST_SRCS} ${tmp} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source and Header Files with CMake\nDESCRIPTION: This CMake snippet demonstrates how to collect header, source, and test source files using custom CMake functions. The `collect_headers`, `collect_sources`, and `collect_test_sources` functions are assumed to be defined elsewhere. The results are stored in the specified variables and are accessible in the parent scope. This allows for centralized management of source files for compilation or other purposes.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/filter/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting DALI Source Files with CMake\nDESCRIPTION: This CMake snippet sets the DALI source files using the `set` command. It defines the paths to various C++ source files that are compiled to build the DALI library. These source files implement the core functionality of the library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/util/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_SRCS ${DALI_SRCS}\n  \"${CMAKE_CURRENT_SOURCE_DIR}/file.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/image.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/mmaped_file.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/std_file.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/odirect_file.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/ocv.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/random_crop_generator.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/user_stream.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/numpy.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/uri.cc\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Header and Source Files with CMake\nDESCRIPTION: This CMake snippet uses custom functions to collect header, source, and test source files. The `collect_headers` function populates the `DALI_INST_HDRS` variable, `collect_sources` populates `DALI_SRCS`, and `collect_test_sources` populates `DALI_TEST_SRCS`. The `PARENT_SCOPE` option makes these variables available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/workspace/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Shard Size Calculation Formula\nDESCRIPTION: This formula calculates the size of a specific shard (shard ID `id`) given the total dataset size (`dataset_size`) and the number of shards (`num_shards`). It utilizes the `floor` function to ensure integer values for shard boundaries. This calculation is used when the pipeline stays on the same shard for all epochs.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/advanced_topics_sharding.rst#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nfloor((id + 1) * dataset_size / num_shards) -\n    floor(id * dataset_size / num_shards)\n```\n\n----------------------------------------\n\nTITLE: File reader pipeline (Python)\nDESCRIPTION: This snippet defines a DALI pipeline for reading image files from a directory. It uses `fn.readers.file` to read the data, sharding it across multiple GPUs. The data is then passed to the `common_pipeline` function for image processing.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/pytorch/pytorch-various-readers.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def\ndef file_reader_pipeline(num_gpus):\n    jpegs, labels = fn.readers.file(\n        file_root=image_dir,\n        random_shuffle=True,\n        shard_id=Pipeline.current().device_id,\n        num_shards=num_gpus,\n        name=\"Reader\",\n    )\n\n    return common_pipeline(jpegs, labels)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers and Sources CMake\nDESCRIPTION: Uses custom CMake functions `collect_headers`, `collect_sources`, and `collect_test_sources` to gather header files, source files for operators, and test source files, respectively. The `PARENT_SCOPE` argument makes these collected files available in the parent scope for further processing during the build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Numpy Reader with Relative ROI\nDESCRIPTION: This snippet shows the use of the Numpy reader with ROI extraction, this time using relative coordinates.  The `rel_roi_start` and `rel_roi_end` arguments specify the ROI as a fraction of the total array dimensions.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(batch_size=batch_size, num_threads=3, device_id=0)\ndef pipe_roi2():\n    data = fn.readers.numpy(\n        device=\"cpu\",\n        file_root=data_dir,\n        files=files,\n        rel_roi_start=[0.1, 0.01],\n        rel_roi_end=[0.4, 0.5],\n    )\n    return data\n\n\ndata_roi2 = run(pipe_roi2())\nplot_batch(data_roi2)\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Inclusion (NVOF)\nDESCRIPTION: This snippet conditionally includes the 'optical_flow' subdirectory if the BUILD_NVOF variable is set. This allows for optional inclusion of NVOF-related components in the build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/sequence/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_NVOF)\n  add_subdirectory(optical_flow)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers using CMake\nDESCRIPTION: This CMake snippet defines a macro call to collect header files within the DALI project. It uses a custom function `collect_headers` that populates the `DALI_INST_HDRS` variable in the parent scope with the list of collected header files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/sequence/optical_flow/optical_flow_adapter/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files in DALI Kernel using CMake\nDESCRIPTION: This snippet collects the source files for the DALI kernel using the `collect_sources` CMake function. The collected source files are stored in the `DALI_KERNEL_SRCS` variable, which is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/test/warp_test/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources with CMake\nDESCRIPTION: This CMake function `collect_test_sources` is used to gather test source files and store them in the `DALI_OPERATOR_TEST_SRCS` variable. The `PARENT_SCOPE` option makes the variable available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/paste/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: DALI Pipeline with Constant Wrapper and Explicit Type\nDESCRIPTION: This example shows how to use the DALI `Constant` wrapper with an explicitly specified data type (`UINT8`). It defines a DALI `Constant` with an integer and `DALIDataType.UINT8`, creates a pipeline that multiplies it with a `uint8` tensor, and runs the pipeline, printing the result.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nconstant = Constant(10, types.DALIDataType.UINT8)\npipe = arithmetic_constant_pipeline((lambda x: x * constant), np.uint8)\nbuild_and_run_with_const(pipe, \"*\", constant)\n```\n\n----------------------------------------\n\nTITLE: Convert COCO to TFRecord (Python & Shell)\nDESCRIPTION: This snippet converts the downloaded COCO validation dataset to TFRecord format.  It creates a `tfrecord` directory for storing the output. It then sets the `PYTHONPATH` to include the current directory (`.`) to ensure that the conversion script can be found and executed. The `create_coco_tfrecord.py` script is called with arguments specifying the image directory, annotation file, output file prefix, and the number of shards.  The script relies on `PYTHONPATH` being correctly configured and the existence of `create_coco_tfrecord.py` in the specified location.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/efficientdet/dataset/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n!mkdir tfrecord\n!PYTHONPATH=\".:$PYTHONPATH\"  python dataset/create_coco_tfrecord.py \\\n  --image_dir=val2017 \\\n  --caption_annotations_file=annotations/captions_val2017.json \\\n  --output_file_prefix=tfrecord/val \\\n  --num_shards=32\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers in CMake\nDESCRIPTION: This snippet uses a custom CMake function `collect_headers` to gather the header files for the DALI library. The collected headers are stored in the `DALI_INST_HDRS` variable, and the `PARENT_SCOPE` option makes the variable available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/erase/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Test Sources with CMake\nDESCRIPTION: This snippet demonstrates how to collect header files, source files, and test source files using custom CMake functions. The `collect_headers`, `collect_sources`, and `collect_test_sources` functions populate the `DALI_INST_HDRS`, `DALI_KERNEL_SRCS`, and `DALI_KERNEL_TEST_SRCS` variables, respectively.  The `PARENT_SCOPE` option makes these variables accessible in the parent directory's CMake context.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/resample/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Install Simpleaudio Dependency Python\nDESCRIPTION: Installs the `simpleaudio` package using pip. This is used for an external decoding of the audio file to verify DALI's output. This snippet should only be executed if `simpleaudio` isn't installed.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/audio_processing/audio_decoder.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport sys\n\n!{sys.executable} -m pip install simpleaudio\n```\n\n----------------------------------------\n\nTITLE: Translate Y Augmentation Definition\nDESCRIPTION: This code snippet defines the `translate_y` augmentation using the `@augmentation` decorator. It applies a translation along the y-axis with a shape-relative offset using `nvidia.dali.fn.transforms.translation` and `nvidia.dali.fn.warp_affine`. It accepts the data, relative offset, shape of the image, fill_value, and interpolation type as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0., 1.), randomly_negate=True, ...)\ndef translate_y(data, rel_offset, shape, fill_value=128, interp_type=None)\n```\n\n----------------------------------------\n\nTITLE: Print Stacked Tensor (Outermost Axis)\nDESCRIPTION: This snippet prints the stacked tensor with the new axis inserted at the outermost position (axis=0) and its shape. It retrieves the output from the DALI pipeline (`pipe_stack`) and prints the first element of the output tensor, along with its shape, using `o[0].at(0)`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/tensor_join.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Stacking - insert outermost axis:\")\nprint(o[0].at(0))\nprint(\"Shape: \", o[0].at(0).shape)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries\nDESCRIPTION: This code snippet imports necessary libraries for video processing, including DALI (nvidia.dali), numpy for numerical operations, os for file system interactions, tempfile for creating temporary files, and PIL (if available) for image handling.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/video/video_file_list_outputs.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nimport tempfile\nfrom nvidia.dali import pipeline_def\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\n\ntry:\n    from PIL import Image\n\n    has_PIL = True\nexcept ImportError:\n    has_PIL = False\n```\n\n----------------------------------------\n\nTITLE: Building DALI with CUDA Version - Bash\nDESCRIPTION: This code snippet demonstrates how to specify the CUDA version when building DALI using the `build.sh` script. It sets the `CUDA_VERSION` environment variable before executing the script, allowing you to build DALI with a specific CUDA toolkit version. The resulting Python wheel will be placed in the DALI_root/wheelhouse directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/compilation.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VERSION=Z ./build.sh\n```\n\n----------------------------------------\n\nTITLE: Collecting Files with CMake Functions\nDESCRIPTION: This snippet invokes custom CMake functions to collect header, source, and test source files. `collect_headers` gathers header files, `collect_sources` gathers source files, and `collect_test_sources` gathers test source files. The collected file lists are stored in CMake variables within the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/transpose/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers with CMake\nDESCRIPTION: This CMake command collects the header files for the NVIDIA DALI library. The `DALI_INST_HDRS` variable will contain the list of header files. `PARENT_SCOPE` ensures the variable is accessible in the parent scope of the current CMake context.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/cache/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: Collects header files for the DALI installation and makes them available in the parent scope. This enables other parts of the CMake project to access and use these header files during compilation.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/nvjpeg/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: GTest Test Suite Naming Example C++\nDESCRIPTION: Illustrates correct and incorrect naming conventions for GTest test suites and test cases in DALI.  Names of TestSuites should start with a capital letter and end with `Test`. Additionally, both suite and case name mustn't contain underscores (`_`).\nSOURCE: https://github.com/nvidia/dali/blob/main/STYLE_GUIDE.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nTEST(MatTest, IdentityMatrix) {}  // OK\nTEST_F(PregnancyTest, AlwaysPositive) {}  // OK\nTYPED_TEST(CannyOperatorTest, EmptyImage) {}  // OK\nTYPED_TEST_SUITE(Skittles, InTheSky);  // Wrong! Should be \"SkittlesTest\"\nINSTANTIATE_TYPED_TEST_SUITE_P(Integral, HelloTest, IntegralTypes);  // OK. \"Integral\" is a prefix for type-parameterized test suite\n```\n\n----------------------------------------\n\nTITLE: Define Training Parameters\nDESCRIPTION: Defines the training parameters, including the path to the MNIST dataset, batch size, dropout rate, image size, number of classes, hidden layer size, number of epochs, iterations per epoch, and the number of GPUs to use.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset-multigpu.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Path to MNIST dataset\ndata_path = os.path.join(os.environ[\"DALI_EXTRA_PATH\"], \"db/MNIST/training/\")\n\nBATCH_SIZE = 64\nDROPOUT = 0.2\nIMAGE_SIZE = 28\nNUM_CLASSES = 10\nHIDDEN_SIZE = 128\nEPOCHS = 5\nITERATIONS = 100\nNUM_DEVICES = 2\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in DALI\nDESCRIPTION: This snippet adds several subdirectories to the build process. Each subdirectory likely contains source code and CMakeLists.txt files defining a specific module or functionality, such as color manipulation or convolution. This allows for modular organization of the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(color_manipulation)\nadd_subdirectory(convolution)\nadd_subdirectory(geom)\nadd_subdirectory(jpeg)\nadd_subdirectory(pointwise)\nadd_subdirectory(resample)\nadd_subdirectory(paste)\nadd_subdirectory(structure)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Test Sources with CMake\nDESCRIPTION: This snippet uses custom CMake functions to collect header, source, and test source files into lists. These lists are then available in the parent scope, allowing other CMake scripts to access and use these files for building the project.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/jpeg/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: Collects headers for DALI installation and makes them available in the parent scope. This allows other parts of the project to access and use the collected header files during compilation. 'DALI_INST_HDRS' will contain the list of header files and 'PARENT_SCOPE' ensures the variables are available in the parent directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/signal/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Replicating Model for pmap (Python)\nDESCRIPTION: This snippet initializes the model and replicates it across all available GPUs using `jax.tree_map` and `jnp.array`. This is necessary for training with JAX's `pmap` function, as each device needs its own copy of the model.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/jax-multigpu_example.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom model import init_model, accuracy\n\n\nmodel = init_model()\nmodel = jax.tree_map(lambda x: jnp.array([x] * jax.device_count()), model)\n```\n\n----------------------------------------\n\nTITLE: Specifying Output Data Type for Reductions in DALI (Python)\nDESCRIPTION: This code snippet demonstrates how to specify the desired output data type for reductions using the `dtype` argument. It performs a sum reduction and specifies the output type to be `INT64` and `FLOAT` respectively.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reductions.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\nwith pipe:\n    input = fn.external_source(source=get_batch, dtype=types.INT32)\n    sum_int_64 = fn.reductions.sum(input, dtype=types.INT64)\n    sum_float = fn.reductions.sum(input, dtype=types.FLOAT)\n\n    pipe.set_outputs(sum_int_64, sum_float)\n\nrun_and_print(pipe)\n```\n\n----------------------------------------\n\nTITLE: Finding GCC System Include Directories using CMake\nDESCRIPTION: This CMake command determines the system include directories used by the GCC compiler. It leverages CMake's `DETERMINE_GCC_SYSTEM_INCLUDE_DIRS` function to infer the compiler include paths based on the C++ compiler and flags. The result is stored in the `INFERRED_COMPILER_INCLUDE` variable.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/nvimgcodec/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nDETERMINE_GCC_SYSTEM_INCLUDE_DIRS(\"c++\" \"${CMAKE_CXX_COMPILER}\" \"${CMAKE_CXX_FLAGS}\" INFERRED_COMPILER_INCLUDE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers and Sources for DALI Kernel\nDESCRIPTION: This snippet uses custom CMake macros to collect headers and sources for the DALI kernel.  `collect_headers` gathers header files, `collect_sources` collects source files, and `collect_test_sources` collects test source files. The `list(FILTER)` command excludes specific files (cufft_wrap.cc) from the `DALI_KERNEL_SRCS` list.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/signal/fft/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n\nlist(FILTER DALI_KERNEL_SRCS EXCLUDE REGEX \".*cufft_wrap.cc\")\nset(DALI_KERNEL_SRCS ${DALI_KERNEL_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files in DALI with CMake\nDESCRIPTION: This CMake code snippet utilizes the `collect_sources` function to collect all source files in the DALI project. The gathered source files are stored in the `DALI_KERNEL_SRCS` variable and are accessible in the parent scope. The collected sources are then used during the project's compilation phase.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/structure/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers and Sources\nDESCRIPTION: This snippet collects header files and source files needed for building the DALI Python backend. `collect_headers` gathers header files and stores them in `DALI_INST_HDRS`. `collect_sources` gathers source files for the Python backend and stores them in `DALI_PYTHON_BACKEND_SRCS`.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/python/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_PYTHON_BACKEND_SRCS)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Source Files with CMake\nDESCRIPTION: This CMake snippet uses the `collect_test_sources` function to identify all source files related to testing. The identified test sources are stored in the `DALI_TEST_SRCS` variable, which is accessible in the parent scope. This variable is typically used with a testing framework (like Google Test) to create test executables and run automated tests for the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/graph/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Project Definition\nDESCRIPTION: This snippet defines the project name as `custom_dummy_plugin` and specifies the supported languages: CUDA, C++, and C. This allows CMake to properly configure the project for these languages.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/customdummy/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nproject(custom_dummy_plugin LANGUAGES CUDA CXX C)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files with CMake\nDESCRIPTION: This snippet uses a custom CMake macro `collect_sources` to gather all source files for the DALI kernel. The collected source files are then stored in the `DALI_KERNEL_SRCS` variable and made available in the parent scope. This allows other parts of the CMake configuration to access and use these source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/test/resampling_test/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Filtering Source Files in CMake\nDESCRIPTION: Filters the DALI operator source files to exclude files matching the specified regular expression (in this case, '.*nvjpeg_wrap.cc'). This prevents these files from being included in the build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/nvjpeg/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nlist(FILTER DALI_OPERATOR_SRCS EXCLUDE REGEX \".*nvjpeg_wrap.cc\")\nset(DALI_OPERATOR_SRCS ${DALI_OPERATOR_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: CMake Build Configuration for DALI Plugin\nDESCRIPTION: This CMakeLists.txt file configures the build process for a DALI plugin. It finds the CUDA and DALI libraries, sets compile flags, and creates a shared library (plugin) that can be loaded by DALI at runtime. This configuration is specific to building a custom DALI operator.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/create_a_custom_operator.ipynb#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18)\nproject(dali_customdummy)\n\nset(CMAKE_CXX_STANDARD 14)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nfind_package(CUDA REQUIRED)\n\n# adjust if DALI is not installed in /opt/dali\n# set(DALI_DIR /opt/dali)\nfind_package(DALI REQUIRED)\n\ninclude_directories(${CUDA_INCLUDE_DIRS} ${DALI_INCLUDE_DIRS})\n\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${DALI_CXX_FLAGS}\")\n\nCUDA_ADD_LIBRARY(dali_customdummy SHARED\n    customdummy/dummy.cc\n    customdummy/dummy.cu\n)\n\ntarget_link_libraries(dali_customdummy ${DALI_LIBRARIES} ${CUDA_LIBRARIES})\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files (CMake)\nDESCRIPTION: This snippet collects all header files and makes them available in the parent scope. This ensures that header files can be used by other parts of the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/loader/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Copying Post-Build Files\nDESCRIPTION: This snippet copies various files after the build process, including Python modules, license files, and other resources. `copy_post_build` is a custom function used to perform the copying operation. These files are necessary for the distribution of DALI's Python backend.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/python/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncopy_post_build(dali_python \"${PROJECT_SOURCE_DIR}/dali/python/nvidia\" \"${PROJECT_BINARY_DIR}/dali/python\")\nconfigure_file(\"${PROJECT_SOURCE_DIR}/dali/python/__init__.py.in\" \"${PROJECT_BINARY_DIR}/dali/python/nvidia/dali/__init__.py\")\nconfigure_file(\"${PROJECT_SOURCE_DIR}/dali/python/setup.py.in\" \"${PROJECT_BINARY_DIR}/stage/setup.py\")\ncopy_post_build(dali_python \"${PROJECT_BINARY_DIR}/stage/setup.py\" \"${PROJECT_BINARY_DIR}/dali/python\")\ncopy_post_build(dali_python \"${PROJECT_SOURCE_DIR}/dali/python/MANIFEST.in\" \"${PROJECT_BINARY_DIR}/dali/python\")\ncopy_post_build(dali_python \"${PROJECT_SOURCE_DIR}/tools/rec2idx.py\" \"${PROJECT_BINARY_DIR}/dali/python\")\ncopy_post_build(dali_python \"${PROJECT_SOURCE_DIR}/tools/tfrecord2idx\" \"${PROJECT_BINARY_DIR}/dali/python\")\ncopy_post_build(dali_python \"${PROJECT_SOURCE_DIR}/tools/wds2idx.py\" \"${PROJECT_BINARY_DIR}/dali/python\")\ncopy_post_build(dali_python \"${PROJECT_SOURCE_DIR}/Acknowledgements.txt\" \"${PROJECT_BINARY_DIR}/dali/python/nvidia/dali\")\ncopy_post_build(dali_python \"${PROJECT_SOURCE_DIR}/COPYRIGHT\" \"${PROJECT_BINARY_DIR}/dali/python/nvidia/dali\")\ncopy_post_build(dali_python \"${PROJECT_SOURCE_DIR}/LICENSE\" \"${PROJECT_BINARY_DIR}/dali/python/nvidia/dali\")\ncopy_post_build(dali_python \"${PROJECT_SOURCE_DIR}/internal_tools/find_dali.cmake\" \"${PROJECT_BINARY_DIR}/dali/cmake\")\n```\n\n----------------------------------------\n\nTITLE: Inference Usage\nDESCRIPTION: This shows the usage for the inference script.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/yolov4/readme.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nusage: main.py infer [-h] image [--weights WEIGHTS] [--classes CLASSES]\n                     [--output OUTPUT]\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files with CMake\nDESCRIPTION: This CMake function call collects all header files and stores them in the `DALI_INST_HDRS` variable. The `PARENT_SCOPE` argument ensures that the variable is accessible in the parent scope, allowing other CMake scripts to utilize the collected header files for compilation or other purposes.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/common/join/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Linking optional dependencies\nDESCRIPTION: This snippet conditionally links the `dali` library with optional dependencies such as NVML, CUFILE and AWSSDK, based on whether the corresponding build options (`BUILD_NVML`, `BUILD_CUFILE`, `BUILD_AWSSDK`) are enabled.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_NVML)\n  target_link_libraries(dali PRIVATE dynlink_nvml)\nendif(BUILD_NVML)\n\nif (BUILD_CUFILE)\n  target_link_libraries(dali PRIVATE dynlink_cufile)\nendif()\n\nif (BUILD_AWSSDK)\n  target_include_directories(dali PRIVATE ${AWSSDK_INCLUDE_DIR})\n  target_link_libraries(dali PRIVATE ${AWSSDK_LIBRARIES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting up test sources and language properties in CMake\nDESCRIPTION: This snippet finds all source files (.cc, .cu, .h) in the current directory and adds them to the `DALI_TEST_SRCS` variable. It also adjusts the language property of the source files, ensuring proper compilation.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/test/operators/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB tmp *.cc *.cu *.h)\nadjust_source_file_language_property(\"${tmp}\")\nset(DALI_TEST_SRCS ${DALI_TEST_SRCS} ${tmp} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Replicating the Flax model state for multi-GPU training\nDESCRIPTION: This code replicates the Flax model state across multiple GPUs using `jax.pmap`. It initializes a random number generator key, sets the learning rate and momentum, and then uses `jax.pmap` to call `create_model_state` on each device. This ensures that each GPU has its own copy of the model state, enabling parallel training.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/flax-basic_example.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrng = jax.random.PRNGKey(0)\nrng, init_rng = jax.random.split(rng)\n\nlearning_rate = 0.1\nmomentum = 0.9\n\nmodel_state = jax.pmap(create_model_state, static_broadcasted_argnums=(1, 2))( \n    jax.random.split(init_rng, jax.device_count()), learning_rate, momentum\n)\n```\n\n----------------------------------------\n\nTITLE: Translate X No Shape Augmentation Definition\nDESCRIPTION: This code snippet defines the `translate_x_no_shape` augmentation using the `@augmentation` decorator. It applies a translation along the x-axis with an absolute offset using `nvidia.dali.fn.transforms.translation` and `nvidia.dali.fn.warp_affine`. It accepts the data, offset, fill_value, and interpolation type as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0, 250), randomly_negate=True, ...)\ndef translate_x_no_shape(data, offset, fill_value=128, interp_type=None)\n```\n\n----------------------------------------\n\nTITLE: Translate Y No Shape Augmentation Definition\nDESCRIPTION: This code snippet defines the `translate_y_no_shape` augmentation using the `@augmentation` decorator. It applies a translation along the y-axis with an absolute offset using `nvidia.dali.fn.transforms.translation` and `nvidia.dali.fn.warp_affine`. It accepts the data, offset, fill_value, and interpolation type as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0, 250), randomly_negate=True, ...)\ndef translate_y_no_shape(data, offset, fill_value=128, interp_type=None)\n```\n\n----------------------------------------\n\nTITLE: Printing Training Accuracy from Logs\nDESCRIPTION: This code snippet iterates through the log files in the specified directory and calls the `print_logs` function to print the training accuracy from each file. It assumes that the log files are located in the `/tmp/dali_pax_logs/summaries/train/` directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/pax-basic_example.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor file in os.listdir(\"/tmp/dali_pax_logs/summaries/train/\"):\n    print_logs(os.path.join(\"/tmp/dali_pax_logs/summaries/train/\", file))\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Include Directories CMake\nDESCRIPTION: This snippet transforms a list of paths (`INFERED_COMPILER_INCLUDE`) into a list of include directives (-I<path>). These are then stored in the `DEFAULT_COMPILER_INCLUDE` variable, which is later used as part of the compilation command for the nvjpeg stub.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/nvjpeg/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(DEFAULT_COMPILER_INCLUDE)\nforeach(incl_dir ${INFERED_COMPILER_INCLUDE})\n  set(DEFAULT_COMPILER_INCLUDE \"${DEFAULT_COMPILER_INCLUDE} -I${incl_dir}\")\nendforeach(incl_dir)\nseparate_arguments(DEFAULT_COMPILER_INCLUDE UNIX_COMMAND  \"${DEFAULT_COMPILER_INCLUDE}\")\n```\n\n----------------------------------------\n\nTITLE: Build NaiveHistogram shared object\nDESCRIPTION: This code snippet builds the `libnaivehistogram.so` shared object using CMake. It first changes the directory to `naive_histogram`, creates a `build` directory, enters it, runs `cmake ..`, and then uses `make -j` to compile the project.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cd naive_histogram\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make -j\n```\n\n----------------------------------------\n\nTITLE: Project Definition in CMake\nDESCRIPTION: This line defines the project 'dali_kernels' and specifies the languages used (CUDA, C++, C).\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(dali_kernels CUDA CXX C)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for WebDataset Loader (CMake)\nDESCRIPTION: This snippet adds the `webdataset` subdirectory to the build process if the `BUILD_LIBTAR` flag is enabled. This includes the source code necessary for the webdataset loader functionality.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/loader/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_LIBTAR)\n  add_subdirectory(webdataset)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Print Concatenated Tensor (Inner Axis)\nDESCRIPTION: This snippet prints the concatenated tensor along the inner axis (axis=2) and its shape. It retrieves the output from the DALI pipeline (`pipe_cat`) and prints the first element of the output tensor, along with its shape, using `o[2].at(0)`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/tensor_join.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Concatenation along inner axis:\")\nprint(o[2].at(0))\nprint(\"Shape: \", o[2].at(0).shape)\n```\n\n----------------------------------------\n\nTITLE: Conditional Inclusion of S3 Discover Files (CMake)\nDESCRIPTION: This snippet conditionally includes the S3 discover files source file (`discover_files_s3.cc`) if the `BUILD_AWSSDK` flag is enabled. This allows DALI to discover files on Amazon S3 when AWSSDK is available.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/loader/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_AWSSDK)\n  set(DALI_OPERATOR_SRCS ${DALI_OPERATOR_SRCS}\n    \"${CMAKE_CURRENT_SOURCE_DIR}/discover_files_s3.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Kernel Headers in CMake\nDESCRIPTION: This snippet uses the `collect_headers` CMake function to collect header files. The results are stored in the `DALI_INST_HDRS` variable, which is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/color_manipulation/debayer/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Version\nDESCRIPTION: Sets the CUDA version, checking if it's provided and throwing an error if not.  It uses a `CACHE STRING` so it can be configured via the command line. If no version is provided, cmake will halt with an error message.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali_tf_plugin/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nSET(CUDA_VERSION \"\" CACHE STRING \"CUDA version\")\nif (\"${CUDA_VERSION}\" STREQUAL \"\")\n  message(FATAL_ERROR \"CUDA version not provided. Pass CUDA version to cmake (e.g -DCUDA_VERSION:STRING=\\\"10.1\\\")\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Benchmark EfficientNet with DALI Proxy and AutoAugment\nDESCRIPTION: This command benchmarks EfficientNet training using DALI Proxy for data loading with AutoAugment enabled. The `data-backend` flag is set to `dali_proxy` and the `automatic-augmentation` flag is set to `autoaugment`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# DALI proxy with AutoAugment\npython multiproc.py --nproc_per_node 8 ./main.py --amp --static-loss-scale 128 \\\n                    --batch-size 128 --epochs 4 --no-checkpoints --training-only \\\n                    --data-backend dali_proxy --automatic-augmentation autoaugment \\\n                    --workspace $RESULT_WORKSPACE \\\n                    --report-file bench_report_dali_proxy_aa.json $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources with CMake\nDESCRIPTION: This CMake directive uses the `collect_test_sources` function to gather test source files related to DALI operators. `DALI_OPERATOR_TEST_SRCS` is the variable to store collected test source file paths. The `PARENT_SCOPE` argument ensures that the collected test sources are available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/io/file/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources for DALI Operator Test - CMake\nDESCRIPTION: This snippet collects test-related source files and appends them to the `DALI_OPERATOR_TEST_SRCS` variable. The `PARENT_SCOPE` option makes the updated variable available in the calling scope, allowing these test sources to be used in the compilation of the `dali_operator_test` target.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/c_api_2/op_test/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Resuming Training from Checkpoint\nDESCRIPTION: This command resumes training from a previously saved checkpoint. It uses the ``-w`` flag to specify the path to the checkpoint file.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/yolov4/readme.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n-w /ckpt/epoch_3.h5\n```\n\n----------------------------------------\n\nTITLE: Setting target properties for library output directory in CMake\nDESCRIPTION: This snippet sets the `LIBRARY_OUTPUT_DIRECTORY` property for the `testoperatorplugin` target.  It specifies that the generated library should be placed in the `TEST_BINARY_DIR` directory, ensuring that the library is placed in the correct location for testing.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/test/operators/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${lib_name} PROPERTIES LIBRARY_OUTPUT_DIRECTORY ${TEST_BINARY_DIR})\n```\n\n----------------------------------------\n\nTITLE: Appending Operator Sources in CMake\nDESCRIPTION: Appends source files for file and numpy reader operators to the `DALI_OPERATOR_SRCS` list. These source files contain the implementation of the respective operators.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/file_reader_op.cc\")\nlist(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/numpy_reader_op.cc\")\n```\n\n----------------------------------------\n\nTITLE: Setting GCC System Include Directories with CMake\nDESCRIPTION: This CMake code snippet uses the `DETERMINE_GCC_SYSTEM_INCLUDE_DIRS` macro to determine the GCC system include directories for C++.  It then converts these directories into a list of `-I` include directives stored in the `DEFAULT_COMPILER_INCLUDE` variable for use in compilation. The `separate_arguments` function is used to ensure the arguments are properly quoted.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/util/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nDETERMINE_GCC_SYSTEM_INCLUDE_DIRS(\"c++\" \"${CMAKE_CXX_COMPILER}\" \"${CMAKE_CXX_FLAGS}\" INFERED_COMPILER_INCLUDE)\nset(DEFAULT_COMPILER_INCLUDE)\nforeach(incl_dir ${INFERED_COMPILER_INCLUDE})\n  set(DEFAULT_COMPILER_INCLUDE \"${DEFAULT_COMPILER_INCLUDE} -I${incl_dir}\")\nendforeach(incl_dir)\nseparate_arguments(DEFAULT_COMPILER_INCLUDE UNIX_COMMAND  \"${DEFAULT_COMPILER_INCLUDE}\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers for DALI (CMake)\nDESCRIPTION: This line uses a custom CMake function `collect_headers` to find and collect all header files within the DALI project and stores their paths into the variable `DALI_INST_HDRS`. The `PARENT_SCOPE` option makes the variable available in the parent scope, allowing other CMake scripts to access the list of header files for installation or dependency tracking.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/random/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files in CMake\nDESCRIPTION: Collects header files and source files for the DALI core library and its tests.  Uses custom CMake functions `collect_headers`, `collect_sources`, and `collect_test_sources`.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_CORE_SRCS)\ncollect_test_sources(DALI_CORE_TEST_SRCS)\n```\n\n----------------------------------------\n\nTITLE: Custom DataLoader with DALI Server\nDESCRIPTION: This example shows how to integrate DALI with a custom DataLoader. It calls the `dali_server.produce_data()` method explicitly to replace `DALIOutputBatchRef` instances with the actual processed data.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/plugins/pytorch_dali_proxy.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwith dali_proxy.DALIServer(pipeline) as dali_server:\n   dataset = CustomDataset(dali_server.proxy, data=images)\n   loader = MyCustomDataloader(...)\n   for data, _ in loader:\n      # Replaces instances of ``DALIOutputBatchRef`` with actual data\n      processed_data = dali_server.produce_data(data)\n      print(processed_data.shape)  # data is now ready\n```\n\n----------------------------------------\n\nTITLE: Creating Caffe/Caffe2 Protobuf Libraries with CMake\nDESCRIPTION: This snippet creates object libraries (`CAFFE_PROTO` and `CAFFE2_PROTO`) from the generated C++ source and header files created by `protobuf_generate_cpp`. These libraries can then be linked into other DALI components or applications.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/parser/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(CAFFE_PROTO OBJECT ${CAFFE_PROTO_HEADERS} ${CAFFE_PROTO_SRCS})\nadd_library(CAFFE2_PROTO OBJECT ${CAFFE2_PROTO_HEADERS} ${CAFFE2_PROTO_SRCS})\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers and Sources with CMake\nDESCRIPTION: This CMake snippet collects header files, source files, and test source files for the DALI project. It utilizes custom CMake functions `collect_headers`, `collect_sources`, and `collect_test_sources` to manage the project structure and compilation process. The `PARENT_SCOPE` argument ensures that the variables are accessible in the parent CMakeLists.txt files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/input/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Project Definition and Subdirectory Inclusion in CMake\nDESCRIPTION: Defines the project name and supported languages (CUDA, CXX, C). Includes subdirectories for memory management (mm), operating system (os), and execution (exec) components.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(dali_core CUDA CXX C)\n\nadd_subdirectory(mm)\nadd_subdirectory(os)\nadd_subdirectory(exec)\n```\n\n----------------------------------------\n\nTITLE: Run the Pipeline with Scalar Broadcasting\nDESCRIPTION: Instantiates, builds, and runs the `pipeline_4` which uses broadcasting to add a batch of scalars to a batch of tensors.  The `run()` method returns the output tensors.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npipe = pipeline_4()\npipe.build()\nout = pipe.run()\n```\n\n----------------------------------------\n\nTITLE: Inferring and Setting Compiler Include Directories in CMake\nDESCRIPTION: This snippet determines GCC system include directories, formats them into include directives, and stores them for later use.  This is used to ensure correct compilation even when the environment is not standard.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nDETERMINE_GCC_SYSTEM_INCLUDE_DIRS(\"c++\" \"${CMAKE_CXX_COMPILER}\" \"${CMAKE_CXX_FLAGS}\" INFERED_COMPILER_INCLUDE)\nset(DEFAULT_COMPILER_INCLUDE)\nforeach(incl_dir ${INFERED_COMPILER_INCLUDE})\n  set(DEFAULT_COMPILER_INCLUDE \"${DEFAULT_COMPILER_INCLUDE} -I${incl_dir}\")\nendforeach(incl_dir)\nseparate_arguments(DEFAULT_COMPILER_INCLUDE UNIX_COMMAND  \"${DEFAULT_COMPILER_INCLUDE}\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: Collects the installation headers and makes them available in the parent scope. The comment indicates that only supported headers will be included.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE) # TODO (ONLY SUPPORTED ONES)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Source Files in DALI Kernel using CMake\nDESCRIPTION: This snippet collects the test source files for the DALI kernel using the `collect_test_sources` CMake function. The collected test source files are stored in the `DALI_KERNEL_TEST_SRCS` variable, which is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/test/warp_test/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Core Sources with CMake\nDESCRIPTION: This snippet employs the `collect_sources` function to collect source files related to the DALI core. The `DALI_CORE_SRCS` variable will hold the list of collected source files. `PARENT_SCOPE` makes this variable available in the parent scope where this CMake code is executed.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/exec/tasking/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_CORE_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in DALI with CMake\nDESCRIPTION: This CMake snippet collects source files and makes them available within the parent scope. It uses the `collect_sources` function, which is presumably defined elsewhere in the DALI build system. `DALI_KERNEL_SRCS` is the variable that will store the collected source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/normalize/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Test Sources with CMake\nDESCRIPTION: This snippet employs the `collect_test_sources` CMake function to identify and store the source files for DALI's core unit tests. The identified test source files are saved in the `DALI_CORE_TEST_SRCS` variable and made accessible to the parent scope for building and executing tests.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/mm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_CORE_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Kernel Test Sources with CMake\nDESCRIPTION: This CMake command collects test source files for the DALI kernel and makes them available in the parent scope. `DALI_KERNEL_TEST_SRCS` is the variable that stores the collected test source files. Using `PARENT_SCOPE` makes the collected sources available in the scope that called the macro.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/signal/window/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional NVCOMP Subdirectory Inclusion CMake\nDESCRIPTION: Conditionally includes the `inflate` subdirectory if the `BUILD_NVCOMP` variable is set. This likely corresponds to an inflate implementation using NVIDIA's compression library. No specific inputs or outputs other than the presence or absence of the inflate component in the build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_NVCOMP)\n  add_subdirectory(inflate)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Appending Header File to DALI_INST_HDRS with CMake\nDESCRIPTION: This snippet appends the `video_decoder_base.h` header file to the `DALI_INST_HDRS` list. This list is presumably used to track the header files that need to be installed or otherwise processed as part of the DALI build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/decoder/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND DALI_INST_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/video_decoder_base.h\")\n```\n\n----------------------------------------\n\nTITLE: BibTeX entry for FlowNet 2.0\nDESCRIPTION: BibTeX entry for citing the paper \"FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\".\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/video_superres/README.rst#_snippet_5\n\nLANGUAGE: BibTeX\nCODE:\n```\n@InProceedings{IMKDB17,\n author       = \"E. Ilg and N. Mayer and T. Saikia and M. Keuper and A.\n Dosovitskiy and T. Brox\",\n title        = \"FlowNet 2.0: Evolution of Optical Flow Estimation with\n Deep Networks\",\n booktitle    = \"IEEE Conference on Computer Vision and Pattern Recognition\n (CVPR)\",\n month        = \"Jul\",\n year         = \"2017\",\n url          = \"http://lmb.informatik.uni-freiburg.de//Publications/2017/IMKDB17\"\n}\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Kernel Sources in CMake\nDESCRIPTION: This snippet uses a custom CMake function `collect_sources` to gather the source files for the DALI kernel. The collected sources are stored in the `DALI_KERNEL_SRCS` variable, and the `PARENT_SCOPE` option ensures the variable is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/erase/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation for CVCUDA in CMake\nDESCRIPTION: This CMake code snippet conditionally includes the 'experimental' subdirectory based on the value of the `BUILD_CVCUDA` variable. If `BUILD_CVCUDA` is true, the experimental features are built.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/resize/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_CVCUDA)\n  add_subdirectory(experimental)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: Adds the resampling_test and warp_test directories to the current CMake build process. This allows CMake to find and process the CMakeLists.txt files within these subdirectories, incorporating their targets and build instructions into the overall project build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/test/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(resampling_test)\nadd_subdirectory(warp_test)\n```\n\n----------------------------------------\n\nTITLE: Executing Python to Get DALI Compile Flags\nDESCRIPTION: This CMake command executes a Python script to retrieve the DALI compile flags using `nvidia.dali.sysconfig.get_compile_flags()`. The output is stored in the `DALI_COMPILE_FLAGS` variable, and whitespace is stripped from the variable.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nexecute_process(\n        COMMAND python -c \"import nvidia.dali as dali; print(\\\" \\\".join(dali.sysconfig.get_compile_flags()))\\\"\")\n        OUTPUT_VARIABLE DALI_COMPILE_FLAGS)\nstring(STRIP ${DALI_COMPILE_FLAGS} DALI_COMPILE_FLAGS)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files for DALI in CMake\nDESCRIPTION: This line uses a custom CMake function (collect_sources) to gather all source files for the DALI project. The collected sources are stored in the DALI_SRCS variable in the parent scope, making them available for compilation.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/c_api_2/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Compiling DALI within Docker for Jetson\nDESCRIPTION: This command runs a Docker container created in the previous step, mounting the current DALI source directory to the container.  This allows the build process to access the DALI source code and output the compiled wheel to the wheelhouse directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/compilation.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -v $(pwd):/dali nvidia/dali:builder_aarch64-linux\n```\n\n----------------------------------------\n\nTITLE: Create a new DALI Pipeline instance\nDESCRIPTION: This code creates a new instance of the DALI `Pipeline` class, which will be used to deserialize the serialized pipeline. The same `batch_size`, `num_threads`, `device_id`, and `seed` are used to ensure consistency with the original pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/advanced/serialization.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipe2 = Pipeline(batch_size=batch_size, num_threads=2, device_id=0, seed=12)\n```\n\n----------------------------------------\n\nTITLE: Configuring Files\nDESCRIPTION: Configures the `setup.py.in` and `__init__.py.in` files using CMake's `configure_file` command.  The input files are processed, replacing variables with their current values, and the output files are written to the build directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali_tf_plugin/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nconfigure_file(\"${PROJECT_SOURCE_DIR}/setup.py.in\" \"${PROJECT_BINARY_DIR}/setup.py\")\nconfigure_file(\"${PROJECT_SOURCE_DIR}/nvidia/dali_tf_plugin/__init__.py.in\" \"${PROJECT_BINARY_DIR}/nvidia/dali_tf_plugin/__init__.py\")\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Test Sources using CMake\nDESCRIPTION: This CMake snippet uses a custom CMake macro call to collect the C++ source files for testing DALI operators. The `collect_test_sources` function populates the `DALI_OPERATOR_TEST_SRCS` variable in the parent scope with the list of collected test source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/sequence/optical_flow/optical_flow_adapter/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional Operator Source Appending (CFITSIO) in CMake\nDESCRIPTION: Conditionally appends source files for FITS reader operators to the `DALI_OPERATOR_SRCS` list, based on the `BUILD_CFITSIO` build flag. This allows for including FITS reader support only when the CFITSIO library is available.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_CFITSIO)\n  list(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/fits_reader_op.cc\")\n  list(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/fits_reader_gpu_op.cu\")\nendif(BUILD_CFITSIO)\n```\n\n----------------------------------------\n\nTITLE: Conditional Inclusion of GPU-Accelerated Numpy Loader (CMake)\nDESCRIPTION: This snippet conditionally includes the GPU-accelerated numpy loader source file (`numpy_loader_gpu.cc`) if the `BUILD_CUFILE` flag is enabled. This allows DALI to leverage GPU acceleration for loading numpy arrays when CUFILE is available.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/loader/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_CUFILE)\n  set(DALI_OPERATOR_SRCS ${DALI_OPERATOR_SRCS}\n    \"${CMAKE_CURRENT_SOURCE_DIR}/numpy_loader_gpu.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Sources with CMake\nDESCRIPTION: This snippet utilizes the `collect_sources` CMake function to gather and store the source files associated with the DALI library's core functionality. The list of source files is stored in the `DALI_CORE_SRCS` variable and propagated to the parent scope, making them available for compilation.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/mm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_CORE_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files with CMake\nDESCRIPTION: This CMake snippet uses the `collect_sources` function to gather all source files within the project. The resulting list of source files is stored in the `DALI_SRCS` variable, and is available in the parent scope. This variable can then be used in the `add_library` or `add_executable` commands to specify which source files should be compiled into the library or executable.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/graph/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Importing DALI and NumPy\nDESCRIPTION: This code snippet imports the necessary modules from NumPy and NVIDIA DALI for creating and using pipelines with arithmetic operations. It imports `numpy`, `Pipeline`, `ops`, `fn`, `types`, and `Constant`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom nvidia.dali.pipeline import Pipeline\nimport nvidia.dali.ops as ops\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\nfrom nvidia.dali.types import Constant\n\nbatch_size = 1\n```\n\n----------------------------------------\n\nTITLE: Creating NVCUVID Object Library (CMake)\nDESCRIPTION: This snippet creates an object library named `NVCUVID_GEN` from the generated stub file (`dynlink_nvcuvid_gen.cc`). Object libraries are collections of compiled object files that can be linked into other libraries or executables.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/dynlink_nvcuvid/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(NVCUVID_GEN OBJECT ${NVCUVID_GENERATED_STUB})\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: Collects the source files for the DALI operators and makes them available in the parent scope. This allows the source files to be compiled and linked into the DALI library. 'DALI_OPERATOR_SRCS' will contain the list of source files and 'PARENT_SCOPE' ensures the variables are available in the parent directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/signal/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files with CMake\nDESCRIPTION: This snippet uses a custom CMake function `collect_sources` to find all source files in the specified directories and stores them in the `DALI_OPERATOR_SRCS` variable. The `PARENT_SCOPE` option ensures that the variable is available to the parent CMake scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/ssd/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Installing DALI Nightly (CUDA 11.0) via pip\nDESCRIPTION: Installs the latest nightly build of NVIDIA DALI and the TensorFlow plugin built for CUDA 11.0 using pip. This command fetches the package from NVIDIA's nightly build index and upgrades if an older version is present.  It requires `tensorflow-gpu` to be installed beforehand.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist/nightly --upgrade nvidia-dali-nightly-cuda110\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist/nightly --upgrade nvidia-dali-tf-plugin-nightly-cuda110\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files with CMake\nDESCRIPTION: This CMake snippet uses the `collect_headers` function to find all header files within the project. The discovered headers are then stored in the `DALI_INST_HDRS` variable, which is accessible in the parent scope. This allows other CMake scripts to easily use the header file paths for compilation or installation purposes.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/graph/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Dynamic cuFFT and NPP Linking\nDESCRIPTION: This conditional block handles dynamic linking of cuFFT and NPP libraries based on the `WITH_DYNAMIC_CUFFT` option. If enabled, it links with `dynlink_cufft` and `dynlink_npp`; otherwise, it links with the standard CUDA cuFFT library. Exclude-libs is used to avoid conflicts.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif (WITH_DYNAMIC_CUFFT)\n  target_link_libraries(dali_kernels PRIVATE dynlink_cufft)\n  target_link_libraries(dali_kernels PRIVATE dynlink_npp)\n  target_link_libraries(dali_kernels PRIVATE \"-Wl,--exclude-libs,$<TARGET_FILE_NAME:dynlink_npp>\")\nelse()\n  target_link_libraries(dali_kernels PRIVATE ${CUDA_cufft_LIBRARY})\n  target_link_libraries(dali_kernels PRIVATE \"-Wl,--exclude-libs,${CUDA_cufft_LIBRARY}\")\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources and Headers in CMake\nDESCRIPTION: This snippet uses custom CMake functions to collect header files, source files, and test source files for the DALI kernel. The collected files are stored in the specified variables and are available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/color_manipulation/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking with PyTorch and AutoAugment\nDESCRIPTION: This code snippet demonstrates how to run a benchmark using PyTorch as the data backend with automatic augmentation. It specifies the workspace, report file, and path to the ImageNet dataset.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n--data-backend pytorch --automatic-augmentation autoaugment\n--workspace $RESULT_WORKSPACE\n--report-file bench_report_pytorch_aa.json $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Installing the Distribution CMake\nDESCRIPTION: This snippet installs the generated source distribution (tar.gz file) to the specified destination directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/pkg_src/dist\n        DESTINATION . FILES_MATCHING PATTERN \"*.tar.gz\")\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: Adds the `loader` and `parser` subdirectories to the build process using the `add_subdirectory` command. This allows for modular configuration of the project.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(loader)\nadd_subdirectory(parser)\n```\n\n----------------------------------------\n\nTITLE: Collecting headers, sources, and test sources in CMake\nDESCRIPTION: This snippet uses CMake functions to collect headers, source files, and test sources for the NVIDIA DALI project.  `collect_headers`, `collect_sources`, and `collect_test_sources` are custom functions that populate CMake lists.  `PARENT_SCOPE` makes the variables accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/image/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Including CUDA Toolkit Include Directories\nDESCRIPTION: This snippet includes the CUDA toolkit include directories. This allows the project to use CUDA headers for CUDA development.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/customdummy/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(SYSTEM \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}\")\n```\n\n----------------------------------------\n\nTITLE: Training an Estimator Model with DALI Dataset\nDESCRIPTION: This code shows how to train a TensorFlow Estimator model using the DALI dataset through the `train_data_fn` input function. The `model.train` function is called with the input function and the number of steps for training.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Running the training on the GPU\nmodel.train(input_fn=train_data_fn, steps=EPOCHS * ITERATIONS_PER_EPOCH)\n```\n\n----------------------------------------\n\nTITLE: Setting DALI Test Sources with CMake\nDESCRIPTION: This CMake snippet sets the DALI test source files using the `set` command. It specifies the paths to various C++ source files containing unit tests for the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/util/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_TEST_SRCS ${DALI_TEST_SRCS}\n  \"${CMAKE_CURRENT_SOURCE_DIR}/random_crop_generator_test.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/numpy_test.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/uri_test.cc\")\n```\n\n----------------------------------------\n\nTITLE: Sharpness Augmentation Definition\nDESCRIPTION: This code snippet defines the `sharpness` augmentation using the `@augmentation` decorator. The outputs correspond to PIL's ImageEnhance.Sharpness.  It takes the data and a kernel parameter as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@augmentation(mag_range=(0, 0.9), randomly_negate=True, ...)\ndef sharpness(data, kernel)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: This snippet uses a custom CMake function `collect_headers` to find all header files within the current scope (including subdirectories). The collected headers are stored in the `DALI_INST_HDRS` variable with `PARENT_SCOPE`, making them available in the parent CMakeLists.txt.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/operator/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Including Utility File\nDESCRIPTION: Includes the `Utils.cmake` file from the DALI root directory. This file likely contains utility functions used throughout the build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali_tf_plugin/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(${DALI_ROOT}/cmake/Utils.cmake)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Sources with CMake\nDESCRIPTION: This CMake command collects the source files for the NVIDIA DALI operators. The `DALI_OPERATOR_SRCS` variable will contain the list of source files. `PARENT_SCOPE` makes the variable available in the parent CMake scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/cache/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Filtering Test Source Files\nDESCRIPTION: Similar to the source file filtering, this block filters test source files based on patterns defined by `KERNEL_TEST_SRCS_PATTERN` and `KERNEL_TEST_SRCS_PATTERN_EXCLUDE`.  It ensures that the dali_kernel_test.cc file is always included.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT KERNEL_TEST_SRCS_PATTERN STREQUAL \"\" OR\n    NOT KERNEL_TEST_SRCS_PATTERN_EXCLUDE STREQUAL \"\")\n  set(EXTRA_FILES \"\")\n  list(APPEND EXTRA_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/dali_kernel_test.cc\")\n  custom_filter(CMAKE_CURRENT_SOURCE_DIR\n                DALI_KERNEL_TEST_SRCS\n                EXTRA_FILES KERNEL_TEST_SRCS_PATTERN\n                KERNEL_TEST_SRCS_PATTERN_EXCLUDE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Custom Data Generation for Higher Dimensionality in DALI (Python)\nDESCRIPTION: This snippet defines a custom `get_batch` function that generates input data with higher dimensionality (2x2x2).  This function is used in subsequent code samples to demonstrate reductions on higher-dimensional data.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reductions.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_batch():\n    return [\n        np.reshape(np.arange(8, dtype=np.int32), (2, 2, 2)) * (i + 1)\n        for i in range(batch_size)\n    ]\n```\n\n----------------------------------------\n\nTITLE: Building DALI Python Library with CMake\nDESCRIPTION: This CMake snippet builds the DALI Python function library based on whether prebuilt DALI libraries are available. If `PREBUILD_DALI_LIBS` is true, it attempts to find the prebuilt library and links it. Otherwise, it links against the DALI library built from source.  The `build_per_python_lib` macro handles the actual library creation, setting output name, directory, public and private libraries, excluded libraries and source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/python_function/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (PREBUILD_DALI_LIBS)\n  # find prebuild DALI libs\n  find_library(PREBUILD_DALI_LIB NAMES dali)\n  build_per_python_lib(${dali_python_function_lib}\n                      OUTPUT_NAME ${dali_python_function_lib}\n                      OUTPUT_DIR ${DALI_LIBRARY_OUTPUT_DIR}\n                      PUBLIC_LIBS ${PREBUILD_DALI_LIB}\n                      PRIV_LIBS ${DALI_LIBS}\n                      EXCLUDE_LIBS ${exclude_libs}\n                      SRC ${PYTHON_FUNCTION_SRCS})\nelse(PREBUILD_DALI_LIBS)\n  build_per_python_lib(${dali_python_function_lib}\n                      OUTPUT_NAME ${dali_python_function_lib}\n                      OUTPUT_DIR ${DALI_LIBRARY_OUTPUT_DIR}\n                      PUBLIC_LIBS dali\n                      PRIV_LIBS ${DALI_LIBS}\n                      EXCLUDE_LIBS ${exclude_libs}\n                      SRC ${PYTHON_FUNCTION_SRCS})\nendif(PREBUILD_DALI_LIBS)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Sources with CMake\nDESCRIPTION: This snippet uses the `collect_sources` CMake function to gather source files and store them in the `DALI_OPERATOR_SRCS` variable. The `PARENT_SCOPE` argument makes the variable available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/reader/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers with CMake\nDESCRIPTION: This snippet uses the `collect_headers` function (assumed to be defined elsewhere in the project's CMake infrastructure) to gather header files. `DALI_INST_HDRS` is likely a variable that will store the list of collected header files. `PARENT_SCOPE` makes this variable available in the parent scope where this CMake code is executed.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/exec/tasking/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Installing DALI TensorFlow Plugin (CUDA 12.0) via pip - shorthand\nDESCRIPTION: Installs the latest official release of the NVIDIA DALI TensorFlow plugin built for CUDA 12.0 using pip. This is a shorthand command that assumes the package is available in the default PyPI index or a configured custom index. It requires `tensorflow-gpu` to be installed beforehand and `nvidia-dali-cuda120`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install nvidia-dali-tf-plugin-cuda120\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files with CMake\nDESCRIPTION: This snippet utilizes a custom CMake function `collect_headers` to gather header files related to DALI operators. The resulting list of header files is stored in the `DALI_INST_HDRS` variable and is made available in the parent scope using the `PARENT_SCOPE` keyword.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/erase/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files with CMake\nDESCRIPTION: This CMake command collects all header files and stores them in the `DALI_INST_HDRS` variable. `PARENT_SCOPE` makes the variable available in the parent scope, allowing other parts of the CMake configuration to access the collected header files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/exec/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags\nDESCRIPTION: This snippet sets the C++ and CUDA compiler flags by appending the DALI compile flags to the existing flags. This ensures that the code is compiled with the correct flags for DALI.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/customdummy/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${DALI_COMPILE_FLAGS} \")\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} ${DALI_COMPILE_FLAGS} \")\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources with CMake\nDESCRIPTION: This snippet employs the `collect_test_sources` CMake function to gather all test source files for the DALI project.  The `PARENT_SCOPE` argument ensures that the collected test sources are accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/util/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Operator Sources with CMake Macro\nDESCRIPTION: This snippet uses the `collect_sources` CMake macro to find and store operator source files in the `DALI_OPERATOR_SRCS` variable, accessible in the parent scope. These source files are used for building the DALI operators.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/convolution/filter_gpu/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Architectures\nDESCRIPTION: This CMake command sets the target CUDA architectures for the project. It defines a list of GPU architectures (e.g., 50, 52, 60, etc.) that the CUDA code will be compiled for.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CUDA_ARCHITECTURES \"50;52;60;61;70;75;80;86\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files with CMake\nDESCRIPTION: This snippet employs a CMake function (collect_sources) to gather all source files within the DALI project and stores them in the DALI_SRCS variable. The PARENT_SCOPE option propagates the variable to the parent scope, making it accessible for use in subsequent build steps.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/plugin/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting C Standard\nDESCRIPTION: This snippet sets the C standard to C11. This ensures that the C code is compiled using the C11 standard.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/customdummy/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_C_STANDARD 11)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Sources using CMake\nDESCRIPTION: This CMake snippet defines a macro call to collect the C++ source files for DALI operators.  It utilizes a custom function `collect_sources` that populates the `DALI_OPERATOR_SRCS` variable in the parent scope with the list of collected source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/sequence/optical_flow/optical_flow_adapter/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting Link Directories\nDESCRIPTION: This snippet adds the DALI library directory to the linker's search path. This allows the linker to find the DALI library when linking the executable.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/customdummy/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nlink_directories(\"${DALI_LIB_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Import necessary libraries\nDESCRIPTION: This snippet imports necessary libraries for working with DALI, numpy for numerical operations, os.path for path manipulation, and matplotlib for plotting the visualized optical flow.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/sequence_processing/optical_flow_example.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os.path\nimport numpy as np\n\nfrom nvidia.dali import pipeline_def\nimport nvidia.dali.fn as fn\n\nfrom matplotlib import pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Setting C++ and CUDA Compiler Flags\nDESCRIPTION: These CMake commands append the DALI compile flags to the existing C++ and CUDA compiler flags. This ensures that the compiler uses the correct flags for building the DALI plugin.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${DALI_COMPILE_FLAGS} \")\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} ${DALI_COMPILE_FLAGS} \")\n```\n\n----------------------------------------\n\nTITLE: List Compiled Plugin\nDESCRIPTION: This shell command lists the compiled DALI plugin library (`.so` file) in the `customdummy/build` directory, confirming that the build process was successful and the plugin is ready to be loaded.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/create_a_custom_operator.ipynb#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\n! ls customdummy/build/*.so\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: This CMake command collects source files and stores them in the `DALI_OPERATOR_SRCS` variable, accessible in the parent scope.  This helps manage the compilation of operator source code.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/signal/fft/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional Inclusion of Nemo ASR Loader Test (CMake)\nDESCRIPTION: This snippet conditionally includes the Nemo ASR loader test source file (`nemo_asr_loader_test.cc`) if the `BUILD_LIBSND` flag is enabled. This ensures that the Nemo ASR loader is tested when LIBSND is available.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/loader/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_LIBSND)\n  set(DALI_OPERATOR_TEST_SRCS ${DALI_OPERATOR_TEST_SRCS}\n    \"${CMAKE_CURRENT_SOURCE_DIR}/nemo_asr_loader_test.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: This snippet uses a custom CMake function `collect_test_sources` to find all test source files within the current scope (including subdirectories). The collected sources are stored in the `DALI_TEST_SRCS` variable with `PARENT_SCOPE`, making them available in the parent CMakeLists.txt.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/operator/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Configuring DALI Operator Tests in CMake\nDESCRIPTION: This section configures the test executable for the DALI operators. It specifies the source files, links against the `dali_operators` library and other dependencies, and defines properties such as output name and runtime output directory. It also adds a check target to run the tests.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_TEST)\n  # TODO(janton): create a test_utils_lib with dali_test_config.cc and other common utilities\n  adjust_source_file_language_property(\"${DALI_OPERATOR_TEST_SRCS}\")\n  add_executable(dali_operator_test\n    ${DALI_OPERATOR_TEST_SRCS}\n    ${DALI_ROOT}/dali/test/dali_test_config.cc\n    ${DALI_ROOT}/dali/test/dali_operator_test_utils.cc\n    ${DALI_ROOT}/dali/test/operators/passthrough_with_trace.cc\n    ${DALI_ROOT}/dali/test/operators/identity_input.cc)\n\n  target_link_libraries(dali_operator_test PUBLIC dali_operators)\n  target_link_libraries(dali_operator_test PRIVATE gtest dynlink_cuda ${DALI_LIBS})\n  if (BUILD_NVML)\n    target_link_libraries(dali_operator_test PRIVATE dynlink_nvml)\n  endif(BUILD_NVML)\n  if (WITH_DYNAMIC_NPP)\n    target_link_libraries(dali_operator_test PRIVATE dynlink_npp)\n    target_link_libraries(dali_operator_test PRIVATE \"-Wl,--exclude-libs,$<TARGET_FILE_NAME:dynlink_npp>\"\n    )\n  endif(WITH_DYNAMIC_NPP)\n  if (BUILD_CUFILE)\n    target_link_libraries(dali_operator_test PRIVATE dynlink_cufile)\n  endif()\n  target_link_libraries(dali_operator_test PRIVATE \"-Wl,--exclude-libs,${exclude_libs}\")\n  target_link_libraries(dali_operator_test PRIVATE \"-pie\")\n  set_target_properties(dali_operator_test PROPERTIES POSITION_INDEPENDENT_CODE ON)\n  set_target_properties(dali_operator_test PROPERTIES OUTPUT_NAME \"dali_operator_test.bin\")\n  set_target_properties(dali_operator_test PROPERTIES\n    RUNTIME_OUTPUT_DIRECTORY ${TEST_BINARY_DIR})\n\n  add_check_gtest_target(\"check-operator-gtest\" dali_operator_test ${TEST_BINARY_DIR})\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: This CMake function collects header files and stores them in the `DALI_INST_HDRS` variable.  The `PARENT_SCOPE` option makes this variable available in the parent scope, allowing other parts of the project to access the collected header files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/host/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional AWSSDK Sources with CMake\nDESCRIPTION: This CMake snippet conditionally includes AWSSDK-related source files based on the `BUILD_AWSSDK` option. When this option is enabled, the necessary source files for AWS S3 integration are included in the DALI library's build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/util/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_AWSSDK)\n  set(DALI_SRCS ${DALI_SRCS}\n  \"${CMAKE_CURRENT_SOURCE_DIR}/s3_file.cc\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/s3_filesystem.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for GPU Implementations (CMake)\nDESCRIPTION: This snippet adds subdirectories to the build process for Gaussian Blur, Laplacian, and Filter GPU implementations. Each subdirectory contains the CMake configuration and source code for the corresponding operator.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/convolution/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(gaussian_blur_gpu)\nadd_subdirectory(laplacian_gpu)\nadd_subdirectory(filter_gpu)\n```\n\n----------------------------------------\n\nTITLE: Getting DALI Version\nDESCRIPTION: Retrieves the DALI version from the `VERSION` file in the DALI root directory. The `get_dali_version` function (presumably defined in `Utils.cmake`) reads and parses the version information.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali_tf_plugin/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nget_dali_version(${DALI_ROOT}/VERSION DALI_VERSION)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: Collects header files for the DALI library and makes them available in the parent scope. This ensures that the headers are accessible to other parts of the project.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/audio/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Installing DALI (CUDA 12.0) via pip\nDESCRIPTION: Installs the latest official release of NVIDIA DALI built for CUDA 12.0 using pip. This command fetches the package from NVIDIA's PyPI index and upgrades if an older version is present. Requires pip and a compatible Python environment.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install --extra-index-url https://pypi.nvidia.com --upgrade nvidia-dali-cuda120\n```\n\n----------------------------------------\n\nTITLE: Collecting Operator Sources in DALI with CMake\nDESCRIPTION: This CMake command collects all source files related to DALI operators and stores them in the `DALI_OPERATOR_SRCS` variable, making it available in the parent scope. It uses the custom CMake function `collect_sources` to handle the file collection process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/reduce/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Generating CUFile Stubs with Custom Command in CMake\nDESCRIPTION: Generates CUFile stubs using a Python script, similar to the CUDA stub generation. It defines a custom command that executes the script, specifying input files, output file, and dependencies. It uses `add_custom_command` to define the generation step and creates `dynlink_cufile` library. `DEFAULT_COMPILER_INCLUDE` is used to propagate compiler include directories.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_CUFILE)\n  set(CUFILE_GENERATED_STUB \"${CMAKE_CURRENT_BINARY_DIR}/dynlink_cufile_gen.cc\")\n  add_custom_command(\n      OUTPUT ${CUFILE_GENERATED_STUB}\n      COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/stub_codegen.py --unique_prefix=Cufile --\n                  \"${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/cufile.json\" ${CUFILE_GENERATED_STUB}\n                  \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}/cufile.h\" \"-I${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}\"\n                  # for some reason QNX fails with 'too many errors emitted' is this is not set\n                  \"-ferror-limit=0\"\n                  ${DEFAULT_COMPILER_INCLUDE}\n      DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/stub_codegen.py\n              \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}/cufile.h\"\n              \"${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/cufile.json\"\n      COMMENT \"Running cufile.h stub generator\"\n      VERBATIM)\n\n  set_source_files_properties(${CUFILE_GENERATED_STUB} PROPERTIES GENERATED TRUE)\n  add_library(dynlink_cufile STATIC dynlink_cufile.cc ${CUFILE_GENERATED_STUB})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting NumPy Print Options\nDESCRIPTION: This line sets the NumPy print options to display numbers with a precision of 2 decimal places. This helps in visualizing the output of floating-point operations.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_type_promotions.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnp.set_printoptions(precision=2)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers with CMake Macro\nDESCRIPTION: This snippet uses the `collect_headers` CMake macro to find and store header files in the `DALI_INST_HDRS` variable, accessible in the parent scope.  The collected headers are intended for installation.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/convolution/filter_gpu/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet adds the `builtin` and `checkpointing` subdirectories to the build process.  It allows CMake to process the CMakeLists.txt files within those directories. No specific dependencies are required.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/operator/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(builtin)\nadd_subdirectory(checkpointing)\n```\n\n----------------------------------------\n\nTITLE: Checking Python Executable\nDESCRIPTION: Checks if a Python executable has been found. If not, it issues a fatal error, stopping the build process. It will print the Python executable path used.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED PYTHON_EXECUTABLE)\n  message(FATAL_ERROR \"No Python executable found.\")\nendif()\nmessage(STATUS \"Using Python ${PYTHON_EXECUTABLE}\")\n```\n\n----------------------------------------\n\nTITLE: Deserialize and Build the DALI Pipeline\nDESCRIPTION: This code deserializes the serialized pipeline string `s` into the `pipe2` instance and builds the pipeline. The `deserialize_and_build()` method is used to perform the deserialization and construction of the pipeline.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/advanced/serialization.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe2.deserialize_and_build(s)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources for DALI Operators\nDESCRIPTION: This snippet uses the `collect_test_sources` CMake function to gather all source files for DALI operator tests. The test source files are stored in the `DALI_OPERATOR_TEST_SRCS` variable, which is available in the parent scope. This allows for the compilation and execution of tests.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/sequence/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source and Header Files in CMake\nDESCRIPTION: This snippet collects header, source, and test source files for the DALI project. `collect_headers`, `collect_sources`, and `collect_test_sources` are assumed to be custom CMake functions that glob files and set variables (DALI_INST_HDRS, DALI_SRCS, DALI_TEST_SRCS) in the parent scope, making them available to other parts of the CMake configuration. PARENT_SCOPE makes the variables visible in the parent directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version\nDESCRIPTION: This CMake command specifies the minimum required version of CMake for the project. It ensures that the project can be built with CMake version 3.18 or higher.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18)\n```\n\n----------------------------------------\n\nTITLE: Train the Model\nDESCRIPTION: Trains the TensorFlow model using the distributed DALI dataset. The `model.fit` method is used to train the model for a specified number of epochs and steps per epoch.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset-multigpu.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=ITERATIONS)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: Adds the 'mel_scale' and 'mfcc' subdirectories to the current CMake project. This allows the build system to recursively process the CMakeLists.txt files within these directories.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/audio/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(mel_scale)\nadd_subdirectory(mfcc)\n```\n\n----------------------------------------\n\nTITLE: Adding Dimensions in DALI\nDESCRIPTION: This snippet demonstrates how to use `dali.newaxis` to add a new dimension of size 1 to the output. The new dimension can be at the beginning or end of the tensor.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/indexing.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrailing_channel = grayscale[:,:,dali.newaxis]\nleading_channel = grayscale[dali.newaxis]\n```\n\n----------------------------------------\n\nTITLE: Training Usage\nDESCRIPTION: This shows the usage for the training script.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/yolov4/readme.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nusage: main.py train [-h] file_root annotations\n  [--batch_size BATCH_SIZE] [--epochs EPOCHS] [--steps STEPS] [--output OUTPUT]\n  [--start_weights START_WEIGHTS] [--log_dir LOG_DIR] [--ckpt_dir CKPT_DIR]\n  [--pipeline PIPELINE] [--multigpu] [--use_mosaic] [--learning_rate LEARNING_RATE]\n  [--eval_file_root EVAL_FILE_ROOT] [--eval_annotations EVAL_ANNOTATIONS]\n  [--eval_steps EVAL_STEPS] [--eval_frequency EVAL_FREQUENCY]\n  [--seed SEED]\n```\n\n----------------------------------------\n\nTITLE: Visualizing the output image (CPU)\nDESCRIPTION: This snippet uses Matplotlib to display an image from the processed batch and prints its shape and label.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/external_input.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nimg = batch_cpu.at(2)\nprint(img.shape)\nprint(labels_cpu.at(2))\nplt.axis(\"off\")\nplt.imshow(img)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files with CMake\nDESCRIPTION: This snippet uses the `collect_sources` CMake function to gather source files for DALI operators. The list of source files is stored in the `DALI_OPERATOR_SRCS` variable and is propagated to the parent scope via `PARENT_SCOPE`.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/erase/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Check difference between batches\nDESCRIPTION: This code defines a function `check_difference` that calculates the absolute difference between each element in two batches and sums them, effectively quantifying the dissimilarity between the two batches. It iterates through each sample in the batch and calculates the sum of absolute differences between corresponding elements.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/advanced/serialization.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef check_difference(batch_1, batch_2):\n    return [\n        np.sum(np.abs(batch_1.at(i) - batch_2.at(i))) for i in range(batch_size)\n    ]\n```\n\n----------------------------------------\n\nTITLE: Setting Default Compiler Includes (CMake)\nDESCRIPTION: This snippet transforms a list of paths into a list of include directives using a foreach loop. It iterates through the `INFERED_COMPILER_INCLUDE` list and prepends each directory with `-I` to create compiler include directives, which are then stored in the `DEFAULT_COMPILER_INCLUDE` variable. `separate_arguments` is used to split the string into a list of arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/dynlink_nvcuvid/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(DEFAULT_COMPILER_INCLUDE)\nforeach(incl_dir ${INFERED_COMPILER_INCLUDE})\n  set(DEFAULT_COMPILER_INCLUDE \"${DEFAULT_COMPILER_INCLUDE} -I${incl_dir}\")\nendforeach(incl_dir)\nseparate_arguments(DEFAULT_COMPILER_INCLUDE UNIX_COMMAND  \"${DEFAULT_COMPILER_INCLUDE}\")\n```\n\n----------------------------------------\n\nTITLE: Setting DALI Root Directory\nDESCRIPTION: Sets the `DALI_ROOT` variable to point to the root directory of the DALI project. This variable is used to locate other necessary files and directories within the DALI project.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali_tf_plugin/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_ROOT \"${PROJECT_SOURCE_DIR}/..\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Tests (CMake)\nDESCRIPTION: This snippet uses custom CMake functions `collect_headers`, `collect_sources`, and `collect_test_sources` to gather header files, source files, and test source files, respectively.  The `PARENT_SCOPE` argument makes these variables available in the parent scope. These variables are used later in the build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/dynlink_nvcuvid/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Excluding NVML Sources with CMake\nDESCRIPTION: This CMake snippet filters the `DALI_SRCS` list to exclude any source files that contain \"nvml\" in their name. This ensures that NVML-related sources are not included in the main DALI library build if needed, potentially depending on other build options or conditions.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/util/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nlist(FILTER DALI_SRCS EXCLUDE REGEX \".*nvml.*\\.cc\")\nset(DALI_SRCS ${DALI_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers with CMake\nDESCRIPTION: This CMake directive uses the `collect_headers` function to gather header files related to DALI operators. `DALI_INST_HDRS` is the variable to store collected header file paths. The `PARENT_SCOPE` argument ensures that the collected headers are available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/io/file/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: This CMake command collects header files and stores them in the `DALI_INST_HDRS` variable, accessible in the parent scope. This is used to manage header dependencies and installation.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/signal/fft/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files with CMake\nDESCRIPTION: This CMake command collects all source files and stores them in the `DALI_CORE_SRCS` variable. The `PARENT_SCOPE` argument makes the variable accessible in the parent scope, making the source files available for the build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/exec/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_CORE_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files with CMake\nDESCRIPTION: This CMake function `collect_headers` gathers header files and stores them in the `DALI_INST_HDRS` variable. The `PARENT_SCOPE` option makes the variable available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/paste/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: This code uses the `collect_test_sources` CMake macro to gather test source files within the NVIDIA DALI project. The collected test sources are stored in the `DALI_KERNEL_TEST_SRCS` variable, scoped to the parent scope, and are used for building and running tests.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/jpeg/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Upstream DALI Repository using Git\nDESCRIPTION: This snippet provides the commands to clone a forked DALI repository and add an upstream remote to track changes in the main DALI repository. This allows contributors to keep their fork synchronized with the official DALI codebase and simplifies pulling the latest changes.\nSOURCE: https://github.com/nvidia/dali/blob/main/guided_contribution_tutorial.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --recursive https://github.com/<your_name>/DALI\ncd DALI\ngit remote add upstream https://github.com/NVIDIA/DALI\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: Collects the test source files for the DALI operators and makes them available in the parent scope. These test files are used for unit testing and integration testing. 'DALI_OPERATOR_TEST_SRCS' will contain the list of test source files and 'PARENT_SCOPE' ensures the variables are available in the parent directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/signal/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Kernel Test Sources in CMake\nDESCRIPTION: This snippet uses a custom CMake function `collect_test_sources` to gather the test source files for the DALI kernel.  The collected sources are stored in the `DALI_KERNEL_TEST_SRCS` variable. The `PARENT_SCOPE` argument ensures that the variable is visible in the parent CMake scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/erase/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Installing Legacy DALI (CUDA 9.0) via pip\nDESCRIPTION: Installs an older version of NVIDIA DALI and the TensorFlow plugin built for CUDA 9.0 using pip. This command fetches the package from NVIDIA's CUDA 9.0 index and upgrades if an older version is present.  This targets DALI versions 0.22 and lower and requires setting the correct pip index.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/9.0 --upgrade nvidia-dali\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/9.0 --upgrade nvidia-dali-tf-plugin\n```\n\n----------------------------------------\n\nTITLE: Setting Plugin Sources\nDESCRIPTION: This CMake command defines the source files for the plugin. It includes the C++ file `naive_histogram.cc` and the CUDA file `naive_histogram.cu`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nset(PLUGIN_SOURCES\n        ${CMAKE_CURRENT_SOURCE_DIR}/naive_histogram.cc\n        ${CMAKE_CURRENT_SOURCE_DIR}/naive_histogram.cu\n        )\n```\n\n----------------------------------------\n\nTITLE: Adding Video Subdirectory\nDESCRIPTION: Adds the 'video' subdirectory to the build process. This indicates that the 'video' directory contains source code and a CMakeLists.txt file that needs to be processed.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(video)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers with CMake\nDESCRIPTION: This CMake command is used to collect header files within the current directory and make them available under the variable `DALI_INST_HDRS` in the parent scope. This ensures that the headers are accessible to other parts of the project that depend on this operator.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/io/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Sources with CMake\nDESCRIPTION: This command collects the source files for DALI operators and makes them available in the parent scope for compilation.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/geometry/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Installing Legacy DALI (CUDA 10.2) via pip\nDESCRIPTION: Installs an older version of NVIDIA DALI and the TensorFlow plugin built for CUDA 10.2 using pip. This command fetches the package from NVIDIA's main redist index and upgrades if an older version is present. This targets DALI versions from 1.4.0 up to 1.20 and requires setting the correct pip index.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda102\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-tf-plugin-cuda102\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: These lines add various subdirectories to the project, each containing different parts of the DALI kernels.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(audio)\nadd_subdirectory(common)\nadd_subdirectory(erase)\nadd_subdirectory(imgproc)\nadd_subdirectory(math)\nadd_subdirectory(normalize)\nadd_subdirectory(reduce)\nadd_subdirectory(signal)\nadd_subdirectory(slice)\nadd_subdirectory(test)\nadd_subdirectory(transpose)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: Adds the 'fused' subdirectory to the current CMake build context. This allows the CMake project to include and build the CMakeLists.txt file located in the 'fused' directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/nvjpeg/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(fused)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in DALI with CMake\nDESCRIPTION: This CMake macro collects test source files and stores them in the `DALI_TEST_SRCS` variable within the parent scope. It's used for managing test source files.  The macro requires CMake and the `collect_test_sources` function to be defined elsewhere.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/data/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Generating Protobuf Files in CMake\nDESCRIPTION: This snippet uses the `protobuf_generate_cpp` function to generate C++ source and header files from the `proto/dali.proto` protobuf definition file. It then creates an object library named `DALI_PROTO` from these generated files and installs the generated header files to `include/dali/pipeline`. This allows the DALI project to serialize and deserialize data using protobuf.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nprotobuf_generate_cpp(DALI_PROTO_SRCS DALI_PROTO_HEADERS proto/dali.proto)\nadd_library(DALI_PROTO OBJECT ${DALI_PROTO_HEADERS} ${DALI_PROTO_SRCS})\ninstall(FILES ${DALI_PROTO_HEADERS} DESTINATION include/dali/pipeline)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources with CMake\nDESCRIPTION: This snippet utilizes the `collect_sources` CMake function to gather all source files necessary for the DALI project. The `PARENT_SCOPE` argument makes the collected source files available in the parent scope where the function is invoked.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/util/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Test Sources in CMake\nDESCRIPTION: This CMake snippet uses custom `collect_headers`, `collect_sources`, and `collect_test_sources` functions to gather lists of header files, source files, and test source files, respectively. The gathered lists are stored in the `DALI_INST_HDRS`, `DALI_KERNEL_SRCS`, and `DALI_KERNEL_TEST_SRCS` variables, and the `PARENT_SCOPE` option makes these variables available in the parent scope (i.e., the CMakeLists.txt file that includes this one).\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/signal/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Creating Shared Library in CMake\nDESCRIPTION: This CMake code snippet defines a shared library named `dali_customdummyplugin` using the source files found in the `SRCS` variable. It links the library against the CUDA runtime library (`${CUDART_LIB}`) and the core DALI library (`dali`).\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/test/plugins/dummy/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(lib_name \"dali_customdummyplugin\")\nadjust_source_file_language_property(\"${SRCS}\")\nadd_library(${lib_name} SHARED ${SRCS})\ntarget_link_libraries(${lib_name} PRIVATE ${CUDART_LIB})\ntarget_link_libraries(${lib_name} PUBLIC dali)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: Collects source files for DALI operators and makes them available in the parent scope. This allows the CMake project to compile these source files into the DALI operators.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/nvjpeg/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers for DALI (CMake)\nDESCRIPTION: This CMake command uses the `collect_headers` macro to gather header files for the DALI project. The collected headers are stored in the `DALI_INST_HDRS` variable and are made available in the parent scope. This is crucial for compiling and linking DALI components that depend on these headers.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/imgcodec/util/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files with CMake\nDESCRIPTION: The `collect_sources` function is called to gather all source files. The collected sources are stored in the `DALI_KERNEL_SRCS` variable. `PARENT_SCOPE` ensures that the variable is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/math/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Installing DALI headers\nDESCRIPTION: This snippet installs DALI headers to the `DALI_INCLUDE_DIR` within the build directory. It iterates through the `DALI_INST_HDRS` list, copies the headers, and also copies the boost preprocessor headers. The installation process is dependent on the `BUILD_PYTHON` option.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_PYTHON)\n  if (PREBUILD_DALI_LIBS)\n    add_custom_target(install_headers ALL)\n  else (PREBUILD_DALI_LIBS)\n    add_custom_target(install_headers ALL\n        DEPENDS dali dali_operators\n        )\n  endif (PREBUILD_DALI_LIBS)\n\n  # Process the DALI_INST_HDRS list\n  foreach(INSTALL_HEADER ${DALI_INST_HDRS})\n    file(RELATIVE_PATH HEADER_RELATIVE ${PROJECT_SOURCE_DIR} ${INSTALL_HEADER})\n    add_custom_command(\n      TARGET install_headers\n      COMMAND install -D \"${INSTALL_HEADER}\" \"${PROJECT_BINARY_DIR}/${DALI_INCLUDE_DIR}/${HEADER_RELATIVE}\")\n  endforeach(INSTALL_HEADER)\n\n  # Copy proper `include` dir\n  add_custom_command(\n    TARGET install_headers\n    COMMAND cp -r \"${PROJECT_SOURCE_DIR}/include/.\" \"${PROJECT_BINARY_DIR}/${DALI_INCLUDE_DIR}\"\n  )\n\n  # Copy boost/preprocessor include files\n  add_custom_command(\n    TARGET install_headers\n    COMMAND cp -rL \"${PROJECT_SOURCE_DIR}/third_party/boost/preprocessor/include/.\" \"${PROJECT_BINARY_DIR}/${DALI_INCLUDE_DIR}/\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers for DALI Operators\nDESCRIPTION: This snippet uses the `collect_headers` CMake function to gather header files related to DALI operators. The `DALI_INST_HDRS` variable likely holds the list of headers to be collected, and `PARENT_SCOPE` indicates that the collected headers are available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/audio/mel_scale/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: Collects test source files for the DALI operators and makes them available in the parent scope. This allows the build system to build and run the operator tests.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/audio/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Sources with CMake\nDESCRIPTION: This snippet uses the `collect_sources` CMake function to gather source files for DALI operators, storing the results in the `DALI_OPERATOR_SRCS` variable. The `PARENT_SCOPE` option makes the variable available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/resize/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting minimum CMake version\nDESCRIPTION: Specifies the minimum CMake version required for building the project. This ensures that the CMake version used is compatible with the project's CMake code.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.16)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Test Sources in CMake\nDESCRIPTION: This snippet uses custom CMake functions (likely defined elsewhere in the project) to collect header files, operator source files, and test source files.  It uses `PARENT_SCOPE` to make the collected file lists available to the calling scope. The lists `DALI_INST_HDRS`, `DALI_OPERATOR_SRCS`, and `DALI_OPERATOR_TEST_SRCS` are populated with the file paths.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/mask/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source and Header Files in CMake\nDESCRIPTION: This snippet uses custom CMake macros (`collect_headers`, `collect_sources`, `collect_test_sources`) to gather header, source, and test source files, respectively. The collected files are stored in variables (`DALI_INST_HDRS`, `DALI_OPERATOR_SRCS`, `DALI_OPERATOR_TEST_SRCS`) and made available in the parent scope using the `PARENT_SCOPE` option. These macros likely perform file system operations to locate and store the file paths.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Serialize the DALI Pipeline\nDESCRIPTION: This code serializes the DALI pipeline `pipe` into a string representation using the `serialize()` method.  This string can be used to recreate the pipeline later.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/advanced/serialization.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ns = pipe.serialize()\n```\n\n----------------------------------------\n\nTITLE: Including CMake Modules\nDESCRIPTION: Includes standard CMake modules for installation directories, fetching external content, and common project configurations.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(GNUInstallDirs)\ninclude(FetchContent)\ninclude(common.cmake)\n```\n\n----------------------------------------\n\nTITLE: Adding subdirectories for DALI components\nDESCRIPTION: This snippet includes subdirectories for building different parts of the DALI library, such as the core, NPP integration, kernels, pipeline, operators, and Python bindings.  The inclusion of specific subdirectories depends on CMake build options like `BUILD_DALI_KERNELS`, `BUILD_DALI_PIPELINE`, `BUILD_DALI_OPERATORS`, `BUILD_NVJPEG`, `BUILD_NVIMAGECODEC`, and `BUILD_PYTHON`.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(core)\n\nadd_subdirectory(npp)\n\nif (BUILD_DALI_KERNELS)\n  add_subdirectory(kernels)\nendif()\n\nif (BUILD_DALI_PIPELINE)\n  add_subdirectory(pipeline)\n  add_subdirectory(util)\n  add_subdirectory(plugin)\n  add_subdirectory(c_api)\n  add_subdirectory(c_api_2)\nendif()\n\nif(BUILD_DALI_OPERATORS)\n  if(BUILD_NVJPEG)\n    add_subdirectory(nvjpeg)\n  endif()\n\n  if(BUILD_NVIMAGECODEC)\n    add_subdirectory(nvimgcodec)\n  endif()\n\n  add_subdirectory(operators)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing DALI TensorFlow Plugin (CUDA 12.0) via pip\nDESCRIPTION: Installs the latest official release of the NVIDIA DALI TensorFlow plugin built for CUDA 12.0 using pip. This command fetches the package from NVIDIA's PyPI index and upgrades if an older version is present. It requires `tensorflow-gpu` to be installed beforehand and `nvidia-dali-cuda120`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install --extra-index-url https://pypi.nvidia.com --upgrade nvidia-dali-tf-plugin-cuda120\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI test sources in CMake\nDESCRIPTION: The `collect_test_sources` CMake function collects all source files specifically for testing the DALI core and stores them in the `DALI_CORE_TEST_SRCS` variable within the parent scope. These test sources are used during the testing phase of the build process. The variable `DALI_CORE_TEST_SRCS` is available to other CMake files through the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/os/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_test_sources(DALI_CORE_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources and Headers with CMake\nDESCRIPTION: This CMake code snippet uses custom functions to collect header files, operator source files, and operator test source files. The `collect_headers` function retrieves all header files, `collect_sources` gathers operator source files, and `collect_test_sources` finds operator test source files. The results are stored in the specified variables, which are scoped to the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/peek_shape/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# Get all the source files and dump test files\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Full and Empty Axis Reductions in DALI (Python)\nDESCRIPTION: This snippet demonstrates full and empty axis reductions in DALI using both axis indices and layout names. Passing all axes results in a full reduction (reduction over all elements), while passing empty axes results in no reduction.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/reductions.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(batch_size=batch_size, num_threads=4, device_id=0)\nwith pipe:\n    input = fn.external_source(source=get_batch, layout=\"AB\", dtype=types.INT64)\n    min_axes_full = fn.reductions.min(input, axes=(0, 1))\n    min_axes_empty = fn.reductions.min(input, axes=())\n    min_layout_full = fn.reductions.min(input, axis_names=\"AB\")\n    min_layout_empty = fn.reductions.min(input, axis_names=\"\")\n\n    pipe.set_outputs(\n        min_axes_full, min_axes_empty, min_layout_full, min_layout_empty\n    )\n\nrun_and_print(pipe)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers for DALI\nDESCRIPTION: This CMake function collects header files and assigns them to the variable DALI_INST_HDRS in the parent scope. This allows other parts of the CMake project to access and use the collected header files, typically for installation or other build-related tasks.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/operator/builtin/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional Inclusion of Nemo ASR Loader (CMake)\nDESCRIPTION: This snippet conditionally includes the Nemo ASR loader source file (`nemo_asr_loader.cc`) if the `BUILD_LIBSND` flag is enabled. This allows DALI to load audio data from Nemo ASR format when LIBSND is available.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/loader/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_LIBSND)\n  set(DALI_OPERATOR_SRCS ${DALI_OPERATOR_SRCS}\n    \"${CMAKE_CURRENT_SOURCE_DIR}/nemo_asr_loader.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Source Files with CMake\nDESCRIPTION: The `collect_test_sources` function is called to gather all test source files. The collected test sources are stored in the `DALI_KERNEL_TEST_SRCS` variable. `PARENT_SCOPE` ensures that the variable is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/math/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Appending Sequence Reader Source in CMake\nDESCRIPTION: Appends the source file for the sequence reader operator to the `DALI_OPERATOR_SRCS` list. This adds the implementation for reading sequences of data.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/sequence_reader_op.cc\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: Collects header files and stores their paths in the DALI_INST_HDRS variable. The PARENT_SCOPE option makes the variable available in the parent scope, allowing other parts of the CMake configuration to access the list of header files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/segmentation/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Source Files with CMake\nDESCRIPTION: This snippet uses the `collect_test_sources` CMake function to collect test source files for DALI operators. The resulting list is stored in the `DALI_OPERATOR_TEST_SRCS` variable and is available in the parent scope due to the `PARENT_SCOPE` keyword.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/erase/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: This CMake command collects test source files and stores them in the `DALI_OPERATOR_TEST_SRCS` variable, accessible in the parent scope. This organizes test files for compilation and execution.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/signal/fft/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional Operator Source Appending (CUFILE) in CMake\nDESCRIPTION: Conditionally appends source files related to CUFILE to the `DALI_OPERATOR_SRCS` list, based on the `BUILD_CUFILE` build flag.  This includes `gds_mem.cc`, `numpy_reader_gpu_op.cc`, and `numpy_reader_gpu_op_impl.cu`.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_CUFILE)\n  list(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/gds_mem.cc\")\n  list(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/numpy_reader_gpu_op.cc\")\n  list(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/numpy_reader_gpu_op_impl.cu\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories and Adding Subdirectory\nDESCRIPTION: Sets the include directories for the project and adds the VideoCodecSDKUtils subdirectory to the build.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nmessage(STATUS \"pynvvideocodec_SOURCE_DIR=${pynvvideocodec_SOURCE_DIR}\")\ninclude_directories(SYSTEM \"${pynvvideocodec_SOURCE_DIR}/src/\")\nset(USE_PKGCONFIG OFF)\nset(ENV{FFMPEG_DIR} ${FFMPEG_DIR})\nadd_subdirectory(\"${pynvvideocodec_SOURCE_DIR}/src/VideoCodecSDKUtils/\")\n```\n\n----------------------------------------\n\nTITLE: Generating CUDA cuFFT Stub with Python in CMake\nDESCRIPTION: This snippet uses a custom command in CMake to generate a CUDA cuFFT stub. It utilizes a Python script (`stub_codegen.py`) to generate the stub code. The script takes the cuFFT header file and a JSON file as input. The generated stub file is then added as a source file to the `dynlink_cufft` static library. This section is conditional, based on `WITH_DYNAMIC_CUFFT`.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/signal/fft/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (WITH_DYNAMIC_CUFFT)\n    set(CUFFT_GENERATED_STUB \"${CMAKE_CURRENT_BINARY_DIR}/dynlink_cufft_gen.cc\")\n    add_custom_command(\n        OUTPUT ${CUFFT_GENERATED_STUB}\n        COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/../../../../internal_tools/stub_generator/stub_codegen.py --unique_prefix=Cufft --\n                    \"${CMAKE_CURRENT_SOURCE_DIR}/../../../../internal_tools/stub_generator/cufft.json\" ${CUFFT_GENERATED_STUB}\n                    \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}/cufft.h\" \"-I${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}\"\n                    # for some reason QNX fails with 'too many errors emitted' is this is not set\n                    \"-ferror-limit=0\"\n                    ${DEFAULT_COMPILER_INCLUDE}\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/../../../../internal_tools/stub_generator/stub_codegen.py\n                \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}/cufft.h\"\n                \"${CMAKE_CURRENT_SOURCE_DIR}/../../../../internal_tools/stub_generator/cufft.json\"\n        COMMENT \"Running cufft.h stub generator\"\n        VERBATIM)\n\n    set_source_files_properties(${CUFFT_GENERATED_STUB} PROPERTIES GENERATED TRUE)\n    add_library(dynlink_cufft STATIC cufft_wrap.cc ${CUFFT_GENERATED_STUB})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Run the Naive Histogram test\nDESCRIPTION: This code snippet executes the `naive_histogram_test.py` script. It first changes the directory to `naive_histogram` and then runs the python script, which calculates the histogram on the provided sample images. The output is a 2D array representing the histogram data for multiple images.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ cd naive_histogram\n$ python naive_histogram_test.py\n[[11355 10555 10499 10724 10687 11213 11388 11474 11715 11407 11291 11093\n  10757 10481 10547 11177 10081 10353 10380 10691 10947 10851 10872 10582]\n [44322 44408 45633 49539 53415 46655 46081 44979 42273 41195 43601 43466\n  42768 43041 42755 43519 44542 47158 50718 49510 45163 44758 45982 46359]]\n```\n\n----------------------------------------\n\nTITLE: Conditional CFITSIO Sources and Tests with CMake\nDESCRIPTION: This CMake snippet conditionally includes CFITSIO-related source and test files based on the `BUILD_CFITSIO` option. When enabled, CFITSIO functionalities' sources and its tests are added to the DALI library's compilation and testing process, respectively.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/util/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_CFITSIO)\n  set(DALI_SRCS ${DALI_SRCS}\n  \"${CMAKE_CURRENT_SOURCE_DIR}/fits.cc\")\n\n  set(DALI_TEST_SRCS ${DALI_TEST_SRCS}\n  \"${CMAKE_CURRENT_SOURCE_DIR}/fits_test.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building a shared library for DALI operator plugin in CMake\nDESCRIPTION: This snippet defines a shared library named `testoperatorplugin` built from the source files specified in `tmp`. It links the library with CUDA runtime library (`CUDART_LIB`) and the DALI library, making it available for importing as a plugin in Python. It also conditionally links NVML if `BUILD_NVML` is enabled.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/test/operators/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(lib_name \"testoperatorplugin\")\nadd_library(${lib_name} SHARED ${tmp})\ntarget_link_libraries(${lib_name} PRIVATE ${CUDART_LIB})\ntarget_link_libraries(${lib_name} PUBLIC dali)\nif (BUILD_NVML)\n  target_link_libraries(${lib_name} PRIVATE dynlink_nvml)\nendif(BUILD_NVML)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source and Header Files in CMake\nDESCRIPTION: This CMake code snippet uses custom functions to collect header files, source files, and test source files. The collected files are stored in the `DALI_INST_HDRS`, `DALI_OPERATOR_SRCS`, and `DALI_OPERATOR_TEST_SRCS` variables, respectively, and are available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/resize/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources for DALI Operators\nDESCRIPTION: This snippet utilizes the `collect_sources` CMake function to collect source files for DALI operators. The `DALI_OPERATOR_SRCS` variable is intended to hold the list of source files, and `PARENT_SCOPE` makes them available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/audio/mel_scale/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Determining GCC System Include Directories with CMake\nDESCRIPTION: This snippet uses the `DETERMINE_GCC_SYSTEM_INCLUDE_DIRS` macro to infer the GCC system include directories based on the C++ compiler and flags. The inferred include directories are then stored in the `INFERED_COMPILER_INCLUDE` variable, which is later used to set up the compiler include paths.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/signal/fft/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nDETERMINE_GCC_SYSTEM_INCLUDE_DIRS(\"c++\" \"${CMAKE_CXX_COMPILER}\" \"${CMAKE_CXX_FLAGS}\" INFERED_COMPILER_INCLUDE)\n\n# transform a list of paths into a list of include directives\nset(DEFAULT_COMPILER_INCLUDE)\nforeach(incl_dir ${INFERED_COMPILER_INCLUDE})\n  set(DEFAULT_COMPILER_INCLUDE \"${DEFAULT_COMPILER_INCLUDE} -I${incl_dir}\")\nendforeach(incl_dir)\nseparate_arguments(DEFAULT_COMPILER_INCLUDE UNIX_COMMAND  \"${DEFAULT_COMPILER_INCLUDE}\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: This CMake snippet uses the `collect_sources` function to gather all source files within the NVIDIA DALI project. The collected source files are stored in the `DALI_OPERATOR_SRCS` variable and are available in the parent scope. These source files are used to build the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/crop/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Test Sources with CMake\nDESCRIPTION: This CMake code snippet uses custom macros to collect header files, source files, and test source files for the DALI library. It leverages `collect_headers`, `collect_sources`, and `collect_test_sources` macros. The `PARENT_SCOPE` argument ensures that the collected file lists are accessible in the parent CMake scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/loader/webdataset/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Kernel Test Sources in CMake\nDESCRIPTION: This snippet utilizes the `collect_test_sources` CMake function to gather test source files. The gathered file list is then stored in the `DALI_KERNEL_TEST_SRCS` variable, accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/color_manipulation/debayer/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Kernel Sources with CMake\nDESCRIPTION: This CMake command collects source files for the DALI kernel and makes them available in the parent scope.  `DALI_KERNEL_SRCS` will be the variable containing the collected source files. The `PARENT_SCOPE` option makes the variable available to the calling CMake scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/signal/window/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source and Test Files with CMake functions\nDESCRIPTION: This snippet utilizes custom CMake functions `collect_headers`, `collect_sources`, and `collect_test_sources` to gather different types of files for the NVIDIA DALI project. These functions likely search the project directories for relevant files and append them to the specified lists (DALI_INST_HDRS, DALI_OPERATOR_SRCS, DALI_OPERATOR_TEST_SRCS). The `PARENT_SCOPE` option makes the collected lists available in the directory that included this CMake file.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/convolution/laplacian_gpu/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional CUFILE Headers and Sources with CMake\nDESCRIPTION: This CMake snippet conditionally includes CUFILE-related header and source files based on the `BUILD_CUFILE` option. If `BUILD_CUFILE` is enabled, additional header and source files related to CUFILE integration are added to the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/util/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_CUFILE)\n  set(DALI_INST_HDRS ${DALI_INST_HDRS}\n    \"${CMAKE_CURRENT_SOURCE_DIR}/cufile.h\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/cufile_helper.h\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/std_cufile.h\")\n\n  set(DALI_SRCS ${DALI_SRCS}\n    \"${CMAKE_CURRENT_SOURCE_DIR}/cufile.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/std_cufile.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Source Files with CMake\nDESCRIPTION: This snippet uses a custom CMake function `collect_test_sources` to find all test source files in the specified directories and stores them in the `DALI_OPERATOR_TEST_SRCS` variable. The `PARENT_SCOPE` option makes the variable accessible in the parent CMake scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/ssd/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This CMake code adds specified subdirectories to the current build process.  Each `add_subdirectory` call tells CMake to process the `CMakeLists.txt` file within that directory and include it in the overall build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(dynlink_nvcuvid)\nadd_subdirectory(decoder)\nadd_subdirectory(input)\nadd_subdirectory(reader)\nadd_subdirectory(legacy)\n```\n\n----------------------------------------\n\nTITLE: Including FFmpeg Support\nDESCRIPTION: Includes the `ffmpeg.cmake` module for FFmpeg support in the project.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(ffmpeg.cmake)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: This CMake function collects header files and stores them in the `DALI_INST_HDRS` variable. The `PARENT_SCOPE` option makes this variable available in the parent scope, allowing other parts of the CMake configuration to access the collected header files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/common/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Creating DALI Core Library in CMake\nDESCRIPTION: Creates the `dali_core` library, sets include directories, and links dependencies.  It uses `adjust_source_file_language_property` to ensure the correct language is used for the source files. It then sets include directories and links against dynlink_cuda, CUDART, and dynlink_cufile if BUILD_CUFILE is enabled. It also links system libraries and sets library output directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nadjust_source_file_language_property(\"${DALI_CORE_SRCS}\")\nadd_library(dali_core ${LIBTYPE} ${DALI_CORE_SRCS})\ntarget_include_directories(dali_core SYSTEM PUBLIC ${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES})\ntarget_link_libraries(dali_core PRIVATE dynlink_cuda ${CUDART_LIB})\nif (BUILD_CUFILE)\n    target_link_libraries(dali_core PRIVATE dynlink_cufile)\n  endif()\ntarget_link_libraries(dali_core PUBLIC ${DALI_SYSTEM_LIBS})\ntarget_link_libraries(dali_core PRIVATE \"-Wl,--exclude-libs,${exclude_libs}\")\nset_target_properties(dali_core PROPERTIES\n    LIBRARY_OUTPUT_DIRECTORY \"${DALI_LIBRARY_OUTPUT_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources for DALI Operators (CMake)\nDESCRIPTION: This CMake command uses the `collect_test_sources` macro to gather test source files for DALI operators. The collected test sources are stored in the `DALI_OPERATOR_TEST_SRCS` variable and are made available in the parent scope. These test sources are essential for verifying the correctness and functionality of DALI operators.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/imgcodec/util/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources with CMake\nDESCRIPTION: This CMake command uses the `collect_sources` function to collect all source files.  It stores the collected source files in the `DALI_KERNEL_SRCS` variable, which is made available in the parent scope using `PARENT_SCOPE`.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/paste/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers using CMake\nDESCRIPTION: Collects the header files for the DALI library and makes them available in the parent scope using the `collect_headers` custom function. `DALI_INST_HDRS` is the variable where collected headers will be stored.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/inflate/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources CMake\nDESCRIPTION: Collects all source files needed for the DALI library. It uses the `collect_sources` CMake function, passing the variable `DALI_SRCS` to store the collected source files and `PARENT_SCOPE` to make the source files available in the parent scope. These sources are used to build the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/c_api/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Kernel Sources with CMake\nDESCRIPTION: The `collect_sources` function gathers all source files related to the DALI kernel and stores them in the `DALI_KERNEL_SRCS` variable. The `PARENT_SCOPE` option ensures these sources are accessible in the parent scope for compilation purposes. These kernel sources are essential for the core functionality of the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/pointwise/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting headers, sources, and test sources using CMake\nDESCRIPTION: This CMake code snippet uses the `collect_headers`, `collect_sources`, and `collect_test_sources` functions to gather the respective files for the NVIDIA DALI project. The `PARENT_SCOPE` argument ensures that the variables DALI_INST_HDRS, DALI_OPERATOR_SRCS, and DALI_OPERATOR_TEST_SRCS are available in the scope that includes this file. These functions are assumed to be defined elsewhere in the CMake configuration.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/audio/mfcc/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding DALI Fuzzing Target in CMake\nDESCRIPTION: This snippet defines a function `DALI_ADD_FUZZING_TARGET` to add an executable target for fuzzing. The function takes the target name, binary name, and target source file as input. It links the target with necessary DALI libraries, sets properties like position-independent code and output directory, and adds compile flags.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/fuzzing/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(DALI_ADD_FUZZING_TARGET TARGET_NAME BINARY_NAME TARGET_SRC)\n  add_executable(${TARGET_NAME} \"${DALI_FUZZING_SRCS}\" \"${CMAKE_CURRENT_SOURCE_DIR}/${TARGET_SRC}\")\n  target_link_libraries(${TARGET_NAME} PRIVATE dali dali_operators ${DALI_LIBS})\n  target_link_libraries(${TARGET_NAME} PRIVATE \"-pie\")\n  set_target_properties(${TARGET_NAME} PROPERTIES POSITION_INDEPENDENT_CODE ON)\n  set_target_properties(${TARGET_NAME} PROPERTIES OUTPUT_NAME \"${BINARY_NAME}\")\n  set_target_properties(${TARGET_NAME} PROPERTIES\n    RUNTIME_OUTPUT_DIRECTORY \"${PROJECT_BINARY_DIR}/${DALI_WHEEL_DIR}/test\")\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Addition\nDESCRIPTION: This snippet demonstrates conditional addition of subdirectories based on the `BUILD_CVCUDA` variable. If `BUILD_CVCUDA` is true, the `filter` and `morphology` subdirectories are included, otherwise they are skipped. This allows for optional inclusion of components based on build configuration.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_CVCUDA)\n    add_subdirectory(filter)\n    add_subdirectory(morphology)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Sources using CMake\nDESCRIPTION: Collects the source files for the DALI operators and makes them available in the parent scope using the `collect_sources` custom function. `DALI_OPERATOR_SRCS` is the variable where collected sources will be stored.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/inflate/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: AFL Fuzzing Output Example\nDESCRIPTION: This is example of AFL output when running fuzzing on DALI. It displays real-time information about the fuzzing process, including execution speed, path coverage, and detected crashes or hangs. This output helps monitor the fuzzing progress and identify potential issues.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/fuzzing/README.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n            american fuzzy lop 2.52b (dali_rn50_fuzzing_target.bin)\n\n process timing  overall results \n        run time : 0 days, 0 hrs, 1 min, 9 sec          cycles done : 0      \n   last new path : none seen yet                        total paths : 4      \n last uniq crash : none seen yet                       uniq crashes : 0      \n  last uniq hang : none seen yet                         uniq hangs : 0      \n cycle progress  map coverage \n  now processing : 2 (50.00%)            map density : 10.86% / 10.86%       \n paths timed out : 1 (25.00%)         count coverage : 1.00 bits/tuple       \n stage progress  findings in depth \n  now trying : trim 16/16             favored paths : 2 (50.00%)             \n stage execs : 25/47 (53.19%)          new edges on : 4 (100.00%)            \n total execs : 123                    total crashes : 0 (0 unique)           \n  exec speed : 0.00/sec (zzzz...)      total tmouts : 0 (0 unique)           \n fuzzing strategy yields  path geometry \n   bit flips : 0/0, 0/0, 0/0                             levels : 1          \n  byte flips : 0/0, 0/0, 0/0                            pending : 4          \n arithmetics : 0/0, 0/0, 0/0                           pend fav : 2          \n  known ints : 0/0, 0/0, 0/0                          own finds : 0          \n  dictionary : 0/0, 0/0, 0/0                           imported : n/a        \n       havoc : 0/0, 0/0                               stability : 99.80%     \n        trim : 0.00%/60, n/a                         \n          [cpu000: 14%]\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: This CMake command collects all header files and makes them available in the parent scope. It is a custom function that likely uses CMake's file globbing capabilities to find header files based on a defined pattern and assigns them to the `DALI_INST_HDRS` variable.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/convolution/gaussian_blur_gpu/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Source Files with CMake\nDESCRIPTION: This CMake function collects all test source files in the current directory and its subdirectories and stores them in the variable `DALI_OPERATOR_TEST_SRCS`. The `PARENT_SCOPE` option makes the variable available in the parent scope, making it accessible to other parts of the build system that manages tests.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/math/expressions/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files for DALI Operators (CMake)\nDESCRIPTION: This line uses a custom CMake function `collect_sources` to find and collect all source files related to DALI operators and stores their paths into the variable `DALI_OPERATOR_SRCS`. The `PARENT_SCOPE` option makes the variable available in the parent scope, which enables other parts of the build system to access the list of source files during compilation and linking.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/random/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: This CMake command collects all source files and makes them available in the parent scope. It likely uses CMake's file globbing to identify source files based on a defined pattern and assigns them to the `DALI_OPERATOR_SRCS` variable for use in building the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/convolution/gaussian_blur_gpu/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: ResNet Pipeline Fuzzing Target in CMake\nDESCRIPTION: This snippet creates a fuzzing target for the ResNet pipeline in DALI. It calls the `dali_add_fuzzing_target` function, specifying the target name, binary name, and source file for the ResNet pipeline fuzzing target.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/fuzzing/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ndali_add_fuzzing_target(dali_rn50_fuzzing dali_rn50_fuzzing_target.bin rn50_target.cc)\n```\n\n----------------------------------------\n\nTITLE: Generating NVCUVID Stub (CMake)\nDESCRIPTION: This snippet defines a custom command to generate a stub file (`dynlink_nvcuvid_gen.cc`) for NVCUVID using a python script (`stub_codegen.py`). It specifies the output file, the command to execute the python script with necessary arguments (including include directories and input JSON), dependencies and a comment. The `VERBATIM` argument ensures that the command is executed exactly as specified.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/dynlink_nvcuvid/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(NVCUVID_GENERATED_STUB \"${CMAKE_CURRENT_BINARY_DIR}/dynlink_nvcuvid_gen.cc\")\nadd_custom_command(\n    OUTPUT ${NVCUVID_GENERATED_STUB}\n    COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/../../../../internal_tools/stub_generator/stub_codegen.py --unique_prefix=Nvcuvid --\n                \"${CMAKE_CURRENT_SOURCE_DIR}/../../../../internal_tools/stub_generator/nvcuvid.json\" ${NVCUVID_GENERATED_STUB}\n                \"${CMAKE_CURRENT_SOURCE_DIR}/nvcuvid.h\" \"-I${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}\"\n                \"-I${CMAKE_SOURCE_DIR}/include\" \"-I${CMAKE_SOURCE_DIR}\"\n                # for some reason QNX fails with 'too many errors emitted' is this is not set\n                \"-ferror-limit=0\"\n                ${DEFAULT_COMPILER_INCLUDE}\n    DEPENDS  ${CMAKE_CURRENT_SOURCE_DIR}/../../../../internal_tools/stub_generator/stub_codegen.py\n            \"${CMAKE_CURRENT_SOURCE_DIR}/nvcuvid.h\"\n            \"${CMAKE_CURRENT_SOURCE_DIR}/cuviddec.h\"\n            \"${CMAKE_CURRENT_SOURCE_DIR}/../../../../internal_tools/stub_generator/nvcuvid.json\"\n    COMMENT \"Running nvcuvid.h stub generator\"\n    VERBATIM)\n```\n\n----------------------------------------\n\nTITLE: Doxygen Comment Example C++\nDESCRIPTION: Shows the Javadoc-styled comment format used for Doxygen documentation in DALI C++ code. This style is used to generate API documentation for the library.\nSOURCE: https://github.com/nvidia/dali/blob/main/STYLE_GUIDE.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n/**\n * ... text ...\n */\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: Adds the 'decibel' subdirectory to the build process. This includes any CMakeLists.txt file present within the 'decibel' directory and its subdirectories into the build process, enabling the compilation and linking of its contents.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/signal/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(decibel)\n```\n\n----------------------------------------\n\nTITLE: Collecting Files with CMake Macros\nDESCRIPTION: This CMake code snippet demonstrates how to use custom macros `collect_headers`, `collect_sources`, and `collect_test_sources` to populate CMake lists with paths to header, source, and test source files respectively. The `PARENT_SCOPE` argument ensures that the created variables are available in the parent scope of the current CMake context.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/math/expressions/expression_factory_instances/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding and Linking the DALI Operators Library in CMake\nDESCRIPTION: This section defines and configures the `dali_operators` library, specifying its type, source files, and linking dependencies. It also sets the output directory for the library and links it against other DALI libraries, CUDA runtime, and other conditional libraries.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadjust_source_file_language_property(\"${DALI_OPERATOR_SRCS}\")\nadd_library(dali_operators ${LIBTYPE} ${DALI_OPERATOR_SRCS} ${DALI_OPERATOR_OBJ})\nset_target_properties(dali_operators PROPERTIES\n    LIBRARY_OUTPUT_DIRECTORY \"${DALI_LIBRARY_OUTPUT_DIR}\")\ntarget_link_libraries(dali_operators PUBLIC dali dali_kernels dali_core)\ntarget_link_libraries(dali_operators PRIVATE dynlink_cuda ${DALI_LIBS})\nif (BUILD_NVML)\n  target_link_libraries(dali_operators PRIVATE dynlink_nvml)\n  target_link_libraries(dali_operators PRIVATE \"-Wl,--exclude-libs,$<TARGET_FILE_NAME:dynlink_nvml>\"\n  )\nendif(BUILD_NVML)\n\nif (BUILD_CVCUDA)\n  target_link_libraries(dali_operators PRIVATE cvcuda nvcv_types)\nendif(BUILD_CVCUDA)\n\nif (BUILD_NVJPEG AND WITH_DYNAMIC_NVJPEG)\n  target_link_libraries(dali_operators PRIVATE dynlink_nvjpeg)\n  target_link_libraries(dali_operators PRIVATE \"-Wl,--exclude-libs,$<TARGET_FILE_NAME:dynlink_nvjpeg>\"\n  )\nendif(BUILD_NVJPEG AND WITH_DYNAMIC_NVJPEG)\n\nif (BUILD_NVIMAGECODEC)\n  if (WITH_DYNAMIC_NVIMGCODEC)\n    target_link_libraries(dali_operators PRIVATE dynlink_nvimgcodec)\n    target_link_libraries(dali_operators PRIVATE \"-Wl,--exclude-libs,$<TARGET_FILE_NAME:dynlink_nvimgcodec>\"\n    )\n  else()\n    add_dependencies(dali_operators nvImageCodec)\n    message(STATUS \"Linking dali_operators with nvImageCodec static libs: ${NVIMGCODEC_LIBS}\")\n    target_link_libraries(dali_operators PRIVATE ${NVIMGCODEC_LIBS})\n  endif()\nendif()\n\nif (WITH_DYNAMIC_NPP)\n  target_link_libraries(dali_operators PRIVATE dynlink_npp)\n  target_link_libraries(dali_operators PRIVATE \"-Wl,--exclude-libs,$<TARGET_FILE_NAME:dynlink_npp>\"\n  )\nendif(WITH_DYNAMIC_NPP)\n\nif (BUILD_CUFILE)\n  target_link_libraries(dali_operators PRIVATE dynlink_cufile)\nendif()\n# Exclude (most) statically linked dali dependencies from the exports of libdali_operators.so\ntarget_link_libraries(dali_operators PRIVATE \"-Wl,--exclude-libs,${exclude_libs}\")\n# Options for using Dockerfile FFmpeg version\ntarget_compile_definitions(dali_operators PUBLIC HAVE_AVSTREAM_CODECPAR=1)\ntarget_compile_definitions(dali_operators PUBLIC HAVE_AVBSFCONTEXT=1)\n\nset(lib_exports \"libdali_operators.map\")\nconfigure_file(\"${DALI_ROOT}/cmake/${lib_exports}.in\" \"${CMAKE_BINARY_DIR}/${lib_exports}\")\ntarget_link_libraries(dali_operators PRIVATE -Wl,--version-script=${CMAKE_BINARY_DIR}/${lib_exports})\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources for DALI Operators\nDESCRIPTION: This snippet employs the `collect_test_sources` CMake function to gather test source files for DALI operators. The `DALI_OPERATOR_TEST_SRCS` variable is used to store the collected test sources, and `PARENT_SCOPE` allows them to be accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/audio/mel_scale/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files and Headers in DALI with CMake\nDESCRIPTION: This CMake snippet collects header files, source files, and test source files.  It uses custom functions `collect_headers`, `collect_sources`, and `collect_test_sources` to gather files and store them in lists with PARENT_SCOPE visibility for use in other parts of the build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/numba_function/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources for DALI Operator Test - CMake\nDESCRIPTION: This snippet collects all source files from the current directory and adds them to the `DALI_OPERATOR_TEST_SRCS` variable, making them available for compilation into the `dali_operator_test` target. The `PARENT_SCOPE` option ensures that the variable is available in the calling scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/c_api_2/op_test/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: This CMake snippet utilizes the `collect_test_sources` function to gather all test source files related to DALI operators. These test source files are stored in the `DALI_OPERATOR_TEST_SRCS` variable, accessible in the parent scope, for use in building and running DALI operator tests.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/crop/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Excluding CUDA Stub Files from Core Sources in CMake\nDESCRIPTION: Excludes CUDA stub files (dynlink_cuda.cc) from the list of core source files. This ensures that the generated stubs are not directly included in the core library build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nlist(FILTER DALI_CORE_SRCS EXCLUDE REGEX \".*dynlink_cuda.cc\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers and Sources in CMake\nDESCRIPTION: This snippet uses custom CMake functions (`collect_headers`, `collect_sources`, `collect_test_sources`) to gather header files, source files, and test sources, respectively. The collected files are stored in variables (`DALI_INST_HDRS`, `DALI_OPERATOR_SRCS`, `DALI_OPERATOR_TEST_SRCS`) and are available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/math/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Test Sources with CMake\nDESCRIPTION: This snippet uses the `collect_test_sources` CMake function to gather test source files and store them in the `DALI_OPERATOR_TEST_SRCS` variable. The `PARENT_SCOPE` argument makes the variable available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/reader/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Defining Project and Plugin Name\nDESCRIPTION: Defines the plugin name and sets up the project using the defined plugin name. Includes CUDA, CXX, and C as supported languages.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(PLUGIN_NAME \"video\")\nproject(dali_plugin_${PLUGIN_NAME} LANGUAGES CUDA CXX C)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: Collects source files related to DALI operators and stores them in the DALI_OPERATOR_SRCS variable, making them available in the parent scope. This ensures all necessary source files are compiled into the library. The collected sources are stored in `DALI_OPERATOR_SRCS` and accessible from the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/imgcodec/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Fetching PyNvVideoCodec\nDESCRIPTION: Fetches the PyNvVideoCodec library using `FetchContent`. If the source URL and SHA256 hash are not provided in the environment, default values are used.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(PYNVVIDEOCODEC_SOURCE_URL $ENV{PYNVVIDEOCODEC_SOURCE_URL})\nset(PYNVVIDEOCODEC_SOURCE_SHA256 $ENV{PYNVVIDEOCODEC_SOURCE_SHA256})\nif (\"${PYNVVIDEOCODEC_SOURCE_URL}\" STREQUAL \"\")\n  set(PYNVVIDEOCODEC_SOURCE_URL    \"https://files.pythonhosted.org/packages/23/3b/df68633705b2d777edb4d9f3391f106cfd8cd629a760973074493b065340/PyNvVideoCodec-1.0.2.tar.gz\")\n  set(PYNVVIDEOCODEC_SOURCE_SHA256 \"02093d826eeece53928102e99e413f6c6d84e1b3c186062f2b5676d21945ac62\")\nendif()\nmessage(STATUS \"PYNVVIDEOCODEC_SOURCE_URL=${PYNVVIDEOCODEC_SOURCE_URL}\")\n\nFetchContent_Declare(\n    pynvvideocodec\n    URL      ${PYNVVIDEOCODEC_SOURCE_URL}\n    URL_HASH SHA256=${PYNVVIDEOCODEC_SOURCE_SHA256}\n)\nFetchContent_Populate(pynvvideocodec)\n```\n\n----------------------------------------\n\nTITLE: Creating Include Directives from Path List in CMake\nDESCRIPTION: This CMake snippet transforms a list of paths (contained in `INFERRED_COMPILER_INCLUDE`) into a list of include directives suitable for passing to the compiler. It iterates through each path, prepends `-I` to it, and concatenates the results into the `DEFAULT_COMPILER_INCLUDE` variable. Finally, it uses `separate_arguments` to handle possible spaces or special characters within the paths.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/nvimgcodec/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(DEFAULT_COMPILER_INCLUDE)\nforeach(incl_dir ${INFERRED_COMPILER_INCLUDE})\n  set(DEFAULT_COMPILER_INCLUDE \"${DEFAULT_COMPILER_INCLUDE} -I${incl_dir}\")\nendforeach(incl_dir)\nseparate_arguments(DEFAULT_COMPILER_INCLUDE UNIX_COMMAND  \"${DEFAULT_COMPILER_INCLUDE}\")\n```\n\n----------------------------------------\n\nTITLE: Adding a Subdirectory using CMake\nDESCRIPTION: This command adds the 'dummy' subdirectory to the CMake project. It instructs CMake to process the CMakeLists.txt file located within the 'dummy' directory, making the targets and definitions within that subdirectory available for the overall build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/test/plugins/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(dummy)\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Addition in CMake\nDESCRIPTION: Conditionally adds the 'fft' subdirectory to the build process if the 'BUILD_FFTS' variable is enabled. This allows for including the FFT-related components in the DALI library only when needed, reducing unnecessary dependencies when FFT support is not required.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/signal/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_FFTS)\n  add_subdirectory(fft)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: This snippet uses the `collect_test_sources` CMake function to gather test source files and store them in the `DALI_OPERATOR_TEST_SRCS` variable within the parent scope. This helps manage the test sources related to DALI operators.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/geometry/affine_transforms/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers with CMake\nDESCRIPTION: This snippet uses the `collect_headers` CMake function to gather header files and store them in the `DALI_INST_HDRS` variable.  The `PARENT_SCOPE` argument makes the variable available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/reader/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files in CMake\nDESCRIPTION: Collects all source files (excluding headers) and test source files within the project. The collect_sources and collect_test_sources functions are custom CMake functions. The collected file lists are stored in the DALI_KERNEL_SRCS and DALI_KERNEL_TEST_SRCS variables and are available in the parent scope for use in other parts of the CMake configuration.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/test/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source and Header Files with CMake\nDESCRIPTION: This CMake snippet uses the custom functions `collect_headers`, `collect_sources`, and `collect_test_sources` to gather all relevant files for the DALI project. It stores the results in CMake variables, making them available for subsequent build steps.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/executor/executor2/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in DALI with CMake\nDESCRIPTION: This CMake command collects all header files and stores them in the `DALI_INST_HDRS` variable, making it available in the parent scope. It relies on the custom CMake function `collect_headers` to perform the file collection.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/reduce/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: This snippet collects all the source files within the project and stores them in the `DALI_OPERATOR_SRCS` variable. This variable can then be used to specify which files should be compiled and linked into the final library. The `PARENT_SCOPE` option makes the collected source files available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/bbox/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Target CMake\nDESCRIPTION: This snippet adds a custom target named `dali-plugin-video` which depends on the python executable and `setup.py` file. This will generate a source distribution of the plugin.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(dali-plugin-video ALL)\nadd_custom_command(\n    TARGET dali-plugin-video\n    WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/pkg_src\n    COMMAND ${PYTHON_EXECUTABLE} setup.py sdist)\n```\n\n----------------------------------------\n\nTITLE: Conditional NVML Support with CMake\nDESCRIPTION: This CMake snippet configures NVML (NVIDIA Management Library) support based on the `BUILD_NVML` option. If enabled, it includes NVML-related headers. It conditionally generates a stub if `LINK_DRIVER` is not set, then creates the `dynlink_nvml` library, which links to NVML.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/util/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_NVML)\n  set(DALI_INST_HDRS ${DALI_INST_HDRS}\n    \"${CMAKE_CURRENT_SOURCE_DIR}/nvml.h\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/nvml_wrap.h\")\n\n  if (NOT LINK_DRIVER)\n    set(NVML_GENERATED_STUB \"${CMAKE_CURRENT_BINARY_DIR}/dynlink_nvml_gen.cc\")\n    add_custom_command(\n        OUTPUT ${NVML_GENERATED_STUB}\n        COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/stub_codegen.py --unique_prefix=Nvml --\n                    \"${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/nvml.json\" ${NVML_GENERATED_STUB}\n                    \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}/nvml.h\" \"-I${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}\"\n                    # for some reason QNX fails with 'too many errors emitted' is this is not set\n                    \"-ferror-limit=0\"\n                    ${DEFAULT_COMPILER_INCLUDE}\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/stub_codegen.py\n                \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}/nvml.h\"\n                \"${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/nvml.json\"\n        COMMENT \"Running nvml.h stub generator\"\n        VERBATIM)\n\n    set_source_files_properties(${NVML_GENERATED_STUB} PROPERTIES GENERATED TRUE)\n    add_library(dynlink_nvml STATIC nvml_wrap.cc nvml.cc ${NVML_GENERATED_STUB})\n  else()\n    add_library(dynlink_nvml STATIC nvml_wrap.cc nvml.cc)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Test Sources (CMake)\nDESCRIPTION: This CMake command utilizes the `collect_test_sources` macro to collect source files specifically for DALI operator tests. The collected files are stored in the `DALI_OPERATOR_TEST_SRCS` variable, and `PARENT_SCOPE` makes them accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/audio/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Suppressing CMake Policy Warning\nDESCRIPTION: Silences the `DOWNLOAD_EXTRACT_TIMESTAMP` warning in CMake versions 3.24 and later by setting the `CMP0135` policy to `NEW`.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (CMAKE_VERSION VERSION_GREATER_EQUAL \"3.24.0\")\n  cmake_policy(SET CMP0135 NEW)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources with CMake\nDESCRIPTION: This command collects test source files related to the DALI operator and stores them in the `DALI_OPERATOR_TEST_SRCS` variable. The `PARENT_SCOPE` makes them accessible to other parts of the testing framework.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/io/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Building the test suite\nDESCRIPTION: This snippet creates the `dali_test` executable, linking against the `dali` library and other dependencies like gtest. It configures the output directory, position independent code, and adds a custom target for running the tests using `add_check_gtest_target`.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_DALI_PIPELINE AND BUILD_TEST)\n  add_subdirectory(test)\n  adjust_source_file_language_property(\"${DALI_TEST_SRCS}\")\n  add_executable(dali_test \"${DALI_TEST_SRCS}\")\n\n  target_link_libraries(dali_test PUBLIC dali dali_core dali_kernels ${DALI_LIBS} gtest)\n  target_link_libraries(dali_test PRIVATE dynlink_cuda ${CUDART_LIB})\n  if (BUILD_NVML)\n    target_link_libraries(dali_test PRIVATE dynlink_nvml)\n  endif(BUILD_NVML)\n  if (BUILD_CUFILE)\n    target_link_libraries(dali_test PRIVATE dynlink_cufile)\n  endif()\n  target_link_libraries(dali_test PRIVATE \"-pie\")\n  set_target_properties(dali_test PROPERTIES RUNTIME_OUTPUT_DIRECTORY ${TEST_BINARY_DIR})\n  set_target_properties(dali_test PROPERTIES POSITION_INDEPENDENT_CODE ON)\n  set_target_properties(dali_test PROPERTIES OUTPUT_NAME \"dali_test.bin\")\n\n  add_check_gtest_target(\"check-dali-gtest\" dali_test ${TEST_BINARY_DIR})\nendif()\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for DALI and CUDA Project\nDESCRIPTION: This CMake code snippet sets up a project named `CustomCppLib` that uses NVIDIA DALI and CUDA. It finds CUDA, includes DALI, creates a shared library named `test_lib`, and links the necessary DALI and CUDA libraries. It uses `find_dali` to locate DALI include and library directories.\nSOURCE: https://github.com/nvidia/dali/blob/main/qa/TL1_custom_cpp_lib/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\nproject(CustomCppLib)\nfind_package(CUDA REQUIRED)\ninclude(/opt/dali/internal_tools/find_dali.cmake)\nadd_library(test_lib SHARED main_stub.cc)\nfind_dali(DALI_INCLUDE_DIR DALI_LIB_DIR DALI_LIBRARIES)\ntarget_include_directories(test_lib PUBLIC ${DALI_INCLUDE_DIR} ${CUDA_INCLUDE_DIRS})\ntarget_link_directories(test_lib PUBLIC ${DALI_LIB_DIR})\ntarget_link_libraries(test_lib ${DALI_LIBRARIES} ${CUDA_LIBRARIES})\n```\n\n----------------------------------------\n\nTITLE: Configuring Files CMake\nDESCRIPTION: This snippet configures `setup.py.in` and `__init__.py.in` using CMake's `configure_file` command. This allows CMake variables to be substituted into these files during the build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nconfigure_file(\"${DALI_ROOT}/plugins/setup.py.in\" \"pkg_src/setup.py\")\nconfigure_file(\"${DALI_ROOT}/plugins/__init__.py.in\" \"pkg_src/src/nvidia/dali/plugin/video/__init__.py\")\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Inclusion with CMake\nDESCRIPTION: This snippet conditionally includes the `operators` and `plugins` subdirectories in the build process if the `BUILD_TEST` flag is enabled. This allows for modular compilation where test-related components are only included during test builds.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/test/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_TEST)\n  add_subdirectory(operators)\n  add_subdirectory(plugins)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers (CMake)\nDESCRIPTION: This CMake command uses a macro (`collect_headers`) to gather header files for the DALI project. The `DALI_INST_HDRS` variable is used to store the collected header files, and `PARENT_SCOPE` ensures the variable is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/audio/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers, Sources, and Test Sources (CMake)\nDESCRIPTION: This snippet utilizes custom CMake functions (collect_headers, collect_sources, collect_test_sources) to gather header files, source files, and test source files related to DALI operators. The collected file lists are stored in CMake variables (DALI_INST_HDRS, DALI_OPERATOR_SRCS, DALI_OPERATOR_TEST_SRCS) and are made available in the parent scope for use in other parts of the build system. The exact implementation of the collect_* functions is external to this snippet.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers with CMake\nDESCRIPTION: This CMake command calls the `collect_headers` function to gather all header files. It takes `DALI_INST_HDRS` as an argument to store the list of header files and `PARENT_SCOPE` to make the variable accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/paste/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers and Sources with CMake\nDESCRIPTION: This snippet uses custom CMake functions `collect_headers`, `collect_sources`, and `collect_test_sources` to gather the necessary files for building the DALI Python function library. The collected files are stored in the specified variables with parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/python_function/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(PYTHON_FUNCTION_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Auto Contrast Augmentation Definition\nDESCRIPTION: This code snippet defines the `auto_contrast` augmentation using the `@augmentation` decorator. It applies automatic contrast adjustment. It takes the data and an underscore as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n@augmentation\ndef auto_contrast(data, _)\n```\n\n----------------------------------------\n\nTITLE: Getting DALI Version\nDESCRIPTION: Calls the `get_dali_version` function from `Utils.cmake` to extract the DALI version from the `VERSION` file. The result is stored in the `DALI_VERSION` variable.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nget_dali_version(${DALI_ROOT}/VERSION DALI_VERSION)\n```\n\n----------------------------------------\n\nTITLE: Defining DALI Fuzzing Sources\nDESCRIPTION: This snippet defines the source files needed for fuzzing DALI. `dali_test_config.cc` is included, providing necessary configurations and utilities for the fuzzing tests. These sources are later used when creating the fuzzing targets.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/fuzzing/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_FUZZING_SRCS\n  \"${PROJECT_SOURCE_DIR}/dali/test/dali_test_config.cc\"\n)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files for DALI\nDESCRIPTION: This CMake function collects all source files and assigns them to the variable DALI_SRCS in the parent scope. This provides a centralized way to manage the source files used in the project, making it easier to compile the library or application.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/operator/builtin/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Source Files in DALI with CMake\nDESCRIPTION: This CMake code snippet employs the `collect_test_sources` function to collect all test source files in the DALI project. The collected test sources are stored in the `DALI_KERNEL_TEST_SRCS` variable and are available in the parent scope. These files are typically used for unit testing and integration testing of the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/structure/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Numpy Reader for 3D Data\nDESCRIPTION: This snippet demonstrates the use of the Numpy reader with 3D MRI data.  It defines a DALI pipeline (`pipe_3d`) that reads 3D numpy arrays from the specified directory. It then prints the shape of the loaded data and displays the first four slices of the first sample in the batch.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/data_loading/numpy_reader.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(batch_size=batch_size, num_threads=3, device_id=0)\ndef pipe_3d():\n    data = fn.readers.numpy(\n        device=\"cpu\", file_root=data_dir_3d, file_filter=\"*.npy\"\n    )\n    return data\n\n\ndata_3d = run(pipe_3d())\nfor s in range(len(data_3d)):\n    print(f\"Sample {s} shape: {data_3d[s].shape}\")\n\n# Displaying first 4 slices of the first sample in the batch\nplot_batch(\n    [\n        data_3d[0][0, :, :],\n        data_3d[0][1, :, :],\n        data_3d[0][2, :, :],\n        data_3d[0][3, :, :],\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Test Sources using CMake\nDESCRIPTION: Collects the test source files for the DALI operators and makes them available in the parent scope using the `collect_test_sources` custom function. `DALI_OPERATOR_TEST_SRCS` is the variable where collected test sources will be stored.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/inflate/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Enabling Separable Compilation for CUDA\nDESCRIPTION: This line enables separable compilation for CUDA, which can improve build times.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CUDA_SEPARABLE_COMPILATION ON)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Headers with CMake\nDESCRIPTION: This snippet uses the `collect_headers` CMake function to find and store DALI operator header files. The `DALI_INST_HDRS` variable will contain the list of collected header files, and the `PARENT_SCOPE` argument ensures that this variable is accessible in the parent scope of the current CMake context.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/resize/experimental/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting C Standard\nDESCRIPTION: This CMake command sets the C standard to C11.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_C_STANDARD 11)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: This snippet uses the `collect_sources` CMake function to gather source files and store them in the `DALI_OPERATOR_SRCS` variable within the parent scope. This helps manage the source files used for building the DALI operators.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/geometry/affine_transforms/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources for DALI\nDESCRIPTION: This CMake function collects test source files and assigns them to the variable DALI_TEST_SRCS in the parent scope. This is essential for managing and compiling the unit tests associated with the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/operator/builtin/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Excluding CUFile Stub Files from Core Sources in CMake\nDESCRIPTION: Excludes CUFile stub files (dynlink_cufile.cc) from the list of core source files. This ensures that the generated stubs are not directly included in the core library build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nlist(FILTER DALI_CORE_SRCS EXCLUDE REGEX \".*dynlink_cufile.cc\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Operator Test Sources with CMake Macro\nDESCRIPTION: This snippet uses the `collect_test_sources` CMake macro to find and store operator test source files in the `DALI_OPERATOR_TEST_SRCS` variable, accessible in the parent scope. These source files are used for building the tests for DALI operators.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/convolution/filter_gpu/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: This CMake function collects test source files and stores them in the `DALI_KERNEL_TEST_SRCS` variable. The `PARENT_SCOPE` option makes this variable available in the parent scope, allowing other parts of the CMake configuration to access the collected test source files, typically for building and running tests.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/common/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers and Sources in DALI with CMake\nDESCRIPTION: This CMake code snippet collects header files, source files, and test source files for the NVIDIA DALI project.  It uses the custom CMake functions `collect_headers`, `collect_sources`, and `collect_test_sources` to populate CMake variables with the corresponding lists of files, making them available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/reduce/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional LMDB Benchmark Inclusion (CMake)\nDESCRIPTION: This snippet conditionally includes LMDB-related benchmark sources if the `BUILD_LMDB` option is enabled. The `list(APPEND ...)` command adds the specified source files to the `DALI_BENCHMARK_SRCS` list.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/benchmark/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_LMDB)\n    list(APPEND DALI_BENCHMARK_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/caffe_alexnet_bench.cc\")\n    list(APPEND DALI_BENCHMARK_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/caffe2_alexnet_bench.cc\")\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: This CMake function collects source files and stores them in the `DALI_KERNEL_SRCS` variable.  The `PARENT_SCOPE` option makes this variable available in the parent scope, enabling other parts of the CMake configuration to utilize the collected source files for building the DALI kernel.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/common/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources with CMake\nDESCRIPTION: This CMake command employs the `collect_test_sources` function to gather all test source files.  The `DALI_KERNEL_TEST_SRCS` variable stores the list of test source files, with `PARENT_SCOPE` making them available to the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/paste/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in DALI using CMake\nDESCRIPTION: This CMake command uses the `collect_sources` function to gather source files and store them in the `DALI_OPERATOR_SRCS` variable. The `PARENT_SCOPE` argument makes the variable available to the parent scope, typically the calling CMakeLists.txt file. These are the main source files used for building the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/random/noise/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory with CMake\nDESCRIPTION: This line adds a subdirectory named `mel_scale` to the current CMake project. This allows CMake to process the CMakeLists.txt file in the specified directory and include its build targets in the main project's build process. This assumes `mel_scale` directory exists and contains a CMakeLists.txt file.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/audio/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(mel_scale)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources with CMake\nDESCRIPTION: This snippet utilizes a custom CMake macro `collect_test_sources` to collect test source files for the DALI kernel. The gathered test source files are stored in the `DALI_KERNEL_TEST_SRCS` variable and made available in the parent scope. This allows for easy access to the test sources during the build and testing process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/test/resampling_test/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional NVJPEG Subdirectory Inclusion CMake\nDESCRIPTION: Conditionally includes the `nvjpeg` subdirectory if the `BUILD_NVJPEG` variable is set. This is typically used when building with support for NVIDIA's JPEG codec. No specific inputs or outputs other than the presence or absence of the nvjpeg component in the build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_NVJPEG)\n  add_subdirectory(nvjpeg)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers with CMake\nDESCRIPTION: This CMake macro, `collect_headers`, is designed to gather header files for the DALI project. It takes a variable name (`DALI_INST_HDRS`) and a scope (`PARENT_SCOPE`) as input, presumably to store and propagate the collected header files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/sequence/optical_flow/optical_flow_impl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Parsing CUDA Version\nDESCRIPTION: Parses the CUDA version string into major, minor, and patch components using the `parse_cuda_version` function defined in `Utils.cmake`.  These components are then available as variables for the rest of the build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali_tf_plugin/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nparse_cuda_version(${CUDA_VERSION} CUDA_VERSION_MAJOR CUDA_VERSION_MINOR CUDA_VERSION_PATCH CUDA_VERSION_SHORT CUDA_VERSION_SHORT_DIGIT_ONLY)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: Adds the `image`, `jpeg`, `cache`, `host`, and `peek_shape` subdirectories to the build. This ensures that the corresponding code is compiled and linked into the final library. No specific inputs or outputs other than the components being included in the build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(image)\nadd_subdirectory(jpeg)\nadd_subdirectory(cache)\nadd_subdirectory(host)\nadd_subdirectory(peek_shape)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers with CMake\nDESCRIPTION: This CMake function `collect_headers` collects all header files within the specified directories and stores them in the `DALI_INST_HDRS` variable. The `PARENT_SCOPE` option makes the collected headers available in the parent scope, allowing other CMake scripts to use them for compilation and linking.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/pointwise/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in DALI with CMake\nDESCRIPTION: This CMake snippet collects the source files required to build the DALI kernel. The `collect_sources` function is used to gather the source files and stores them in the `DALI_KERNEL_SRCS` variable. The `PARENT_SCOPE` argument makes these sources available to the parent scope for compilation.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/geom/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources with CMake\nDESCRIPTION: This CMake snippet leverages the `collect_test_sources` function to gather all source files specifically related to testing the DALI kernel. These test source files are stored in the `DALI_KERNEL_TEST_SRCS` variable and propagated to the parent scope for use in building and running tests.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/signal/decibel/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting Library Output Directory in CMake\nDESCRIPTION: This snippet sets the output directory for the compiled shared library to the directory specified by the `TEST_BINARY_DIR` variable. This ensures that the built plugin is placed in the desired location for testing or deployment.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/test/plugins/dummy/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(${lib_name} PROPERTIES\n    LIBRARY_OUTPUT_DIRECTORY ${TEST_BINARY_DIR})\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: This CMake snippet uses the `collect_headers` function to gather all header files within the NVIDIA DALI project. The collected header files are stored in the `DALI_INST_HDRS` variable and are available in the parent scope for use in subsequent build steps, such as installation or packaging.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/crop/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: This snippet uses the `collect_headers` CMake function to gather header files and store them in the `DALI_INST_HDRS` variable within the parent scope. This helps organize header dependencies for the DALI project.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/geometry/affine_transforms/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting DALI Header Files with CMake\nDESCRIPTION: This CMake snippet sets the DALI header files using the `set` command and specifies the paths to various header files required for the DALI library. These headers are later used during the library build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/util/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_INST_HDRS ${DALI_INST_HDRS}\n  \"${CMAKE_CURRENT_SOURCE_DIR}/crop_window.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/file.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/image.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/mmaped_file.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/std_file.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/odirect_file.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/ocv.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/random_crop_generator.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/thread_safe_queue.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/numpy.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/user_stream.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/uri.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/s3_file.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/s3_filesystem.h\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/s3_client_manager.h\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources with CMake\nDESCRIPTION: This CMake function collects test source files and stores them in the `DALI_KERNEL_TEST_SRCS` variable in the parent scope. These sources are generally used for unit testing and integration testing of the DALI kernel.  The test sources collected would be compiled into test executables.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/audio/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Defining DALI TensorFlow Project\nDESCRIPTION: Defines the project name as `dali_tf`. This is a standard CMake command that sets up the project context.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali_tf_plugin/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nproject(dali_tf)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for Noise Module in DALI (CMake)\nDESCRIPTION: This line adds the 'noise' subdirectory to the current build process. This assumes that a CMakeLists.txt file exists within the 'noise' directory, defining how the noise module should be built and integrated into the overall DALI project. This allows for modular organization and independent building of parts of the library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/random/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(noise)\n```\n\n----------------------------------------\n\nTITLE: Adding DALI Operator Subdirectories (CMake)\nDESCRIPTION: This snippet adds several subdirectories, each representing a DALI operator (erase, reduce, resize, slice, transpose), to the current CMake project. This allows the CMake build system to process the CMakeLists.txt files located within those subdirectories, incorporating them into the overall build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(erase)\nadd_subdirectory(reduce)\nadd_subdirectory(resize)\nadd_subdirectory(slice)\nadd_subdirectory(transpose)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources with CMake\nDESCRIPTION: The `collect_test_sources` CMake macro collects the source files for the DALI project's tests. It receives a variable name (`DALI_OPERATOR_TEST_SRCS`) along with a scope (`PARENT_SCOPE`) which are presumably used to store and propagate the test source file list.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/sequence/optical_flow/optical_flow_impl/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Including DALI utils\nDESCRIPTION: Includes the `Utils.cmake` file from the DALI root directory. This file likely contains utility functions and macros used throughout the DALI build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(${DALI_ROOT}/cmake/Utils.cmake)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources with CMake\nDESCRIPTION: This CMake directive uses the `collect_sources` function to gather source files related to DALI operators. `DALI_OPERATOR_SRCS` is the variable to store collected source file paths. The `PARENT_SCOPE` argument ensures that the collected sources are available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/io/file/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources with CMake\nDESCRIPTION: This snippet uses the `collect_test_sources` function to collect source files for testing the DALI core. The `DALI_CORE_TEST_SRCS` variable will store the list of collected test source files. `PARENT_SCOPE` makes this variable available in the parent scope where this CMake code is executed.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/exec/tasking/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_CORE_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Test Sources in DALI with CMake\nDESCRIPTION: This snippet demonstrates the usage of custom CMake functions to collect header files, source files, and test source files. It uses `collect_headers` to gather header files, `collect_sources` to gather source files, and `collect_test_sources` to collect test source files. The `PARENT_SCOPE` argument ensures that the collected files are available in the parent scope for further use in the CMake configuration.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/morphology/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Finding DALI Library Directory using Python\nDESCRIPTION: This snippet uses Python to find the DALI library directory. It executes a Python command to import the `nvidia.dali` module and retrieve the library directory using `dali.sysconfig.get_lib_dir()`. The output is then stored in the `DALI_LIB_DIR` variable, after removing any leading/trailing whitespace.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/customdummy/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nexecute_process(\n        COMMAND python -c \"import nvidia.dali as dali; \\\n                           print(dali.sysconfig.get_lib_dir())\"\n        OUTPUT_VARIABLE DALI_LIB_DIR)\nstring(STRIP ${DALI_LIB_DIR} DALI_LIB_DIR)\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Minimum Version\nDESCRIPTION: Specifies the minimum required CMake version for the project. This ensures that the CMake features used in the script are supported.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.21)\n```\n\n----------------------------------------\n\nTITLE: Setting Generated Source File Properties (CMake)\nDESCRIPTION: This snippet sets the `GENERATED` property to `TRUE` for the generated stub file (`dynlink_nvcuvid_gen.cc`). This tells CMake that the file is generated as part of the build process and should not be considered as a source file that needs to be manually managed.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/dynlink_nvcuvid/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset_source_files_properties(${NVCUVID_GENERATED_STUB} PROPERTIES GENERATED TRUE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: Collects test source files and stores their paths in the DALI_OPERATOR_TEST_SRCS variable. The PARENT_SCOPE option makes the variable available in the parent scope, allowing other parts of the CMake configuration to access the list of test source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/segmentation/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Signing off Git Commits with -s flag\nDESCRIPTION: This command demonstrates how to sign off a Git commit using the `-s` flag. This appends a `Signed-off-by` line to the commit message, indicating that the contributor agrees to the Developer Certificate of Origin (DCO). The DCO certifies that the contribution is the original work of the contributor or that they have the rights to submit it under the same or a compatible license.\nSOURCE: https://github.com/nvidia/dali/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ git commit -s -m \"Add cool feature.\"\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: Collects test source files for DALI operators and stores them in the DALI_OPERATOR_TEST_SRCS variable, making them available in the parent scope. This allows for building and running tests for the operators. The test sources are stored in `DALI_OPERATOR_TEST_SRCS` and accessible from the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/imgcodec/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Inclusion with CMake\nDESCRIPTION: This snippet conditionally includes the `cvcuda` subdirectory in the CMake project. The inclusion depends on whether the `BUILD_CVCUDA` variable is set to true. This is commonly used to enable or disable certain features or components based on build configurations.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/remap/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_CVCUDA)\n    add_subdirectory(cvcuda)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Test Sources Variable in CMake\nDESCRIPTION: This line sets the DALI_OPERATOR_TEST_SRCS variable, likely containing a list of source files for operator tests.  It uses the PARENT_SCOPE option to make this variable available in the parent directory's CMake context.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/c_api_2/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(DALI_OPERATOR_TEST_SRCS ${DALI_OPERATOR_TEST_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting DALI Flavor and Version\nDESCRIPTION: Conditionally sets the DALI flavor and version based on the `DALI_BUILD_FLAVOR` variable. If a build flavor is defined, it's appended to the version string and stored in separate variables.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali_tf_plugin/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif (DALI_BUILD_FLAVOR)\n  set(DALI_FLAVOR \"${DALI_BUILD_FLAVOR} \")\n  set(DALI_FLAVOR_MINUS \"-${DALI_BUILD_FLAVOR}\")\n  set(DALI_VERSION \"${DALI_VERSION}.${TIMESTAMP}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files in CMake\nDESCRIPTION: This snippet employs a custom CMake function, 'collect_sources', to identify and collect source files.  The list of found source files is stored in the 'DALI_SRCS' variable, accessible in the parent scope for subsequent build steps and dependency management.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/executor/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_sources(DALI_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources CMake\nDESCRIPTION: Collects source files specifically for operator tests within the DALI library. It employs the `collect_test_sources` CMake function, storing the file paths in the `DALI_OPERATOR_TEST_SRCS` variable and making them accessible in the parent scope. These sources are used to build the test executables.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/c_api/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source and Header Files\nDESCRIPTION: These calls to custom CMake functions collect header and source files for the DALI kernels and tests. They use `PARENT_SCOPE` to make the headers available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_KERNEL_SRCS)\ncollect_test_sources(DALI_KERNEL_TEST_SRCS)\n```\n\n----------------------------------------\n\nTITLE: Project Definition\nDESCRIPTION: This CMake command defines the project name and the languages used in the project (CUDA, C++, and C).\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nproject(naive_histogram_plugin LANGUAGES CUDA CXX C)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in DALI with CMake\nDESCRIPTION: This CMake command collects all source files for DALI operator tests and stores them in the `DALI_OPERATOR_TEST_SRCS` variable, accessible in the parent scope. The custom CMake function `collect_test_sources` is used to locate and collect these test-related source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/reduce/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting Variables to Parent Scope (CMake)\nDESCRIPTION: This snippet sets the `DALI_OPERATOR_SRCS` and `DALI_OPERATOR_TEST_SRCS` variables in the parent scope, making them available to other CMake files in the project.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/loader/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_OPERATOR_SRCS ${DALI_OPERATOR_SRCS} PARENT_SCOPE)\nset(DALI_OPERATOR_TEST_SRCS ${DALI_OPERATOR_TEST_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources and Headers with CMake\nDESCRIPTION: This CMake snippet uses custom functions to collect header files, source files, and test source files for the DALI library. `collect_headers` gathers header files, `collect_sources` gathers source files, and `collect_test_sources` gathers test-related source files. These functions are expected to be defined elsewhere in the CMake project.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/color_manipulation/equalize/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet adds the 'debayer' and 'equalize' subdirectories to the current CMake project. This allows the build system to process the CMakeLists.txt files present in these directories.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/color_manipulation/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(debayer)\nadd_subdirectory(equalize)\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files in DALI with CMake\nDESCRIPTION: This CMake code snippet uses the `collect_headers` function to gather all header files within the DALI project. The collected headers are stored in the `DALI_INST_HDRS` variable and are available in the parent scope for further use in the build process. No external dependencies are explicitly shown in the snippet.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/structure/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers and Sources with CMake\nDESCRIPTION: This CMake code snippet utilizes custom functions, `collect_headers`, `collect_sources`, and `collect_test_sources`, to gather header and source files. The `collect_headers` function collects installation headers, `collect_sources` collects operator source files, and `collect_test_sources` collects test source files.  The `PARENT_SCOPE` keyword makes the collected variables accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/util/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n\nset(DALI_OPERATOR_SRCS ${DALI_OPERATOR_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in DALI using CMake\nDESCRIPTION: This CMake command uses the `collect_test_sources` function to gather source files for tests and store them in the `DALI_OPERATOR_TEST_SRCS` variable. The `PARENT_SCOPE` argument makes the variable available to the parent scope. These are the source files used to create and run unit and integration tests for the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/random/noise/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Copying Files CMake\nDESCRIPTION: This snippet copies various files required for the plugin's source distribution. It includes source code, license files, and CMake configuration files.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(COPY \"pkg_src\" DESTINATION \".\") # copy to build folder\nfile(COPY \"${DALI_ROOT}/Acknowledgements.txt\" DESTINATION \"pkg_src\")\nfile(COPY \"${DALI_ROOT}/COPYRIGHT\" DESTINATION \"pkg_src\")\nfile(COPY \"${DALI_ROOT}/LICENSE\" DESTINATION \"pkg_src\")\nfile(COPY \"${DALI_ROOT}/plugins/common.cmake\" DESTINATION \"pkg_src\")\nfile(COPY \"${DALI_ROOT}/plugins/MANIFEST.in\" DESTINATION \"pkg_src\")\nfile(COPY \"${DALI_ROOT}/plugins/generate_plugin_stubs.py\" DESTINATION \"pkg_src\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources in CMake\nDESCRIPTION: Collects source files for the DALI operators and makes them available in the parent scope. This allows the build system to compile the operator implementations.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/audio/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet adds several subdirectories to the current CMake project. Each subdirectory likely contains source code and its own CMakeLists.txt to define its build process. This structure helps organize the DALI project into logical modules like data handling, execution, graph construction, operators, protobuf definitions, utilities, and workspace management.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(data)\nadd_subdirectory(executor)\nadd_subdirectory(graph)\nadd_subdirectory(operator)\nadd_subdirectory(proto)\nadd_subdirectory(util)\nadd_subdirectory(workspace)\n```\n\n----------------------------------------\n\nTITLE: Installing DALI Video Plugin\nDESCRIPTION: Installs the DALI video plugin library and associated directories containing stubs into the specified destination directories.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS dali_${PLUGIN_NAME}\n    RUNTIME DESTINATION nvidia/dali/plugin/${PLUGIN_NAME}\n    LIBRARY DESTINATION nvidia/dali/plugin/${PLUGIN_NAME}\n)\ninstall(DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/fn/plugin/${PLUGIN_NAME}/\n    DESTINATION nvidia/dali/fn/plugin/${PLUGIN_NAME}\n)\ninstall(DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/ops/plugin/${PLUGIN_NAME}/\n    DESTINATION nvidia/dali/ops/plugin/${PLUGIN_NAME}\n)\n```\n\n----------------------------------------\n\nTITLE: Build DALI with AFL compiler extensions\nDESCRIPTION: This cmake command configures the DALI build to use the AFL compiler wrappers, enable fuzzing, disable Python and other optional components, and set CUDA architecture. This prepares DALI for fuzzing with AFL.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/fuzzing/README.md#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -DCMAKE_CXX_COMPILER=afl-clang-fast++ -DCMAKE_C_COMPILER=afl-clang-fast -DCUDA_TARGET_ARCHS=61 -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_FUZZING=ON -DBUILD_PYTHON=OFF -DBUILD_LMDB=OFF -DBUILD_NVOF=OFF ..\n```\n\n----------------------------------------\n\nTITLE: Adding Shared Library\nDESCRIPTION: This CMake command creates a shared library named `naivehistogram` from the specified source files. The library will be linked against the DALI library.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(naivehistogram SHARED ${PLUGIN_SOURCES})\ntarget_link_libraries(naivehistogram dali)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory with CMake\nDESCRIPTION: This line adds the `affine_transforms` subdirectory to the build, allowing CMake to process its CMakeLists.txt and include its contents in the overall build process.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/geometry/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(affine_transforms)\n```\n\n----------------------------------------\n\nTITLE: Defining the project name\nDESCRIPTION: Defines the project name as 'dali_plugins'. This name is used for various build-related tasks, such as generating build files and naming the resulting libraries or executables.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nproject(dali_plugins)\n```\n\n----------------------------------------\n\nTITLE: Collecting Kernel Test Sources with CMake\nDESCRIPTION: This CMake function `collect_test_sources` is used to gather all source files for testing the DALI kernel. The collected sources are stored in the `DALI_KERNEL_TEST_SRCS` variable and are available in the parent scope due to the `PARENT_SCOPE` argument. These test sources are crucial for ensuring the correctness and stability of the DALI kernel.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/pointwise/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source and Header Files CMake\nDESCRIPTION: This CMake code snippet defines calls to custom CMake functions to gather different types of files for the project. `collect_headers` gathers header files and stores them in the `DALI_INST_HDRS` variable. `collect_sources` gathers source files and stores them in `DALI_OPERATOR_SRCS`. `collect_test_sources` gathers test source files and stores them in `DALI_OPERATOR_TEST_SRCS`. The `PARENT_SCOPE` keyword makes these variables available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/math/normalize/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files with CMake\nDESCRIPTION: The `collect_headers` function is called to gather all header files. The collected headers are stored in the `DALI_INST_HDRS` variable. `PARENT_SCOPE` ensures that the variable is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/math/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Determining GCC System Include Directories CMake\nDESCRIPTION: This snippet uses the CMake function `DETERMINE_GCC_SYSTEM_INCLUDE_DIRS` to find the system include directories used by the C++ compiler. It populates the `INFERED_COMPILER_INCLUDE` variable with the list of these directories. It's used to ensure proper compilation flags when generating the nvjpeg stub.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/nvjpeg/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nDETERMINE_GCC_SYSTEM_INCLUDE_DIRS(\"c++\" \"${CMAKE_CXX_COMPILER}\" \"${CMAKE_CXX_FLAGS}\" INFERED_COMPILER_INCLUDE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files with CMake\nDESCRIPTION: This CMake function collects all source files in the current directory and its subdirectories and stores them in the variable `DALI_OPERATOR_SRCS`. The `PARENT_SCOPE` option makes the variable available in the parent scope, allowing other parts of the build system to access the collected source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/math/expressions/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources and Test Sources using CMake\nDESCRIPTION: This CMake script calls three custom macros: `collect_headers`, `collect_sources`, and `collect_test_sources`. These macros are used to collect header files, source code files, and test source code files, respectively, within the DALI project. The collected files are stored in CMake variables (`DALI_INST_HDRS`, `DALI_OPERATOR_SRCS`, `DALI_OPERATOR_TEST_SRCS`) in the parent scope, making them accessible to other parts of the CMake build system.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/signal/decibel/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Appending CPU Video Decoder Source File with CMake\nDESCRIPTION: This snippet conditionally appends the `video_decoder_cpu.cc` source file to the `DALI_OPERATOR_SRCS` list if the `BUILD_FFMPEG` flag is set. The `BUILD_FFMPEG` flag determines whether the CPU-based video decoder implementation should be included in the build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/decoder/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_FFMPEG)\n  list(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/video_decoder_cpu.cc\")\nendif (BUILD_FFMPEG)\n```\n\n----------------------------------------\n\nTITLE: Adding Tasking Subdirectory in CMake\nDESCRIPTION: This CMake command adds the `tasking` subdirectory to the build process. It allows CMake to locate and process the CMakeLists.txt file within the `tasking` directory, incorporating it into the overall build configuration.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/exec/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(tasking)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI sources using CMake\nDESCRIPTION: This CMake snippet collects header files, source files, and test source files for the NVIDIA DALI library. It relies on custom CMake functions: `collect_headers`, `collect_sources`, and `collect_test_sources`. The collected files are stored in variables `DALI_INST_HDRS`, `DALI_OPERATOR_SRCS`, and `DALI_OPERATOR_TEST_SRCS` respectively, and passed to the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/segmentation/utils/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting DALI Flavor (Conditional)\nDESCRIPTION: Conditionally sets the DALI flavor variables based on the `DALI_BUILD_FLAVOR` variable. These variables are used to customize the build process based on the specific DALI flavor.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(DALI_BUILD_FLAVOR)\n  set(DALI_FLAVOR \"${DALI_BUILD_FLAVOR} \")\n  set(DALI_FLAVOR_MINUS \"-${DALI_BUILD_FLAVOR}\")\n  set(DALI_VERSION \"${DALI_VERSION}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting DALI Build Flavor and Version\nDESCRIPTION: This snippet sets the DALI build flavor and version based on the `DALI_BUILD_FLAVOR` variable. If `DALI_BUILD_FLAVOR` is defined, it sets `DALI_FLAVOR`, `DALI_FLAVOR_MINUS`, and `DALI_VERSION` variables.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/python/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (DALI_BUILD_FLAVOR)\n  set(DALI_FLAVOR \"${DALI_BUILD_FLAVOR} \")\n  set(DALI_FLAVOR_MINUS \"-${DALI_BUILD_FLAVOR}\")\n  set(DALI_VERSION \"${DALI_VERSION}.${TIMESTAMP}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Flags for C++17\nDESCRIPTION: This snippet sets the CUDA flags to use the C++17 standard. It ensures that the CUDA code is compiled using the C++17 standard.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/customdummy/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -std=c++17\")\n```\n\n----------------------------------------\n\nTITLE: Including CUDA Toolkit Directories\nDESCRIPTION: This CMake command includes the CUDA toolkit include directories. This allows the compiler to find the necessary CUDA header files.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(SYSTEM \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources for DALI in CMake\nDESCRIPTION: This line collects source files specifically intended for testing the DALI project. The collected sources are stored in the DALI_TEST_SRCS variable, available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/c_api_2/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Finding DALI Compile Flags using Python\nDESCRIPTION: This snippet executes a python command to retrieve DALI compile flags using `dali.sysconfig.get_compile_flags()`. The output flags are then stored in `DALI_COMPILE_FLAGS` variable after removing any leading or trailing whitespace.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/customdummy/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nexecute_process(\n        COMMAND python -c \"import nvidia.dali as dali; print(\\\" \\\n                          \\\".join(dali.sysconfig.get_compile_flags()))\"\n        OUTPUT_VARIABLE DALI_COMPILE_FLAGS)\nstring(STRIP ${DALI_COMPILE_FLAGS} DALI_COMPILE_FLAGS)\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files with CMake\nDESCRIPTION: This snippet uses a custom CMake function `collect_headers` to find all header files in the specified directories and stores them in the `DALI_INST_HDRS` variable. The `PARENT_SCOPE` option makes the variable available to the parent scope, which is typically the main CMakeLists.txt file.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/ssd/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version\nDESCRIPTION: Specifies the minimum required version of CMake for the project. Ensures that the CMake version used is compatible with the project's build scripts.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali_tf_plugin/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.2)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI headers in CMake\nDESCRIPTION: This CMake function `collect_headers` is used to collect all header files and store them in the `DALI_INST_HDRS` variable within the parent scope. This ensures that the headers are available for use in other parts of the build process. The variable `DALI_INST_HDRS` is created or updated in the parent scope so other CMake scripts can access them.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/os/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Finding CUDA Library\nDESCRIPTION: Finds the CUDA library (libcuda.so stub) to be preloaded to generate Python signature files.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nfind_library(cuda_LIBRARY cuda\n             PATHS ${CMAKE_CUDA_IMPLICIT_LINK_DIRECTORIES}\n             PATH_SUFFIXES lib/stubs lib64/stubs)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet demonstrates how to add subdirectories to the current CMake project. Each `add_subdirectory` call includes the CMakeLists.txt file from the specified subdirectory, allowing for modular project organization. These subdirectories likely contain source code related to specific image processing operations.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(color)\nadd_subdirectory(crop)\nadd_subdirectory(convolution)\nadd_subdirectory(distortion)\nif (BUILD_CVCUDA)\n    add_subdirectory(filter)\n    add_subdirectory(morphology)\nendif()\nadd_subdirectory(mask)\nadd_subdirectory(paste)\nadd_subdirectory(remap)\nadd_subdirectory(resize)\n```\n\n----------------------------------------\n\nTITLE: Adding Optical Flow Subdirectories with CMake\nDESCRIPTION: This snippet uses the `add_subdirectory` CMake command to include the `optical_flow_impl` and `optical_flow_adapter` directories in the build process. This command ensures that the CMakeLists.txt files within those subdirectories are processed and their targets are added to the overall build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/sequence/optical_flow/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(optical_flow_impl)\nadd_subdirectory(optical_flow_adapter)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: Adds the 'utils' subdirectory to the current CMake project. This allows the CMake build process to include the CMakeLists.txt file located within the 'utils' directory, incorporating its build instructions and targets into the overall project build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/segmentation/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(utils)\n```\n\n----------------------------------------\n\nTITLE: Enabling Checkpointing in DALI Pipeline (Python)\nDESCRIPTION: This snippet demonstrates how to enable checkpointing when defining a DALI pipeline using the `@pipeline_def` decorator.  By setting `enable_checkpointing=True`, DALI tracks the state of operators for saving and restoring.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/advanced_topics_checkpointing.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@pipeline_def(..., enable_checkpointing=True)\ndef pipeline():\n    ...\n\np = pipeline()\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers with CMake\nDESCRIPTION: This command collects the DALI installation headers and makes them available in the parent scope. This ensures that other parts of the project can access the necessary header files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/geometry/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers and Sources with CMake Macros\nDESCRIPTION: This snippet shows the usage of custom CMake macros `collect_headers`, `collect_sources`, and `collect_test_sources` to gather header, source, and test files respectively. Each macro takes a variable name and a scope (PARENT_SCOPE in this case) as arguments. This populates the specified CMake list with the files found. These functions are expected to be defined elsewhere in the CMake infrastructure.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/remap/cvcuda/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Omitting Trailing Dimensions in DALI\nDESCRIPTION: This example demonstrates that when indexing and slicing multidimensional data, the trailing dimensions can be omitted. This is equivalent to passing a full-range slice to all trailing dimensions.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/indexing.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwide = letterboxed[20:-20,:,:]   # slice height, keep width and channels\nwide = letterboxed[20:-20,:]     # this line is equivalent to the previous one\nwide = letterboxed[20:-20]       # ...and so is this one\n```\n\n----------------------------------------\n\nTITLE: Setting up library versioning\nDESCRIPTION: This snippet configures a version script for the `libdali.so` library, which is used to manage symbol visibility. It links the library with the version script and adds dependencies.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_DALI_PIPELINE)\n  # Define symbol version script for libdali.so\n  set(dali_lib_exports \"libdali.map\")\n  configure_file(\"${PROJECT_SOURCE_DIR}/cmake/${dali_lib_exports}.in\" \"${CMAKE_BINARY_DIR}/${dali_lib_exports}\")\n  target_link_libraries(dali PRIVATE -Wl,--version-script=${CMAKE_BINARY_DIR}/${dali_lib_exports})\n\n  # Link in dali's dependencies\n  message(STATUS \"Adding dependencies to target `dali`: '${DALI_LIBS}'\")\n  target_link_libraries(dali PUBLIC dali_core dali_kernels)\n  target_link_libraries(dali PRIVATE ${DALI_LIBS} dynlink_cuda)\n  # Exclude (most) statically linked dali dependencies from the exports of libdali.so\n  target_link_libraries(dali PRIVATE \"-Wl,--exclude-libs,${exclude_libs}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Sources Scope in CMake\nDESCRIPTION: Sets the `DALI_OPERATOR_SRCS` variable in the parent scope, making the list of operator source files available to other parts of the build system.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_OPERATOR_SRCS ${DALI_OPERATOR_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Generating CUDA Stubs with Custom Command in CMake\nDESCRIPTION: This generates CUDA stubs using a Python script. It defines a custom command that executes the script, specifying input files, output file, and dependencies. It uses `add_custom_command` to define the generation step and creates `dynlink_cuda` library. `DEFAULT_COMPILER_INCLUDE` is used to propagate compiler include directories.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT LINK_DRIVER)\n  set(CUDA_GENERATED_STUB \"${CMAKE_CURRENT_BINARY_DIR}/dynlink_cuda_gen.cc\")\n  add_custom_command(\n      OUTPUT ${CUDA_GENERATED_STUB}\n      COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/stub_codegen.py --unique_prefix=Cuda --\n                  \"${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/cuda.json\" ${CUDA_GENERATED_STUB}\n                  \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}/cuda.h\" \"-I${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}\"\n                  # for some reason QNX fails with 'too many errors emitted' is this is not set\n                  \"-ferror-limit=0\"\n                  ${DEFAULT_COMPILER_INCLUDE}\n      DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/stub_codegen.py\n              \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}/cuda.h\"\n              \"${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/cuda.json\"\n      COMMENT \"Running cuda.h stub generator\"\n      VERBATIM)\n\n  set_source_files_properties(${CUDA_GENERATED_STUB} PROPERTIES GENERATED TRUE)\n  add_library(dynlink_cuda STATIC dynlink_cuda.cc ${CUDA_GENERATED_STUB})\nelse()\n  add_library(dynlink_cuda STATIC dynlink_cuda.cc)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: Collects header files and stores them in the DALI_INST_HDRS variable, making them available in the parent scope.  This ensures proper header inclusion during compilation. The headers are stored in `DALI_INST_HDRS` and are accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/imgcodec/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in CMake\nDESCRIPTION: This snippet collects all the test source files within the project and stores them in the `DALI_OPERATOR_TEST_SRCS` variable. These files are used to build and run unit tests for the project. The `PARENT_SCOPE` option makes the collected test source files available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/bbox/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Import DALI and TensorFlow Libraries\nDESCRIPTION: Imports necessary libraries from NVIDIA DALI and TensorFlow, including DALI's core functionalities, TensorFlow plugin, and logging. The TensorFlow logger level is set to ERROR to reduce verbosity.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/tensorflow/tensorflow-dataset-multigpu.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali as dali\nfrom nvidia.dali import pipeline_def, Pipeline\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\n\nimport os\n\nimport nvidia.dali.plugin.tf as dali_tf\nimport tensorflow as tf\n\nimport logging\n\ntf.get_logger().setLevel(logging.ERROR)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory with CMake\nDESCRIPTION: This CMake command adds a subdirectory to the current build process. It is used to incorporate the build configurations of the 'file' subdirectory into the current CMake project.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/io/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(file)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: Adds a subdirectory named 'util' to the current build process. This allows for modularizing the project and organizing related code.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/imgcodec/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(util)\n```\n\n----------------------------------------\n\nTITLE: Executing Python to Get DALI Lib Dir\nDESCRIPTION: This CMake command executes a Python script to retrieve the DALI library directory using `nvidia.dali.sysconfig.get_lib_dir()`. The output is stored in the `DALI_LIB_DIR` variable, and whitespace is stripped from the variable.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nexecute_process(\n        COMMAND python -c \"import nvidia.dali as dali; print(dali.sysconfig.get_lib_dir())\"\n        OUTPUT_VARIABLE DALI_LIB_DIR)\nstring(STRIP ${DALI_LIB_DIR} DALI_LIB_DIR)\n```\n\n----------------------------------------\n\nTITLE: Generating nvjpeg Stub CMake\nDESCRIPTION: This snippet generates the `dynlink_nvjpeg_gen.cc` stub file using a custom command. It invokes a python script `stub_codegen.py` with the nvjpeg header and json definition files as input. The output is the generated C++ stub file. `WITH_DYNAMIC_NVJPEG` needs to be enabled for this part of the script to be executed.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/nvjpeg/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(WITH_DYNAMIC_NVJPEG)\n  set(NVJPEG_GENERATED_STUB \"${CMAKE_CURRENT_BINARY_DIR}/dynlink_nvjpeg_gen.cc\")\n  add_custom_command(\n    OUTPUT ${NVJPEG_GENERATED_STUB}\n    COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/stub_codegen.py --unique_prefix=Nvjpeg --\n    \"${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/nvjpeg.json\" ${NVJPEG_GENERATED_STUB}\n    \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}/nvjpeg.h\" \"-I${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}\"\n\n    # for some reason QNX fails with 'too many errors emitted' is this is not set\n    \"-ferror-limit=0\"\n    ${DEFAULT_COMPILER_INCLUDE}\n    DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/stub_codegen.py\n    \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}/nvjpeg.h\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/../../internal_tools/stub_generator/nvjpeg.json\"\n    COMMENT \"Running nvjpeg.h stub generator\"\n    VERBATIM)\n\n  set_source_files_properties(${NVJPEG_GENERATED_STUB} PROPERTIES GENERATED TRUE)\n  add_library(dynlink_nvjpeg STATIC nvjpeg_wrap.cc ${NVJPEG_GENERATED_STUB})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources for DALI Operators\nDESCRIPTION: This snippet uses the `collect_sources` CMake function to gather all source files related to DALI operators and store them in the DALI_OPERATOR_SRCS variable, which is accessible in the parent scope. This is essential for compiling the DALI operators.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/sequence/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting Plugin Variables CMake\nDESCRIPTION: This snippet sets the variables for the DALI plugin name, description, and extra libraries. These variables are used in the configured files for the plugin.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(DALI_PLUGIN_NAME \"video\")\nset(DALI_PLUGIN_DESCRIPTION \"Video Processing plugin with full NVENC/NVDEC hardware acceleration\")\nset(DALI_PLUGIN_EXTRA_LIBS )\n```\n\n----------------------------------------\n\nTITLE: Extract ImageNet Training Data\nDESCRIPTION: This bash script extracts the ImageNet training data from the downloaded tar archive. It creates a `train` directory, moves the archive into it, extracts the archive, removes the archive, finds all the tar files in the subdirectories, and extracts them into their respective directories.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir train && mv ILSVRC2012_img_train.tar train/ && cd train\ntar -xvf ILSVRC2012_img_train.tar && rm -f ILSVRC2012_img_train.tar\nfind . -name \"*.tar\" | while read NAME ; do mkdir -p \"${NAME%.tar}\"\ntar -xvf \"${NAME}\" -C \"${NAME%.tar}\"; rm -f \"${NAME}\"; done\ncd ..\n```\n\n----------------------------------------\n\nTITLE: Building aarch64 Jetson Linux Docker Container\nDESCRIPTION: This command builds a Docker image for cross-compiling DALI for aarch64 Jetson Linux.  It uses the provided Dockerfile and tags the image as nvidia/dali:builder_aarch64-linux.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/compilation.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t nvidia/dali:builder_aarch64-linux -f docker/Dockerfile.build.aarch64-linux .\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources with CMake\nDESCRIPTION: This CMake command gathers source files and stores them in the `DALI_OPERATOR_SRCS` variable. The `PARENT_SCOPE` option ensures that these sources are accessible to other parts of the project.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/io/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting Python Executable Path\nDESCRIPTION: Sets the path to the Python executable. If the path is not set in the environment, a fatal error is raised.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(PYTHON_EXECUTABLE $ENV{PYTHON_EXECUTABLE})\nif(NOT EXISTS ${PYTHON_EXECUTABLE})\n  message(FATAL_ERROR \"Python executable not set\\n\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Finding Python Interpreter\nDESCRIPTION: Finds the Python interpreter and sets the `PYTHON_EXECUTABLE` variable if it is not already defined.  It uses `find_package(Python COMPONENTS Interpreter)` to locate the interpreter.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED PYTHON_EXECUTABLE)\n  find_package(Python COMPONENTS Interpreter)\n  set(PYTHONINTERP_FOUND ${Python_Interpreter_FOUND})\n  set(PYTHON_EXECUTABLE ${Python_EXECUTABLE})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files with CMake\nDESCRIPTION: This CMake function collects all header files in the current directory and its subdirectories and stores them in the variable `DALI_INST_HDRS`. The `PARENT_SCOPE` option makes the variable available in the parent scope, allowing other parts of the build system to access the collected header files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/math/expressions/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Defining Data Paths for MNIST Dataset (Caffe2)\nDESCRIPTION: This code snippet defines the paths to the training and validation data for the MNIST dataset in Caffe2 format, assuming the `DALI_EXTRA_PATH` environment variable is set. It uses the `os.path.join` function to construct the full paths to the data directories. These paths are then used to configure the DALI data readers.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/frameworks/jax/pax-basic_example.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\ntraining_data_path = os.path.join(\n    os.environ[\"DALI_EXTRA_PATH\"], \"db/MNIST/training/\"\n)\nvalidation_data_path = os.path.join(\n    os.environ[\"DALI_EXTRA_PATH\"], \"db/MNIST/testing/\"\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: This CMake command adds the `fused` subdirectory to the current build process. This allows CMake to find and process the `CMakeLists.txt` file located within the `fused` directory, incorporating its targets and configurations into the overall build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/host/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(fused)\n```\n\n----------------------------------------\n\nTITLE: Adding a Shared Library\nDESCRIPTION: This snippet adds a shared library named `dali_customdummy` from the source files `dummy.cc` and `dummy.cu`. It then links the library to the DALI library. The shared library will be created for a custom DALI plugin.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/customdummy/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(dali_customdummy SHARED dummy.cc dummy.cu)\ntarget_link_libraries(dali_customdummy dali)\n```\n\n----------------------------------------\n\nTITLE: Benchmark EfficientNet with PyTorch and AutoAugment\nDESCRIPTION: This command intends to benchmark EfficientNet training using the PyTorch data loader with AutoAugment. However, the command is incomplete in the provided text.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n# PyTorch with AutoAugment:\npython multiproc.py --nproc_per_node 8 ./main.py --amp --static-loss-scale 128 \\\n                    --batch-size 128 --epochs 4 --no-checkpoints --training-only\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Test Sources with CMake\nDESCRIPTION: This snippet uses the `collect_test_sources` CMake function to find and store DALI operator test source files. The `DALI_OPERATOR_TEST_SRCS` variable will store the list of collected test source files. `PARENT_SCOPE` makes this variable available in the parent CMake scope, enabling the use of these test sources in other parts of the build system.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/resize/experimental/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: DALI Benchmark Executable Creation (CMake)\nDESCRIPTION: This snippet creates the `dali_benchmark` executable. It first adjusts the source file language properties and then uses `add_executable` to create the executable from the specified source files.  Finally, it links the necessary libraries and sets target properties for the executable.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/benchmark/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadjust_source_file_language_property(\"${DALI_BENCHMARK_SRCS}\")\n  add_executable(dali_benchmark \"${DALI_BENCHMARK_SRCS}\")\n\n  target_link_libraries(dali_benchmark PRIVATE dali dali_operators benchmark ${DALI_LIBS})\n  if (BUILD_NVML)\n    target_link_libraries(dali_benchmark PRIVATE dynlink_nvml)\n  endif(BUILD_NVML)\n  target_link_libraries(dali_benchmark PRIVATE \"-pie\")\n  set_target_properties(dali_benchmark PROPERTIES POSITION_INDEPENDENT_CODE ON)\n  set_target_properties(dali_benchmark PROPERTIES OUTPUT_NAME \"dali_benchmark.bin\")\n\n  set_target_properties(dali_benchmark PROPERTIES\n    RUNTIME_OUTPUT_DIRECTORY \"${PROJECT_BINARY_DIR}/${DALI_WHEEL_DIR}/test\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers and Sources (CMake)\nDESCRIPTION: This snippet utilizes the `collect_headers`, `collect_sources`, and `collect_test_sources` CMake macros to gather header, source, and test source files, respectively. These variables (DALI_INST_HDRS, DALI_OPERATOR_SRCS, DALI_OPERATOR_TEST_SRCS) are defined with the collected files and made available in the parent scope using the PARENT_SCOPE option.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/convolution/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources with CMake\nDESCRIPTION: This CMake function collects source files and stores them in the `DALI_KERNEL_SRCS` variable in the parent scope. This variable can then be used by CMake to compile the source files into a library or executable. This collects all the source files for the DALI kernel.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/audio/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting DALI root directory\nDESCRIPTION: Sets the `DALI_ROOT` variable to the parent directory of the current source directory. This is used to locate DALI-related files and scripts.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(DALI_ROOT \"${PROJECT_SOURCE_DIR}/..\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Operator Sources with CMake\nDESCRIPTION: The `collect_sources` CMake function collects operator source files and stores them in the `DALI_OPERATOR_SRCS` variable. The `PARENT_SCOPE` option ensures the variable is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/paste/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Generating TensorFlow Protobuf Files (Conditional) with CMake\nDESCRIPTION: This snippet conditionally generates C++ source and header files from TensorFlow protobuf definition files (`example.proto` and `feature.proto`) using the `protobuf_generate_cpp` CMake function. The generation is only performed if the `BUILD_PROTO3` variable is set to true.  The result are stored in the `TF_PROTO_SRCS` and `TF_PROTO_HEADERS` variables.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/parser/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_PROTO3)\n  protobuf_generate_cpp(TF_PROTO_SRCS TF_PROTO_HEADERS proto/example.proto proto/feature.proto)\n  add_library(TF_PROTO OBJECT ${TF_PROTO_HEADERS} ${TF_PROTO_SRCS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers with CMake\nDESCRIPTION: This CMake snippet utilizes the `collect_headers` function to gather all header files required for the DALI project. The headers are stored in the `DALI_INST_HDRS` variable and made accessible in the parent scope, allowing other parts of the CMake configuration to utilize these headers for compilation or installation.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/signal/decibel/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers with CMake\nDESCRIPTION: This snippet uses the `collect_headers` CMake function to gather all header files required for the DALI project. The `PARENT_SCOPE` argument ensures that the collected headers are accessible in the parent scope where the function is called.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/util/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Convert Pascal VOC 2012 to TFRecord (Python & Shell)\nDESCRIPTION: This snippet converts the downloaded Pascal VOC 2012 dataset to TFRecord format. It creates a `tfrecord` directory. It then sets the `PYTHONPATH` and calls the `create_pascal_tfrecord.py` script with the `--data_dir`, `--year`, and `--output_path` arguments. The script expects the Pascal VOC data to be in the `VOCdevkit` directory. The script requires a properly configured `PYTHONPATH` and the existence of `create_pascal_tfrecord.py` in the specified location.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/efficientdet/dataset/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n!mkdir tfrecord\n!PYTHONPATH=\".:$PYTHONPATH\"  python dataset/create_pascal_tfrecord.py  \\\n    --data_dir=VOCdevkit --year=VOC2012  --output_path=tfrecord/pascal\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Test Sources using CMake\nDESCRIPTION: This CMake snippet collects header files, source files, and test source files using custom functions. `collect_headers` collects header files, `collect_sources` collects source files, and `collect_test_sources` collects test source files. The collected files are stored in `DALI_INST_HDRS`, `DALI_KERNEL_SRCS`, and `DALI_KERNEL_TEST_SRCS` respectively, and made available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/audio/mel_scale/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Source Files with CMake\nDESCRIPTION: This CMake command collects test source files and stores them in the `DALI_CORE_TEST_SRCS` variable. `PARENT_SCOPE` ensures that the variable is available in the parent scope, which is useful for defining test targets and linking them to the core library.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/exec/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_CORE_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Files with CMake Macros\nDESCRIPTION: This snippet uses CMake macros to collect header, source, and test source files. It defines three variables: DALI_INST_HDRS, DALI_SRCS, and DALI_TEST_SRCS, which are populated by the collect_headers, collect_sources, and collect_test_sources macros, respectively. The PARENT_SCOPE keyword makes the variables available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/proto/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Test Sources using CMake\nDESCRIPTION: This CMake snippet utilizes custom functions to gather header files, source files, and test source files for the DALI project. It uses `collect_headers`, `collect_sources`, and `collect_test_sources` functions to populate CMake lists. The `PARENT_SCOPE` argument makes these lists accessible in the parent scope of the CMake script.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/transpose/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: This snippet collects all the header files within the project and makes them available for inclusion in other parts of the project. `DALI_INST_HDRS` variable stores the list of header files. The `PARENT_SCOPE` option makes the collected headers available in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/bbox/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources with CMake\nDESCRIPTION: This snippet uses the `collect_test_sources` CMake function to collect test source files. The collected test source file paths are stored in the `DALI_OPERATOR_TEST_SRCS` variable, making them accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/nvjpeg/fused/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Shard Size Calculation Formula with Epoch Rotation\nDESCRIPTION: This formula calculates the shard size considering epoch rotation.  The `epoch_num` is added to the shard ID (`id`) and the modulo operator (`%`) is used to handle the shard rotation, ensuring that each pipeline sees a different shard in each epoch.  It calculates the shard size for the rotated shard within the overall dataset.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/advanced_topics_sharding.rst#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nfloor(((id + epoch_num) % num_shards + 1) * dataset_size / num_shards) -\n    floor(((id + epoch_num) % num_shards) * dataset_size / num_shards)\n```\n\n----------------------------------------\n\nTITLE: CMake Executable Definition and Linking\nDESCRIPTION: This snippet defines the executable 'nodeps_test' using 'main.cc' as the source file. It then links the executable against the 'dali_core' and 'dali_kernels' libraries, which are presumably built within the DALI subdirectory.\nSOURCE: https://github.com/nvidia/dali/blob/main/qa/TL1_nodeps_build/CMakeLists_submodule.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(nodeps_test main.cc)\ntarget_link_libraries(nodeps_test dali_core dali_kernels)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory Conditional\nDESCRIPTION: This command adds a subdirectory named \"conditional\" to the current build. This typically indicates that the \"conditional\" subdirectory contains additional CMakeLists.txt files defining build targets or settings related to conditional compilation or features.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/operator/builtin/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(conditional)\n```\n\n----------------------------------------\n\nTITLE: Setting LD_PRELOAD for CUDA\nDESCRIPTION: Sets the `LD_PRELOAD` environment variable to include the CUDA library, ensuring it's loaded when generating Python stubs.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT $ENV{LD_PRELOAD} STREQUAL \"\")\n    set(NEW_LD_PRELOAD \"$ENV{LD_PRELOAD} ${cuda_LIBRARY}\")\nelse()\n    set(NEW_LD_PRELOAD \"${cuda_LIBRARY}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring DALI Core Tests in CMake\nDESCRIPTION: Creates and configures the `dali_core_test` executable, links it against the `dali_core` library and other dependencies (gtest, dynlink_cuda, DALI_LIBS, dynlink_cufile if BUILD_CUFILE is enabled). Also configures properties, output name, and runtime output directory for the test executable. Finally, it adds a GTest check target.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_TEST)\n  adjust_source_file_language_property(\"${DALI_CORE_TEST_SRCS}\")\n  add_executable(dali_core_test \"${DALI_CORE_TEST_SRCS}\")\n  target_link_libraries(dali_core_test PUBLIC dali_core)\n  target_link_libraries(dali_core_test PRIVATE gtest dynlink_cuda ${DALI_LIBS})\n  if (BUILD_CUFILE)\n    target_link_libraries(dali_core_test PRIVATE dynlink_cufile)\n  endif()\n  target_link_libraries(dali_core_test PRIVATE \"-Wl,--exclude-libs,${exclude_libs}\")\n  target_link_libraries(dali_core_test PRIVATE \"-pie\")\n  set_target_properties(dali_core_test PROPERTIES POSITION_INDEPENDENT_CODE ON)\n  set_target_properties(dali_core_test PROPERTIES OUTPUT_NAME \"dali_core_test.bin\")\n\n  set_target_properties(dali_core_test PROPERTIES\n    RUNTIME_OUTPUT_DIRECTORY ${TEST_BINARY_DIR})\n\n  add_check_gtest_target(\"check-core-gtest\" dali_core_test ${TEST_BINARY_DIR})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Library Exports\nDESCRIPTION: This section configures the library exports using a version script. It copies a template version script to the binary directory and links the `dali_kernels` library with the script, which controls which symbols are exported.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nset(lib_exports \"libdali_kernels.map\")\nconfigure_file(\"${DALI_ROOT}/cmake/${lib_exports}.in\" \"${CMAKE_BINARY_DIR}/${lib_exports}\")\ntarget_link_libraries(dali_kernels PRIVATE  -Wl,--version-script=${CMAKE_BINARY_DIR}/${lib_exports})\n```\n\n----------------------------------------\n\nTITLE: Setting DALI Variables in Parent Scope with CMake\nDESCRIPTION: This snippet sets the `DALI_OPERATOR_SRCS` and `DALI_INST_HDRS` variables in the parent scope. This allows other CMake scripts to access the lists of source files and header files that have been configured in this file.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/decoder/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(DALI_OPERATOR_SRCS ${DALI_OPERATOR_SRCS} PARENT_SCOPE)\nset(DALI_INST_HDRS ${DALI_INST_HDRS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory using CMake\nDESCRIPTION: This snippet adds a subdirectory named `expression_factory_instances` to the current CMake project. This allows CMake to process the CMakeLists.txt file in that subdirectory and include its targets in the build process. The subdirectory typically contains additional source code or configurations.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/math/expressions/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(expression_factory_instances)\n```\n\n----------------------------------------\n\nTITLE: Conditionally Removing a Source File in CMake\nDESCRIPTION: This snippet conditionally removes the `tiff_libtiff.cc` source file from the `DALI_OPERATOR_SRCS` list based on the `BUILD_LIBTIFF` CMake option. If `BUILD_LIBTIFF` is not enabled, the specified source file is removed from the compilation process. This allows to exclude tiff support if the tiff library is not enabled.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/image/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT BUILD_LIBTIFF)\n    list(REMOVE_ITEM DALI_OPERATOR_SRCS\n        ${CMAKE_CURRENT_SOURCE_DIR}/tiff_libtiff.cc\n    )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Video Plugin Sources\nDESCRIPTION: Defines the source files for the DALI video plugin, including video decoder and color space conversion implementations.\nSOURCE: https://github.com/nvidia/dali/blob/main/plugins/video/pkg_src/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nset(VIDEO_PLUGIN_SOURCES\n    src/decoder/video_decoder_mixed.cc\n    src/decoder/color_space.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Collecting Sources with CMake\nDESCRIPTION: This CMake macro, `collect_sources`, is used to collect source files for the DALI project. It accepts a variable name (`DALI_OPERATOR_SRCS`) and a scope (`PARENT_SCOPE`), which are likely used to store and make available the list of collected source files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/sequence/optical_flow/optical_flow_impl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Creating the `dali` library\nDESCRIPTION: This snippet creates the `dali` library, linking against the listed source files, proto objects and dependencies. The `LIBTYPE` variable determines whether it's a shared or static library.  The output directory is set to `DALI_LIBRARY_OUTPUT_DIR`.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_PROTOBUF)\n  set(DALI_PROTO_OBJ $<TARGET_OBJECTS:DALI_PROTO>)\n  adjust_source_file_language_property(\"${DALI_SRCS}\")\n  add_library(dali ${LIBTYPE} ${DALI_SRCS} ${DALI_PROTO_OBJ} ${CUDART_LIB})\n  set_target_properties(dali PROPERTIES LIBRARY_OUTPUT_DIRECTORY \"${DALI_LIBRARY_OUTPUT_DIR}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in DALI with CMake\nDESCRIPTION: This CMake macro collects header files and stores them in the `DALI_INST_HDRS` variable within the parent scope.  It's used for managing header files required for the DALI library. The macro requires CMake and the `collect_headers` function to be defined elsewhere.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/data/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files with GLOB in CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command to find all `.cc` and `.cu` files in the current directory and store their paths in the `SRCS` variable. This dynamically includes source files for the plugin build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/test/plugins/dummy/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cc *.cu)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Operator Test Sources in CMake\nDESCRIPTION: This CMake macro function collects test source files related to DALI operators and stores them in the `DALI_OPERATOR_TEST_SRCS` variable. It utilizes the `collect_test_sources` function to find and add test source files to the specified variable. The `PARENT_SCOPE` argument ensures that the variable is accessible in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/decoder/host/fused/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Invert Augmentation Definition\nDESCRIPTION: This code snippet defines the `invert` augmentation using the `@augmentation` decorator. It inverts the image. It takes the data and an underscore as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n@augmentation\ndef invert(data, _)\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Source and Header Files with CMake\nDESCRIPTION: This CMake snippet collects header files, source files, and test source files for the NVIDIA DALI project.  It uses the `collect_headers`, `collect_sources`, and `collect_test_sources` functions, which are presumably defined elsewhere in the CMake configuration. The files are collected into `DALI_INST_HDRS`, `DALI_OPERATOR_SRCS`, and `DALI_OPERATOR_TEST_SRCS` respectively, and made available in parent scopes.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/generic/slice/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Installing pynvml with Specific Version\nDESCRIPTION: This snippet installs the `pynvml` package with a specific version (11.0.0).  `pynvml` provides Python bindings for the NVIDIA Management Library (NVML), allowing monitoring and management of NVIDIA GPUs. Specifying a version ensures compatibility and avoids potential issues with newer or older versions.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/requirements.txt#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npynvml==11.0.0\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files in DALI\nDESCRIPTION: This snippet uses custom CMake functions (collect_headers, collect_sources, collect_test_sources) to gather header, source, and test source files. The `PARENT_SCOPE` argument makes the collected file lists available in the parent scope, likely for use in defining libraries or executables.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files in CMake\nDESCRIPTION: This snippet uses a custom CMake function, 'collect_headers', to gather header files from specified locations.  The result is stored in the 'DALI_INST_HDRS' variable in the parent scope, making the list of header files available for other parts of the CMake configuration.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/executor/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in DALI with CMake\nDESCRIPTION: This CMake snippet collects the source files for the DALI kernel tests. The `collect_test_sources` function is called to gather the test source files, which are stored in `DALI_KERNEL_TEST_SRCS`. The `PARENT_SCOPE` argument ensures that these test sources are accessible in the parent scope for compilation and execution.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/geom/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Evaluating EfficientDet with DALI (GPU)\nDESCRIPTION: This command evaluates the EfficientDet model using the DALI GPU pipeline. It requires the evaluation data file pattern, number of evaluation steps, and the path to the trained weights file.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/efficientdet/README.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\npython3 eval.py \\\n            --pipeline dali_gpu \\\n\t    --input_type tfrecord \\\n            --eval_file_pattern './tfrecords/eval*.tfrecord' \\\n            --eval_steps 5000 \\\n            --weights final_weights.h5\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI Headers with CMake\nDESCRIPTION: This CMake command collects header files for the DALI library and makes them available in the parent scope. `DALI_INST_HDRS` is the variable that will contain the list of headers collected.  The `PARENT_SCOPE` option ensures the variable is accessible in the calling scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/signal/window/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Test Sources with CMake\nDESCRIPTION: This CMake code snippet utilizes custom functions `collect_headers`, `collect_sources`, and `collect_test_sources` to gather header files, source files, and test source files respectively. The results are stored in variables DALI_INST_HDRS, DALI_KERNEL_SRCS, and DALI_KERNEL_TEST_SRCS. The `PARENT_SCOPE` option ensures that these variables are accessible in the parent scope, making them available for use in other parts of the CMake configuration.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/slice/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: This line adds the 'join' subdirectory to the build process. It allows including the functionality and build configurations defined within that subdirectory into the main DALI kernel build.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(join)\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv4 with DALI and TensorFlow\nDESCRIPTION: This command trains the YOLOv4 model using DALI for data loading on all available GPUs. It specifies the training data path, annotations file, batch size, number of epochs, steps per epoch, and output file. Mosaic augmentation is enabled and DALI is configured to run on the GPU.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/yolov4/readme.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython src/main.py train /coco/train2017 /coco/annotations/instances_train2017.json \\\n  -b 8 -e 6 -s 1000 -o output.h5 \\\n  --pipeline dali-gpu --multigpu --use_mosaic\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers and Sources in CMake\nDESCRIPTION: These CMake functions collect header files, source files, and test sources within the DALI project. `collect_headers`, `collect_sources`, and `collect_test_sources` are assumed to be custom CMake functions that gather the specified file types and store them in the given variables with `PARENT_SCOPE` to make them available to the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Run the DALI Pipeline\nDESCRIPTION: Instantiates the pipeline, builds it, and runs it to obtain the results of the arithmetic operations. This step is crucial for executing the defined data processing flow within DALI.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/general/expressions/expr_examples.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipe = pipeline()\npipe.build()\nout = pipe.run()\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Sources in DALI with CMake\nDESCRIPTION: This CMake snippet uses the `collect_test_sources` macro to find and aggregate all test source files in the current directory and its subdirectories. The resulting list of file paths is stored in the `DALI_OPERATOR_TEST_SRCS` variable, which is accessible in the parent scope. This assumes a certain file structure to identify files for tests.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/image/distortion/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Conditional Operator Source Appending (LMDB) in CMake\nDESCRIPTION: Conditionally appends source files for Caffe reader operators to the `DALI_OPERATOR_SRCS` list, based on the `BUILD_LMDB` build flag. This allows for including Caffe reader support only when the LMDB library is available.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_LMDB)\n  list(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/caffe_reader_op.cc\")\n  list(APPEND DALI_OPERATOR_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/caffe2_reader_op.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting DALI sources in CMake\nDESCRIPTION: The `collect_sources` CMake function collects all source files and stores them in the `DALI_CORE_SRCS` variable within the parent scope. This list of sources is used later in the build process to compile the DALI core library. The variable `DALI_CORE_SRCS` is created or updated in the parent scope to be accessible by other CMake files.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/os/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_sources(DALI_CORE_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version\nDESCRIPTION: This snippet sets the minimum required version of CMake for the project to 3.10. This ensures that the CMake features used in the script are supported by the CMake version installed on the system.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/customdummy/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.10)\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Architectures\nDESCRIPTION: This snippet defines the CUDA architectures that the project will be compiled for. It specifies a list of architectures (50, 60, 70, 80, 90), which correspond to different NVIDIA GPU generations. This allows the compiled code to be optimized for specific GPU hardware.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/customdummy/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CUDA_ARCHITECTURES \"50;60;70;80;90\")\n```\n\n----------------------------------------\n\nTITLE: Conditionally Removing Source File in CMake\nDESCRIPTION: This snippet conditionally removes `shared_mem.cc` from the `DALI_CORE_SRCS` list if the `BUILD_SHM_WRAPPER` option is not enabled. This allows for excluding shared memory functionality when it's not required.  The updated `DALI_CORE_SRCS` variable is then set in the parent scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/core/os/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT BUILD_SHM_WRAPPER)\n  list(REMOVE_ITEM DALI_CORE_SRCS \"${CMAKE_CURRENT_SOURCE_DIR}/shared_mem.cc\")\n  set(DALI_CORE_SRCS ${DALI_CORE_SRCS} PARENT_SCOPE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers in CMake\nDESCRIPTION: This snippet uses the `collect_headers` CMake macro to gather header files within the NVIDIA DALI project. The macro populates the `DALI_INST_HDRS` variable in the parent scope, making the header files available for use in other parts of the project.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/jpeg/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Test Source Files with CMake\nDESCRIPTION: This CMake macro collects test source files and makes them available in the parent scope. It finds test-related source files based on project conventions and adds them to the `DALI_KERNEL_TEST_SRCS` variable. The PARENT_SCOPE option ensures that the variable is available in the calling scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/convolution/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_test_sources(DALI_KERNEL_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Determining GCC System Include Dirs (CMake)\nDESCRIPTION: This snippet uses the `DETERMINE_GCC_SYSTEM_INCLUDE_DIRS` function to infer the system include directories used by the C++ compiler. It passes the compiler path, compiler flags, and the variable to store the inferred include directories (`INFERED_COMPILER_INCLUDE`).\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/video/dynlink_nvcuvid/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nDETERMINE_GCC_SYSTEM_INCLUDE_DIRS(\"c++\" \"${CMAKE_CXX_COMPILER}\" \"${CMAKE_CXX_FLAGS}\" INFERED_COMPILER_INCLUDE)\n```\n\n----------------------------------------\n\nTITLE: Building the benchmark suite\nDESCRIPTION: This snippet includes the `benchmark` subdirectory if the `BUILD_BENCHMARK` option is enabled, allowing the DALI benchmark suite to be built.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_BENCHMARK)\n  # get benchmark main\n  add_subdirectory(benchmark)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for Operator Tests in CMake\nDESCRIPTION: This line adds the 'op_test' subdirectory to the build process. This is typically used to organize tests that require non-builtin operators within the DALI project.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/c_api_2/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(op_test)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers, Sources, and Test Sources with CMake\nDESCRIPTION: This snippet uses custom CMake functions (likely defined elsewhere) to collect header files, source files, and test source files for the DALI operator. These functions populate the `DALI_INST_HDRS`, `DALI_OPERATOR_SRCS`, and `DALI_OPERATOR_TEST_SRCS` variables respectively, making them available for use in the build process, typically to define libraries or executables. `PARENT_SCOPE` makes those variables accessible in the parent scope of the current CMake file.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/sequence/optical_flow/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\ncollect_sources(DALI_OPERATOR_SRCS PARENT_SCOPE)\ncollect_test_sources(DALI_OPERATOR_TEST_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Collecting Headers with CMake\nDESCRIPTION: This CMake function collects header files and stores them in the `DALI_INST_HDRS` variable in the parent scope.  This is used for installation purposes, to make header files available to other projects that depend on this one. The headers collected will typically be those meant for public use.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/audio/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_headers(DALI_INST_HDRS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet adds the `expressions` and `normalize` subdirectories to the current CMake project. This allows the build system to process the CMakeLists.txt files within those subdirectories, incorporating their targets and configurations into the main project.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/math/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(expressions)\nadd_subdirectory(normalize)\n```\n\n----------------------------------------\n\nTITLE: Conditional Inclusion of FITS Loader (CMake)\nDESCRIPTION: This snippet conditionally includes the FITS loader source files (`fits_loader.cc` and `fits_loader_gpu.cc`) if the `BUILD_CFITSIO` flag is enabled. This allows DALI to load data from FITS files when CFITSIO is available.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/operators/reader/loader/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_CFITSIO)\n  set(DALI_OPERATOR_SRCS ${DALI_OPERATOR_SRCS}\n    \"${CMAKE_CURRENT_SOURCE_DIR}/fits_loader.cc\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/fits_loader_gpu.cc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: CMake DALI Subdirectory and Includes\nDESCRIPTION: This snippet adds the DALI library as a subdirectory to the project. It also includes several directories in the include path, including DALI's include directory, the root DALI directory, and the boost preprocessor include directory.\nSOURCE: https://github.com/nvidia/dali/blob/main/qa/TL1_nodeps_build/CMakeLists_submodule.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(dali)\n\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/dali/include)\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/dali/)\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/dali/third_party/boost/preprocessor/include)\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files with CMake\nDESCRIPTION: This CMake macro collects source files and makes them available in the parent scope. It identifies source files based on project conventions and adds them to the `DALI_KERNEL_SRCS` variable. The PARENT_SCOPE option makes this variable accessible to the calling scope.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/kernels/imgproc/convolution/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncollect_sources(DALI_KERNEL_SRCS PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: This snippet adds a subdirectory named 'executor2' to the current CMake project. This allows including the CMakeLists.txt file present within 'executor2' in the current build process, enabling modular project structures and separation of concerns.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/pipeline/executor/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(executor2)\n```\n\n----------------------------------------\n\nTITLE: Installing DALI Weekly (CUDA 12.0) via pip\nDESCRIPTION: Installs the latest weekly build of NVIDIA DALI and the TensorFlow plugin built for CUDA 12.0 using pip. This command fetches the package from NVIDIA's weekly build index and upgrades if an older version is present. It requires `tensorflow-gpu` to be installed beforehand.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist/weekly --upgrade nvidia-dali-weekly-cuda120\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist/weekly --upgrade nvidia-dali-tf-plugin-weekly-cuda120\n```\n\n----------------------------------------\n\nTITLE: Identity Augmentation Definition\nDESCRIPTION: This code snippet defines the `identity` augmentation using the `@augmentation` decorator. It represents the identity operation, where no processing is applied. It takes the data and an underscore as arguments.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/auto_aug/augmentations.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n@augmentation\ndef identity(data, _)\n```\n\n----------------------------------------\n\nTITLE: Setting DALI variables\nDESCRIPTION: This snippet sets several variables used throughout the DALI build process. It defines source file lists, output directories for libraries and test binaries, and other configuration parameters.\nSOURCE: https://github.com/nvidia/dali/blob/main/dali/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(DALI_SRCS)\nset(DALI_OPERATOR_SRCS)\nset(DALI_TEST_SRCS)\nset(DALI_BENCHMARK_SRCS)\nset(DALI_TF_SRCS)\n\nset(dali_python_function_lib \"python_function_plugin\")\nset(DALI_WHEEL_DIR \"dali/python/nvidia/dali\")\nset(DALI_INCLUDE_DIR \"${DALI_WHEEL_DIR}/include/\")\nset(DALI_LIBRARY_OUTPUT_DIR \"${PROJECT_BINARY_DIR}/${DALI_WHEEL_DIR}\")\nset(TEST_BINARY_DIR \"${PROJECT_BINARY_DIR}/${DALI_WHEEL_DIR}/test\")\n```\n\n----------------------------------------\n\nTITLE: Installing Legacy DALI (CUDA 10.0) via pip\nDESCRIPTION: Installs an older version of NVIDIA DALI and the TensorFlow plugin built for CUDA 10.0 using pip. This command fetches the package from NVIDIA's CUDA 10.0 index and upgrades if an older version is present.  This targets DALI versions up to 1.3.0 and requires setting the correct pip index.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/installation.rst#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/10.0 --upgrade nvidia-dali\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/10.0 --upgrade nvidia-dali-tf-plugin\n```\n\n----------------------------------------\n\nTITLE: Benchmark EfficientNet with PyTorch (No Augmentations)\nDESCRIPTION: This command benchmarks EfficientNet training using the native PyTorch data loader without any automatic augmentations. The `data-backend` flag is set to `pytorch` and the `automatic-augmentation` flag is set to `disabled`.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/pytorch/efficientnet/readme.rst#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n# PyTorch without automatic augmentations\npython multiproc.py --nproc_per_node 8 ./main.py --amp --static-loss-scale 128 \\\n                    --batch-size 128 --epochs 4 --no-checkpoints --training-only \\\n                    --data-backend pytorch --automatic-augmentation disabled \\\n                    --workspace $RESULT_WORKSPACE \\\n                    --report-file bench_report_pytorch.json $PATH_TO_IMAGENET\n```\n\n----------------------------------------\n\nTITLE: Display Training Options - Bash\nDESCRIPTION: This command displays all available options for the train.py script. It allows users to customize the training process by specifying different parameters and configurations. It requires access to the train.py script.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/paddle/resnet50/paddle-resnet50.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --help\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard\nDESCRIPTION: These CMake commands set the C++ standard to C++17, make it required, and disable extensions.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/naive_histogram/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\n```\n\n----------------------------------------\n\nTITLE: Evaluation Usage\nDESCRIPTION: This shows the usage for the evaluation script.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/tensorflow/yolov4/readme.rst#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nusage: main.py eval [-h] file_root annotations [--weights WEIGHTS]\n                    [--batch_size BATCH_SIZE] [--steps STEPS]\n```\n\n----------------------------------------\n\nTITLE: Restoring DALI Pipeline from Checkpoint (Python)\nDESCRIPTION: This snippet illustrates how to restore a DALI pipeline from a saved checkpoint by passing the `checkpoint` argument to the `Pipeline` constructor. The checkpoint data is read from the file and used to initialize the pipeline's state. The pipeline being restored must be the same as the original.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/advanced_topics_checkpointing.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint = open('checkpoint_file.cpt', 'rb').read()\np_restored = pipeline(checkpoint=checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Legacy Operator Object API Pipeline in DALI\nDESCRIPTION: This code snippet demonstrates how to define a DALI pipeline using the legacy operator object API. It defines a custom pipeline class `CustomPipe` that initializes operator objects such as `dali.ops.readers.File`, `dali.ops.ImageDecoder`, `dali.ops.Rotate`, and `dali.ops.Resize`. The `define_graph` method defines the pipeline graph using these operator objects. This API is now discouraged.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/supported_ops_legacy.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport nvidia.dali as dali\n\nclass CustomPipe(Pipeline):\n    def __init__(self, batch_size, num_threads, device_id):\n        super(CustomPipe, self).__init__(batch_size, num_threads, device_id)\n        self.reader = dali.ops.readers.File(file_root='./my_file_root')\n        self.decoder = dali.ops.ImageDecoder(device='mixed')\n        self.rotate = dali.ops.Rotate()\n        self.resize = dali.ops.Resize(resize_x=300, resize_y=300)\n        self.rng = dali.ops.random.Uniform(range=(-45, 45))\n\n    def define_graph(self):\n        files, labels = self.reader()\n        images = self.decoder(files)\n        images = self.rotate(images, angle=self.rng())\n        images = self.resize(images)\n        return images, labels\n\npipe = CustomPipe(batch_size = 3, num_threads = 2, device_id = 0)\noutputs = pipe.run()\n```\n\n----------------------------------------\n\nTITLE: BibTeX entry for Video Super-Resolution with Motion Compensation\nDESCRIPTION: BibTeX entry for citing the paper \"End-to-End Learning of Video Super-Resolution with Motion Compensation\".\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/use_cases/video_superres/README.rst#_snippet_4\n\nLANGUAGE: BibTeX\nCODE:\n```\n@InProceedings{IB17,\n author       = \"O. Makansi and E. Ilg and and Thomas Brox\",\n title        = \"End-to-End Learning of Video Super-Resolution with Motion\n Compensation\",\n booktitle    = \"German Conference on Pattern Recognition (GCPR) 2017\",\n month        = \" \",\n year         = \"2017\",\n url          = \"http://lmb.informatik.uni-freiburg.de/Publications/2017/IB17\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard\nDESCRIPTION: This snippet sets the C++ standard to C++17, requires it, and disables extensions. It ensures that the C++ code is compiled using the C++17 standard and that non-standard extensions are disabled.\nSOURCE: https://github.com/nvidia/dali/blob/main/docs/examples/custom_operations/custom_operator/customdummy/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\n```"
  }
]